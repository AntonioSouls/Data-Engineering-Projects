<!DOCTYPE html><html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data</title>
<!--Generated on Mon Oct  7 14:25:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">




<base href="https://arxiv.org/html/2410.05078v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2410.05078v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2410.05078v1/#myForm">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2410.05078v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2410.05078v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S1" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S1.SS0.SSS0.Px1" title="In 1 Introduction ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Main Contributions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S2" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S3" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S3.SS0.SSS0.Px1" title="In 3 Related Work ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Compression Without Transformers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S3.SS0.SSS0.Px2" title="In 3 Related Work ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Online Transformers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S3.SS0.SSS0.Px3" title="In 3 Related Work ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Pre-Trained Transformers</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px1" title="In 4 Methods ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px2" title="In 4 Methods ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px3" title="In 4 Methods ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">(No) Tokenization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px4" title="In 4 Methods ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px5" title="In 4 Methods ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Training Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px6" title="In 4 Methods ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Out-of-Distribution Evaluation Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.SS0.SSS0.Px1" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Small Transformers Can Be Domain-General Compressors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.SS0.SSS0.Px2" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">What You See Is What You Get</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.SS0.SSS0.Px3" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Scaling Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.SS0.SSS0.Px4" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Model Size vs.&nbsp;Context Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.SS0.SSS0.Px5" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Sliding Window</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S6" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S6.SS0.SSS0.Px1" title="In 6 Discussion ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Limitations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S7" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Experimental Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS1" title="In Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Training Data Sources</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS1.SSS0.Px1" title="In A.1 Training Data Sources ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Text</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS1.SSS0.Px2" title="In A.1 Training Data Sources ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Image</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS1.SSS0.Px3" title="In A.1 Training Data Sources ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Audio</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2" title="In Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Out-of-Distribution Evaluation Data Sources</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2.SSS0.Px1" title="In A.2 Out-of-Distribution Evaluation Data Sources ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Text</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2.SSS0.Px2" title="In A.2 Out-of-Distribution Evaluation Data Sources ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2.SSS0.Px3" title="In A.2 Out-of-Distribution Evaluation Data Sources ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Audio</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2.SSS0.Px4" title="In A.2 Out-of-Distribution Evaluation Data Sources ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Multimodal Evaluations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="In Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Sweeps</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3.SSS0.Px1" title="In A.3 Sweeps ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Model Size vs.&nbsp;Dataset Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3.SSS0.Px2" title="In A.3 Sweeps ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Model Size vs.&nbsp;Context Size</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS4" title="In Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Computational Resources</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.SS1" title="In Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Compression Ratios</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.SS2" title="In Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Running Times</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.SS3" title="In Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>Compressing Model Parameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.SS4" title="In Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>Scaling Analysis for Multimodal Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY 4.0</a><div id="watermark-tr">arXiv:2410.05078v1 [cs.LG] 07 Oct 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.5">\correspondingauthor</span>
<p class="ltx_p" id="p1.4"><sup class="ltx_sup" id="p1.4.1">∗</sup>Equal contribution.
Correspondence to {anianr, timgen}@google.com.
<sup class="ltx_sup" id="p1.4.2">1</sup>Chandar Research Lab. MILA - Quebec AI Institute. Polytechnique Montréal.
<sup class="ltx_sup" id="p1.4.3">2</sup>Google DeepMind.
<math alttext="\dagger" class="ltx_Math" display="inline" id="p1.4.m4.1"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.4.m4.1d">†</annotation></semantics></math> Work performed while the author was at Google DeepMind.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<h1 class="ltx_title ltx_title_document">Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Heurtel-Depeiges<sup class="ltx_sup" id="id5.2.id1"><span class="ltx_text ltx_font_italic" id="id5.2.id1.1">∗1†</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Anian Ruoss<sup class="ltx_sup" id="id6.2.id1"><span class="ltx_text ltx_font_italic" id="id6.2.id1.1">∗2</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Joel Veness<sup class="ltx_sup" id="id7.2.id1"><span class="ltx_text ltx_font_italic" id="id7.2.id1.1">2</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Tim Genewein<sup class="ltx_sup" id="id8.2.id1"><span class="ltx_text ltx_font_italic" id="id8.2.id1.1">2</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id9.id1">Foundation models have recently been shown to be strong data compressors.
However, when accounting for their excessive parameter count, their compression ratios are actually inferior to standard compression algorithms.
Moreover, naively reducing the number of parameters may not necessarily help as it leads to worse predictions and thus weaker compression.
In this paper, we conduct a large-scale empirical study to investigate whether there is a sweet spot where competitive compression ratios with pre-trained vanilla transformers are possible.
To this end, we train families of models on 165GB of raw byte sequences of either text, image, or audio data (and all possible combinations of the three) and then compress 1GB of out-of-distribution (OOD) data from each modality.
We find that relatively small models (i.e., millions of parameters) can outperform standard general-purpose compression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG, JPEG&nbsp;2000, FLAC) — even when factoring in parameter count.
We achieve, e.g., the lowest compression ratio of 0.49 on OOD audio data (vs.&nbsp;0.54 for FLAC).
To study the impact of model- and dataset scale, we conduct extensive ablations and hyperparameter sweeps, and we investigate the effect of unimodal versus multimodal training.
We find that even small models can be trained to perform well on multiple modalities, but, in contrast to previously reported results with large-scale foundation models, transfer to unseen modalities is generally weak.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Strong predictive models can straightforwardly be turned into strong lossless compressors, e.g., via arithmetic coding&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pasco, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib52" title="">1977</a>; Rissanen, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib58" title="">1976</a>; Witten et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib81" title="">1987</a>)</cite>.
Consequently, large pre-trained foundation models, such as LLMs, achieve very high data compression on their training distributions and beyond&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Delétang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>.
However, when factoring in these models’ parameter count into the compression ratio, too large models actually perform worse.
For this reason, large foundation models with parameter counts on the order of billions cannot compete with standard compression algorithms such as gzip&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Deutsch, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib12" title="">1996</a>)</cite> or LZMA2&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pavlov, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib53" title="">2019</a>)</cite>.
The goal of this paper is thus to investigate whether pre-trained vanilla transformers can achieve compression ratios that are competitive with standard algorithms across a range of data modalities.
This places fairly tight constraints on the maximal model size, leading us to investigate families of relatively small transformers (with millions of parameters).
Note that our aim is not to build a practical transformer-based data compressor, as the computational footprint (running time, memory, FLOPs) of even small models is far beyond standard compressors.
Instead, studying compression via pre-trained models provides insight into the models’ <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">learned</em> inductive biases, e.g., whether they are domain-general, how they depend on the training data composition, and whether there is transfer between modalities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.3">Recently, <cite class="ltx_cite ltx_citemacro_citet">Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite> stated that “language modeling is compression”, pointing out that log-loss minimization is equivalent to optimizing a lossless compression objective. To illustrate this point, the authors used billion-parameter LLMs that were trained exclusively on text (Llama&nbsp;2 from <cite class="ltx_cite ltx_citemacro_citet">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib72" title="">2023b</a>)</cite> and Chinchilla from <cite class="ltx_cite ltx_citemacro_citet">Hoffmann et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib22" title="">2022</a>)</cite>) to compress <math alttext="1" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><mn id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><cn id="S1.p2.1.m1.1.1.cmml" type="integer" xref="S1.p2.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">1</annotation></semantics></math>GB of image and audio data from ImageNet&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Russakovsky et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib59" title="">2015</a>)</cite> and LibriSpeech&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Panayotov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib51" title="">2015</a>)</cite>, respectively.
They found that these models compress better than gzip or LZMA2 and even domain-specific compressors such as PNG&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Boutell, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib5" title="">1997</a>)</cite> and FLAC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Coalson, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib8" title="">2008</a>)</cite>, but only when parameter counts are not being accounted for.
To see if competitive performance is possible, they also trained small-scale transformers (up to <math alttext="3.2" class="ltx_Math" display="inline" id="S1.p2.2.m2.1"><semantics id="S1.p2.2.m2.1a"><mn id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">3.2</mn><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><cn id="S1.p2.2.m2.1.1.cmml" type="float" xref="S1.p2.2.m2.1.1">3.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">3.2</annotation><annotation encoding="application/x-llamapun" id="S1.p2.2.m2.1d">3.2</annotation></semantics></math>M parameters) on <math alttext="1" class="ltx_Math" display="inline" id="S1.p2.3.m3.1"><semantics id="S1.p2.3.m3.1a"><mn id="S1.p2.3.m3.1.1" xref="S1.p2.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S1.p2.3.m3.1b"><cn id="S1.p2.3.m3.1.1.cmml" type="integer" xref="S1.p2.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.3.m3.1c">1</annotation><annotation encoding="application/x-llamapun" id="S1.p2.3.m3.1d">1</annotation></semantics></math>GB of Wikipedia&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib26" title="">2006</a>)</cite>, but found that these models were significantly worse at compressing images and audio data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S1.F1.1.g1" src="https://arxiv.org/html/2410.05078v1/x1.png" width="664">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Overview of our training and evaluation data pipelines.
We consider three data modalities: text, images, and audio.
From these modalities we create training data mixtures of <math alttext="165" class="ltx_Math" display="inline" id="S1.F1.3.m1.1"><semantics id="S1.F1.3.m1.1b"><mn id="S1.F1.3.m1.1.1" xref="S1.F1.3.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><cn id="S1.F1.3.m1.1.1.cmml" type="integer" xref="S1.F1.3.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">165</annotation><annotation encoding="application/x-llamapun" id="S1.F1.3.m1.1e">165</annotation></semantics></math>GB that are either unimodal or multimodal.
After pre-training transformers on each of these datasets, we evaluate their compression ratio (i.e., factoring in models’ parameter counts) on each of the three modalities.
If the corresponding modality has not been seen during training, we refer to the evaluation as ‘out-of-modality’, otherwise it is ‘in-modality’.
Importantly, our evaluation is always performed on out-of-distribution data (different from any of the training data sources), even when it is in-modality.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.2">The obvious open question is whether small transformers pre-trained on large (multimodal) datasets can achieve competitive compression ratios across different modalities and whether there is transfer to unseen modalities, as observed in the large-scale model case.
We therefore conduct an extensive empirical study where we train families of decoder-only transformers on <math alttext="165" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mn id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><cn id="S1.p3.1.m1.1.1.cmml" type="integer" xref="S1.p3.1.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">165</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">165</annotation></semantics></math>GB of either text, image, or audio data and all combinations of the three. We then use these models (with frozen parameters, i.e., offline training) to compress <math alttext="1" class="ltx_Math" display="inline" id="S1.p3.2.m2.1"><semantics id="S1.p3.2.m2.1a"><mn id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><cn id="S1.p3.2.m2.1.1.cmml" type="integer" xref="S1.p3.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="S1.p3.2.m2.1d">1</annotation></semantics></math>GB of out-of-distribution (OOD) data from all three modalities (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S1.F1" title="In 1 Introduction ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>).
We also compare against transformers that are trained purely online, i.e., on the data stream that is being compressed <cite class="ltx_cite ltx_citemacro_citep">(Bellard, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib3" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite>, meaning that storage or communication of the transformer weights for decompression is not required (unlike for our pre-trained models).
These online transformers currently achieve state-of-the-art results on the Large Text Compression Benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mahoney, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib43" title="">2006</a>)</cite>.
Overall we find that small pre-trained transformers achieve competitive compression ratios, as our best models consistently outperform domain-general and domain-specific standard compression algorithms and are on par with the online transformers from <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Main Contributions</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">We make the following key contributions:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p2">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We conduct a large-scale empirical study (hyperparameter sweeps, ablations) on the compression performance of small transformers trained on raw byte sequences of text, image, and audio data (and all combinations), across various model- and dataset sizes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We are the first to show that small pre-trained transformers achieve better compression ratios than general-purpose and domain-specific compressors on <math alttext="1" class="ltx_Math" display="inline" id="S1.I1.i2.p1.1.m1.1"><semantics id="S1.I1.i2.p1.1.m1.1a"><mn id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><cn id="S1.I1.i2.p1.1.m1.1.1.cmml" type="integer" xref="S1.I1.i2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.p1.1.m1.1d">1</annotation></semantics></math>GB of out-of-distribution data across different modalities, e.g., 0.49 on audio vs.&nbsp;0.51 for <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite> &amp; 0.54 for FLAC.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We show that training on multiple modalities only slightly deteriorates the performance on each individual modality but significantly boosts the compression ratios on multimodal data, as long as all the evaluation modalities are part of the training data mixture.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We demonstrate that small pre-trained transformers fail to beat standard compressors on unseen data modalities (i.e., modalities they were not trained on), meaning that there is only weak transfer to novel modalities (which is not the case for LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Delétang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.8">Compression and prediction are “two sides of the same coin”&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(MacKay, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib41" title="">2003</a>)</cite>.
This fundamental duality stems directly from Shannon’s celebrated lossless source coding theorem&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shannon, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib64" title="">1948</a>)</cite>, which states that there is a well-defined lower bound for encoding data from a probabilistic source.
For any data sequence <math alttext="{x_{1:n}:=x_{1}x_{2}\ldots x_{n}\in\mathcal{X}^{n}}" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><msub id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml"><mi id="S2.p1.1.m1.1.1.2.2" xref="S2.p1.1.m1.1.1.2.2.cmml">x</mi><mrow id="S2.p1.1.m1.1.1.2.3" xref="S2.p1.1.m1.1.1.2.3.cmml"><mn id="S2.p1.1.m1.1.1.2.3.2" xref="S2.p1.1.m1.1.1.2.3.2.cmml">1</mn><mo id="S2.p1.1.m1.1.1.2.3.1" lspace="0.278em" rspace="0.278em" xref="S2.p1.1.m1.1.1.2.3.1.cmml">:</mo><mi id="S2.p1.1.m1.1.1.2.3.3" xref="S2.p1.1.m1.1.1.2.3.3.cmml">n</mi></mrow></msub><mo id="S2.p1.1.m1.1.1.3" lspace="0.278em" rspace="0.278em" xref="S2.p1.1.m1.1.1.3.cmml">:=</mo><mrow id="S2.p1.1.m1.1.1.4" xref="S2.p1.1.m1.1.1.4.cmml"><msub id="S2.p1.1.m1.1.1.4.2" xref="S2.p1.1.m1.1.1.4.2.cmml"><mi id="S2.p1.1.m1.1.1.4.2.2" xref="S2.p1.1.m1.1.1.4.2.2.cmml">x</mi><mn id="S2.p1.1.m1.1.1.4.2.3" xref="S2.p1.1.m1.1.1.4.2.3.cmml">1</mn></msub><mo id="S2.p1.1.m1.1.1.4.1" xref="S2.p1.1.m1.1.1.4.1.cmml">⁢</mo><msub id="S2.p1.1.m1.1.1.4.3" xref="S2.p1.1.m1.1.1.4.3.cmml"><mi id="S2.p1.1.m1.1.1.4.3.2" xref="S2.p1.1.m1.1.1.4.3.2.cmml">x</mi><mn id="S2.p1.1.m1.1.1.4.3.3" xref="S2.p1.1.m1.1.1.4.3.3.cmml">2</mn></msub><mo id="S2.p1.1.m1.1.1.4.1a" xref="S2.p1.1.m1.1.1.4.1.cmml">⁢</mo><mi id="S2.p1.1.m1.1.1.4.4" mathvariant="normal" xref="S2.p1.1.m1.1.1.4.4.cmml">…</mi><mo id="S2.p1.1.m1.1.1.4.1b" xref="S2.p1.1.m1.1.1.4.1.cmml">⁢</mo><msub id="S2.p1.1.m1.1.1.4.5" xref="S2.p1.1.m1.1.1.4.5.cmml"><mi id="S2.p1.1.m1.1.1.4.5.2" xref="S2.p1.1.m1.1.1.4.5.2.cmml">x</mi><mi id="S2.p1.1.m1.1.1.4.5.3" xref="S2.p1.1.m1.1.1.4.5.3.cmml">n</mi></msub></mrow><mo id="S2.p1.1.m1.1.1.5" xref="S2.p1.1.m1.1.1.5.cmml">∈</mo><msup id="S2.p1.1.m1.1.1.6" xref="S2.p1.1.m1.1.1.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.1.1.6.2" xref="S2.p1.1.m1.1.1.6.2.cmml">𝒳</mi><mi id="S2.p1.1.m1.1.1.6.3" xref="S2.p1.1.m1.1.1.6.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><and id="S2.p1.1.m1.1.1a.cmml" xref="S2.p1.1.m1.1.1"></and><apply id="S2.p1.1.m1.1.1b.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="latexml" id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">assign</csymbol><apply id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.2.1.cmml" xref="S2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.2.cmml" xref="S2.p1.1.m1.1.1.2.2">𝑥</ci><apply id="S2.p1.1.m1.1.1.2.3.cmml" xref="S2.p1.1.m1.1.1.2.3"><ci id="S2.p1.1.m1.1.1.2.3.1.cmml" xref="S2.p1.1.m1.1.1.2.3.1">:</ci><cn id="S2.p1.1.m1.1.1.2.3.2.cmml" type="integer" xref="S2.p1.1.m1.1.1.2.3.2">1</cn><ci id="S2.p1.1.m1.1.1.2.3.3.cmml" xref="S2.p1.1.m1.1.1.2.3.3">𝑛</ci></apply></apply><apply id="S2.p1.1.m1.1.1.4.cmml" xref="S2.p1.1.m1.1.1.4"><times id="S2.p1.1.m1.1.1.4.1.cmml" xref="S2.p1.1.m1.1.1.4.1"></times><apply id="S2.p1.1.m1.1.1.4.2.cmml" xref="S2.p1.1.m1.1.1.4.2"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.4.2.1.cmml" xref="S2.p1.1.m1.1.1.4.2">subscript</csymbol><ci id="S2.p1.1.m1.1.1.4.2.2.cmml" xref="S2.p1.1.m1.1.1.4.2.2">𝑥</ci><cn id="S2.p1.1.m1.1.1.4.2.3.cmml" type="integer" xref="S2.p1.1.m1.1.1.4.2.3">1</cn></apply><apply id="S2.p1.1.m1.1.1.4.3.cmml" xref="S2.p1.1.m1.1.1.4.3"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.4.3.1.cmml" xref="S2.p1.1.m1.1.1.4.3">subscript</csymbol><ci id="S2.p1.1.m1.1.1.4.3.2.cmml" xref="S2.p1.1.m1.1.1.4.3.2">𝑥</ci><cn id="S2.p1.1.m1.1.1.4.3.3.cmml" type="integer" xref="S2.p1.1.m1.1.1.4.3.3">2</cn></apply><ci id="S2.p1.1.m1.1.1.4.4.cmml" xref="S2.p1.1.m1.1.1.4.4">…</ci><apply id="S2.p1.1.m1.1.1.4.5.cmml" xref="S2.p1.1.m1.1.1.4.5"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.4.5.1.cmml" xref="S2.p1.1.m1.1.1.4.5">subscript</csymbol><ci id="S2.p1.1.m1.1.1.4.5.2.cmml" xref="S2.p1.1.m1.1.1.4.5.2">𝑥</ci><ci id="S2.p1.1.m1.1.1.4.5.3.cmml" xref="S2.p1.1.m1.1.1.4.5.3">𝑛</ci></apply></apply></apply><apply id="S2.p1.1.m1.1.1c.cmml" xref="S2.p1.1.m1.1.1"><in id="S2.p1.1.m1.1.1.5.cmml" xref="S2.p1.1.m1.1.1.5"></in><share href="https://arxiv.org/html/2410.05078v1#S2.p1.1.m1.1.1.4.cmml" id="S2.p1.1.m1.1.1d.cmml" xref="S2.p1.1.m1.1.1"></share><apply id="S2.p1.1.m1.1.1.6.cmml" xref="S2.p1.1.m1.1.1.6"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.6.1.cmml" xref="S2.p1.1.m1.1.1.6">superscript</csymbol><ci id="S2.p1.1.m1.1.1.6.2.cmml" xref="S2.p1.1.m1.1.1.6.2">𝒳</ci><ci id="S2.p1.1.m1.1.1.6.3.cmml" xref="S2.p1.1.m1.1.1.6.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">{x_{1:n}:=x_{1}x_{2}\ldots x_{n}\in\mathcal{X}^{n}}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">italic_x start_POSTSUBSCRIPT 1 : italic_n end_POSTSUBSCRIPT := italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT … italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ caligraphic_X start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> of length <math alttext="n" class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">italic_n</annotation></semantics></math> from a finite alphabet <math alttext="\mathcal{X}" class="ltx_Math" display="inline" id="S2.p1.3.m3.1"><semantics id="S2.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">𝒳</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">𝒳</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\mathcal{X}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.3.m3.1d">caligraphic_X</annotation></semantics></math> sampled from a source <math alttext="{\rho:\mathcal{X}^{*}\mapsto(0,1]}" class="ltx_Math" display="inline" id="S2.p1.4.m4.2"><semantics id="S2.p1.4.m4.2a"><mrow id="S2.p1.4.m4.2.3" xref="S2.p1.4.m4.2.3.cmml"><mi id="S2.p1.4.m4.2.3.2" xref="S2.p1.4.m4.2.3.2.cmml">ρ</mi><mo id="S2.p1.4.m4.2.3.1" lspace="0.278em" rspace="0.278em" xref="S2.p1.4.m4.2.3.1.cmml">:</mo><mrow id="S2.p1.4.m4.2.3.3" xref="S2.p1.4.m4.2.3.3.cmml"><msup id="S2.p1.4.m4.2.3.3.2" xref="S2.p1.4.m4.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.4.m4.2.3.3.2.2" xref="S2.p1.4.m4.2.3.3.2.2.cmml">𝒳</mi><mo id="S2.p1.4.m4.2.3.3.2.3" xref="S2.p1.4.m4.2.3.3.2.3.cmml">∗</mo></msup><mo id="S2.p1.4.m4.2.3.3.1" stretchy="false" xref="S2.p1.4.m4.2.3.3.1.cmml">↦</mo><mrow id="S2.p1.4.m4.2.3.3.3.2" xref="S2.p1.4.m4.2.3.3.3.1.cmml"><mo id="S2.p1.4.m4.2.3.3.3.2.1" stretchy="false" xref="S2.p1.4.m4.2.3.3.3.1.cmml">(</mo><mn id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">0</mn><mo id="S2.p1.4.m4.2.3.3.3.2.2" xref="S2.p1.4.m4.2.3.3.3.1.cmml">,</mo><mn id="S2.p1.4.m4.2.2" xref="S2.p1.4.m4.2.2.cmml">1</mn><mo id="S2.p1.4.m4.2.3.3.3.2.3" stretchy="false" xref="S2.p1.4.m4.2.3.3.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.2b"><apply id="S2.p1.4.m4.2.3.cmml" xref="S2.p1.4.m4.2.3"><ci id="S2.p1.4.m4.2.3.1.cmml" xref="S2.p1.4.m4.2.3.1">:</ci><ci id="S2.p1.4.m4.2.3.2.cmml" xref="S2.p1.4.m4.2.3.2">𝜌</ci><apply id="S2.p1.4.m4.2.3.3.cmml" xref="S2.p1.4.m4.2.3.3"><csymbol cd="latexml" id="S2.p1.4.m4.2.3.3.1.cmml" xref="S2.p1.4.m4.2.3.3.1">maps-to</csymbol><apply id="S2.p1.4.m4.2.3.3.2.cmml" xref="S2.p1.4.m4.2.3.3.2"><csymbol cd="ambiguous" id="S2.p1.4.m4.2.3.3.2.1.cmml" xref="S2.p1.4.m4.2.3.3.2">superscript</csymbol><ci id="S2.p1.4.m4.2.3.3.2.2.cmml" xref="S2.p1.4.m4.2.3.3.2.2">𝒳</ci><times id="S2.p1.4.m4.2.3.3.2.3.cmml" xref="S2.p1.4.m4.2.3.3.2.3"></times></apply><interval closure="open-closed" id="S2.p1.4.m4.2.3.3.3.1.cmml" xref="S2.p1.4.m4.2.3.3.3.2"><cn id="S2.p1.4.m4.1.1.cmml" type="integer" xref="S2.p1.4.m4.1.1">0</cn><cn id="S2.p1.4.m4.2.2.cmml" type="integer" xref="S2.p1.4.m4.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.2c">{\rho:\mathcal{X}^{*}\mapsto(0,1]}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.4.m4.2d">italic_ρ : caligraphic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ↦ ( 0 , 1 ]</annotation></semantics></math>, a lossless compressor <math alttext="{c:\mathcal{X}^{*}\mapsto\{0,1\}^{*}}" class="ltx_Math" display="inline" id="S2.p1.5.m5.2"><semantics id="S2.p1.5.m5.2a"><mrow id="S2.p1.5.m5.2.3" xref="S2.p1.5.m5.2.3.cmml"><mi id="S2.p1.5.m5.2.3.2" xref="S2.p1.5.m5.2.3.2.cmml">c</mi><mo id="S2.p1.5.m5.2.3.1" lspace="0.278em" rspace="0.278em" xref="S2.p1.5.m5.2.3.1.cmml">:</mo><mrow id="S2.p1.5.m5.2.3.3" xref="S2.p1.5.m5.2.3.3.cmml"><msup id="S2.p1.5.m5.2.3.3.2" xref="S2.p1.5.m5.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.5.m5.2.3.3.2.2" xref="S2.p1.5.m5.2.3.3.2.2.cmml">𝒳</mi><mo id="S2.p1.5.m5.2.3.3.2.3" xref="S2.p1.5.m5.2.3.3.2.3.cmml">∗</mo></msup><mo id="S2.p1.5.m5.2.3.3.1" stretchy="false" xref="S2.p1.5.m5.2.3.3.1.cmml">↦</mo><msup id="S2.p1.5.m5.2.3.3.3" xref="S2.p1.5.m5.2.3.3.3.cmml"><mrow id="S2.p1.5.m5.2.3.3.3.2.2" xref="S2.p1.5.m5.2.3.3.3.2.1.cmml"><mo id="S2.p1.5.m5.2.3.3.3.2.2.1" stretchy="false" xref="S2.p1.5.m5.2.3.3.3.2.1.cmml">{</mo><mn id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">0</mn><mo id="S2.p1.5.m5.2.3.3.3.2.2.2" xref="S2.p1.5.m5.2.3.3.3.2.1.cmml">,</mo><mn id="S2.p1.5.m5.2.2" xref="S2.p1.5.m5.2.2.cmml">1</mn><mo id="S2.p1.5.m5.2.3.3.3.2.2.3" stretchy="false" xref="S2.p1.5.m5.2.3.3.3.2.1.cmml">}</mo></mrow><mo id="S2.p1.5.m5.2.3.3.3.3" xref="S2.p1.5.m5.2.3.3.3.3.cmml">∗</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.2b"><apply id="S2.p1.5.m5.2.3.cmml" xref="S2.p1.5.m5.2.3"><ci id="S2.p1.5.m5.2.3.1.cmml" xref="S2.p1.5.m5.2.3.1">:</ci><ci id="S2.p1.5.m5.2.3.2.cmml" xref="S2.p1.5.m5.2.3.2">𝑐</ci><apply id="S2.p1.5.m5.2.3.3.cmml" xref="S2.p1.5.m5.2.3.3"><csymbol cd="latexml" id="S2.p1.5.m5.2.3.3.1.cmml" xref="S2.p1.5.m5.2.3.3.1">maps-to</csymbol><apply id="S2.p1.5.m5.2.3.3.2.cmml" xref="S2.p1.5.m5.2.3.3.2"><csymbol cd="ambiguous" id="S2.p1.5.m5.2.3.3.2.1.cmml" xref="S2.p1.5.m5.2.3.3.2">superscript</csymbol><ci id="S2.p1.5.m5.2.3.3.2.2.cmml" xref="S2.p1.5.m5.2.3.3.2.2">𝒳</ci><times id="S2.p1.5.m5.2.3.3.2.3.cmml" xref="S2.p1.5.m5.2.3.3.2.3"></times></apply><apply id="S2.p1.5.m5.2.3.3.3.cmml" xref="S2.p1.5.m5.2.3.3.3"><csymbol cd="ambiguous" id="S2.p1.5.m5.2.3.3.3.1.cmml" xref="S2.p1.5.m5.2.3.3.3">superscript</csymbol><set id="S2.p1.5.m5.2.3.3.3.2.1.cmml" xref="S2.p1.5.m5.2.3.3.3.2.2"><cn id="S2.p1.5.m5.1.1.cmml" type="integer" xref="S2.p1.5.m5.1.1">0</cn><cn id="S2.p1.5.m5.2.2.cmml" type="integer" xref="S2.p1.5.m5.2.2">1</cn></set><times id="S2.p1.5.m5.2.3.3.3.3.cmml" xref="S2.p1.5.m5.2.3.3.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.2c">{c:\mathcal{X}^{*}\mapsto\{0,1\}^{*}}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.5.m5.2d">italic_c : caligraphic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ↦ { 0 , 1 } start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> assigns a code <math alttext="c(x_{1:n})" class="ltx_Math" display="inline" id="S2.p1.6.m6.1"><semantics id="S2.p1.6.m6.1a"><mrow id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml"><mi id="S2.p1.6.m6.1.1.3" xref="S2.p1.6.m6.1.1.3.cmml">c</mi><mo id="S2.p1.6.m6.1.1.2" xref="S2.p1.6.m6.1.1.2.cmml">⁢</mo><mrow id="S2.p1.6.m6.1.1.1.1" xref="S2.p1.6.m6.1.1.1.1.1.cmml"><mo id="S2.p1.6.m6.1.1.1.1.2" stretchy="false" xref="S2.p1.6.m6.1.1.1.1.1.cmml">(</mo><msub id="S2.p1.6.m6.1.1.1.1.1" xref="S2.p1.6.m6.1.1.1.1.1.cmml"><mi id="S2.p1.6.m6.1.1.1.1.1.2" xref="S2.p1.6.m6.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.p1.6.m6.1.1.1.1.1.3" xref="S2.p1.6.m6.1.1.1.1.1.3.cmml"><mn id="S2.p1.6.m6.1.1.1.1.1.3.2" xref="S2.p1.6.m6.1.1.1.1.1.3.2.cmml">1</mn><mo id="S2.p1.6.m6.1.1.1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.p1.6.m6.1.1.1.1.1.3.1.cmml">:</mo><mi id="S2.p1.6.m6.1.1.1.1.1.3.3" xref="S2.p1.6.m6.1.1.1.1.1.3.3.cmml">n</mi></mrow></msub><mo id="S2.p1.6.m6.1.1.1.1.3" stretchy="false" xref="S2.p1.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><apply id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1"><times id="S2.p1.6.m6.1.1.2.cmml" xref="S2.p1.6.m6.1.1.2"></times><ci id="S2.p1.6.m6.1.1.3.cmml" xref="S2.p1.6.m6.1.1.3">𝑐</ci><apply id="S2.p1.6.m6.1.1.1.1.1.cmml" xref="S2.p1.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.6.m6.1.1.1.1.1.1.cmml" xref="S2.p1.6.m6.1.1.1.1">subscript</csymbol><ci id="S2.p1.6.m6.1.1.1.1.1.2.cmml" xref="S2.p1.6.m6.1.1.1.1.1.2">𝑥</ci><apply id="S2.p1.6.m6.1.1.1.1.1.3.cmml" xref="S2.p1.6.m6.1.1.1.1.1.3"><ci id="S2.p1.6.m6.1.1.1.1.1.3.1.cmml" xref="S2.p1.6.m6.1.1.1.1.1.3.1">:</ci><cn id="S2.p1.6.m6.1.1.1.1.1.3.2.cmml" type="integer" xref="S2.p1.6.m6.1.1.1.1.1.3.2">1</cn><ci id="S2.p1.6.m6.1.1.1.1.1.3.3.cmml" xref="S2.p1.6.m6.1.1.1.1.1.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">c(x_{1:n})</annotation><annotation encoding="application/x-llamapun" id="S2.p1.6.m6.1d">italic_c ( italic_x start_POSTSUBSCRIPT 1 : italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>, i.e., a sequence of bits, from which the original sequence is recoverable without loss of information. The goal is to
minimize the expected length: <math alttext="{L_{\rho}:=\mathrm{\mathbb{missing}}{E}_{x\sim\rho}[\ell_{c}(x)]}" class="ltx_Math" display="inline" id="S2.p1.7.m7.2"><semantics id="S2.p1.7.m7.2a"><mrow id="S2.p1.7.m7.2.2" xref="S2.p1.7.m7.2.2.cmml"><msub id="S2.p1.7.m7.2.2.3" xref="S2.p1.7.m7.2.2.3.cmml"><mi id="S2.p1.7.m7.2.2.3.2" xref="S2.p1.7.m7.2.2.3.2.cmml">L</mi><mi id="S2.p1.7.m7.2.2.3.3" xref="S2.p1.7.m7.2.2.3.3.cmml">ρ</mi></msub><mo id="S2.p1.7.m7.2.2.2" lspace="0.278em" rspace="0.278em" xref="S2.p1.7.m7.2.2.2.cmml">:=</mo><mrow id="S2.p1.7.m7.2.2.1" xref="S2.p1.7.m7.2.2.1.cmml"><mi id="S2.p1.7.m7.2.2.1.3" xref="S2.p1.7.m7.2.2.1.3.cmml">missing</mi><mo id="S2.p1.7.m7.2.2.1.2" xref="S2.p1.7.m7.2.2.1.2.cmml">⁢</mo><msub id="S2.p1.7.m7.2.2.1.4" xref="S2.p1.7.m7.2.2.1.4.cmml"><mi id="S2.p1.7.m7.2.2.1.4.2" xref="S2.p1.7.m7.2.2.1.4.2.cmml">E</mi><mrow id="S2.p1.7.m7.2.2.1.4.3" xref="S2.p1.7.m7.2.2.1.4.3.cmml"><mi id="S2.p1.7.m7.2.2.1.4.3.2" xref="S2.p1.7.m7.2.2.1.4.3.2.cmml">x</mi><mo id="S2.p1.7.m7.2.2.1.4.3.1" xref="S2.p1.7.m7.2.2.1.4.3.1.cmml">∼</mo><mi id="S2.p1.7.m7.2.2.1.4.3.3" xref="S2.p1.7.m7.2.2.1.4.3.3.cmml">ρ</mi></mrow></msub><mo id="S2.p1.7.m7.2.2.1.2a" xref="S2.p1.7.m7.2.2.1.2.cmml">⁢</mo><mrow id="S2.p1.7.m7.2.2.1.1.1" xref="S2.p1.7.m7.2.2.1.1.2.cmml"><mo id="S2.p1.7.m7.2.2.1.1.1.2" stretchy="false" xref="S2.p1.7.m7.2.2.1.1.2.1.cmml">[</mo><mrow id="S2.p1.7.m7.2.2.1.1.1.1" xref="S2.p1.7.m7.2.2.1.1.1.1.cmml"><msub id="S2.p1.7.m7.2.2.1.1.1.1.2" xref="S2.p1.7.m7.2.2.1.1.1.1.2.cmml"><mi id="S2.p1.7.m7.2.2.1.1.1.1.2.2" mathvariant="normal" xref="S2.p1.7.m7.2.2.1.1.1.1.2.2.cmml">ℓ</mi><mi id="S2.p1.7.m7.2.2.1.1.1.1.2.3" xref="S2.p1.7.m7.2.2.1.1.1.1.2.3.cmml">c</mi></msub><mo id="S2.p1.7.m7.2.2.1.1.1.1.1" xref="S2.p1.7.m7.2.2.1.1.1.1.1.cmml">⁢</mo><mrow id="S2.p1.7.m7.2.2.1.1.1.1.3.2" xref="S2.p1.7.m7.2.2.1.1.1.1.cmml"><mo id="S2.p1.7.m7.2.2.1.1.1.1.3.2.1" stretchy="false" xref="S2.p1.7.m7.2.2.1.1.1.1.cmml">(</mo><mi id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml">x</mi><mo id="S2.p1.7.m7.2.2.1.1.1.1.3.2.2" stretchy="false" xref="S2.p1.7.m7.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.p1.7.m7.2.2.1.1.1.3" stretchy="false" xref="S2.p1.7.m7.2.2.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.2b"><apply id="S2.p1.7.m7.2.2.cmml" xref="S2.p1.7.m7.2.2"><csymbol cd="latexml" id="S2.p1.7.m7.2.2.2.cmml" xref="S2.p1.7.m7.2.2.2">assign</csymbol><apply id="S2.p1.7.m7.2.2.3.cmml" xref="S2.p1.7.m7.2.2.3"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.3.1.cmml" xref="S2.p1.7.m7.2.2.3">subscript</csymbol><ci id="S2.p1.7.m7.2.2.3.2.cmml" xref="S2.p1.7.m7.2.2.3.2">𝐿</ci><ci id="S2.p1.7.m7.2.2.3.3.cmml" xref="S2.p1.7.m7.2.2.3.3">𝜌</ci></apply><apply id="S2.p1.7.m7.2.2.1.cmml" xref="S2.p1.7.m7.2.2.1"><times id="S2.p1.7.m7.2.2.1.2.cmml" xref="S2.p1.7.m7.2.2.1.2"></times><ci id="S2.p1.7.m7.2.2.1.3.cmml" xref="S2.p1.7.m7.2.2.1.3">missing</ci><apply id="S2.p1.7.m7.2.2.1.4.cmml" xref="S2.p1.7.m7.2.2.1.4"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.1.4.1.cmml" xref="S2.p1.7.m7.2.2.1.4">subscript</csymbol><ci id="S2.p1.7.m7.2.2.1.4.2.cmml" xref="S2.p1.7.m7.2.2.1.4.2">𝐸</ci><apply id="S2.p1.7.m7.2.2.1.4.3.cmml" xref="S2.p1.7.m7.2.2.1.4.3"><csymbol cd="latexml" id="S2.p1.7.m7.2.2.1.4.3.1.cmml" xref="S2.p1.7.m7.2.2.1.4.3.1">similar-to</csymbol><ci id="S2.p1.7.m7.2.2.1.4.3.2.cmml" xref="S2.p1.7.m7.2.2.1.4.3.2">𝑥</ci><ci id="S2.p1.7.m7.2.2.1.4.3.3.cmml" xref="S2.p1.7.m7.2.2.1.4.3.3">𝜌</ci></apply></apply><apply id="S2.p1.7.m7.2.2.1.1.2.cmml" xref="S2.p1.7.m7.2.2.1.1.1"><csymbol cd="latexml" id="S2.p1.7.m7.2.2.1.1.2.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S2.p1.7.m7.2.2.1.1.1.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1"><times id="S2.p1.7.m7.2.2.1.1.1.1.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1"></times><apply id="S2.p1.7.m7.2.2.1.1.1.1.2.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.1.1.1.1.2.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.2">subscript</csymbol><ci id="S2.p1.7.m7.2.2.1.1.1.1.2.2.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.2.2">ℓ</ci><ci id="S2.p1.7.m7.2.2.1.1.1.1.2.3.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.2.3">𝑐</ci></apply><ci id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.2c">{L_{\rho}:=\mathrm{\mathbb{missing}}{E}_{x\sim\rho}[\ell_{c}(x)]}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.7.m7.2d">italic_L start_POSTSUBSCRIPT italic_ρ end_POSTSUBSCRIPT := roman_missing italic_E start_POSTSUBSCRIPT italic_x ∼ italic_ρ end_POSTSUBSCRIPT [ roman_ℓ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_x ) ]</annotation></semantics></math> by encoding rare sequences with more bits and frequent sequences with fewer bits.
Shannon’s source coding theorem states that the minimal expected length is lower-bounded by the Shannon entropy of the source: <math alttext="{L_{\rho}\geq H(\rho):=\mathbb{E}_{x\sim\rho}[-\log_{2}\rho(x)]}" class="ltx_Math" display="inline" id="S2.p1.8.m8.3"><semantics id="S2.p1.8.m8.3a"><mrow id="S2.p1.8.m8.3.3" xref="S2.p1.8.m8.3.3.cmml"><msub id="S2.p1.8.m8.3.3.3" xref="S2.p1.8.m8.3.3.3.cmml"><mi id="S2.p1.8.m8.3.3.3.2" xref="S2.p1.8.m8.3.3.3.2.cmml">L</mi><mi id="S2.p1.8.m8.3.3.3.3" xref="S2.p1.8.m8.3.3.3.3.cmml">ρ</mi></msub><mo id="S2.p1.8.m8.3.3.4" xref="S2.p1.8.m8.3.3.4.cmml">≥</mo><mrow id="S2.p1.8.m8.3.3.5" xref="S2.p1.8.m8.3.3.5.cmml"><mi id="S2.p1.8.m8.3.3.5.2" xref="S2.p1.8.m8.3.3.5.2.cmml">H</mi><mo id="S2.p1.8.m8.3.3.5.1" xref="S2.p1.8.m8.3.3.5.1.cmml">⁢</mo><mrow id="S2.p1.8.m8.3.3.5.3.2" xref="S2.p1.8.m8.3.3.5.cmml"><mo id="S2.p1.8.m8.3.3.5.3.2.1" stretchy="false" xref="S2.p1.8.m8.3.3.5.cmml">(</mo><mi id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml">ρ</mi><mo id="S2.p1.8.m8.3.3.5.3.2.2" rspace="0.278em" stretchy="false" xref="S2.p1.8.m8.3.3.5.cmml">)</mo></mrow></mrow><mo id="S2.p1.8.m8.3.3.6" rspace="0.278em" xref="S2.p1.8.m8.3.3.6.cmml">:=</mo><mrow id="S2.p1.8.m8.3.3.1" xref="S2.p1.8.m8.3.3.1.cmml"><msub id="S2.p1.8.m8.3.3.1.3" xref="S2.p1.8.m8.3.3.1.3.cmml"><mi id="S2.p1.8.m8.3.3.1.3.2" xref="S2.p1.8.m8.3.3.1.3.2.cmml">𝔼</mi><mrow id="S2.p1.8.m8.3.3.1.3.3" xref="S2.p1.8.m8.3.3.1.3.3.cmml"><mi id="S2.p1.8.m8.3.3.1.3.3.2" xref="S2.p1.8.m8.3.3.1.3.3.2.cmml">x</mi><mo id="S2.p1.8.m8.3.3.1.3.3.1" xref="S2.p1.8.m8.3.3.1.3.3.1.cmml">∼</mo><mi id="S2.p1.8.m8.3.3.1.3.3.3" xref="S2.p1.8.m8.3.3.1.3.3.3.cmml">ρ</mi></mrow></msub><mo id="S2.p1.8.m8.3.3.1.2" xref="S2.p1.8.m8.3.3.1.2.cmml">⁢</mo><mrow id="S2.p1.8.m8.3.3.1.1.1" xref="S2.p1.8.m8.3.3.1.1.2.cmml"><mo id="S2.p1.8.m8.3.3.1.1.1.2" stretchy="false" xref="S2.p1.8.m8.3.3.1.1.2.1.cmml">[</mo><mrow id="S2.p1.8.m8.3.3.1.1.1.1" xref="S2.p1.8.m8.3.3.1.1.1.1.cmml"><mo id="S2.p1.8.m8.3.3.1.1.1.1a" rspace="0.167em" xref="S2.p1.8.m8.3.3.1.1.1.1.cmml">−</mo><mrow id="S2.p1.8.m8.3.3.1.1.1.1.2" xref="S2.p1.8.m8.3.3.1.1.1.1.2.cmml"><mrow id="S2.p1.8.m8.3.3.1.1.1.1.2.2" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.cmml"><msub id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.cmml"><mi id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.2" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.2.cmml">log</mi><mn id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.3" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.3.cmml">2</mn></msub><mo id="S2.p1.8.m8.3.3.1.1.1.1.2.2a" lspace="0.167em" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.cmml">⁡</mo><mi id="S2.p1.8.m8.3.3.1.1.1.1.2.2.2" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.2.cmml">ρ</mi></mrow><mo id="S2.p1.8.m8.3.3.1.1.1.1.2.1" xref="S2.p1.8.m8.3.3.1.1.1.1.2.1.cmml">⁢</mo><mrow id="S2.p1.8.m8.3.3.1.1.1.1.2.3.2" xref="S2.p1.8.m8.3.3.1.1.1.1.2.cmml"><mo id="S2.p1.8.m8.3.3.1.1.1.1.2.3.2.1" stretchy="false" xref="S2.p1.8.m8.3.3.1.1.1.1.2.cmml">(</mo><mi id="S2.p1.8.m8.2.2" xref="S2.p1.8.m8.2.2.cmml">x</mi><mo id="S2.p1.8.m8.3.3.1.1.1.1.2.3.2.2" stretchy="false" xref="S2.p1.8.m8.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.p1.8.m8.3.3.1.1.1.3" stretchy="false" xref="S2.p1.8.m8.3.3.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.3b"><apply id="S2.p1.8.m8.3.3.cmml" xref="S2.p1.8.m8.3.3"><and id="S2.p1.8.m8.3.3a.cmml" xref="S2.p1.8.m8.3.3"></and><apply id="S2.p1.8.m8.3.3b.cmml" xref="S2.p1.8.m8.3.3"><geq id="S2.p1.8.m8.3.3.4.cmml" xref="S2.p1.8.m8.3.3.4"></geq><apply id="S2.p1.8.m8.3.3.3.cmml" xref="S2.p1.8.m8.3.3.3"><csymbol cd="ambiguous" id="S2.p1.8.m8.3.3.3.1.cmml" xref="S2.p1.8.m8.3.3.3">subscript</csymbol><ci id="S2.p1.8.m8.3.3.3.2.cmml" xref="S2.p1.8.m8.3.3.3.2">𝐿</ci><ci id="S2.p1.8.m8.3.3.3.3.cmml" xref="S2.p1.8.m8.3.3.3.3">𝜌</ci></apply><apply id="S2.p1.8.m8.3.3.5.cmml" xref="S2.p1.8.m8.3.3.5"><times id="S2.p1.8.m8.3.3.5.1.cmml" xref="S2.p1.8.m8.3.3.5.1"></times><ci id="S2.p1.8.m8.3.3.5.2.cmml" xref="S2.p1.8.m8.3.3.5.2">𝐻</ci><ci id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1">𝜌</ci></apply></apply><apply id="S2.p1.8.m8.3.3c.cmml" xref="S2.p1.8.m8.3.3"><csymbol cd="latexml" id="S2.p1.8.m8.3.3.6.cmml" xref="S2.p1.8.m8.3.3.6">assign</csymbol><share href="https://arxiv.org/html/2410.05078v1#S2.p1.8.m8.3.3.5.cmml" id="S2.p1.8.m8.3.3d.cmml" xref="S2.p1.8.m8.3.3"></share><apply id="S2.p1.8.m8.3.3.1.cmml" xref="S2.p1.8.m8.3.3.1"><times id="S2.p1.8.m8.3.3.1.2.cmml" xref="S2.p1.8.m8.3.3.1.2"></times><apply id="S2.p1.8.m8.3.3.1.3.cmml" xref="S2.p1.8.m8.3.3.1.3"><csymbol cd="ambiguous" id="S2.p1.8.m8.3.3.1.3.1.cmml" xref="S2.p1.8.m8.3.3.1.3">subscript</csymbol><ci id="S2.p1.8.m8.3.3.1.3.2.cmml" xref="S2.p1.8.m8.3.3.1.3.2">𝔼</ci><apply id="S2.p1.8.m8.3.3.1.3.3.cmml" xref="S2.p1.8.m8.3.3.1.3.3"><csymbol cd="latexml" id="S2.p1.8.m8.3.3.1.3.3.1.cmml" xref="S2.p1.8.m8.3.3.1.3.3.1">similar-to</csymbol><ci id="S2.p1.8.m8.3.3.1.3.3.2.cmml" xref="S2.p1.8.m8.3.3.1.3.3.2">𝑥</ci><ci id="S2.p1.8.m8.3.3.1.3.3.3.cmml" xref="S2.p1.8.m8.3.3.1.3.3.3">𝜌</ci></apply></apply><apply id="S2.p1.8.m8.3.3.1.1.2.cmml" xref="S2.p1.8.m8.3.3.1.1.1"><csymbol cd="latexml" id="S2.p1.8.m8.3.3.1.1.2.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.2">delimited-[]</csymbol><apply id="S2.p1.8.m8.3.3.1.1.1.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1"><minus id="S2.p1.8.m8.3.3.1.1.1.1.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1"></minus><apply id="S2.p1.8.m8.3.3.1.1.1.1.2.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2"><times id="S2.p1.8.m8.3.3.1.1.1.1.2.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.1"></times><apply id="S2.p1.8.m8.3.3.1.1.1.1.2.2.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2"><apply id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1">subscript</csymbol><log id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.2.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.2"></log><cn id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.3.cmml" type="integer" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.3">2</cn></apply><ci id="S2.p1.8.m8.3.3.1.1.1.1.2.2.2.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.2">𝜌</ci></apply><ci id="S2.p1.8.m8.2.2.cmml" xref="S2.p1.8.m8.2.2">𝑥</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.3c">{L_{\rho}\geq H(\rho):=\mathbb{E}_{x\sim\rho}[-\log_{2}\rho(x)]}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.8.m8.3d">italic_L start_POSTSUBSCRIPT italic_ρ end_POSTSUBSCRIPT ≥ italic_H ( italic_ρ ) := blackboard_E start_POSTSUBSCRIPT italic_x ∼ italic_ρ end_POSTSUBSCRIPT [ - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_ρ ( italic_x ) ]</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.3">If the source’s statistics are unknown, good compression becomes a statistical modeling problem, i.e., good compression relies entirely on being able to predict well sequentially.
For any predictor&nbsp;<math alttext="{\pi:\mathcal{X}^{*}\mapsto(0,1]}" class="ltx_Math" display="inline" id="S2.p2.1.m1.2"><semantics id="S2.p2.1.m1.2a"><mrow id="S2.p2.1.m1.2.3" xref="S2.p2.1.m1.2.3.cmml"><mi id="S2.p2.1.m1.2.3.2" xref="S2.p2.1.m1.2.3.2.cmml">π</mi><mo id="S2.p2.1.m1.2.3.1" lspace="0.278em" rspace="0.278em" xref="S2.p2.1.m1.2.3.1.cmml">:</mo><mrow id="S2.p2.1.m1.2.3.3" xref="S2.p2.1.m1.2.3.3.cmml"><msup id="S2.p2.1.m1.2.3.3.2" xref="S2.p2.1.m1.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p2.1.m1.2.3.3.2.2" xref="S2.p2.1.m1.2.3.3.2.2.cmml">𝒳</mi><mo id="S2.p2.1.m1.2.3.3.2.3" xref="S2.p2.1.m1.2.3.3.2.3.cmml">∗</mo></msup><mo id="S2.p2.1.m1.2.3.3.1" stretchy="false" xref="S2.p2.1.m1.2.3.3.1.cmml">↦</mo><mrow id="S2.p2.1.m1.2.3.3.3.2" xref="S2.p2.1.m1.2.3.3.3.1.cmml"><mo id="S2.p2.1.m1.2.3.3.3.2.1" stretchy="false" xref="S2.p2.1.m1.2.3.3.3.1.cmml">(</mo><mn id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">0</mn><mo id="S2.p2.1.m1.2.3.3.3.2.2" xref="S2.p2.1.m1.2.3.3.3.1.cmml">,</mo><mn id="S2.p2.1.m1.2.2" xref="S2.p2.1.m1.2.2.cmml">1</mn><mo id="S2.p2.1.m1.2.3.3.3.2.3" stretchy="false" xref="S2.p2.1.m1.2.3.3.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.2b"><apply id="S2.p2.1.m1.2.3.cmml" xref="S2.p2.1.m1.2.3"><ci id="S2.p2.1.m1.2.3.1.cmml" xref="S2.p2.1.m1.2.3.1">:</ci><ci id="S2.p2.1.m1.2.3.2.cmml" xref="S2.p2.1.m1.2.3.2">𝜋</ci><apply id="S2.p2.1.m1.2.3.3.cmml" xref="S2.p2.1.m1.2.3.3"><csymbol cd="latexml" id="S2.p2.1.m1.2.3.3.1.cmml" xref="S2.p2.1.m1.2.3.3.1">maps-to</csymbol><apply id="S2.p2.1.m1.2.3.3.2.cmml" xref="S2.p2.1.m1.2.3.3.2"><csymbol cd="ambiguous" id="S2.p2.1.m1.2.3.3.2.1.cmml" xref="S2.p2.1.m1.2.3.3.2">superscript</csymbol><ci id="S2.p2.1.m1.2.3.3.2.2.cmml" xref="S2.p2.1.m1.2.3.3.2.2">𝒳</ci><times id="S2.p2.1.m1.2.3.3.2.3.cmml" xref="S2.p2.1.m1.2.3.3.2.3"></times></apply><interval closure="open-closed" id="S2.p2.1.m1.2.3.3.3.1.cmml" xref="S2.p2.1.m1.2.3.3.3.2"><cn id="S2.p2.1.m1.1.1.cmml" type="integer" xref="S2.p2.1.m1.1.1">0</cn><cn id="S2.p2.1.m1.2.2.cmml" type="integer" xref="S2.p2.1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.2c">{\pi:\mathcal{X}^{*}\mapsto(0,1]}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.2d">italic_π : caligraphic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ↦ ( 0 , 1 ]</annotation></semantics></math> the expected coding length&nbsp;<math alttext="L_{\pi}^{\rho}" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><msubsup id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><mi id="S2.p2.2.m2.1.1.2.2" xref="S2.p2.2.m2.1.1.2.2.cmml">L</mi><mi id="S2.p2.2.m2.1.1.2.3" xref="S2.p2.2.m2.1.1.2.3.cmml">π</mi><mi id="S2.p2.2.m2.1.1.3" xref="S2.p2.2.m2.1.1.3.cmml">ρ</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1">superscript</csymbol><apply id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.2.1.cmml" xref="S2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.p2.2.m2.1.1.2.2.cmml" xref="S2.p2.2.m2.1.1.2.2">𝐿</ci><ci id="S2.p2.2.m2.1.1.2.3.cmml" xref="S2.p2.2.m2.1.1.2.3">𝜋</ci></apply><ci id="S2.p2.2.m2.1.1.3.cmml" xref="S2.p2.2.m2.1.1.3">𝜌</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">L_{\pi}^{\rho}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">italic_L start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ρ end_POSTSUPERSCRIPT</annotation></semantics></math> for data drawn from <math alttext="\rho" class="ltx_Math" display="inline" id="S2.p2.3.m3.1"><semantics id="S2.p2.3.m3.1a"><mi id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">italic_ρ</annotation></semantics></math> is at least the cross entropy:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{\pi}^{\rho}\geq\mathbb{E}_{x\sim\rho}[-\log_{2}\pi(x)]=\mathbb{E}_{x\sim%
\rho}\left[-\log_{2}\frac{\pi(x)\rho(x)}{\rho(x)}\right]=H(\rho)+D_{\text{KL}}%
(\rho||\pi)\geq H(\rho)," class="ltx_math_unparsed" display="block" id="S2.Ex1.m1.5"><semantics id="S2.Ex1.m1.5a"><mrow id="S2.Ex1.m1.5b"><msubsup id="S2.Ex1.m1.5.6"><mi id="S2.Ex1.m1.5.6.2.2">L</mi><mi id="S2.Ex1.m1.5.6.2.3">π</mi><mi id="S2.Ex1.m1.5.6.3">ρ</mi></msubsup><mo id="S2.Ex1.m1.5.7">≥</mo><msub id="S2.Ex1.m1.5.8"><mi id="S2.Ex1.m1.5.8.2">𝔼</mi><mrow id="S2.Ex1.m1.5.8.3"><mi id="S2.Ex1.m1.5.8.3.2">x</mi><mo id="S2.Ex1.m1.5.8.3.1">∼</mo><mi id="S2.Ex1.m1.5.8.3.3">ρ</mi></mrow></msub><mrow id="S2.Ex1.m1.5.9"><mo id="S2.Ex1.m1.5.9.1" stretchy="false">[</mo><mo id="S2.Ex1.m1.5.9.2" lspace="0em">−</mo><msub id="S2.Ex1.m1.5.9.3"><mi id="S2.Ex1.m1.5.9.3.2">log</mi><mn id="S2.Ex1.m1.5.9.3.3">2</mn></msub><mi id="S2.Ex1.m1.5.9.4">π</mi><mrow id="S2.Ex1.m1.5.9.5"><mo id="S2.Ex1.m1.5.9.5.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.4.4">x</mi><mo id="S2.Ex1.m1.5.9.5.2" stretchy="false">)</mo></mrow><mo id="S2.Ex1.m1.5.9.6" stretchy="false">]</mo></mrow><mo id="S2.Ex1.m1.5.10">=</mo><msub id="S2.Ex1.m1.5.11"><mi id="S2.Ex1.m1.5.11.2">𝔼</mi><mrow id="S2.Ex1.m1.5.11.3"><mi id="S2.Ex1.m1.5.11.3.2">x</mi><mo id="S2.Ex1.m1.5.11.3.1">∼</mo><mi id="S2.Ex1.m1.5.11.3.3">ρ</mi></mrow></msub><mrow id="S2.Ex1.m1.5.12"><mo id="S2.Ex1.m1.5.12.1">[</mo><mo id="S2.Ex1.m1.5.12.2" lspace="0em">−</mo><msub id="S2.Ex1.m1.5.12.3"><mi id="S2.Ex1.m1.5.12.3.2">log</mi><mn id="S2.Ex1.m1.5.12.3.3">2</mn></msub><mfrac id="S2.Ex1.m1.3.3"><mrow id="S2.Ex1.m1.2.2.2"><mi id="S2.Ex1.m1.2.2.2.4">π</mi><mo id="S2.Ex1.m1.2.2.2.3">⁢</mo><mrow id="S2.Ex1.m1.2.2.2.5.2"><mo id="S2.Ex1.m1.2.2.2.5.2.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.1.1.1.1">x</mi><mo id="S2.Ex1.m1.2.2.2.5.2.2" stretchy="false">)</mo></mrow><mo id="S2.Ex1.m1.2.2.2.3a">⁢</mo><mi id="S2.Ex1.m1.2.2.2.6">ρ</mi><mo id="S2.Ex1.m1.2.2.2.3b">⁢</mo><mrow id="S2.Ex1.m1.2.2.2.7.2"><mo id="S2.Ex1.m1.2.2.2.7.2.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.2.2.2.2">x</mi><mo id="S2.Ex1.m1.2.2.2.7.2.2" stretchy="false">)</mo></mrow></mrow><mrow id="S2.Ex1.m1.3.3.3"><mi id="S2.Ex1.m1.3.3.3.3">ρ</mi><mo id="S2.Ex1.m1.3.3.3.2">⁢</mo><mrow id="S2.Ex1.m1.3.3.3.4.2"><mo id="S2.Ex1.m1.3.3.3.4.2.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.3.3.3.1">x</mi><mo id="S2.Ex1.m1.3.3.3.4.2.2" stretchy="false">)</mo></mrow></mrow></mfrac><mo id="S2.Ex1.m1.5.12.4">]</mo></mrow><mo id="S2.Ex1.m1.5.13">=</mo><mi id="S2.Ex1.m1.5.14">H</mi><mrow id="S2.Ex1.m1.5.15"><mo id="S2.Ex1.m1.5.15.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.5.5">ρ</mi><mo id="S2.Ex1.m1.5.15.2" stretchy="false">)</mo></mrow><mo id="S2.Ex1.m1.5.16">+</mo><msub id="S2.Ex1.m1.5.17"><mi id="S2.Ex1.m1.5.17.2">D</mi><mtext id="S2.Ex1.m1.5.17.3">KL</mtext></msub><mrow id="S2.Ex1.m1.5.18"><mo id="S2.Ex1.m1.5.18.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.5.18.2">ρ</mi><mo fence="false" id="S2.Ex1.m1.5.18.3" rspace="0.167em" stretchy="false">|</mo><mo fence="false" id="S2.Ex1.m1.5.18.4" rspace="0.167em" stretchy="false">|</mo><mi id="S2.Ex1.m1.5.18.5">π</mi><mo id="S2.Ex1.m1.5.18.6" stretchy="false">)</mo></mrow><mo id="S2.Ex1.m1.5.19">≥</mo><mi id="S2.Ex1.m1.5.20">H</mi><mrow id="S2.Ex1.m1.5.21"><mo id="S2.Ex1.m1.5.21.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.5.21.2">ρ</mi><mo id="S2.Ex1.m1.5.21.3" stretchy="false">)</mo></mrow><mo id="S2.Ex1.m1.5.22">,</mo></mrow><annotation encoding="application/x-tex" id="S2.Ex1.m1.5c">L_{\pi}^{\rho}\geq\mathbb{E}_{x\sim\rho}[-\log_{2}\pi(x)]=\mathbb{E}_{x\sim%
\rho}\left[-\log_{2}\frac{\pi(x)\rho(x)}{\rho(x)}\right]=H(\rho)+D_{\text{KL}}%
(\rho||\pi)\geq H(\rho),</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.5d">italic_L start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ρ end_POSTSUPERSCRIPT ≥ blackboard_E start_POSTSUBSCRIPT italic_x ∼ italic_ρ end_POSTSUBSCRIPT [ - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_π ( italic_x ) ] = blackboard_E start_POSTSUBSCRIPT italic_x ∼ italic_ρ end_POSTSUBSCRIPT [ - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT divide start_ARG italic_π ( italic_x ) italic_ρ ( italic_x ) end_ARG start_ARG italic_ρ ( italic_x ) end_ARG ] = italic_H ( italic_ρ ) + italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_ρ | | italic_π ) ≥ italic_H ( italic_ρ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p2.9">which is also lower-bounded by the Shannon entropy of <math alttext="\rho" class="ltx_Math" display="inline" id="S2.p2.4.m1.1"><semantics id="S2.p2.4.m1.1a"><mi id="S2.p2.4.m1.1.1" xref="S2.p2.4.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.4.m1.1b"><ci id="S2.p2.4.m1.1.1.cmml" xref="S2.p2.4.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S2.p2.4.m1.1d">italic_ρ</annotation></semantics></math>.
A mismatch between <math alttext="\pi" class="ltx_Math" display="inline" id="S2.p2.5.m2.1"><semantics id="S2.p2.5.m2.1a"><mi id="S2.p2.5.m2.1.1" xref="S2.p2.5.m2.1.1.cmml">π</mi><annotation-xml encoding="MathML-Content" id="S2.p2.5.m2.1b"><ci id="S2.p2.5.m2.1.1.cmml" xref="S2.p2.5.m2.1.1">𝜋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m2.1c">\pi</annotation><annotation encoding="application/x-llamapun" id="S2.p2.5.m2.1d">italic_π</annotation></semantics></math> and <math alttext="\rho" class="ltx_Math" display="inline" id="S2.p2.6.m3.1"><semantics id="S2.p2.6.m3.1a"><mi id="S2.p2.6.m3.1.1" xref="S2.p2.6.m3.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.6.m3.1b"><ci id="S2.p2.6.m3.1.1.cmml" xref="S2.p2.6.m3.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m3.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S2.p2.6.m3.1d">italic_ρ</annotation></semantics></math> thus leads to an excess length given by their KL divergence, and minimal coding length (maximal compression) implies <math alttext="\pi=\rho" class="ltx_Math" display="inline" id="S2.p2.7.m4.1"><semantics id="S2.p2.7.m4.1a"><mrow id="S2.p2.7.m4.1.1" xref="S2.p2.7.m4.1.1.cmml"><mi id="S2.p2.7.m4.1.1.2" xref="S2.p2.7.m4.1.1.2.cmml">π</mi><mo id="S2.p2.7.m4.1.1.1" xref="S2.p2.7.m4.1.1.1.cmml">=</mo><mi id="S2.p2.7.m4.1.1.3" xref="S2.p2.7.m4.1.1.3.cmml">ρ</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.7.m4.1b"><apply id="S2.p2.7.m4.1.1.cmml" xref="S2.p2.7.m4.1.1"><eq id="S2.p2.7.m4.1.1.1.cmml" xref="S2.p2.7.m4.1.1.1"></eq><ci id="S2.p2.7.m4.1.1.2.cmml" xref="S2.p2.7.m4.1.1.2">𝜋</ci><ci id="S2.p2.7.m4.1.1.3.cmml" xref="S2.p2.7.m4.1.1.3">𝜌</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m4.1c">\pi=\rho</annotation><annotation encoding="application/x-llamapun" id="S2.p2.7.m4.1d">italic_π = italic_ρ</annotation></semantics></math> across the whole support of <math alttext="\rho" class="ltx_Math" display="inline" id="S2.p2.8.m5.1"><semantics id="S2.p2.8.m5.1a"><mi id="S2.p2.8.m5.1.1" xref="S2.p2.8.m5.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.8.m5.1b"><ci id="S2.p2.8.m5.1.1.cmml" xref="S2.p2.8.m5.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m5.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S2.p2.8.m5.1d">italic_ρ</annotation></semantics></math>.
Accordingly, some AI researchers have argued that compressing well is fundamentally connected to intelligence (e.g., Chaitin’s famous “Compression is Comprehension”&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chaitin, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib6" title="">2006</a>)</cite>; <cite class="ltx_cite ltx_citemacro_citet">Rathmanner and Hutter (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib56" title="">2011</a>); Grau-Moya et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib21" title="">2024</a>)</cite>), and that building universal compressors will accelerate AI development (cf.&nbsp;the Hutter prize&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib26" title="">2006</a>)</cite>, an ongoing competition to compress (<math alttext="1" class="ltx_Math" display="inline" id="S2.p2.9.m6.1"><semantics id="S2.p2.9.m6.1a"><mn id="S2.p2.9.m6.1.1" xref="S2.p2.9.m6.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S2.p2.9.m6.1b"><cn id="S2.p2.9.m6.1.1.cmml" type="integer" xref="S2.p2.9.m6.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.9.m6.1c">1</annotation><annotation encoding="application/x-llamapun" id="S2.p2.9.m6.1d">1</annotation></semantics></math>GB of) human knowledge).
The duality between compression and prediction has also led to the (algorithmic) information-theoretic formulation of universal prediction, i.e., Solomonoff induction&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Solomonoff, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib68" title="">1964a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib69" title="">b</a>; Li and Vitányi, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib37" title="">2019</a>)</cite>, one of two key ingredients for AIXI&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Legg and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib34" title="">2007</a>; Hutter et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib27" title="">2024</a>)</cite>, the theory of artificial superintelligence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Consequently, <cite class="ltx_cite ltx_citemacro_citet">Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite> argue that lossless compression performance lends itself as a domain-general metric for assessing any predictor’s quality, including foundation models.
They further emphasize that foundation models trained by minimizing log-loss (a.k.a., next-token prediction-error or cross entropy loss) are explicitly trained to minimize the expected coding length:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\pi}L_{\pi}^{\rho}=\min_{\pi}\underbrace{\mathbb{E}_{x\sim\rho}[-\log_{2%
}\pi(x)]}_{\text{``log loss''}}=\min_{\pi}\mathbb{E}_{x\sim\rho}\left[\sum_{i}%
-\log_{2}\pi(x_{i}|x_{<i})\right]." class="ltx_Math" display="block" id="S2.E1.m1.3"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.3" xref="S2.E1.m1.3.3.1.1.3.cmml"><munder id="S2.E1.m1.3.3.1.1.3.1" xref="S2.E1.m1.3.3.1.1.3.1.cmml"><mi id="S2.E1.m1.3.3.1.1.3.1.2" xref="S2.E1.m1.3.3.1.1.3.1.2.cmml">min</mi><mi id="S2.E1.m1.3.3.1.1.3.1.3" xref="S2.E1.m1.3.3.1.1.3.1.3.cmml">π</mi></munder><mo id="S2.E1.m1.3.3.1.1.3a" lspace="0.167em" xref="S2.E1.m1.3.3.1.1.3.cmml">⁡</mo><msubsup id="S2.E1.m1.3.3.1.1.3.2" xref="S2.E1.m1.3.3.1.1.3.2.cmml"><mi id="S2.E1.m1.3.3.1.1.3.2.2.2" xref="S2.E1.m1.3.3.1.1.3.2.2.2.cmml">L</mi><mi id="S2.E1.m1.3.3.1.1.3.2.2.3" xref="S2.E1.m1.3.3.1.1.3.2.2.3.cmml">π</mi><mi id="S2.E1.m1.3.3.1.1.3.2.3" xref="S2.E1.m1.3.3.1.1.3.2.3.cmml">ρ</mi></msubsup></mrow><mo id="S2.E1.m1.3.3.1.1.4" xref="S2.E1.m1.3.3.1.1.4.cmml">=</mo><mrow id="S2.E1.m1.3.3.1.1.5" xref="S2.E1.m1.3.3.1.1.5.cmml"><munder id="S2.E1.m1.3.3.1.1.5.1" xref="S2.E1.m1.3.3.1.1.5.1.cmml"><mi id="S2.E1.m1.3.3.1.1.5.1.2" xref="S2.E1.m1.3.3.1.1.5.1.2.cmml">min</mi><mi id="S2.E1.m1.3.3.1.1.5.1.3" xref="S2.E1.m1.3.3.1.1.5.1.3.cmml">π</mi></munder><mo id="S2.E1.m1.3.3.1.1.5a" lspace="0.167em" xref="S2.E1.m1.3.3.1.1.5.cmml">⁡</mo><munder id="S2.E1.m1.3.3.1.1.5.2" xref="S2.E1.m1.3.3.1.1.5.2.cmml"><munder accentunder="true" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><msub id="S2.E1.m1.2.2.2.4" xref="S2.E1.m1.2.2.2.4.cmml"><mi id="S2.E1.m1.2.2.2.4.2" xref="S2.E1.m1.2.2.2.4.2.cmml">𝔼</mi><mrow id="S2.E1.m1.2.2.2.4.3" xref="S2.E1.m1.2.2.2.4.3.cmml"><mi id="S2.E1.m1.2.2.2.4.3.2" xref="S2.E1.m1.2.2.2.4.3.2.cmml">x</mi><mo id="S2.E1.m1.2.2.2.4.3.1" xref="S2.E1.m1.2.2.2.4.3.1.cmml">∼</mo><mi id="S2.E1.m1.2.2.2.4.3.3" xref="S2.E1.m1.2.2.2.4.3.3.cmml">ρ</mi></mrow></msub><mo id="S2.E1.m1.2.2.2.3" xref="S2.E1.m1.2.2.2.3.cmml">⁢</mo><mrow id="S2.E1.m1.2.2.2.2.1" xref="S2.E1.m1.2.2.2.2.2.cmml"><mo id="S2.E1.m1.2.2.2.2.1.2" stretchy="false" xref="S2.E1.m1.2.2.2.2.2.1.cmml">[</mo><mrow id="S2.E1.m1.2.2.2.2.1.1" xref="S2.E1.m1.2.2.2.2.1.1.cmml"><mo id="S2.E1.m1.2.2.2.2.1.1a" rspace="0.167em" xref="S2.E1.m1.2.2.2.2.1.1.cmml">−</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml"><mrow id="S2.E1.m1.2.2.2.2.1.1.2.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.cmml"><msub id="S2.E1.m1.2.2.2.2.1.1.2.2.1" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.2.2.1.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1.2.cmml">log</mi><mn id="S2.E1.m1.2.2.2.2.1.1.2.2.1.3" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1.3.cmml">2</mn></msub><mo id="S2.E1.m1.2.2.2.2.1.1.2.2a" lspace="0.167em" xref="S2.E1.m1.2.2.2.2.1.1.2.2.cmml">⁡</mo><mi id="S2.E1.m1.2.2.2.2.1.1.2.2.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.cmml">π</mi></mrow><mo id="S2.E1.m1.2.2.2.2.1.1.2.1" xref="S2.E1.m1.2.2.2.2.1.1.2.1.cmml">⁢</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.2.3.2" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml"><mo id="S2.E1.m1.2.2.2.2.1.1.2.3.2.1" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml">(</mo><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">x</mi><mo id="S2.E1.m1.2.2.2.2.1.1.2.3.2.2" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.2.2.2.2.1.3" stretchy="false" xref="S2.E1.m1.2.2.2.2.2.1.cmml">]</mo></mrow></mrow><mo id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">⏟</mo></munder><mtext id="S2.E1.m1.3.3.1.1.5.2.2" xref="S2.E1.m1.3.3.1.1.5.2.2a.cmml">“log loss”</mtext></munder></mrow><mo id="S2.E1.m1.3.3.1.1.6" xref="S2.E1.m1.3.3.1.1.6.cmml">=</mo><mrow id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.3.cmml"><munder id="S2.E1.m1.3.3.1.1.1.3.1" xref="S2.E1.m1.3.3.1.1.1.3.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3.1.2" xref="S2.E1.m1.3.3.1.1.1.3.1.2.cmml">min</mi><mi id="S2.E1.m1.3.3.1.1.1.3.1.3" xref="S2.E1.m1.3.3.1.1.1.3.1.3.cmml">π</mi></munder><mo id="S2.E1.m1.3.3.1.1.1.3a" lspace="0.167em" xref="S2.E1.m1.3.3.1.1.1.3.cmml">⁡</mo><msub id="S2.E1.m1.3.3.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.3.2.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3.2.2" xref="S2.E1.m1.3.3.1.1.1.3.2.2.cmml">𝔼</mi><mrow id="S2.E1.m1.3.3.1.1.1.3.2.3" xref="S2.E1.m1.3.3.1.1.1.3.2.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3.2.3.2" xref="S2.E1.m1.3.3.1.1.1.3.2.3.2.cmml">x</mi><mo id="S2.E1.m1.3.3.1.1.1.3.2.3.1" xref="S2.E1.m1.3.3.1.1.1.3.2.3.1.cmml">∼</mo><mi id="S2.E1.m1.3.3.1.1.1.3.2.3.3" xref="S2.E1.m1.3.3.1.1.1.3.2.3.3.cmml">ρ</mi></mrow></msub></mrow><mo id="S2.E1.m1.3.3.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml"><mo id="S2.E1.m1.3.3.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.2.1.cmml">[</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><munder id="S2.E1.m1.3.3.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.cmml"><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.2" lspace="0em" movablelimits="false" rspace="0em" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.2.cmml">∑</mo><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml">i</mi></munder><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.2" lspace="0em" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.2.cmml">log</mi><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.3.cmml">2</mn></msub><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3a" lspace="0.167em" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">π</mi></mrow><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.3.3.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S2.E1.m1.3.3.1.2" lspace="0em" xref="S2.E1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1"><and id="S2.E1.m1.3.3.1.1a.cmml" xref="S2.E1.m1.3.3.1"></and><apply id="S2.E1.m1.3.3.1.1b.cmml" xref="S2.E1.m1.3.3.1"><eq id="S2.E1.m1.3.3.1.1.4.cmml" xref="S2.E1.m1.3.3.1.1.4"></eq><apply id="S2.E1.m1.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3"><apply id="S2.E1.m1.3.3.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1">subscript</csymbol><min id="S2.E1.m1.3.3.1.1.3.1.2.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2"></min><ci id="S2.E1.m1.3.3.1.1.3.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3.1.3">𝜋</ci></apply><apply id="S2.E1.m1.3.3.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2">superscript</csymbol><apply id="S2.E1.m1.3.3.1.1.3.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.3.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2.2">𝐿</ci><ci id="S2.E1.m1.3.3.1.1.3.2.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2.3">𝜋</ci></apply><ci id="S2.E1.m1.3.3.1.1.3.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3">𝜌</ci></apply></apply><apply id="S2.E1.m1.3.3.1.1.5.cmml" xref="S2.E1.m1.3.3.1.1.5"><apply id="S2.E1.m1.3.3.1.1.5.1.cmml" xref="S2.E1.m1.3.3.1.1.5.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.5.1.1.cmml" xref="S2.E1.m1.3.3.1.1.5.1">subscript</csymbol><min id="S2.E1.m1.3.3.1.1.5.1.2.cmml" xref="S2.E1.m1.3.3.1.1.5.1.2"></min><ci id="S2.E1.m1.3.3.1.1.5.1.3.cmml" xref="S2.E1.m1.3.3.1.1.5.1.3">𝜋</ci></apply><apply id="S2.E1.m1.3.3.1.1.5.2.cmml" xref="S2.E1.m1.3.3.1.1.5.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.5.2.1.cmml" xref="S2.E1.m1.3.3.1.1.5.2">subscript</csymbol><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><ci id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3">⏟</ci><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><times id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.3"></times><apply id="S2.E1.m1.2.2.2.4.cmml" xref="S2.E1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.4.1.cmml" xref="S2.E1.m1.2.2.2.4">subscript</csymbol><ci id="S2.E1.m1.2.2.2.4.2.cmml" xref="S2.E1.m1.2.2.2.4.2">𝔼</ci><apply id="S2.E1.m1.2.2.2.4.3.cmml" xref="S2.E1.m1.2.2.2.4.3"><csymbol cd="latexml" id="S2.E1.m1.2.2.2.4.3.1.cmml" xref="S2.E1.m1.2.2.2.4.3.1">similar-to</csymbol><ci id="S2.E1.m1.2.2.2.4.3.2.cmml" xref="S2.E1.m1.2.2.2.4.3.2">𝑥</ci><ci id="S2.E1.m1.2.2.2.4.3.3.cmml" xref="S2.E1.m1.2.2.2.4.3.3">𝜌</ci></apply></apply><apply id="S2.E1.m1.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1"><csymbol cd="latexml" id="S2.E1.m1.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.2">delimited-[]</csymbol><apply id="S2.E1.m1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1"><minus id="S2.E1.m1.2.2.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1"></minus><apply id="S2.E1.m1.2.2.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2"><times id="S2.E1.m1.2.2.2.2.1.1.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.1"></times><apply id="S2.E1.m1.2.2.2.2.1.1.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2"><apply id="S2.E1.m1.2.2.2.2.1.1.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1">subscript</csymbol><log id="S2.E1.m1.2.2.2.2.1.1.2.2.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1.2"></log><cn id="S2.E1.m1.2.2.2.2.1.1.2.2.1.3.cmml" type="integer" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1.3">2</cn></apply><ci id="S2.E1.m1.2.2.2.2.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2">𝜋</ci></apply><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">𝑥</ci></apply></apply></apply></apply></apply><ci id="S2.E1.m1.3.3.1.1.5.2.2a.cmml" xref="S2.E1.m1.3.3.1.1.5.2.2"><mtext id="S2.E1.m1.3.3.1.1.5.2.2.cmml" mathsize="70%" xref="S2.E1.m1.3.3.1.1.5.2.2">“log loss”</mtext></ci></apply></apply></apply><apply id="S2.E1.m1.3.3.1.1c.cmml" xref="S2.E1.m1.3.3.1"><eq id="S2.E1.m1.3.3.1.1.6.cmml" xref="S2.E1.m1.3.3.1.1.6"></eq><share href="https://arxiv.org/html/2410.05078v1#S2.E1.m1.3.3.1.1.5.cmml" id="S2.E1.m1.3.3.1.1d.cmml" xref="S2.E1.m1.3.3.1"></share><apply id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.2"></times><apply id="S2.E1.m1.3.3.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3"><apply id="S2.E1.m1.3.3.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.3.1">subscript</csymbol><min id="S2.E1.m1.3.3.1.1.1.3.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.3.1.2"></min><ci id="S2.E1.m1.3.3.1.1.1.3.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3.1.3">𝜋</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.3.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.3.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2.2">𝔼</ci><apply id="S2.E1.m1.3.3.1.1.1.3.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2.3"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.3.2.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2.3.1">similar-to</csymbol><ci id="S2.E1.m1.3.3.1.1.1.3.2.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2.3.2">𝑥</ci><ci id="S2.E1.m1.3.3.1.1.1.3.2.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2.3.3">𝜌</ci></apply></apply></apply><apply id="S2.E1.m1.3.3.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1"><minus id="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2"></minus><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><sum id="S2.E1.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.2"></sum><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3"><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1">subscript</csymbol><log id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.2"></log><cn id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.3.cmml" type="integer" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.3">2</cn></apply><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2">𝜋</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">𝑥</ci><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3"><lt id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\min_{\pi}L_{\pi}^{\rho}=\min_{\pi}\underbrace{\mathbb{E}_{x\sim\rho}[-\log_{2%
}\pi(x)]}_{\text{``log loss''}}=\min_{\pi}\mathbb{E}_{x\sim\rho}\left[\sum_{i}%
-\log_{2}\pi(x_{i}|x_{&lt;i})\right].</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">roman_min start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ρ end_POSTSUPERSCRIPT = roman_min start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT under⏟ start_ARG blackboard_E start_POSTSUBSCRIPT italic_x ∼ italic_ρ end_POSTSUBSCRIPT [ - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_π ( italic_x ) ] end_ARG start_POSTSUBSCRIPT “log loss” end_POSTSUBSCRIPT = roman_min start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_x ∼ italic_ρ end_POSTSUBSCRIPT [ ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_π ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p3.2">Note that the problem of constructing the actual codes that achieve (near) minimal expected code length given a predictor is largely solved in information theory, with gold-standard algorithms such as Huffman coding&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Huffman, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib25" title="">1952</a>)</cite>, arithmetic coding&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pasco, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib52" title="">1977</a>; Rissanen, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib58" title="">1976</a>; Witten et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib81" title="">1987</a>)</cite>, or asymmetric numeral systems&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Duda, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib14" title="">2009</a>)</cite>.
The latter two compress strings online by iteratively converting them into a single binary number with increasing precision (see <cite class="ltx_cite ltx_citemacro_citet">Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite> for an illustration or Chapter&nbsp;2 in <cite class="ltx_cite ltx_citemacro_citet">Hutter et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib27" title="">2024</a>)</cite>).
Arithmetic coding is an example of an online compression algorithm since it only requires a single pass through the data and compresses on the fly (unlike offline compressors, such as Huffman coding, that require multiple passes through the data).
Both our models and <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite>, which we compare against, use arithmetic coding and compress online.
However, the difference is that we pre-train our predictor, i.e., we perform <em class="ltx_emph ltx_font_italic" id="S2.p3.2.1">offline training</em> on a dataset and then freeze its parameters (non-adaptive arithmetic coding), whereas <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite> performs <em class="ltx_emph ltx_font_italic" id="S2.p3.2.2">online adaptation</em> of the model parameters on the data stream that is being compressed (adaptive arithmetic coding).
As a result, and unlike our compressors, <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite> does not communicate the trained weights for decompression but only the model architecture and training algorithm (i.e., the model parameters do not need to be factored into the compression ratio).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Compression Without Transformers</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">Using neural networks as predictors for lossless compression has been extensively studied, both in conjunction with arithmetic coding&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lehtokangas et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib35" title="">1993</a>; Schmidhuber and Heil, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib62" title="">1994</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib63" title="">1996</a>; Mahoney, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib42" title="">2000</a>; Mikolov, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib47" title="">2012</a>; Knoll, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib32" title="">2014</a>; van&nbsp;den Oord and Schrauwen, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib75" title="">2014</a>; Cox, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib10" title="">2016</a>; Schiopu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib61" title="">2018</a>; Goyal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib19" title="">2019</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib39" title="">2019</a>; Mentzer et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib45" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib46" title="">2020</a>; Schiopu and Munteanu, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib60" title="">2020</a>; Rhee et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib57" title="">2022</a>)</cite> and with asymmetric numeral systems&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hoogeboom et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib23" title="">2019</a>; Kingma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib31" title="">2019</a>; Townsend et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib73" title="">2019</a>; Barzen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib2" title="">2022</a>)</cite>.
Lossy neural compression has been achieved, e.g., by overfitting tiny networks to individual data points and transmitting the model weights rather than the original data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dupont et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib15" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib16" title="">2022</a>; Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib7" title="">2021</a>; Ladune et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib33" title="">2023</a>; Kim et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib29" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Online Transformers</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">Most of the above approaches use a separate training set to pre-train models that are then used to compress a test set.
Alternatively, the model can also be trained from scratch on the data stream that is being compressed&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bellard, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib3" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>; Goyal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib20" title="">2020</a>; Mao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib44" title="">2022</a>)</cite>.
The main advantage of these adaptive online compressors is that they are (quasi) parameterless (since they are initialized from scratch when compressing a new stream of data), meaning that the model size does not explicitly affect the compression ratio, even for relatively large models (though it implicitly affects the training performance, e.g., large models train more slowly meaning that larger chunks of the initial data stream are only weakly compressed).
The transformer-based adaptive online compressor of <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite> is currently state-of-the-art on the Large Text Compression Benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mahoney, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib43" title="">2006</a>)</cite>, and our evaluation (in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5" title="5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>) shows that our best models are on par across all modalities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Pre-Trained Transformers</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">Most closely related to our work is the line of research by <cite class="ltx_cite ltx_citemacro_citet">Valmeekam et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib74" title="">2023</a>); Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>); Huang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib24" title="">2024</a>); Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib38" title="">2024</a>); Mittu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib50" title="">2024</a>)</cite>, which investigates lossless compression via arithmetic coding with pre-trained foundation models, i.e., the Llama models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib71" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib72" title="">b</a>; Dubey et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib13" title="">2024</a>)</cite> and Chinchilla&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib22" title="">2022</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>, in particular, also report good compression rates on unseen modalities (LLMs trained only on text compress images and audio data well).
However, these studies differ from our work as they do not take the model size into account for the compression ratios, except for <cite class="ltx_cite ltx_citemacro_citet">Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>, who report both “raw” and “adjusted” compression ratios and find that LLMs are not competitive in terms of adjusted (i.e., the actual) compression ratios.
To the best of our knowledge, our paper is the first to systematically investigate the use of appropriately sized pre-trained transformers for multimodal lossless compression in a regime where competitive performance w.r.t.&nbsp;standard compression algorithms is possible.
In this regime, our study is the most comprehensive in that it also investigates multimodal training and cross-modal transfer of pre-trained transformers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methods</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We now describe our experimental setup (with additional details, e.g., sweeps, in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1" title="Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">A</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">We compare to various standard compressors, both general-purpose, i.e., gzip&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Deutsch, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib12" title="">1996</a>)</cite> and LZMA2&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pavlov, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib53" title="">2019</a>)</cite>, and domain-specific, i.e., FLAC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Coalson, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib8" title="">2008</a>)</cite> for audio data and PNG&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Boutell, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib5" title="">1997</a>)</cite> and lossless JPEG&nbsp;2000&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Skodras et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib67" title="">2001</a>)</cite> for images.
Both gzip and LZMA2 (which is used by the 7zip software) are based on Huffman coding&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Huffman, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib25" title="">1952</a>)</cite> and the Lempel-Ziv-Welch algorithm&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Welch, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib79" title="">1984</a>)</cite>.
We use the default parameters for gzip, LZMA2, and JPEG&nbsp;2000, compression level 12 for FLAC, and instruct PNG to find the optimal encoder settings.
We also compare to the online transformer from <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite>, with the default v3.3 parameters, which is the current state-of-the-art on the Large Text Compression Benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mahoney, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib43" title="">2006</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Models</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.8">We focus on decoder-only transformers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib76" title="">2017</a>)</cite> with SwiGLU activations&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shazeer, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib66" title="">2020</a>)</cite> and post-layer normalization.
Unless otherwise noted, we use <math alttext="8" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><cn id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.1.m1.1d">8</annotation></semantics></math> heads, an embedding dimension of <math alttext="64" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px2.p1.2.m2.1a"><mn id="S4.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.2.m2.1b"><cn id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.2.m2.1c">64</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.2.m2.1d">64</annotation></semantics></math>, a context size of <math alttext="4096" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S4.SS0.SSS0.Px2.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.3.m3.1b"><cn id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.3.m3.1c">4096</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.3.m3.1d">4096</annotation></semantics></math> (bytes), and sliding windows without overlap or memory (full details in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="A.3 Sweeps ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">A.3</span></a>).
We train our models with the Adam optimizer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib30" title="">2015</a>)</cite> for <math alttext="2.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="S4.SS0.SSS0.Px2.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml">2.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.4.m4.1b"><cn id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1">2.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.4.m4.1c">2.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.4.m4.1d">2.5</annotation></semantics></math> million steps with a batch size of <math alttext="32" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.5.m5.1"><semantics id="S4.SS0.SSS0.Px2.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.5.m5.1b"><cn id="S4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.5.m5.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.5.m5.1c">32</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.5.m5.1d">32</annotation></semantics></math>, which, for <math alttext="165" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.6.m6.1"><semantics id="S4.SS0.SSS0.Px2.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px2.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px2.p1.6.m6.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.6.m6.1b"><cn id="S4.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.6.m6.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.6.m6.1c">165</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.6.m6.1d">165</annotation></semantics></math>GB of data, roughly corresponds to <math alttext="2" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.7.m7.1"><semantics id="S4.SS0.SSS0.Px2.p1.7.m7.1a"><mn id="S4.SS0.SSS0.Px2.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px2.p1.7.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.7.m7.1b"><cn id="S4.SS0.SSS0.Px2.p1.7.m7.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.7.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.7.m7.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.7.m7.1d">2</annotation></semantics></math> epochs.
Due to the duality of compression and prediction, we minimize the standard (sequential) <math alttext="\log" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.8.m8.1"><semantics id="S4.SS0.SSS0.Px2.p1.8.m8.1a"><mi id="S4.SS0.SSS0.Px2.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px2.p1.8.m8.1.1.cmml">log</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.8.m8.1b"><log id="S4.SS0.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.8.m8.1.1"></log></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.8.m8.1c">\log</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.8.m8.1d">roman_log</annotation></semantics></math>-loss&nbsp;(<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S2.E1" title="In 2 Background ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Eq.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>) during training, which is a maximum-compression objective (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S2" title="2 Background ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">(No) Tokenization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">Tokenization is a commonly-used, <em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS0.Px3.p1.1.1">domain-specific</em> pre-compression step to boost transformers’ performance by increasing their vocabulary size in order to fit more information into their limited context window&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lester et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib36" title="">2024</a>)</cite>, i.e., increased information density at the cost of increased entropy.
However, since our goal is to be domain-general, we do not use tokenization and instead feed our models directly with byte streams (we still have to choose how to flatten images and how to sample audio signals, which are minimal domain-specific preprocessing steps).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px4.p1.7">To evaluate performance, we compute the compression ratio (lower is better):</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{compression\leavevmode\nobreak\ ratio}:=\frac{\mathrm{size\leavevmode%
\nobreak\ of\leavevmode\nobreak\ compressed\leavevmode\nobreak\ data}+\mathrm{%
size\leavevmode\nobreak\ of\leavevmode\nobreak\ compressor}}{\mathrm{size%
\leavevmode\nobreak\ of\leavevmode\nobreak\ uncompressed\leavevmode\nobreak\ %
data}}," class="ltx_Math" display="block" id="S4.E2.m1.1"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.2.2.cmml">compression</mi><mo id="S4.E2.m1.1.1.1.1.2.1" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.2.3.cmml">ratio</mi></mrow><mo id="S4.E2.m1.1.1.1.1.1" lspace="0.278em" rspace="0.278em" xref="S4.E2.m1.1.1.1.1.1.cmml">:=</mo><mfrac id="S4.E2.m1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.cmml"><mrow id="S4.E2.m1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.3.2.cmml"><mrow id="S4.E2.m1.1.1.1.1.3.2.2" xref="S4.E2.m1.1.1.1.1.3.2.2.cmml"><mi id="S4.E2.m1.1.1.1.1.3.2.2.2" xref="S4.E2.m1.1.1.1.1.3.2.2.2.cmml">size</mi><mo id="S4.E2.m1.1.1.1.1.3.2.2.1" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.3.2.2.3" xref="S4.E2.m1.1.1.1.1.3.2.2.3.cmml">of</mi><mo id="S4.E2.m1.1.1.1.1.3.2.2.1a" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.3.2.2.4" xref="S4.E2.m1.1.1.1.1.3.2.2.4.cmml">compressed</mi><mo id="S4.E2.m1.1.1.1.1.3.2.2.1b" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.3.2.2.5" xref="S4.E2.m1.1.1.1.1.3.2.2.5.cmml">data</mi></mrow><mo id="S4.E2.m1.1.1.1.1.3.2.1" xref="S4.E2.m1.1.1.1.1.3.2.1.cmml">+</mo><mrow id="S4.E2.m1.1.1.1.1.3.2.3" xref="S4.E2.m1.1.1.1.1.3.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.2.3.2" xref="S4.E2.m1.1.1.1.1.3.2.3.2.cmml">size</mi><mo id="S4.E2.m1.1.1.1.1.3.2.3.1" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.3.2.3.3" xref="S4.E2.m1.1.1.1.1.3.2.3.3.cmml">of</mi><mo id="S4.E2.m1.1.1.1.1.3.2.3.1a" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.3.2.3.4" xref="S4.E2.m1.1.1.1.1.3.2.3.4.cmml">compressor</mi></mrow></mrow><mrow id="S4.E2.m1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.3.2" xref="S4.E2.m1.1.1.1.1.3.3.2.cmml">size</mi><mo id="S4.E2.m1.1.1.1.1.3.3.1" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.3.3.3" xref="S4.E2.m1.1.1.1.1.3.3.3.cmml">of</mi><mo id="S4.E2.m1.1.1.1.1.3.3.1a" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.3.3.4" xref="S4.E2.m1.1.1.1.1.3.3.4.cmml">uncompressed</mi><mo id="S4.E2.m1.1.1.1.1.3.3.1b" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.3.3.5" xref="S4.E2.m1.1.1.1.1.3.3.5.cmml">data</mi></mrow></mfrac></mrow><mo id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><csymbol cd="latexml" id="S4.E2.m1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1">assign</csymbol><apply id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2"><times id="S4.E2.m1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1"></times><ci id="S4.E2.m1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2">compression</ci><ci id="S4.E2.m1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3">ratio</ci></apply><apply id="S4.E2.m1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3"><divide id="S4.E2.m1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3"></divide><apply id="S4.E2.m1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2"><plus id="S4.E2.m1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.1"></plus><apply id="S4.E2.m1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2"><times id="S4.E2.m1.1.1.1.1.3.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.1"></times><ci id="S4.E2.m1.1.1.1.1.3.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.2">size</ci><ci id="S4.E2.m1.1.1.1.1.3.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.3">of</ci><ci id="S4.E2.m1.1.1.1.1.3.2.2.4.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.4">compressed</ci><ci id="S4.E2.m1.1.1.1.1.3.2.2.5.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.5">data</ci></apply><apply id="S4.E2.m1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3"><times id="S4.E2.m1.1.1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.2">size</ci><ci id="S4.E2.m1.1.1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.3">of</ci><ci id="S4.E2.m1.1.1.1.1.3.2.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.4">compressor</ci></apply></apply><apply id="S4.E2.m1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3"><times id="S4.E2.m1.1.1.1.1.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.3.2">size</ci><ci id="S4.E2.m1.1.1.1.1.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3">of</ci><ci id="S4.E2.m1.1.1.1.1.3.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.3.4">uncompressed</ci><ci id="S4.E2.m1.1.1.1.1.3.3.5.cmml" xref="S4.E2.m1.1.1.1.1.3.3.5">data</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\mathrm{compression\leavevmode\nobreak\ ratio}:=\frac{\mathrm{size\leavevmode%
\nobreak\ of\leavevmode\nobreak\ compressed\leavevmode\nobreak\ data}+\mathrm{%
size\leavevmode\nobreak\ of\leavevmode\nobreak\ compressor}}{\mathrm{size%
\leavevmode\nobreak\ of\leavevmode\nobreak\ uncompressed\leavevmode\nobreak\ %
data}},</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.1d">roman_compression roman_ratio := divide start_ARG roman_size roman_of roman_compressed roman_data + roman_size roman_of roman_compressor end_ARG start_ARG roman_size roman_of roman_uncompressed roman_data end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS0.SSS0.Px4.p1.6">which accounts for the model size and is equivalent to the “adjusted compression rate” of <cite class="ltx_cite ltx_citemacro_citet">Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>.
We always evaluate on <math alttext="1" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px4.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.1.m1.1b"><cn id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.1.m1.1d">1</annotation></semantics></math>GB of out-of-distribution data, i.e., <math alttext="\mathrm{size\leavevmode\nobreak\ of\leavevmode\nobreak\ uncompressed%
\leavevmode\nobreak\ data}=1\mathrm{GB}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px4.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px4.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.cmml"><mrow id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.cmml"><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.2" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.2.cmml">size</mi><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1" lspace="0.500em" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.3" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.3.cmml">of</mi><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1a" lspace="0.500em" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.4" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.4.cmml">uncompressed</mi><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1b" lspace="0.500em" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.5" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.5.cmml">data</mi></mrow><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.cmml"><mn id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.2" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.2.cmml">1</mn><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.3" mathvariant="normal" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.3.cmml">G</mi><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1a" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.4" mathvariant="normal" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.4.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1"><eq id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1"></eq><apply id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2"><times id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1"></times><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.2.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.2">size</ci><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.3.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.3">of</ci><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.4.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.4">uncompressed</ci><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.5.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.5">data</ci></apply><apply id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3"><times id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1"></times><cn id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.2.cmml" type="integer" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.2">1</cn><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.3">G</ci><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.4.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.4">B</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.2.m2.1c">\mathrm{size\leavevmode\nobreak\ of\leavevmode\nobreak\ uncompressed%
\leavevmode\nobreak\ data}=1\mathrm{GB}</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.2.m2.1d">roman_size roman_of roman_uncompressed roman_data = 1 roman_G roman_B</annotation></semantics></math>.
As <cite class="ltx_cite ltx_citemacro_citet">Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>, we compute the size of the compressor by encoding the model weights with <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px4.p1.6.1">float16</span> (2 bytes per parameter) since this level of quantization does not significantly affect performance&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Tao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib70" title="">2022</a>)</cite> and is standard for model inference.
As a result, our model sizes range from <math alttext="0.8" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.3.m3.1"><semantics id="S4.SS0.SSS0.Px4.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px4.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.3.m3.1b"><cn id="S4.SS0.SSS0.Px4.p1.3.m3.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.3.m3.1c">0.8</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.3.m3.1d">0.8</annotation></semantics></math>MB to <math alttext="40.3" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.4.m4.1"><semantics id="S4.SS0.SSS0.Px4.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px4.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px4.p1.4.m4.1.1.cmml">40.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.4.m4.1b"><cn id="S4.SS0.SSS0.Px4.p1.4.m4.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px4.p1.4.m4.1.1">40.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.4.m4.1c">40.3</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.4.m4.1d">40.3</annotation></semantics></math>MB.
Note that, similar to <cite class="ltx_cite ltx_citemacro_citet">Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>, we do not compress the model parameters, since naive approaches (e.g., compressing them with gzip) do not significantly decrease the model size (only by around <math alttext="7" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.5.m5.1"><semantics id="S4.SS0.SSS0.Px4.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px4.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px4.p1.5.m5.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.5.m5.1b"><cn id="S4.SS0.SSS0.Px4.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px4.p1.5.m5.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.5.m5.1c">7</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.5.m5.1d">7</annotation></semantics></math>%, which corresponds to a decrease in compression ratio of only <math alttext="0.002821" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.6.m6.1"><semantics id="S4.SS0.SSS0.Px4.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px4.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px4.p1.6.m6.1.1.cmml">0.002821</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.6.m6.1b"><cn id="S4.SS0.SSS0.Px4.p1.6.m6.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px4.p1.6.m6.1.1">0.002821</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.6.m6.1c">0.002821</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.6.m6.1d">0.002821</annotation></semantics></math> for our largest model).
However, as a result, the compression ratio we report is technically an upper bound, which could be improved by (losslessly) compressing the parameters (though with limited room for improvement in our regime, even in the best case).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Training Datasets</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px5.p1.13">A key point of our investigation is to evaluate how well pre-trained transformers can compress data from different modalities — both if the modality was or was not part of the training data (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S1.F1" title="In 1 Introduction ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a> visualizes our data collection process).
We create three different unimodal training datasets with audio, images, and text data, and four multimodal training sets (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS1" title="A.1 Training Data Sources ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">A.1</span></a> describes the datasets in full detail).
This yields seven pre-training datasets in total, each consisting of <math alttext="165" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px5.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.1.m1.1b"><cn id="S4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.1.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.1.m1.1c">165</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.1.m1.1d">165</annotation></semantics></math>GB of data: (i) <math alttext="165" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px5.p1.2.m2.1a"><mn id="S4.SS0.SSS0.Px5.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.2.m2.1b"><cn id="S4.SS0.SSS0.Px5.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.2.m2.1c">165</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.2.m2.1d">165</annotation></semantics></math>GB of audio; (ii) <math alttext="165" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.3.m3.1"><semantics id="S4.SS0.SSS0.Px5.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px5.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px5.p1.3.m3.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.3.m3.1b"><cn id="S4.SS0.SSS0.Px5.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.3.m3.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.3.m3.1c">165</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.3.m3.1d">165</annotation></semantics></math>GB of images; (iii) <math alttext="165" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.4.m4.1"><semantics id="S4.SS0.SSS0.Px5.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px5.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px5.p1.4.m4.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.4.m4.1b"><cn id="S4.SS0.SSS0.Px5.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.4.m4.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.4.m4.1c">165</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.4.m4.1d">165</annotation></semantics></math>GB of text; (iv) <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.5.m5.1"><semantics id="S4.SS0.SSS0.Px5.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px5.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px5.p1.5.m5.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.5.m5.1b"><cn id="S4.SS0.SSS0.Px5.p1.5.m5.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.5.m5.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.5.m5.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.5.m5.1d">82.5</annotation></semantics></math>GB of audio and <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.6.m6.1"><semantics id="S4.SS0.SSS0.Px5.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px5.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px5.p1.6.m6.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.6.m6.1b"><cn id="S4.SS0.SSS0.Px5.p1.6.m6.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.6.m6.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.6.m6.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.6.m6.1d">82.5</annotation></semantics></math>GB of images; (v) <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.7.m7.1"><semantics id="S4.SS0.SSS0.Px5.p1.7.m7.1a"><mn id="S4.SS0.SSS0.Px5.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px5.p1.7.m7.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.7.m7.1b"><cn id="S4.SS0.SSS0.Px5.p1.7.m7.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.7.m7.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.7.m7.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.7.m7.1d">82.5</annotation></semantics></math>GB of audio and <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.8.m8.1"><semantics id="S4.SS0.SSS0.Px5.p1.8.m8.1a"><mn id="S4.SS0.SSS0.Px5.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px5.p1.8.m8.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.8.m8.1b"><cn id="S4.SS0.SSS0.Px5.p1.8.m8.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.8.m8.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.8.m8.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.8.m8.1d">82.5</annotation></semantics></math>GB of text; (vi) <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.9.m9.1"><semantics id="S4.SS0.SSS0.Px5.p1.9.m9.1a"><mn id="S4.SS0.SSS0.Px5.p1.9.m9.1.1" xref="S4.SS0.SSS0.Px5.p1.9.m9.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.9.m9.1b"><cn id="S4.SS0.SSS0.Px5.p1.9.m9.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.9.m9.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.9.m9.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.9.m9.1d">82.5</annotation></semantics></math>GB of images and <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.10.m10.1"><semantics id="S4.SS0.SSS0.Px5.p1.10.m10.1a"><mn id="S4.SS0.SSS0.Px5.p1.10.m10.1.1" xref="S4.SS0.SSS0.Px5.p1.10.m10.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.10.m10.1b"><cn id="S4.SS0.SSS0.Px5.p1.10.m10.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.10.m10.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.10.m10.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.10.m10.1d">82.5</annotation></semantics></math>GB of text; and (vii) <math alttext="55" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.11.m11.1"><semantics id="S4.SS0.SSS0.Px5.p1.11.m11.1a"><mn id="S4.SS0.SSS0.Px5.p1.11.m11.1.1" xref="S4.SS0.SSS0.Px5.p1.11.m11.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.11.m11.1b"><cn id="S4.SS0.SSS0.Px5.p1.11.m11.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.11.m11.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.11.m11.1c">55</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.11.m11.1d">55</annotation></semantics></math>GB audio, <math alttext="55" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.12.m12.1"><semantics id="S4.SS0.SSS0.Px5.p1.12.m12.1a"><mn id="S4.SS0.SSS0.Px5.p1.12.m12.1.1" xref="S4.SS0.SSS0.Px5.p1.12.m12.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.12.m12.1b"><cn id="S4.SS0.SSS0.Px5.p1.12.m12.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.12.m12.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.12.m12.1c">55</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.12.m12.1d">55</annotation></semantics></math>GB of images, and <math alttext="55" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.13.m13.1"><semantics id="S4.SS0.SSS0.Px5.p1.13.m13.1a"><mn id="S4.SS0.SSS0.Px5.p1.13.m13.1.1" xref="S4.SS0.SSS0.Px5.p1.13.m13.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.13.m13.1b"><cn id="S4.SS0.SSS0.Px5.p1.13.m13.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.13.m13.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.13.m13.1c">55</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.13.m13.1d">55</annotation></semantics></math>GB text.
By training our models on all seven training data mixtures, we can investigate <em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS0.Px5.p1.13.1">in-modality</em> and <em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS0.Px5.p1.13.2">out-of-modality</em> compression ratios.
For example, for a model trained on the text dataset, the in-modality compression ratio can be determined by evaluating on text, while audio or image data provide out-of-modality compression ratios.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px6">
<h4 class="ltx_title ltx_title_paragraph">Out-of-Distribution Evaluation Datasets</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px6.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px6.p1.1">To mimic the setting for which standard compression algorithms were developed (and thereby ensure a fair comparison), where the compressor is programmed with only minimal statistical assumptions about the data (with stronger assumptions for domain-specific compressors), we evaluate on unseen, out-of-distribution (OOD) datasets for each modality and not on in-distribution held-out datasets (as commonly done in machine learning).
To do so, we create a single OOD dataset of <math alttext="1" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px6.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px6.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px6.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px6.p1.1.m1.1b"><cn id="S4.SS0.SSS0.Px6.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px6.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px6.p1.1.m1.1d">1</annotation></semantics></math>GB for each modality (full details in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2" title="A.2 Out-of-Distribution Evaluation Data Sources ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">A.2</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present our extensive experimental evaluation (additional results in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2" title="Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">B</span></a>).
Unless otherwise noted, we report the best results over two hyperparameter sweeps (described in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="A.3 Sweeps ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">A.3</span></a>): (i) over the model- and dataset sizes, and (ii) over the model- and context sizes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="516" id="S5.F2.1.g1" src="https://arxiv.org/html/2410.05078v1/x2.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Small pre-trained transformers can be domain-general compressors (panels correspond to evaluation data mixtures, bars to training data mixtures).
On every out-of-distribution evaluation data mixture, our method (i.e., the bars) outperforms standard compression algorithms (all horizontal lines except for ‘Bellard’) and is on par with Bellard’s online adaptive transformers (the dark blue line) — as long as the evaluation modality was included in the training data mixture.
For unseen modalities we observe very little cross-modal transfer (which is different from observations made with foundation models <cite class="ltx_cite ltx_citemacro_cite">Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>).
Unimodal training leads to models that are good for their respective modality, but multimodal training yields models that perform almost as well as the unimodal models across all their training modalities (despite seeing a lot less data per modality than the unimodal models), i.e., one can trade off a small amount of performance on each individual modality in return for a strong domain-general compressor via multimodal training (gray bar).
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Small Transformers Can Be Domain-General Compressors</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> shows the best compression ratio attained on each of the seven out-of-distribution evaluation datasets when training a model on each of the seven training data mixtures (we report the best-performing model from our two sweeps for each training-evaluation pair).
We observe that transformers can achieve state-of-the-art in-modality compression ratios, regardless of the concrete composition of the training mixture, outperforming standard compression algorithms (even domain-specific ones) in all cases where all evaluation modalities are part of the training mixture.
In these cases, transformers thus learn the prototypical statistical patterns related to that modality during pre-training.
Importantly, by comparing models trained on unimodal vs.&nbsp;multimodal data, we observe that multimodal training only slightly decreases the compression performance compared to the unimodal models on their respective modalities (despite only having half or a third amount of data from that modality).
This means that it is possible to trade off a small amount of performance on each individual modality to obtain a very strong domain-general compressor via multimodal training (the gray bar in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="245" id="S5.F3.1.g1" src="https://arxiv.org/html/2410.05078v1/x3.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
What you see is what you get.
Each panel visualizes the compression ratios for one of our modalities when training models on varying dataset mixtures and sizes.
Although one can replace a large proportion of a unimodal training dataset with a multimodal training mixture and not incur a significant loss on the original modality, transformers (at our tested model sizes) do not exhibit improved transfer from the out-of-modality data (i.e., the multimodal models are worse than the unimodal ones, even when trained on much more data from that particular modality).
The upshot is that the multimodal training data does not hurt much (note the scale of the y-axis), but leads to significantly improved multimodal compression performance as shown in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">What You See Is What You Get</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.13">While <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> shows that substituting half or two thirds of the training set with data from other modalities only leads to a small performance loss compared to the unimodally trained models, it is unclear whether simply training on a smaller amount of unimodal data (i.e., decreasing the unimodal training dataset size to, e.g., <math alttext="82.5" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.1.m1.1b"><cn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.1.m1.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.1.m1.1d">82.5</annotation></semantics></math>GB and not substituting <math alttext="82.5" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px2.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.2.m2.1b"><cn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.2.m2.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.2.m2.1d">82.5</annotation></semantics></math>GB with data from another modality) would give the same performance, or whether there is some transfer between modalities (as suggested by <cite class="ltx_cite ltx_citemacro_citet">Mirchandani et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib48" title="">2023</a>)</cite>) that compensates for the smaller amount of data per individual modality.
To investigate this, we run an ablation where we subdivide each of our seven training sets into <math alttext="5" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S5.SS0.SSS0.Px2.p1.3.m3.1a"><mn id="S5.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.3.m3.1b"><cn id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.3.m3.1c">5</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.3.m3.1d">5</annotation></semantics></math> different sizes: <math alttext="20\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="S5.SS0.SSS0.Px2.p1.4.m4.1a"><mrow id="S5.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml">20</mn><mo id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.4.m4.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.4.m4.1d">20 %</annotation></semantics></math>, <math alttext="40\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.5.m5.1"><semantics id="S5.SS0.SSS0.Px2.p1.5.m5.1a"><mrow id="S5.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.5.m5.1.1.2" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml">40</mn><mo id="S5.SS0.SSS0.Px2.p1.5.m5.1.1.1" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.5.m5.1b"><apply id="S5.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.5.m5.1c">40\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.5.m5.1d">40 %</annotation></semantics></math>, <math alttext="60\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.6.m6.1"><semantics id="S5.SS0.SSS0.Px2.p1.6.m6.1a"><mrow id="S5.SS0.SSS0.Px2.p1.6.m6.1.1" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.6.m6.1.1.2" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1.2.cmml">60</mn><mo id="S5.SS0.SSS0.Px2.p1.6.m6.1.1.1" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.6.m6.1b"><apply id="S5.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.6.m6.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.6.m6.1c">60\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.6.m6.1d">60 %</annotation></semantics></math>, <math alttext="80\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.7.m7.1"><semantics id="S5.SS0.SSS0.Px2.p1.7.m7.1a"><mrow id="S5.SS0.SSS0.Px2.p1.7.m7.1.1" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.7.m7.1.1.2" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1.2.cmml">80</mn><mo id="S5.SS0.SSS0.Px2.p1.7.m7.1.1.1" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.7.m7.1b"><apply id="S5.SS0.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.7.m7.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.7.m7.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.7.m7.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.7.m7.1d">80 %</annotation></semantics></math>, and <math alttext="100\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.8.m8.1"><semantics id="S5.SS0.SSS0.Px2.p1.8.m8.1a"><mrow id="S5.SS0.SSS0.Px2.p1.8.m8.1.1" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.8.m8.1.1.2" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1.2.cmml">100</mn><mo id="S5.SS0.SSS0.Px2.p1.8.m8.1.1.1" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.8.m8.1b"><apply id="S5.SS0.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.8.m8.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.8.m8.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.8.m8.1c">100\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.8.m8.1d">100 %</annotation></semantics></math> of the respective dataset (uniformly subsampled).
We train a series of models (sweeping over their number of layers; see <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="A.3 Sweeps ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">A.3</span></a>) on each dataset mixture and each dataset size, and then evaluate as before.
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F3" title="In Small Transformers Can Be Domain-General Compressors ‣ 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> shows that, for our models and datasets, there is little transfer between modalities.
For all cases of audio, text, and (less clearly) images, it is better to train on a smaller unimodal dataset to get the best unimodal performance, as opposed to training on a much larger multimodal dataset.
For example, training on a pure text dataset of <math alttext="33" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.9.m9.1"><semantics id="S5.SS0.SSS0.Px2.p1.9.m9.1a"><mn id="S5.SS0.SSS0.Px2.p1.9.m9.1.1" xref="S5.SS0.SSS0.Px2.p1.9.m9.1.1.cmml">33</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.9.m9.1b"><cn id="S5.SS0.SSS0.Px2.p1.9.m9.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.9.m9.1.1">33</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.9.m9.1c">33</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.9.m9.1d">33</annotation></semantics></math>GB (<math alttext="20\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.10.m10.1"><semantics id="S5.SS0.SSS0.Px2.p1.10.m10.1a"><mrow id="S5.SS0.SSS0.Px2.p1.10.m10.1.1" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.10.m10.1.1.2" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1.2.cmml">20</mn><mo id="S5.SS0.SSS0.Px2.p1.10.m10.1.1.1" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.10.m10.1b"><apply id="S5.SS0.SSS0.Px2.p1.10.m10.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.10.m10.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.10.m10.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.10.m10.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.10.m10.1d">20 %</annotation></semantics></math> of <math alttext="165" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.11.m11.1"><semantics id="S5.SS0.SSS0.Px2.p1.11.m11.1a"><mn id="S5.SS0.SSS0.Px2.p1.11.m11.1.1" xref="S5.SS0.SSS0.Px2.p1.11.m11.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.11.m11.1b"><cn id="S5.SS0.SSS0.Px2.p1.11.m11.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.11.m11.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.11.m11.1c">165</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.11.m11.1d">165</annotation></semantics></math>GB) outperforms training on a dataset consisting of <math alttext="82.5" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.12.m12.1"><semantics id="S5.SS0.SSS0.Px2.p1.12.m12.1a"><mn id="S5.SS0.SSS0.Px2.p1.12.m12.1.1" xref="S5.SS0.SSS0.Px2.p1.12.m12.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.12.m12.1b"><cn id="S5.SS0.SSS0.Px2.p1.12.m12.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px2.p1.12.m12.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.12.m12.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.12.m12.1d">82.5</annotation></semantics></math>GB (i.e., more than twice as much) text and of <math alttext="82.5" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.13.m13.1"><semantics id="S5.SS0.SSS0.Px2.p1.13.m13.1a"><mn id="S5.SS0.SSS0.Px2.p1.13.m13.1.1" xref="S5.SS0.SSS0.Px2.p1.13.m13.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.13.m13.1b"><cn id="S5.SS0.SSS0.Px2.p1.13.m13.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px2.p1.13.m13.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.13.m13.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.13.m13.1d">82.5</annotation></semantics></math>GB images/audio.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S5.F4.1.g1" src="https://arxiv.org/html/2410.05078v1/x4.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Simultaneously scaling training dataset and model size (for unimodal training- and evaluation data).
The colors indicate the model size, and the lines correspond to different dataset sizes (<math alttext="20\%" class="ltx_Math" display="inline" id="S5.F4.8.m1.1"><semantics id="S5.F4.8.m1.1b"><mrow id="S5.F4.8.m1.1.1" xref="S5.F4.8.m1.1.1.cmml"><mn id="S5.F4.8.m1.1.1.2" xref="S5.F4.8.m1.1.1.2.cmml">20</mn><mo id="S5.F4.8.m1.1.1.1" xref="S5.F4.8.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.8.m1.1c"><apply id="S5.F4.8.m1.1.1.cmml" xref="S5.F4.8.m1.1.1"><csymbol cd="latexml" id="S5.F4.8.m1.1.1.1.cmml" xref="S5.F4.8.m1.1.1.1">percent</csymbol><cn id="S5.F4.8.m1.1.1.2.cmml" type="integer" xref="S5.F4.8.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.8.m1.1d">20\%</annotation><annotation encoding="application/x-llamapun" id="S5.F4.8.m1.1e">20 %</annotation></semantics></math>, <math alttext="40\%" class="ltx_Math" display="inline" id="S5.F4.9.m2.1"><semantics id="S5.F4.9.m2.1b"><mrow id="S5.F4.9.m2.1.1" xref="S5.F4.9.m2.1.1.cmml"><mn id="S5.F4.9.m2.1.1.2" xref="S5.F4.9.m2.1.1.2.cmml">40</mn><mo id="S5.F4.9.m2.1.1.1" xref="S5.F4.9.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.9.m2.1c"><apply id="S5.F4.9.m2.1.1.cmml" xref="S5.F4.9.m2.1.1"><csymbol cd="latexml" id="S5.F4.9.m2.1.1.1.cmml" xref="S5.F4.9.m2.1.1.1">percent</csymbol><cn id="S5.F4.9.m2.1.1.2.cmml" type="integer" xref="S5.F4.9.m2.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.9.m2.1d">40\%</annotation><annotation encoding="application/x-llamapun" id="S5.F4.9.m2.1e">40 %</annotation></semantics></math>, <math alttext="60\%" class="ltx_Math" display="inline" id="S5.F4.10.m3.1"><semantics id="S5.F4.10.m3.1b"><mrow id="S5.F4.10.m3.1.1" xref="S5.F4.10.m3.1.1.cmml"><mn id="S5.F4.10.m3.1.1.2" xref="S5.F4.10.m3.1.1.2.cmml">60</mn><mo id="S5.F4.10.m3.1.1.1" xref="S5.F4.10.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.10.m3.1c"><apply id="S5.F4.10.m3.1.1.cmml" xref="S5.F4.10.m3.1.1"><csymbol cd="latexml" id="S5.F4.10.m3.1.1.1.cmml" xref="S5.F4.10.m3.1.1.1">percent</csymbol><cn id="S5.F4.10.m3.1.1.2.cmml" type="integer" xref="S5.F4.10.m3.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.10.m3.1d">60\%</annotation><annotation encoding="application/x-llamapun" id="S5.F4.10.m3.1e">60 %</annotation></semantics></math>, <math alttext="80\%" class="ltx_Math" display="inline" id="S5.F4.11.m4.1"><semantics id="S5.F4.11.m4.1b"><mrow id="S5.F4.11.m4.1.1" xref="S5.F4.11.m4.1.1.cmml"><mn id="S5.F4.11.m4.1.1.2" xref="S5.F4.11.m4.1.1.2.cmml">80</mn><mo id="S5.F4.11.m4.1.1.1" xref="S5.F4.11.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.11.m4.1c"><apply id="S5.F4.11.m4.1.1.cmml" xref="S5.F4.11.m4.1.1"><csymbol cd="latexml" id="S5.F4.11.m4.1.1.1.cmml" xref="S5.F4.11.m4.1.1.1">percent</csymbol><cn id="S5.F4.11.m4.1.1.2.cmml" type="integer" xref="S5.F4.11.m4.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.11.m4.1d">80\%</annotation><annotation encoding="application/x-llamapun" id="S5.F4.11.m4.1e">80 %</annotation></semantics></math>, and <math alttext="100\%" class="ltx_Math" display="inline" id="S5.F4.12.m5.1"><semantics id="S5.F4.12.m5.1b"><mrow id="S5.F4.12.m5.1.1" xref="S5.F4.12.m5.1.1.cmml"><mn id="S5.F4.12.m5.1.1.2" xref="S5.F4.12.m5.1.1.2.cmml">100</mn><mo id="S5.F4.12.m5.1.1.1" xref="S5.F4.12.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.12.m5.1c"><apply id="S5.F4.12.m5.1.1.cmml" xref="S5.F4.12.m5.1.1"><csymbol cd="latexml" id="S5.F4.12.m5.1.1.1.cmml" xref="S5.F4.12.m5.1.1.1">percent</csymbol><cn id="S5.F4.12.m5.1.1.2.cmml" type="integer" xref="S5.F4.12.m5.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.12.m5.1d">100\%</annotation><annotation encoding="application/x-llamapun" id="S5.F4.12.m5.1e">100 %</annotation></semantics></math>).
We always train for <math alttext="2" class="ltx_Math" display="inline" id="S5.F4.13.m6.1"><semantics id="S5.F4.13.m6.1b"><mn id="S5.F4.13.m6.1.1" xref="S5.F4.13.m6.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.F4.13.m6.1c"><cn id="S5.F4.13.m6.1.1.cmml" type="integer" xref="S5.F4.13.m6.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.13.m6.1d">2</annotation><annotation encoding="application/x-llamapun" id="S5.F4.13.m6.1e">2</annotation></semantics></math> epochs, regardless of dataset size, i.e., smaller datasets require fewer FLOPS.
As expected, increasing the number of parameters and the dataset size boosts compression (at the cost of increased training FLOPS).
Note that our out-of-distribution evaluation makes models more prone to overfitting, as seen, e.g., for our largest models on images, making scaling more complex than traditionally observed LLM scaling laws.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Scaling Analysis</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">Since there is a non-trivial relationship between model size and dataset size, we perform a scaling analysis on both of these factors (details in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="A.3 Sweeps ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">A.3</span></a>).
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get ‣ 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a> shows trends akin to the scaling laws observed for LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kaplan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib28" title="">2020</a>)</cite>, which state that better prediction (in our case compression) is only possible by scaling both models and datasets, in a particular way.
Note that, different to traditional scaling laws for models trained on internet-scale datasets, the distribution shift in our evaluation makes it easier for the model to overfit to the training distribution.
However, as the number of parameters and the training flops of our small models increase, the adjusted compression ratio improves, eventually beating standard compression algorithms.
We do observe gradual overfitting on the image dataset for our models trained only on images.
However, this phenomenon can be mitigated by including other modalities in the training mixture (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.F1" title="In B.4 Scaling Analysis for Multimodal Training ‣ Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">A1</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Model Size vs.&nbsp;Context Size</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="257" id="S5.F5.1.g1" src="https://arxiv.org/html/2410.05078v1/x5.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Relationship between context- and model size.
Given a certain training compute budget (in FLOPS), one can either increase the context size (measured in bytes) or the model size, leading to a non-trivial trade-off.
Our results show that this trade-off is highly modality-dependent (also note the different scales on the y-axis, meaning that the magnitude of the effect varies significantly with modality).
For text, shorter context sizes and larger models are beneficial (indicating the importance of short-term dependencies for our data and model scale).
For images, larger context is generally beneficial, which makes sense, given that a single image consists of <math alttext="512\cdot 512\cdot 3=786432" class="ltx_Math" display="inline" id="S5.F5.3.m1.1"><semantics id="S5.F5.3.m1.1b"><mrow id="S5.F5.3.m1.1.1" xref="S5.F5.3.m1.1.1.cmml"><mrow id="S5.F5.3.m1.1.1.2" xref="S5.F5.3.m1.1.1.2.cmml"><mn id="S5.F5.3.m1.1.1.2.2" xref="S5.F5.3.m1.1.1.2.2.cmml">512</mn><mo id="S5.F5.3.m1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S5.F5.3.m1.1.1.2.1.cmml">⋅</mo><mn id="S5.F5.3.m1.1.1.2.3" xref="S5.F5.3.m1.1.1.2.3.cmml">512</mn><mo id="S5.F5.3.m1.1.1.2.1b" lspace="0.222em" rspace="0.222em" xref="S5.F5.3.m1.1.1.2.1.cmml">⋅</mo><mn id="S5.F5.3.m1.1.1.2.4" xref="S5.F5.3.m1.1.1.2.4.cmml">3</mn></mrow><mo id="S5.F5.3.m1.1.1.1" xref="S5.F5.3.m1.1.1.1.cmml">=</mo><mn id="S5.F5.3.m1.1.1.3" xref="S5.F5.3.m1.1.1.3.cmml">786432</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F5.3.m1.1c"><apply id="S5.F5.3.m1.1.1.cmml" xref="S5.F5.3.m1.1.1"><eq id="S5.F5.3.m1.1.1.1.cmml" xref="S5.F5.3.m1.1.1.1"></eq><apply id="S5.F5.3.m1.1.1.2.cmml" xref="S5.F5.3.m1.1.1.2"><ci id="S5.F5.3.m1.1.1.2.1.cmml" xref="S5.F5.3.m1.1.1.2.1">⋅</ci><cn id="S5.F5.3.m1.1.1.2.2.cmml" type="integer" xref="S5.F5.3.m1.1.1.2.2">512</cn><cn id="S5.F5.3.m1.1.1.2.3.cmml" type="integer" xref="S5.F5.3.m1.1.1.2.3">512</cn><cn id="S5.F5.3.m1.1.1.2.4.cmml" type="integer" xref="S5.F5.3.m1.1.1.2.4">3</cn></apply><cn id="S5.F5.3.m1.1.1.3.cmml" type="integer" xref="S5.F5.3.m1.1.1.3">786432</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.3.m1.1d">512\cdot 512\cdot 3=786432</annotation><annotation encoding="application/x-llamapun" id="S5.F5.3.m1.1e">512 ⋅ 512 ⋅ 3 = 786432</annotation></semantics></math> bytes, which far exceeds our models’ contexts, i.e., models with larger context can process larger fractions of an image.
Finally, for audio data the relationship is complex with intermediate context length and larger models performing better (though the reverse is true for short context length).
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.2">The previous two experiments investigated the impact of training dataset size and model size, which revealed a complex, “scaling law”-like, relationship between the two factors and the overall training budget in FLOPS.
In this experiment, we investigate the impact of the length of the context window.
Since the context window length has a large impact on the overall FLOPS footprint (attention scales quadratically with the input sequence length), we also vary the size of our models to explore whether there is a sweet spot in terms of training compute budget allocation (details in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="A.3 Sweeps ‣ Appendix A Experimental Details ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">A.3</span></a>).
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F5" title="In Model Size vs. Context Size ‣ 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a> shows that the optimal trade-off strongly depends on the data modality.
The top performing models for text have a context window less than or equal to <math alttext="2048" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px4.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px4.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.1.m1.1b"><cn id="S5.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.1.m1.1c">2048</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px4.p1.1.m1.1d">2048</annotation></semantics></math> bytes, indicating that short term dependencies are more important than long ones in this case.
For images, the best compromise overall is to choose a larger context window of <math alttext="8192" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px4.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px4.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px4.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px4.p1.2.m2.1.1.cmml">8192</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.2.m2.1b"><cn id="S5.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px4.p1.2.m2.1.1">8192</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.2.m2.1c">8192</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px4.p1.2.m2.1d">8192</annotation></semantics></math>, which means decreasing the model size.
For audio data, the trade-off is even more complex.
Overall these results highlight the difficulty of tuning architectures to achieve best performance across many modalities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="329" id="S5.F6.1.g1" src="https://arxiv.org/html/2410.05078v1/x6.png" width="332">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
Impact of the sliding window overlap (for unimodal training and evaluation).
Having overlapping context windows only leads to marginal performance increases (most significant for images) in our experiments but comes at a huge cost in terms of computational efficiency.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Sliding Window</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px5.p1.5">In all experiments so far we used a sliding window without overlap to process the evaluation byte streams, i.e., we completely fill a whole context window, process it, and then slide it forward by the size of the context window to process the next chunk of data.
This means that bytes early in the context window are not conditioned on a lot of data (in theory, conditioning on more data should help with prediction and thus compression, which may well be exploitable by transformers’ in-context learning abilities&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Genewein et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib18" title="">2023</a>; Ge et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib17" title="">2024</a>)</cite>).
However, sliding the context window with more overlap requires more forward passes to process the same amount of data, which significantly increases the computational cost with increasing overlap.
With no overlap processing <math alttext="4096" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px5.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px5.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.1.m1.1b"><cn id="S5.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.1.m1.1c">4096</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px5.p1.1.m1.1d">4096</annotation></semantics></math> bytes with a context window of <math alttext="4096" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px5.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px5.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px5.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px5.p1.2.m2.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.2.m2.1b"><cn id="S5.SS0.SSS0.Px5.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px5.p1.2.m2.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.2.m2.1c">4096</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px5.p1.2.m2.1d">4096</annotation></semantics></math> takes a single forward pass.
In the most extreme case of maximal overlap it would take <math alttext="4095" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px5.p1.3.m3.1"><semantics id="S5.SS0.SSS0.Px5.p1.3.m3.1a"><mn id="S5.SS0.SSS0.Px5.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px5.p1.3.m3.1.1.cmml">4095</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.3.m3.1b"><cn id="S5.SS0.SSS0.Px5.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px5.p1.3.m3.1.1">4095</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.3.m3.1c">4095</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px5.p1.3.m3.1d">4095</annotation></semantics></math> forward passes, where the context window is moved by a single byte each time (though each prediction could be conditioned on the full <math alttext="4095" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px5.p1.4.m4.1"><semantics id="S5.SS0.SSS0.Px5.p1.4.m4.1a"><mn id="S5.SS0.SSS0.Px5.p1.4.m4.1.1" xref="S5.SS0.SSS0.Px5.p1.4.m4.1.1.cmml">4095</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.4.m4.1b"><cn id="S5.SS0.SSS0.Px5.p1.4.m4.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px5.p1.4.m4.1.1">4095</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.4.m4.1c">4095</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px5.p1.4.m4.1d">4095</annotation></semantics></math> preceding bytes).
In our final experiment, we investigate the effect of different overlaps between context windows.
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F6" title="In Model Size vs. Context Size ‣ 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">6</span></a> shows that for our data and model sizes, increasing the overlap window (for a context length of 4096) has relatively little effect.
The strongest effect is observed for image data, which makes sense given that 4096 bytes only corresponds to a small fraction of an image and there are obvious long-range dependencies between channels of the same image.
Beyond an overlap of <math alttext="2048" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px5.p1.5.m5.1"><semantics id="S5.SS0.SSS0.Px5.p1.5.m5.1a"><mn id="S5.SS0.SSS0.Px5.p1.5.m5.1.1" xref="S5.SS0.SSS0.Px5.p1.5.m5.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.5.m5.1b"><cn id="S5.SS0.SSS0.Px5.p1.5.m5.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px5.p1.5.m5.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.5.m5.1c">2048</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px5.p1.5.m5.1d">2048</annotation></semantics></math> we do not see much benefit of further increasing the overlap window in our experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.2">The main goal of our work is to investigate whether pre-trained transformers can be competitive with standard compressors, even when taking their parameter size into account.
In contrast to previous work, this places our models into a relatively small regime, where it is unclear whether models will learn well from large datasets at all and have non-trivial out-of-distribution and cross-modality transfer.
This could partly be countered by training larger models and then subsequently compressing the model parameters themselves.
We chose not to do this in our case since naive lossless compression of model parameters leads to a <math alttext="10\%" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mn id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">10</mn><mo id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><csymbol cd="latexml" id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1">percent</csymbol><cn id="S6.p1.1.m1.1.1.2.cmml" type="integer" xref="S6.p1.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">10 %</annotation></semantics></math> reduction at best (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.T3" title="In B.3 Compressing Model Parameters ‣ Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">A3</span></a>), and even best-case scenarios would only lead to marginal improvements in compression ratio given the size of our largest models.
For very large (e.g., foundation) models, compressing weights to achieve competitive compression ratios may be interesting, though it will be necessary to use lossy weight compression techniques&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Tao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib70" title="">2022</a>)</cite>, which lead to non-trivial trade-offs between high (lossy) compression and maintaining strong predictor performance, i.e., the two summands in the numerator of <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.E2" title="In Evaluation ‣ 4 Methods ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Eq.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>.
Exploring these trade-offs is an interesting direction for future research but beyond the scope of our work.
Another way to allow for larger models would be to simply evaluate on a larger test set.
We deliberately chose to use <math alttext="1" class="ltx_Math" display="inline" id="S6.p1.2.m2.1"><semantics id="S6.p1.2.m2.1a"><mn id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><cn id="S6.p1.2.m2.1.1.cmml" type="integer" xref="S6.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="S6.p1.2.m2.1d">1</annotation></semantics></math>GB of test data as a regime where standard compression algorithms are hard to beat.
Additionally, evaluations on larger test data, and in settings where model parameters are not taken into account have previously conducted&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Delétang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>; Valmeekam et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib74" title="">2023</a>; Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib38" title="">2024</a>)</cite> (where significant amounts of cross-domain transfer have also been found, unlike in our experiments).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Note that, similar to <cite class="ltx_cite ltx_citemacro_citet">Xue et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib82" title="">2022</a>)</cite>, we do not use a tokenizer, which has two reasons.
First, tokenizers are typically pre-trained per modality, and we want to rule out bad cross-modality transfer resulting from a bad tokenizer.
Second, tokenization acts as a pre-trained “pre-compression” step (<cite class="ltx_cite ltx_citemacro_citet">Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite> make a similar comment).
This pre-compression increases information density in the context window at the cost of increasing entropy, which can make the prediction problem harder: <cite class="ltx_cite ltx_citemacro_citet">Lester et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib36" title="">2024</a>)</cite> even show that when using a strong neural-based pre-compressor (together with arithmetic coding) to train LLMs, training performance can collapse catastrophically.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Limitations</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">All our claims regarding the universality of our compressors (or the lack thereof) are limited to the model size regime and the particular modalities and datasets we studied.
We cannot rule out that there are cases where even in-modality transfer is weak (e.g., when using another out-of-disribution image evaluation dataset with very different statistics), or that there may be cases of non-trivial cross-modal transfer (which we have not observed).
Similarly, our claims regarding outperforming standard compression algorithms are limited to our experiments. We cannot rule out that there are datasets (such as spreadsheet data, or code, which, technically, are both text) where no pre-trained transformer outperforms, e.g., LZMA2 (in fact, we think its plausible that such datasets can be constructed synthetically).
Finally, note that the goal of our study is not to build a practical transformer-based universal compressor to compete with standard compressors in terms of computational footprint.
As <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.T2" title="In B.2 Running Times ‣ Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">A2</span></a> shows, our models are orders of magnitude slower for encoding data (and have significantly larger memory- and FLOPS-demands), and they are about three times slower than Bellard’s online adaptive transformer.
This is only the forward-pass cost, which can be done for a whole context window at once (without overlap).
If our models were used do decode, which has to be performed token-by-token to obtain the correct conditioning, our running time demands would be even worse, making our models clearly uncompetitive in that sense.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S7" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper we have shown that it is possible to use pre-trained vanilla transformers as competitive “zero-shot” compressors on out-of-distribution evaluation data, where competitive means achieving better compression ratios than both domain-general and domain-specific standard compression algorithms.
We found this to be true for text, images, and audio data, and for all possible combinations of the three — but only as long as the corresponding modalities have been seen during training.
We further found that, despite their relatively small size, our models have the capacity to train on multiple modalities, and then compress these well, without losing much performance compared to a purely unimodal model.
On the other hand, we found that even multimodal training does not lead to the emergence of a universal compression ability that would yield strong compression performance on unseen modalities.
This is in contrast to observations made by <cite class="ltx_cite ltx_citemacro_citet">Delétang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite> on LLMs and indicates that there is a qualitative difference between small and (very) large models, even when the small models are trained on large amounts of data.
Overall our results suggest that small transformers can be pre-trained to recognize and exploit statistical regularities on par and even better than hand-crafted standard compressors and current state-of-the-art adaptive online neural compressors, but we do not observe the emergence of a general compression ability with our model sizes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1" lang="en">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank
Benjamin Beyret,
Emilien Dupont,
Daniele Calandriello,
Grégoire Delétang,
Jordi Grau-Moya,
Li Kevin Wenliang,
Laurent Orseau,
Marcus Hutter,
Matthew Aitchison,
Sarath Chandar,
Satinder Baveja,
Theophane Weber,
Zhengdong Wang,
and Zoubin Ghahramani
for their helpful feedback and insightful discussions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ardila et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Ardila, M.&nbsp;Branson, K.&nbsp;Davis, M.&nbsp;Kohler, J.&nbsp;Meyer, M.&nbsp;Henretty, R.&nbsp;Morais,
L.&nbsp;Saunders, F.&nbsp;M. Tyers, and G.&nbsp;Weber.

</span>
<span class="ltx_bibblock">Common voice: A massively-multilingual speech corpus.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">LREC</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barzen et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;L.&nbsp;C. Barzen, F.&nbsp;Glazov, J.&nbsp;Geistert, and T.&nbsp;Sikora.

</span>
<span class="ltx_bibblock">Accelerated deep lossless image coding with unified paralleleized
GPU coding architecture.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">PCS</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bellard (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Bellard.

</span>
<span class="ltx_bibblock">Lossless data compression with neural networks.

</span>
<span class="ltx_bibblock">Technical report, Amarisoft, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bellard (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Bellard.

</span>
<span class="ltx_bibblock">NNCP v2: Lossless data compression with transformer.

</span>
<span class="ltx_bibblock">Technical report, Amarisoft, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boutell (1997)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Boutell.

</span>
<span class="ltx_bibblock">PNG (portable network graphics) specification version 1.0.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">RFC</em>, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaitin (2006)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;J. Chaitin.

</span>
<span class="ltx_bibblock">The limits of reason.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Scientific American</em>, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Chen, B.&nbsp;He, H.&nbsp;Wang, Y.&nbsp;Ren, S.&nbsp;Lim, and A.&nbsp;Shrivastava.

</span>
<span class="ltx_bibblock">Nerv: Neural representations for videos.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">NeurIPS</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coalson (2008)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Coalson.

</span>
<span class="ltx_bibblock">Free lossless audio codec, 2008.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://xiph.org/flac" title="">https://xiph.org/flac</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohan et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Cohan, F.&nbsp;Dernoncourt, D.&nbsp;S. Kim, T.&nbsp;Bui, S.&nbsp;Kim, W.&nbsp;Chang, and N.&nbsp;Goharian.

</span>
<span class="ltx_bibblock">A discourse-aware attention model for abstractive summarization of
long documents.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">NAACL-HLT (2)</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cox (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Cox.

</span>
<span class="ltx_bibblock">Syntactically informed text compression with recurrent neural
networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv:1608.02893</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Delétang et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Delétang, A.&nbsp;Ruoss, P.&nbsp;Duquenne, E.&nbsp;Catt, T.&nbsp;Genewein, C.&nbsp;Mattern,
J.&nbsp;Grau-Moya, L.&nbsp;K. Wenliang, M.&nbsp;Aitchison, L.&nbsp;Orseau, M.&nbsp;Hutter, and
J.&nbsp;Veness.

</span>
<span class="ltx_bibblock">Language modeling is compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">ICLR</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deutsch (1996)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Deutsch.

</span>
<span class="ltx_bibblock">GZIP file format specification version 4.3.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">RFC</em>, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Dubey, A.&nbsp;Jauhri, A.&nbsp;Pandey, A.&nbsp;Kadian, A.&nbsp;Al-Dahle, A.&nbsp;Letman, A.&nbsp;Mathur,
A.&nbsp;Schelten, A.&nbsp;Yang, A.&nbsp;Fan, A.&nbsp;Goyal, A.&nbsp;Hartshorn, A.&nbsp;Yang, A.&nbsp;Mitra,
A.&nbsp;Sravankumar, A.&nbsp;Korenev, A.&nbsp;Hinsvark, A.&nbsp;Rao, A.&nbsp;Zhang, A.&nbsp;Rodriguez,
A.&nbsp;Gregerson, A.&nbsp;Spataru, B.&nbsp;Rozière, B.&nbsp;Biron, B.&nbsp;Tang, B.&nbsp;Chern,
C.&nbsp;Caucheteux, C.&nbsp;Nayak, C.&nbsp;Bi, C.&nbsp;Marra, C.&nbsp;McConnell, C.&nbsp;Keller, C.&nbsp;Touret,
C.&nbsp;Wu, C.&nbsp;Wong, C.&nbsp;C. Ferrer, C.&nbsp;Nikolaidis, D.&nbsp;Allonsius, D.&nbsp;Song, D.&nbsp;Pintz,
D.&nbsp;Livshits, D.&nbsp;Esiobu, D.&nbsp;Choudhary, D.&nbsp;Mahajan, D.&nbsp;Garcia-Olano,
D.&nbsp;Perino, D.&nbsp;Hupkes, E.&nbsp;Lakomkin, E.&nbsp;AlBadawy, E.&nbsp;Lobanova, E.&nbsp;Dinan, E.&nbsp;M.
Smith, F.&nbsp;Radenovic, F.&nbsp;Zhang, G.&nbsp;Synnaeve, G.&nbsp;Lee, G.&nbsp;L. Anderson, G.&nbsp;Nail,
G.&nbsp;Mialon, G.&nbsp;Pang, G.&nbsp;Cucurell, H.&nbsp;Nguyen, H.&nbsp;Korevaar, H.&nbsp;Xu, H.&nbsp;Touvron,
I.&nbsp;Zarov, I.&nbsp;A. Ibarra, I.&nbsp;M. Kloumann, I.&nbsp;Misra, I.&nbsp;Evtimov, J.&nbsp;Copet,
J.&nbsp;Lee, J.&nbsp;Geffert, J.&nbsp;Vranes, J.&nbsp;Park, J.&nbsp;Mahadeokar, J.&nbsp;Shah, J.&nbsp;van&nbsp;der
Linde, J.&nbsp;Billock, J.&nbsp;Hong, J.&nbsp;Lee, J.&nbsp;Fu, J.&nbsp;Chi, J.&nbsp;Huang, J.&nbsp;Liu, J.&nbsp;Wang,
J.&nbsp;Yu, J.&nbsp;Bitton, J.&nbsp;Spisak, J.&nbsp;Park, J.&nbsp;Rocca, J.&nbsp;Johnstun, J.&nbsp;Saxe, J.&nbsp;Jia,
K.&nbsp;V. Alwala, K.&nbsp;Upasani, K.&nbsp;Plawiak, K.&nbsp;Li, K.&nbsp;Heafield, K.&nbsp;Stone, and
et&nbsp;al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv:2407.21783</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duda (2009)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Duda.

</span>
<span class="ltx_bibblock">Asymmetric numeral systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv:0902.0271</em>, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dupont et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Dupont, A.&nbsp;Golinski, M.&nbsp;Alizadeh, Y.&nbsp;W. Teh, and A.&nbsp;Doucet.

</span>
<span class="ltx_bibblock">COIN: compression with implicit neural representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv:2103.03123</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dupont et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Dupont, H.&nbsp;Loya, M.&nbsp;Alizadeh, A.&nbsp;Golinski, Y.&nbsp;W. Teh, and A.&nbsp;Doucet.

</span>
<span class="ltx_bibblock">COIN++: neural compression across modalities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Trans. Mach. Learn. Res.</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Ge, J.&nbsp;Hu, L.&nbsp;Wang, X.&nbsp;Wang, S.&nbsp;Chen, and F.&nbsp;Wei.

</span>
<span class="ltx_bibblock">In-context autoencoder for context compression in a large language
model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ICLR</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Genewein et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Genewein, G.&nbsp;Delétang, A.&nbsp;Ruoss, L.&nbsp;K. Wenliang, E.&nbsp;Catt,
V.&nbsp;Dutordoir, J.&nbsp;Grau-Moya, L.&nbsp;Orseau, M.&nbsp;Hutter, and J.&nbsp;Veness.

</span>
<span class="ltx_bibblock">Memory-based meta-learning on non-stationary distributions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ICML</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Goyal, K.&nbsp;Tatwawadi, S.&nbsp;Chandak, and I.&nbsp;Ochoa.

</span>
<span class="ltx_bibblock">Deepzip: Lossless data compression using recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">DCC</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Goyal, K.&nbsp;Tatwawadi, S.&nbsp;Chandak, and I.&nbsp;Ochoa.

</span>
<span class="ltx_bibblock">Dzip: Improved general-purpose lossless compression based on novel
neural network modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">DCC</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grau-Moya et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Grau-Moya, T.&nbsp;Genewein, M.&nbsp;Hutter, L.&nbsp;Orseau, G.&nbsp;Delétang, E.&nbsp;Catt,
A.&nbsp;Ruoss, L.&nbsp;K. Wenliang, C.&nbsp;Mattern, M.&nbsp;Aitchison, and J.&nbsp;Veness.

</span>
<span class="ltx_bibblock">Learning universal predictors.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ICML</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Hoffmann, S.&nbsp;Borgeaud, A.&nbsp;Mensch, E.&nbsp;Buchatskaya, T.&nbsp;Cai, E.&nbsp;Rutherford,
D.&nbsp;de&nbsp;Las&nbsp;Casas, L.&nbsp;A. Hendricks, J.&nbsp;Welbl, A.&nbsp;Clark, T.&nbsp;Hennigan, E.&nbsp;Noland,
K.&nbsp;Millican, G.&nbsp;van&nbsp;den Driessche, B.&nbsp;Damoc, A.&nbsp;Guy, S.&nbsp;Osindero,
K.&nbsp;Simonyan, E.&nbsp;Elsen, J.&nbsp;W. Rae, O.&nbsp;Vinyals, and L.&nbsp;Sifre.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arxiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoogeboom et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Hoogeboom, J.&nbsp;W.&nbsp;T. Peters, R.&nbsp;van&nbsp;den Berg, and M.&nbsp;Welling.

</span>
<span class="ltx_bibblock">Integer discrete flows and lossless compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">NeurIPS</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Huang, J.&nbsp;Zhang, Z.&nbsp;Shan, and J.&nbsp;He.

</span>
<span class="ltx_bibblock">Compression represents intelligence linearly.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv:2404.09937</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huffman (1952)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;A. Huffman.

</span>
<span class="ltx_bibblock">A method for the construction of minimum-redundancy codes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IRE</em>, 1952.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutter (2006)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Hutter.

</span>
<span class="ltx_bibblock">500’000€&nbsp;prize for compressing human knowledge, 2006.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://prize.hutter1.net/" title="">http://prize.hutter1.net</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutter et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Hutter, D.&nbsp;Quarel, and E.&nbsp;Catt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">An Introduction to Universal Artificial Intelligence</em>.

</span>
<span class="ltx_bibblock">Chapman &amp; Hall, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Kaplan, S.&nbsp;McCandlish, T.&nbsp;Henighan, T.&nbsp;B. Brown, B.&nbsp;Chess, R.&nbsp;Child,
S.&nbsp;Gray, A.&nbsp;Radford, J.&nbsp;Wu, and D.&nbsp;Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Kim, M.&nbsp;Bauer, L.&nbsp;Theis, J.&nbsp;R. Schwarz, and E.&nbsp;Dupont.

</span>
<span class="ltx_bibblock">C3: high-performance and low-complexity neural compression from a
single image or video.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv:2312.02753</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;P. Kingma and J.&nbsp;Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">ICLR (Poster)</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;H. Kingma, P.&nbsp;Abbeel, and J.&nbsp;Ho.

</span>
<span class="ltx_bibblock">Bit-swap: Recursive bits-back coding for lossless compression with
hierarchical latent variables.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">ICML</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Knoll (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Knoll.

</span>
<span class="ltx_bibblock">CMIX, 2014.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.byronknoll.com/cmix.html" title="">http://www.byronknoll.com/cmix.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ladune et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Ladune, P.&nbsp;Philippe, F.&nbsp;Henry, G.&nbsp;Clare, and T.&nbsp;Leguay.

</span>
<span class="ltx_bibblock">COOL-CHIC: coordinate-based low complexity hierarchical image
codec.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">ICCV</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Legg and Hutter (2007)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Legg and M.&nbsp;Hutter.

</span>
<span class="ltx_bibblock">Universal intelligence: A definition of machine intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Minds Mach.</em>, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lehtokangas et&nbsp;al. (1993)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Lehtokangas, J.&nbsp;Saarinen, P.&nbsp;Huuhtanen, and K.&nbsp;Kaski.

</span>
<span class="ltx_bibblock">Neural network optimization tool based on predictive MDL principle
for time series prediction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">ICTAI</em>, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Lester, J.&nbsp;Lee, A.&nbsp;Alemi, J.&nbsp;Pennington, A.&nbsp;Roberts, J.&nbsp;Sohl-Dickstein,
and N.&nbsp;Constant.

</span>
<span class="ltx_bibblock">Training llms over neurally compressed text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arxiv:2404.03626</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Vitányi (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Li and P.&nbsp;M.&nbsp;B. Vitányi.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">An Introduction to Kolmogorov Complexity and Its Applications,
4th Edition</em>.

</span>
<span class="ltx_bibblock">Springer, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Li, C.&nbsp;Huang, X.&nbsp;Wang, H.&nbsp;Hu, C.&nbsp;Wyeth, D.&nbsp;Bu, Q.&nbsp;Yu, W.&nbsp;Gao, X.&nbsp;Liu, and
M.&nbsp;Li.

</span>
<span class="ltx_bibblock">Understanding is compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arxiv:2407.07723</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Q.&nbsp;Liu, Y.&nbsp;Xu, and Z.&nbsp;Li.

</span>
<span class="ltx_bibblock">DecMac: A deep context model for high efficiency arithmetic
coding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">ICAIIC</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Liu, P.&nbsp;Luo, X.&nbsp;Wang, and X.&nbsp;Tang.

</span>
<span class="ltx_bibblock">Deep learning face attributes in the wild.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">ICCV</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MacKay (2003)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;J.&nbsp;C. MacKay.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Information theory, inference, and learning algorithms</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahoney (2000)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;V. Mahoney.

</span>
<span class="ltx_bibblock">Fast text compression with neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">FLAIRS</em>, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahoney (2006)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;V. Mahoney.

</span>
<span class="ltx_bibblock">Large text compression benchmark, 2006.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mattmahoney.net/dc/text.html" title="">https://www.mattmahoney.net/dc/text.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Mao, Y.&nbsp;Cui, T.&nbsp;Kuo, and C.&nbsp;J. Xue.

</span>
<span class="ltx_bibblock">TRACE: A fast transformer-based general-purpose lossless
compressor.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">WWW</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mentzer et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Mentzer, E.&nbsp;Agustsson, M.&nbsp;Tschannen, R.&nbsp;Timofte, and L.&nbsp;V. Gool.

</span>
<span class="ltx_bibblock">Practical full resolution learned lossless image compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CVPR</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mentzer et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Mentzer, L.&nbsp;V. Gool, and M.&nbsp;Tschannen.

</span>
<span class="ltx_bibblock">Learning better lossless compression using lossy compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">CVPR</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Mikolov.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Statistical Language Models Based on Neural Networks</em>.

</span>
<span class="ltx_bibblock">PhD thesis, Brno Universtiy of Technology, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mirchandani et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Mirchandani, F.&nbsp;Xia, P.&nbsp;Florence, B.&nbsp;Ichter, D.&nbsp;Driess, M.&nbsp;G. Arenas,
K.&nbsp;Rao, D.&nbsp;Sadigh, and A.&nbsp;Zeng.

</span>
<span class="ltx_bibblock">Large language models as general pattern machines.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">CoRL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Mishra, D.&nbsp;Khashabi, C.&nbsp;Baral, and H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">Cross-task generalization via natural language crowdsourcing
instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">ACL (1)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mittu et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Mittu, Y.&nbsp;Bu, A.&nbsp;Gupta, A.&nbsp;Devireddy, A.&nbsp;E. Ozdarendeli, A.&nbsp;Singh, and
G.&nbsp;Anumanchipalli.

</span>
<span class="ltx_bibblock">Finezip : Pushing the limits of large language models for practical
lossless text compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv:2409.17141</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Panayotov, G.&nbsp;Chen, D.&nbsp;Povey, and S.&nbsp;Khudanpur.

</span>
<span class="ltx_bibblock">Librispeech: An ASR corpus based on public domain audio books.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">ICASSP</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasco (1977)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;C. Pasco.

</span>
<span class="ltx_bibblock">Source coding algorithms for fast data compression (ph.d. thesis
abstr.).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">IEEE Trans. Inf. Theory</em>, 1977.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlov (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Pavlov.

</span>
<span class="ltx_bibblock">7z Format, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.7-zip.org/7z.html" title="">http://www.7-zip.org/7z.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pot et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Pot, A.&nbsp;Mohiuddin, P.&nbsp;Ruyssen, M.&nbsp;Michalski, R.&nbsp;S.&nbsp;J. Simsa, and M.&nbsp;Wicke.

</span>
<span class="ltx_bibblock">TensorFlow Datasets, a collection of ready-to-use datasets, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.tensorflow.org/datasets" title="">https://www.tensorflow.org/datasets</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;W. Rae, A.&nbsp;Potapenko, S.&nbsp;M. Jayakumar, C.&nbsp;Hillier, and T.&nbsp;P. Lillicrap.

</span>
<span class="ltx_bibblock">Compressive transformers for long-range sequence modelling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">ICLR</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rathmanner and Hutter (2011)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Rathmanner and M.&nbsp;Hutter.

</span>
<span class="ltx_bibblock">A philosophical treatise of universal induction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Entropy</em>, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rhee et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Rhee, Y.&nbsp;I. Jang, S.&nbsp;Kim, and N.&nbsp;I. Cho.

</span>
<span class="ltx_bibblock">LC-FDNet: Learned lossless image compression with frequency
decomposition network.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">CVPR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rissanen (1976)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Rissanen.

</span>
<span class="ltx_bibblock">Generalized kraft inequality and arithmetic coding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">IBM J. Res. Dev.</em>, 1976.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russakovsky et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Russakovsky, J.&nbsp;Deng, H.&nbsp;Su, J.&nbsp;Krause, S.&nbsp;Satheesh, S.&nbsp;Ma, Z.&nbsp;Huang,
A.&nbsp;Karpathy, A.&nbsp;Khosla, M.&nbsp;S. Bernstein, A.&nbsp;C. Berg, and L.&nbsp;Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Int. J. Comput. Vis.</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schiopu and Munteanu (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Schiopu and A.&nbsp;Munteanu.

</span>
<span class="ltx_bibblock">Deep-learning-based lossless image coding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">IEEE Trans. Circuits Syst. Video Technol.</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schiopu et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Schiopu, Y.&nbsp;Liu, and A.&nbsp;Munteanu.

</span>
<span class="ltx_bibblock">CNN-based prediction for lossless coding of photographic images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">PCS</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmidhuber and Heil (1994)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Schmidhuber and S.&nbsp;Heil.

</span>
<span class="ltx_bibblock">Predictive coding with neural nets: Application to text compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">NIPS</em>, pages 1047–1054. MIT Press, 1994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmidhuber and Heil (1996)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Schmidhuber and S.&nbsp;Heil.

</span>
<span class="ltx_bibblock">Sequential neural text compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">IEEE Trans. Neural Networks</em>, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shannon (1948)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;E. Shannon.

</span>
<span class="ltx_bibblock">A mathematical theory of communication.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Bell Syst. Tech. J.</em>, 1948.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Sharma, C.&nbsp;Li, and L.&nbsp;Wang.

</span>
<span class="ltx_bibblock">BIGPATENT: A large-scale dataset for abstractive and coherent
summarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">ACL (1)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Shazeer.

</span>
<span class="ltx_bibblock">GLU variants improve transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv:2002.05202</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skodras et&nbsp;al. (2001)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Skodras, C.&nbsp;A. Christopoulos, and T.&nbsp;Ebrahimi.

</span>
<span class="ltx_bibblock">The JPEG 2000 still image compression standard.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">IEEE Signal Process. Mag.</em>, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solomonoff (1964a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;J. Solomonoff.

</span>
<span class="ltx_bibblock">A formal theory of inductive inference. part I.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Inf. Control.</em>, 1964a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solomonoff (1964b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;J. Solomonoff.

</span>
<span class="ltx_bibblock">A formal theory of inductive inference. part II.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Inf. Control.</em>, 1964b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tao et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Tao, L.&nbsp;Hou, W.&nbsp;Zhang, L.&nbsp;Shang, X.&nbsp;Jiang, Q.&nbsp;Liu, P.&nbsp;Luo, and N.&nbsp;Wong.

</span>
<span class="ltx_bibblock">Compression of generative pre-trained language models via
quantization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">ACL (1)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, T.&nbsp;Lavril, G.&nbsp;Izacard, X.&nbsp;Martinet, M.&nbsp;Lachaux, T.&nbsp;Lacroix,
B.&nbsp;Rozière, N.&nbsp;Goyal, E.&nbsp;Hambro, F.&nbsp;Azhar, A.&nbsp;Rodriguez, A.&nbsp;Joulin,
E.&nbsp;Grave, and G.&nbsp;Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, L.&nbsp;Martin, K.&nbsp;Stone, P.&nbsp;Albert, A.&nbsp;Almahairi, Y.&nbsp;Babaei,
N.&nbsp;Bashlykov, S.&nbsp;Batra, P.&nbsp;Bhargava, S.&nbsp;Bhosale, D.&nbsp;Bikel, L.&nbsp;Blecher,
C.&nbsp;Canton-Ferrer, M.&nbsp;Chen, G.&nbsp;Cucurull, D.&nbsp;Esiobu, J.&nbsp;Fernandes, J.&nbsp;Fu,
W.&nbsp;Fu, B.&nbsp;Fuller, C.&nbsp;Gao, V.&nbsp;Goswami, N.&nbsp;Goyal, A.&nbsp;Hartshorn, S.&nbsp;Hosseini,
R.&nbsp;Hou, H.&nbsp;Inan, M.&nbsp;Kardas, V.&nbsp;Kerkez, M.&nbsp;Khabsa, I.&nbsp;Kloumann, A.&nbsp;Korenev,
P.&nbsp;S. Koura, M.&nbsp;Lachaux, T.&nbsp;Lavril, J.&nbsp;Lee, D.&nbsp;Liskovich, Y.&nbsp;Lu, Y.&nbsp;Mao,
X.&nbsp;Martinet, T.&nbsp;Mihaylov, P.&nbsp;Mishra, I.&nbsp;Molybog, Y.&nbsp;Nie, A.&nbsp;Poulton,
J.&nbsp;Reizenstein, R.&nbsp;Rungta, K.&nbsp;Saladi, A.&nbsp;Schelten, R.&nbsp;Silva, E.&nbsp;M. Smith,
R.&nbsp;Subramanian, X.&nbsp;E. Tan, B.&nbsp;Tang, R.&nbsp;Taylor, A.&nbsp;Williams, J.&nbsp;X. Kuan,
P.&nbsp;Xu, Z.&nbsp;Yan, I.&nbsp;Zarov, Y.&nbsp;Zhang, A.&nbsp;Fan, M.&nbsp;Kambadur, S.&nbsp;Narang,
A.&nbsp;Rodriguez, R.&nbsp;Stojnic, S.&nbsp;Edunov, and T.&nbsp;Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Townsend et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Townsend, T.&nbsp;Bird, and D.&nbsp;Barber.

</span>
<span class="ltx_bibblock">Practical lossless compression with latent variables using bits back
coding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">ICLR (Poster)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Valmeekam et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;S.&nbsp;K. Valmeekam, K.&nbsp;Narayanan, D.&nbsp;Kalathil, J.&nbsp;Chamberland, and
S.&nbsp;Shakkottai.

</span>
<span class="ltx_bibblock">Llmzip: Lossless text compression using large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv:2306.04050</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van&nbsp;den Oord and Schrauwen (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;van&nbsp;den Oord and B.&nbsp;Schrauwen.

</span>
<span class="ltx_bibblock">The student-t mixture as a natural image patch prior with application
to image compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">J. Mach. Learn. Res.</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Vaswani, N.&nbsp;Shazeer, N.&nbsp;Parmar, J.&nbsp;Uszkoreit, L.&nbsp;Jones, A.&nbsp;N. Gomez,
L.&nbsp;Kaiser, and I.&nbsp;Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">NIPS</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Völske et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Völske, M.&nbsp;Potthast, S.&nbsp;Syed, and B.&nbsp;Stein.

</span>
<span class="ltx_bibblock">TL;DR: Mining Reddit to learn automatic summarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Proceedings of the Workshop on New Frontiers in
Summarization</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, S.&nbsp;Mishra, P.&nbsp;Alipoormolabashi, Y.&nbsp;Kordi, A.&nbsp;Mirzaei, A.&nbsp;Naik,
A.&nbsp;Ashok, A.&nbsp;S. Dhanasekaran, A.&nbsp;Arunkumar, D.&nbsp;Stap, E.&nbsp;Pathak,
G.&nbsp;Karamanolakis, H.&nbsp;G. Lai, I.&nbsp;Purohit, I.&nbsp;Mondal, J.&nbsp;Anderson, K.&nbsp;Kuznia,
K.&nbsp;Doshi, K.&nbsp;K. Pal, M.&nbsp;Patel, M.&nbsp;Moradshahi, M.&nbsp;Parmar, M.&nbsp;Purohit,
N.&nbsp;Varshney, P.&nbsp;R. Kaza, P.&nbsp;Verma, R.&nbsp;S. Puri, R.&nbsp;Karia, S.&nbsp;Doshi, S.&nbsp;K.
Sampat, S.&nbsp;Mishra, S.&nbsp;R. A, S.&nbsp;Patro, T.&nbsp;Dixit, and X.&nbsp;Shen.

</span>
<span class="ltx_bibblock">Super-naturalinstructions: Generalization via declarative
instructions on 1600+ NLP tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">EMNLP</em>, pages 5085–5109. Association for Computational
Linguistics, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welch (1984)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;A. Welch.

</span>
<span class="ltx_bibblock">A technique for high-performance data compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Computer</em>, 1984.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wikimedia (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wikimedia.

</span>
<span class="ltx_bibblock">Wikimedia downloads, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dumps.wikimedia.org/" title="">https://dumps.wikimedia.org</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Witten et&nbsp;al. (1987)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;H. Witten, R.&nbsp;M. Neal, and J.&nbsp;G. Cleary.

</span>
<span class="ltx_bibblock">Arithmetic coding for data compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Commun. ACM</em>, 1987.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Xue, A.&nbsp;Barua, N.&nbsp;Constant, R.&nbsp;Al-Rfou, S.&nbsp;Narang, M.&nbsp;Kale, A.&nbsp;Roberts,
and C.&nbsp;Raffel.

</span>
<span class="ltx_bibblock">Byt5: Towards a token-free future with pre-trained byte-to-byte
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Trans. Assoc. Comput. Linguistics</em>, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experimental Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Training Data Sources</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We source all of our data from the following open-source TensorFlow datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pot et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib54" title="">2019</a>)</cite>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Text</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.6">Since most of TensorFlow’s text datasets are quite small, we concatenate the following five datasets into a single collection of <math alttext="165" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="A1.SS1.SSS0.Px1.p1.1.m1.1a"><mn id="A1.SS1.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.1.m1.1b"><cn id="A1.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS1.SSS0.Px1.p1.1.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.1.m1.1c">165</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.1.m1.1d">165</annotation></semantics></math>GB:
(i) <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS0.Px1.p1.6.1">Wikipedia</em>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wikimedia, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib80" title="">2023</a>)</cite>, the filtered UTF-8 encoded text from an XML dump from 2023-06-01, containing all languages but predominantly English and western languages (<math alttext="113.9" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="A1.SS1.SSS0.Px1.p1.2.m2.1a"><mn id="A1.SS1.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">113.9</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.2.m2.1b"><cn id="A1.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" type="float" xref="A1.SS1.SSS0.Px1.p1.2.m2.1.1">113.9</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.2.m2.1c">113.9</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.2.m2.1d">113.9</annotation></semantics></math>GB);
(ii) <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS0.Px1.p1.6.2">PG-19</em>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib55" title="">2020</a>)</cite>, books from the Project Gutenberg, also encoded in UTF-8 (<math alttext="9.4" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="A1.SS1.SSS0.Px1.p1.3.m3.1a"><mn id="A1.SS1.SSS0.Px1.p1.3.m3.1.1" xref="A1.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">9.4</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.3.m3.1b"><cn id="A1.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" type="float" xref="A1.SS1.SSS0.Px1.p1.3.m3.1.1">9.4</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.3.m3.1c">9.4</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.3.m3.1d">9.4</annotation></semantics></math>GB);
(iii) <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS0.Px1.p1.6.3">Big Patent</em>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sharma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib65" title="">2019</a>)</cite>, a dataset of patents in English (<math alttext="30.2" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.4.m4.1"><semantics id="A1.SS1.SSS0.Px1.p1.4.m4.1a"><mn id="A1.SS1.SSS0.Px1.p1.4.m4.1.1" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">30.2</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.4.m4.1b"><cn id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" type="float" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1">30.2</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.4.m4.1c">30.2</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.4.m4.1d">30.2</annotation></semantics></math>GB);
(iv) <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS0.Px1.p1.6.4">Scientific Papers</em>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cohan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib9" title="">2018</a>)</cite>, from arXiv and PubMed, containing the raw text including the LaTeX code (<math alttext="8.1" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.5.m5.1"><semantics id="A1.SS1.SSS0.Px1.p1.5.m5.1a"><mn id="A1.SS1.SSS0.Px1.p1.5.m5.1.1" xref="A1.SS1.SSS0.Px1.p1.5.m5.1.1.cmml">8.1</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.5.m5.1b"><cn id="A1.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" type="float" xref="A1.SS1.SSS0.Px1.p1.5.m5.1.1">8.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.5.m5.1c">8.1</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.5.m5.1d">8.1</annotation></semantics></math>GB);
and (v) <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS0.Px1.p1.6.5">Natural Instructions</em>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mishra et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib49" title="">2022</a>; Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib78" title="">2022</a>)</cite>, tasks formulated in English covering different domains and lanugages (<math alttext="4.1" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.6.m6.1"><semantics id="A1.SS1.SSS0.Px1.p1.6.m6.1a"><mn id="A1.SS1.SSS0.Px1.p1.6.m6.1.1" xref="A1.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">4.1</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.6.m6.1b"><cn id="A1.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" type="float" xref="A1.SS1.SSS0.Px1.p1.6.m6.1.1">4.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.6.m6.1c">4.1</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.6.m6.1d">4.1</annotation></semantics></math>GB).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Image</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p1.2">We collect a subset of <math alttext="165" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="A1.SS1.SSS0.Px2.p1.1.m1.1a"><mn id="A1.SS1.SSS0.Px2.p1.1.m1.1.1" xref="A1.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px2.p1.1.m1.1b"><cn id="A1.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS1.SSS0.Px2.p1.1.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px2.p1.1.m1.1c">165</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px2.p1.1.m1.1d">165</annotation></semantics></math>GB of the ImageNet dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Russakovsky et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib59" title="">2015</a>)</cite>, uniformly sampled across the <math alttext="1000" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="A1.SS1.SSS0.Px2.p1.2.m2.1a"><mn id="A1.SS1.SSS0.Px2.p1.2.m2.1.1" xref="A1.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px2.p1.2.m2.1b"><cn id="A1.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" type="integer" xref="A1.SS1.SSS0.Px2.p1.2.m2.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px2.p1.2.m2.1c">1000</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px2.p1.2.m2.1d">1000</annotation></semantics></math>&nbsp;classes, which contains <span class="ltx_text ltx_number" id="A1.SS1.SSS0.Px2.p1.2.1">14 197 122</span> annotated images (of varying resolutions) from the WordNet hierarchy.
We decode the images into RGB arrays (three <span class="ltx_text ltx_font_typewriter" id="A1.SS1.SSS0.Px2.p1.2.2">uint8</span> channels), flatten them, and concatenate them into a byte stream of flattened images.
As a consequence, we ignore image boundaries when sampling from this data source (i.e., sequences are not guaranteed to start or end at the start or end of an image).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Audio</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px3.p1.1">We create a subset of <math alttext="165" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="A1.SS1.SSS0.Px3.p1.1.m1.1a"><mn id="A1.SS1.SSS0.Px3.p1.1.m1.1.1" xref="A1.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px3.p1.1.m1.1b"><cn id="A1.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS1.SSS0.Px3.p1.1.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px3.p1.1.m1.1c">165</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px3.p1.1.m1.1d">165</annotation></semantics></math>GB from the Common Voice dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ardila et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib1" title="">2020</a>)</cite>, a multilingual dataset of voice recordings.
We downsample the dataset from 48 kHz to 16 kHz and encode the waveform as <span class="ltx_text ltx_font_typewriter" id="A1.SS1.SSS0.Px3.p1.1.1">int16</span>, i.e., with two bytes per sample.
As for images, we concatenate all individual audio samples into a single byte stream.
Accordingly, there is no guarantee that a sequence sampled from our dataset starts or ends at the beginning of a recording.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Out-of-Distribution Evaluation Data Sources</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">We source all of our data from the following open-source TensorFlow datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pot et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib54" title="">2019</a>)</cite>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Text</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p1.2">We consider a <math alttext="1" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="A1.SS2.SSS0.Px1.p1.1.m1.1a"><mn id="A1.SS2.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px1.p1.1.m1.1b"><cn id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px1.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px1.p1.1.m1.1d">1</annotation></semantics></math>GB subset of the <em class="ltx_emph ltx_font_italic" id="A1.SS2.SSS0.Px1.p1.2.1">Reddit</em> dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Völske et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib77" title="">2017</a>)</cite>, which contains <math alttext="3.8" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="A1.SS2.SSS0.Px1.p1.2.m2.1a"><mn id="A1.SS2.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">3.8</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px1.p1.2.m2.1b"><cn id="A1.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" type="float" xref="A1.SS2.SSS0.Px1.p1.2.m2.1.1">3.8</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px1.p1.2.m2.1c">3.8</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px1.p1.2.m2.1d">3.8</annotation></semantics></math>
million Reddit posts encoded in UTF-8.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Images</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px2.p1.2">We create a <math alttext="1" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="A1.SS2.SSS0.Px2.p1.1.m1.1a"><mn id="A1.SS2.SSS0.Px2.p1.1.m1.1.1" xref="A1.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px2.p1.1.m1.1b"><cn id="A1.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS2.SSS0.Px2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px2.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px2.p1.1.m1.1d">1</annotation></semantics></math>GB subset of the CelebA HQ dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib40" title="">2015</a>)</cite> with a resolution of <math alttext="512\times 512" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="A1.SS2.SSS0.Px2.p1.2.m2.1a"><mrow id="A1.SS2.SSS0.Px2.p1.2.m2.1.1" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mn id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">512</mn><mo id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml">×</mo><mn id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1"><times id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.1"></times><cn id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" type="integer" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.2">512</cn><cn id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" type="integer" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px2.p1.2.m2.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px2.p1.2.m2.1d">512 × 512</annotation></semantics></math>.
We process the images in the same way as for our image training set, i.e., flattening and concatenation, and we subsample uniformly across classes of CelebA.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Audio</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px3.p1.2">We use <math alttext="1" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="A1.SS2.SSS0.Px3.p1.1.m1.1a"><mn id="A1.SS2.SSS0.Px3.p1.1.m1.1.1" xref="A1.SS2.SSS0.Px3.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px3.p1.1.m1.1b"><cn id="A1.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS2.SSS0.Px3.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px3.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px3.p1.1.m1.1d">1</annotation></semantics></math>GB from the <em class="ltx_emph ltx_font_italic" id="A1.SS2.SSS0.Px3.p1.2.1">LibriSpeech</em>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Panayotov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib51" title="">2015</a>)</cite> dataset, which contains roughly <math alttext="1000" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px3.p1.2.m2.1"><semantics id="A1.SS2.SSS0.Px3.p1.2.m2.1a"><mn id="A1.SS2.SSS0.Px3.p1.2.m2.1.1" xref="A1.SS2.SSS0.Px3.p1.2.m2.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px3.p1.2.m2.1b"><cn id="A1.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" type="integer" xref="A1.SS2.SSS0.Px3.p1.2.m2.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px3.p1.2.m2.1c">1000</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px3.p1.2.m2.1d">1000</annotation></semantics></math> hours of English speech data derived from audiobooks that have been segmented and aligned in the LibriVox project.
The data is already in 16kHz (with a sample size of 2 bytes), and we simply concatenate samples into a single byte stream.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Multimodal Evaluations</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px4.p1.1">For our evaluations on multimodal data, we use the unimodal evaluations on <math alttext="1" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px4.p1.1.m1.1"><semantics id="A1.SS2.SSS0.Px4.p1.1.m1.1a"><mn id="A1.SS2.SSS0.Px4.p1.1.m1.1.1" xref="A1.SS2.SSS0.Px4.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px4.p1.1.m1.1b"><cn id="A1.SS2.SSS0.Px4.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS2.SSS0.Px4.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px4.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px4.p1.1.m1.1d">1</annotation></semantics></math>GB of data as described above and average the results accordingly (both for our models but also all standard compression algorithms, and Bellard’s online adaptive transformer), either over two or three evaluations depending on the evaluation mixture composition.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Sweeps</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="A1.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Model Size vs.&nbsp;Dataset Size</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS3.SSS0.Px1.p1.16">The experiment to investigate the impact of training dataset- and model size, with results shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get ‣ 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>, used the following model parameters.
Dataset sizes were <math alttext="20\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="A1.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="A1.SS3.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="A1.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml">20</mn><mo id="A1.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="A1.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.1.m1.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.1.m1.1d">20 %</annotation></semantics></math>, <math alttext="40\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="A1.SS3.SSS0.Px1.p1.2.m2.1a"><mrow id="A1.SS3.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="A1.SS3.SSS0.Px1.p1.2.m2.1.1.2" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1.2.cmml">40</mn><mo id="A1.SS3.SSS0.Px1.p1.2.m2.1.1.1" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.2.m2.1b"><apply id="A1.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px1.p1.2.m2.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.2.m2.1c">40\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.2.m2.1d">40 %</annotation></semantics></math>, <math alttext="60\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="A1.SS3.SSS0.Px1.p1.3.m3.1a"><mrow id="A1.SS3.SSS0.Px1.p1.3.m3.1.1" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.cmml"><mn id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml">60</mn><mo id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.1" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.3.m3.1b"><apply id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.3.m3.1c">60\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.3.m3.1d">60 %</annotation></semantics></math>, <math alttext="80\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.4.m4.1"><semantics id="A1.SS3.SSS0.Px1.p1.4.m4.1a"><mrow id="A1.SS3.SSS0.Px1.p1.4.m4.1.1" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.cmml"><mn id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.cmml">80</mn><mo id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.4.m4.1b"><apply id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.4.m4.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.4.m4.1d">80 %</annotation></semantics></math>, and <math alttext="100\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.5.m5.1"><semantics id="A1.SS3.SSS0.Px1.p1.5.m5.1a"><mrow id="A1.SS3.SSS0.Px1.p1.5.m5.1.1" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.cmml"><mn id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2.cmml">100</mn><mo id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.5.m5.1b"><apply id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.5.m5.1c">100\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.5.m5.1d">100 %</annotation></semantics></math> of the full <math alttext="165" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.6.m6.1"><semantics id="A1.SS3.SSS0.Px1.p1.6.m6.1a"><mn id="A1.SS3.SSS0.Px1.p1.6.m6.1.1" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.6.m6.1b"><cn id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.6.m6.1c">165</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.6.m6.1d">165</annotation></semantics></math>GB for each training set mixture (uni- and multimodal).
All models used a context size of <math alttext="4096" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.7.m7.1"><semantics id="A1.SS3.SSS0.Px1.p1.7.m7.1a"><mn id="A1.SS3.SSS0.Px1.p1.7.m7.1.1" xref="A1.SS3.SSS0.Px1.p1.7.m7.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.7.m7.1b"><cn id="A1.SS3.SSS0.Px1.p1.7.m7.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.7.m7.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.7.m7.1c">4096</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.7.m7.1d">4096</annotation></semantics></math>, <math alttext="8" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.8.m8.1"><semantics id="A1.SS3.SSS0.Px1.p1.8.m8.1a"><mn id="A1.SS3.SSS0.Px1.p1.8.m8.1.1" xref="A1.SS3.SSS0.Px1.p1.8.m8.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.8.m8.1b"><cn id="A1.SS3.SSS0.Px1.p1.8.m8.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.8.m8.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.8.m8.1c">8</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.8.m8.1d">8</annotation></semantics></math> attention heads per layer, a widening factor of <math alttext="4" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.9.m9.1"><semantics id="A1.SS3.SSS0.Px1.p1.9.m9.1a"><mn id="A1.SS3.SSS0.Px1.p1.9.m9.1.1" xref="A1.SS3.SSS0.Px1.p1.9.m9.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.9.m9.1b"><cn id="A1.SS3.SSS0.Px1.p1.9.m9.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.9.m9.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.9.m9.1c">4</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.9.m9.1d">4</annotation></semantics></math> and the number of layers was either <math alttext="2" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.10.m10.1"><semantics id="A1.SS3.SSS0.Px1.p1.10.m10.1a"><mn id="A1.SS3.SSS0.Px1.p1.10.m10.1.1" xref="A1.SS3.SSS0.Px1.p1.10.m10.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.10.m10.1b"><cn id="A1.SS3.SSS0.Px1.p1.10.m10.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.10.m10.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.10.m10.1c">2</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.10.m10.1d">2</annotation></semantics></math>, <math alttext="4" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.11.m11.1"><semantics id="A1.SS3.SSS0.Px1.p1.11.m11.1a"><mn id="A1.SS3.SSS0.Px1.p1.11.m11.1.1" xref="A1.SS3.SSS0.Px1.p1.11.m11.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.11.m11.1b"><cn id="A1.SS3.SSS0.Px1.p1.11.m11.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.11.m11.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.11.m11.1c">4</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.11.m11.1d">4</annotation></semantics></math>, <math alttext="6" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.12.m12.1"><semantics id="A1.SS3.SSS0.Px1.p1.12.m12.1a"><mn id="A1.SS3.SSS0.Px1.p1.12.m12.1.1" xref="A1.SS3.SSS0.Px1.p1.12.m12.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.12.m12.1b"><cn id="A1.SS3.SSS0.Px1.p1.12.m12.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.12.m12.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.12.m12.1c">6</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.12.m12.1d">6</annotation></semantics></math>, <math alttext="8" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.13.m13.1"><semantics id="A1.SS3.SSS0.Px1.p1.13.m13.1a"><mn id="A1.SS3.SSS0.Px1.p1.13.m13.1.1" xref="A1.SS3.SSS0.Px1.p1.13.m13.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.13.m13.1b"><cn id="A1.SS3.SSS0.Px1.p1.13.m13.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.13.m13.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.13.m13.1c">8</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.13.m13.1d">8</annotation></semantics></math>, or <math alttext="10" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.14.m14.1"><semantics id="A1.SS3.SSS0.Px1.p1.14.m14.1a"><mn id="A1.SS3.SSS0.Px1.p1.14.m14.1.1" xref="A1.SS3.SSS0.Px1.p1.14.m14.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.14.m14.1b"><cn id="A1.SS3.SSS0.Px1.p1.14.m14.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.14.m14.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.14.m14.1c">10</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.14.m14.1d">10</annotation></semantics></math>.
Models were trained with a batch size of <math alttext="32" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.15.m15.1"><semantics id="A1.SS3.SSS0.Px1.p1.15.m15.1a"><mn id="A1.SS3.SSS0.Px1.p1.15.m15.1.1" xref="A1.SS3.SSS0.Px1.p1.15.m15.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.15.m15.1b"><cn id="A1.SS3.SSS0.Px1.p1.15.m15.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.15.m15.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.15.m15.1c">32</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.15.m15.1d">32</annotation></semantics></math>.
The learning rate was <math alttext="1\text{\times}{10}^{-4}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.16.m16.3"><semantics id="A1.SS3.SSS0.Px1.p1.16.m16.3a"><mrow id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml"><mn id="A1.SS3.SSS0.Px1.p1.16.m16.1.1.1.1.1.1.1" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml">1</mn><mtext id="A1.SS3.SSS0.Px1.p1.16.m16.2.2.2.2.2.2.2" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml">×</mtext><msup id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.3.3.3.3" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml"><mn id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.3.3.3.3.2" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml">10</mn><mrow id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.3.3.3.3.3.2" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml"><mo id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.3.3.3.3.3.2a" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml">−</mo><mn id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.3.3.3.3.3.2.2" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.16.m16.3b"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3">1E-4</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.16.m16.3c">1\text{\times}{10}^{-4}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.16.m16.3d">start_ARG 1 end_ARG start_ARG times end_ARG start_ARG power start_ARG 10 end_ARG start_ARG - 4 end_ARG end_ARG</annotation></semantics></math>, and a sinusoid positional encoding was used.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Model Size vs.&nbsp;Context Size</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS3.SSS0.Px2.p1.12"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F5" title="In Model Size vs. Context Size ‣ 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a> in the main paper shows the relationship between context length and model size.
For this experiment we performed a large-scale sweep with the goal of covering a good range of training FLOPS budget with models that make various trade-offs between model size and context length (given the same model size, compute demand increases with increasing context length).
The main question was whether there is a qualitatively similar relationship across parameters, and whether there is a clear sweet spot — see the main paper for results and discussion.
For our sweep we used the same model parameters as in the previous paragraph (the training data size was always at <math alttext="100\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.1.m1.1"><semantics id="A1.SS3.SSS0.Px2.p1.1.m1.1a"><mrow id="A1.SS3.SSS0.Px2.p1.1.m1.1.1" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="A1.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">100</mn><mo id="A1.SS3.SSS0.Px2.p1.1.m1.1.1.1" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="A1.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.1.m1.1c">100\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.1.m1.1d">100 %</annotation></semantics></math>) and sweep over the following four context sizes (with training batch size in brackets): <math alttext="{[1024\leavevmode\nobreak\ (128),2048\leavevmode\nobreak\ (64),4096\leavevmode%
\nobreak\ (32),8192\leavevmode\nobreak\ (16)]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.2.m2.8"><semantics id="A1.SS3.SSS0.Px2.p1.2.m2.8a"><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml"><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml">[</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.cmml"><mn id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.2.cmml">1024</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.1" lspace="0.500em" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.1.cmml">⁢</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.3.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.3.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.cmml">(</mo><mn id="A1.SS3.SSS0.Px2.p1.2.m2.1.1" xref="A1.SS3.SSS0.Px2.p1.2.m2.1.1.cmml">128</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.3.2.2" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.cmml">)</mo></mrow></mrow><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.6" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml">,</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.cmml"><mn id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.2.cmml">2048</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.1" lspace="0.500em" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.1.cmml">⁢</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.3.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.cmml"><mo id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.3.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.cmml">(</mo><mn id="A1.SS3.SSS0.Px2.p1.2.m2.2.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.2.2.cmml">64</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.3.2.2" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.cmml">)</mo></mrow></mrow><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.7" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml">,</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.cmml"><mn id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.2.cmml">4096</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.1" lspace="0.500em" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.1.cmml">⁢</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.3.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.cmml"><mo id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.3.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.cmml">(</mo><mn id="A1.SS3.SSS0.Px2.p1.2.m2.3.3" xref="A1.SS3.SSS0.Px2.p1.2.m2.3.3.cmml">32</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.3.2.2" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.cmml">)</mo></mrow></mrow><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.8" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml">,</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.cmml"><mn id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.2.cmml">8192</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.1" lspace="0.500em" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.1.cmml">⁢</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.3.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.cmml"><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.3.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.cmml">(</mo><mn id="A1.SS3.SSS0.Px2.p1.2.m2.4.4" xref="A1.SS3.SSS0.Px2.p1.2.m2.4.4.cmml">16</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.3.2.2" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.cmml">)</mo></mrow></mrow><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.9" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.2.m2.8b"><list id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4"><apply id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1"><times id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.1.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.1"></times><cn id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.2">1024</cn><cn id="A1.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.1.1">128</cn></apply><apply id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2"><times id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.1.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.1"></times><cn id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.2">2048</cn><cn id="A1.SS3.SSS0.Px2.p1.2.m2.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.2.2">64</cn></apply><apply id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3"><times id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.1.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.1"></times><cn id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.2">4096</cn><cn id="A1.SS3.SSS0.Px2.p1.2.m2.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.3.3">32</cn></apply><apply id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4"><times id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.1.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.1"></times><cn id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.2">8192</cn><cn id="A1.SS3.SSS0.Px2.p1.2.m2.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.4.4">16</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.2.m2.8c">{[1024\leavevmode\nobreak\ (128),2048\leavevmode\nobreak\ (64),4096\leavevmode%
\nobreak\ (32),8192\leavevmode\nobreak\ (16)]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.2.m2.8d">[ 1024 ( 128 ) , 2048 ( 64 ) , 4096 ( 32 ) , 8192 ( 16 ) ]</annotation></semantics></math>.
For each context size we train five models (XS, S, M, L, and XL) on all three unimodal datasets, respectively.
Each model has a different combination of embedding dimension and number of layers for each different context size.
The XS models have embedding dimensions <math alttext="{[112,96,80,64]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.3.m3.4"><semantics id="A1.SS3.SSS0.Px2.p1.3.m3.4a"><mrow id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.3.m3.1.1" xref="A1.SS3.SSS0.Px2.p1.3.m3.1.1.cmml">112</mn><mo id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.3.m3.2.2" xref="A1.SS3.SSS0.Px2.p1.3.m3.2.2.cmml">96</mn><mo id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.3.m3.3.3" xref="A1.SS3.SSS0.Px2.p1.3.m3.3.3.cmml">80</mn><mo id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.3.m3.4.4" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.4.cmml">64</mn><mo id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.3.m3.4b"><list id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.3.m3.1.1">112</cn><cn id="A1.SS3.SSS0.Px2.p1.3.m3.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.3.m3.2.2">96</cn><cn id="A1.SS3.SSS0.Px2.p1.3.m3.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.3.m3.3.3">80</cn><cn id="A1.SS3.SSS0.Px2.p1.3.m3.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.4">64</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.3.m3.4c">{[112,96,80,64]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.3.m3.4d">[ 112 , 96 , 80 , 64 ]</annotation></semantics></math> and numbers of layers <math alttext="{[11,7,5,3]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.4.m4.4"><semantics id="A1.SS3.SSS0.Px2.p1.4.m4.4a"><mrow id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.4.m4.1.1" xref="A1.SS3.SSS0.Px2.p1.4.m4.1.1.cmml">11</mn><mo id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.4.m4.2.2" xref="A1.SS3.SSS0.Px2.p1.4.m4.2.2.cmml">7</mn><mo id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.4.m4.3.3" xref="A1.SS3.SSS0.Px2.p1.4.m4.3.3.cmml">5</mn><mo id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.4.m4.4.4" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.4.cmml">3</mn><mo id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.4.m4.4b"><list id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.4.m4.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.4.m4.1.1">11</cn><cn id="A1.SS3.SSS0.Px2.p1.4.m4.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.4.m4.2.2">7</cn><cn id="A1.SS3.SSS0.Px2.p1.4.m4.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.4.m4.3.3">5</cn><cn id="A1.SS3.SSS0.Px2.p1.4.m4.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.4">3</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.4.m4.4c">{[11,7,5,3]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.4.m4.4d">[ 11 , 7 , 5 , 3 ]</annotation></semantics></math> for the different context sizes respectively (i.e., wider and deeper models for shorter contexts and more narrow and more shallow models for long context size).
The S models have embedding dimensions <math alttext="{[192,160,112,96]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.5.m5.4"><semantics id="A1.SS3.SSS0.Px2.p1.5.m5.4a"><mrow id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.5.m5.1.1" xref="A1.SS3.SSS0.Px2.p1.5.m5.1.1.cmml">192</mn><mo id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.5.m5.2.2" xref="A1.SS3.SSS0.Px2.p1.5.m5.2.2.cmml">160</mn><mo id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.5.m5.3.3" xref="A1.SS3.SSS0.Px2.p1.5.m5.3.3.cmml">112</mn><mo id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.5.m5.4.4" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.4.cmml">96</mn><mo id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.5.m5.4b"><list id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.5.m5.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.5.m5.1.1">192</cn><cn id="A1.SS3.SSS0.Px2.p1.5.m5.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.5.m5.2.2">160</cn><cn id="A1.SS3.SSS0.Px2.p1.5.m5.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.5.m5.3.3">112</cn><cn id="A1.SS3.SSS0.Px2.p1.5.m5.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.4">96</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.5.m5.4c">{[192,160,112,96]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.5.m5.4d">[ 192 , 160 , 112 , 96 ]</annotation></semantics></math> and numbers of layers <math alttext="{[10,8,6,4]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.6.m6.4"><semantics id="A1.SS3.SSS0.Px2.p1.6.m6.4a"><mrow id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.6.m6.1.1" xref="A1.SS3.SSS0.Px2.p1.6.m6.1.1.cmml">10</mn><mo id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.6.m6.2.2" xref="A1.SS3.SSS0.Px2.p1.6.m6.2.2.cmml">8</mn><mo id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.6.m6.3.3" xref="A1.SS3.SSS0.Px2.p1.6.m6.3.3.cmml">6</mn><mo id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.6.m6.4.4" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.4.cmml">4</mn><mo id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.6.m6.4b"><list id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.6.m6.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.6.m6.1.1">10</cn><cn id="A1.SS3.SSS0.Px2.p1.6.m6.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.6.m6.2.2">8</cn><cn id="A1.SS3.SSS0.Px2.p1.6.m6.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.6.m6.3.3">6</cn><cn id="A1.SS3.SSS0.Px2.p1.6.m6.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.4">4</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.6.m6.4c">{[10,8,6,4]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.6.m6.4d">[ 10 , 8 , 6 , 4 ]</annotation></semantics></math>.
The M models have embedding dimensions <math alttext="{[224,192,144,112]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.7.m7.4"><semantics id="A1.SS3.SSS0.Px2.p1.7.m7.4a"><mrow id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.7.m7.1.1" xref="A1.SS3.SSS0.Px2.p1.7.m7.1.1.cmml">224</mn><mo id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.7.m7.2.2" xref="A1.SS3.SSS0.Px2.p1.7.m7.2.2.cmml">192</mn><mo id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.7.m7.3.3" xref="A1.SS3.SSS0.Px2.p1.7.m7.3.3.cmml">144</mn><mo id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.7.m7.4.4" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.4.cmml">112</mn><mo id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.7.m7.4b"><list id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.7.m7.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.7.m7.1.1">224</cn><cn id="A1.SS3.SSS0.Px2.p1.7.m7.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.7.m7.2.2">192</cn><cn id="A1.SS3.SSS0.Px2.p1.7.m7.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.7.m7.3.3">144</cn><cn id="A1.SS3.SSS0.Px2.p1.7.m7.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.4">112</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.7.m7.4c">{[224,192,144,112]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.7.m7.4d">[ 224 , 192 , 144 , 112 ]</annotation></semantics></math> and numbers of layers <math alttext="{[12,9,7,5]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.8.m8.4"><semantics id="A1.SS3.SSS0.Px2.p1.8.m8.4a"><mrow id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.8.m8.1.1" xref="A1.SS3.SSS0.Px2.p1.8.m8.1.1.cmml">12</mn><mo id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.8.m8.2.2" xref="A1.SS3.SSS0.Px2.p1.8.m8.2.2.cmml">9</mn><mo id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.8.m8.3.3" xref="A1.SS3.SSS0.Px2.p1.8.m8.3.3.cmml">7</mn><mo id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.8.m8.4.4" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.4.cmml">5</mn><mo id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.8.m8.4b"><list id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.8.m8.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.8.m8.1.1">12</cn><cn id="A1.SS3.SSS0.Px2.p1.8.m8.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.8.m8.2.2">9</cn><cn id="A1.SS3.SSS0.Px2.p1.8.m8.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.8.m8.3.3">7</cn><cn id="A1.SS3.SSS0.Px2.p1.8.m8.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.4">5</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.8.m8.4c">{[12,9,7,5]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.8.m8.4d">[ 12 , 9 , 7 , 5 ]</annotation></semantics></math>.
The L models have embedding dimensions <math alttext="{[272,240,176,144]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.9.m9.4"><semantics id="A1.SS3.SSS0.Px2.p1.9.m9.4a"><mrow id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.9.m9.1.1" xref="A1.SS3.SSS0.Px2.p1.9.m9.1.1.cmml">272</mn><mo id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.9.m9.2.2" xref="A1.SS3.SSS0.Px2.p1.9.m9.2.2.cmml">240</mn><mo id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.9.m9.3.3" xref="A1.SS3.SSS0.Px2.p1.9.m9.3.3.cmml">176</mn><mo id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.9.m9.4.4" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.4.cmml">144</mn><mo id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.9.m9.4b"><list id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.9.m9.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.9.m9.1.1">272</cn><cn id="A1.SS3.SSS0.Px2.p1.9.m9.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.9.m9.2.2">240</cn><cn id="A1.SS3.SSS0.Px2.p1.9.m9.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.9.m9.3.3">176</cn><cn id="A1.SS3.SSS0.Px2.p1.9.m9.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.4">144</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.9.m9.4c">{[272,240,176,144]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.9.m9.4d">[ 272 , 240 , 176 , 144 ]</annotation></semantics></math> and numbers of layers <math alttext="{[13,10,8,5]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.10.m10.4"><semantics id="A1.SS3.SSS0.Px2.p1.10.m10.4a"><mrow id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.10.m10.1.1" xref="A1.SS3.SSS0.Px2.p1.10.m10.1.1.cmml">13</mn><mo id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.10.m10.2.2" xref="A1.SS3.SSS0.Px2.p1.10.m10.2.2.cmml">10</mn><mo id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.10.m10.3.3" xref="A1.SS3.SSS0.Px2.p1.10.m10.3.3.cmml">8</mn><mo id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.10.m10.4.4" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.4.cmml">5</mn><mo id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.10.m10.4b"><list id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.10.m10.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.10.m10.1.1">13</cn><cn id="A1.SS3.SSS0.Px2.p1.10.m10.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.10.m10.2.2">10</cn><cn id="A1.SS3.SSS0.Px2.p1.10.m10.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.10.m10.3.3">8</cn><cn id="A1.SS3.SSS0.Px2.p1.10.m10.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.4">5</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.10.m10.4c">{[13,10,8,5]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.10.m10.4d">[ 13 , 10 , 8 , 5 ]</annotation></semantics></math>.
The XL models have embedding dimensions <math alttext="{[320,304,240,160]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.11.m11.4"><semantics id="A1.SS3.SSS0.Px2.p1.11.m11.4a"><mrow id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.11.m11.1.1" xref="A1.SS3.SSS0.Px2.p1.11.m11.1.1.cmml">320</mn><mo id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.11.m11.2.2" xref="A1.SS3.SSS0.Px2.p1.11.m11.2.2.cmml">304</mn><mo id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.11.m11.3.3" xref="A1.SS3.SSS0.Px2.p1.11.m11.3.3.cmml">240</mn><mo id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.11.m11.4.4" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.4.cmml">160</mn><mo id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.11.m11.4b"><list id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.11.m11.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.11.m11.1.1">320</cn><cn id="A1.SS3.SSS0.Px2.p1.11.m11.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.11.m11.2.2">304</cn><cn id="A1.SS3.SSS0.Px2.p1.11.m11.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.11.m11.3.3">240</cn><cn id="A1.SS3.SSS0.Px2.p1.11.m11.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.4">160</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.11.m11.4c">{[320,304,240,160]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.11.m11.4d">[ 320 , 304 , 240 , 160 ]</annotation></semantics></math> and numbers of layers <math alttext="{[12,9,7,6]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.12.m12.4"><semantics id="A1.SS3.SSS0.Px2.p1.12.m12.4a"><mrow id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.12.m12.1.1" xref="A1.SS3.SSS0.Px2.p1.12.m12.1.1.cmml">12</mn><mo id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.12.m12.2.2" xref="A1.SS3.SSS0.Px2.p1.12.m12.2.2.cmml">9</mn><mo id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.12.m12.3.3" xref="A1.SS3.SSS0.Px2.p1.12.m12.3.3.cmml">7</mn><mo id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.12.m12.4.4" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.4.cmml">6</mn><mo id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.12.m12.4b"><list id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.12.m12.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.12.m12.1.1">12</cn><cn id="A1.SS3.SSS0.Px2.p1.12.m12.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.12.m12.2.2">9</cn><cn id="A1.SS3.SSS0.Px2.p1.12.m12.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.12.m12.3.3">7</cn><cn id="A1.SS3.SSS0.Px2.p1.12.m12.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.4">6</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.12.m12.4c">{[12,9,7,6]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.12.m12.4d">[ 12 , 9 , 7 , 6 ]</annotation></semantics></math>.
The main goal with these settings is to create families of models that have roughly the same demand in terms of FLOPS (iso-FLOPS) but very different trade-offs in terms of model- and context size.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Computational Resources</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">We trained every model on 16 NVIDIA A100 GPUs from our internal cluster.
We trained 315 models in total, yielding a computational footprint of 5040 A100s.
We ran Bellard’s code on an NVIDIA GeForce RTX 4090 GPU with a 24-core Intel i9-13900KF CPU @ 3Ghz.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Compression Ratios</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table A1: </span>
Best compression ratios for each compressor.
This table shows the same results as <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> but as precise numerical values to facilitate detailed comparison.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T1.1" style="width:390.3pt;height:155.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.7pt,3.5pt) scale(0.957125589040112,0.957125589040112) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T1.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A2.T1.1.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="A2.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.1.1.2.1">Out-of-Distribution Compression Ratio</span></td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.1.1">Evaluation Modality</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.2.1">Ours</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.3.1">Bellard</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.4.1">gzip</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.5.1">LZMA2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.6"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.6.1">FLAC</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.7"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.7.1">PNG</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.8"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.8.1">JPEG&nbsp;2000</span></td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T1.1.1.3.3.1">Audio</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.2"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.3.3.2.1">0.487</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.3">0.509</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.4">0.813</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.5">0.699</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.6">0.538</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.4.4.1">Image</th>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.2">0.285</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.4.4.3.1">0.281</span></td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.4">0.698</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.5">0.545</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.6">-</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.7">0.426</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T1.1.1.4.4.8">0.390</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.5.5.1">Text</th>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.2">0.217</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.5.5.3.1">0.204</span></td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.4">0.394</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.5">0.286</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.6">-</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T1.1.1.5.5.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.6.6.1">Audio + Image</th>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.2"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.6.6.2.1">0.393</span></td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.3">0.395</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.4">0.756</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.5">0.622</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.6">-</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T1.1.1.6.6.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.7.7.1">Audio + Text</th>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.2">0.362</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.7.7.3.1">0.357</span></td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.4">0.604</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.5">0.493</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.6">-</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T1.1.1.7.7.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.8.8.1">Image + Text</th>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.2">0.270</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.8.8.3.1">0.243</span></td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.4">0.546</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.5">0.415</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.6">-</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T1.1.1.8.8.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T1.1.1.9.9.1">Audio + Image + Text</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.2">0.349</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.9.9.3.1">0.331</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.4">0.635</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.5">0.510</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.6">-</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.8">-</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.T1" title="In B.1 Compression Ratios ‣ Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">A1</span></a> shows the optimal compression ratios that each of the compressors achieve on all of the different evaluation modalities (note that all evaluations are on out-of-distribution data).
The same values as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> in the main paper and given here as precise numerical values for completeness.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Running Times</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A2.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table A2: </span>Running times to compress <math alttext="1" class="ltx_Math" display="inline" id="A2.T2.2.m1.1"><semantics id="A2.T2.2.m1.1b"><mn id="A2.T2.2.m1.1.1" xref="A2.T2.2.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A2.T2.2.m1.1c"><cn id="A2.T2.2.m1.1.1.cmml" type="integer" xref="A2.T2.2.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.2.m1.1d">1</annotation><annotation encoding="application/x-llamapun" id="A2.T2.2.m1.1e">1</annotation></semantics></math>GB of data for all compressors used in our study. Note that we use the best model per modality, which have different sizes and thus different running times.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T2.3" style="width:390.3pt;height:88pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.5pt,1.0pt) scale(0.977333018201536,0.977333018201536) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T2.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T2.3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A2.T2.3.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="7" id="A2.T2.3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.1.1.2.1">Running Times [s]</span></th>
</tr>
<tr class="ltx_tr" id="A2.T2.3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="A2.T2.3.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.1.1">Evaluation Modality</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.2.1">Ours</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.3.1">Bellard</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.4.1">gzip</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.5.1">LZMA2</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.6"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.6.1">FLAC</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.7"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.7.1">PNG</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.8"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.8.1">JPEG&nbsp;2000</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T2.3.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T2.3.1.3.1.1">Audio</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.2"><span class="ltx_text ltx_number" id="A2.T2.3.1.3.1.2.1">305 609</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.3"><span class="ltx_text ltx_number" id="A2.T2.3.1.3.1.3.1">101 178</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.4">55</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.5">524</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.6">169</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T2.3.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T2.3.1.4.2.1">Image</th>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.2"><span class="ltx_text ltx_number" id="A2.T2.3.1.4.2.2.1">222 065</span></td>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.3"><span class="ltx_text ltx_number" id="A2.T2.3.1.4.2.3.1">103 391</span></td>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.4">47</td>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.5">436</td>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.6">174</td>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.7">495</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T2.3.1.4.2.8">99</td>
</tr>
<tr class="ltx_tr" id="A2.T2.3.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T2.3.1.5.3.1">Text</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.2"><span class="ltx_text ltx_number" id="A2.T2.3.1.5.3.2.1">452 355</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.3"><span class="ltx_text ltx_number" id="A2.T2.3.1.5.3.3.1">100 657</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.4">102</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.5">881</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.6">184</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.8">-</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.T2" title="In B.2 Running Times ‣ Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">A2</span></a> shows the wall-clock running times in seconds for compressing <math alttext="1" class="ltx_Math" display="inline" id="A2.SS2.p1.1.m1.1"><semantics id="A2.SS2.p1.1.m1.1a"><mn id="A2.SS2.p1.1.m1.1.1" xref="A2.SS2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.1.m1.1b"><cn id="A2.SS2.p1.1.m1.1.1.cmml" type="integer" xref="A2.SS2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.1.m1.1d">1</annotation></semantics></math>GB of data from each of the three modalities for our models, Bellard’s online adaptive transformer <cite class="ltx_cite ltx_citemacro_citep">(Bellard, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite>, and the standard compression algorithms used in our work.
As the table clearly shows, our models and Bellard’s model are orders of magnitudes slower (let alone the increased computational demand and GPU requirements).
Note that running times for our models differ, because we pick the best model per modality, which are models of different sizes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Compressing Model Parameters</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A2.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table A3: </span>
Compression ratios for model parameters.
We losslessly compress the trained model parameters with standard compressors.
For each modality we choose the best-performing model.
As is shown, the maximal compression is <math alttext="11\%" class="ltx_Math" display="inline" id="A2.T3.2.m1.1"><semantics id="A2.T3.2.m1.1b"><mrow id="A2.T3.2.m1.1.1" xref="A2.T3.2.m1.1.1.cmml"><mn id="A2.T3.2.m1.1.1.2" xref="A2.T3.2.m1.1.1.2.cmml">11</mn><mo id="A2.T3.2.m1.1.1.1" xref="A2.T3.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.2.m1.1c"><apply id="A2.T3.2.m1.1.1.cmml" xref="A2.T3.2.m1.1.1"><csymbol cd="latexml" id="A2.T3.2.m1.1.1.1.cmml" xref="A2.T3.2.m1.1.1.1">percent</csymbol><cn id="A2.T3.2.m1.1.1.2.cmml" type="integer" xref="A2.T3.2.m1.1.1.2">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.2.m1.1d">11\%</annotation><annotation encoding="application/x-llamapun" id="A2.T3.2.m1.1e">11 %</annotation></semantics></math>, which would affect the overall compression ratio on the corresponding evaluation data only very marginally.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T3.3" style="width:195.1pt;height:120.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(10.1pt,-6.2pt) scale(1.11501282825398,1.11501282825398) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T3.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T3.3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A2.T3.3.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A2.T3.3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T3.3.1.1.1.2.1">Model Parameter</span></th>
</tr>
<tr class="ltx_tr" id="A2.T3.3.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="A2.T3.3.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2" id="A2.T3.3.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A2.T3.3.1.2.2.2.1">Compression Ratio</span></th>
</tr>
<tr class="ltx_tr" id="A2.T3.3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="A2.T3.3.1.3.3.1"><span class="ltx_text ltx_font_bold" id="A2.T3.3.1.3.3.1.1">Evaluation Modality</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T3.3.1.3.3.2"><span class="ltx_text ltx_font_bold" id="A2.T3.3.1.3.3.2.1">gzip</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T3.3.1.3.3.3"><span class="ltx_text ltx_font_bold" id="A2.T3.3.1.3.3.3.1">LZMA2</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T3.3.1.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T3.3.1.4.1.1">Audio</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T3.3.1.4.1.2">0.93</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="A2.T3.3.1.4.1.3">0.90</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.1.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.3.1.5.2.1">Image</th>
<td class="ltx_td ltx_align_right" id="A2.T3.3.1.5.2.2">0.93</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T3.3.1.5.2.3">0.90</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.1.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T3.3.1.6.3.1">Text</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T3.3.1.6.3.2">0.92</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" id="A2.T3.3.1.6.3.3">0.89</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.1">Throughout our paper we report compression rates that take uncompressed model parameters into account. As discussed in the main paper, compression ratios could be improved by also compressing model parameters. However, as <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.T3" title="In B.3 Compressing Model Parameters ‣ Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">A3</span></a> shows, naively compressing model parameters with a lossless compressor does not lead to much compression, which would translate into very marginal gains on the overall compression ratio. While it is possible to investigate more sophisticated compression schemes, in particular lossy compression of network weights (though this opens the problem of having to solve a trade-off between increasing weight compression and maintaining compression performance), this is beyond the scope of our paper. Accordingly, our compression rates can be understood as (somewhat) conservative estimates that give (in our case fairly tight) upper bounds on compression performance. The topic of compressing network weights to achieve competitive compression ratios would be of greater significance in a regime where models are significantly larger than ours (but the evaluation data stays roughly at the same size).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Scaling Analysis for Multimodal Training</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="A2.F1.1.g1" src="https://arxiv.org/html/2410.05078v1/x7.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure A1: </span>Similar to <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get ‣ 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a> in the main paper, but here the models are trained on a uniform mixture over all three modalities (<math alttext="55" class="ltx_Math" display="inline" id="A2.F1.9.m1.1"><semantics id="A2.F1.9.m1.1b"><mn id="A2.F1.9.m1.1.1" xref="A2.F1.9.m1.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="A2.F1.9.m1.1c"><cn id="A2.F1.9.m1.1.1.cmml" type="integer" xref="A2.F1.9.m1.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.9.m1.1d">55</annotation><annotation encoding="application/x-llamapun" id="A2.F1.9.m1.1e">55</annotation></semantics></math>GB per modality).
The plot shows compression performance evaluated on the unimodal datasets as training progresses for various model- and training set sizes (models are different colors, each line is a different training set size of either <math alttext="20\%" class="ltx_Math" display="inline" id="A2.F1.10.m2.1"><semantics id="A2.F1.10.m2.1b"><mrow id="A2.F1.10.m2.1.1" xref="A2.F1.10.m2.1.1.cmml"><mn id="A2.F1.10.m2.1.1.2" xref="A2.F1.10.m2.1.1.2.cmml">20</mn><mo id="A2.F1.10.m2.1.1.1" xref="A2.F1.10.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.F1.10.m2.1c"><apply id="A2.F1.10.m2.1.1.cmml" xref="A2.F1.10.m2.1.1"><csymbol cd="latexml" id="A2.F1.10.m2.1.1.1.cmml" xref="A2.F1.10.m2.1.1.1">percent</csymbol><cn id="A2.F1.10.m2.1.1.2.cmml" type="integer" xref="A2.F1.10.m2.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.10.m2.1d">20\%</annotation><annotation encoding="application/x-llamapun" id="A2.F1.10.m2.1e">20 %</annotation></semantics></math>, <math alttext="40\%" class="ltx_Math" display="inline" id="A2.F1.11.m3.1"><semantics id="A2.F1.11.m3.1b"><mrow id="A2.F1.11.m3.1.1" xref="A2.F1.11.m3.1.1.cmml"><mn id="A2.F1.11.m3.1.1.2" xref="A2.F1.11.m3.1.1.2.cmml">40</mn><mo id="A2.F1.11.m3.1.1.1" xref="A2.F1.11.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.F1.11.m3.1c"><apply id="A2.F1.11.m3.1.1.cmml" xref="A2.F1.11.m3.1.1"><csymbol cd="latexml" id="A2.F1.11.m3.1.1.1.cmml" xref="A2.F1.11.m3.1.1.1">percent</csymbol><cn id="A2.F1.11.m3.1.1.2.cmml" type="integer" xref="A2.F1.11.m3.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.11.m3.1d">40\%</annotation><annotation encoding="application/x-llamapun" id="A2.F1.11.m3.1e">40 %</annotation></semantics></math>, <math alttext="60\%" class="ltx_Math" display="inline" id="A2.F1.12.m4.1"><semantics id="A2.F1.12.m4.1b"><mrow id="A2.F1.12.m4.1.1" xref="A2.F1.12.m4.1.1.cmml"><mn id="A2.F1.12.m4.1.1.2" xref="A2.F1.12.m4.1.1.2.cmml">60</mn><mo id="A2.F1.12.m4.1.1.1" xref="A2.F1.12.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.F1.12.m4.1c"><apply id="A2.F1.12.m4.1.1.cmml" xref="A2.F1.12.m4.1.1"><csymbol cd="latexml" id="A2.F1.12.m4.1.1.1.cmml" xref="A2.F1.12.m4.1.1.1">percent</csymbol><cn id="A2.F1.12.m4.1.1.2.cmml" type="integer" xref="A2.F1.12.m4.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.12.m4.1d">60\%</annotation><annotation encoding="application/x-llamapun" id="A2.F1.12.m4.1e">60 %</annotation></semantics></math>, <math alttext="80\%" class="ltx_Math" display="inline" id="A2.F1.13.m5.1"><semantics id="A2.F1.13.m5.1b"><mrow id="A2.F1.13.m5.1.1" xref="A2.F1.13.m5.1.1.cmml"><mn id="A2.F1.13.m5.1.1.2" xref="A2.F1.13.m5.1.1.2.cmml">80</mn><mo id="A2.F1.13.m5.1.1.1" xref="A2.F1.13.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.F1.13.m5.1c"><apply id="A2.F1.13.m5.1.1.cmml" xref="A2.F1.13.m5.1.1"><csymbol cd="latexml" id="A2.F1.13.m5.1.1.1.cmml" xref="A2.F1.13.m5.1.1.1">percent</csymbol><cn id="A2.F1.13.m5.1.1.2.cmml" type="integer" xref="A2.F1.13.m5.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.13.m5.1d">80\%</annotation><annotation encoding="application/x-llamapun" id="A2.F1.13.m5.1e">80 %</annotation></semantics></math>, and <math alttext="100\%" class="ltx_Math" display="inline" id="A2.F1.14.m6.1"><semantics id="A2.F1.14.m6.1b"><mrow id="A2.F1.14.m6.1.1" xref="A2.F1.14.m6.1.1.cmml"><mn id="A2.F1.14.m6.1.1.2" xref="A2.F1.14.m6.1.1.2.cmml">100</mn><mo id="A2.F1.14.m6.1.1.1" xref="A2.F1.14.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.F1.14.m6.1c"><apply id="A2.F1.14.m6.1.1.cmml" xref="A2.F1.14.m6.1.1"><csymbol cd="latexml" id="A2.F1.14.m6.1.1.1.cmml" xref="A2.F1.14.m6.1.1.1">percent</csymbol><cn id="A2.F1.14.m6.1.1.2.cmml" type="integer" xref="A2.F1.14.m6.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.14.m6.1d">100\%</annotation><annotation encoding="application/x-llamapun" id="A2.F1.14.m6.1e">100 %</annotation></semantics></math>).
We always train for <math alttext="2" class="ltx_Math" display="inline" id="A2.F1.15.m7.1"><semantics id="A2.F1.15.m7.1b"><mn id="A2.F1.15.m7.1.1" xref="A2.F1.15.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A2.F1.15.m7.1c"><cn id="A2.F1.15.m7.1.1.cmml" type="integer" xref="A2.F1.15.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.15.m7.1d">2</annotation><annotation encoding="application/x-llamapun" id="A2.F1.15.m7.1e">2</annotation></semantics></math> epochs, regardless of dataset size, i.e., smaller datasets require fewer FLOPS.
In contrast to <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get ‣ 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>, where models are trained on unimodal data, we observe no overfitting, e.g., on images, even for the largest models tested.
Note, however, that the compression ratios are slightly worse than for unimodal training, which is in line with our other expriments that show small losses when training on multimodal data.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS4.p1">
<p class="ltx_p" id="A2.SS4.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.F1" title="In B.4 Scaling Analysis for Multimodal Training ‣ Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">A1</span></a> shows the results of simultaneously scaling dataset- and model size across training.
In contrast to the similar <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get ‣ 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a> in the main paper, where models were trained on unimodal data, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.F1" title="In B.4 Scaling Analysis for Multimodal Training ‣ Appendix B Additional Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">A1</span></a> shows models trained on multimodal data (i.e., the uniform mixture across all three modalities, with <math alttext="55" class="ltx_Math" display="inline" id="A2.SS4.p1.1.m1.1"><semantics id="A2.SS4.p1.1.m1.1a"><mn id="A2.SS4.p1.1.m1.1.1" xref="A2.SS4.p1.1.m1.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="A2.SS4.p1.1.m1.1b"><cn id="A2.SS4.p1.1.m1.1.1.cmml" type="integer" xref="A2.SS4.p1.1.m1.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS4.p1.1.m1.1c">55</annotation><annotation encoding="application/x-llamapun" id="A2.SS4.p1.1.m1.1d">55</annotation></semantics></math>GB per modality).
The multimodal training mixture acts as a regularizer, which can clearly be seen by the lack of overfitting of the largest models on images.
Compare this against the unimodal training results in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get ‣ 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a> where overfitting can be observed.
In line with our other main results in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F3" title="In Small Transformers Can Be Domain-General Compressors ‣ 5 Results ‣ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>, the overall compression ratios are slightly worse for the models trained on multimodal data compared to unimodal training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body></html>