<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2007.11858] Whole-Body Human Pose Estimation in the Wild</title><meta property="og:description" content="This paper investigates the task of 2D human whole-body pose estimation, which aims to localize dense landmarks on the entire human body including face, hands, body, and feet. As existing datasets do not have whole-bod…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Whole-Body Human Pose Estimation in the Wild">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Whole-Body Human Pose Estimation in the Wild">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2007.11858">

<!--Generated on Sat Mar  2 10:01:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Whole-body human pose estimation,  facial landmark detection,  hand keypoint estimation">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Whole-Body Human Pose Estimation 
<br class="ltx_break">in the Wild</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sheng Jin 
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5736-7434" title="ORCID identifier" class="ltx_ref">0000-0001-5736-7434</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lumin Xu
</span><span class="ltx_author_notes">3322</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jin Xu
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Can Wang
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Wentao Liu
</span><span class="ltx_author_notes">2†2†</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chen Qian
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wanli Ouyang
</span><span class="ltx_author_notes">44</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ping Luo 
<br class="ltx_break">
<sup id="id5.5.id1" class="ltx_sup"><span id="id5.5.id1.1" class="ltx_text ltx_font_italic">1</span></sup> The University of Hong Kong  <sup id="id6.6.id2" class="ltx_sup"><span id="id6.6.id2.1" class="ltx_text ltx_font_italic">2</span></sup> SenseTime Research 
<br class="ltx_break"><sup id="id7.7.id3" class="ltx_sup"><span id="id7.7.id3.1" class="ltx_text ltx_font_italic">3</span></sup> The Chinese University of Hong Kong  <sup id="id8.8.id4" class="ltx_sup"><span id="id8.8.id4.1" class="ltx_text ltx_font_italic">4</span></sup> The University of Sydney 
<br class="ltx_break"><span id="id9.9.id5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{jinsheng</span>
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> xulumin
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> wangcan
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> liuwentao
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> qianchen}@sensetime.com  wanli.ouyang@sydney.edu.au
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> pluo@cs.hku.hk
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">This paper investigates the task of 2D human whole-body pose estimation, which aims to localize dense landmarks on the entire human body including face, hands, body, and feet. As existing datasets do not have whole-body annotations, previous methods have to assemble different deep models trained independently on different datasets of the human face, hand, and body, struggling with dataset biases and large model complexity. To fill in this blank, we introduce COCO-WholeBody which extends COCO dataset with whole-body annotations. To our best knowledge, it is the first benchmark that has manual annotations on the entire human body, including 133 dense landmarks with 68 on the face, 42 on hands and 23 on the body and feet. A single-network model, named ZoomNet, is devised to take into account the hierarchical structure of the full human body to solve the scale variation of different body parts of the same person. ZoomNet is able to significantly outperform existing methods on the proposed COCO-WholeBody dataset. Extensive experiments show that COCO-WholeBody not only can be used to train deep models from scratch for whole-body pose estimation but also can serve as a powerful pre-training dataset for many different tasks such as facial landmark detection and hand keypoint estimation. The dataset is publicly available at <a target="_blank" href="https://github.com/jin-s13/COCO-WholeBody" title="" class="ltx_ref">https://github.com/jin-s13/COCO-WholeBody</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Whole-body human pose estimation, facial landmark detection, hand keypoint estimation
</div>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><sup id="footnote1.1" class="ltx_sup"><span id="footnote1.1.1" class="ltx_text ltx_font_italic">†</span></sup>Corresponding author.</span></span></span>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2007.11858/assets/images/introduction.jpg" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="328" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.3.2" class="ltx_text" style="font-size:90%;">The proposed COCO-WholeBody dataset provides manual annotations of dense landmarks on the entire human body including body, face, hands, and feet. (a) visualizes an image as an example. The whole-body human pose estimation is challenging because different body parts have different variations such as scale. (b) shows that ZoomNet significantly outperforms the prior arts on this challenging task.
(c) and (d) show that existing facial/hand landmark estimation algorithms can be improved by pretraining on COCO-WholeBody.
</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Human pose estimation has significant progress in the past few years. Recently, a more challenging task called <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">whole-body</em> pose estimation is proposed and attracts much attention. As shown in Fig. <a href="#S0.F1" title="Figure 1 ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>a., whole-body pose estimation aims at localizing keypoints of body, face, hand, and foot simultaneously. This task is important for the development of downstream applications, such as virtual reality, augmented reality, human mesh recovery, and action recognition.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In recent years, deep neural networks (DNNs) become popular for keypoint estimation. However, to our knowledge, existing datasets of human pose estimation do not have manual annotations of the entire human body.
Therefore, previous works trained their models separately on different datasets of face, hand and human body. For example, OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> combines multiple DNNs trained independently on different datasets, including one DNN for body pose estimation on COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, one DNN for face keypoint detection by combining many datasets (<em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">i.e.</em> Multi-PIE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, FRGC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and i-bug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>), and another DNN for hand keypoint detection on Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. These methods may have several drawbacks. First, the data size of the current in-the-wild datasets of 2D hand keypoints is limited. Most approaches of hand pose estimation have to use lab-recorded <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> or synthetic datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, hampering the performance of the existing methods in real-world scenarios. Second, the variations such as illumination, pose and scales in the existing human face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, hand <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, and body datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> are different, inevitably introducing dataset biases to the learned deep networks, thus hindering the development of algorithms to comprehensively consider the task as a whole.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address the above issues, we propose a novel large-scale dataset for whole-body pose estimation, named COCO-WholeBody, which fully annotates the bounding boxes of face and hand, as well as the keypoints of face, hand, and foot for the images from COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. To our knowledge, this is the first dataset that has whole-body annotations. COCO-WholeBody enables us to take into account the hierarchical structure of the human body and the correlations between different body parts to estimate the entire body pose. Therefore, it enables the development of a more reliable human body pose estimator. In addition, it will also stimulate productive research on related areas such as face and hand detection, face alignment and 2D hand pose estimation. The effectiveness of COCO-WholeBody is validated by using cross-dataset evaluation, which demonstrates that COCO-WholeBody can be used as a powerful pre-training dataset for various tasks, such as facial landmark localization and hand keypoint estimation. We overview the cross-dataset evaluations as shown in Fig.<a href="#S0.F1" title="Figure 1 ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>c., d.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The task of whole-body pose estimation has not been fully exploited in the literature because of missing a representative benchmark. Previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> are mainly the bottom-up approaches, which simultaneously detect the keypoints for all persons in an image at once. They are generally efficient, however, they might suffer from scale variance of persons, causing inferior performance for small persons. Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> found that the top-down alternatives would have higher accuracy, because top-down methods normalize the human instances to roughly the same scale and are less sensitive to the scale variance of different human instances. However, to our knowledge, there is no existing top-down approach for whole-body pose estimation. With COCO-WholeBody, we are able to fill in this blank by designing a top-down whole-body pose estimator. However, predicting all the keypoints for whole-body pose estimation will lead to inferior performance, because the scales of human body, face and hand are different. For example, human body pose estimation requires a large receptive field to handle occlusion and complex poses, while face and hand keypoint estimation requires higher image resolution for accurate localization. If all the keypoints are treated equally and directly predicted at once, the performance is suboptimal.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.2" class="ltx_p">To solve this technical problem, we propose ZoomNet to effectively handle the scale variance in whole-body pose estimation. ZoomNet follows the top-down paradigm. Given a human bounding box of each person, ZoomNet first localizes the easy-to-detect body keypoints and estimates the rough position of hands and face. Then it zooms in to focus on the hand/face areas and predicts keypoints using features with higher resolution for accurate localization. Unlike previous approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> which usually assemble multiple networks, ZoomNet has a single network that is end-to-end trainable. It unifies five network heads including the human body pose estimator, hand and face detectors, and hand and face pose estimators into a single network with shared low-level features. Extensive experiments show that ZoomNet outperforms the state-of-the-arts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> by a large margin, <em id="S1.p5.2.1" class="ltx_emph ltx_font_italic">i.e.</em> <math id="S1.p5.1.m1.1" class="ltx_Math" alttext="0.541" display="inline"><semantics id="S1.p5.1.m1.1a"><mn id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">0.541</mn><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><cn type="float" id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">0.541</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">0.541</annotation></semantics></math> vs <math id="S1.p5.2.m2.1" class="ltx_Math" alttext="0.338" display="inline"><semantics id="S1.p5.2.m2.1a"><mn id="S1.p5.2.m2.1.1" xref="S1.p5.2.m2.1.1.cmml">0.338</mn><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.1b"><cn type="float" id="S1.p5.2.m2.1.1.cmml" xref="S1.p5.2.m2.1.1">0.338</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.1c">0.338</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> for whole-body mAP on COCO-WholeBody.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our major contributions can be summarized as follows. <span id="S1.p6.1.1" class="ltx_text ltx_font_bold">(1)</span> We propose the first benchmark dataset for whole-body human pose estimation, termed COCO-WholeBody, which encourages more exploration of this task. To evaluate the effectiveness of COCO-WholeBody, we extensively examine the performance of several representative approaches on this dataset. Also, the generalization ability of COCO-WholeBody is validated by cross-dataset evaluations, showing that COCO-WholeBody can serve as a powerful pre-training dataset for many tasks, such as facial landmark localization and hand keypoint estimation.
<span id="S1.p6.1.2" class="ltx_text ltx_font_bold">(2)</span> We propose a top-down single-network model, ZoomNet to solve the scale variance of different body parts in a single person. Extensive experiments show that the proposed method significantly outperforms previous state-of-the-arts.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>2D Keypoint Localization Dataset</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">As shown in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 2D Keypoint Localization Dataset ‣ 2 Related Work ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, there are many datasets separately annotated for localizing the keypoints of body <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, hand <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> or face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. These datasets are briefly discussed in this section.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.4.1.1" class="ltx_text" style="font-size:129%;">Table 1</span>: </span><span id="S2.T1.5.2" class="ltx_text" style="font-size:129%;">Overview of some popular public datasets for 2D keypoint estimation in RGB images. Kpt stands for keypoints, and #Kpt means the annotated number. “Wild” denotes whether the dataset is collected in-the-wild. * means head box.</span></figcaption>
<table id="S2.T1.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.6.1.1" class="ltx_tr">
<td id="S2.T1.6.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.1.1.1.1" class="ltx_text" style="font-size:70%;">DataSet</span></td>
<td id="S2.T1.6.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.1.1.2.1" class="ltx_text" style="font-size:70%;">Images</span></td>
<td id="S2.T1.6.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.1.1.3.1" class="ltx_text" style="font-size:70%;">#Kpt</span></td>
<td id="S2.T1.6.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.1.1.4.1" class="ltx_text" style="font-size:70%;">Wild</span></td>
<td id="S2.T1.6.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.1.1.5.1" class="ltx_text" style="font-size:70%;">Body</span></td>
<td id="S2.T1.6.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.1.1.6.1" class="ltx_text" style="font-size:70%;">Hand</span></td>
<td id="S2.T1.6.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.1.1.7.1" class="ltx_text" style="font-size:70%;">Face</span></td>
<td id="S2.T1.6.1.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.1.1.8.1" class="ltx_text" style="font-size:70%;">Body</span></td>
<td id="S2.T1.6.1.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.1.1.9.1" class="ltx_text" style="font-size:70%;">Hand</span></td>
<td id="S2.T1.6.1.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.1.1.10.1" class="ltx_text" style="font-size:70%;">Face</span></td>
<td id="S2.T1.6.1.1.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.1.1.11.1" class="ltx_text" style="font-size:70%;">Total</span></td>
</tr>
<tr id="S2.T1.6.2.2" class="ltx_tr">
<td id="S2.T1.6.2.2.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.2.2.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.2.2.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.2.2.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.2.2.5" class="ltx_td ltx_align_center"><span id="S2.T1.6.2.2.5.1" class="ltx_text" style="font-size:70%;">Box</span></td>
<td id="S2.T1.6.2.2.6" class="ltx_td ltx_align_center"><span id="S2.T1.6.2.2.6.1" class="ltx_text" style="font-size:70%;">Box</span></td>
<td id="S2.T1.6.2.2.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.2.2.7.1" class="ltx_text" style="font-size:70%;">Box</span></td>
<td id="S2.T1.6.2.2.8" class="ltx_td ltx_align_center"><span id="S2.T1.6.2.2.8.1" class="ltx_text" style="font-size:70%;">Kpt</span></td>
<td id="S2.T1.6.2.2.9" class="ltx_td ltx_align_center"><span id="S2.T1.6.2.2.9.1" class="ltx_text" style="font-size:70%;">Kpt</span></td>
<td id="S2.T1.6.2.2.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.2.2.10.1" class="ltx_text" style="font-size:70%;">Kpt</span></td>
<td id="S2.T1.6.2.2.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.2.2.11.1" class="ltx_text" style="font-size:70%;">Instances</span></td>
</tr>
<tr id="S2.T1.6.3.3" class="ltx_tr">
<td id="S2.T1.6.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<em id="S2.T1.6.3.3.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">MPII</em><span id="S2.T1.6.3.3.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.3.3.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S2.T1.6.3.3.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S2.T1.6.3.3.2.1" class="ltx_text" style="font-size:70%;">25K</span></td>
<td id="S2.T1.6.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S2.T1.6.3.3.3.1" class="ltx_text" style="font-size:70%;">16</span></td>
<td id="S2.T1.6.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S2.T1.6.3.3.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.3.3.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.6.3.3.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.3.3.6" class="ltx_td ltx_border_tt"></td>
<td id="S2.T1.6.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S2.T1.6.3.3.7.1" class="ltx_text" style="font-size:70%;">*</span></td>
<td id="S2.T1.6.3.3.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.6.3.3.8.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.3.3.9" class="ltx_td ltx_border_tt"></td>
<td id="S2.T1.6.3.3.10" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S2.T1.6.3.3.11" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.6.3.3.11.1" class="ltx_text" style="font-size:70%;">40K</span></td>
</tr>
<tr id="S2.T1.6.4.4" class="ltx_tr">
<td id="S2.T1.6.4.4.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.4.4.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">MPII-TRB</em><span id="S2.T1.6.4.4.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.4.4.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S2.T1.6.4.4.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.4.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.4.4.2.1" class="ltx_text" style="font-size:70%;">25K</span></td>
<td id="S2.T1.6.4.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.4.4.3.1" class="ltx_text" style="font-size:70%;">40</span></td>
<td id="S2.T1.6.4.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.4.4.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.4.4.5" class="ltx_td ltx_align_center"><span id="S2.T1.6.4.4.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.4.4.6" class="ltx_td"></td>
<td id="S2.T1.6.4.4.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.4.4.7.1" class="ltx_text" style="font-size:70%;">*</span></td>
<td id="S2.T1.6.4.4.8" class="ltx_td ltx_align_center"><span id="S2.T1.6.4.4.8.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.4.4.9" class="ltx_td"></td>
<td id="S2.T1.6.4.4.10" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.4.4.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.4.4.11.1" class="ltx_text" style="font-size:70%;">40K</span></td>
</tr>
<tr id="S2.T1.6.5.5" class="ltx_tr">
<td id="S2.T1.6.5.5.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.5.5.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">CrowdPose</em><span id="S2.T1.6.5.5.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.5.5.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S2.T1.6.5.5.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.5.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.5.5.2.1" class="ltx_text" style="font-size:70%;">20K</span></td>
<td id="S2.T1.6.5.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.5.5.3.1" class="ltx_text" style="font-size:70%;">14</span></td>
<td id="S2.T1.6.5.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.5.5.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.5.5.5" class="ltx_td ltx_align_center"><span id="S2.T1.6.5.5.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.5.5.6" class="ltx_td"></td>
<td id="S2.T1.6.5.5.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.5.5.8" class="ltx_td ltx_align_center"><span id="S2.T1.6.5.5.8.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.5.5.9" class="ltx_td"></td>
<td id="S2.T1.6.5.5.10" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.5.5.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.5.5.11.1" class="ltx_text" style="font-size:70%;">80K</span></td>
</tr>
<tr id="S2.T1.6.6.6" class="ltx_tr">
<td id="S2.T1.6.6.6.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.6.6.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">PoseTrack</em><span id="S2.T1.6.6.6.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.6.6.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S2.T1.6.6.6.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.6.6.2.1" class="ltx_text" style="font-size:70%;">23K</span></td>
<td id="S2.T1.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.6.6.3.1" class="ltx_text" style="font-size:70%;">15</span></td>
<td id="S2.T1.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.6.6.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.6.6.5" class="ltx_td ltx_align_center"><span id="S2.T1.6.6.6.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.6.6.6" class="ltx_td"></td>
<td id="S2.T1.6.6.6.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.6.6.8" class="ltx_td ltx_align_center"><span id="S2.T1.6.6.6.8.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.6.6.9" class="ltx_td"></td>
<td id="S2.T1.6.6.6.10" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.6.6.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.6.6.11.1" class="ltx_text" style="font-size:70%;">150K</span></td>
</tr>
<tr id="S2.T1.6.7.7" class="ltx_tr">
<td id="S2.T1.6.7.7.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.7.7.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">AI Challenger</em><span id="S2.T1.6.7.7.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.7.7.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib62" title="" class="ltx_ref">62</a><span id="S2.T1.6.7.7.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.7.7.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.7.7.2.1" class="ltx_text" style="font-size:70%;">300K</span></td>
<td id="S2.T1.6.7.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.7.7.3.1" class="ltx_text" style="font-size:70%;">14</span></td>
<td id="S2.T1.6.7.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.7.7.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.7.7.5" class="ltx_td ltx_align_center"><span id="S2.T1.6.7.7.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.7.7.6" class="ltx_td"></td>
<td id="S2.T1.6.7.7.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.7.7.8" class="ltx_td ltx_align_center"><span id="S2.T1.6.7.7.8.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.7.7.9" class="ltx_td"></td>
<td id="S2.T1.6.7.7.10" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.7.7.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.7.7.11.1" class="ltx_text" style="font-size:70%;">700K</span></td>
</tr>
<tr id="S2.T1.6.8.8" class="ltx_tr">
<td id="S2.T1.6.8.8.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.8.8.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">COCO</em><span id="S2.T1.6.8.8.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.8.8.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S2.T1.6.8.8.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.8.8.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.8.8.2.1" class="ltx_text" style="font-size:70%;">200K</span></td>
<td id="S2.T1.6.8.8.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.8.8.3.1" class="ltx_text" style="font-size:70%;">17</span></td>
<td id="S2.T1.6.8.8.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.8.8.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.8.8.5" class="ltx_td ltx_align_center"><span id="S2.T1.6.8.8.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.8.8.6" class="ltx_td"></td>
<td id="S2.T1.6.8.8.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.8.8.7.1" class="ltx_text" style="font-size:70%;">*</span></td>
<td id="S2.T1.6.8.8.8" class="ltx_td ltx_align_center"><span id="S2.T1.6.8.8.8.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.8.8.9" class="ltx_td"></td>
<td id="S2.T1.6.8.8.10" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.8.8.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.8.8.11.1" class="ltx_text" style="font-size:70%;">250K</span></td>
</tr>
<tr id="S2.T1.6.9.9" class="ltx_tr">
<td id="S2.T1.6.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<em id="S2.T1.6.9.9.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">OneHand10K</em><span id="S2.T1.6.9.9.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.9.9.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib60" title="" class="ltx_ref">60</a><span id="S2.T1.6.9.9.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.9.9.2.1" class="ltx_text" style="font-size:70%;">10K</span></td>
<td id="S2.T1.6.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.9.9.3.1" class="ltx_text" style="font-size:70%;">21</span></td>
<td id="S2.T1.6.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.9.9.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.9.9.5" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.6.9.9.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.9.9.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.9.9.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.6.9.9.8" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.6.9.9.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.9.9.9.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.9.9.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.6.9.9.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.9.9.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.6.10.10" class="ltx_tr">
<td id="S2.T1.6.10.10.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.10.10.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">SynthHand</em><span id="S2.T1.6.10.10.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.10.10.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S2.T1.6.10.10.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.10.10.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.10.10.2.1" class="ltx_text" style="font-size:70%;">63K</span></td>
<td id="S2.T1.6.10.10.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.10.10.3.1" class="ltx_text" style="font-size:70%;">21</span></td>
<td id="S2.T1.6.10.10.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.10.10.5" class="ltx_td"></td>
<td id="S2.T1.6.10.10.6" class="ltx_td ltx_align_center"><span id="S2.T1.6.10.10.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.10.10.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.10.10.8" class="ltx_td"></td>
<td id="S2.T1.6.10.10.9" class="ltx_td ltx_align_center"><span id="S2.T1.6.10.10.9.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.10.10.10" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.10.10.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.10.10.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.6.11.11" class="ltx_tr">
<td id="S2.T1.6.11.11.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.11.11.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">RHD</em><span id="S2.T1.6.11.11.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.11.11.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib70" title="" class="ltx_ref">70</a><span id="S2.T1.6.11.11.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.11.11.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.11.11.2.1" class="ltx_text" style="font-size:70%;">41K</span></td>
<td id="S2.T1.6.11.11.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.11.11.3.1" class="ltx_text" style="font-size:70%;">21</span></td>
<td id="S2.T1.6.11.11.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.11.11.5" class="ltx_td"></td>
<td id="S2.T1.6.11.11.6" class="ltx_td ltx_align_center"><span id="S2.T1.6.11.11.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.11.11.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.11.11.8" class="ltx_td"></td>
<td id="S2.T1.6.11.11.9" class="ltx_td ltx_align_center"><span id="S2.T1.6.11.11.9.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.11.11.10" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.11.11.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.11.11.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.6.12.12" class="ltx_tr">
<td id="S2.T1.6.12.12.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.12.12.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">FreiHand</em><span id="S2.T1.6.12.12.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.12.12.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib71" title="" class="ltx_ref">71</a><span id="S2.T1.6.12.12.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.12.12.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.12.12.2.1" class="ltx_text" style="font-size:70%;">130K</span></td>
<td id="S2.T1.6.12.12.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.12.12.3.1" class="ltx_text" style="font-size:70%;">21</span></td>
<td id="S2.T1.6.12.12.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.12.12.5" class="ltx_td"></td>
<td id="S2.T1.6.12.12.6" class="ltx_td"></td>
<td id="S2.T1.6.12.12.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.12.12.8" class="ltx_td"></td>
<td id="S2.T1.6.12.12.9" class="ltx_td ltx_align_center"><span id="S2.T1.6.12.12.9.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.12.12.10" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.12.12.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.12.12.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.6.13.13" class="ltx_tr">
<td id="S2.T1.6.13.13.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.13.13.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">MHP</em><span id="S2.T1.6.13.13.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.13.13.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S2.T1.6.13.13.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.13.13.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.13.13.2.1" class="ltx_text" style="font-size:70%;">80K</span></td>
<td id="S2.T1.6.13.13.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.13.13.3.1" class="ltx_text" style="font-size:70%;">21</span></td>
<td id="S2.T1.6.13.13.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.13.13.5" class="ltx_td"></td>
<td id="S2.T1.6.13.13.6" class="ltx_td ltx_align_center"><span id="S2.T1.6.13.13.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.13.13.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.13.13.8" class="ltx_td"></td>
<td id="S2.T1.6.13.13.9" class="ltx_td ltx_align_center"><span id="S2.T1.6.13.13.9.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.13.13.10" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.13.13.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.13.13.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.6.14.14" class="ltx_tr">
<td id="S2.T1.6.14.14.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.14.14.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">GANerated</em><span id="S2.T1.6.14.14.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.14.14.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S2.T1.6.14.14.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.14.14.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.14.14.2.1" class="ltx_text" style="font-size:70%;">330K</span></td>
<td id="S2.T1.6.14.14.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.14.14.3.1" class="ltx_text" style="font-size:70%;">21</span></td>
<td id="S2.T1.6.14.14.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.14.14.5" class="ltx_td"></td>
<td id="S2.T1.6.14.14.6" class="ltx_td"></td>
<td id="S2.T1.6.14.14.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.14.14.8" class="ltx_td"></td>
<td id="S2.T1.6.14.14.9" class="ltx_td ltx_align_center"><span id="S2.T1.6.14.14.9.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.14.14.10" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.14.14.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.14.14.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.6.15.15" class="ltx_tr">
<td id="S2.T1.6.15.15.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.15.15.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">Panoptic</em><span id="S2.T1.6.15.15.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.15.15.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib51" title="" class="ltx_ref">51</a><span id="S2.T1.6.15.15.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.15.15.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.15.15.2.1" class="ltx_text" style="font-size:70%;">15K</span></td>
<td id="S2.T1.6.15.15.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.15.15.3.1" class="ltx_text" style="font-size:70%;">21</span></td>
<td id="S2.T1.6.15.15.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.15.15.5" class="ltx_td"></td>
<td id="S2.T1.6.15.15.6" class="ltx_td ltx_align_center"><span id="S2.T1.6.15.15.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.15.15.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.15.15.8" class="ltx_td"></td>
<td id="S2.T1.6.15.15.9" class="ltx_td ltx_align_center"><span id="S2.T1.6.15.15.9.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.15.15.10" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.6.15.15.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.15.15.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.6.16.16" class="ltx_tr">
<td id="S2.T1.6.16.16.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<em id="S2.T1.6.16.16.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">WFLW</em><span id="S2.T1.6.16.16.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.16.16.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib63" title="" class="ltx_ref">63</a><span id="S2.T1.6.16.16.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.16.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.16.16.2.1" class="ltx_text" style="font-size:70%;">10K</span></td>
<td id="S2.T1.6.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.16.16.3.1" class="ltx_text" style="font-size:70%;">98</span></td>
<td id="S2.T1.6.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.16.16.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.16.16.5" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.6.16.16.6" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.6.16.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.16.16.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.16.16.8" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.6.16.16.9" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.6.16.16.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.6.16.16.10.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.16.16.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.16.16.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.6.17.17" class="ltx_tr">
<td id="S2.T1.6.17.17.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.17.17.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">AFLW</em><span id="S2.T1.6.17.17.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.17.17.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S2.T1.6.17.17.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.17.17.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.17.17.2.1" class="ltx_text" style="font-size:70%;">25K</span></td>
<td id="S2.T1.6.17.17.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.17.17.3.1" class="ltx_text" style="font-size:70%;">19</span></td>
<td id="S2.T1.6.17.17.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.17.17.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.17.17.5" class="ltx_td"></td>
<td id="S2.T1.6.17.17.6" class="ltx_td"></td>
<td id="S2.T1.6.17.17.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.17.17.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.17.17.8" class="ltx_td"></td>
<td id="S2.T1.6.17.17.9" class="ltx_td"></td>
<td id="S2.T1.6.17.17.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.17.17.10.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.17.17.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.17.17.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.6.18.18" class="ltx_tr">
<td id="S2.T1.6.18.18.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.18.18.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">COFW</em><span id="S2.T1.6.18.18.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.18.18.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S2.T1.6.18.18.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.18.18.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.18.18.2.1" class="ltx_text" style="font-size:70%;">1852</span></td>
<td id="S2.T1.6.18.18.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.18.18.3.1" class="ltx_text" style="font-size:70%;">29</span></td>
<td id="S2.T1.6.18.18.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.18.18.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.18.18.5" class="ltx_td"></td>
<td id="S2.T1.6.18.18.6" class="ltx_td"></td>
<td id="S2.T1.6.18.18.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.18.18.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.18.18.8" class="ltx_td"></td>
<td id="S2.T1.6.18.18.9" class="ltx_td"></td>
<td id="S2.T1.6.18.18.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.18.18.10.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.18.18.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.18.18.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.6.19.19" class="ltx_tr">
<td id="S2.T1.6.19.19.1" class="ltx_td ltx_align_center ltx_border_r">
<em id="S2.T1.6.19.19.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">300W</em><span id="S2.T1.6.19.19.1.2" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.19.19.1.3.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib49" title="" class="ltx_ref">49</a><span id="S2.T1.6.19.19.1.4.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.6.19.19.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.19.19.2.1" class="ltx_text" style="font-size:70%;">3837</span></td>
<td id="S2.T1.6.19.19.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.19.19.3.1" class="ltx_text" style="font-size:70%;">68</span></td>
<td id="S2.T1.6.19.19.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.19.19.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.19.19.5" class="ltx_td"></td>
<td id="S2.T1.6.19.19.6" class="ltx_td"></td>
<td id="S2.T1.6.19.19.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.19.19.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.19.19.8" class="ltx_td"></td>
<td id="S2.T1.6.19.19.9" class="ltx_td"></td>
<td id="S2.T1.6.19.19.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.6.19.19.10.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.19.19.11" class="ltx_td ltx_align_center"><span id="S2.T1.6.19.19.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.6.20.20" class="ltx_tr">
<td id="S2.T1.6.20.20.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.6.20.20.1.1" class="ltx_text" style="font-size:70%;">COCO-WholeBody</span></td>
<td id="S2.T1.6.20.20.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.6.20.20.2.1" class="ltx_text" style="font-size:70%;">200K</span></td>
<td id="S2.T1.6.20.20.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.6.20.20.3.1" class="ltx_text" style="font-size:70%;">133</span></td>
<td id="S2.T1.6.20.20.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.6.20.20.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.20.20.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S2.T1.6.20.20.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.20.20.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S2.T1.6.20.20.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.20.20.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.6.20.20.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.20.20.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S2.T1.6.20.20.8.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.20.20.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S2.T1.6.20.20.9.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.20.20.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.6.20.20.10.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S2.T1.6.20.20.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S2.T1.6.20.20.11.1" class="ltx_text" style="font-size:70%;">250K</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Body Pose Dataset.</span>
There have been several body pose datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. <em id="S2.SS1.p2.1.2" class="ltx_emph ltx_font_italic">COCO</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> is one of the most popular, which offers 17-keypoint annotations in uncontrolled conditions. Our COCO-WholeBody is an extension of COCO, with densely annotated 133 face/hand/foot keypoints. The task of whole-body pose estimation is more challenging, due to 1) higher localization accuracy required for face/hands and 2) scale variance between body and face/hands.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Hand Keypoint Dataset.</span> Most existing 2D RGB-based hand keypoint datasets are either synthetic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> or captured in the lab environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. For example, <em id="S2.SS1.p3.1.2" class="ltx_emph ltx_font_italic">Panoptic</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> is a well-known hand pose estimation dataset, recorded in the CMU’s Panoptic studio with multiview dome settings. However, it is limited to a controlled laboratory environment with a simple background. OneHand10K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> is a recent in-the-wild 2d hand pose dataset. However, the size is still limited. Our COCO-WholeBody is complementary to these RGB-based hand keypoint datasets. It contains about 100K 21-keypoint labeled hands and hand boxes that are captured in unconstrained environment. To the best of our knowledge, it is the largest in-the-wild dataset for 2D RGB-based hand keypoint estimation. It is very challenging, due to occlusion, hand-hand interaction, hand-object interaction, motion blur, and small scales.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_bold">Face Keypoint Dataset.</span> Face keypoint datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> play a crucial role for the development of facial landmark detection a.k.a. face alignment. Among them, <em id="S2.SS1.p4.1.2" class="ltx_emph ltx_font_italic">300W</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> is the most popular. It is a combination of LFPW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, AFW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, HELEN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, XM2VTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> with 68 landmarks annotated for each face image. Our proposed COCO-WholeBody follows the same annotation settings as 300W and 68 keypoints for each face are annotated. Compared to 300W, COCO-WholeBody is much larger and is more challenging as it contains more blurry and small-scale facial images (see Fig <a href="#S3.F5" title="Figure 5 ‣ 3.2 Evaluation Protocol and Evaluation Metrics ‣ 3 COCO-WholeBody Dataset ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>a.).</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p"><span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_bold">DensePose Dataset.</span>
Our work is also related to DensePose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> which provides a dense 3D surface-based representation for human shape. However, since the keypoints in DensePose are uniformly sampled, they lack specific joint articulation information and details of face/hands are missing.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Keypoints Localization Method</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_bold">Body Pose Estimation.</span> Recent multi-person body pose estimation approaches can be divided into bottom-up and top-down approaches. Bottom-up approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> first detect all the keypoints of every person in images and then group them into individuals. Top-down methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> first detect the bounding boxes and then predict the human body keypoints in each box. By resizing and cropping, top-down approaches normalize the poses to approximately the same scale. Therefore, they are more robust to human-level scale variance and recent state-of-the-arts are obtained by top-down approaches. However, direct usage of the top-down methods for whole-body pose estimation will encounter the problem of scale variance of different body parts (body vs face/hand). To tackle this problem, we propose ZoomNet, a single-network top-down approach that zooms in to the hand/face regions and predicts the hand/face keypoints using higher image resolution for accurate localization.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Face/Hand/Foot Keypoint Localization.</span>
Previous works mostly treat the tasks of face/hand/foot keypoint localization independently and solve by different models. For facial keypoint localization, cascaded networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> and multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> are widely adopted. For hand keypoint estimation, most work rely on auxiliary information such as depth information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> or multi-view <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> information. For foot keypoint estimation, Cao <em id="S2.SS2.p2.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> proposed a generic bottom-up method. In this paper, we propose ZoomNet to solve the tasks of face/hand/foot keypoint localization as a whole. It takes into account the inherent hierarchical structure of the full human body to solve the scale variation of different parts in the same person.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Whole-Body Pose Estimation.</span> Whole-body pose estimation has not been well studied in the literature due to the lack of a representative benchmark. OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> applies multiple models (body keypoint estimator) to handle different kinds of keypoints. It first detects body and foot keypoints, and estimates the hand and face position. Then it applies extra models for face and hand pose estimation. Since OpenPose relies on multiple networks, it is hard to train and suffers from increased runtime and computational complexity. Unlike OpenPose, our proposed ZoomNet is a “single-network” method as it integrates five previously separated models (human body pose estimator, hand/face detectors, and hand/face pose estimators) into a single network with shared low-level features. Recently, Hidalgo <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">et al</span>. proposes an elegant method SN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> for bottom-up whole-body keypoint estimation. SN is based on PAF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> which predicts the keypoint heatmaps for detection and part affinity maps for grouping. Since there exists no such dataset with whole-body annotations, they used a set of different datasets and carefully designed the sampling rules to train the model. However, bottom-up approaches cannot handle scale variation problem well and would have difficulty in detecting face and hand keypoints accurately. In comparison, our ZoomNet is a top-down approach that well handles the extreme scale variance problem. Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> also explore the task of monocular 3D whole-body capture. Romero <span id="S2.SS2.p3.1.3" class="ltx_text ltx_font_italic">et al</span>. proposes a generative 3D model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> to express body and hands. Xiang <span id="S2.SS2.p3.1.4" class="ltx_text ltx_font_italic">et al</span>. introduces a 3D deformable human model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> to reconstruct whole-body pose and Joo <span id="S2.SS2.p3.1.5" class="ltx_text ltx_font_italic">et al</span>. presents Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> which encompasses the expressive power for body, hands, and facial expression. Their methods still rely on OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to localize 2d body keypoints in images.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>COCO-WholeBody Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">COCO-WholeBody is the first large-scale dataset with the whole-body pose annotation available, to the best of our knowledge. In this section, we will describe the annotation protocols and some informative statistics.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2007.11858/assets/images/hand_face_example.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="51" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Annotation examples for face/hand keypoints in COCO-WholeBody.</span></figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2007.11858/assets/images/label.jpg" id="S3.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="509" height="586" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2007.11858/assets/images/statistics.jpg" id="S3.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="340" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">(a) COCO-WholeBody annotation for 133 keypoints. (b)Statistics of COCO-WholeBody. The number of annotated keypoints and boxes of left hand (lhand), right hand (rhand), face and body are reported.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Annotation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We annotate the face, hand and foot keypoints on the whole train/val set of COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> dataset and form the whole-body annotations with the original body keypoint labels together (see Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 COCO-WholeBody Dataset ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). For each person, we annotate 4 types of bounding boxes (person box, face box, left-hand box, and right-hand box) and 133 keypoints (17 for body, 6 for feet, 68 for face and 42 for hands). The face/hand box is defined as the minimal bounding rectangle of the keypoints. The keypoint annotations are illustrated in Fig. <a href="#S3.F3.sf1" title="In Figure 3 ‣ 3 COCO-WholeBody Dataset ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>. The face/hand boxes are labeled as <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">valid</em>, only if the face/hand images are clear enough for keypoint labeling. Invalid boxes may be blurry or severely occluded. We only label keypoints for <em id="S3.SS1.p1.1.2" class="ltx_emph ltx_font_italic">valid</em> boxes. Manual annotation for whole-body poses in an uncontrolled environment, especially for massive and dense hand and face keypoints, requires trained experts and enormous workload. As a rough estimate, the manual labeling cost of a professional annotator is up to: 10 min/face, 1.5 min/hand, and 10 sec/box. To speed up the annotation process, we follow the semi-automatic methodology to use a set of pre-trained models (for face and hand separately) to pre-annotate and then conduct manual correction. Foot keypoints are directly manually labeled, since its labeling cost is relatively low. Specifically, the annotation process contains the following steps:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">For each individual person, we manually label the face box, the left-hand box, and the right-hand box. The validity of the boxes is also labeled.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Quality control. The annotation quality of the boxes is guaranteed through the strict quality inspection performed by another group of the annotators.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">For each valid face/hand box, we use pre-trained face/hand keypoint detectors to produce pseudo keypoint labels. We use a combination of the publicly available datasets to train a robust face keypoint detector and a hand keypoint detector based on HRNetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Manual correction of pseudo labels and further quality control. About 28% of the hand keypoints and 6% of the face keypoints are labeled as invalid and manually corrected by human annotators. By using the semi-automatic annotation, we saw about <math id="S3.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="89\%" display="inline"><semantics id="S3.I1.i4.p1.1.m1.1a"><mrow id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml"><mn id="S3.I1.i4.p1.1.m1.1.1.2" xref="S3.I1.i4.p1.1.m1.1.1.2.cmml">89</mn><mo id="S3.I1.i4.p1.1.m1.1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><apply id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.I1.i4.p1.1.m1.1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.I1.i4.p1.1.m1.1.1.2.cmml" xref="S3.I1.i4.p1.1.m1.1.1.2">89</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">89\%</annotation></semantics></math> reduction in the time required for annotation.</p>
</div>
</li>
</ol>
<p id="S3.SS1.p1.2" class="ltx_p">To measure the annotation quality, we also had 3 annotators to label the same batch of 500 images for face/hand/foot keypoints. The standard deviation of the human annotation is calculated for each keypoint (see Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2 Evaluation Protocol and Evaluation Metrics ‣ 3 COCO-WholeBody Dataset ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>a.), which is used to calculate the normalized factor of whole-body keypoint for evaluation. For “body keypoints”, we directly use the standard deviation reported in COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation Protocol and Evaluation Metrics</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The evaluation protocol of whole-body pose estimation follows the current practices in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. All algorithms are trained on COCO-WholeBody training set and evaluated on COCO-WholeBody validation set. We use mean Average Precision (mAP) and Average Recall (mAR) for evaluation, where Object Keypoint Similarity (OKS) is used to measure the similarity between the prediction and the ground truth poses. Invalid boxes and keypoints are masked out during both training and evaluation, thus not affecting the results. The ignored regions are masked out, and only visible keypoints are considered during evaluation. As shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2 Evaluation Protocol and Evaluation Metrics ‣ 3 COCO-WholeBody Dataset ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>b., we also develop a tool for deeper performance analysis based on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> which will be provided to facilitate offline evaluation.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2007.11858/assets/images/error.jpg" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">(a) The normalized standard deviation of manual annotation for each keypoint. Body keypoints have larger manual annotation variance than face and hand keypoints. (b) An example of error diagnosis results of ZoomNet for whole-body pose estimation: jitter, inversion, swap and missing.</span></figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2007.11858/assets/images/body_face_hand_compare.jpg" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">COCO-WholeBody is challenging as it contains (a) large “scale variance” of body/face/hand, measured by the average keypoint distance, (b) more blurry face images than 300W and (c) more complex hand poses than Panoptic.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Dataset Statistics</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">Dataset Size.</span>
COCO-WholeBody is a large-scale dataset with keypoint and bounding box annotations. The number of annotated keypoints as well as boxes of left hand (lhand), right hand (rhand), face and body are shown in Fig. <a href="#S3.F3.sf2" title="In Figure 3 ‣ 3 COCO-WholeBody Dataset ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>. About 130K face and left/right hand boxes are labeled, resulting in more than 800K hand keyponits and 4M face keypoints in total.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Scale Difference.</span>
Distribution of the average keypoint distance of different parts in WholeBody Dataset is summarized in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.2 Evaluation Protocol and Evaluation Metrics ‣ 3 COCO-WholeBody Dataset ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>a. We calculate the distance between keypoint pairs in the tree-structured skeleton. Hand/face have obviously much smaller scales than body. The various scale distribution makes it challenging to localize keypoints of different human parts simultaneously.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.4" class="ltx_p"><span id="S3.SS3.p3.4.1" class="ltx_text ltx_font_bold">Facial Image “Blurriness”.</span>
Face image “blurriness” is a key factor for facial landmark localization. We choose a variation of the Laplacian method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> to measure it. Specifically, an image is first converted into a grayscale image and resized into <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="112\times 112" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mn id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">112</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">112</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><times id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">112</cn><cn type="integer" id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">112</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">112\times 112</annotation></semantics></math>. The log10 of the Laplacian of the converted image is used as the “blurriness” measurement (the higher the better). The distribution of the blurriness is shown in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.2 Evaluation Protocol and Evaluation Metrics ‣ 3 COCO-WholeBody Dataset ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>b. We find that most facial images fall in the interval between <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mn id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><cn type="integer" id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">1</annotation></semantics></math> and <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mn id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><cn type="integer" id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">3</annotation></semantics></math> and are clear enough for accurate keypoint localization. Compared with 300W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, WholeBody has a larger variance of blurriness and contains more challenging images (blurriness <math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="&lt;1" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><mrow id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.2" xref="S3.SS3.p3.4.m4.1.1.2.cmml"></mi><mo id="S3.SS3.p3.4.m4.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.cmml">&lt;</mo><mn id="S3.SS3.p3.4.m4.1.1.3" xref="S3.SS3.p3.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><apply id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1"><lt id="S3.SS3.p3.4.m4.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1"></lt><csymbol cd="latexml" id="S3.SS3.p3.4.m4.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2">absent</csymbol><cn type="integer" id="S3.SS3.p3.4.m4.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">&lt;1</annotation></semantics></math>).</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Gesture Variances for Hands.</span>
We first normalize the 2D hand poses by rotating and scaling and then cluster them into three main categories: “fist”, “palm” and “others”. Unlike most previous hand datasets that are collected in constrained environments, our WholeBody-Hand is collected in-the-wild. Compared with Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, WholeBody-Hand is more challenging as it contains a larger proportion of hand images grasping or holding objects.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">Overall, COCO-WholeBody is a large-scale dataset with great diversity, which will not only promote researches on the whole-body pose estimation but also contribute to other related areas, such as face and hand keypoint estimation.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>ZoomNet: Whole-Body Pose Estimation</h2>

<figure id="S4.F6" class="ltx_figure"><img src="/html/2007.11858/assets/images/architecture.jpg" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;"> ZoomNet is a single-network model, which consists of FeatureNet, BodyNet and Face/HandHead. FeatureNet extracts low-level shared features for BodyNet and Face/HandHead. BodyNet predicts body/foot keypoints and the approximate regions of face/hands, while Face/HandHead zooms in to these regions and predict face/hand keypoints with features of higher resolution.
</span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we will introduce our whole-body pose estimation pipeline. Given an RGB image, we follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> to use an off-the-shelf FasterRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> human detector to generate human body candidates. For each human body candidate, ZoomNet localizes the whole-body keypoints. As shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ 4 ZoomNet: Whole-Body Pose Estimation ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, ZoomNet predicts body/foot keypoints and face/hand keypoints successively in a single network, consisting of the following submodules:</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.7" class="ltx_p"><span id="S4.p2.7.1" class="ltx_text ltx_font_bold">FeatureNet</span>: the input image is processed by FeatureNet to extract shared features (<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="F1" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">​</mo><mn id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><times id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1"></times><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">𝐹</ci><cn type="integer" id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">F1</annotation></semantics></math> and <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="F2" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml">​</mo><mn id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><times id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1"></times><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">𝐹</ci><cn type="integer" id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">F2</annotation></semantics></math>). It consists of two convolutional layers, each of which downsamples the corresponding input to 1/2 resolution, and a bottleneck block for effective feature learning. The input image size is <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mn id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p2.3.m3.1.1.1" xref="S4.p2.3.m3.1.1.1.cmml">×</mo><mn id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><times id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1"></times><cn type="integer" id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">384</cn><cn type="integer" id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">384\times 288</annotation></semantics></math> and the output feature map sizes for <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="F1" display="inline"><semantics id="S4.p2.4.m4.1a"><mrow id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml"><mi id="S4.p2.4.m4.1.1.2" xref="S4.p2.4.m4.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.p2.4.m4.1.1.1" xref="S4.p2.4.m4.1.1.1.cmml">​</mo><mn id="S4.p2.4.m4.1.1.3" xref="S4.p2.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><apply id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1"><times id="S4.p2.4.m4.1.1.1.cmml" xref="S4.p2.4.m4.1.1.1"></times><ci id="S4.p2.4.m4.1.1.2.cmml" xref="S4.p2.4.m4.1.1.2">𝐹</ci><cn type="integer" id="S4.p2.4.m4.1.1.3.cmml" xref="S4.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">F1</annotation></semantics></math> and <math id="S4.p2.5.m5.1" class="ltx_Math" alttext="F2" display="inline"><semantics id="S4.p2.5.m5.1a"><mrow id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml"><mi id="S4.p2.5.m5.1.1.2" xref="S4.p2.5.m5.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.p2.5.m5.1.1.1" xref="S4.p2.5.m5.1.1.1.cmml">​</mo><mn id="S4.p2.5.m5.1.1.3" xref="S4.p2.5.m5.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><apply id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1"><times id="S4.p2.5.m5.1.1.1.cmml" xref="S4.p2.5.m5.1.1.1"></times><ci id="S4.p2.5.m5.1.1.2.cmml" xref="S4.p2.5.m5.1.1.2">𝐹</ci><cn type="integer" id="S4.p2.5.m5.1.1.3.cmml" xref="S4.p2.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">F2</annotation></semantics></math> are <math id="S4.p2.6.m6.1" class="ltx_Math" alttext="192\times 144" display="inline"><semantics id="S4.p2.6.m6.1a"><mrow id="S4.p2.6.m6.1.1" xref="S4.p2.6.m6.1.1.cmml"><mn id="S4.p2.6.m6.1.1.2" xref="S4.p2.6.m6.1.1.2.cmml">192</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p2.6.m6.1.1.1" xref="S4.p2.6.m6.1.1.1.cmml">×</mo><mn id="S4.p2.6.m6.1.1.3" xref="S4.p2.6.m6.1.1.3.cmml">144</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><apply id="S4.p2.6.m6.1.1.cmml" xref="S4.p2.6.m6.1.1"><times id="S4.p2.6.m6.1.1.1.cmml" xref="S4.p2.6.m6.1.1.1"></times><cn type="integer" id="S4.p2.6.m6.1.1.2.cmml" xref="S4.p2.6.m6.1.1.2">192</cn><cn type="integer" id="S4.p2.6.m6.1.1.3.cmml" xref="S4.p2.6.m6.1.1.3">144</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">192\times 144</annotation></semantics></math> and <math id="S4.p2.7.m7.1" class="ltx_Math" alttext="96\times 72" display="inline"><semantics id="S4.p2.7.m7.1a"><mrow id="S4.p2.7.m7.1.1" xref="S4.p2.7.m7.1.1.cmml"><mn id="S4.p2.7.m7.1.1.2" xref="S4.p2.7.m7.1.1.2.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p2.7.m7.1.1.1" xref="S4.p2.7.m7.1.1.1.cmml">×</mo><mn id="S4.p2.7.m7.1.1.3" xref="S4.p2.7.m7.1.1.3.cmml">72</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.7.m7.1b"><apply id="S4.p2.7.m7.1.1.cmml" xref="S4.p2.7.m7.1.1"><times id="S4.p2.7.m7.1.1.1.cmml" xref="S4.p2.7.m7.1.1.1"></times><cn type="integer" id="S4.p2.7.m7.1.1.2.cmml" xref="S4.p2.7.m7.1.1.2">96</cn><cn type="integer" id="S4.p2.7.m7.1.1.3.cmml" xref="S4.p2.7.m7.1.1.3">72</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.7.m7.1c">96\times 72</annotation></semantics></math>, respectively.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">BodyNet</span>: using the features extracted from FeatureNet, BodyNet predicts body/foot keypoints and face/hand bounding boxes at the same time. Each bounding box is represented by four corner points and one center point. In total, 38 keypoints are generated for each person simultaneously. BodyNet is a multi-resolution network with 38 output channels.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.3" class="ltx_p"><span id="S4.p4.3.1" class="ltx_text ltx_font_bold">HandHead and FaceHead</span>: Using face and hand bounding boxes predicted by BodyNet, we crop the features in the corresponding areas from F1 and F2. The features from F1 are resized to <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S4.p4.1.m1.1a"><mrow id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mn id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p4.1.m1.1.1.1" xref="S4.p4.1.m1.1.1.1.cmml">×</mo><mn id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><times id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1"></times><cn type="integer" id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">64</cn><cn type="integer" id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">64\times 64</annotation></semantics></math> and those from F2 are resized to <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="32\times 32" display="inline"><semantics id="S4.p4.2.m2.1a"><mrow id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml"><mn id="S4.p4.2.m2.1.1.2" xref="S4.p4.2.m2.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p4.2.m2.1.1.1" xref="S4.p4.2.m2.1.1.1.cmml">×</mo><mn id="S4.p4.2.m2.1.1.3" xref="S4.p4.2.m2.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><apply id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"><times id="S4.p4.2.m2.1.1.1.cmml" xref="S4.p4.2.m2.1.1.1"></times><cn type="integer" id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1.2">32</cn><cn type="integer" id="S4.p4.2.m2.1.1.3.cmml" xref="S4.p4.2.m2.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">32\times 32</annotation></semantics></math>. Then HandHead and FaceHead are applied to predict the heatmaps of face/hand keypoints with the output resolution of <math id="S4.p4.3.m3.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S4.p4.3.m3.1a"><mrow id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml"><mn id="S4.p4.3.m3.1.1.2" xref="S4.p4.3.m3.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p4.3.m3.1.1.1" xref="S4.p4.3.m3.1.1.1.cmml">×</mo><mn id="S4.p4.3.m3.1.1.3" xref="S4.p4.3.m3.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><apply id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1"><times id="S4.p4.3.m3.1.1.1.cmml" xref="S4.p4.3.m3.1.1.1"></times><cn type="integer" id="S4.p4.3.m3.1.1.2.cmml" xref="S4.p4.3.m3.1.1.2">64</cn><cn type="integer" id="S4.p4.3.m3.1.1.3.cmml" xref="S4.p4.3.m3.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">64\times 64</annotation></semantics></math> in parallel.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">ZoomNet can be based on any state-of-the-art network architecture. In our implementation, we choose HRNet-W32 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> as the backbone of BodyNet and HRNetV2p-W18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> as the backbone of FaceHead/HandHead. Please refer to Supplementary for more implementation details.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Localizing body keypoints and face/hand boxes with BodyNet</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Our face/hand box localization is inspired by CornerNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, which represents the object with keypoint pairs and designs a one-stage keypoint-based detector. In our case, each person has three types of bounding boxes to predict: the face box, the left-hand box, and the right-hand box. Four corner points and one center point are used to represent a box. We use 2D confidence heatmaps to encode both the human body keypoints and the corner keypoints. During inference, the bounding box is obtained by the closest bounding box of the 4 corner points.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Face/hand keypoint estimation with HandHead and FaceHead</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">Given the face/hand bounding boxes predicted by BodyNet, RoIAlign <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> is applied to extract the features of the face/hand areas from the feature maps <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="F1" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">​</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝐹</ci><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">F1</annotation></semantics></math> and <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="F2" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">​</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝐹</ci><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">F2</annotation></semantics></math> of FeatureNet. The corresponding visual features are cropped and up-scaled to a higher resolution. With the extracted features, HandHead and FaceHead are used for face and hand keypoint estimation. HandHead and FaceHead use the same network architecture (HRNet-W18). The features extracted by RoIAlign are processed by the HandHead and FaceHead separately. In this way, we are able to preserve the high-resolution for the hand/face regions, and larger receptive fields for body keypoint estimation at the same time.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation on COCO-WholeBody Dataset</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">To the best of our knowledge, there are only two existing approaches that target at the 2D whole-body pose estimation task, <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">i</span>.<span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_italic">e</span>. OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and SN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. To extensively evaluate the performance of the existing methods on the proposed COCO-WholeBody Dataset, we also build upon the existing multi-person human body pose estimation approaches, including both bottom-up (<span id="S5.SS1.p1.1.3" class="ltx_text ltx_font_italic">i</span>.<span id="S5.SS1.p1.1.4" class="ltx_text ltx_font_italic">e</span>. Partial Affinity Field (PAF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and Associate Embedding (AE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>) and top-down methods (<span id="S5.SS1.p1.1.5" class="ltx_text ltx_font_italic">i</span>.<span id="S5.SS1.p1.1.6" class="ltx_text ltx_font_italic">e</span>. HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>), and adapt them to the more challenging whole-body pose estimation task using official codes (see Supplementary for more details). For fair comparisons, we retrain all methods on COCO-WholeBody and evaluate their performance with single-scale testing as shown in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Evaluation on COCO-WholeBody Dataset ‣ 5 Experiments ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We show that our proposed ZoomNet outperforms them by a large margin.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Among these methods, SN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, PAF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, AE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> follow a one-stage paradigm and predict all the keypoints simultaneously. Interestingly, we find that in the task of whole-body pose estimation, directly learning to predict all 133 keypoints simultaneously, including body, face, hand keypoints, may harm the original body keypoint estimation accuracy. In Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Evaluation on COCO-WholeBody Dataset ‣ 5 Experiments ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, “-body” means that we only train the model on the original COCO-body keypoint (17 keypoints). We compare the body keypoint estimation results of the model learning the whole-body keypoints versus the model learning the body keypoints only. We observe considerable accuracy decrease by comparing PAF vs PAF-body (-14.3% mAP and -14.2% mAR), AE vs AE-body(-17.7% mAP and -17.0% mAR) and HRNet vs HRNet-body(-9.9% mAP and -10.0% mAR). In comparison, our proposed ZoomNet uses a two-stage framework, which decouples the body keypoint estimation and face/hand keypoint estimation. The accuracy of body keypoint estimation is less affected (-1.5% mAP and -0.7% mAR).</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> can be viewed as the <em id="S5.SS1.p3.1.1" class="ltx_emph ltx_font_italic">one-stage</em> alternative of ZoomNet, since they share the same network backbone (HRNet-W32). ZoomNet significantly outperforms HRNet by 10.9% mAP and 13.8% mAR, demonstrating the effectiveness of the “zoom-in” design for solving the scale variation.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is a <em id="S5.SS1.p4.1.1" class="ltx_emph ltx_font_italic">multi-model</em> approach, where the hand/face model and the body model are not jointly trained, leading to sub-optimal results. In addition, the hand/face boxes of OpenPose are roughly estimated by hand-crafted rules from the estimated body keypoints. Therefore, the accuracy of the hand/face boxes is limited, which will hinder hand/face pose estimation.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.4.1.1" class="ltx_text" style="font-size:129%;">Table 2</span>: </span><span id="S5.T2.5.2" class="ltx_text" style="font-size:129%;">Whole-body pose estimation results on COCO-WholeBody dataset. For fair comparisons, results are obtained using single-scale testing.</span></figcaption>
<table id="S5.T2.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.6.1.1" class="ltx_tr">
<th id="S5.T2.6.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T2.6.1.1.1.1" class="ltx_text" style="font-size:70%;">Method</span></th>
<td id="S5.T2.6.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T2.6.1.1.2.1" class="ltx_text" style="font-size:70%;">body</span></td>
<td id="S5.T2.6.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T2.6.1.1.3.1" class="ltx_text" style="font-size:70%;">foot</span></td>
<td id="S5.T2.6.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T2.6.1.1.4.1" class="ltx_text" style="font-size:70%;">face</span></td>
<td id="S5.T2.6.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T2.6.1.1.5.1" class="ltx_text" style="font-size:70%;">hand</span></td>
<td id="S5.T2.6.1.1.6" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S5.T2.6.1.1.6.1" class="ltx_text" style="font-size:70%;">whole-body</span></td>
</tr>
<tr id="S5.T2.6.2.2" class="ltx_tr">
<th id="S5.T2.6.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S5.T2.6.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.2.2.2.1" class="ltx_text" style="font-size:70%;">AP</span></td>
<td id="S5.T2.6.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.2.2.3.1" class="ltx_text" style="font-size:70%;">AR</span></td>
<td id="S5.T2.6.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.2.2.4.1" class="ltx_text" style="font-size:70%;">AP</span></td>
<td id="S5.T2.6.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.2.2.5.1" class="ltx_text" style="font-size:70%;">AR</span></td>
<td id="S5.T2.6.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.2.2.6.1" class="ltx_text" style="font-size:70%;">AP</span></td>
<td id="S5.T2.6.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.2.2.7.1" class="ltx_text" style="font-size:70%;">AR</span></td>
<td id="S5.T2.6.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.2.2.8.1" class="ltx_text" style="font-size:70%;">AP</span></td>
<td id="S5.T2.6.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.2.2.9.1" class="ltx_text" style="font-size:70%;">AR</span></td>
<td id="S5.T2.6.2.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.2.2.10.1" class="ltx_text" style="font-size:70%;">AP</span></td>
<td id="S5.T2.6.2.2.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.2.2.11.1" class="ltx_text" style="font-size:70%;">AR</span></td>
</tr>
<tr id="S5.T2.6.3.3" class="ltx_tr">
<th id="S5.T2.6.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S5.T2.6.3.3.1.1" class="ltx_text" style="font-size:70%;">OpenPose </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.6.3.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S5.T2.6.3.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T2.6.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.3.2.1" class="ltx_text" style="font-size:70%;">0.563</span></td>
<td id="S5.T2.6.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.3.3.3.1" class="ltx_text" style="font-size:70%;">0.612</span></td>
<td id="S5.T2.6.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.3.4.1" class="ltx_text" style="font-size:70%;">0.532</span></td>
<td id="S5.T2.6.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.3.3.5.1" class="ltx_text" style="font-size:70%;">0.645</span></td>
<td id="S5.T2.6.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.3.6.1" class="ltx_text" style="font-size:70%;">0.482</span></td>
<td id="S5.T2.6.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.3.3.7.1" class="ltx_text" style="font-size:70%;">0.626</span></td>
<td id="S5.T2.6.3.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.3.8.1" class="ltx_text" style="font-size:70%;">0.198</span></td>
<td id="S5.T2.6.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.3.3.9.1" class="ltx_text" style="font-size:70%;">0.342</span></td>
<td id="S5.T2.6.3.3.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.3.10.1" class="ltx_text" style="font-size:70%;">0.338</span></td>
<td id="S5.T2.6.3.3.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.3.11.1" class="ltx_text" style="font-size:70%;">0.449</span></td>
</tr>
<tr id="S5.T2.6.4.4" class="ltx_tr">
<th id="S5.T2.6.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S5.T2.6.4.4.1.1" class="ltx_text" style="font-size:70%;">SN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.6.4.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S5.T2.6.4.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T2.6.4.4.2" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.4.2.1" class="ltx_text" style="font-size:70%;">0.280</span></td>
<td id="S5.T2.6.4.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.4.4.3.1" class="ltx_text" style="font-size:70%;">0.336</span></td>
<td id="S5.T2.6.4.4.4" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.4.4.1" class="ltx_text" style="font-size:70%;">0.121</span></td>
<td id="S5.T2.6.4.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.4.4.5.1" class="ltx_text" style="font-size:70%;">0.277</span></td>
<td id="S5.T2.6.4.4.6" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.4.6.1" class="ltx_text" style="font-size:70%;">0.382</span></td>
<td id="S5.T2.6.4.4.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.4.4.7.1" class="ltx_text" style="font-size:70%;">0.440</span></td>
<td id="S5.T2.6.4.4.8" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.4.8.1" class="ltx_text" style="font-size:70%;">0.138</span></td>
<td id="S5.T2.6.4.4.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.4.4.9.1" class="ltx_text" style="font-size:70%;">0.336</span></td>
<td id="S5.T2.6.4.4.10" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.4.10.1" class="ltx_text" style="font-size:70%;">0.161</span></td>
<td id="S5.T2.6.4.4.11" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.4.11.1" class="ltx_text" style="font-size:70%;">0.209</span></td>
</tr>
<tr id="S5.T2.6.5.5" class="ltx_tr">
<th id="S5.T2.6.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S5.T2.6.5.5.1.1" class="ltx_text" style="font-size:70%;">PAF </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.6.5.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S5.T2.6.5.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T2.6.5.5.2" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.5.2.1" class="ltx_text" style="font-size:70%;">0.266</span></td>
<td id="S5.T2.6.5.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.5.5.3.1" class="ltx_text" style="font-size:70%;">0.328</span></td>
<td id="S5.T2.6.5.5.4" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.5.4.1" class="ltx_text" style="font-size:70%;">0.100</span></td>
<td id="S5.T2.6.5.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.5.5.5.1" class="ltx_text" style="font-size:70%;">0.257</span></td>
<td id="S5.T2.6.5.5.6" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.5.6.1" class="ltx_text" style="font-size:70%;">0.309</span></td>
<td id="S5.T2.6.5.5.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.5.5.7.1" class="ltx_text" style="font-size:70%;">0.362</span></td>
<td id="S5.T2.6.5.5.8" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.5.8.1" class="ltx_text" style="font-size:70%;">0.133</span></td>
<td id="S5.T2.6.5.5.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.5.5.9.1" class="ltx_text" style="font-size:70%;">0.321</span></td>
<td id="S5.T2.6.5.5.10" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.5.10.1" class="ltx_text" style="font-size:70%;">0.141</span></td>
<td id="S5.T2.6.5.5.11" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.5.11.1" class="ltx_text" style="font-size:70%;">0.185</span></td>
</tr>
<tr id="S5.T2.6.6.6" class="ltx_tr">
<th id="S5.T2.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S5.T2.6.6.6.1.1" class="ltx_text" style="font-size:70%;">PAF-body </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.6.6.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S5.T2.6.6.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T2.6.6.6.2" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.6.2.1" class="ltx_text" style="font-size:70%;">0.409</span></td>
<td id="S5.T2.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.6.6.3.1" class="ltx_text" style="font-size:70%;">0.470</span></td>
<td id="S5.T2.6.6.6.4" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.6.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.6.6.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.6.6.6" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.6.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.6.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.6.6.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.6.6.8" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.6.8.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.6.6.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.6.6.9.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.6.6.10" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.6.10.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.6.6.11" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.6.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S5.T2.6.7.7" class="ltx_tr">
<th id="S5.T2.6.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S5.T2.6.7.7.1.1" class="ltx_text" style="font-size:70%;">AE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.6.7.7.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S5.T2.6.7.7.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T2.6.7.7.2" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.7.2.1" class="ltx_text" style="font-size:70%;">0.405</span></td>
<td id="S5.T2.6.7.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.7.7.3.1" class="ltx_text" style="font-size:70%;">0.464</span></td>
<td id="S5.T2.6.7.7.4" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.7.4.1" class="ltx_text" style="font-size:70%;">0.077</span></td>
<td id="S5.T2.6.7.7.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.7.7.5.1" class="ltx_text" style="font-size:70%;">0.160</span></td>
<td id="S5.T2.6.7.7.6" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.7.6.1" class="ltx_text" style="font-size:70%;">0.477</span></td>
<td id="S5.T2.6.7.7.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.7.7.7.1" class="ltx_text" style="font-size:70%;">0.580</span></td>
<td id="S5.T2.6.7.7.8" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.7.8.1" class="ltx_text" style="font-size:70%;">0.341</span></td>
<td id="S5.T2.6.7.7.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.7.7.9.1" class="ltx_text" style="font-size:70%;">0.435</span></td>
<td id="S5.T2.6.7.7.10" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.7.10.1" class="ltx_text" style="font-size:70%;">0.274</span></td>
<td id="S5.T2.6.7.7.11" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.7.11.1" class="ltx_text" style="font-size:70%;">0.350</span></td>
</tr>
<tr id="S5.T2.6.8.8" class="ltx_tr">
<th id="S5.T2.6.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S5.T2.6.8.8.1.1" class="ltx_text" style="font-size:70%;">AE-body </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.6.8.8.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S5.T2.6.8.8.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T2.6.8.8.2" class="ltx_td ltx_align_center"><span id="S5.T2.6.8.8.2.1" class="ltx_text" style="font-size:70%;">0.582</span></td>
<td id="S5.T2.6.8.8.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.8.8.3.1" class="ltx_text" style="font-size:70%;">0.634</span></td>
<td id="S5.T2.6.8.8.4" class="ltx_td ltx_align_center"><span id="S5.T2.6.8.8.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.8.8.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.8.8.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.8.8.6" class="ltx_td ltx_align_center"><span id="S5.T2.6.8.8.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.8.8.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.8.8.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.8.8.8" class="ltx_td ltx_align_center"><span id="S5.T2.6.8.8.8.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.8.8.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.8.8.9.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.8.8.10" class="ltx_td ltx_align_center"><span id="S5.T2.6.8.8.10.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.8.8.11" class="ltx_td ltx_align_center"><span id="S5.T2.6.8.8.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S5.T2.6.9.9" class="ltx_tr">
<th id="S5.T2.6.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S5.T2.6.9.9.1.1" class="ltx_text" style="font-size:70%;">HRNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.6.9.9.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="S5.T2.6.9.9.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T2.6.9.9.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.9.9.2.1" class="ltx_text" style="font-size:70%;">0.659</span></td>
<td id="S5.T2.6.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.9.9.3.1" class="ltx_text" style="font-size:70%;">0.709</span></td>
<td id="S5.T2.6.9.9.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.9.9.4.1" class="ltx_text" style="font-size:70%;">0.314</span></td>
<td id="S5.T2.6.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.9.9.5.1" class="ltx_text" style="font-size:70%;">0.424</span></td>
<td id="S5.T2.6.9.9.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.9.9.6.1" class="ltx_text" style="font-size:70%;">0.523</span></td>
<td id="S5.T2.6.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.9.9.7.1" class="ltx_text" style="font-size:70%;">0.582</span></td>
<td id="S5.T2.6.9.9.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.9.9.8.1" class="ltx_text" style="font-size:70%;">0.300</span></td>
<td id="S5.T2.6.9.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.6.9.9.9.1" class="ltx_text" style="font-size:70%;">0.363</span></td>
<td id="S5.T2.6.9.9.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.9.9.10.1" class="ltx_text" style="font-size:70%;">0.432</span></td>
<td id="S5.T2.6.9.9.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.9.9.11.1" class="ltx_text" style="font-size:70%;">0.520</span></td>
</tr>
<tr id="S5.T2.6.10.10" class="ltx_tr">
<th id="S5.T2.6.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S5.T2.6.10.10.1.1" class="ltx_text" style="font-size:70%;">HRNet-body </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.6.10.10.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="S5.T2.6.10.10.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T2.6.10.10.2" class="ltx_td ltx_align_center"><span id="S5.T2.6.10.10.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.758</span></td>
<td id="S5.T2.6.10.10.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.10.10.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.809</span></td>
<td id="S5.T2.6.10.10.4" class="ltx_td ltx_align_center"><span id="S5.T2.6.10.10.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.10.10.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.10.10.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.10.10.6" class="ltx_td ltx_align_center"><span id="S5.T2.6.10.10.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.10.10.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.10.10.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.10.10.8" class="ltx_td ltx_align_center"><span id="S5.T2.6.10.10.8.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.10.10.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.6.10.10.9.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.10.10.10" class="ltx_td ltx_align_center"><span id="S5.T2.6.10.10.10.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T2.6.10.10.11" class="ltx_td ltx_align_center"><span id="S5.T2.6.10.10.11.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S5.T2.6.11.11" class="ltx_tr">
<th id="S5.T2.6.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T2.6.11.11.1.1" class="ltx_text" style="font-size:70%;">ZoomNet</span></th>
<td id="S5.T2.6.11.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.11.11.2.1" class="ltx_text" style="font-size:70%;">0.743</span></td>
<td id="S5.T2.6.11.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T2.6.11.11.3.1" class="ltx_text" style="font-size:70%;">0.802</span></td>
<td id="S5.T2.6.11.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.11.11.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.798</span></td>
<td id="S5.T2.6.11.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T2.6.11.11.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.869</span></td>
<td id="S5.T2.6.11.11.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.11.11.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.623</span></td>
<td id="S5.T2.6.11.11.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T2.6.11.11.7.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.701</span></td>
<td id="S5.T2.6.11.11.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.11.11.8.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.401</span></td>
<td id="S5.T2.6.11.11.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T2.6.11.11.9.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.498</span></td>
<td id="S5.T2.6.11.11.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.11.11.10.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.541</span></td>
<td id="S5.T2.6.11.11.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.11.11.11.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.658</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p"><span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold">Model complexity analysis.</span>
The model complexity of ZoomNet is 27.36G Flops. By contrast, the model complexity of OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is 451.09G Flops in total (137.52G for BodyNet, 106.77G for FaceNet and <math id="S5.SS1.p5.1.m1.1" class="ltx_Math" alttext="103.40\times 2=206.80" display="inline"><semantics id="S5.SS1.p5.1.m1.1a"><mrow id="S5.SS1.p5.1.m1.1.1" xref="S5.SS1.p5.1.m1.1.1.cmml"><mrow id="S5.SS1.p5.1.m1.1.1.2" xref="S5.SS1.p5.1.m1.1.1.2.cmml"><mn id="S5.SS1.p5.1.m1.1.1.2.2" xref="S5.SS1.p5.1.m1.1.1.2.2.cmml">103.40</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p5.1.m1.1.1.2.1" xref="S5.SS1.p5.1.m1.1.1.2.1.cmml">×</mo><mn id="S5.SS1.p5.1.m1.1.1.2.3" xref="S5.SS1.p5.1.m1.1.1.2.3.cmml">2</mn></mrow><mo id="S5.SS1.p5.1.m1.1.1.1" xref="S5.SS1.p5.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p5.1.m1.1.1.3" xref="S5.SS1.p5.1.m1.1.1.3.cmml">206.80</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.1.m1.1b"><apply id="S5.SS1.p5.1.m1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1"><eq id="S5.SS1.p5.1.m1.1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1.1"></eq><apply id="S5.SS1.p5.1.m1.1.1.2.cmml" xref="S5.SS1.p5.1.m1.1.1.2"><times id="S5.SS1.p5.1.m1.1.1.2.1.cmml" xref="S5.SS1.p5.1.m1.1.1.2.1"></times><cn type="float" id="S5.SS1.p5.1.m1.1.1.2.2.cmml" xref="S5.SS1.p5.1.m1.1.1.2.2">103.40</cn><cn type="integer" id="S5.SS1.p5.1.m1.1.1.2.3.cmml" xref="S5.SS1.p5.1.m1.1.1.2.3">2</cn></apply><cn type="float" id="S5.SS1.p5.1.m1.1.1.3.cmml" xref="S5.SS1.p5.1.m1.1.1.3">206.80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.1.m1.1c">103.40\times 2=206.80</annotation></semantics></math>G for HandNet), and that of SN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is 272.30G Flops. We also report the average runtime cost on COCO-WholeBody on one GTX-1080 GPU. SN is about 215.5ms/image, while ZoomNet is about 174.7ms/image on average (including a Faster RCNN human detector which takes about 106ms/image).</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Cross-dataset Evaluation</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this section, we show that the proposed COCO-WholeBody is complementary to other separately labeled benchmarks by evaluating its generalization ability.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">WholeBody-Face (WBF) Dataset.</span>
We build WholeBody-Face (WBF) by extracting cropped face images/annotations from COCO-WholeBody. We conduct experiments on 300W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> benchmark. We follow the common settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> to train models on 3,148 training images, validate on the “common” set and evaluate on the “challenging”, “full” and “test” sets. We use the normalized mean error (NME) for evaluation and inter-ocular distance as normalization. The results are shown in Table <a href="#S5.T3.st1" title="In Table 3 ‣ 5.2 Cross-dataset Evaluation ‣ 5 Experiments ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>. HR-Ours is our implementation of HRNetV2-W18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> (HR). <sup id="S5.SS2.p2.1.2" class="ltx_sup"><span id="S5.SS2.p2.1.2.1" class="ltx_text ltx_font_italic">∗</span></sup>HR-Ours is obtained by training HR on WBF only and directly testing on 300W, which already outperforms RCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. After finetuning on 300W, it gets significantly better performance on “challenging” (4.73 vs 5.15), “full” (3.21 vs 3.33) and “test” (3.68 vs 3.91) than the prior arts.</p>
</div>
<figure id="S5.T3" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.9.4.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.6.3" class="ltx_text" style="font-size:90%;">(a) Facial landmark localization (NME) on <math id="S5.T3.4.1.m1.1" class="ltx_Math" alttext="300" display="inline"><semantics id="S5.T3.4.1.m1.1b"><mn id="S5.T3.4.1.m1.1.1" xref="S5.T3.4.1.m1.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="S5.T3.4.1.m1.1c"><cn type="integer" id="S5.T3.4.1.m1.1.1.cmml" xref="S5.T3.4.1.m1.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.1.m1.1d">300</annotation></semantics></math>W: “common” (for val), “challenging”, “full” and “test”. <sup id="S5.T3.6.3.1" class="ltx_sup"><span id="S5.T3.6.3.1.1" class="ltx_text ltx_font_italic">∗</span></sup> means only training on WBF. <math id="S5.T3.6.3.m3.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.6.3.m3.1b"><mo stretchy="false" id="S5.T3.6.3.m3.1.1" xref="S5.T3.6.3.m3.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.3.m3.1c"><ci id="S5.T3.6.3.m3.1.1.cmml" xref="S5.T3.6.3.m3.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.3.m3.1d">\downarrow</annotation></semantics></math> means lower is better. (b) Cross-dataset evaluation results of HR. Different training and testing settings are evaluated on two datasets: WBH and Panoptic (Pano.) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T3.st1" class="ltx_table ltx_figure_panel">
<table id="S5.T3.st1.36" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.st1.4.4" class="ltx_tr">
<th id="S5.T3.st1.4.4.5" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"></th>
<th id="S5.T3.st1.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.4.4.6.1" class="ltx_text" style="font-size:70%;">extra.</span></th>
<td id="S5.T3.st1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st1.1.1.1.1" class="ltx_text" style="font-size:70%;">comm. </span><math id="S5.T3.st1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.st1.1.1.1.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T3.st1.1.1.1.m1.1.1" xref="S5.T3.st1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.st1.1.1.1.m1.1b"><ci id="S5.T3.st1.1.1.1.m1.1.1.cmml" xref="S5.T3.st1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.st1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st1.2.2.2.1" class="ltx_text" style="font-size:70%;">chall. </span><math id="S5.T3.st1.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.st1.2.2.2.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T3.st1.2.2.2.m1.1.1" xref="S5.T3.st1.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.st1.2.2.2.m1.1b"><ci id="S5.T3.st1.2.2.2.m1.1.1.cmml" xref="S5.T3.st1.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.st1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st1.3.3.3.1" class="ltx_text" style="font-size:70%;">full </span><math id="S5.T3.st1.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.st1.3.3.3.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T3.st1.3.3.3.m1.1.1" xref="S5.T3.st1.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.st1.3.3.3.m1.1b"><ci id="S5.T3.st1.3.3.3.m1.1.1.cmml" xref="S5.T3.st1.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.st1.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st1.4.4.4.1" class="ltx_text" style="font-size:70%;">test </span><math id="S5.T3.st1.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.st1.4.4.4.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T3.st1.4.4.4.m1.1.1" xref="S5.T3.st1.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.st1.4.4.4.m1.1b"><ci id="S5.T3.st1.4.4.4.m1.1.1.cmml" xref="S5.T3.st1.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T3.st1.7.7" class="ltx_tr">
<th id="S5.T3.st1.7.7.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st1.7.7.4.1" class="ltx_text" style="font-size:70%;">RCN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.st1.7.7.4.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S5.T3.st1.7.7.4.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S5.T3.st1.7.7.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.7.7.5.1" class="ltx_text" style="font-size:70%;">-</span></th>
<td id="S5.T3.st1.5.5.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.5.5.1.m1.1" class="ltx_Math" alttext="4.67" display="inline"><semantics id="S5.T3.st1.5.5.1.m1.1a"><mn mathsize="70%" id="S5.T3.st1.5.5.1.m1.1.1" xref="S5.T3.st1.5.5.1.m1.1.1.cmml">4.67</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.5.5.1.m1.1b"><cn type="float" id="S5.T3.st1.5.5.1.m1.1.1.cmml" xref="S5.T3.st1.5.5.1.m1.1.1">4.67</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.5.5.1.m1.1c">4.67</annotation></semantics></math></td>
<td id="S5.T3.st1.6.6.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.6.6.2.m1.1" class="ltx_Math" alttext="8.44" display="inline"><semantics id="S5.T3.st1.6.6.2.m1.1a"><mn mathsize="70%" id="S5.T3.st1.6.6.2.m1.1.1" xref="S5.T3.st1.6.6.2.m1.1.1.cmml">8.44</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.6.6.2.m1.1b"><cn type="float" id="S5.T3.st1.6.6.2.m1.1.1.cmml" xref="S5.T3.st1.6.6.2.m1.1.1">8.44</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.6.6.2.m1.1c">8.44</annotation></semantics></math></td>
<td id="S5.T3.st1.7.7.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.7.7.3.m1.1" class="ltx_Math" alttext="5.41" display="inline"><semantics id="S5.T3.st1.7.7.3.m1.1a"><mn mathsize="70%" id="S5.T3.st1.7.7.3.m1.1.1" xref="S5.T3.st1.7.7.3.m1.1.1.cmml">5.41</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.7.7.3.m1.1b"><cn type="float" id="S5.T3.st1.7.7.3.m1.1.1.cmml" xref="S5.T3.st1.7.7.3.m1.1.1">5.41</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.7.7.3.m1.1c">5.41</annotation></semantics></math></td>
<td id="S5.T3.st1.7.7.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.7.7.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S5.T3.st1.11.11" class="ltx_tr">
<th id="S5.T3.st1.11.11.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st1.11.11.5.1" class="ltx_text" style="font-size:70%;">DAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.st1.11.11.5.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S5.T3.st1.11.11.5.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S5.T3.st1.11.11.6" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.11.11.6.1" class="ltx_text" style="font-size:70%;">-</span></th>
<td id="S5.T3.st1.8.8.1" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.8.8.1.m1.1" class="ltx_Math" alttext="3.19" display="inline"><semantics id="S5.T3.st1.8.8.1.m1.1a"><mn mathsize="70%" id="S5.T3.st1.8.8.1.m1.1.1" xref="S5.T3.st1.8.8.1.m1.1.1.cmml">3.19</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.8.8.1.m1.1b"><cn type="float" id="S5.T3.st1.8.8.1.m1.1.1.cmml" xref="S5.T3.st1.8.8.1.m1.1.1">3.19</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.8.8.1.m1.1c">3.19</annotation></semantics></math></td>
<td id="S5.T3.st1.9.9.2" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.9.9.2.m1.1" class="ltx_Math" alttext="5.24" display="inline"><semantics id="S5.T3.st1.9.9.2.m1.1a"><mn mathsize="70%" id="S5.T3.st1.9.9.2.m1.1.1" xref="S5.T3.st1.9.9.2.m1.1.1.cmml">5.24</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.9.9.2.m1.1b"><cn type="float" id="S5.T3.st1.9.9.2.m1.1.1.cmml" xref="S5.T3.st1.9.9.2.m1.1.1">5.24</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.9.9.2.m1.1c">5.24</annotation></semantics></math></td>
<td id="S5.T3.st1.10.10.3" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.10.10.3.m1.1" class="ltx_Math" alttext="3.59" display="inline"><semantics id="S5.T3.st1.10.10.3.m1.1a"><mn mathsize="70%" id="S5.T3.st1.10.10.3.m1.1.1" xref="S5.T3.st1.10.10.3.m1.1.1.cmml">3.59</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.10.10.3.m1.1b"><cn type="float" id="S5.T3.st1.10.10.3.m1.1.1.cmml" xref="S5.T3.st1.10.10.3.m1.1.1">3.59</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.10.10.3.m1.1c">3.59</annotation></semantics></math></td>
<td id="S5.T3.st1.11.11.4" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.11.11.4.m1.1" class="ltx_Math" alttext="4.30" display="inline"><semantics id="S5.T3.st1.11.11.4.m1.1a"><mn mathsize="70%" id="S5.T3.st1.11.11.4.m1.1.1" xref="S5.T3.st1.11.11.4.m1.1.1.cmml">4.30</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.11.11.4.m1.1b"><cn type="float" id="S5.T3.st1.11.11.4.m1.1.1.cmml" xref="S5.T3.st1.11.11.4.m1.1.1">4.30</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.11.11.4.m1.1c">4.30</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.st1.16.16" class="ltx_tr">
<th id="S5.T3.st1.16.16.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st1.16.16.6.1" class="ltx_text" style="font-size:70%;">DCFE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.st1.16.16.6.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib59" title="" class="ltx_ref">59</a><span id="S5.T3.st1.16.16.6.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S5.T3.st1.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st1.12.12.1.1" class="ltx_text" style="font-size:70%;">w/</span><math id="S5.T3.st1.12.12.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S5.T3.st1.12.12.1.m1.1a"><mn mathsize="70%" id="S5.T3.st1.12.12.1.m1.1.1" xref="S5.T3.st1.12.12.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.12.12.1.m1.1b"><cn type="integer" id="S5.T3.st1.12.12.1.m1.1.1.cmml" xref="S5.T3.st1.12.12.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.12.12.1.m1.1c">3</annotation></semantics></math><span id="S5.T3.st1.12.12.1.2" class="ltx_text" style="font-size:70%;">D</span>
</th>
<td id="S5.T3.st1.13.13.2" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.13.13.2.m1.1" class="ltx_Math" alttext="\mathbf{2.76}" display="inline"><semantics id="S5.T3.st1.13.13.2.m1.1a"><mn class="ltx_mathvariant_bold" mathsize="70%" mathvariant="bold" id="S5.T3.st1.13.13.2.m1.1.1" xref="S5.T3.st1.13.13.2.m1.1.1.cmml">2.76</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.13.13.2.m1.1b"><cn type="float" id="S5.T3.st1.13.13.2.m1.1.1.cmml" xref="S5.T3.st1.13.13.2.m1.1.1">2.76</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.13.13.2.m1.1c">\mathbf{2.76}</annotation></semantics></math></td>
<td id="S5.T3.st1.14.14.3" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.14.14.3.m1.1" class="ltx_Math" alttext="5.22" display="inline"><semantics id="S5.T3.st1.14.14.3.m1.1a"><mn mathsize="70%" id="S5.T3.st1.14.14.3.m1.1.1" xref="S5.T3.st1.14.14.3.m1.1.1.cmml">5.22</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.14.14.3.m1.1b"><cn type="float" id="S5.T3.st1.14.14.3.m1.1.1.cmml" xref="S5.T3.st1.14.14.3.m1.1.1">5.22</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.14.14.3.m1.1c">5.22</annotation></semantics></math></td>
<td id="S5.T3.st1.15.15.4" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.15.15.4.m1.1" class="ltx_Math" alttext="3.24" display="inline"><semantics id="S5.T3.st1.15.15.4.m1.1a"><mn mathsize="70%" id="S5.T3.st1.15.15.4.m1.1.1" xref="S5.T3.st1.15.15.4.m1.1.1.cmml">3.24</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.15.15.4.m1.1b"><cn type="float" id="S5.T3.st1.15.15.4.m1.1.1.cmml" xref="S5.T3.st1.15.15.4.m1.1.1">3.24</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.15.15.4.m1.1c">3.24</annotation></semantics></math></td>
<td id="S5.T3.st1.16.16.5" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.16.16.5.m1.1" class="ltx_Math" alttext="3.88" display="inline"><semantics id="S5.T3.st1.16.16.5.m1.1a"><mn mathsize="70%" id="S5.T3.st1.16.16.5.m1.1.1" xref="S5.T3.st1.16.16.5.m1.1.1.cmml">3.88</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.16.16.5.m1.1b"><cn type="float" id="S5.T3.st1.16.16.5.m1.1.1.cmml" xref="S5.T3.st1.16.16.5.m1.1.1">3.88</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.16.16.5.m1.1c">3.88</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.st1.19.19" class="ltx_tr">
<th id="S5.T3.st1.19.19.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st1.19.19.4.1" class="ltx_text" style="font-size:70%;">LAB </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.st1.19.19.4.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib63" title="" class="ltx_ref">63</a><span id="S5.T3.st1.19.19.4.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S5.T3.st1.19.19.5" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.19.19.5.1" class="ltx_text" style="font-size:70%;">w/B</span></th>
<td id="S5.T3.st1.17.17.1" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.17.17.1.m1.1" class="ltx_Math" alttext="2.98" display="inline"><semantics id="S5.T3.st1.17.17.1.m1.1a"><mn mathsize="70%" id="S5.T3.st1.17.17.1.m1.1.1" xref="S5.T3.st1.17.17.1.m1.1.1.cmml">2.98</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.17.17.1.m1.1b"><cn type="float" id="S5.T3.st1.17.17.1.m1.1.1.cmml" xref="S5.T3.st1.17.17.1.m1.1.1">2.98</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.17.17.1.m1.1c">2.98</annotation></semantics></math></td>
<td id="S5.T3.st1.18.18.2" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.18.18.2.m1.1" class="ltx_Math" alttext="5.19" display="inline"><semantics id="S5.T3.st1.18.18.2.m1.1a"><mn mathsize="70%" id="S5.T3.st1.18.18.2.m1.1.1" xref="S5.T3.st1.18.18.2.m1.1.1.cmml">5.19</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.18.18.2.m1.1b"><cn type="float" id="S5.T3.st1.18.18.2.m1.1.1.cmml" xref="S5.T3.st1.18.18.2.m1.1.1">5.19</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.18.18.2.m1.1c">5.19</annotation></semantics></math></td>
<td id="S5.T3.st1.19.19.3" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.19.19.3.m1.1" class="ltx_Math" alttext="3.49" display="inline"><semantics id="S5.T3.st1.19.19.3.m1.1a"><mn mathsize="70%" id="S5.T3.st1.19.19.3.m1.1.1" xref="S5.T3.st1.19.19.3.m1.1.1.cmml">3.49</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.19.19.3.m1.1b"><cn type="float" id="S5.T3.st1.19.19.3.m1.1.1.cmml" xref="S5.T3.st1.19.19.3.m1.1.1">3.49</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.19.19.3.m1.1c">3.49</annotation></semantics></math></td>
<td id="S5.T3.st1.19.19.6" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.19.19.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S5.T3.st1.23.23" class="ltx_tr">
<th id="S5.T3.st1.23.23.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st1.23.23.5.1" class="ltx_text" style="font-size:70%;">HR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.st1.23.23.5.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib54" title="" class="ltx_ref">54</a><span id="S5.T3.st1.23.23.5.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S5.T3.st1.23.23.6" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.23.23.6.1" class="ltx_text" style="font-size:70%;">-</span></th>
<td id="S5.T3.st1.20.20.1" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.20.20.1.m1.1" class="ltx_Math" alttext="2.87" display="inline"><semantics id="S5.T3.st1.20.20.1.m1.1a"><mn mathsize="70%" id="S5.T3.st1.20.20.1.m1.1.1" xref="S5.T3.st1.20.20.1.m1.1.1.cmml">2.87</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.20.20.1.m1.1b"><cn type="float" id="S5.T3.st1.20.20.1.m1.1.1.cmml" xref="S5.T3.st1.20.20.1.m1.1.1">2.87</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.20.20.1.m1.1c">2.87</annotation></semantics></math></td>
<td id="S5.T3.st1.21.21.2" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.21.21.2.m1.1" class="ltx_Math" alttext="5.15" display="inline"><semantics id="S5.T3.st1.21.21.2.m1.1a"><mn mathsize="70%" id="S5.T3.st1.21.21.2.m1.1.1" xref="S5.T3.st1.21.21.2.m1.1.1.cmml">5.15</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.21.21.2.m1.1b"><cn type="float" id="S5.T3.st1.21.21.2.m1.1.1.cmml" xref="S5.T3.st1.21.21.2.m1.1.1">5.15</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.21.21.2.m1.1c">5.15</annotation></semantics></math></td>
<td id="S5.T3.st1.22.22.3" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.22.22.3.m1.1" class="ltx_Math" alttext="3.32" display="inline"><semantics id="S5.T3.st1.22.22.3.m1.1a"><mn mathsize="70%" id="S5.T3.st1.22.22.3.m1.1.1" xref="S5.T3.st1.22.22.3.m1.1.1.cmml">3.32</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.22.22.3.m1.1b"><cn type="float" id="S5.T3.st1.22.22.3.m1.1.1.cmml" xref="S5.T3.st1.22.22.3.m1.1.1">3.32</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.22.22.3.m1.1c">3.32</annotation></semantics></math></td>
<td id="S5.T3.st1.23.23.4" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.23.23.4.m1.1" class="ltx_Math" alttext="3.85" display="inline"><semantics id="S5.T3.st1.23.23.4.m1.1a"><mn mathsize="70%" id="S5.T3.st1.23.23.4.m1.1.1" xref="S5.T3.st1.23.23.4.m1.1.1.cmml">3.85</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.23.23.4.m1.1b"><cn type="float" id="S5.T3.st1.23.23.4.m1.1.1.cmml" xref="S5.T3.st1.23.23.4.m1.1.1">3.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.23.23.4.m1.1c">3.85</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.st1.28.28" class="ltx_tr">
<th id="S5.T3.st1.24.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<sup id="S5.T3.st1.24.24.1.1" class="ltx_sup"><span id="S5.T3.st1.24.24.1.1.1" class="ltx_text ltx_font_italic" style="font-size:70%;">∗</span></sup><span id="S5.T3.st1.24.24.1.2" class="ltx_text" style="font-size:70%;">HR-Ours</span>
</th>
<th id="S5.T3.st1.28.28.6" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.28.28.6.1" class="ltx_text" style="font-size:70%;">-</span></th>
<td id="S5.T3.st1.25.25.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.25.25.2.m1.1" class="ltx_Math" alttext="4.61" display="inline"><semantics id="S5.T3.st1.25.25.2.m1.1a"><mn mathsize="70%" id="S5.T3.st1.25.25.2.m1.1.1" xref="S5.T3.st1.25.25.2.m1.1.1.cmml">4.61</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.25.25.2.m1.1b"><cn type="float" id="S5.T3.st1.25.25.2.m1.1.1.cmml" xref="S5.T3.st1.25.25.2.m1.1.1">4.61</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.25.25.2.m1.1c">4.61</annotation></semantics></math></td>
<td id="S5.T3.st1.26.26.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.26.26.3.m1.1" class="ltx_Math" alttext="7.50" display="inline"><semantics id="S5.T3.st1.26.26.3.m1.1a"><mn mathsize="70%" id="S5.T3.st1.26.26.3.m1.1.1" xref="S5.T3.st1.26.26.3.m1.1.1.cmml">7.50</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.26.26.3.m1.1b"><cn type="float" id="S5.T3.st1.26.26.3.m1.1.1.cmml" xref="S5.T3.st1.26.26.3.m1.1.1">7.50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.26.26.3.m1.1c">7.50</annotation></semantics></math></td>
<td id="S5.T3.st1.27.27.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.27.27.4.m1.1" class="ltx_Math" alttext="5.17" display="inline"><semantics id="S5.T3.st1.27.27.4.m1.1a"><mn mathsize="70%" id="S5.T3.st1.27.27.4.m1.1.1" xref="S5.T3.st1.27.27.4.m1.1.1.cmml">5.17</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.27.27.4.m1.1b"><cn type="float" id="S5.T3.st1.27.27.4.m1.1.1.cmml" xref="S5.T3.st1.27.27.4.m1.1.1">5.17</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.27.27.4.m1.1c">5.17</annotation></semantics></math></td>
<td id="S5.T3.st1.28.28.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.28.28.5.m1.1" class="ltx_Math" alttext="5.66" display="inline"><semantics id="S5.T3.st1.28.28.5.m1.1a"><mn mathsize="70%" id="S5.T3.st1.28.28.5.m1.1.1" xref="S5.T3.st1.28.28.5.m1.1.1.cmml">5.66</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.28.28.5.m1.1b"><cn type="float" id="S5.T3.st1.28.28.5.m1.1.1.cmml" xref="S5.T3.st1.28.28.5.m1.1.1">5.66</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.28.28.5.m1.1c">5.66</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.st1.32.32" class="ltx_tr">
<th id="S5.T3.st1.32.32.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.32.32.5.1" class="ltx_text" style="font-size:70%;">HR-Ours</span></th>
<th id="S5.T3.st1.32.32.6" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.32.32.6.1" class="ltx_text" style="font-size:70%;">-</span></th>
<td id="S5.T3.st1.29.29.1" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.29.29.1.m1.1" class="ltx_Math" alttext="2.89" display="inline"><semantics id="S5.T3.st1.29.29.1.m1.1a"><mn mathsize="70%" id="S5.T3.st1.29.29.1.m1.1.1" xref="S5.T3.st1.29.29.1.m1.1.1.cmml">2.89</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.29.29.1.m1.1b"><cn type="float" id="S5.T3.st1.29.29.1.m1.1.1.cmml" xref="S5.T3.st1.29.29.1.m1.1.1">2.89</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.29.29.1.m1.1c">2.89</annotation></semantics></math></td>
<td id="S5.T3.st1.30.30.2" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.30.30.2.m1.1" class="ltx_Math" alttext="5.15" display="inline"><semantics id="S5.T3.st1.30.30.2.m1.1a"><mn mathsize="70%" id="S5.T3.st1.30.30.2.m1.1.1" xref="S5.T3.st1.30.30.2.m1.1.1.cmml">5.15</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.30.30.2.m1.1b"><cn type="float" id="S5.T3.st1.30.30.2.m1.1.1.cmml" xref="S5.T3.st1.30.30.2.m1.1.1">5.15</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.30.30.2.m1.1c">5.15</annotation></semantics></math></td>
<td id="S5.T3.st1.31.31.3" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.31.31.3.m1.1" class="ltx_Math" alttext="3.33" display="inline"><semantics id="S5.T3.st1.31.31.3.m1.1a"><mn mathsize="70%" id="S5.T3.st1.31.31.3.m1.1.1" xref="S5.T3.st1.31.31.3.m1.1.1.cmml">3.33</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.31.31.3.m1.1b"><cn type="float" id="S5.T3.st1.31.31.3.m1.1.1.cmml" xref="S5.T3.st1.31.31.3.m1.1.1">3.33</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.31.31.3.m1.1c">3.33</annotation></semantics></math></td>
<td id="S5.T3.st1.32.32.4" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.32.32.4.m1.1" class="ltx_Math" alttext="3.91" display="inline"><semantics id="S5.T3.st1.32.32.4.m1.1a"><mn mathsize="70%" id="S5.T3.st1.32.32.4.m1.1.1" xref="S5.T3.st1.32.32.4.m1.1.1.cmml">3.91</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.32.32.4.m1.1b"><cn type="float" id="S5.T3.st1.32.32.4.m1.1.1.cmml" xref="S5.T3.st1.32.32.4.m1.1.1">3.91</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.32.32.4.m1.1c">3.91</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.st1.36.36" class="ltx_tr">
<th id="S5.T3.st1.36.36.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.36.36.5.1" class="ltx_text" style="font-size:70%;">HR-Ours</span></th>
<th id="S5.T3.st1.36.36.6" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st1.36.36.6.1" class="ltx_text" style="font-size:70%;">WBF</span></th>
<td id="S5.T3.st1.33.33.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.33.33.1.m1.1" class="ltx_Math" alttext="2.84" display="inline"><semantics id="S5.T3.st1.33.33.1.m1.1a"><mn mathsize="70%" id="S5.T3.st1.33.33.1.m1.1.1" xref="S5.T3.st1.33.33.1.m1.1.1.cmml">2.84</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.33.33.1.m1.1b"><cn type="float" id="S5.T3.st1.33.33.1.m1.1.1.cmml" xref="S5.T3.st1.33.33.1.m1.1.1">2.84</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.33.33.1.m1.1c">2.84</annotation></semantics></math></td>
<td id="S5.T3.st1.34.34.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.34.34.2.m1.1" class="ltx_Math" alttext="\mathbf{4.73}" display="inline"><semantics id="S5.T3.st1.34.34.2.m1.1a"><mn class="ltx_mathvariant_bold" mathsize="70%" mathvariant="bold" id="S5.T3.st1.34.34.2.m1.1.1" xref="S5.T3.st1.34.34.2.m1.1.1.cmml">4.73</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.34.34.2.m1.1b"><cn type="float" id="S5.T3.st1.34.34.2.m1.1.1.cmml" xref="S5.T3.st1.34.34.2.m1.1.1">4.73</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.34.34.2.m1.1c">\mathbf{4.73}</annotation></semantics></math></td>
<td id="S5.T3.st1.35.35.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.35.35.3.m1.1" class="ltx_Math" alttext="\mathbf{3.21}" display="inline"><semantics id="S5.T3.st1.35.35.3.m1.1a"><mn class="ltx_mathvariant_bold" mathsize="70%" mathvariant="bold" id="S5.T3.st1.35.35.3.m1.1.1" xref="S5.T3.st1.35.35.3.m1.1.1.cmml">3.21</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.35.35.3.m1.1b"><cn type="float" id="S5.T3.st1.35.35.3.m1.1.1.cmml" xref="S5.T3.st1.35.35.3.m1.1.1">3.21</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.35.35.3.m1.1c">\mathbf{3.21}</annotation></semantics></math></td>
<td id="S5.T3.st1.36.36.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st1.36.36.4.m1.1" class="ltx_Math" alttext="\mathbf{3.68}" display="inline"><semantics id="S5.T3.st1.36.36.4.m1.1a"><mn class="ltx_mathvariant_bold" mathsize="70%" mathvariant="bold" id="S5.T3.st1.36.36.4.m1.1.1" xref="S5.T3.st1.36.36.4.m1.1.1.cmml">3.68</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st1.36.36.4.m1.1b"><cn type="float" id="S5.T3.st1.36.36.4.m1.1.1.cmml" xref="S5.T3.st1.36.36.4.m1.1.1">3.68</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st1.36.36.4.m1.1c">\mathbf{3.68}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.st1.39.1.1" class="ltx_text" style="font-size:129%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T3.st2" class="ltx_table ltx_figure_panel">
<table id="S5.T3.st2.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.st2.2.2" class="ltx_tr">
<th id="S5.T3.st2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.2.2.3.1" class="ltx_text" style="font-size:70%;">#</span></th>
<th id="S5.T3.st2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.2.2.4.1" class="ltx_text" style="font-size:70%;">Train-set</span></th>
<th id="S5.T3.st2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.2.2.5.1" class="ltx_text" style="font-size:70%;">Test-set</span></th>
<th id="S5.T3.st2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st2.1.1.1.1" class="ltx_text" style="font-size:70%;">EPE </span><math id="S5.T3.st2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.st2.1.1.1.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T3.st2.1.1.1.m1.1.1" xref="S5.T3.st2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.st2.1.1.1.m1.1b"><ci id="S5.T3.st2.1.1.1.m1.1.1.cmml" xref="S5.T3.st2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S5.T3.st2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st2.2.2.2.1" class="ltx_text" style="font-size:70%;">NME </span><math id="S5.T3.st2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.st2.2.2.2.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T3.st2.2.2.2.m1.1.1" xref="S5.T3.st2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.st2.2.2.2.m1.1b"><ci id="S5.T3.st2.2.2.2.m1.1.1.cmml" xref="S5.T3.st2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.st2.4.4" class="ltx_tr">
<td id="S5.T3.st2.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.4.4.3.1" class="ltx_text" style="font-size:70%;">1</span></td>
<td id="S5.T3.st2.4.4.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.4.4.4.1" class="ltx_text" style="font-size:70%;">Pano.</span></td>
<td id="S5.T3.st2.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.4.4.5.1" class="ltx_text" style="font-size:70%;">Pano.</span></td>
<td id="S5.T3.st2.3.3.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st2.3.3.1.m1.1" class="ltx_Math" alttext="{7.49}" display="inline"><semantics id="S5.T3.st2.3.3.1.m1.1a"><mn mathsize="70%" id="S5.T3.st2.3.3.1.m1.1.1" xref="S5.T3.st2.3.3.1.m1.1.1.cmml">7.49</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st2.3.3.1.m1.1b"><cn type="float" id="S5.T3.st2.3.3.1.m1.1.1.cmml" xref="S5.T3.st2.3.3.1.m1.1.1">7.49</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.3.3.1.m1.1c">{7.49}</annotation></semantics></math></td>
<td id="S5.T3.st2.4.4.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st2.4.4.2.m1.1" class="ltx_Math" alttext="{0.68}" display="inline"><semantics id="S5.T3.st2.4.4.2.m1.1a"><mn mathsize="70%" id="S5.T3.st2.4.4.2.m1.1.1" xref="S5.T3.st2.4.4.2.m1.1.1.cmml">0.68</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st2.4.4.2.m1.1b"><cn type="float" id="S5.T3.st2.4.4.2.m1.1.1.cmml" xref="S5.T3.st2.4.4.2.m1.1.1">0.68</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.4.4.2.m1.1c">{0.68}</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.st2.7.7" class="ltx_tr">
<td id="S5.T3.st2.7.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.7.7.4.1" class="ltx_text" style="font-size:70%;">2</span></td>
<td id="S5.T3.st2.5.5.1" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st2.5.5.1.1" class="ltx_text" style="font-size:70%;">WBH </span><math id="S5.T3.st2.5.5.1.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S5.T3.st2.5.5.1.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T3.st2.5.5.1.m1.1.1" xref="S5.T3.st2.5.5.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S5.T3.st2.5.5.1.m1.1b"><ci id="S5.T3.st2.5.5.1.m1.1.1.cmml" xref="S5.T3.st2.5.5.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.5.5.1.m1.1c">\Rightarrow</annotation></semantics></math><span id="S5.T3.st2.5.5.1.2" class="ltx_text" style="font-size:70%;"> Pano.</span>
</td>
<td id="S5.T3.st2.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.7.7.5.1" class="ltx_text" style="font-size:70%;">Pano.</span></td>
<td id="S5.T3.st2.6.6.2" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st2.6.6.2.m1.1" class="ltx_Math" alttext="\mathbf{7.00}" display="inline"><semantics id="S5.T3.st2.6.6.2.m1.1a"><mn class="ltx_mathvariant_bold" mathsize="70%" mathvariant="bold" id="S5.T3.st2.6.6.2.m1.1.1" xref="S5.T3.st2.6.6.2.m1.1.1.cmml">7.00</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st2.6.6.2.m1.1b"><cn type="float" id="S5.T3.st2.6.6.2.m1.1.1.cmml" xref="S5.T3.st2.6.6.2.m1.1.1">7.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.6.6.2.m1.1c">\mathbf{7.00}</annotation></semantics></math></td>
<td id="S5.T3.st2.7.7.3" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st2.7.7.3.m1.1" class="ltx_Math" alttext="\mathbf{0.63}" display="inline"><semantics id="S5.T3.st2.7.7.3.m1.1a"><mn class="ltx_mathvariant_bold" mathsize="70%" mathvariant="bold" id="S5.T3.st2.7.7.3.m1.1.1" xref="S5.T3.st2.7.7.3.m1.1.1.cmml">0.63</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st2.7.7.3.m1.1b"><cn type="float" id="S5.T3.st2.7.7.3.m1.1.1.cmml" xref="S5.T3.st2.7.7.3.m1.1.1">0.63</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.7.7.3.m1.1c">\mathbf{0.63}</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.st2.9.9" class="ltx_tr">
<td id="S5.T3.st2.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.9.9.3.1" class="ltx_text" style="font-size:70%;">3</span></td>
<td id="S5.T3.st2.9.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.9.9.4.1" class="ltx_text" style="font-size:70%;">WBH</span></td>
<td id="S5.T3.st2.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.9.9.5.1" class="ltx_text" style="font-size:70%;">WBH</span></td>
<td id="S5.T3.st2.8.8.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st2.8.8.1.m1.1" class="ltx_Math" alttext="{2.76}" display="inline"><semantics id="S5.T3.st2.8.8.1.m1.1a"><mn mathsize="70%" id="S5.T3.st2.8.8.1.m1.1.1" xref="S5.T3.st2.8.8.1.m1.1.1.cmml">2.76</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st2.8.8.1.m1.1b"><cn type="float" id="S5.T3.st2.8.8.1.m1.1.1.cmml" xref="S5.T3.st2.8.8.1.m1.1.1">2.76</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.8.8.1.m1.1c">{2.76}</annotation></semantics></math></td>
<td id="S5.T3.st2.9.9.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st2.9.9.2.m1.1" class="ltx_Math" alttext="{6.66}" display="inline"><semantics id="S5.T3.st2.9.9.2.m1.1a"><mn mathsize="70%" id="S5.T3.st2.9.9.2.m1.1.1" xref="S5.T3.st2.9.9.2.m1.1.1.cmml">6.66</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st2.9.9.2.m1.1b"><cn type="float" id="S5.T3.st2.9.9.2.m1.1.1.cmml" xref="S5.T3.st2.9.9.2.m1.1.1">6.66</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.9.9.2.m1.1c">{6.66}</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.st2.12.12" class="ltx_tr">
<td id="S5.T3.st2.12.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.12.12.4.1" class="ltx_text" style="font-size:70%;">4</span></td>
<td id="S5.T3.st2.10.10.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T3.st2.10.10.1.1" class="ltx_text" style="font-size:70%;">Pano. </span><math id="S5.T3.st2.10.10.1.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S5.T3.st2.10.10.1.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T3.st2.10.10.1.m1.1.1" xref="S5.T3.st2.10.10.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S5.T3.st2.10.10.1.m1.1b"><ci id="S5.T3.st2.10.10.1.m1.1.1.cmml" xref="S5.T3.st2.10.10.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.10.10.1.m1.1c">\Rightarrow</annotation></semantics></math><span id="S5.T3.st2.10.10.1.2" class="ltx_text" style="font-size:70%;"> WBH</span>
</td>
<td id="S5.T3.st2.12.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T3.st2.12.12.5.1" class="ltx_text" style="font-size:70%;">WBH</span></td>
<td id="S5.T3.st2.11.11.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st2.11.11.2.m1.1" class="ltx_Math" alttext="\mathbf{2.70}" display="inline"><semantics id="S5.T3.st2.11.11.2.m1.1a"><mn class="ltx_mathvariant_bold" mathsize="70%" mathvariant="bold" id="S5.T3.st2.11.11.2.m1.1.1" xref="S5.T3.st2.11.11.2.m1.1.1.cmml">2.70</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st2.11.11.2.m1.1b"><cn type="float" id="S5.T3.st2.11.11.2.m1.1.1.cmml" xref="S5.T3.st2.11.11.2.m1.1.1">2.70</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.11.11.2.m1.1c">\mathbf{2.70}</annotation></semantics></math></td>
<td id="S5.T3.st2.12.12.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;"><math id="S5.T3.st2.12.12.3.m1.1" class="ltx_Math" alttext="\mathbf{6.49}" display="inline"><semantics id="S5.T3.st2.12.12.3.m1.1a"><mn class="ltx_mathvariant_bold" mathsize="70%" mathvariant="bold" id="S5.T3.st2.12.12.3.m1.1.1" xref="S5.T3.st2.12.12.3.m1.1.1.cmml">6.49</mn><annotation-xml encoding="MathML-Content" id="S5.T3.st2.12.12.3.m1.1b"><cn type="float" id="S5.T3.st2.12.12.3.m1.1.1.cmml" xref="S5.T3.st2.12.12.3.m1.1.1">6.49</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.st2.12.12.3.m1.1c">\mathbf{6.49}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.st2.15.1.1" class="ltx_text" style="font-size:129%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.7" class="ltx_p"><span id="S5.SS2.p3.7.1" class="ltx_text ltx_font_bold">WholeBody-Hand (WBH) Dataset.</span>
For hand pose estimation, we experiment with HRNetV2-W18 (HR) on CMU Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> (Pano.), which is a standard benchmark for hand keypoint localization. We randomly split Pano <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> by a rule of 70%-30% for training and validation. We report both the end-point-error (EPE) and the normalized mean error (NME) for evaluation. In NME, the hand bounding box is used as normalization. As shown in Table <a href="#S5.T3.st2" title="In Table 3 ‣ 5.2 Cross-dataset Evaluation ‣ 5 Experiments ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>, we analyze the generalization ability of WholeBody-Hand (WBH) by comparing the (1) HR trained on Pano., (2) HR pretrained on WBH and then finetuned on Pano., (3) HR trained on WBH, and (4) HR pretrained on Pano. and then finetuned on WBH. Comparing #1 and #2, we observe that pretraining on WBH brings about <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="6.5" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mn id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">6.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><cn type="float" id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">6.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">6.5</annotation></semantics></math>% improvement (from <math id="S5.SS2.p3.2.m2.1" class="ltx_Math" alttext="7.49" display="inline"><semantics id="S5.SS2.p3.2.m2.1a"><mn id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml">7.49</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><cn type="float" id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1">7.49</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">7.49</annotation></semantics></math> to <math id="S5.SS2.p3.3.m3.1" class="ltx_Math" alttext="7.00" display="inline"><semantics id="S5.SS2.p3.3.m3.1a"><mn id="S5.SS2.p3.3.m3.1.1" xref="S5.SS2.p3.3.m3.1.1.cmml">7.00</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.3.m3.1b"><cn type="float" id="S5.SS2.p3.3.m3.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1">7.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.1c">7.00</annotation></semantics></math>) in EPE on Pano. Comparing #1 and #3, we find that WBH vs Pano. is (<math id="S5.SS2.p3.4.m4.1" class="ltx_Math" alttext="6.66" display="inline"><semantics id="S5.SS2.p3.4.m4.1a"><mn id="S5.SS2.p3.4.m4.1.1" xref="S5.SS2.p3.4.m4.1.1.cmml">6.66</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.4.m4.1b"><cn type="float" id="S5.SS2.p3.4.m4.1.1.cmml" xref="S5.SS2.p3.4.m4.1.1">6.66</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.4.m4.1c">6.66</annotation></semantics></math> vs <math id="S5.SS2.p3.5.m5.1" class="ltx_Math" alttext="0.68" display="inline"><semantics id="S5.SS2.p3.5.m5.1a"><mn id="S5.SS2.p3.5.m5.1.1" xref="S5.SS2.p3.5.m5.1.1.cmml">0.68</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.5.m5.1b"><cn type="float" id="S5.SS2.p3.5.m5.1.1.cmml" xref="S5.SS2.p3.5.m5.1.1">0.68</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.5.m5.1c">0.68</annotation></semantics></math>) NME and (<math id="S5.SS2.p3.6.m6.1" class="ltx_Math" alttext="2.76" display="inline"><semantics id="S5.SS2.p3.6.m6.1a"><mn id="S5.SS2.p3.6.m6.1.1" xref="S5.SS2.p3.6.m6.1.1.cmml">2.76</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.6.m6.1b"><cn type="float" id="S5.SS2.p3.6.m6.1.1.cmml" xref="S5.SS2.p3.6.m6.1.1">2.76</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.6.m6.1c">2.76</annotation></semantics></math> vs <math id="S5.SS2.p3.7.m7.1" class="ltx_Math" alttext="7.49" display="inline"><semantics id="S5.SS2.p3.7.m7.1a"><mn id="S5.SS2.p3.7.m7.1.1" xref="S5.SS2.p3.7.m7.1.1.cmml">7.49</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.7.m7.1b"><cn type="float" id="S5.SS2.p3.7.m7.1.1.cmml" xref="S5.SS2.p3.7.m7.1.1">7.49</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.7.m7.1c">7.49</annotation></semantics></math>) EPE, when training/testing on its own dataset. This implies that the proposed WBH is much more challenging and that hand scales in WBH are smaller.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Analysis</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p"><span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_bold">Effect of the bounding box accuracy on the keypoint estimation.</span> We experiment by replacing our predicted face/hand bounding boxes with the ground-truth bounding boxes and re-run our FaceHead/HandHead of ZoomNet to obtain the final face/hand keypoint detection result. As shown in table <a href="#S5.T4.st1" title="In Table 4 ‣ 5.3 Analysis ‣ 5 Experiments ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>, using ground truth bounding boxes (Oracle) significantly improves the mAP of face/hand/whole-body by 19.6%, 8.4% and 23.6% respectively.</p>
</div>
<figure id="S5.T4" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.3.2" class="ltx_text" style="font-size:90%;">Effect of bounding box accuracy on keypoint estimation, where Oracle means using gt boxes. (b) Effect of person scales on whole-body pose estimation.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T4.st1" class="ltx_table ltx_figure_panel">
<table id="S5.T4.st1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.st1.2.1.1" class="ltx_tr">
<th id="S5.T4.st1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.1.1.1.1" class="ltx_text" style="font-size:70%;">Method</span></th>
<th id="S5.T4.st1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;" colspan="2"><span id="S5.T4.st1.2.1.1.2.1" class="ltx_text" style="font-size:70%;">face</span></th>
<th id="S5.T4.st1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;" colspan="2"><span id="S5.T4.st1.2.1.1.3.1" class="ltx_text" style="font-size:70%;">hand</span></th>
<th id="S5.T4.st1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;" colspan="2"><span id="S5.T4.st1.2.1.1.4.1" class="ltx_text" style="font-size:70%;">whole-body</span></th>
</tr>
<tr id="S5.T4.st1.2.2.2" class="ltx_tr">
<th id="S5.T4.st1.2.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"></th>
<th id="S5.T4.st1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.2.2.2.1" class="ltx_text" style="font-size:70%;">AP</span></th>
<th id="S5.T4.st1.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.2.2.3.1" class="ltx_text" style="font-size:70%;">AR</span></th>
<th id="S5.T4.st1.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.2.2.4.1" class="ltx_text" style="font-size:70%;">AP</span></th>
<th id="S5.T4.st1.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.2.2.5.1" class="ltx_text" style="font-size:70%;">AR</span></th>
<th id="S5.T4.st1.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.2.2.6.1" class="ltx_text" style="font-size:70%;">AP</span></th>
<th id="S5.T4.st1.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.2.2.7.1" class="ltx_text" style="font-size:70%;">AR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.st1.2.3.1" class="ltx_tr">
<th id="S5.T4.st1.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.3.1.1.1" class="ltx_text" style="font-size:70%;">Oracle</span></th>
<td id="S5.T4.st1.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.3.1.2.1" class="ltx_text" style="font-size:70%;">0.819</span></td>
<td id="S5.T4.st1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.3.1.3.1" class="ltx_text" style="font-size:70%;">0.854</span></td>
<td id="S5.T4.st1.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.3.1.4.1" class="ltx_text" style="font-size:70%;">0.485</span></td>
<td id="S5.T4.st1.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.3.1.5.1" class="ltx_text" style="font-size:70%;">0.578</span></td>
<td id="S5.T4.st1.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.3.1.6.1" class="ltx_text" style="font-size:70%;">0.777</span></td>
<td id="S5.T4.st1.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.3.1.7.1" class="ltx_text" style="font-size:70%;">0.856</span></td>
</tr>
<tr id="S5.T4.st1.2.4.2" class="ltx_tr">
<th id="S5.T4.st1.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.4.2.1.1" class="ltx_text" style="font-size:70%;">Ours</span></th>
<td id="S5.T4.st1.2.4.2.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.4.2.2.1" class="ltx_text" style="font-size:70%;">0.623</span></td>
<td id="S5.T4.st1.2.4.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.4.2.3.1" class="ltx_text" style="font-size:70%;">0.701</span></td>
<td id="S5.T4.st1.2.4.2.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.4.2.4.1" class="ltx_text" style="font-size:70%;">0.401</span></td>
<td id="S5.T4.st1.2.4.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.4.2.5.1" class="ltx_text" style="font-size:70%;">0.498</span></td>
<td id="S5.T4.st1.2.4.2.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.4.2.6.1" class="ltx_text" style="font-size:70%;">0.541</span></td>
<td id="S5.T4.st1.2.4.2.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st1.2.4.2.7.1" class="ltx_text" style="font-size:70%;">0.658</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.st1.4.1.1" class="ltx_text" style="font-size:129%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T4.st2" class="ltx_table ltx_figure_panel">
<table id="S5.T4.st2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.st2.2.1.1" class="ltx_tr">
<th id="S5.T4.st2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.1.1.1.1" class="ltx_text" style="font-size:70%;">Method</span></th>
<th id="S5.T4.st2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;" colspan="2"><span id="S5.T4.st2.2.1.1.2.1" class="ltx_text" style="font-size:70%;">mAP</span></th>
<th id="S5.T4.st2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;" colspan="2"><span id="S5.T4.st2.2.1.1.3.1" class="ltx_text" style="font-size:70%;">mAR</span></th>
</tr>
<tr id="S5.T4.st2.2.2.2" class="ltx_tr">
<th id="S5.T4.st2.2.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"></th>
<th id="S5.T4.st2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">medium</span></th>
<th id="S5.T4.st2.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.2.2.3.1" class="ltx_text" style="font-size:70%;">large</span></th>
<th id="S5.T4.st2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.2.2.4.1" class="ltx_text" style="font-size:70%;">medium</span></th>
<th id="S5.T4.st2.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.2.2.5.1" class="ltx_text" style="font-size:70%;">large</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.st2.2.3.1" class="ltx_tr">
<th id="S5.T4.st2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T4.st2.2.3.1.1.1" class="ltx_text" style="font-size:70%;">PAF </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.st2.2.3.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S5.T4.st2.2.3.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T4.st2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.3.1.2.1" class="ltx_text" style="font-size:70%;">0.100</span></td>
<td id="S5.T4.st2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.3.1.3.1" class="ltx_text" style="font-size:70%;">0.220</span></td>
<td id="S5.T4.st2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.3.1.4.1" class="ltx_text" style="font-size:70%;">0.113</span></td>
<td id="S5.T4.st2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.3.1.5.1" class="ltx_text" style="font-size:70%;">0.284</span></td>
</tr>
<tr id="S5.T4.st2.2.4.2" class="ltx_tr">
<th id="S5.T4.st2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T4.st2.2.4.2.1.1" class="ltx_text" style="font-size:70%;">SN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.st2.2.4.2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S5.T4.st2.2.4.2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T4.st2.2.4.2.2" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.4.2.2.1" class="ltx_text" style="font-size:70%;">0.117</span></td>
<td id="S5.T4.st2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.4.2.3.1" class="ltx_text" style="font-size:70%;">0.252</span></td>
<td id="S5.T4.st2.2.4.2.4" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.4.2.4.1" class="ltx_text" style="font-size:70%;">0.132</span></td>
<td id="S5.T4.st2.2.4.2.5" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.4.2.5.1" class="ltx_text" style="font-size:70%;">0.315</span></td>
</tr>
<tr id="S5.T4.st2.2.5.3" class="ltx_tr">
<th id="S5.T4.st2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T4.st2.2.5.3.1.1" class="ltx_text" style="font-size:70%;">AE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.st2.2.5.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S5.T4.st2.2.5.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T4.st2.2.5.3.2" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.5.3.2.1" class="ltx_text" style="font-size:70%;">0.190</span></td>
<td id="S5.T4.st2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.5.3.3.1" class="ltx_text" style="font-size:70%;">0.401</span></td>
<td id="S5.T4.st2.2.5.3.4" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.5.3.4.1" class="ltx_text" style="font-size:70%;">0.241</span></td>
<td id="S5.T4.st2.2.5.3.5" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.5.3.5.1" class="ltx_text" style="font-size:70%;">0.499</span></td>
</tr>
<tr id="S5.T4.st2.2.6.4" class="ltx_tr">
<th id="S5.T4.st2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T4.st2.2.6.4.1.1" class="ltx_text" style="font-size:70%;">OpenPose </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.st2.2.6.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S5.T4.st2.2.6.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T4.st2.2.6.4.2" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.6.4.2.1" class="ltx_text" style="font-size:70%;">0.398</span></td>
<td id="S5.T4.st2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.6.4.3.1" class="ltx_text" style="font-size:70%;">0.302</span></td>
<td id="S5.T4.st2.2.6.4.4" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.6.4.4.1" class="ltx_text" style="font-size:70%;">0.425</span></td>
<td id="S5.T4.st2.2.6.4.5" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.6.4.5.1" class="ltx_text" style="font-size:70%;">0.484</span></td>
</tr>
<tr id="S5.T4.st2.2.7.5" class="ltx_tr">
<th id="S5.T4.st2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T4.st2.2.7.5.1.1" class="ltx_text" style="font-size:70%;">HRNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.st2.2.7.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="S5.T4.st2.2.7.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T4.st2.2.7.5.2" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.7.5.2.1" class="ltx_text" style="font-size:70%;">0.471</span></td>
<td id="S5.T4.st2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.7.5.3.1" class="ltx_text" style="font-size:70%;">0.410</span></td>
<td id="S5.T4.st2.2.7.5.4" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.7.5.4.1" class="ltx_text" style="font-size:70%;">0.538</span></td>
<td id="S5.T4.st2.2.7.5.5" class="ltx_td ltx_align_center" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.7.5.5.1" class="ltx_text" style="font-size:70%;">0.497</span></td>
</tr>
<tr id="S5.T4.st2.2.8.6" class="ltx_tr">
<th id="S5.T4.st2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.8.6.1.1" class="ltx_text" style="font-size:70%;">Ours</span></th>
<td id="S5.T4.st2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.8.6.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.594</span></td>
<td id="S5.T4.st2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.8.6.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.519</span></td>
<td id="S5.T4.st2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.8.6.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.677</span></td>
<td id="S5.T4.st2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T4.st2.2.8.6.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.635</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.st2.4.1.1" class="ltx_text" style="font-size:129%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.2" class="ltx_p"><span id="S5.SS3.p2.2.1" class="ltx_text ltx_font_bold">Effect of the person scale on whole-body pose estimation.</span>
As shown in Table <a href="#S5.T4.st2" title="In Table 4 ‣ 5.3 Analysis ‣ 5 Experiments ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a>, we investigate the effect of person scales. Interestingly, for bottom-up whole-body methods (PAF, SN and AE), the mAP for medium scale is worse than that of large scale, since they are more sensitive to the scale variance and are difficult in detecting smaller-scale people. For top-down approaches such as HRNet and ZoomNet, mAP for medium scale is better, since larger-scale person requires relatively more accurate keypoint localization. For ZoomNet, the gap between the medium and large person scale is about <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="7.5\%" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mn id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">7.5</mn><mo id="S5.SS3.p2.1.m1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">7.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">7.5\%</annotation></semantics></math> mAP and <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="4.2\%" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mrow id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml"><mn id="S5.SS3.p2.2.m2.1.1.2" xref="S5.SS3.p2.2.m2.1.1.2.cmml">4.2</mn><mo id="S5.SS3.p2.2.m2.1.1.1" xref="S5.SS3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><apply id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.SS3.p2.2.m2.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS3.p2.2.m2.1.1.2.cmml" xref="S5.SS3.p2.2.m2.1.1.2">4.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">4.2\%</annotation></semantics></math> mAR.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p"><span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">Effect of blurriness and poses on facial landmark detection.</span> In Table. <a href="#S5.T5" title="Table 5 ‣ 5.3 Analysis ‣ 5 Experiments ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we evaluate the performance on different levels of image blurriness and facial poses (yaw angles) on WBF. The model is significantly affected by image blur (2.51 vs 19.13), while more robust to different face poses (9.02 vs 13.77).</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p"><span id="S5.SS3.p4.1.1" class="ltx_text ltx_font_bold">Effect of hand poses on hand keypoint estimation.</span> As shown in Table. <a href="#S5.T5" title="Table 5 ‣ 5.3 Analysis ‣ 5 Experiments ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we evaluate the performance on different hand poses (fist, palm or others) on WBH (NME). We show that estimating the poses of “palm” or “others” (with various gestures) is more challenging than that of “fist” (with similar patterns).</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.28.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><em id="S5.T5.29.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">left:</em><span id="S5.T5.30.3" class="ltx_text" style="font-size:90%;"> Effect of blurriness/poses on facial landmark detection (NME) on WholeBody-Face (WBF). <em id="S5.T5.30.3.1" class="ltx_emph ltx_font_italic">right:</em> Effect of hand poses on hand keypoint estimation (NME) on WholeBody-Hand (WBH).</span></figcaption>
<table id="S5.T5.24.24" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.2.2.2" class="ltx_tr">
<td id="S5.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;" colspan="10">
<span id="S5.T5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">WBF (NME </span><math id="S5.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.1.1.1.1.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T5.1.1.1.1.m1.1.1" xref="S5.T5.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math><span id="S5.T5.1.1.1.1.2" class="ltx_text" style="font-size:70%;">)</span>
</td>
<td id="S5.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;" colspan="4">
<span id="S5.T5.2.2.2.2.1" class="ltx_text" style="font-size:70%;">WBH (NME </span><math id="S5.T5.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.2.2.2.2.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T5.2.2.2.2.m1.1.1" xref="S5.T5.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.2.m1.1b"><ci id="S5.T5.2.2.2.2.m1.1.1.cmml" xref="S5.T5.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math><span id="S5.T5.2.2.2.2.2" class="ltx_text" style="font-size:70%;">)</span>
</td>
</tr>
<tr id="S5.T5.24.24.25.1" class="ltx_tr">
<td id="S5.T5.24.24.25.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;" colspan="5"><span id="S5.T5.24.24.25.1.1.1" class="ltx_text" style="font-size:70%;">Blurriness</span></td>
<td id="S5.T5.24.24.25.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;" colspan="5"><span id="S5.T5.24.24.25.1.2.1" class="ltx_text" style="font-size:70%;">Yaw Angles</span></td>
<td id="S5.T5.24.24.25.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;" colspan="4"><span id="S5.T5.24.24.25.1.3.1" class="ltx_text" style="font-size:70%;">Pose</span></td>
</tr>
<tr id="S5.T5.10.10.10" class="ltx_tr">
<td id="S5.T5.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.3.3.3.1.m1.1" class="ltx_Math" alttext="&lt;1" display="inline"><semantics id="S5.T5.3.3.3.1.m1.1a"><mrow id="S5.T5.3.3.3.1.m1.1.1" xref="S5.T5.3.3.3.1.m1.1.1.cmml"><mi id="S5.T5.3.3.3.1.m1.1.1.2" xref="S5.T5.3.3.3.1.m1.1.1.2.cmml"></mi><mo mathsize="70%" id="S5.T5.3.3.3.1.m1.1.1.1" xref="S5.T5.3.3.3.1.m1.1.1.1.cmml">&lt;</mo><mn mathsize="70%" id="S5.T5.3.3.3.1.m1.1.1.3" xref="S5.T5.3.3.3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.1.m1.1b"><apply id="S5.T5.3.3.3.1.m1.1.1.cmml" xref="S5.T5.3.3.3.1.m1.1.1"><lt id="S5.T5.3.3.3.1.m1.1.1.1.cmml" xref="S5.T5.3.3.3.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S5.T5.3.3.3.1.m1.1.1.2.cmml" xref="S5.T5.3.3.3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S5.T5.3.3.3.1.m1.1.1.3.cmml" xref="S5.T5.3.3.3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.1.m1.1c">&lt;1</annotation></semantics></math></td>
<td id="S5.T5.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.4.4.4.2.m1.1" class="ltx_Math" alttext="1-2" display="inline"><semantics id="S5.T5.4.4.4.2.m1.1a"><mrow id="S5.T5.4.4.4.2.m1.1.1" xref="S5.T5.4.4.4.2.m1.1.1.cmml"><mn mathsize="70%" id="S5.T5.4.4.4.2.m1.1.1.2" xref="S5.T5.4.4.4.2.m1.1.1.2.cmml">1</mn><mo mathsize="70%" id="S5.T5.4.4.4.2.m1.1.1.1" xref="S5.T5.4.4.4.2.m1.1.1.1.cmml">−</mo><mn mathsize="70%" id="S5.T5.4.4.4.2.m1.1.1.3" xref="S5.T5.4.4.4.2.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.4.2.m1.1b"><apply id="S5.T5.4.4.4.2.m1.1.1.cmml" xref="S5.T5.4.4.4.2.m1.1.1"><minus id="S5.T5.4.4.4.2.m1.1.1.1.cmml" xref="S5.T5.4.4.4.2.m1.1.1.1"></minus><cn type="integer" id="S5.T5.4.4.4.2.m1.1.1.2.cmml" xref="S5.T5.4.4.4.2.m1.1.1.2">1</cn><cn type="integer" id="S5.T5.4.4.4.2.m1.1.1.3.cmml" xref="S5.T5.4.4.4.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.4.2.m1.1c">1-2</annotation></semantics></math></td>
<td id="S5.T5.5.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.5.5.5.3.m1.1" class="ltx_Math" alttext="2-3" display="inline"><semantics id="S5.T5.5.5.5.3.m1.1a"><mrow id="S5.T5.5.5.5.3.m1.1.1" xref="S5.T5.5.5.5.3.m1.1.1.cmml"><mn mathsize="70%" id="S5.T5.5.5.5.3.m1.1.1.2" xref="S5.T5.5.5.5.3.m1.1.1.2.cmml">2</mn><mo mathsize="70%" id="S5.T5.5.5.5.3.m1.1.1.1" xref="S5.T5.5.5.5.3.m1.1.1.1.cmml">−</mo><mn mathsize="70%" id="S5.T5.5.5.5.3.m1.1.1.3" xref="S5.T5.5.5.5.3.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.5.5.5.3.m1.1b"><apply id="S5.T5.5.5.5.3.m1.1.1.cmml" xref="S5.T5.5.5.5.3.m1.1.1"><minus id="S5.T5.5.5.5.3.m1.1.1.1.cmml" xref="S5.T5.5.5.5.3.m1.1.1.1"></minus><cn type="integer" id="S5.T5.5.5.5.3.m1.1.1.2.cmml" xref="S5.T5.5.5.5.3.m1.1.1.2">2</cn><cn type="integer" id="S5.T5.5.5.5.3.m1.1.1.3.cmml" xref="S5.T5.5.5.5.3.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.5.5.3.m1.1c">2-3</annotation></semantics></math></td>
<td id="S5.T5.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.6.6.6.4.m1.1" class="ltx_Math" alttext="&gt;3" display="inline"><semantics id="S5.T5.6.6.6.4.m1.1a"><mrow id="S5.T5.6.6.6.4.m1.1.1" xref="S5.T5.6.6.6.4.m1.1.1.cmml"><mi id="S5.T5.6.6.6.4.m1.1.1.2" xref="S5.T5.6.6.6.4.m1.1.1.2.cmml"></mi><mo mathsize="70%" id="S5.T5.6.6.6.4.m1.1.1.1" xref="S5.T5.6.6.6.4.m1.1.1.1.cmml">&gt;</mo><mn mathsize="70%" id="S5.T5.6.6.6.4.m1.1.1.3" xref="S5.T5.6.6.6.4.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.6.6.6.4.m1.1b"><apply id="S5.T5.6.6.6.4.m1.1.1.cmml" xref="S5.T5.6.6.6.4.m1.1.1"><gt id="S5.T5.6.6.6.4.m1.1.1.1.cmml" xref="S5.T5.6.6.6.4.m1.1.1.1"></gt><csymbol cd="latexml" id="S5.T5.6.6.6.4.m1.1.1.2.cmml" xref="S5.T5.6.6.6.4.m1.1.1.2">absent</csymbol><cn type="integer" id="S5.T5.6.6.6.4.m1.1.1.3.cmml" xref="S5.T5.6.6.6.4.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.6.6.4.m1.1c">&gt;3</annotation></semantics></math></td>
<td id="S5.T5.10.10.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T5.10.10.10.9.1" class="ltx_text" style="font-size:70%;">ALL</span></td>
<td id="S5.T5.7.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.7.7.7.5.m1.1" class="ltx_Math" alttext="&lt;15^{\circ}" display="inline"><semantics id="S5.T5.7.7.7.5.m1.1a"><mrow id="S5.T5.7.7.7.5.m1.1.1" xref="S5.T5.7.7.7.5.m1.1.1.cmml"><mi id="S5.T5.7.7.7.5.m1.1.1.2" xref="S5.T5.7.7.7.5.m1.1.1.2.cmml"></mi><mo mathsize="70%" id="S5.T5.7.7.7.5.m1.1.1.1" xref="S5.T5.7.7.7.5.m1.1.1.1.cmml">&lt;</mo><msup id="S5.T5.7.7.7.5.m1.1.1.3" xref="S5.T5.7.7.7.5.m1.1.1.3.cmml"><mn mathsize="70%" id="S5.T5.7.7.7.5.m1.1.1.3.2" xref="S5.T5.7.7.7.5.m1.1.1.3.2.cmml">15</mn><mo mathsize="70%" id="S5.T5.7.7.7.5.m1.1.1.3.3" xref="S5.T5.7.7.7.5.m1.1.1.3.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.7.7.7.5.m1.1b"><apply id="S5.T5.7.7.7.5.m1.1.1.cmml" xref="S5.T5.7.7.7.5.m1.1.1"><lt id="S5.T5.7.7.7.5.m1.1.1.1.cmml" xref="S5.T5.7.7.7.5.m1.1.1.1"></lt><csymbol cd="latexml" id="S5.T5.7.7.7.5.m1.1.1.2.cmml" xref="S5.T5.7.7.7.5.m1.1.1.2">absent</csymbol><apply id="S5.T5.7.7.7.5.m1.1.1.3.cmml" xref="S5.T5.7.7.7.5.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T5.7.7.7.5.m1.1.1.3.1.cmml" xref="S5.T5.7.7.7.5.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.T5.7.7.7.5.m1.1.1.3.2.cmml" xref="S5.T5.7.7.7.5.m1.1.1.3.2">15</cn><compose id="S5.T5.7.7.7.5.m1.1.1.3.3.cmml" xref="S5.T5.7.7.7.5.m1.1.1.3.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.7.7.5.m1.1c">&lt;15^{\circ}</annotation></semantics></math></td>
<td id="S5.T5.8.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.8.8.8.6.m1.1" class="ltx_Math" alttext="15^{\circ}-30^{\circ}" display="inline"><semantics id="S5.T5.8.8.8.6.m1.1a"><mrow id="S5.T5.8.8.8.6.m1.1.1" xref="S5.T5.8.8.8.6.m1.1.1.cmml"><msup id="S5.T5.8.8.8.6.m1.1.1.2" xref="S5.T5.8.8.8.6.m1.1.1.2.cmml"><mn mathsize="70%" id="S5.T5.8.8.8.6.m1.1.1.2.2" xref="S5.T5.8.8.8.6.m1.1.1.2.2.cmml">15</mn><mo mathsize="70%" id="S5.T5.8.8.8.6.m1.1.1.2.3" xref="S5.T5.8.8.8.6.m1.1.1.2.3.cmml">∘</mo></msup><mo mathsize="70%" id="S5.T5.8.8.8.6.m1.1.1.1" xref="S5.T5.8.8.8.6.m1.1.1.1.cmml">−</mo><msup id="S5.T5.8.8.8.6.m1.1.1.3" xref="S5.T5.8.8.8.6.m1.1.1.3.cmml"><mn mathsize="70%" id="S5.T5.8.8.8.6.m1.1.1.3.2" xref="S5.T5.8.8.8.6.m1.1.1.3.2.cmml">30</mn><mo mathsize="70%" id="S5.T5.8.8.8.6.m1.1.1.3.3" xref="S5.T5.8.8.8.6.m1.1.1.3.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.8.8.8.6.m1.1b"><apply id="S5.T5.8.8.8.6.m1.1.1.cmml" xref="S5.T5.8.8.8.6.m1.1.1"><minus id="S5.T5.8.8.8.6.m1.1.1.1.cmml" xref="S5.T5.8.8.8.6.m1.1.1.1"></minus><apply id="S5.T5.8.8.8.6.m1.1.1.2.cmml" xref="S5.T5.8.8.8.6.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T5.8.8.8.6.m1.1.1.2.1.cmml" xref="S5.T5.8.8.8.6.m1.1.1.2">superscript</csymbol><cn type="integer" id="S5.T5.8.8.8.6.m1.1.1.2.2.cmml" xref="S5.T5.8.8.8.6.m1.1.1.2.2">15</cn><compose id="S5.T5.8.8.8.6.m1.1.1.2.3.cmml" xref="S5.T5.8.8.8.6.m1.1.1.2.3"></compose></apply><apply id="S5.T5.8.8.8.6.m1.1.1.3.cmml" xref="S5.T5.8.8.8.6.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T5.8.8.8.6.m1.1.1.3.1.cmml" xref="S5.T5.8.8.8.6.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.T5.8.8.8.6.m1.1.1.3.2.cmml" xref="S5.T5.8.8.8.6.m1.1.1.3.2">30</cn><compose id="S5.T5.8.8.8.6.m1.1.1.3.3.cmml" xref="S5.T5.8.8.8.6.m1.1.1.3.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.8.8.6.m1.1c">15^{\circ}-30^{\circ}</annotation></semantics></math></td>
<td id="S5.T5.9.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.9.9.9.7.m1.1" class="ltx_Math" alttext="30^{\circ}-45^{\circ}" display="inline"><semantics id="S5.T5.9.9.9.7.m1.1a"><mrow id="S5.T5.9.9.9.7.m1.1.1" xref="S5.T5.9.9.9.7.m1.1.1.cmml"><msup id="S5.T5.9.9.9.7.m1.1.1.2" xref="S5.T5.9.9.9.7.m1.1.1.2.cmml"><mn mathsize="70%" id="S5.T5.9.9.9.7.m1.1.1.2.2" xref="S5.T5.9.9.9.7.m1.1.1.2.2.cmml">30</mn><mo mathsize="70%" id="S5.T5.9.9.9.7.m1.1.1.2.3" xref="S5.T5.9.9.9.7.m1.1.1.2.3.cmml">∘</mo></msup><mo mathsize="70%" id="S5.T5.9.9.9.7.m1.1.1.1" xref="S5.T5.9.9.9.7.m1.1.1.1.cmml">−</mo><msup id="S5.T5.9.9.9.7.m1.1.1.3" xref="S5.T5.9.9.9.7.m1.1.1.3.cmml"><mn mathsize="70%" id="S5.T5.9.9.9.7.m1.1.1.3.2" xref="S5.T5.9.9.9.7.m1.1.1.3.2.cmml">45</mn><mo mathsize="70%" id="S5.T5.9.9.9.7.m1.1.1.3.3" xref="S5.T5.9.9.9.7.m1.1.1.3.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.9.9.9.7.m1.1b"><apply id="S5.T5.9.9.9.7.m1.1.1.cmml" xref="S5.T5.9.9.9.7.m1.1.1"><minus id="S5.T5.9.9.9.7.m1.1.1.1.cmml" xref="S5.T5.9.9.9.7.m1.1.1.1"></minus><apply id="S5.T5.9.9.9.7.m1.1.1.2.cmml" xref="S5.T5.9.9.9.7.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T5.9.9.9.7.m1.1.1.2.1.cmml" xref="S5.T5.9.9.9.7.m1.1.1.2">superscript</csymbol><cn type="integer" id="S5.T5.9.9.9.7.m1.1.1.2.2.cmml" xref="S5.T5.9.9.9.7.m1.1.1.2.2">30</cn><compose id="S5.T5.9.9.9.7.m1.1.1.2.3.cmml" xref="S5.T5.9.9.9.7.m1.1.1.2.3"></compose></apply><apply id="S5.T5.9.9.9.7.m1.1.1.3.cmml" xref="S5.T5.9.9.9.7.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T5.9.9.9.7.m1.1.1.3.1.cmml" xref="S5.T5.9.9.9.7.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.T5.9.9.9.7.m1.1.1.3.2.cmml" xref="S5.T5.9.9.9.7.m1.1.1.3.2">45</cn><compose id="S5.T5.9.9.9.7.m1.1.1.3.3.cmml" xref="S5.T5.9.9.9.7.m1.1.1.3.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.9.9.9.7.m1.1c">30^{\circ}-45^{\circ}</annotation></semantics></math></td>
<td id="S5.T5.10.10.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.10.10.10.8.m1.1" class="ltx_Math" alttext="&gt;45^{\circ}" display="inline"><semantics id="S5.T5.10.10.10.8.m1.1a"><mrow id="S5.T5.10.10.10.8.m1.1.1" xref="S5.T5.10.10.10.8.m1.1.1.cmml"><mi id="S5.T5.10.10.10.8.m1.1.1.2" xref="S5.T5.10.10.10.8.m1.1.1.2.cmml"></mi><mo mathsize="70%" id="S5.T5.10.10.10.8.m1.1.1.1" xref="S5.T5.10.10.10.8.m1.1.1.1.cmml">&gt;</mo><msup id="S5.T5.10.10.10.8.m1.1.1.3" xref="S5.T5.10.10.10.8.m1.1.1.3.cmml"><mn mathsize="70%" id="S5.T5.10.10.10.8.m1.1.1.3.2" xref="S5.T5.10.10.10.8.m1.1.1.3.2.cmml">45</mn><mo mathsize="70%" id="S5.T5.10.10.10.8.m1.1.1.3.3" xref="S5.T5.10.10.10.8.m1.1.1.3.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.10.10.10.8.m1.1b"><apply id="S5.T5.10.10.10.8.m1.1.1.cmml" xref="S5.T5.10.10.10.8.m1.1.1"><gt id="S5.T5.10.10.10.8.m1.1.1.1.cmml" xref="S5.T5.10.10.10.8.m1.1.1.1"></gt><csymbol cd="latexml" id="S5.T5.10.10.10.8.m1.1.1.2.cmml" xref="S5.T5.10.10.10.8.m1.1.1.2">absent</csymbol><apply id="S5.T5.10.10.10.8.m1.1.1.3.cmml" xref="S5.T5.10.10.10.8.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T5.10.10.10.8.m1.1.1.3.1.cmml" xref="S5.T5.10.10.10.8.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.T5.10.10.10.8.m1.1.1.3.2.cmml" xref="S5.T5.10.10.10.8.m1.1.1.3.2">45</cn><compose id="S5.T5.10.10.10.8.m1.1.1.3.3.cmml" xref="S5.T5.10.10.10.8.m1.1.1.3.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.10.10.10.8.m1.1c">&gt;45^{\circ}</annotation></semantics></math></td>
<td id="S5.T5.10.10.10.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T5.10.10.10.10.1" class="ltx_text" style="font-size:70%;">ALL</span></td>
<td id="S5.T5.10.10.10.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T5.10.10.10.11.1" class="ltx_text" style="font-size:70%;">fist</span></td>
<td id="S5.T5.10.10.10.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T5.10.10.10.12.1" class="ltx_text" style="font-size:70%;">palm</span></td>
<td id="S5.T5.10.10.10.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T5.10.10.10.13.1" class="ltx_text" style="font-size:70%;">others</span></td>
<td id="S5.T5.10.10.10.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T5.10.10.10.14.1" class="ltx_text" style="font-size:70%;">ALL</span></td>
</tr>
<tr id="S5.T5.24.24.24" class="ltx_tr">
<td id="S5.T5.11.11.11.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.11.11.11.1.m1.1" class="ltx_Math" alttext="19.13" display="inline"><semantics id="S5.T5.11.11.11.1.m1.1a"><mn mathsize="70%" id="S5.T5.11.11.11.1.m1.1.1" xref="S5.T5.11.11.11.1.m1.1.1.cmml">19.13</mn><annotation-xml encoding="MathML-Content" id="S5.T5.11.11.11.1.m1.1b"><cn type="float" id="S5.T5.11.11.11.1.m1.1.1.cmml" xref="S5.T5.11.11.11.1.m1.1.1">19.13</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.11.11.11.1.m1.1c">19.13</annotation></semantics></math></td>
<td id="S5.T5.12.12.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.12.12.12.2.m1.1" class="ltx_Math" alttext="10.85" display="inline"><semantics id="S5.T5.12.12.12.2.m1.1a"><mn mathsize="70%" id="S5.T5.12.12.12.2.m1.1.1" xref="S5.T5.12.12.12.2.m1.1.1.cmml">10.85</mn><annotation-xml encoding="MathML-Content" id="S5.T5.12.12.12.2.m1.1b"><cn type="float" id="S5.T5.12.12.12.2.m1.1.1.cmml" xref="S5.T5.12.12.12.2.m1.1.1">10.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.12.12.12.2.m1.1c">10.85</annotation></semantics></math></td>
<td id="S5.T5.13.13.13.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.13.13.13.3.m1.1" class="ltx_Math" alttext="4.91" display="inline"><semantics id="S5.T5.13.13.13.3.m1.1a"><mn mathsize="70%" id="S5.T5.13.13.13.3.m1.1.1" xref="S5.T5.13.13.13.3.m1.1.1.cmml">4.91</mn><annotation-xml encoding="MathML-Content" id="S5.T5.13.13.13.3.m1.1b"><cn type="float" id="S5.T5.13.13.13.3.m1.1.1.cmml" xref="S5.T5.13.13.13.3.m1.1.1">4.91</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.13.13.13.3.m1.1c">4.91</annotation></semantics></math></td>
<td id="S5.T5.14.14.14.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.14.14.14.4.m1.1" class="ltx_Math" alttext="2.51" display="inline"><semantics id="S5.T5.14.14.14.4.m1.1a"><mn mathsize="70%" id="S5.T5.14.14.14.4.m1.1.1" xref="S5.T5.14.14.14.4.m1.1.1.cmml">2.51</mn><annotation-xml encoding="MathML-Content" id="S5.T5.14.14.14.4.m1.1b"><cn type="float" id="S5.T5.14.14.14.4.m1.1.1.cmml" xref="S5.T5.14.14.14.4.m1.1.1">2.51</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.14.14.14.4.m1.1c">2.51</annotation></semantics></math></td>
<td id="S5.T5.15.15.15.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.15.15.15.5.m1.1" class="ltx_Math" alttext="10.17" display="inline"><semantics id="S5.T5.15.15.15.5.m1.1a"><mn mathsize="70%" id="S5.T5.15.15.15.5.m1.1.1" xref="S5.T5.15.15.15.5.m1.1.1.cmml">10.17</mn><annotation-xml encoding="MathML-Content" id="S5.T5.15.15.15.5.m1.1b"><cn type="float" id="S5.T5.15.15.15.5.m1.1.1.cmml" xref="S5.T5.15.15.15.5.m1.1.1">10.17</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.15.15.15.5.m1.1c">10.17</annotation></semantics></math></td>
<td id="S5.T5.16.16.16.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.16.16.16.6.m1.1" class="ltx_Math" alttext="9.02" display="inline"><semantics id="S5.T5.16.16.16.6.m1.1a"><mn mathsize="70%" id="S5.T5.16.16.16.6.m1.1.1" xref="S5.T5.16.16.16.6.m1.1.1.cmml">9.02</mn><annotation-xml encoding="MathML-Content" id="S5.T5.16.16.16.6.m1.1b"><cn type="float" id="S5.T5.16.16.16.6.m1.1.1.cmml" xref="S5.T5.16.16.16.6.m1.1.1">9.02</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.16.16.16.6.m1.1c">9.02</annotation></semantics></math></td>
<td id="S5.T5.17.17.17.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.17.17.17.7.m1.1" class="ltx_Math" alttext="10.56" display="inline"><semantics id="S5.T5.17.17.17.7.m1.1a"><mn mathsize="70%" id="S5.T5.17.17.17.7.m1.1.1" xref="S5.T5.17.17.17.7.m1.1.1.cmml">10.56</mn><annotation-xml encoding="MathML-Content" id="S5.T5.17.17.17.7.m1.1b"><cn type="float" id="S5.T5.17.17.17.7.m1.1.1.cmml" xref="S5.T5.17.17.17.7.m1.1.1">10.56</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.17.17.17.7.m1.1c">10.56</annotation></semantics></math></td>
<td id="S5.T5.18.18.18.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.18.18.18.8.m1.1" class="ltx_Math" alttext="12.10" display="inline"><semantics id="S5.T5.18.18.18.8.m1.1a"><mn mathsize="70%" id="S5.T5.18.18.18.8.m1.1.1" xref="S5.T5.18.18.18.8.m1.1.1.cmml">12.10</mn><annotation-xml encoding="MathML-Content" id="S5.T5.18.18.18.8.m1.1b"><cn type="float" id="S5.T5.18.18.18.8.m1.1.1.cmml" xref="S5.T5.18.18.18.8.m1.1.1">12.10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.18.18.18.8.m1.1c">12.10</annotation></semantics></math></td>
<td id="S5.T5.19.19.19.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.19.19.19.9.m1.1" class="ltx_Math" alttext="13.77" display="inline"><semantics id="S5.T5.19.19.19.9.m1.1a"><mn mathsize="70%" id="S5.T5.19.19.19.9.m1.1.1" xref="S5.T5.19.19.19.9.m1.1.1.cmml">13.77</mn><annotation-xml encoding="MathML-Content" id="S5.T5.19.19.19.9.m1.1b"><cn type="float" id="S5.T5.19.19.19.9.m1.1.1.cmml" xref="S5.T5.19.19.19.9.m1.1.1">13.77</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.19.19.19.9.m1.1c">13.77</annotation></semantics></math></td>
<td id="S5.T5.20.20.20.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.20.20.20.10.m1.1" class="ltx_Math" alttext="10.17" display="inline"><semantics id="S5.T5.20.20.20.10.m1.1a"><mn mathsize="70%" id="S5.T5.20.20.20.10.m1.1.1" xref="S5.T5.20.20.20.10.m1.1.1.cmml">10.17</mn><annotation-xml encoding="MathML-Content" id="S5.T5.20.20.20.10.m1.1b"><cn type="float" id="S5.T5.20.20.20.10.m1.1.1.cmml" xref="S5.T5.20.20.20.10.m1.1.1">10.17</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.20.20.20.10.m1.1c">10.17</annotation></semantics></math></td>
<td id="S5.T5.21.21.21.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.21.21.21.11.m1.1" class="ltx_Math" alttext="6.09" display="inline"><semantics id="S5.T5.21.21.21.11.m1.1a"><mn mathsize="70%" id="S5.T5.21.21.21.11.m1.1.1" xref="S5.T5.21.21.21.11.m1.1.1.cmml">6.09</mn><annotation-xml encoding="MathML-Content" id="S5.T5.21.21.21.11.m1.1b"><cn type="float" id="S5.T5.21.21.21.11.m1.1.1.cmml" xref="S5.T5.21.21.21.11.m1.1.1">6.09</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.21.21.21.11.m1.1c">6.09</annotation></semantics></math></td>
<td id="S5.T5.22.22.22.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.22.22.22.12.m1.1" class="ltx_Math" alttext="7.10" display="inline"><semantics id="S5.T5.22.22.22.12.m1.1a"><mn mathsize="70%" id="S5.T5.22.22.22.12.m1.1.1" xref="S5.T5.22.22.22.12.m1.1.1.cmml">7.10</mn><annotation-xml encoding="MathML-Content" id="S5.T5.22.22.22.12.m1.1b"><cn type="float" id="S5.T5.22.22.22.12.m1.1.1.cmml" xref="S5.T5.22.22.22.12.m1.1.1">7.10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.22.22.22.12.m1.1c">7.10</annotation></semantics></math></td>
<td id="S5.T5.23.23.23.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.23.23.23.13.m1.1" class="ltx_Math" alttext="6.33" display="inline"><semantics id="S5.T5.23.23.23.13.m1.1a"><mn mathsize="70%" id="S5.T5.23.23.23.13.m1.1.1" xref="S5.T5.23.23.23.13.m1.1.1.cmml">6.33</mn><annotation-xml encoding="MathML-Content" id="S5.T5.23.23.23.13.m1.1b"><cn type="float" id="S5.T5.23.23.23.13.m1.1.1.cmml" xref="S5.T5.23.23.23.13.m1.1.1">6.33</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.23.23.23.13.m1.1c">6.33</annotation></semantics></math></td>
<td id="S5.T5.24.24.24.14" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><math id="S5.T5.24.24.24.14.m1.1" class="ltx_Math" alttext="6.66" display="inline"><semantics id="S5.T5.24.24.24.14.m1.1a"><mn mathsize="70%" id="S5.T5.24.24.24.14.m1.1.1" xref="S5.T5.24.24.24.14.m1.1.1.cmml">6.66</mn><annotation-xml encoding="MathML-Content" id="S5.T5.24.24.24.14.m1.1b"><cn type="float" id="S5.T5.24.24.24.14.m1.1.1.cmml" xref="S5.T5.24.24.24.14.m1.1.1">6.66</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.24.24.24.14.m1.1c">6.66</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we proposed the first large-scale benchmark for whole-body human pose estimation. We extensively evaluate the performance of the existing approaches on our proposed COCO-WholeBody Dataset. Cross-dataset evaluation also demonstrates the generalization ability of the proposed dataset. Moreover, to solve the problem of extreme scale difference among body parts, ZoomNet is proposed to pay more attention to the hard-to-detect face/hand keypoints. Experiments show that ZoomNet significantly outperforms the prior arts.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgement.</span> This work is partially supported by the SenseTime Donation for Research, HKU Seed Fund for Basic Research, Startup Fund, General Research Fund No.27208720, the Australian Research Council Grant DP200103223 and Australian Medical Research Future Fund MRFAI000085.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alp Güler, R., Neverova, N., Kokkinos, I.: Densepose: Dense human pose
estimation in the wild. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2018)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Andriluka, M., Iqbal, U., Insafutdinov, E., Pishchulin, L., Milan, A., Gall,
J., Schiele, B.: Posetrack: A benchmark for human pose estimation and
tracking. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2018)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Andriluka, M., Pishchulin, L., Gehler, P., Schiele, B.: 2d human pose
estimation: New benchmark and state of the art analysis. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Belhumeur, P.N., Jacobs, D.W., Kriegman, D.J., Kumar, N.: Localizing parts of
faces using a consensus of exemplars. IEEE transactions on pattern analysis
and machine intelligence (2013)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Burgos-Artizzu, X.P., Perona, P., Dollár, P.: Robust face landmark estimation
under occlusion. In: Proceedings of the 2013 IEEE International Conference on
Computer Vision (2013)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Cao, X., Wei, Y., Wen, F., Sun, J.: Face alignment by explicit shape
regression. International Journal of Computer Vision (2014)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y.: Openpose: realtime
multi-person 2d pose estimation using part affinity fields. arXiv preprint
arXiv:1812.08008 (2018)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Cao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2d pose
estimation using part affinity fields. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) (2017)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J.: Cascaded pyramid
network for multi-person pose estimation. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2018)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Duan, H., Lin, K.Y., Jin, S., Liu, W., Qian, C., Ouyang, W.: Trb: A novel
triplet representation for understanding 2d human body. In: Proceedings of
the IEEE International Conference on Computer Vision. pp. 9479–9488 (2019)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Eichner, M., Ferrari, V.: We are family: Joint pose estimation of multiple
persons. In: Proceedings of the European Conference on Computer Vision (ECCV)
(2010)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Fang, H.S., Xie, S., Tai, Y.W., Lu, C.: Rmpe: Regional multi-person pose
estimation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2017)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Gomez-Donoso, F., Orts-Escolano, S., Cazorla, M.: Large-scale multiview 3d hand
pose dataset. arXiv preprint arXiv:1707.03742 (2017)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Gross, R., Matthews, I., Cohn, J., Kanade, T., Baker, S.: Multi-pie. In: Image
and Vision Computing (2010)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Guan, H., Chang, J.S., Chen, L., Feris, R.S., Turk, M.: Multi-view
appearance-based 3d hand pose estimation. In: IEEE Conference on Computer
Vision and Pattern Recognition Workshop (2006)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask r-cnn. arXiv preprint
arXiv:1703.06870 (2017)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Hidalgo, G., Raaj, Y., Idrees, H., Xiang, D., Joo, H., Simon, T., Sheikh, Y.:
Single-network whole-body pose estimation. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Honari, S., Yosinski, J., Vincent, P., Pal, C.: Recombinator networks: Learning
coarse-to-fine feature aggregation. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (2016)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Insafutdinov, E., Andriluka, M., Pishchulin, L., Tang, S., Levinkov, E.,
Andres, B., Schiele, B., Campus, S.I.: Arttrack: Articulated multi-person
tracking in the wild. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2017)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka, M., Schiele, B.:
Deepercut: A deeper, stronger, and faster multi-person pose estimation model.
In: Proceedings of the European Conference on Computer Vision (ECCV) (2016)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Iqbal, U., Milan, A., Gall, J.: Pose-track: Joint multi-person pose estimation
and tracking. arXiv preprint arXiv:1611.07727 (2016)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Jin, S., Liu, W., Ouyang, W., Qian, C.: Multi-person articulated tracking with
spatial and temporal embeddings. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jin, S., Ma, X., Han, Z., Wu, Y., Yang, W., Liu, W., Qian, C., Ouyang, W.:
Towards multi-person pose tracking: Bottom-up and top-down methods. In: IEEE
International Conference on Computer Vision Workshop (2017)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Joo, H., Simon, T., Sheikh, Y.: Total capture: A 3d deformation model for
tracking faces, hands, and bodies. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (2018)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980 (2014)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial
landmarks in the wild: A large-scale, real-world database for facial landmark
localization. In: IEEE International Conference on Computer Vision Workshop
(2011)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Kowalski, M., Naruniec, J., Trzcinski, T.: Deep alignment network: A
convolutional neural network for robust face alignment. In: IEEE Conference
on Computer Vision and Pattern Recognition Workshop (2017)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Law, H., Deng, J.: Cornernet: Detecting objects as paired keypoints. In:
Proceedings of the European Conference on Computer Vision (ECCV) (2018)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Le, V., Brandt, J., Lin, Z., Bourdev, L., Huang, T.S.: Interactive facial
feature localization. In: Proceedings of the European Conference on Computer
Vision (ECCV) (2012)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Li, J., Wang, C., Zhu, H., Mao, Y., Fang, H.S., Lu, C.: Crowdpose: Efficient
crowded scenes pose estimation and a new benchmark. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. pp. 10863–10872
(2019)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In:
Proceedings of the European Conference on Computer Vision (ECCV) (2014)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Liu, W., Chen, J., Li, C., Qian, C., Chu, X., Hu, X.: A cascaded inception of
inception network with attention modulated feature fusion for human pose
estimation. In: The Thirty-Second AAAI Conference on Artificial Intelligence
(2018)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Messer, K., Matas, J., Kittler, J., Luettin, J., Maitre, G.: Xm2vtsdb: The
extended m2vts database. In: Second international conference on audio and
video-based biometric person authentication (1999)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Mueller, F., Bernard, F., Sotnychenko, O., Mehta, D., Sridhar, S., Casas, D.,
Theobalt, C.: Ganerated hands for real-time 3d hand tracking from monocular
rgb. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (2018)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Mueller, F., Mehta, D., Sotnychenko, O., Sridhar, S., Casas, D., Theobalt, C.:
Real-time hand tracking under occlusion from an egocentric rgb-d sensor. In:
Proceedings of International Conference on Computer Vision (ICCV) (2017)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Neverova, N., Wolf, C., Taylor, G.W., Nebout, F.: Multi-scale deep learning for
gesture detection and localization. In: Proceedings of the European
Conference on Computer Vision (ECCV) (2014)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Newell, A., Huang, Z., Deng, J.: Associative embedding: End-to-end learning for
joint detection and grouping. In: Advances in Neural Information Processing
Systems (2017)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose
estimation. In: Proceedings of the European Conference on Computer Vision
(ECCV) (2016)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Nie, X., Feng, J., Xing, J., Yan, S.: Generative partition networks for
multi-person pose estimation. arXiv preprint arXiv:1705.07422 (2017)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Oikonomidis, I., Kyriazis, N., Argyros, A.A.: Tracking the articulated motion
of two strongly interacting hands. In: IEEE Conference on Computer Vision and
Pattern Recognition (2012)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Papandreou, G., Zhu, T., Chen, L.C., Gidaris, S., Tompson, J., Murphy, K.:
Personlab: Person pose estimation and instance segmentation with a bottom-up,
part-based, geometric embedding model. arXiv preprint arXiv:1803.08225
(2018)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A., Tompson, J., Bregler, C.,
Murphy, K.: Towards accurate multi-person pose estimation in the wild. arXiv
preprint arXiv:1701.01779 (2017)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Pech-Pacheco, J. L., C., G., Chamorro-Martinez, J., Fernández-Valdivia, J.:
Diatom autofocusing in brightfield microscopy: a comparative study. In: ICPR
(2000)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Phillips, P.J., Flynn, P.J., Scruggs, T., Bowyer, K.W., Chang, J., Hoffman, K.,
Marques, J., Min, J., Worek, W.: Overview of the face recognition grand
challenge. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2005)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler,
P.V., Schiele, B.: Deepcut: Joint subset partition and labeling for multi
person pose estimation. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2016)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time
object detection with region proposal networks. In: Advances in Neural
Information Processing Systems (NIPS) (2015)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Romero, J., Tzionas, D., Black, M.J.: Embodied hands: Modeling and capturing
hands and bodies together. ACM Transactions on Graphics (ToG) (2017)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Ronchi, M.R., Perona, P.: Benchmarking and Error Diagnosis in Multi-Instance
Pose Estimation. Proceedings of International Conference on Computer Vision
(ICCV) (2017)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Sagonas, C., Tzimiropoulos, G., Zafeiriou, S., Pantic, M.: 300 faces
in-the-wild challenge: The first facial landmark localization challenge. In:
IEEE International Conference on Computer Vision Workshop (2013)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Sharp, T., Keskin, C., Robertson, D., Taylor, J., Shotton, J., Kim, D.,
Rhemann, C., Leichter, I., Vinnikov, A., Wei, Y., et al.: Accurate, robust,
and flexible real-time hand tracking. In: Proceedings of the 33rd Annual ACM
Conference on Human Factors in Computing Systems (2015)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Simon, T., Joo, H., Matthews, I., Sheikh, Y.: Hand keypoint detection in single
images using multiview bootstrapping. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) (2017)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Sridhar, S., Mueller, F., Oulasvirta, A., Theobalt, C.: Fast and robust hand
tracking using detection-guided optimization. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2015)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution representation
learning for human pose estimation. arXiv preprint arXiv:1902.09212 (2019)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Sun, K., Zhao, Y., Jiang, B., Cheng, T., Xiao, B., Liu, D., Mu, Y., Wang, X.,
Liu, W., Wang, J.: High-resolution representations for labeling pixels and
regions. arXiv preprint arXiv:1904.04514 (2019)

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Sun, Y., Wang, X., Tang, X.: Deep convolutional network cascade for facial
point detection. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) (2013)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Tompson, J., Stein, M., Lecun, Y., Perlin, K.: Real-time continuous pose
recovery of human hands using convolutional networks. ACM Transactions on
Graphics (ToG) (2014)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Trigeorgis, G., Snape, P., Nicolaou, M.A., Antonakos, E., Zafeiriou, S.:
Mnemonic descent method: A recurrent process applied for end-to-end face
alignment. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2016)

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Tzimiropoulos, G.: Project-out cascaded regression with an application to face
alignment. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2015)

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Valle, R., Buenaposada, J.M., Valdes, A., Baumela, L.: A deeply-initialized
coarse-to-fine ensemble of regression trees for face alignment. In:
Proceedings of the European Conference on Computer Vision (ECCV) (2018)

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Wang, Y., Peng, C., Liu, Y.: Mask-pose cascaded cnn for 2d hand pose estimation
from single color image. IEEE Transactions on Circuits and Systems for Video
Technology (2018)

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Wei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional pose
machines. In: The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2016)

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Wu, J., Zheng, H., Zhao, B., Li, Y., Yan, B., Liang, R., Wang, W., Zhou, S.,
Lin, G., Fu, Y., et al.: Ai challenger: a large-scale dataset for going
deeper in image understanding. arXiv preprint arXiv:1711.06475 (2017)

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Wu, W., Qian, C., Yang, S., Wang, Q., Cai, Y., Zhou, Q.: Look at boundary: A
boundary-aware face alignment algorithm. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2018)

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Xiang, D., Joo, H., Sheikh, Y.: Monocular total capture: Posing face, body, and
hands in the wild. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Xiao, B., Wu, H., Wei, Y.: Simple baselines for human pose estimation and
tracking. In: Proceedings of the European Conference on Computer Vision
(ECCV) (2018)

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Xiong, X., De la Torre, F.: Supervised descent method and its applications to
face alignment. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2013)

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Yuan, S., Ye, Q., Stenger, B., Jain, S., Kim, T.K.: Bighand2. 2m benchmark:
Hand pose dataset and state of the art analysis. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2017)

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Zhang, Z., Luo, P., Loy, C.C., Tang, X.: Learning deep representation for face
alignment with auxiliary attributes. IEEE transactions on pattern analysis
and machine intelligence (2015)

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Zhu, X., Ramanan, D.: Face detection, pose estimation, and landmark
localization in the wild. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2012)

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Zimmermann, C., Brox, T.: Learning to estimate 3d hand pose from single rgb
images. arXiv preprint arXiv: 1705.01389 (2017)

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Zimmermann, C., Ceylan, D., Yang, J., Russell, B., Argus, M., Brox, T.:
Freihand: A dataset for markerless capture of hand pose and shape from single
rgb images. In: Proceedings of International Conference on Computer Vision
(ICCV) (2019)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Pt0.A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.A </span>Annotation Details</h2>

<div id="Pt0.A1.p1" class="ltx_para">
<p id="Pt0.A1.p1.1" class="ltx_p">The annotation of face/hand keypoints in our COCO-WholeBody dataset follows semi-automatic methodology. Firstly, face/hand bounding boxes are annotated manually. Secondly, we utilize a face model and a hand model, which are trained on large-scale face datasets and hand datasets respectively, to pre-annotate the face and hand keypoints. Next, manual correction of the face/hand keypoints is conducted. Foot keypoints are directly manually labeled. Note that, quality inspections are conducted in every step.</p>
</div>
<div id="Pt0.A1.p2" class="ltx_para">
<p id="Pt0.A1.p2.1" class="ltx_p"><span id="Pt0.A1.p2.1.1" class="ltx_text ltx_font_bold">Face and hand bounding box:</span>
To ensure the quality of face/hand bounding boxes, well-defined standards are followed. <span id="Pt0.A1.p2.1.2" class="ltx_text ltx_font_bold">Face bounding box</span> is labeled only if the box is bigger than 8 pixels and the rotation angle of the face is less than <math id="Pt0.A1.p2.1.m1.1" class="ltx_Math" alttext="100^{\circ}" display="inline"><semantics id="Pt0.A1.p2.1.m1.1a"><msup id="Pt0.A1.p2.1.m1.1.1" xref="Pt0.A1.p2.1.m1.1.1.cmml"><mn id="Pt0.A1.p2.1.m1.1.1.2" xref="Pt0.A1.p2.1.m1.1.1.2.cmml">100</mn><mo id="Pt0.A1.p2.1.m1.1.1.3" xref="Pt0.A1.p2.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="Pt0.A1.p2.1.m1.1b"><apply id="Pt0.A1.p2.1.m1.1.1.cmml" xref="Pt0.A1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Pt0.A1.p2.1.m1.1.1.1.cmml" xref="Pt0.A1.p2.1.m1.1.1">superscript</csymbol><cn type="integer" id="Pt0.A1.p2.1.m1.1.1.2.cmml" xref="Pt0.A1.p2.1.m1.1.1.2">100</cn><compose id="Pt0.A1.p2.1.m1.1.1.3.cmml" xref="Pt0.A1.p2.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.p2.1.m1.1c">100^{\circ}</annotation></semantics></math> from the frontal view. As for some special cases, faces of real persons in photos, posters, and clothes are labeled but faces of sculptures, models, cartoons, paintings, and animals are not. The face bounding box is defined as the minimal bounding rectangle of the face keypoints. Quality inspections are conducted by another group of annotators and bounding boxes whose positions are inaccurate are re-annotated. <span id="Pt0.A1.p2.1.3" class="ltx_text ltx_font_bold">Hand bounding box</span> is labeled when the hand image is vivid and the position of the hand keypoints can be well-determined. The box is regarded as invalid if the corresponding hand is severely occluded or part of the hand is out of the image. Special case settings follow those of face bounding box and independent quality inspections are conducted. Examples of face/hand bounding boxes are shown in Fig. <a href="#Pt0.A1.F7" title="Figure 7 ‣ Appendix 0.A Annotation Details ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, where only the green boxes meet our annotation requirements. More visualization results for bounding boxes are demonstrated in Fig. <a href="#Pt0.A1.F8" title="Figure 8 ‣ Appendix 0.A Annotation Details ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> Line#1. We have three types of bounding boxes, <span id="Pt0.A1.p2.1.4" class="ltx_text ltx_font_italic">i</span>.<span id="Pt0.A1.p2.1.5" class="ltx_text ltx_font_italic">e</span>. body (green), face (purple), left hand (blue) and right hand (red).</p>
</div>
<div id="Pt0.A1.p3" class="ltx_para">
<p id="Pt0.A1.p3.1" class="ltx_p"><span id="Pt0.A1.p3.1.1" class="ltx_text ltx_font_bold">Face Keypoints:</span>
We apply the 68-joint face model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> as shown in Fig. <a href="#Pt0.A1.F7" title="Figure 7 ‣ Appendix 0.A Annotation Details ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(b). A few occluded keypoints may be estimated by annotators if most keypoints are visible in the image. In Fig. <a href="#Pt0.A1.F8" title="Figure 8 ‣ Appendix 0.A Annotation Details ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, Line#2 and Line#3 visualize more examples of the face keypoint annotations.</p>
</div>
<div id="Pt0.A1.p4" class="ltx_para">
<p id="Pt0.A1.p4.1" class="ltx_p"><span id="Pt0.A1.p4.1.1" class="ltx_text ltx_font_bold">Hand Keypoints:</span>
Self-occlusion is very common for hand keypoints. As a result, the annotation for hand keypoints requires trained experts and enormous workload although pseudo labels are given. We use 21-joint hand model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and annotate quite a lot of challenging cases. Annotation is shown in Fig. <a href="#Pt0.A1.F7" title="Figure 7 ‣ Appendix 0.A Annotation Details ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(c) and more examples are visualized in Fig. <a href="#Pt0.A1.F8" title="Figure 8 ‣ Appendix 0.A Annotation Details ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, where Line#4 and Line#5 visualize some examples of the hand keypoint annotations for various hand poses.</p>
</div>
<div id="Pt0.A1.p5" class="ltx_para">
<p id="Pt0.A1.p5.1" class="ltx_p"><span id="Pt0.A1.p5.1.1" class="ltx_text ltx_font_bold">Foot Keypoints:</span>
Six foot keypoints are defined following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. The order in the annotation file is as follows: left big toe, left small toe, left heel, right big toe, right small toe, and right heel. The keypoints are defined in the inner center rather than on the surface to fit in images in different views. Qualitative examples are shown in Fig. <a href="#Pt0.A1.F7" title="Figure 7 ‣ Appendix 0.A Annotation Details ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(d).</p>
</div>
<figure id="Pt0.A1.F7" class="ltx_figure"><img src="/html/2007.11858/assets/supp_images/anno_protocol.jpg" id="Pt0.A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="328" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A1.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="Pt0.A1.F7.3.2" class="ltx_text" style="font-size:90%;">Face/hand bounding box annotation. Bounding boxes should tightly enclose all the keypoints. Positive (green) and negative (orange) cases are shown.</span></figcaption>
</figure>
<figure id="Pt0.A1.F8" class="ltx_figure"><img src="/html/2007.11858/assets/supp_images/anno_result.jpg" id="Pt0.A1.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A1.F8.4.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="Pt0.A1.F8.5.2" class="ltx_text" style="font-size:90%;">Annotation examples. Line #1: We use different colors to distinguish different types of bounding boxes, <span id="Pt0.A1.F8.5.2.1" class="ltx_text ltx_font_italic">i</span>.<span id="Pt0.A1.F8.5.2.2" class="ltx_text ltx_font_italic">e</span>. body (green), face (purple), left hand (blue) and right hand (red). Line #2 and Line#3: Face keypoints. Line #4 and Line#5: Hand keypoints.</span></figcaption>
</figure>
</section>
<section id="Pt0.A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.B </span>Baseline Implementation Details</h2>

<div id="Pt0.A2.p1" class="ltx_para">
<p id="Pt0.A2.p1.1" class="ltx_p">We used the official codes to reproduce existing methods. We keep all training parameters (e.g. input size, #iterations, learning rate, and so on) the same, except #keypoints (# means the number of). We also trained all the existing methods on the original 17-keypoint COCO dataset and verified that our re-implementation is the same as the original papers. For fair comparisons, all experimental results are obtained with single-scale testing. The implementation details of the baseline methods we used in the experiments are listed as following:</p>
</div>
<div id="Pt0.A2.p2" class="ltx_para">
<p id="Pt0.A2.p2.1" class="ltx_p"><span id="Pt0.A2.p2.1.1" class="ltx_text ltx_font_bold">OpenPose Whole-body System</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is a Multi-Network whole-body pose estimation system, which consists of a body keypoint model, a facial landmark detector and a hand pose estimator. We reimplement the approach by training these models on COCO-WholeBody dataset separately based on the official training codes <span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/CMU-Perceptual-Computing-Lab/openpose</span></span></span>.</p>
</div>
<div id="Pt0.A2.p3" class="ltx_para">
<p id="Pt0.A2.p3.1" class="ltx_p"><span id="Pt0.A2.p3.1.1" class="ltx_text ltx_font_bold">Single-Network Whole-body Pose Estimation</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is a recently proposed method for whole-body pose estimation. We follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and retrain the whole-body keypoint estimator <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/CMU-Perceptual-Computing-Lab/openpose_train</span></span></span> in our COCO-WholeBody dataset. The number of keypoints is 133, and the number of PAFs is 134 as we designed a tree structure except for the two loops around the lips. Face, hand and foot keypoints are connected to the corresponding nearest body keypoints. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, we applied 3 stages for PAF and 1 stage for confidence maps. We use a batch size of 10 images in each GPU and Adam optimization with an initial learning rate of 1e-3 to train the model.</p>
</div>
<div id="Pt0.A2.p4" class="ltx_para">
<p id="Pt0.A2.p4.1" class="ltx_p"><span id="Pt0.A2.p4.1.1" class="ltx_text ltx_font_bold">Part-affinity Fields (PAF)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is also re-implemented for the whole-body pose estimation task based on the open-source codes <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation</span></span></span>. The settings of PAFs and confidence maps are the same as Single-Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and CPM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> network is used as its backbone. We use SGD with an initial learning rate of 1 to train the model. Note that, the direction of limb (or value of the affinity fields) is calculated in the image scale before down-sampling, see Fig. <a href="#Pt0.A2.F9" title="Figure 9 ‣ Appendix 0.B Baseline Implementation Details ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. Therefore, for most tiny hands and faces, the PAF prediction and keypoint grouping will not be affected.</p>
</div>
<figure id="Pt0.A2.F9" class="ltx_figure"><img src="/html/2007.11858/assets/supp_images/paf.jpg" id="Pt0.A2.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="389" height="166" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A2.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="Pt0.A2.F9.3.2" class="ltx_text" style="font-size:90%;">Visualizations of Part-affinity Fields.</span></figcaption>
</figure>
<div id="Pt0.A2.p5" class="ltx_para">
<p id="Pt0.A2.p5.1" class="ltx_p"><span id="Pt0.A2.p5.1.1" class="ltx_text ltx_font_bold">Associative Embedding (AE)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> learns to group keypoints by associative embedding, which is flexible in terms of various numbers of keypoints to predict. The official open-source codes <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/princeton-vl/pose-ae-train</span></span></span> are used in our implementation. We use the 4-stacked hourglass backbone and follow the same training settings as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> in our experiments.</p>
</div>
<div id="Pt0.A2.p6" class="ltx_para">
<p id="Pt0.A2.p6.1" class="ltx_p"><span id="Pt0.A2.p6.1.1" class="ltx_text ltx_font_bold">HRNet</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> is the recent state-of-the-art model for the task of multi-person human pose estimation. We retrain the model <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</span></span></span> to fit for the whole-body pose estimation task by directly adding the number of keypoints to 133. For fair comparisons, we choose HRNet-w32 as the backbone in the experiments. Note that this model can be viewed as the single-stage alternative of our multi-stage ZoomNet. The comparison between HRNet and ZoomNet demonstrates the effectiveness of the multi-stage keypoint localization.</p>
</div>
</section>
<section id="Pt0.A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.C </span>ZoomNet Implementation Details</h2>

<div id="Pt0.A3.p1" class="ltx_para">
<p id="Pt0.A3.p1.1" class="ltx_p">We use 2D gaussian confidence heatmaps with <math id="Pt0.A3.p1.1.m1.1" class="ltx_Math" alttext="\sigma=3" display="inline"><semantics id="Pt0.A3.p1.1.m1.1a"><mrow id="Pt0.A3.p1.1.m1.1.1" xref="Pt0.A3.p1.1.m1.1.1.cmml"><mi id="Pt0.A3.p1.1.m1.1.1.2" xref="Pt0.A3.p1.1.m1.1.1.2.cmml">σ</mi><mo id="Pt0.A3.p1.1.m1.1.1.1" xref="Pt0.A3.p1.1.m1.1.1.1.cmml">=</mo><mn id="Pt0.A3.p1.1.m1.1.1.3" xref="Pt0.A3.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A3.p1.1.m1.1b"><apply id="Pt0.A3.p1.1.m1.1.1.cmml" xref="Pt0.A3.p1.1.m1.1.1"><eq id="Pt0.A3.p1.1.m1.1.1.1.cmml" xref="Pt0.A3.p1.1.m1.1.1.1"></eq><ci id="Pt0.A3.p1.1.m1.1.1.2.cmml" xref="Pt0.A3.p1.1.m1.1.1.2">𝜎</ci><cn type="integer" id="Pt0.A3.p1.1.m1.1.1.3.cmml" xref="Pt0.A3.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A3.p1.1.m1.1c">\sigma=3</annotation></semantics></math> to encode the keypoint locations.
The sum of squared error (SSE) loss function between the predicted heatmaps and the ground truth heatmaps is used for training both corner keypoints and body keypoints. The losses of different body parts (body, face, hand, and feet) are summed up with the same loss weight.</p>
</div>
<div id="Pt0.A3.p2" class="ltx_para">
<p id="Pt0.A3.p2.4" class="ltx_p">We follow the same setting as HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> to use data augmentation with random scaling ([-35%, 35%]), random rotation ([<math id="Pt0.A3.p2.1.m1.1" class="ltx_Math" alttext="-45^{\circ}" display="inline"><semantics id="Pt0.A3.p2.1.m1.1a"><mrow id="Pt0.A3.p2.1.m1.1.1" xref="Pt0.A3.p2.1.m1.1.1.cmml"><mo id="Pt0.A3.p2.1.m1.1.1a" xref="Pt0.A3.p2.1.m1.1.1.cmml">−</mo><msup id="Pt0.A3.p2.1.m1.1.1.2" xref="Pt0.A3.p2.1.m1.1.1.2.cmml"><mn id="Pt0.A3.p2.1.m1.1.1.2.2" xref="Pt0.A3.p2.1.m1.1.1.2.2.cmml">45</mn><mo id="Pt0.A3.p2.1.m1.1.1.2.3" xref="Pt0.A3.p2.1.m1.1.1.2.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A3.p2.1.m1.1b"><apply id="Pt0.A3.p2.1.m1.1.1.cmml" xref="Pt0.A3.p2.1.m1.1.1"><minus id="Pt0.A3.p2.1.m1.1.1.1.cmml" xref="Pt0.A3.p2.1.m1.1.1"></minus><apply id="Pt0.A3.p2.1.m1.1.1.2.cmml" xref="Pt0.A3.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="Pt0.A3.p2.1.m1.1.1.2.1.cmml" xref="Pt0.A3.p2.1.m1.1.1.2">superscript</csymbol><cn type="integer" id="Pt0.A3.p2.1.m1.1.1.2.2.cmml" xref="Pt0.A3.p2.1.m1.1.1.2.2">45</cn><compose id="Pt0.A3.p2.1.m1.1.1.2.3.cmml" xref="Pt0.A3.p2.1.m1.1.1.2.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A3.p2.1.m1.1c">-45^{\circ}</annotation></semantics></math>, <math id="Pt0.A3.p2.2.m2.1" class="ltx_Math" alttext="45^{\circ}" display="inline"><semantics id="Pt0.A3.p2.2.m2.1a"><msup id="Pt0.A3.p2.2.m2.1.1" xref="Pt0.A3.p2.2.m2.1.1.cmml"><mn id="Pt0.A3.p2.2.m2.1.1.2" xref="Pt0.A3.p2.2.m2.1.1.2.cmml">45</mn><mo id="Pt0.A3.p2.2.m2.1.1.3" xref="Pt0.A3.p2.2.m2.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="Pt0.A3.p2.2.m2.1b"><apply id="Pt0.A3.p2.2.m2.1.1.cmml" xref="Pt0.A3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="Pt0.A3.p2.2.m2.1.1.1.cmml" xref="Pt0.A3.p2.2.m2.1.1">superscript</csymbol><cn type="integer" id="Pt0.A3.p2.2.m2.1.1.2.cmml" xref="Pt0.A3.p2.2.m2.1.1.2">45</cn><compose id="Pt0.A3.p2.2.m2.1.1.3.cmml" xref="Pt0.A3.p2.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A3.p2.2.m2.1c">45^{\circ}</annotation></semantics></math>]) and flipping.
BodyNet and FaceHead/HandHead are first pre-trained separately and then end-to-end finetuned as a whole for 120 epochs in total. ZoomNet is trained on 8 GPUs with a batch size of 32 in each GPU. We use Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> with the base learning rate of 1e-3, and decay it to 1e-4 and 1e-5 at the <math id="Pt0.A3.p2.3.m3.1" class="ltx_Math" alttext="80" display="inline"><semantics id="Pt0.A3.p2.3.m3.1a"><mn id="Pt0.A3.p2.3.m3.1.1" xref="Pt0.A3.p2.3.m3.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="Pt0.A3.p2.3.m3.1b"><cn type="integer" id="Pt0.A3.p2.3.m3.1.1.cmml" xref="Pt0.A3.p2.3.m3.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A3.p2.3.m3.1c">80</annotation></semantics></math>th and <math id="Pt0.A3.p2.4.m4.1" class="ltx_Math" alttext="100" display="inline"><semantics id="Pt0.A3.p2.4.m4.1a"><mn id="Pt0.A3.p2.4.m4.1.1" xref="Pt0.A3.p2.4.m4.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="Pt0.A3.p2.4.m4.1b"><cn type="integer" id="Pt0.A3.p2.4.m4.1.1.cmml" xref="Pt0.A3.p2.4.m4.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A3.p2.4.m4.1c">100</annotation></semantics></math>th epochs respectively.</p>
</div>
</section>
<section id="Pt0.A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.D </span>Analysis</h2>

<section id="Pt0.A4.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">0.D.0.1 </span>Experiments on Foot Keypoint Dataset</h4>

<div id="Pt0.A4.SS0.SSS1.p1" class="ltx_para">
<p id="Pt0.A4.SS0.SSS1.p1.1" class="ltx_p">Cao <span id="Pt0.A4.SS0.SSS1.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. released the first human foot dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> (COCO-foot), which extends COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> dataset with 15k foot annotations. We also evaluate our proposed ZoomNet on COCO-foot dataset and directly compare with OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and SN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> in Table <a href="#Pt0.A4.T6" title="Table 6 ‣ 0.D.0.1 Experiments on Foot Keypoint Dataset ‣ Appendix 0.D Analysis ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We find that our proposed ZoomNet outperforms SN significantly.</p>
</div>
<figure id="Pt0.A4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="Pt0.A4.T6.2.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="Pt0.A4.T6.3.2" class="ltx_text" style="font-size:90%;">Body-foot AP on COCO-foot benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Some results are copied from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Our proposed ZoomNet outperforms SN significantly.</span></figcaption>
<table id="Pt0.A4.T6.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Pt0.A4.T6.4.1.1" class="ltx_tr">
<th id="Pt0.A4.T6.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Method</th>
<th id="Pt0.A4.T6.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Body AP</th>
<th id="Pt0.A4.T6.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Foot AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Pt0.A4.T6.4.2.1" class="ltx_tr">
<td id="Pt0.A4.T6.4.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Body-foot OpenPose (multi-scale) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="Pt0.A4.T6.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.3</td>
<td id="Pt0.A4.T6.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">77.9</td>
</tr>
<tr id="Pt0.A4.T6.4.3.2" class="ltx_tr">
<td id="Pt0.A4.T6.4.3.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Body-foot SN (multi-scale) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="Pt0.A4.T6.4.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">66.4</td>
<td id="Pt0.A4.T6.4.3.2.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">76.8</td>
</tr>
<tr id="Pt0.A4.T6.4.4.3" class="ltx_tr">
<td id="Pt0.A4.T6.4.4.3.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Body-foot ZoomNet</td>
<td id="Pt0.A4.T6.4.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="Pt0.A4.T6.4.4.3.2.1" class="ltx_text ltx_font_bold">75.4</span></td>
<td id="Pt0.A4.T6.4.4.3.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="Pt0.A4.T6.4.4.3.3.1" class="ltx_text ltx_font_bold">84.7</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="Pt0.A4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.D.1 </span>Experiments about joint learning.</h3>

<div id="Pt0.A4.SS1.p1" class="ltx_para">
<p id="Pt0.A4.SS1.p1.1" class="ltx_p">In Table <a href="#Pt0.A4.T7" title="Table 7 ‣ 0.D.1 Experiments about joint learning. ‣ Appendix 0.D Analysis ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we explore the effectiveness of joint training of BodyNet, FaceHead and HandHead in ZoomNet. We compare (1) joint training, (2) reusing features, and (3) fully independent face/hand detectors. Joint learning improves over “reusing features” on the performance of face (0.623 vs 0.609) and hand (0.401 vs 0.393) for more efficient feature learning. Fully independent method requires two additional models with increased complexity, but achieves limited gain (0.543 vs 0.541).</p>
</div>
<figure id="Pt0.A4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="Pt0.A4.T7.2.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="Pt0.A4.T7.3.2" class="ltx_text" style="font-size:90%;">Effectiveness of joint learning.</span></figcaption>
<table id="Pt0.A4.T7.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Pt0.A4.T7.4.1.1" class="ltx_tr">
<th id="Pt0.A4.T7.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Method</th>
<th id="Pt0.A4.T7.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Body AP</th>
<th id="Pt0.A4.T7.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Foot AP</th>
<th id="Pt0.A4.T7.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Face AP</th>
<th id="Pt0.A4.T7.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Hand AP</th>
<th id="Pt0.A4.T7.4.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">WholeBody AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Pt0.A4.T7.4.2.1" class="ltx_tr">
<th id="Pt0.A4.T7.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">joint training</th>
<td id="Pt0.A4.T7.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.743</td>
<td id="Pt0.A4.T7.4.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.798</td>
<td id="Pt0.A4.T7.4.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.623</td>
<td id="Pt0.A4.T7.4.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.401</td>
<td id="Pt0.A4.T7.4.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.541</td>
</tr>
<tr id="Pt0.A4.T7.4.3.2" class="ltx_tr">
<th id="Pt0.A4.T7.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">reusing features</th>
<td id="Pt0.A4.T7.4.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.745</td>
<td id="Pt0.A4.T7.4.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.796</td>
<td id="Pt0.A4.T7.4.3.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.609</td>
<td id="Pt0.A4.T7.4.3.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.393</td>
<td id="Pt0.A4.T7.4.3.2.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.539</td>
</tr>
<tr id="Pt0.A4.T7.4.4.3" class="ltx_tr">
<th id="Pt0.A4.T7.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">fully independent</th>
<td id="Pt0.A4.T7.4.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.745</td>
<td id="Pt0.A4.T7.4.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.796</td>
<td id="Pt0.A4.T7.4.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.623</td>
<td id="Pt0.A4.T7.4.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.419</td>
<td id="Pt0.A4.T7.4.4.3.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.543</td>
</tr>
</tbody>
</table>
</figure>
<section id="Pt0.A4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">0.D.1.1 </span>Face/Hand Bounding Box Detection</h4>

<div id="Pt0.A4.SS1.SSS1.p1" class="ltx_para">
<p id="Pt0.A4.SS1.SSS1.p1.1" class="ltx_p">In this section, we compare the results of face and hand bounding box detection. Compared to human body detection, detecting small objects such as face and hands are more challenging, since they only occupy a relatively small area in the whole image. General detection approaches such as Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> usually treat body/face/hands as normal objects and detect all of them at once. However, note that the human body is inherently a multi-level structure, where the face/hands are low-level objects of the high-level human body. Intuitively, the location of the human body will guide the detection of face/hands. Common detection methods usually ignore the inherent correlation between the human body and the face/hands, which will lead to inferior performance. To deal with the scale variance problem, ZoomNet first locates all the person bounding boxes from the image and then detects the face and hands in each bounding box. This multi-level design enables the model to focus on the potential location of the sub-objects and ignore the disturbing background. Therefore, it is beneficial for detecting small sub-objects such as face and hands. As shown in Table <a href="#Pt0.A4.T8" title="Table 8 ‣ 0.D.1.1 Face/Hand Bounding Box Detection ‣ 0.D.1 Experiments about joint learning. ‣ Appendix 0.D Analysis ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, ZoomNet outperforms the Faster RCNN model by a large margin, demonstrating the effectiveness of our multi-level object detection.</p>
</div>
<figure id="Pt0.A4.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="Pt0.A4.T8.2.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="Pt0.A4.T8.3.2" class="ltx_text" style="font-size:90%;">Face/hand bounding box detection results on our COCO-WholeBody benchmark. Our proposed ZoomNet outperforms Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> because of its multi-level design which better handles the scale variance.</span></figcaption>
<table id="Pt0.A4.T8.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Pt0.A4.T8.4.1.1" class="ltx_tr">
<th id="Pt0.A4.T8.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Method</th>
<th id="Pt0.A4.T8.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2">face</th>
<th id="Pt0.A4.T8.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2">lefthand</th>
<th id="Pt0.A4.T8.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2">righthand</th>
</tr>
<tr id="Pt0.A4.T8.4.2.2" class="ltx_tr">
<th id="Pt0.A4.T8.4.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"></th>
<th id="Pt0.A4.T8.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">AP</th>
<th id="Pt0.A4.T8.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">AR</th>
<th id="Pt0.A4.T8.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">AP</th>
<th id="Pt0.A4.T8.4.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">AR</th>
<th id="Pt0.A4.T8.4.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">AP</th>
<th id="Pt0.A4.T8.4.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">AR</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Pt0.A4.T8.4.3.1" class="ltx_tr">
<th id="Pt0.A4.T8.4.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</th>
<td id="Pt0.A4.T8.4.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.439</td>
<td id="Pt0.A4.T8.4.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.712</td>
<td id="Pt0.A4.T8.4.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.266</td>
<td id="Pt0.A4.T8.4.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.440</td>
<td id="Pt0.A4.T8.4.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.262</td>
<td id="Pt0.A4.T8.4.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.430</td>
</tr>
<tr id="Pt0.A4.T8.4.4.2" class="ltx_tr">
<th id="Pt0.A4.T8.4.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">ZoomNet</th>
<td id="Pt0.A4.T8.4.4.2.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="Pt0.A4.T8.4.4.2.2.1" class="ltx_text ltx_font_bold">0.582</span></td>
<td id="Pt0.A4.T8.4.4.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="Pt0.A4.T8.4.4.2.3.1" class="ltx_text ltx_font_bold">0.728</span></td>
<td id="Pt0.A4.T8.4.4.2.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="Pt0.A4.T8.4.4.2.4.1" class="ltx_text ltx_font_bold">0.349</span></td>
<td id="Pt0.A4.T8.4.4.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="Pt0.A4.T8.4.4.2.5.1" class="ltx_text ltx_font_bold">0.463</span></td>
<td id="Pt0.A4.T8.4.4.2.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="Pt0.A4.T8.4.4.2.6.1" class="ltx_text ltx_font_bold">0.356</span></td>
<td id="Pt0.A4.T8.4.4.2.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="Pt0.A4.T8.4.4.2.7.1" class="ltx_text ltx_font_bold">0.458</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="Pt0.A4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">0.D.1.2 </span>Error Analysis</h4>

<div id="Pt0.A4.SS1.SSS2.p1" class="ltx_para">
<p id="Pt0.A4.SS1.SSS2.p1.1" class="ltx_p">In this section, we provide a more detailed error analysis for ZoomNet and Single-Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The breakdown of errors over different body parts is shown in Fig. <a href="#Pt0.A4.F10" title="Figure 10 ‣ 0.D.1.2 Error Analysis ‣ 0.D.1 Experiments about joint learning. ‣ Appendix 0.D Analysis ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. We follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> to define four types of localization errors, <span id="Pt0.A4.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">i</span>.<span id="Pt0.A4.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">e</span>. Jitter, Miss, Inversion, and Swap. <em id="Pt0.A4.SS1.SSS2.p1.1.3" class="ltx_emph ltx_font_italic">Jitter</em> means small error around the correct keypoint location, while <em id="Pt0.A4.SS1.SSS2.p1.1.4" class="ltx_emph ltx_font_italic">Miss</em> means the detection is not within the proximity of any ground truth body part. <em id="Pt0.A4.SS1.SSS2.p1.1.5" class="ltx_emph ltx_font_italic">Inversion</em> means the joint type of detected keypoint is wrong. <em id="Pt0.A4.SS1.SSS2.p1.1.6" class="ltx_emph ltx_font_italic">Swap</em> means the detected keypoint is grouped to a wrong person instance. On the other hand, <em id="Pt0.A4.SS1.SSS2.p1.1.7" class="ltx_emph ltx_font_italic">Good</em> indicates correct prediction.</p>
</div>
<div id="Pt0.A4.SS1.SSS2.p2" class="ltx_para">
<p id="Pt0.A4.SS1.SSS2.p2.1" class="ltx_p">We use the pie chart to show the distribution of the localization errors for the body, face, hand, and whole-body. <em id="Pt0.A4.SS1.SSS2.p2.1.1" class="ltx_emph ltx_font_italic">Miss</em> is the major error for all parts, and the accuracy of the hand keypoints is lower than that of the body and face keypoints. Also, ZoomNet has a higher proportion of <em id="Pt0.A4.SS1.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">Good</em> keypoints than Single-Network.</p>
</div>
<figure id="Pt0.A4.F10" class="ltx_figure"><img src="/html/2007.11858/assets/supp_images/error_analysis.jpg" id="Pt0.A4.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="240" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A4.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="Pt0.A4.F10.3.2" class="ltx_text" style="font-size:90%;">Localization error comparison between our proposed ZoomNet (top) and Single-Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> (bottom). ZoomNet significantly outperforms Single-Network in the distribution of the localization error for body, face, hand and whole-body.</span></figcaption>
</figure>
</section>
<section id="Pt0.A4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">0.D.1.3 </span>Size Sensitivity</h4>

<div id="Pt0.A4.SS1.SSS3.p1" class="ltx_para">
<p id="Pt0.A4.SS1.SSS3.p1.4" class="ltx_p">In this section, we analyze the sensitivity of our proposed ZoomNet to different person sizes. To this end, we separate the COCO-WholeBody dataset into four size groups: <span id="Pt0.A4.SS1.SSS3.p1.4.1" class="ltx_text ltx_font_italic">i</span>.<span id="Pt0.A4.SS1.SSS3.p1.4.2" class="ltx_text ltx_font_italic">e</span>. medium (M), large (L), extra-large (XL) and extra-extra large (XX). We follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> to use the area of the person to measure the person size, M for <math id="Pt0.A4.SS1.SSS3.p1.1.m1.2" class="ltx_Math" alttext="area\in[32^{2},64^{2}]" display="inline"><semantics id="Pt0.A4.SS1.SSS3.p1.1.m1.2a"><mrow id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.cmml"><mrow id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.cmml"><mi id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.2" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.1" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.3" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.1a" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.4" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.1b" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.5" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.5.cmml">a</mi></mrow><mo id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.3" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.3.cmml">∈</mo><mrow id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.3" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.3.cmml">[</mo><msup id="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1" xref="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1.cmml"><mn id="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1.2" xref="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1.2.cmml">32</mn><mn id="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1.3" xref="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.4" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.3.cmml">,</mo><msup id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2.cmml"><mn id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2.2" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2.2.cmml">64</mn><mn id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2.3" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2.3.cmml">2</mn></msup><mo stretchy="false" id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.5" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A4.SS1.SSS3.p1.1.m1.2b"><apply id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2"><in id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.3"></in><apply id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4"><times id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.1"></times><ci id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.2">𝑎</ci><ci id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.3">𝑟</ci><ci id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.4.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.4">𝑒</ci><ci id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.5.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.4.5">𝑎</ci></apply><interval closure="closed" id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2"><apply id="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1">superscript</csymbol><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1.2">32</cn><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.1.1.1.1.1.3">2</cn></apply><apply id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2">superscript</csymbol><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2.2">64</cn><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.1.m1.2.2.2.2.2.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A4.SS1.SSS3.p1.1.m1.2c">area\in[32^{2},64^{2}]</annotation></semantics></math>, L for <math id="Pt0.A4.SS1.SSS3.p1.2.m2.2" class="ltx_Math" alttext="area\in[64^{2},96^{2}]" display="inline"><semantics id="Pt0.A4.SS1.SSS3.p1.2.m2.2a"><mrow id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.cmml"><mrow id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.cmml"><mi id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.2" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.1" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.3" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.1a" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.4" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.1b" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.5" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.5.cmml">a</mi></mrow><mo id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.3" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.3.cmml">∈</mo><mrow id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.3.cmml"><mo stretchy="false" id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.3" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.3.cmml">[</mo><msup id="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1" xref="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1.cmml"><mn id="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1.2" xref="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1.2.cmml">64</mn><mn id="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1.3" xref="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1.3.cmml">2</mn></msup><mo id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.4" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.3.cmml">,</mo><msup id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2.cmml"><mn id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2.2" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2.2.cmml">96</mn><mn id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2.3" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2.3.cmml">2</mn></msup><mo stretchy="false" id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.5" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A4.SS1.SSS3.p1.2.m2.2b"><apply id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2"><in id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.3"></in><apply id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4"><times id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.1"></times><ci id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.2">𝑎</ci><ci id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.3">𝑟</ci><ci id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.4.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.4">𝑒</ci><ci id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.5.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.4.5">𝑎</ci></apply><interval closure="closed" id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2"><apply id="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1">superscript</csymbol><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1.2">64</cn><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.1.1.1.1.1.3">2</cn></apply><apply id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2">superscript</csymbol><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2.2">96</cn><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.2.m2.2.2.2.2.2.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A4.SS1.SSS3.p1.2.m2.2c">area\in[64^{2},96^{2}]</annotation></semantics></math>, XL for <math id="Pt0.A4.SS1.SSS3.p1.3.m3.2" class="ltx_Math" alttext="area\in[96^{2},128^{2}]" display="inline"><semantics id="Pt0.A4.SS1.SSS3.p1.3.m3.2a"><mrow id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.cmml"><mrow id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.cmml"><mi id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.2" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.1" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.3" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.1a" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.4" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.1b" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.5" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.5.cmml">a</mi></mrow><mo id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.3" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.3.cmml">∈</mo><mrow id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.3.cmml"><mo stretchy="false" id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.3" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.3.cmml">[</mo><msup id="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1" xref="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1.cmml"><mn id="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1.2" xref="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1.2.cmml">96</mn><mn id="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1.3" xref="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1.3.cmml">2</mn></msup><mo id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.4" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.3.cmml">,</mo><msup id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2.cmml"><mn id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2.2" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2.2.cmml">128</mn><mn id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2.3" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2.3.cmml">2</mn></msup><mo stretchy="false" id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.5" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A4.SS1.SSS3.p1.3.m3.2b"><apply id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2"><in id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.3"></in><apply id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4"><times id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.1"></times><ci id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.2">𝑎</ci><ci id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.3">𝑟</ci><ci id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.4.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.4">𝑒</ci><ci id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.5.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.4.5">𝑎</ci></apply><interval closure="closed" id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2"><apply id="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1">superscript</csymbol><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1.2">96</cn><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.1.1.1.1.1.3">2</cn></apply><apply id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2">superscript</csymbol><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2.2">128</cn><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.3.m3.2.2.2.2.2.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A4.SS1.SSS3.p1.3.m3.2c">area\in[96^{2},128^{2}]</annotation></semantics></math>, and XX for <math id="Pt0.A4.SS1.SSS3.p1.4.m4.1" class="ltx_Math" alttext="area&gt;128^{2}" display="inline"><semantics id="Pt0.A4.SS1.SSS3.p1.4.m4.1a"><mrow id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.cmml"><mrow id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.cmml"><mi id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.2" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.1" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.3" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.1a" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.4" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.1b" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.1.cmml">​</mo><mi id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.5" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.5.cmml">a</mi></mrow><mo id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.1" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.1.cmml">&gt;</mo><msup id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3.cmml"><mn id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3.2" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3.2.cmml">128</mn><mn id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3.3" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A4.SS1.SSS3.p1.4.m4.1b"><apply id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1"><gt id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.1"></gt><apply id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2"><times id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.1"></times><ci id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.2">𝑎</ci><ci id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.3">𝑟</ci><ci id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.4.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.4">𝑒</ci><ci id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.5.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.2.5">𝑎</ci></apply><apply id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3.1.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3">superscript</csymbol><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3.2.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3.2">128</cn><cn type="integer" id="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3.3.cmml" xref="Pt0.A4.SS1.SSS3.p1.4.m4.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A4.SS1.SSS3.p1.4.m4.1c">area&gt;128^{2}</annotation></semantics></math>. In Fig. <a href="#Pt0.A4.F11" title="Figure 11 ‣ 0.D.1.3 Size Sensitivity ‣ 0.D.1 Experiments about joint learning. ‣ Appendix 0.D Analysis ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, we show the AP improvement obtained after correcting each type of localization error. We find that for body and face keypoint localization, the performance can be significantly improved by correcting small-scale human poses, especially the Missing error. For hand pose estimation, errors impact performance more on larger instances. For larger-scale instance, instead of only estimating the rough position, more accurate keypoint localization is required. However, due to the frequent motion blur and severe occlusion (interaction with objects), it is still very challenging to estimating the hand poses of large instances.</p>
</div>
<figure id="Pt0.A4.F11" class="ltx_figure"><img src="/html/2007.11858/assets/supp_images/size_sensitivity.jpg" id="Pt0.A4.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A4.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="Pt0.A4.F11.3.2" class="ltx_text" style="font-size:90%;">The AP improvement obtained by correcting each type of error (including Miss, Swap, Inversion, and Jitter) for body, face, and hand separately. We use the dashed red lines to indicate performance improvement over all the instance sizes.</span></figcaption>
</figure>
</section>
<section id="Pt0.A4.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">0.D.1.4 </span>Qualitative Analysis</h4>

<div id="Pt0.A4.SS1.SSS4.p1" class="ltx_para">
<p id="Pt0.A4.SS1.SSS4.p1.1" class="ltx_p">Fig. <a href="#Pt0.A4.F12" title="Figure 12 ‣ 0.D.1.4 Qualitative Analysis ‣ 0.D.1 Experiments about joint learning. ‣ Appendix 0.D Analysis ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> shows the qualitative evaluation results of our approach, and Fig. <a href="#Pt0.A4.F13" title="Figure 13 ‣ 0.D.1.4 Qualitative Analysis ‣ 0.D.1 Experiments about joint learning. ‣ Appendix 0.D Analysis ‣ Whole-Body Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> qualitatively compares the results of ZoomNet, OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and Single-Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Both of them show the capacity of our proposed ZoomNet in handling challenges including occlusion, close proximity, and small scale persons. We find that our ZoomNet significantly outperforms the previous state-of-the-art method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, especially for face/hand keypoints. First, we observe that compared to these bottom-up approaches, ZoomNet better handles the small scale problem of human instances (see Line#1,2,3). Second, we find that the grouping of OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and Single-Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is sometimes erroneous due to lack of human body constraints (see Line#4). Third, ZoomNet is generally better at localizing the hand/face keypoints with occlusion, pose variations, and small scales (see Line#6,7). ZoomNet improves upon the state-of-the-art methods by zooming in to the hand area for higher resolution. However, we also find some failure cases of our proposed ZoomNet. We observe that it still has difficulty in dealing with small face/hands with low-resolution and motion blur.</p>
</div>
<figure id="Pt0.A4.F12" class="ltx_figure"><img src="/html/2007.11858/assets/supp_images/ZoomNet_result.jpg" id="Pt0.A4.F12.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="538" height="866" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A4.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="Pt0.A4.F12.3.2" class="ltx_text" style="font-size:90%;">Qualitative evaluation results of our approach in handling challenges including occlusion, close proximity, and small scale persons.
</span></figcaption>
</figure>
<figure id="Pt0.A4.F13" class="ltx_figure"><img src="/html/2007.11858/assets/supp_images/vis_compare.jpg" id="Pt0.A4.F13.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="479" height="839" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A4.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="Pt0.A4.F13.3.2" class="ltx_text" style="font-size:90%;">Qualitative comparison between our proposed ZoomNet, OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and Single-Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Our approach outperforms the state-of-the-art approaches especially on face/hand keypoints and are more robust to scale variance.
</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2007.11857" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2007.11858" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2007.11858">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2007.11858" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2007.11859" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 10:01:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
