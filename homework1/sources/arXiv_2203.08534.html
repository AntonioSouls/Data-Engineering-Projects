<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2203.08534] Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video</title><meta property="og:description" content="Learning to capture human motion is essential to 3D human pose and shape estimation from monocular video. However, the existing methods mainly rely on recurrent or convolutional operation to model such temporal informa…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2203.08534">

<!--Generated on Mon Mar 11 06:37:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wen-Li Wei<sup id="id6.4.id1" class="ltx_sup">∗</sup>, Jen-Chun Lin<sup id="id7.5.id2" class="ltx_sup">∗</sup>, Tyng-Luh Liu, and Hong-Yuan Mark Liao<sup id="id8.6.id3" class="ltx_sup"><span id="id8.6.id3.1" class="ltx_text ltx_font_italic">†</span></sup>
<br class="ltx_break">Institute of Information Science, Academia Sinica, Taiwan
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jen-Chun Lin*
<br class="ltx_break">Academia Sinica
<br class="ltx_break"><span id="id9.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">jenchunlin@gmail.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tyng-Luh Liu
<br class="ltx_break">Academia Sinica
<br class="ltx_break"><span id="id10.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">liutyng@iis.sinica.edu.tw</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hong-Yuan Mark Liao
<br class="ltx_break">Academia Sinica
<br class="ltx_break"><span id="id11.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">liao@iis.sinica.edu.tw</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">Learning to capture human motion is essential to 3D human pose and shape estimation from monocular video. However, the existing methods mainly rely on recurrent or convolutional operation to model such temporal information, which limits the ability to capture non-local context relations of human motion. To address this problem, we propose a motion pose and shape network (MPS-Net) to effectively capture humans in motion to estimate accurate and temporally coherent 3D human pose and shape from a video. Specifically, we first propose a motion continuity attention (MoCA) module that leverages visual cues observed from human motion to adaptively recalibrate the range that needs attention in the sequence to better capture the motion continuity dependencies. Then, we develop a hierarchical attentive feature integration (HAFI) module to effectively combine adjacent past and future feature representations to strengthen temporal correlation and refine the feature representation of the current frame. By coupling the MoCA and HAFI modules, the proposed MPS-Net excels in estimating 3D human pose and shape in the video. Though conceptually simple, our MPS-Net not only outperforms the state-of-the-art methods on the 3DPW, MPI-INF-3DHP, and Human3.6M benchmark datasets, but also uses fewer network parameters. The video demos can be found at <a target="_blank" href="https://mps-net.github.io/MPS-Net/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mps-net.github.io/MPS-Net/</a>.</p>
</div>
<div id="id5" class="ltx_logical-block">
<div id="id5.p1" class="ltx_para">
<img src="/html/2203.08534/assets/Fig1.jpg" id="id4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="631" height="210" alt="[Uncaptioned image]">
</div>
<figure id="S0.F1" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.5.2" class="ltx_text" style="font-size:90%;">By coupling <em id="S0.F1.5.2.1" class="ltx_emph ltx_font_italic">motion continuity attention</em> with <em id="S0.F1.5.2.2" class="ltx_emph ltx_font_italic">hierarchical attentive feature integration</em>, the proposed MPS-Net can achieve more accurate pose and shape estimations (bottom row), when dealing with in-the-wild videos. For comparison, the results (top row) obtained by TCMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, the state-of-the-art video-based 3D human pose and shape estimation method, are included. </span></figcaption>
</figure>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>Both authors contributed equally to this work</span></span></span><span id="footnotex2" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>Mark Liao is also a Chair Professor of Providence University</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.2" class="ltx_p">Estimating 3D human pose and shape by taking a simple picture/video without relying on sophisticated 3D scanning devices or multi-view stereo algorithms, has important applications in computer graphics, AR/VR, physical therapy and beyond. Generally speaking, the task is to take a single image or video sequence as input and to estimate the parameters of a 3D human mesh model as output. Take, for example, the SMPL model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. For each image, it needs to estimate <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="85" display="inline"><semantics id="S1.p1.1.m1.1a"><mn id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">85</mn><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><cn type="integer" id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">85</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">85</annotation></semantics></math> (including pose, shape, and camera) parameters, which control the <math id="S1.p1.2.m2.1" class="ltx_Math" alttext="6890" display="inline"><semantics id="S1.p1.2.m2.1a"><mn id="S1.p1.2.m2.1.1" xref="S1.p1.2.m2.1.1.cmml">6890</mn><annotation-xml encoding="MathML-Content" id="S1.p1.2.m2.1b"><cn type="integer" id="S1.p1.2.m2.1.1.cmml" xref="S1.p1.2.m2.1.1">6890</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.2.m2.1c">6890</annotation></semantics></math> vertices that form the full 3D mesh of a human body <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Despite recent progress on 3D human pose and shape estimation, it is still a frontier challenge due to depth ambiguity, limited 3D annotations, and complex motion of non-rigid human body <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Different from 3D human pose and shape estimation from a single image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, estimating it from monocular video is a more complex task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. It needs to not only estimate the pose, shape and camera parameters of each image, but also correlate the continuity of human motion in the sequence. Although existing single image-based methods can predict a reasonable output from a static image, it is difficult for them to estimate temporally coherent and smooth 3D human pose and shape in the video sequence due to the lack of modeling the continuity of human motion in consecutive frames.
To solve this problem, several methods have recently been proposed to extend the single image-based methods to the video cases, which mainly rely on recurrent neural network (RNN) or convolutional neural network (CNN) to model temporal information (<em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.2" class="ltx_text"></span>, continuity of human motion) for coherent predictions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. However, RNNs and CNNs are good at dealing with local neighborhoods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, and the models alone may not be effective for learning long-range dependencies (<em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.4" class="ltx_text"></span>, non-local context relations) between feature representations to describe the relevance of human motion. As a result, there is still room for improvement for existing video-based methods to estimate accurate and smooth 3D human pose and shape (see Figure <a href="#S0.F1" title="Figure 1 ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2203.08534/assets/Fig2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="574" height="490" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.6.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.2.1" class="ltx_text" style="font-size:90%;">Visualization of the attention map generated by the self-attention module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> in 3D human pose and shape estimation. The visualization shows that the attention map is easy to focus attention on less correlated temporal positions (<em id="S1.F2.2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.F2.2.1.2" class="ltx_text"></span>, far apart frames with very different action poses) and lead to inaccurate 3D human pose and shape estimation (see frame <math id="S1.F2.2.1.m1.1" class="ltx_Math" alttext="{\mathbf{I}}_{t}" display="inline"><semantics id="S1.F2.2.1.m1.1b"><msub id="S1.F2.2.1.m1.1.1" xref="S1.F2.2.1.m1.1.1.cmml"><mi id="S1.F2.2.1.m1.1.1.2" xref="S1.F2.2.1.m1.1.1.2.cmml">𝐈</mi><mi id="S1.F2.2.1.m1.1.1.3" xref="S1.F2.2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F2.2.1.m1.1c"><apply id="S1.F2.2.1.m1.1.1.cmml" xref="S1.F2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S1.F2.2.1.m1.1.1.1.cmml" xref="S1.F2.2.1.m1.1.1">subscript</csymbol><ci id="S1.F2.2.1.m1.1.1.2.cmml" xref="S1.F2.2.1.m1.1.1.2">𝐈</ci><ci id="S1.F2.2.1.m1.1.1.3.cmml" xref="S1.F2.2.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.2.1.m1.1d">{\mathbf{I}}_{t}</annotation></semantics></math>). In the attention map, red indicates a higher attention value, and blue indicates a lower one.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address the aforementioned issue, we propose a motion pose and shape network (MPS-Net) for 3D human pose and shape estimation from monocular video. Our key insights are two-fold. First, although a self-attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> has recently been proposed to compensate (<em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p3.1.2" class="ltx_text"></span>, better learn long-range dependencies) for the weaknesses of recurrent and convolutional operations, we empirically find that it is not always good at modeling human motion in the action sequence. Because the attention map computed by the self-attention module is often unstable, which is easy to focus attention on less correlated temporal positions (<em id="S1.p3.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p3.1.4" class="ltx_text"></span>, far apart frames with very different action poses) and ignore the continuity of human motion in the action sequence (see Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). To this end, we propose a motion continuity attention (MoCA) module to achieve the adaptability to diverse temporal content and relations in the action sequence. Specifically, the MoCA module contributes in two points. First, a normalized self-similarity matrix (NSSM) is developed to capture the structure of temporal similarities and dissimilarities of visual representations in the action sequence, thereby revealing the continuity of human motion. Second, NSSM is regarded as the a <em id="S1.p3.1.5" class="ltx_emph ltx_font_italic">priori</em> knowledge and applied to guide the learning of the self-attention module, which allows it to adaptively recalibrate the range that needs attention in the sequence to capture the motion continuity dependencies.
In the second insight, motivated by the temporal feature integration scheme in 3D human mesh estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we develop a hierarchical attentive feature integration (HAFI) module that utilizes adjacent feature representations observed from past and future frames to strengthen temporal correlation and refine the feature representation of the current frame. By coupling the MoCA and HAFI modules, our MPS-Net can effectively capture humans in motion to estimate accurate and temporally coherent 3D human pose and shape from monocular video (see Figure <a href="#S0.F1" title="Figure 1 ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). We characterize the main contributions of our MPS-Net as follows:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a MoCA module that leverages visual cues observed from human motion to adaptively recalibrate the range that needs attention in the sequence to better capture the motion continuity dependencies.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We develop a HAFI module that effectively combines adjacent past and future feature representations in a hierarchical attentive integration manner to strengthen temporal correlation and refine the feature representation of the current frame.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Extensive experiments on three standard benchmark datasets demonstrate that our MPS-Net achieves the state-of-the-art performance against existing methods and uses fewer network parameters.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">3D human pose and shape estimation from a single image.</span> The existing single image-based 3D human pose and shape estimation methods are mainly based on parametric 3D human mesh models, such as SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.p1.1.3" class="ltx_text"></span>, trains a deep-net model to estimate pose, shape, and camera parameters from the input image, and then decodes them into a 3D mesh of the human body through the SMPL model. For example, Kanazawa <em id="S2.p1.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> proposed an end-to-end human mesh recovery (HMR) framework to regress SMPL parameters from a single RGB image. They employ 3D to 2D keypoint reprojection loss and adversarial training to alleviate the limited 3D annotation problem and make the output 3D human mesh anatomically reasonable. Pavlakos <em id="S2.p1.1.6" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.7" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> used 2D joint heatmaps and silhouette as cues to improve the accuracy of SMPL parameter estimation. Similarly, Omran <em id="S2.p1.1.8" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.9" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> used a semantic segmentation scheme to extract body part information as a cue to estimate the SMPL parameters. Kolotouros <em id="S2.p1.1.10" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.11" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> proposed a self-improving framework that integrates the SMPL parameter regressor and iterative fitting scheme to better estimate 3D human pose and shape. Zhang <em id="S2.p1.1.12" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.13" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> designed a pyramidal mesh alignment feedback (PyMAF) loop in the deep SMPL parameter regressor to exploit multi-scale contexts for better mesh-image alignment of the reconstruction.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Several non-parametric 3D human mesh reconstruction methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> have been proposed. For example, Kolotouros <em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> proposed a graph CNN, which takes the 3D human mesh template and image embedding (extracted from ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>) as input to directly regress the vertex coordinates of the 3D mesh. Moon and Lee <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> proposed an I2L-MeshNet, which uses a lixel-based 1D heatmap to directly localize the vertex coordinates of the 3D mesh in a fully convolutional manner.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Despite the above methods are effective for static images, they are difficult to generate temporally coherent and smooth 3D human pose and shape in the video sequence, <em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.p3.1.2" class="ltx_text"></span>, jittery, unstable 3D human motion may occur <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">3D human pose and shape estimation from monocular video.</span> Similar to the single image-based methods, the existing video-based 3D human pose and shape estimation methods are mainly based on the SMPL model. For example, Kanazawa <em id="S2.p4.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p4.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> proposed a convolution-based temporal encoder to learn human motion kinematics by further estimating SMPL parameters in adjacent past and future frames. Doersch <em id="S2.p4.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p4.1.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> trained their model on a sequence of 2D keypoint heatmaps and optical flow by combining CNN and long short-term memory (LSTM) network to demonstrate that considering pre-processed motion information can improve SMPL parameter estimation. Sun <em id="S2.p4.1.6" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p4.1.7" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> proposed a skeleton-disentangling framework, which divides the task into multi-level spatial and temporal sub-problems. They further proposed an unsupervised adversarial training strategy, namely temporal shuffles and order recovery, to encourage temporal feature learning. Kocabas <em id="S2.p4.1.8" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p4.1.9" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> proposed a temporal encoder composed of bidirectional gated recurrent units (GRU) to encode static features into a series of temporally correlated latent features, and feed them to the regressor to estimate SMPL parameters. They further integrated adversarial training strategy that leverages the AMASS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> to distinguish between real human motion and those estimated by its regressor to encourage the generation of reasonable 3D human motion. Luo <em id="S2.p4.1.10" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p4.1.11" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> proposed a two-stage model that first estimates the coarse 3D human motion through a variational motion estimator, and then uses a motion residual regressor to refine the motion estimates. Recently, Choi <em id="S2.p4.1.12" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p4.1.13" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> proposed a temporally consistent mesh recovery (TCMR) system that uses GRU-based temporal encoders with three different encoding strategies to encourage the network to better learn temporal features. In addition, they proposed a temporal feature integration scheme that combines the output of three temporal encoders to help the SMPL parameter regressor estimate accurate and smooth 3D human pose and shape.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Despite the success of RNNs and CNNs, both recurrent and convolutional operations can only deal with local neighborhoods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, which makes it difficult for them to learn long-range dependencies (<em id="S2.p5.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.p5.1.2" class="ltx_text"></span>, non-local context relations) between feature representations in the action sequence. Therefore, existing methods are still struggling to estimate accurate and smooth 3D human pose and shape.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Attention mechanism.</span> The attention mechanism has enjoyed widespread adoption as a computational module for natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and vision-related tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> because of its ability to capture long-range dependencies and selectively concentrate on the relevant subset of the input. There are various ways to implement the attention mechanism. Here we focus on self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. For example, Vaswani <em id="S2.p6.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p6.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> proposed a self-attention-based architecture called <em id="S2.p6.1.4" class="ltx_emph ltx_font_italic">Transformer</em>, in which the self-attention module is designed to update each sentence’s element through the entire sentence’s aggregated information to draw global dependencies between input and output. The <em id="S2.p6.1.5" class="ltx_emph ltx_font_italic">Transformer</em> entirely replaces the recurrent operation with the self-attention module, and greatly improves the performance of machine translation. Later, Wang <em id="S2.p6.1.6" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p6.1.7" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> showed that self-attention is an instantiation of non-local mean <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, and proposed a non-local block for the CNN to capture long-range dependencies. Like the self-attention module proposed in <em id="S2.p6.1.8" class="ltx_emph ltx_font_italic">Transformer</em>, the non-local operation computes the correlation between each position in the input feature representation to generate an attention map, and then performs the attention-guided dense context information aggregation to draw long-range dependencies.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Despite the self-attention mechanism performs well, we empirically find that the attention map computed by the self-attention module (<em id="S2.p7.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p7.1.2" class="ltx_text"></span>, non-local block) is often unstable, which means that it is easy to focus attention on less correlated temporal positions (<em id="S2.p7.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.p7.1.4" class="ltx_text"></span>, far apart frames with very different action poses) and ignore the continuity of human motion in the action sequence (see Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). In this work, we propose the MoCA module, which extends the learning of the self-attention module by introducing the a <em id="S2.p7.1.5" class="ltx_emph ltx_font_italic">priori</em> knowledge of NSSM to adaptively recalibrate the range that needs attention in the sequence, so as to capture motion continuity dependencies. The HAFI module is further proposed to strengthen the temporal correlation and refine the feature representation of each frame through its neighbors.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the overall pipeline of our MPS-Net. We elaborate each module in MPS-Net as follows.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2203.08534/assets/Fig3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="550" height="249" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.4.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.2.1" class="ltx_text" style="font-size:90%;">Overview of our motion pose and shape network (MPS-Net). MPS-Net estimates pose, shape, and camera parameters <math id="S3.F3.2.1.m1.1" class="ltx_Math" alttext="\Theta" display="inline"><semantics id="S3.F3.2.1.m1.1b"><mi mathvariant="normal" id="S3.F3.2.1.m1.1.1" xref="S3.F3.2.1.m1.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.1.m1.1c"><ci id="S3.F3.2.1.m1.1.1.cmml" xref="S3.F3.2.1.m1.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.1.m1.1d">\Theta</annotation></semantics></math> in the video sequence based on the static feature extractor, temporal encoder, temporal feature integration, and SMPL parameter regressor to generate 3D human pose and shape.</span></figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2203.08534/assets/Fig4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="485" height="404" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.20.10.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.18.9" class="ltx_text" style="font-size:90%;">A MoCA module. <math id="S3.F4.10.1.m1.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S3.F4.10.1.m1.1b"><mi id="S3.F4.10.1.m1.1.1" xref="S3.F4.10.1.m1.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S3.F4.10.1.m1.1c"><ci id="S3.F4.10.1.m1.1.1.cmml" xref="S3.F4.10.1.m1.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.10.1.m1.1d">\mathbf{X}</annotation></semantics></math> is shown as the shape of <math id="S3.F4.11.2.m2.1" class="ltx_Math" alttext="T\times 2048" display="inline"><semantics id="S3.F4.11.2.m2.1b"><mrow id="S3.F4.11.2.m2.1.1" xref="S3.F4.11.2.m2.1.1.cmml"><mi id="S3.F4.11.2.m2.1.1.2" xref="S3.F4.11.2.m2.1.1.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.F4.11.2.m2.1.1.1" xref="S3.F4.11.2.m2.1.1.1.cmml">×</mo><mn id="S3.F4.11.2.m2.1.1.3" xref="S3.F4.11.2.m2.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.11.2.m2.1c"><apply id="S3.F4.11.2.m2.1.1.cmml" xref="S3.F4.11.2.m2.1.1"><times id="S3.F4.11.2.m2.1.1.1.cmml" xref="S3.F4.11.2.m2.1.1.1"></times><ci id="S3.F4.11.2.m2.1.1.2.cmml" xref="S3.F4.11.2.m2.1.1.2">𝑇</ci><cn type="integer" id="S3.F4.11.2.m2.1.1.3.cmml" xref="S3.F4.11.2.m2.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.11.2.m2.1d">T\times 2048</annotation></semantics></math> for <math id="S3.F4.12.3.m3.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S3.F4.12.3.m3.1b"><mn id="S3.F4.12.3.m3.1.1" xref="S3.F4.12.3.m3.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S3.F4.12.3.m3.1c"><cn type="integer" id="S3.F4.12.3.m3.1.1.cmml" xref="S3.F4.12.3.m3.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.12.3.m3.1d">2048</annotation></semantics></math> channels. <math id="S3.F4.13.4.m4.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.F4.13.4.m4.1b"><mi id="S3.F4.13.4.m4.1.1" xref="S3.F4.13.4.m4.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.F4.13.4.m4.1c"><ci id="S3.F4.13.4.m4.1.1.cmml" xref="S3.F4.13.4.m4.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.13.4.m4.1d">g</annotation></semantics></math>, <math id="S3.F4.14.5.m5.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.F4.14.5.m5.1b"><mi id="S3.F4.14.5.m5.1.1" xref="S3.F4.14.5.m5.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.F4.14.5.m5.1c"><ci id="S3.F4.14.5.m5.1.1.cmml" xref="S3.F4.14.5.m5.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.14.5.m5.1d">\phi</annotation></semantics></math>, <math id="S3.F4.15.6.m6.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.F4.15.6.m6.1b"><mi id="S3.F4.15.6.m6.1.1" xref="S3.F4.15.6.m6.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.F4.15.6.m6.1c"><ci id="S3.F4.15.6.m6.1.1.cmml" xref="S3.F4.15.6.m6.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.15.6.m6.1d">\theta</annotation></semantics></math>, and <math id="S3.F4.16.7.m7.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S3.F4.16.7.m7.1b"><mi id="S3.F4.16.7.m7.1.1" xref="S3.F4.16.7.m7.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S3.F4.16.7.m7.1c"><ci id="S3.F4.16.7.m7.1.1.cmml" xref="S3.F4.16.7.m7.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.16.7.m7.1d">\rho</annotation></semantics></math> denote convolutional operations, <math id="S3.F4.17.8.m8.1" class="ltx_Math" alttext="\otimes" display="inline"><semantics id="S3.F4.17.8.m8.1b"><mo id="S3.F4.17.8.m8.1.1" xref="S3.F4.17.8.m8.1.1.cmml">⊗</mo><annotation-xml encoding="MathML-Content" id="S3.F4.17.8.m8.1c"><csymbol cd="latexml" id="S3.F4.17.8.m8.1.1.cmml" xref="S3.F4.17.8.m8.1.1">tensor-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.17.8.m8.1d">\otimes</annotation></semantics></math> denotes matrix multiplication, and <math id="S3.F4.18.9.m9.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S3.F4.18.9.m9.1b"><mo id="S3.F4.18.9.m9.1.1" xref="S3.F4.18.9.m9.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S3.F4.18.9.m9.1c"><csymbol cd="latexml" id="S3.F4.18.9.m9.1.1.cmml" xref="S3.F4.18.9.m9.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.18.9.m9.1d">\oplus</annotation></semantics></math> denotes element-wise sum. The computation of softmax is performed on each row.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Temporal encoder</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.7" class="ltx_p">Given an input video sequence <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="{\mathbf{V}}=\{{\mathbf{I}}_{t}\}_{t=1}^{T}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">𝐕</mi><mo id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">=</mo><msubsup id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml">𝐈</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.1.m1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p1.1.m1.1.1.1.1.3.1" xref="S3.SS1.p1.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p1.1.m1.1.1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p1.1.m1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.3.cmml">T</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><eq id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2"></eq><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝐕</ci><apply id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1">subscript</csymbol><set id="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1"><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2">𝐈</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3">𝑡</ci></apply></set><apply id="S3.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3"><eq id="S3.SS1.p1.1.m1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3.1"></eq><ci id="S3.SS1.p1.1.m1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS1.p1.1.m1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.1.m1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">{\mathbf{V}}=\{{\mathbf{I}}_{t}\}_{t=1}^{T}</annotation></semantics></math> with <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">T</annotation></semantics></math> frames. We first use ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> pre-trained by Kolotouros <em id="S3.SS1.p1.7.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS1.p1.7.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to extract the static feature of each frame to form a static feature representation sequence <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="{\mathbf{X}}=\{{\mathbf{x}}_{t}\}_{t=1}^{T}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">𝐗</mi><mo id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">=</mo><msubsup id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml"><mrow id="S3.SS1.p1.3.m3.1.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.3.m3.1.1.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p1.3.m3.1.1.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.2.cmml">𝐱</mi><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S3.SS1.p1.3.m3.1.1.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.3.m3.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p1.3.m3.1.1.1.1.3.1" xref="S3.SS1.p1.3.m3.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p1.3.m3.1.1.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p1.3.m3.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.3.cmml">T</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><eq id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"></eq><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝐗</ci><apply id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.3.m3.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1">subscript</csymbol><set id="S3.SS1.p1.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1"><apply id="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.2">𝐱</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.3">𝑡</ci></apply></set><apply id="S3.SS1.p1.3.m3.1.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.3"><eq id="S3.SS1.p1.3.m3.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.3.1"></eq><ci id="S3.SS1.p1.3.m3.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS1.p1.3.m3.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.3.m3.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">{\mathbf{X}}=\{{\mathbf{x}}_{t}\}_{t=1}^{T}</annotation></semantics></math>, where <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="{\mathbf{x}}_{t}\in{\mathbb{R}}^{2048}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><msub id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2.2" xref="S3.SS1.p1.4.m4.1.1.2.2.cmml">𝐱</mi><mi id="S3.SS1.p1.4.m4.1.1.2.3" xref="S3.SS1.p1.4.m4.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.p1.4.m4.1.1.3.2" xref="S3.SS1.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS1.p1.4.m4.1.1.3.3" xref="S3.SS1.p1.4.m4.1.1.3.3.cmml">2048</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><in id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></in><apply id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.2.1.cmml" xref="S3.SS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2.2">𝐱</ci><ci id="S3.SS1.p1.4.m4.1.1.2.3.cmml" xref="S3.SS1.p1.4.m4.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.p1.4.m4.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">{\mathbf{x}}_{t}\in{\mathbb{R}}^{2048}</annotation></semantics></math>. Then, the extracted <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="{\mathbf{X}}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">{\mathbf{X}}</annotation></semantics></math> is sent to the proposed MoCA module to calculate the temporal feature representation sequence <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="{\mathbf{Z}}=\{{\mathbf{z}}_{t}\}_{t=1}^{T}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">𝐙</mi><mo id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">=</mo><msubsup id="S3.SS1.p1.6.m6.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.cmml"><mrow id="S3.SS1.p1.6.m6.1.1.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.6.m6.1.1.1.1.1.1.2" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.cmml">𝐳</mi><mi id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S3.SS1.p1.6.m6.1.1.1.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.6.m6.1.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.1.3.2" xref="S3.SS1.p1.6.m6.1.1.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p1.6.m6.1.1.1.1.3.1" xref="S3.SS1.p1.6.m6.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p1.6.m6.1.1.1.1.3.3" xref="S3.SS1.p1.6.m6.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p1.6.m6.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.3.cmml">T</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><eq id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2"></eq><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">𝐙</ci><apply id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.6.m6.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1">subscript</csymbol><set id="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1"><apply id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2">𝐳</ci><ci id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.3">𝑡</ci></apply></set><apply id="S3.SS1.p1.6.m6.1.1.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.3"><eq id="S3.SS1.p1.6.m6.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.3.1"></eq><ci id="S3.SS1.p1.6.m6.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS1.p1.6.m6.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.6.m6.1.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">{\mathbf{Z}}=\{{\mathbf{z}}_{t}\}_{t=1}^{T}</annotation></semantics></math>, where <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="{\mathbf{z}}_{t}\in{\mathbb{R}}^{2048}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mrow id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><msub id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2.2" xref="S3.SS1.p1.7.m7.1.1.2.2.cmml">𝐳</mi><mi id="S3.SS1.p1.7.m7.1.1.2.3" xref="S3.SS1.p1.7.m7.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS1.p1.7.m7.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml"><mi id="S3.SS1.p1.7.m7.1.1.3.2" xref="S3.SS1.p1.7.m7.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS1.p1.7.m7.1.1.3.3" xref="S3.SS1.p1.7.m7.1.1.3.3.cmml">2048</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><in id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1"></in><apply id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.2.1.cmml" xref="S3.SS1.p1.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2.2">𝐳</ci><ci id="S3.SS1.p1.7.m7.1.1.2.3.cmml" xref="S3.SS1.p1.7.m7.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.3.1.cmml" xref="S3.SS1.p1.7.m7.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.3.2.cmml" xref="S3.SS1.p1.7.m7.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS1.p1.7.m7.1.1.3.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">{\mathbf{z}}_{t}\in{\mathbb{R}}^{2048}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">MoCA Module.</span> We propose a MoCA operation to extend the non-local operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> in two ways. First, we introduce an NSSM to capture the structure of temporal similarities and dissimilarities of visual representations in the action sequence to reveal the continuity of human motion. Second, we regard NSSM as the a <em id="S3.SS1.p2.1.2" class="ltx_emph ltx_font_italic">priori</em> knowledge and combine it with the attention map generated by the non-local operation to adaptively recalibrate the range that needs attention in the action sequence.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p">We formulate the proposed MoCA module as follows (see Figure <a href="#S3.F4" title="Figure 4 ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). Given the static feature representation sequence <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="{\mathbf{X}}\in{\mathbb{R}}^{T\times 2048}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">𝐗</mi><mo id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml"><mi id="S3.SS1.p3.1.m1.1.1.3.2" xref="S3.SS1.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p3.1.m1.1.1.3.3" xref="S3.SS1.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p3.1.m1.1.1.3.3.2" xref="S3.SS1.p3.1.m1.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.3.3.1" xref="S3.SS1.p3.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p3.1.m1.1.1.3.3.3" xref="S3.SS1.p3.1.m1.1.1.3.3.3.cmml">2048</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><in id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></in><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝐗</ci><apply id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS1.p3.1.m1.1.1.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3"><times id="S3.SS1.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.2">𝑇</ci><cn type="integer" id="S3.SS1.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.3">2048</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">{\mathbf{X}}\in{\mathbb{R}}^{T\times 2048}</annotation></semantics></math>, the goal of the MoCA operation is to obtain a non-local context response <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="{\mathbf{Y}}\in{\mathbb{R}}^{T\times\frac{2048}{m}}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">𝐘</mi><mo id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml"><mi id="S3.SS1.p3.2.m2.1.1.3.2" xref="S3.SS1.p3.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p3.2.m2.1.1.3.3" xref="S3.SS1.p3.2.m2.1.1.3.3.cmml"><mi id="S3.SS1.p3.2.m2.1.1.3.3.2" xref="S3.SS1.p3.2.m2.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.2.m2.1.1.3.3.1" xref="S3.SS1.p3.2.m2.1.1.3.3.1.cmml">×</mo><mfrac id="S3.SS1.p3.2.m2.1.1.3.3.3" xref="S3.SS1.p3.2.m2.1.1.3.3.3.cmml"><mn id="S3.SS1.p3.2.m2.1.1.3.3.3.2" xref="S3.SS1.p3.2.m2.1.1.3.3.3.2.cmml">2048</mn><mi id="S3.SS1.p3.2.m2.1.1.3.3.3.3" xref="S3.SS1.p3.2.m2.1.1.3.3.3.3.cmml">m</mi></mfrac></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><in id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1"></in><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝐘</ci><apply id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.3.1.cmml" xref="S3.SS1.p3.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.3.2.cmml" xref="S3.SS1.p3.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS1.p3.2.m2.1.1.3.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3"><times id="S3.SS1.p3.2.m2.1.1.3.3.1.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3.1"></times><ci id="S3.SS1.p3.2.m2.1.1.3.3.2.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3.2">𝑇</ci><apply id="S3.SS1.p3.2.m2.1.1.3.3.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3.3"><divide id="S3.SS1.p3.2.m2.1.1.3.3.3.1.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3.3"></divide><cn type="integer" id="S3.SS1.p3.2.m2.1.1.3.3.3.2.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3.3.2">2048</cn><ci id="S3.SS1.p3.2.m2.1.1.3.3.3.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3.3.3">𝑚</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">{\mathbf{Y}}\in{\mathbb{R}}^{T\times\frac{2048}{m}}</annotation></semantics></math>, which aims to capture the <span id="S3.SS1.p3.2.1" class="ltx_text ltx_font_italic">motion continuity dependencies</span> across the whole representation sequence by weighted sum of the static features at all temporal positions,</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.6" class="ltx_Math" alttext="{\mathbf{Y}}=\rho([f({\mathbf{X}},{\mathbf{X}}),f(\theta({\mathbf{X}}),\phi({\mathbf{X}}))])g({\mathbf{X}}),\vspace{-3pt}" display="block"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.6.1" xref="S3.E1.m1.6.6.1.1.cmml"><mrow id="S3.E1.m1.6.6.1.1" xref="S3.E1.m1.6.6.1.1.cmml"><mi id="S3.E1.m1.6.6.1.1.3" xref="S3.E1.m1.6.6.1.1.3.cmml">𝐘</mi><mo id="S3.E1.m1.6.6.1.1.2" xref="S3.E1.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.6.6.1.1.1" xref="S3.E1.m1.6.6.1.1.1.cmml"><mi id="S3.E1.m1.6.6.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.3.cmml">ρ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.cmml">[</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝐗</mi><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">𝐗</mi><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.2.4" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.cmml">,</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.4" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.3.cmml">(</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.2.cmml">θ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.1.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.3.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.3.2.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">𝐗</mi><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.3.2.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.4" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.3.cmml">,</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.2.cmml">ϕ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.1.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.3.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.3.2.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.cmml">(</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">𝐗</mi><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.3.2.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.5" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.5" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.cmml">]</mo></mrow><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.1.2a" xref="S3.E1.m1.6.6.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.6.6.1.1.1.4" xref="S3.E1.m1.6.6.1.1.1.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.1.2b" xref="S3.E1.m1.6.6.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.1.5.2" xref="S3.E1.m1.6.6.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.5.2.1" xref="S3.E1.m1.6.6.1.1.1.cmml">(</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">𝐗</mi><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.5.2.2" xref="S3.E1.m1.6.6.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.6.6.1.2" xref="S3.E1.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.6.1.1.cmml" xref="S3.E1.m1.6.6.1"><eq id="S3.E1.m1.6.6.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.2"></eq><ci id="S3.E1.m1.6.6.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.3">𝐘</ci><apply id="S3.E1.m1.6.6.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1"><times id="S3.E1.m1.6.6.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.2"></times><ci id="S3.E1.m1.6.6.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.3">𝜌</ci><interval closure="closed" id="S3.E1.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2"><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.2">𝑓</ci><interval closure="open" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐗</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝐗</ci></interval></apply><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2"><times id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.3"></times><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.4.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.4">𝑓</ci><interval closure="open" id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2"><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1"><times id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.1"></times><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.1.1.1.2">𝜃</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝐗</ci></apply><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2"><times id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.1"></times><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.2.2.2.2.2">italic-ϕ</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝐗</ci></apply></interval></apply></interval><ci id="S3.E1.m1.6.6.1.1.1.4.cmml" xref="S3.E1.m1.6.6.1.1.1.4">𝑔</ci><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">𝐗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">{\mathbf{Y}}=\rho([f({\mathbf{X}},{\mathbf{X}}),f(\theta({\mathbf{X}}),\phi({\mathbf{X}}))])g({\mathbf{X}}),\vspace{-3pt}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.7" class="ltx_p">where <math id="S3.SS1.p3.3.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS1.p3.3.m1.1a"><mi id="S3.SS1.p3.3.m1.1.1" xref="S3.SS1.p3.3.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m1.1b"><ci id="S3.SS1.p3.3.m1.1.1.cmml" xref="S3.SS1.p3.3.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m1.1c">m</annotation></semantics></math> is a reduction ratio used to reduce computational complexity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, and it is set to <math id="S3.SS1.p3.4.m2.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS1.p3.4.m2.1a"><mn id="S3.SS1.p3.4.m2.1.1" xref="S3.SS1.p3.4.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m2.1b"><cn type="integer" id="S3.SS1.p3.4.m2.1.1.cmml" xref="S3.SS1.p3.4.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m2.1c">2</annotation></semantics></math> in our experiments. <math id="S3.SS1.p3.5.m3.1" class="ltx_Math" alttext="g(\cdot)" display="inline"><semantics id="S3.SS1.p3.5.m3.1a"><mrow id="S3.SS1.p3.5.m3.1.2" xref="S3.SS1.p3.5.m3.1.2.cmml"><mi id="S3.SS1.p3.5.m3.1.2.2" xref="S3.SS1.p3.5.m3.1.2.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.5.m3.1.2.1" xref="S3.SS1.p3.5.m3.1.2.1.cmml">​</mo><mrow id="S3.SS1.p3.5.m3.1.2.3.2" xref="S3.SS1.p3.5.m3.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.5.m3.1.2.3.2.1" xref="S3.SS1.p3.5.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p3.5.m3.1.1" xref="S3.SS1.p3.5.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p3.5.m3.1.2.3.2.2" xref="S3.SS1.p3.5.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m3.1b"><apply id="S3.SS1.p3.5.m3.1.2.cmml" xref="S3.SS1.p3.5.m3.1.2"><times id="S3.SS1.p3.5.m3.1.2.1.cmml" xref="S3.SS1.p3.5.m3.1.2.1"></times><ci id="S3.SS1.p3.5.m3.1.2.2.cmml" xref="S3.SS1.p3.5.m3.1.2.2">𝑔</ci><ci id="S3.SS1.p3.5.m3.1.1.cmml" xref="S3.SS1.p3.5.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m3.1c">g(\cdot)</annotation></semantics></math>, <math id="S3.SS1.p3.6.m4.1" class="ltx_Math" alttext="\phi(\cdot)" display="inline"><semantics id="S3.SS1.p3.6.m4.1a"><mrow id="S3.SS1.p3.6.m4.1.2" xref="S3.SS1.p3.6.m4.1.2.cmml"><mi id="S3.SS1.p3.6.m4.1.2.2" xref="S3.SS1.p3.6.m4.1.2.2.cmml">ϕ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.6.m4.1.2.1" xref="S3.SS1.p3.6.m4.1.2.1.cmml">​</mo><mrow id="S3.SS1.p3.6.m4.1.2.3.2" xref="S3.SS1.p3.6.m4.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.6.m4.1.2.3.2.1" xref="S3.SS1.p3.6.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p3.6.m4.1.1" xref="S3.SS1.p3.6.m4.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p3.6.m4.1.2.3.2.2" xref="S3.SS1.p3.6.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m4.1b"><apply id="S3.SS1.p3.6.m4.1.2.cmml" xref="S3.SS1.p3.6.m4.1.2"><times id="S3.SS1.p3.6.m4.1.2.1.cmml" xref="S3.SS1.p3.6.m4.1.2.1"></times><ci id="S3.SS1.p3.6.m4.1.2.2.cmml" xref="S3.SS1.p3.6.m4.1.2.2">italic-ϕ</ci><ci id="S3.SS1.p3.6.m4.1.1.cmml" xref="S3.SS1.p3.6.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m4.1c">\phi(\cdot)</annotation></semantics></math>, and <math id="S3.SS1.p3.7.m5.1" class="ltx_Math" alttext="\theta(\cdot)" display="inline"><semantics id="S3.SS1.p3.7.m5.1a"><mrow id="S3.SS1.p3.7.m5.1.2" xref="S3.SS1.p3.7.m5.1.2.cmml"><mi id="S3.SS1.p3.7.m5.1.2.2" xref="S3.SS1.p3.7.m5.1.2.2.cmml">θ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.7.m5.1.2.1" xref="S3.SS1.p3.7.m5.1.2.1.cmml">​</mo><mrow id="S3.SS1.p3.7.m5.1.2.3.2" xref="S3.SS1.p3.7.m5.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.7.m5.1.2.3.2.1" xref="S3.SS1.p3.7.m5.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p3.7.m5.1.1" xref="S3.SS1.p3.7.m5.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p3.7.m5.1.2.3.2.2" xref="S3.SS1.p3.7.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m5.1b"><apply id="S3.SS1.p3.7.m5.1.2.cmml" xref="S3.SS1.p3.7.m5.1.2"><times id="S3.SS1.p3.7.m5.1.2.1.cmml" xref="S3.SS1.p3.7.m5.1.2.1"></times><ci id="S3.SS1.p3.7.m5.1.2.2.cmml" xref="S3.SS1.p3.7.m5.1.2.2">𝜃</ci><ci id="S3.SS1.p3.7.m5.1.1.cmml" xref="S3.SS1.p3.7.m5.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m5.1c">\theta(\cdot)</annotation></semantics></math> are learnable transformations, which are implemented by using the convolutional operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Thus, the transformations can be written as</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="g({\mathbf{X}})={\mathbf{X}}{\mathbf{W}}_{g}\in{\mathbb{R}}^{T\times\frac{2048}{m}},\vspace{-4pt}" display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><mi id="S3.E2.m1.2.2.1.1.2.2" xref="S3.E2.m1.2.2.1.1.2.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.2.1" xref="S3.E2.m1.2.2.1.1.2.1.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.2.3.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.2.3.2.1" xref="S3.E2.m1.2.2.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">𝐗</mi><mo stretchy="false" id="S3.E2.m1.2.2.1.1.2.3.2.2" xref="S3.E2.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml">=</mo><msub id="S3.E2.m1.2.2.1.1.4" xref="S3.E2.m1.2.2.1.1.4.cmml"><mi id="S3.E2.m1.2.2.1.1.4.2" xref="S3.E2.m1.2.2.1.1.4.2.cmml">𝐗𝐖</mi><mi id="S3.E2.m1.2.2.1.1.4.3" xref="S3.E2.m1.2.2.1.1.4.3.cmml">g</mi></msub><mo id="S3.E2.m1.2.2.1.1.5" xref="S3.E2.m1.2.2.1.1.5.cmml">∈</mo><msup id="S3.E2.m1.2.2.1.1.6" xref="S3.E2.m1.2.2.1.1.6.cmml"><mi id="S3.E2.m1.2.2.1.1.6.2" xref="S3.E2.m1.2.2.1.1.6.2.cmml">ℝ</mi><mrow id="S3.E2.m1.2.2.1.1.6.3" xref="S3.E2.m1.2.2.1.1.6.3.cmml"><mi id="S3.E2.m1.2.2.1.1.6.3.2" xref="S3.E2.m1.2.2.1.1.6.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.2.2.1.1.6.3.1" xref="S3.E2.m1.2.2.1.1.6.3.1.cmml">×</mo><mfrac id="S3.E2.m1.2.2.1.1.6.3.3" xref="S3.E2.m1.2.2.1.1.6.3.3.cmml"><mn id="S3.E2.m1.2.2.1.1.6.3.3.2" xref="S3.E2.m1.2.2.1.1.6.3.3.2.cmml">2048</mn><mi id="S3.E2.m1.2.2.1.1.6.3.3.3" xref="S3.E2.m1.2.2.1.1.6.3.3.3.cmml">m</mi></mfrac></mrow></msup></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><and id="S3.E2.m1.2.2.1.1a.cmml" xref="S3.E2.m1.2.2.1"></and><apply id="S3.E2.m1.2.2.1.1b.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"></eq><apply id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><times id="S3.E2.m1.2.2.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2.1"></times><ci id="S3.E2.m1.2.2.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2">𝑔</ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝐗</ci></apply><apply id="S3.E2.m1.2.2.1.1.4.cmml" xref="S3.E2.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.4.1.cmml" xref="S3.E2.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.4.2.cmml" xref="S3.E2.m1.2.2.1.1.4.2">𝐗𝐖</ci><ci id="S3.E2.m1.2.2.1.1.4.3.cmml" xref="S3.E2.m1.2.2.1.1.4.3">𝑔</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1c.cmml" xref="S3.E2.m1.2.2.1"><in id="S3.E2.m1.2.2.1.1.5.cmml" xref="S3.E2.m1.2.2.1.1.5"></in><share href="#S3.E2.m1.2.2.1.1.4.cmml" id="S3.E2.m1.2.2.1.1d.cmml" xref="S3.E2.m1.2.2.1"></share><apply id="S3.E2.m1.2.2.1.1.6.cmml" xref="S3.E2.m1.2.2.1.1.6"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.6.1.cmml" xref="S3.E2.m1.2.2.1.1.6">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.6.2.cmml" xref="S3.E2.m1.2.2.1.1.6.2">ℝ</ci><apply id="S3.E2.m1.2.2.1.1.6.3.cmml" xref="S3.E2.m1.2.2.1.1.6.3"><times id="S3.E2.m1.2.2.1.1.6.3.1.cmml" xref="S3.E2.m1.2.2.1.1.6.3.1"></times><ci id="S3.E2.m1.2.2.1.1.6.3.2.cmml" xref="S3.E2.m1.2.2.1.1.6.3.2">𝑇</ci><apply id="S3.E2.m1.2.2.1.1.6.3.3.cmml" xref="S3.E2.m1.2.2.1.1.6.3.3"><divide id="S3.E2.m1.2.2.1.1.6.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.6.3.3"></divide><cn type="integer" id="S3.E2.m1.2.2.1.1.6.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.6.3.3.2">2048</cn><ci id="S3.E2.m1.2.2.1.1.6.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.6.3.3.3">𝑚</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">g({\mathbf{X}})={\mathbf{X}}{\mathbf{W}}_{g}\in{\mathbb{R}}^{T\times\frac{2048}{m}},\vspace{-4pt}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="\phi({\mathbf{X}})={\mathbf{X}}{\mathbf{W}}_{\phi}\in{\mathbb{R}}^{T\times\frac{2048}{m}},\vspace{-4pt}" display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml"><mi id="S3.E3.m1.2.2.1.1.2.2" xref="S3.E3.m1.2.2.1.1.2.2.cmml">ϕ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.2.1" xref="S3.E3.m1.2.2.1.1.2.1.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.1.2.3.2" xref="S3.E3.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.2.3.2.1" xref="S3.E3.m1.2.2.1.1.2.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">𝐗</mi><mo stretchy="false" id="S3.E3.m1.2.2.1.1.2.3.2.2" xref="S3.E3.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml">=</mo><msub id="S3.E3.m1.2.2.1.1.4" xref="S3.E3.m1.2.2.1.1.4.cmml"><mi id="S3.E3.m1.2.2.1.1.4.2" xref="S3.E3.m1.2.2.1.1.4.2.cmml">𝐗𝐖</mi><mi id="S3.E3.m1.2.2.1.1.4.3" xref="S3.E3.m1.2.2.1.1.4.3.cmml">ϕ</mi></msub><mo id="S3.E3.m1.2.2.1.1.5" xref="S3.E3.m1.2.2.1.1.5.cmml">∈</mo><msup id="S3.E3.m1.2.2.1.1.6" xref="S3.E3.m1.2.2.1.1.6.cmml"><mi id="S3.E3.m1.2.2.1.1.6.2" xref="S3.E3.m1.2.2.1.1.6.2.cmml">ℝ</mi><mrow id="S3.E3.m1.2.2.1.1.6.3" xref="S3.E3.m1.2.2.1.1.6.3.cmml"><mi id="S3.E3.m1.2.2.1.1.6.3.2" xref="S3.E3.m1.2.2.1.1.6.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.2.2.1.1.6.3.1" xref="S3.E3.m1.2.2.1.1.6.3.1.cmml">×</mo><mfrac id="S3.E3.m1.2.2.1.1.6.3.3" xref="S3.E3.m1.2.2.1.1.6.3.3.cmml"><mn id="S3.E3.m1.2.2.1.1.6.3.3.2" xref="S3.E3.m1.2.2.1.1.6.3.3.2.cmml">2048</mn><mi id="S3.E3.m1.2.2.1.1.6.3.3.3" xref="S3.E3.m1.2.2.1.1.6.3.3.3.cmml">m</mi></mfrac></mrow></msup></mrow><mo id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1"><and id="S3.E3.m1.2.2.1.1a.cmml" xref="S3.E3.m1.2.2.1"></and><apply id="S3.E3.m1.2.2.1.1b.cmml" xref="S3.E3.m1.2.2.1"><eq id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"></eq><apply id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2"><times id="S3.E3.m1.2.2.1.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.2.1"></times><ci id="S3.E3.m1.2.2.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2">italic-ϕ</ci><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝐗</ci></apply><apply id="S3.E3.m1.2.2.1.1.4.cmml" xref="S3.E3.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.4.1.cmml" xref="S3.E3.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.4.2.cmml" xref="S3.E3.m1.2.2.1.1.4.2">𝐗𝐖</ci><ci id="S3.E3.m1.2.2.1.1.4.3.cmml" xref="S3.E3.m1.2.2.1.1.4.3">italic-ϕ</ci></apply></apply><apply id="S3.E3.m1.2.2.1.1c.cmml" xref="S3.E3.m1.2.2.1"><in id="S3.E3.m1.2.2.1.1.5.cmml" xref="S3.E3.m1.2.2.1.1.5"></in><share href="#S3.E3.m1.2.2.1.1.4.cmml" id="S3.E3.m1.2.2.1.1d.cmml" xref="S3.E3.m1.2.2.1"></share><apply id="S3.E3.m1.2.2.1.1.6.cmml" xref="S3.E3.m1.2.2.1.1.6"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.6.1.cmml" xref="S3.E3.m1.2.2.1.1.6">superscript</csymbol><ci id="S3.E3.m1.2.2.1.1.6.2.cmml" xref="S3.E3.m1.2.2.1.1.6.2">ℝ</ci><apply id="S3.E3.m1.2.2.1.1.6.3.cmml" xref="S3.E3.m1.2.2.1.1.6.3"><times id="S3.E3.m1.2.2.1.1.6.3.1.cmml" xref="S3.E3.m1.2.2.1.1.6.3.1"></times><ci id="S3.E3.m1.2.2.1.1.6.3.2.cmml" xref="S3.E3.m1.2.2.1.1.6.3.2">𝑇</ci><apply id="S3.E3.m1.2.2.1.1.6.3.3.cmml" xref="S3.E3.m1.2.2.1.1.6.3.3"><divide id="S3.E3.m1.2.2.1.1.6.3.3.1.cmml" xref="S3.E3.m1.2.2.1.1.6.3.3"></divide><cn type="integer" id="S3.E3.m1.2.2.1.1.6.3.3.2.cmml" xref="S3.E3.m1.2.2.1.1.6.3.3.2">2048</cn><ci id="S3.E3.m1.2.2.1.1.6.3.3.3.cmml" xref="S3.E3.m1.2.2.1.1.6.3.3.3">𝑚</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\phi({\mathbf{X}})={\mathbf{X}}{\mathbf{W}}_{\phi}\in{\mathbb{R}}^{T\times\frac{2048}{m}},\vspace{-4pt}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.15" class="ltx_p">and</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="\theta({\mathbf{X}})={\mathbf{X}}{\mathbf{W}}_{\theta}\in{\mathbb{R}}^{T\times\frac{2048}{m}}," display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1.2" xref="S3.E4.m1.2.2.1.1.2.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2" xref="S3.E4.m1.2.2.1.1.2.2.cmml">θ</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.2.1" xref="S3.E4.m1.2.2.1.1.2.1.cmml">​</mo><mrow id="S3.E4.m1.2.2.1.1.2.3.2" xref="S3.E4.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.2.3.2.1" xref="S3.E4.m1.2.2.1.1.2.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">𝐗</mi><mo stretchy="false" id="S3.E4.m1.2.2.1.1.2.3.2.2" xref="S3.E4.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.2.2.1.1.3" xref="S3.E4.m1.2.2.1.1.3.cmml">=</mo><msub id="S3.E4.m1.2.2.1.1.4" xref="S3.E4.m1.2.2.1.1.4.cmml"><mi id="S3.E4.m1.2.2.1.1.4.2" xref="S3.E4.m1.2.2.1.1.4.2.cmml">𝐗𝐖</mi><mi id="S3.E4.m1.2.2.1.1.4.3" xref="S3.E4.m1.2.2.1.1.4.3.cmml">θ</mi></msub><mo id="S3.E4.m1.2.2.1.1.5" xref="S3.E4.m1.2.2.1.1.5.cmml">∈</mo><msup id="S3.E4.m1.2.2.1.1.6" xref="S3.E4.m1.2.2.1.1.6.cmml"><mi id="S3.E4.m1.2.2.1.1.6.2" xref="S3.E4.m1.2.2.1.1.6.2.cmml">ℝ</mi><mrow id="S3.E4.m1.2.2.1.1.6.3" xref="S3.E4.m1.2.2.1.1.6.3.cmml"><mi id="S3.E4.m1.2.2.1.1.6.3.2" xref="S3.E4.m1.2.2.1.1.6.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.2.2.1.1.6.3.1" xref="S3.E4.m1.2.2.1.1.6.3.1.cmml">×</mo><mfrac id="S3.E4.m1.2.2.1.1.6.3.3" xref="S3.E4.m1.2.2.1.1.6.3.3.cmml"><mn id="S3.E4.m1.2.2.1.1.6.3.3.2" xref="S3.E4.m1.2.2.1.1.6.3.3.2.cmml">2048</mn><mi id="S3.E4.m1.2.2.1.1.6.3.3.3" xref="S3.E4.m1.2.2.1.1.6.3.3.3.cmml">m</mi></mfrac></mrow></msup></mrow><mo id="S3.E4.m1.2.2.1.2" xref="S3.E4.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1"><and id="S3.E4.m1.2.2.1.1a.cmml" xref="S3.E4.m1.2.2.1"></and><apply id="S3.E4.m1.2.2.1.1b.cmml" xref="S3.E4.m1.2.2.1"><eq id="S3.E4.m1.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.3"></eq><apply id="S3.E4.m1.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2"><times id="S3.E4.m1.2.2.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.2.1"></times><ci id="S3.E4.m1.2.2.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2">𝜃</ci><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝐗</ci></apply><apply id="S3.E4.m1.2.2.1.1.4.cmml" xref="S3.E4.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.4.1.cmml" xref="S3.E4.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.4.2.cmml" xref="S3.E4.m1.2.2.1.1.4.2">𝐗𝐖</ci><ci id="S3.E4.m1.2.2.1.1.4.3.cmml" xref="S3.E4.m1.2.2.1.1.4.3">𝜃</ci></apply></apply><apply id="S3.E4.m1.2.2.1.1c.cmml" xref="S3.E4.m1.2.2.1"><in id="S3.E4.m1.2.2.1.1.5.cmml" xref="S3.E4.m1.2.2.1.1.5"></in><share href="#S3.E4.m1.2.2.1.1.4.cmml" id="S3.E4.m1.2.2.1.1d.cmml" xref="S3.E4.m1.2.2.1"></share><apply id="S3.E4.m1.2.2.1.1.6.cmml" xref="S3.E4.m1.2.2.1.1.6"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.6.1.cmml" xref="S3.E4.m1.2.2.1.1.6">superscript</csymbol><ci id="S3.E4.m1.2.2.1.1.6.2.cmml" xref="S3.E4.m1.2.2.1.1.6.2">ℝ</ci><apply id="S3.E4.m1.2.2.1.1.6.3.cmml" xref="S3.E4.m1.2.2.1.1.6.3"><times id="S3.E4.m1.2.2.1.1.6.3.1.cmml" xref="S3.E4.m1.2.2.1.1.6.3.1"></times><ci id="S3.E4.m1.2.2.1.1.6.3.2.cmml" xref="S3.E4.m1.2.2.1.1.6.3.2">𝑇</ci><apply id="S3.E4.m1.2.2.1.1.6.3.3.cmml" xref="S3.E4.m1.2.2.1.1.6.3.3"><divide id="S3.E4.m1.2.2.1.1.6.3.3.1.cmml" xref="S3.E4.m1.2.2.1.1.6.3.3"></divide><cn type="integer" id="S3.E4.m1.2.2.1.1.6.3.3.2.cmml" xref="S3.E4.m1.2.2.1.1.6.3.3.2">2048</cn><ci id="S3.E4.m1.2.2.1.1.6.3.3.3.cmml" xref="S3.E4.m1.2.2.1.1.6.3.3.3">𝑚</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\theta({\mathbf{X}})={\mathbf{X}}{\mathbf{W}}_{\theta}\in{\mathbb{R}}^{T\times\frac{2048}{m}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.12" class="ltx_p">parameterized by the weight matrices <math id="S3.SS1.p3.8.m1.1" class="ltx_Math" alttext="{\mathbf{W}}_{g}" display="inline"><semantics id="S3.SS1.p3.8.m1.1a"><msub id="S3.SS1.p3.8.m1.1.1" xref="S3.SS1.p3.8.m1.1.1.cmml"><mi id="S3.SS1.p3.8.m1.1.1.2" xref="S3.SS1.p3.8.m1.1.1.2.cmml">𝐖</mi><mi id="S3.SS1.p3.8.m1.1.1.3" xref="S3.SS1.p3.8.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.8.m1.1b"><apply id="S3.SS1.p3.8.m1.1.1.cmml" xref="S3.SS1.p3.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.8.m1.1.1.1.cmml" xref="S3.SS1.p3.8.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.8.m1.1.1.2.cmml" xref="S3.SS1.p3.8.m1.1.1.2">𝐖</ci><ci id="S3.SS1.p3.8.m1.1.1.3.cmml" xref="S3.SS1.p3.8.m1.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.8.m1.1c">{\mathbf{W}}_{g}</annotation></semantics></math>,
<math id="S3.SS1.p3.9.m2.1" class="ltx_Math" alttext="{\mathbf{W}}_{\phi}" display="inline"><semantics id="S3.SS1.p3.9.m2.1a"><msub id="S3.SS1.p3.9.m2.1.1" xref="S3.SS1.p3.9.m2.1.1.cmml"><mi id="S3.SS1.p3.9.m2.1.1.2" xref="S3.SS1.p3.9.m2.1.1.2.cmml">𝐖</mi><mi id="S3.SS1.p3.9.m2.1.1.3" xref="S3.SS1.p3.9.m2.1.1.3.cmml">ϕ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.9.m2.1b"><apply id="S3.SS1.p3.9.m2.1.1.cmml" xref="S3.SS1.p3.9.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.9.m2.1.1.1.cmml" xref="S3.SS1.p3.9.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.9.m2.1.1.2.cmml" xref="S3.SS1.p3.9.m2.1.1.2">𝐖</ci><ci id="S3.SS1.p3.9.m2.1.1.3.cmml" xref="S3.SS1.p3.9.m2.1.1.3">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.9.m2.1c">{\mathbf{W}}_{\phi}</annotation></semantics></math>, and
<math id="S3.SS1.p3.10.m3.1" class="ltx_Math" alttext="{\mathbf{W}}_{\theta}\in{\mathbb{R}}^{2048\times\frac{2048}{m}}" display="inline"><semantics id="S3.SS1.p3.10.m3.1a"><mrow id="S3.SS1.p3.10.m3.1.1" xref="S3.SS1.p3.10.m3.1.1.cmml"><msub id="S3.SS1.p3.10.m3.1.1.2" xref="S3.SS1.p3.10.m3.1.1.2.cmml"><mi id="S3.SS1.p3.10.m3.1.1.2.2" xref="S3.SS1.p3.10.m3.1.1.2.2.cmml">𝐖</mi><mi id="S3.SS1.p3.10.m3.1.1.2.3" xref="S3.SS1.p3.10.m3.1.1.2.3.cmml">θ</mi></msub><mo id="S3.SS1.p3.10.m3.1.1.1" xref="S3.SS1.p3.10.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.p3.10.m3.1.1.3" xref="S3.SS1.p3.10.m3.1.1.3.cmml"><mi id="S3.SS1.p3.10.m3.1.1.3.2" xref="S3.SS1.p3.10.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p3.10.m3.1.1.3.3" xref="S3.SS1.p3.10.m3.1.1.3.3.cmml"><mn id="S3.SS1.p3.10.m3.1.1.3.3.2" xref="S3.SS1.p3.10.m3.1.1.3.3.2.cmml">2048</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.10.m3.1.1.3.3.1" xref="S3.SS1.p3.10.m3.1.1.3.3.1.cmml">×</mo><mfrac id="S3.SS1.p3.10.m3.1.1.3.3.3" xref="S3.SS1.p3.10.m3.1.1.3.3.3.cmml"><mn id="S3.SS1.p3.10.m3.1.1.3.3.3.2" xref="S3.SS1.p3.10.m3.1.1.3.3.3.2.cmml">2048</mn><mi id="S3.SS1.p3.10.m3.1.1.3.3.3.3" xref="S3.SS1.p3.10.m3.1.1.3.3.3.3.cmml">m</mi></mfrac></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.10.m3.1b"><apply id="S3.SS1.p3.10.m3.1.1.cmml" xref="S3.SS1.p3.10.m3.1.1"><in id="S3.SS1.p3.10.m3.1.1.1.cmml" xref="S3.SS1.p3.10.m3.1.1.1"></in><apply id="S3.SS1.p3.10.m3.1.1.2.cmml" xref="S3.SS1.p3.10.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.10.m3.1.1.2.1.cmml" xref="S3.SS1.p3.10.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.10.m3.1.1.2.2.cmml" xref="S3.SS1.p3.10.m3.1.1.2.2">𝐖</ci><ci id="S3.SS1.p3.10.m3.1.1.2.3.cmml" xref="S3.SS1.p3.10.m3.1.1.2.3">𝜃</ci></apply><apply id="S3.SS1.p3.10.m3.1.1.3.cmml" xref="S3.SS1.p3.10.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.10.m3.1.1.3.1.cmml" xref="S3.SS1.p3.10.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.10.m3.1.1.3.2.cmml" xref="S3.SS1.p3.10.m3.1.1.3.2">ℝ</ci><apply id="S3.SS1.p3.10.m3.1.1.3.3.cmml" xref="S3.SS1.p3.10.m3.1.1.3.3"><times id="S3.SS1.p3.10.m3.1.1.3.3.1.cmml" xref="S3.SS1.p3.10.m3.1.1.3.3.1"></times><cn type="integer" id="S3.SS1.p3.10.m3.1.1.3.3.2.cmml" xref="S3.SS1.p3.10.m3.1.1.3.3.2">2048</cn><apply id="S3.SS1.p3.10.m3.1.1.3.3.3.cmml" xref="S3.SS1.p3.10.m3.1.1.3.3.3"><divide id="S3.SS1.p3.10.m3.1.1.3.3.3.1.cmml" xref="S3.SS1.p3.10.m3.1.1.3.3.3"></divide><cn type="integer" id="S3.SS1.p3.10.m3.1.1.3.3.3.2.cmml" xref="S3.SS1.p3.10.m3.1.1.3.3.3.2">2048</cn><ci id="S3.SS1.p3.10.m3.1.1.3.3.3.3.cmml" xref="S3.SS1.p3.10.m3.1.1.3.3.3.3">𝑚</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.10.m3.1c">{\mathbf{W}}_{\theta}\in{\mathbb{R}}^{2048\times\frac{2048}{m}}</annotation></semantics></math>, respectively. <math id="S3.SS1.p3.11.m4.2" class="ltx_Math" alttext="f(\cdot,\cdot)" display="inline"><semantics id="S3.SS1.p3.11.m4.2a"><mrow id="S3.SS1.p3.11.m4.2.3" xref="S3.SS1.p3.11.m4.2.3.cmml"><mi id="S3.SS1.p3.11.m4.2.3.2" xref="S3.SS1.p3.11.m4.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.11.m4.2.3.1" xref="S3.SS1.p3.11.m4.2.3.1.cmml">​</mo><mrow id="S3.SS1.p3.11.m4.2.3.3.2" xref="S3.SS1.p3.11.m4.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.11.m4.2.3.3.2.1" xref="S3.SS1.p3.11.m4.2.3.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p3.11.m4.1.1" xref="S3.SS1.p3.11.m4.1.1.cmml">⋅</mo><mo rspace="0em" id="S3.SS1.p3.11.m4.2.3.3.2.2" xref="S3.SS1.p3.11.m4.2.3.3.1.cmml">,</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p3.11.m4.2.2" xref="S3.SS1.p3.11.m4.2.2.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p3.11.m4.2.3.3.2.3" xref="S3.SS1.p3.11.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.11.m4.2b"><apply id="S3.SS1.p3.11.m4.2.3.cmml" xref="S3.SS1.p3.11.m4.2.3"><times id="S3.SS1.p3.11.m4.2.3.1.cmml" xref="S3.SS1.p3.11.m4.2.3.1"></times><ci id="S3.SS1.p3.11.m4.2.3.2.cmml" xref="S3.SS1.p3.11.m4.2.3.2">𝑓</ci><interval closure="open" id="S3.SS1.p3.11.m4.2.3.3.1.cmml" xref="S3.SS1.p3.11.m4.2.3.3.2"><ci id="S3.SS1.p3.11.m4.1.1.cmml" xref="S3.SS1.p3.11.m4.1.1">⋅</ci><ci id="S3.SS1.p3.11.m4.2.2.cmml" xref="S3.SS1.p3.11.m4.2.2">⋅</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.11.m4.2c">f(\cdot,\cdot)</annotation></semantics></math> represents a pairwise function, which computes the affinity between all positions. We use dot product <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> as the operation for <math id="S3.SS1.p3.12.m5.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS1.p3.12.m5.1a"><mi id="S3.SS1.p3.12.m5.1.1" xref="S3.SS1.p3.12.m5.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.12.m5.1b"><ci id="S3.SS1.p3.12.m5.1.1.cmml" xref="S3.SS1.p3.12.m5.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.12.m5.1c">f</annotation></semantics></math>, <em id="S3.SS1.p3.12.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p3.12.2" class="ltx_text"></span>,</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.5" class="ltx_Math" alttext="f(\theta({\mathbf{X}}),\phi({\mathbf{X}}))=\theta({\mathbf{X}})\phi({\mathbf{X}})^{\mathsf{T}},\vspace{-4pt}" display="block"><semantics id="S3.E5.m1.5a"><mrow id="S3.E5.m1.5.5.1" xref="S3.E5.m1.5.5.1.1.cmml"><mrow id="S3.E5.m1.5.5.1.1" xref="S3.E5.m1.5.5.1.1.cmml"><mrow id="S3.E5.m1.5.5.1.1.2" xref="S3.E5.m1.5.5.1.1.2.cmml"><mi id="S3.E5.m1.5.5.1.1.2.4" xref="S3.E5.m1.5.5.1.1.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.1.2.3" xref="S3.E5.m1.5.5.1.1.2.3.cmml">​</mo><mrow id="S3.E5.m1.5.5.1.1.2.2.2" xref="S3.E5.m1.5.5.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E5.m1.5.5.1.1.2.2.2.3" xref="S3.E5.m1.5.5.1.1.2.2.3.cmml">(</mo><mrow id="S3.E5.m1.5.5.1.1.1.1.1.1" xref="S3.E5.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.5.5.1.1.1.1.1.1.2" xref="S3.E5.m1.5.5.1.1.1.1.1.1.2.cmml">θ</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.1.1.1.1.1.1" xref="S3.E5.m1.5.5.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E5.m1.5.5.1.1.1.1.1.1.3.2" xref="S3.E5.m1.5.5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.5.5.1.1.1.1.1.1.3.2.1" xref="S3.E5.m1.5.5.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">𝐗</mi><mo stretchy="false" id="S3.E5.m1.5.5.1.1.1.1.1.1.3.2.2" xref="S3.E5.m1.5.5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.5.5.1.1.2.2.2.4" xref="S3.E5.m1.5.5.1.1.2.2.3.cmml">,</mo><mrow id="S3.E5.m1.5.5.1.1.2.2.2.2" xref="S3.E5.m1.5.5.1.1.2.2.2.2.cmml"><mi id="S3.E5.m1.5.5.1.1.2.2.2.2.2" xref="S3.E5.m1.5.5.1.1.2.2.2.2.2.cmml">ϕ</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.1.2.2.2.2.1" xref="S3.E5.m1.5.5.1.1.2.2.2.2.1.cmml">​</mo><mrow id="S3.E5.m1.5.5.1.1.2.2.2.2.3.2" xref="S3.E5.m1.5.5.1.1.2.2.2.2.cmml"><mo stretchy="false" id="S3.E5.m1.5.5.1.1.2.2.2.2.3.2.1" xref="S3.E5.m1.5.5.1.1.2.2.2.2.cmml">(</mo><mi id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml">𝐗</mi><mo stretchy="false" id="S3.E5.m1.5.5.1.1.2.2.2.2.3.2.2" xref="S3.E5.m1.5.5.1.1.2.2.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E5.m1.5.5.1.1.2.2.2.5" xref="S3.E5.m1.5.5.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.5.5.1.1.3" xref="S3.E5.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E5.m1.5.5.1.1.4" xref="S3.E5.m1.5.5.1.1.4.cmml"><mi id="S3.E5.m1.5.5.1.1.4.2" xref="S3.E5.m1.5.5.1.1.4.2.cmml">θ</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.1.4.1" xref="S3.E5.m1.5.5.1.1.4.1.cmml">​</mo><mrow id="S3.E5.m1.5.5.1.1.4.3.2" xref="S3.E5.m1.5.5.1.1.4.cmml"><mo stretchy="false" id="S3.E5.m1.5.5.1.1.4.3.2.1" xref="S3.E5.m1.5.5.1.1.4.cmml">(</mo><mi id="S3.E5.m1.3.3" xref="S3.E5.m1.3.3.cmml">𝐗</mi><mo stretchy="false" id="S3.E5.m1.5.5.1.1.4.3.2.2" xref="S3.E5.m1.5.5.1.1.4.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.1.4.1a" xref="S3.E5.m1.5.5.1.1.4.1.cmml">​</mo><mi id="S3.E5.m1.5.5.1.1.4.4" xref="S3.E5.m1.5.5.1.1.4.4.cmml">ϕ</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.1.4.1b" xref="S3.E5.m1.5.5.1.1.4.1.cmml">​</mo><msup id="S3.E5.m1.5.5.1.1.4.5" xref="S3.E5.m1.5.5.1.1.4.5.cmml"><mrow id="S3.E5.m1.5.5.1.1.4.5.2.2" xref="S3.E5.m1.5.5.1.1.4.5.cmml"><mo stretchy="false" id="S3.E5.m1.5.5.1.1.4.5.2.2.1" xref="S3.E5.m1.5.5.1.1.4.5.cmml">(</mo><mi id="S3.E5.m1.4.4" xref="S3.E5.m1.4.4.cmml">𝐗</mi><mo stretchy="false" id="S3.E5.m1.5.5.1.1.4.5.2.2.2" xref="S3.E5.m1.5.5.1.1.4.5.cmml">)</mo></mrow><mi id="S3.E5.m1.5.5.1.1.4.5.3" xref="S3.E5.m1.5.5.1.1.4.5.3.cmml">𝖳</mi></msup></mrow></mrow><mo id="S3.E5.m1.5.5.1.2" xref="S3.E5.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.5b"><apply id="S3.E5.m1.5.5.1.1.cmml" xref="S3.E5.m1.5.5.1"><eq id="S3.E5.m1.5.5.1.1.3.cmml" xref="S3.E5.m1.5.5.1.1.3"></eq><apply id="S3.E5.m1.5.5.1.1.2.cmml" xref="S3.E5.m1.5.5.1.1.2"><times id="S3.E5.m1.5.5.1.1.2.3.cmml" xref="S3.E5.m1.5.5.1.1.2.3"></times><ci id="S3.E5.m1.5.5.1.1.2.4.cmml" xref="S3.E5.m1.5.5.1.1.2.4">𝑓</ci><interval closure="open" id="S3.E5.m1.5.5.1.1.2.2.3.cmml" xref="S3.E5.m1.5.5.1.1.2.2.2"><apply id="S3.E5.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E5.m1.5.5.1.1.1.1.1.1"><times id="S3.E5.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.5.5.1.1.1.1.1.1.1"></times><ci id="S3.E5.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.5.5.1.1.1.1.1.1.2">𝜃</ci><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">𝐗</ci></apply><apply id="S3.E5.m1.5.5.1.1.2.2.2.2.cmml" xref="S3.E5.m1.5.5.1.1.2.2.2.2"><times id="S3.E5.m1.5.5.1.1.2.2.2.2.1.cmml" xref="S3.E5.m1.5.5.1.1.2.2.2.2.1"></times><ci id="S3.E5.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S3.E5.m1.5.5.1.1.2.2.2.2.2">italic-ϕ</ci><ci id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2">𝐗</ci></apply></interval></apply><apply id="S3.E5.m1.5.5.1.1.4.cmml" xref="S3.E5.m1.5.5.1.1.4"><times id="S3.E5.m1.5.5.1.1.4.1.cmml" xref="S3.E5.m1.5.5.1.1.4.1"></times><ci id="S3.E5.m1.5.5.1.1.4.2.cmml" xref="S3.E5.m1.5.5.1.1.4.2">𝜃</ci><ci id="S3.E5.m1.3.3.cmml" xref="S3.E5.m1.3.3">𝐗</ci><ci id="S3.E5.m1.5.5.1.1.4.4.cmml" xref="S3.E5.m1.5.5.1.1.4.4">italic-ϕ</ci><apply id="S3.E5.m1.5.5.1.1.4.5.cmml" xref="S3.E5.m1.5.5.1.1.4.5"><csymbol cd="ambiguous" id="S3.E5.m1.5.5.1.1.4.5.1.cmml" xref="S3.E5.m1.5.5.1.1.4.5">superscript</csymbol><ci id="S3.E5.m1.4.4.cmml" xref="S3.E5.m1.4.4">𝐗</ci><ci id="S3.E5.m1.5.5.1.1.4.5.3.cmml" xref="S3.E5.m1.5.5.1.1.4.5.3">𝖳</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.5c">f(\theta({\mathbf{X}}),\phi({\mathbf{X}}))=\theta({\mathbf{X}})\phi({\mathbf{X}})^{\mathsf{T}},\vspace{-4pt}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.14" class="ltx_p">where the size of the resulting pairwise function <math id="S3.SS1.p3.13.m1.4" class="ltx_Math" alttext="f(\theta({\mathbf{X}}),\phi({\mathbf{X}}))" display="inline"><semantics id="S3.SS1.p3.13.m1.4a"><mrow id="S3.SS1.p3.13.m1.4.4" xref="S3.SS1.p3.13.m1.4.4.cmml"><mi id="S3.SS1.p3.13.m1.4.4.4" xref="S3.SS1.p3.13.m1.4.4.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.13.m1.4.4.3" xref="S3.SS1.p3.13.m1.4.4.3.cmml">​</mo><mrow id="S3.SS1.p3.13.m1.4.4.2.2" xref="S3.SS1.p3.13.m1.4.4.2.3.cmml"><mo stretchy="false" id="S3.SS1.p3.13.m1.4.4.2.2.3" xref="S3.SS1.p3.13.m1.4.4.2.3.cmml">(</mo><mrow id="S3.SS1.p3.13.m1.3.3.1.1.1" xref="S3.SS1.p3.13.m1.3.3.1.1.1.cmml"><mi id="S3.SS1.p3.13.m1.3.3.1.1.1.2" xref="S3.SS1.p3.13.m1.3.3.1.1.1.2.cmml">θ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.13.m1.3.3.1.1.1.1" xref="S3.SS1.p3.13.m1.3.3.1.1.1.1.cmml">​</mo><mrow id="S3.SS1.p3.13.m1.3.3.1.1.1.3.2" xref="S3.SS1.p3.13.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p3.13.m1.3.3.1.1.1.3.2.1" xref="S3.SS1.p3.13.m1.3.3.1.1.1.cmml">(</mo><mi id="S3.SS1.p3.13.m1.1.1" xref="S3.SS1.p3.13.m1.1.1.cmml">𝐗</mi><mo stretchy="false" id="S3.SS1.p3.13.m1.3.3.1.1.1.3.2.2" xref="S3.SS1.p3.13.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p3.13.m1.4.4.2.2.4" xref="S3.SS1.p3.13.m1.4.4.2.3.cmml">,</mo><mrow id="S3.SS1.p3.13.m1.4.4.2.2.2" xref="S3.SS1.p3.13.m1.4.4.2.2.2.cmml"><mi id="S3.SS1.p3.13.m1.4.4.2.2.2.2" xref="S3.SS1.p3.13.m1.4.4.2.2.2.2.cmml">ϕ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.13.m1.4.4.2.2.2.1" xref="S3.SS1.p3.13.m1.4.4.2.2.2.1.cmml">​</mo><mrow id="S3.SS1.p3.13.m1.4.4.2.2.2.3.2" xref="S3.SS1.p3.13.m1.4.4.2.2.2.cmml"><mo stretchy="false" id="S3.SS1.p3.13.m1.4.4.2.2.2.3.2.1" xref="S3.SS1.p3.13.m1.4.4.2.2.2.cmml">(</mo><mi id="S3.SS1.p3.13.m1.2.2" xref="S3.SS1.p3.13.m1.2.2.cmml">𝐗</mi><mo stretchy="false" id="S3.SS1.p3.13.m1.4.4.2.2.2.3.2.2" xref="S3.SS1.p3.13.m1.4.4.2.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS1.p3.13.m1.4.4.2.2.5" xref="S3.SS1.p3.13.m1.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.13.m1.4b"><apply id="S3.SS1.p3.13.m1.4.4.cmml" xref="S3.SS1.p3.13.m1.4.4"><times id="S3.SS1.p3.13.m1.4.4.3.cmml" xref="S3.SS1.p3.13.m1.4.4.3"></times><ci id="S3.SS1.p3.13.m1.4.4.4.cmml" xref="S3.SS1.p3.13.m1.4.4.4">𝑓</ci><interval closure="open" id="S3.SS1.p3.13.m1.4.4.2.3.cmml" xref="S3.SS1.p3.13.m1.4.4.2.2"><apply id="S3.SS1.p3.13.m1.3.3.1.1.1.cmml" xref="S3.SS1.p3.13.m1.3.3.1.1.1"><times id="S3.SS1.p3.13.m1.3.3.1.1.1.1.cmml" xref="S3.SS1.p3.13.m1.3.3.1.1.1.1"></times><ci id="S3.SS1.p3.13.m1.3.3.1.1.1.2.cmml" xref="S3.SS1.p3.13.m1.3.3.1.1.1.2">𝜃</ci><ci id="S3.SS1.p3.13.m1.1.1.cmml" xref="S3.SS1.p3.13.m1.1.1">𝐗</ci></apply><apply id="S3.SS1.p3.13.m1.4.4.2.2.2.cmml" xref="S3.SS1.p3.13.m1.4.4.2.2.2"><times id="S3.SS1.p3.13.m1.4.4.2.2.2.1.cmml" xref="S3.SS1.p3.13.m1.4.4.2.2.2.1"></times><ci id="S3.SS1.p3.13.m1.4.4.2.2.2.2.cmml" xref="S3.SS1.p3.13.m1.4.4.2.2.2.2">italic-ϕ</ci><ci id="S3.SS1.p3.13.m1.2.2.cmml" xref="S3.SS1.p3.13.m1.2.2">𝐗</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.13.m1.4c">f(\theta({\mathbf{X}}),\phi({\mathbf{X}}))</annotation></semantics></math> is denoted as <math id="S3.SS1.p3.14.m2.1" class="ltx_Math" alttext="{\mathbb{R}}^{T\times\frac{2048}{m}}\times{\mathbb{R}}^{\frac{2048}{m}\times T}\to{\mathbb{R}}^{T\times T}" display="inline"><semantics id="S3.SS1.p3.14.m2.1a"><mrow id="S3.SS1.p3.14.m2.1.1" xref="S3.SS1.p3.14.m2.1.1.cmml"><mrow id="S3.SS1.p3.14.m2.1.1.2" xref="S3.SS1.p3.14.m2.1.1.2.cmml"><msup id="S3.SS1.p3.14.m2.1.1.2.2" xref="S3.SS1.p3.14.m2.1.1.2.2.cmml"><mi id="S3.SS1.p3.14.m2.1.1.2.2.2" xref="S3.SS1.p3.14.m2.1.1.2.2.2.cmml">ℝ</mi><mrow id="S3.SS1.p3.14.m2.1.1.2.2.3" xref="S3.SS1.p3.14.m2.1.1.2.2.3.cmml"><mi id="S3.SS1.p3.14.m2.1.1.2.2.3.2" xref="S3.SS1.p3.14.m2.1.1.2.2.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.14.m2.1.1.2.2.3.1" xref="S3.SS1.p3.14.m2.1.1.2.2.3.1.cmml">×</mo><mfrac id="S3.SS1.p3.14.m2.1.1.2.2.3.3" xref="S3.SS1.p3.14.m2.1.1.2.2.3.3.cmml"><mn id="S3.SS1.p3.14.m2.1.1.2.2.3.3.2" xref="S3.SS1.p3.14.m2.1.1.2.2.3.3.2.cmml">2048</mn><mi id="S3.SS1.p3.14.m2.1.1.2.2.3.3.3" xref="S3.SS1.p3.14.m2.1.1.2.2.3.3.3.cmml">m</mi></mfrac></mrow></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.14.m2.1.1.2.1" xref="S3.SS1.p3.14.m2.1.1.2.1.cmml">×</mo><msup id="S3.SS1.p3.14.m2.1.1.2.3" xref="S3.SS1.p3.14.m2.1.1.2.3.cmml"><mi id="S3.SS1.p3.14.m2.1.1.2.3.2" xref="S3.SS1.p3.14.m2.1.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p3.14.m2.1.1.2.3.3" xref="S3.SS1.p3.14.m2.1.1.2.3.3.cmml"><mfrac id="S3.SS1.p3.14.m2.1.1.2.3.3.2" xref="S3.SS1.p3.14.m2.1.1.2.3.3.2.cmml"><mn id="S3.SS1.p3.14.m2.1.1.2.3.3.2.2" xref="S3.SS1.p3.14.m2.1.1.2.3.3.2.2.cmml">2048</mn><mi id="S3.SS1.p3.14.m2.1.1.2.3.3.2.3" xref="S3.SS1.p3.14.m2.1.1.2.3.3.2.3.cmml">m</mi></mfrac><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.14.m2.1.1.2.3.3.1" xref="S3.SS1.p3.14.m2.1.1.2.3.3.1.cmml">×</mo><mi id="S3.SS1.p3.14.m2.1.1.2.3.3.3" xref="S3.SS1.p3.14.m2.1.1.2.3.3.3.cmml">T</mi></mrow></msup></mrow><mo stretchy="false" id="S3.SS1.p3.14.m2.1.1.1" xref="S3.SS1.p3.14.m2.1.1.1.cmml">→</mo><msup id="S3.SS1.p3.14.m2.1.1.3" xref="S3.SS1.p3.14.m2.1.1.3.cmml"><mi id="S3.SS1.p3.14.m2.1.1.3.2" xref="S3.SS1.p3.14.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p3.14.m2.1.1.3.3" xref="S3.SS1.p3.14.m2.1.1.3.3.cmml"><mi id="S3.SS1.p3.14.m2.1.1.3.3.2" xref="S3.SS1.p3.14.m2.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.14.m2.1.1.3.3.1" xref="S3.SS1.p3.14.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p3.14.m2.1.1.3.3.3" xref="S3.SS1.p3.14.m2.1.1.3.3.3.cmml">T</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.14.m2.1b"><apply id="S3.SS1.p3.14.m2.1.1.cmml" xref="S3.SS1.p3.14.m2.1.1"><ci id="S3.SS1.p3.14.m2.1.1.1.cmml" xref="S3.SS1.p3.14.m2.1.1.1">→</ci><apply id="S3.SS1.p3.14.m2.1.1.2.cmml" xref="S3.SS1.p3.14.m2.1.1.2"><times id="S3.SS1.p3.14.m2.1.1.2.1.cmml" xref="S3.SS1.p3.14.m2.1.1.2.1"></times><apply id="S3.SS1.p3.14.m2.1.1.2.2.cmml" xref="S3.SS1.p3.14.m2.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.14.m2.1.1.2.2.1.cmml" xref="S3.SS1.p3.14.m2.1.1.2.2">superscript</csymbol><ci id="S3.SS1.p3.14.m2.1.1.2.2.2.cmml" xref="S3.SS1.p3.14.m2.1.1.2.2.2">ℝ</ci><apply id="S3.SS1.p3.14.m2.1.1.2.2.3.cmml" xref="S3.SS1.p3.14.m2.1.1.2.2.3"><times id="S3.SS1.p3.14.m2.1.1.2.2.3.1.cmml" xref="S3.SS1.p3.14.m2.1.1.2.2.3.1"></times><ci id="S3.SS1.p3.14.m2.1.1.2.2.3.2.cmml" xref="S3.SS1.p3.14.m2.1.1.2.2.3.2">𝑇</ci><apply id="S3.SS1.p3.14.m2.1.1.2.2.3.3.cmml" xref="S3.SS1.p3.14.m2.1.1.2.2.3.3"><divide id="S3.SS1.p3.14.m2.1.1.2.2.3.3.1.cmml" xref="S3.SS1.p3.14.m2.1.1.2.2.3.3"></divide><cn type="integer" id="S3.SS1.p3.14.m2.1.1.2.2.3.3.2.cmml" xref="S3.SS1.p3.14.m2.1.1.2.2.3.3.2">2048</cn><ci id="S3.SS1.p3.14.m2.1.1.2.2.3.3.3.cmml" xref="S3.SS1.p3.14.m2.1.1.2.2.3.3.3">𝑚</ci></apply></apply></apply><apply id="S3.SS1.p3.14.m2.1.1.2.3.cmml" xref="S3.SS1.p3.14.m2.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.14.m2.1.1.2.3.1.cmml" xref="S3.SS1.p3.14.m2.1.1.2.3">superscript</csymbol><ci id="S3.SS1.p3.14.m2.1.1.2.3.2.cmml" xref="S3.SS1.p3.14.m2.1.1.2.3.2">ℝ</ci><apply id="S3.SS1.p3.14.m2.1.1.2.3.3.cmml" xref="S3.SS1.p3.14.m2.1.1.2.3.3"><times id="S3.SS1.p3.14.m2.1.1.2.3.3.1.cmml" xref="S3.SS1.p3.14.m2.1.1.2.3.3.1"></times><apply id="S3.SS1.p3.14.m2.1.1.2.3.3.2.cmml" xref="S3.SS1.p3.14.m2.1.1.2.3.3.2"><divide id="S3.SS1.p3.14.m2.1.1.2.3.3.2.1.cmml" xref="S3.SS1.p3.14.m2.1.1.2.3.3.2"></divide><cn type="integer" id="S3.SS1.p3.14.m2.1.1.2.3.3.2.2.cmml" xref="S3.SS1.p3.14.m2.1.1.2.3.3.2.2">2048</cn><ci id="S3.SS1.p3.14.m2.1.1.2.3.3.2.3.cmml" xref="S3.SS1.p3.14.m2.1.1.2.3.3.2.3">𝑚</ci></apply><ci id="S3.SS1.p3.14.m2.1.1.2.3.3.3.cmml" xref="S3.SS1.p3.14.m2.1.1.2.3.3.3">𝑇</ci></apply></apply></apply><apply id="S3.SS1.p3.14.m2.1.1.3.cmml" xref="S3.SS1.p3.14.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.14.m2.1.1.3.1.cmml" xref="S3.SS1.p3.14.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.14.m2.1.1.3.2.cmml" xref="S3.SS1.p3.14.m2.1.1.3.2">ℝ</ci><apply id="S3.SS1.p3.14.m2.1.1.3.3.cmml" xref="S3.SS1.p3.14.m2.1.1.3.3"><times id="S3.SS1.p3.14.m2.1.1.3.3.1.cmml" xref="S3.SS1.p3.14.m2.1.1.3.3.1"></times><ci id="S3.SS1.p3.14.m2.1.1.3.3.2.cmml" xref="S3.SS1.p3.14.m2.1.1.3.3.2">𝑇</ci><ci id="S3.SS1.p3.14.m2.1.1.3.3.3.cmml" xref="S3.SS1.p3.14.m2.1.1.3.3.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.14.m2.1c">{\mathbb{R}}^{T\times\frac{2048}{m}}\times{\mathbb{R}}^{\frac{2048}{m}\times T}\to{\mathbb{R}}^{T\times T}</annotation></semantics></math>, which encodes the mutual similarity between temporal positions under the transformed static feature representation sequence. Then, the softmax operation is used to normalize it into an attention map (see Figure <a href="#S3.F4" title="Figure 4 ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2203.08534/assets/Fig5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="556" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.6.3.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.4.2" class="ltx_text" style="font-size:90%;">A HAFI module. It utilizes the temporal features observed from the past and future frames to refine the temporal feature of the current frame <math id="S3.F5.3.1.m1.1" class="ltx_Math" alttext="{\mathbf{z}}_{t}" display="inline"><semantics id="S3.F5.3.1.m1.1b"><msub id="S3.F5.3.1.m1.1.1" xref="S3.F5.3.1.m1.1.1.cmml"><mi id="S3.F5.3.1.m1.1.1.2" xref="S3.F5.3.1.m1.1.1.2.cmml">𝐳</mi><mi id="S3.F5.3.1.m1.1.1.3" xref="S3.F5.3.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F5.3.1.m1.1c"><apply id="S3.F5.3.1.m1.1.1.cmml" xref="S3.F5.3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F5.3.1.m1.1.1.1.cmml" xref="S3.F5.3.1.m1.1.1">subscript</csymbol><ci id="S3.F5.3.1.m1.1.1.2.cmml" xref="S3.F5.3.1.m1.1.1.2">𝐳</ci><ci id="S3.F5.3.1.m1.1.1.3.cmml" xref="S3.F5.3.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.3.1.m1.1d">{\mathbf{z}}_{t}</annotation></semantics></math> in a hierarchical attentive integration manner. Where <math id="S3.F5.4.2.m2.1" class="ltx_Math" alttext="\otimes" display="inline"><semantics id="S3.F5.4.2.m2.1b"><mo id="S3.F5.4.2.m2.1.1" xref="S3.F5.4.2.m2.1.1.cmml">⊗</mo><annotation-xml encoding="MathML-Content" id="S3.F5.4.2.m2.1c"><csymbol cd="latexml" id="S3.F5.4.2.m2.1.1.cmml" xref="S3.F5.4.2.m2.1.1">tensor-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.4.2.m2.1d">\otimes</annotation></semantics></math> denotes matrix multiplication.</span></figcaption>
</figure>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">We empirically find that although calculating the similarity in the transformed feature space provides an opportunity for insight into implicit long-range dependencies, it may sometimes be unstable and lead to attention on less correlated temporal positions (see Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). To this end, we introduce NSSM into the MoCA operation to enable the MoCA module to learn to focus attention on a more appropriate range of action sequence.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.2" class="ltx_p">Regarding NSSM construction, unlike the non-local operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we directly use the static feature representation sequence <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="{\mathbf{X}}" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">{\mathbf{X}}</annotation></semantics></math> extracted from the input video to reveal the explicit dependencies between the frames through the self-similarity matrix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> construction <math id="S3.SS1.p5.2.m2.2" class="ltx_Math" alttext="f({\mathbf{X}},{\mathbf{X}})={\mathbf{X}}{\mathbf{X}}^{\mathsf{T}}\in{\mathbb{R}}^{T\times T}" display="inline"><semantics id="S3.SS1.p5.2.m2.2a"><mrow id="S3.SS1.p5.2.m2.2.3" xref="S3.SS1.p5.2.m2.2.3.cmml"><mrow id="S3.SS1.p5.2.m2.2.3.2" xref="S3.SS1.p5.2.m2.2.3.2.cmml"><mi id="S3.SS1.p5.2.m2.2.3.2.2" xref="S3.SS1.p5.2.m2.2.3.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.2.m2.2.3.2.1" xref="S3.SS1.p5.2.m2.2.3.2.1.cmml">​</mo><mrow id="S3.SS1.p5.2.m2.2.3.2.3.2" xref="S3.SS1.p5.2.m2.2.3.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p5.2.m2.2.3.2.3.2.1" xref="S3.SS1.p5.2.m2.2.3.2.3.1.cmml">(</mo><mi id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">𝐗</mi><mo id="S3.SS1.p5.2.m2.2.3.2.3.2.2" xref="S3.SS1.p5.2.m2.2.3.2.3.1.cmml">,</mo><mi id="S3.SS1.p5.2.m2.2.2" xref="S3.SS1.p5.2.m2.2.2.cmml">𝐗</mi><mo stretchy="false" id="S3.SS1.p5.2.m2.2.3.2.3.2.3" xref="S3.SS1.p5.2.m2.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p5.2.m2.2.3.3" xref="S3.SS1.p5.2.m2.2.3.3.cmml">=</mo><msup id="S3.SS1.p5.2.m2.2.3.4" xref="S3.SS1.p5.2.m2.2.3.4.cmml"><mi id="S3.SS1.p5.2.m2.2.3.4.2" xref="S3.SS1.p5.2.m2.2.3.4.2.cmml">𝐗𝐗</mi><mi id="S3.SS1.p5.2.m2.2.3.4.3" xref="S3.SS1.p5.2.m2.2.3.4.3.cmml">𝖳</mi></msup><mo id="S3.SS1.p5.2.m2.2.3.5" xref="S3.SS1.p5.2.m2.2.3.5.cmml">∈</mo><msup id="S3.SS1.p5.2.m2.2.3.6" xref="S3.SS1.p5.2.m2.2.3.6.cmml"><mi id="S3.SS1.p5.2.m2.2.3.6.2" xref="S3.SS1.p5.2.m2.2.3.6.2.cmml">ℝ</mi><mrow id="S3.SS1.p5.2.m2.2.3.6.3" xref="S3.SS1.p5.2.m2.2.3.6.3.cmml"><mi id="S3.SS1.p5.2.m2.2.3.6.3.2" xref="S3.SS1.p5.2.m2.2.3.6.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p5.2.m2.2.3.6.3.1" xref="S3.SS1.p5.2.m2.2.3.6.3.1.cmml">×</mo><mi id="S3.SS1.p5.2.m2.2.3.6.3.3" xref="S3.SS1.p5.2.m2.2.3.6.3.3.cmml">T</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.2b"><apply id="S3.SS1.p5.2.m2.2.3.cmml" xref="S3.SS1.p5.2.m2.2.3"><and id="S3.SS1.p5.2.m2.2.3a.cmml" xref="S3.SS1.p5.2.m2.2.3"></and><apply id="S3.SS1.p5.2.m2.2.3b.cmml" xref="S3.SS1.p5.2.m2.2.3"><eq id="S3.SS1.p5.2.m2.2.3.3.cmml" xref="S3.SS1.p5.2.m2.2.3.3"></eq><apply id="S3.SS1.p5.2.m2.2.3.2.cmml" xref="S3.SS1.p5.2.m2.2.3.2"><times id="S3.SS1.p5.2.m2.2.3.2.1.cmml" xref="S3.SS1.p5.2.m2.2.3.2.1"></times><ci id="S3.SS1.p5.2.m2.2.3.2.2.cmml" xref="S3.SS1.p5.2.m2.2.3.2.2">𝑓</ci><interval closure="open" id="S3.SS1.p5.2.m2.2.3.2.3.1.cmml" xref="S3.SS1.p5.2.m2.2.3.2.3.2"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">𝐗</ci><ci id="S3.SS1.p5.2.m2.2.2.cmml" xref="S3.SS1.p5.2.m2.2.2">𝐗</ci></interval></apply><apply id="S3.SS1.p5.2.m2.2.3.4.cmml" xref="S3.SS1.p5.2.m2.2.3.4"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.2.3.4.1.cmml" xref="S3.SS1.p5.2.m2.2.3.4">superscript</csymbol><ci id="S3.SS1.p5.2.m2.2.3.4.2.cmml" xref="S3.SS1.p5.2.m2.2.3.4.2">𝐗𝐗</ci><ci id="S3.SS1.p5.2.m2.2.3.4.3.cmml" xref="S3.SS1.p5.2.m2.2.3.4.3">𝖳</ci></apply></apply><apply id="S3.SS1.p5.2.m2.2.3c.cmml" xref="S3.SS1.p5.2.m2.2.3"><in id="S3.SS1.p5.2.m2.2.3.5.cmml" xref="S3.SS1.p5.2.m2.2.3.5"></in><share href="#S3.SS1.p5.2.m2.2.3.4.cmml" id="S3.SS1.p5.2.m2.2.3d.cmml" xref="S3.SS1.p5.2.m2.2.3"></share><apply id="S3.SS1.p5.2.m2.2.3.6.cmml" xref="S3.SS1.p5.2.m2.2.3.6"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.2.3.6.1.cmml" xref="S3.SS1.p5.2.m2.2.3.6">superscript</csymbol><ci id="S3.SS1.p5.2.m2.2.3.6.2.cmml" xref="S3.SS1.p5.2.m2.2.3.6.2">ℝ</ci><apply id="S3.SS1.p5.2.m2.2.3.6.3.cmml" xref="S3.SS1.p5.2.m2.2.3.6.3"><times id="S3.SS1.p5.2.m2.2.3.6.3.1.cmml" xref="S3.SS1.p5.2.m2.2.3.6.3.1"></times><ci id="S3.SS1.p5.2.m2.2.3.6.3.2.cmml" xref="S3.SS1.p5.2.m2.2.3.6.3.2">𝑇</ci><ci id="S3.SS1.p5.2.m2.2.3.6.3.3.cmml" xref="S3.SS1.p5.2.m2.2.3.6.3.3">𝑇</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.2c">f({\mathbf{X}},{\mathbf{X}})={\mathbf{X}}{\mathbf{X}}^{\mathsf{T}}\in{\mathbb{R}}^{T\times T}</annotation></semantics></math>. In this way, the continuity of human motion in the input video can be more straightforwardly revealed. Similarly, we normalize the resultant self-similarity matrix through the softmax operation to form an NSSM (see Figure <a href="#S3.F4" title="Figure 4 ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) to facilitate subsequent combination with the attention map.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.7" class="ltx_p">For the combination of NSSM and attention map, we first regard NSSM as the a <em id="S3.SS1.p6.7.1" class="ltx_emph ltx_font_italic">priori</em> knowledge to concatenate the attention map through the operation <math id="S3.SS1.p6.1.m1.2" class="ltx_Math" alttext="[\cdot,\cdot]" display="inline"><semantics id="S3.SS1.p6.1.m1.2a"><mrow id="S3.SS1.p6.1.m1.2.3.2" xref="S3.SS1.p6.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p6.1.m1.2.3.2.1" xref="S3.SS1.p6.1.m1.2.3.1.cmml">[</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml">⋅</mo><mo rspace="0em" id="S3.SS1.p6.1.m1.2.3.2.2" xref="S3.SS1.p6.1.m1.2.3.1.cmml">,</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p6.1.m1.2.2" xref="S3.SS1.p6.1.m1.2.2.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p6.1.m1.2.3.2.3" xref="S3.SS1.p6.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.2b"><interval closure="closed" id="S3.SS1.p6.1.m1.2.3.1.cmml" xref="S3.SS1.p6.1.m1.2.3.2"><ci id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">⋅</ci><ci id="S3.SS1.p6.1.m1.2.2.cmml" xref="S3.SS1.p6.1.m1.2.2">⋅</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.2c">[\cdot,\cdot]</annotation></semantics></math>, and then use the learnable transformation <math id="S3.SS1.p6.2.m2.1" class="ltx_Math" alttext="\rho(\cdot)" display="inline"><semantics id="S3.SS1.p6.2.m2.1a"><mrow id="S3.SS1.p6.2.m2.1.2" xref="S3.SS1.p6.2.m2.1.2.cmml"><mi id="S3.SS1.p6.2.m2.1.2.2" xref="S3.SS1.p6.2.m2.1.2.2.cmml">ρ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.2.m2.1.2.1" xref="S3.SS1.p6.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS1.p6.2.m2.1.2.3.2" xref="S3.SS1.p6.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p6.2.m2.1.2.3.2.1" xref="S3.SS1.p6.2.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p6.2.m2.1.2.3.2.2" xref="S3.SS1.p6.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><apply id="S3.SS1.p6.2.m2.1.2.cmml" xref="S3.SS1.p6.2.m2.1.2"><times id="S3.SS1.p6.2.m2.1.2.1.cmml" xref="S3.SS1.p6.2.m2.1.2.1"></times><ci id="S3.SS1.p6.2.m2.1.2.2.cmml" xref="S3.SS1.p6.2.m2.1.2.2">𝜌</ci><ci id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">\rho(\cdot)</annotation></semantics></math>, <em id="S3.SS1.p6.7.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p6.7.3" class="ltx_text"></span>, <math id="S3.SS1.p6.3.m3.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS1.p6.3.m3.1a"><mrow id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml"><mn id="S3.SS1.p6.3.m3.1.1.2" xref="S3.SS1.p6.3.m3.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p6.3.m3.1.1.1" xref="S3.SS1.p6.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS1.p6.3.m3.1.1.3" xref="S3.SS1.p6.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.1b"><apply id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1"><times id="S3.SS1.p6.3.m3.1.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1.1"></times><cn type="integer" id="S3.SS1.p6.3.m3.1.1.2.cmml" xref="S3.SS1.p6.3.m3.1.1.2">1</cn><cn type="integer" id="S3.SS1.p6.3.m3.1.1.3.cmml" xref="S3.SS1.p6.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.1c">1\times 1</annotation></semantics></math> convolution to recalibrate the attention map by referring to NSSM (see Figure <a href="#S3.F4" title="Figure 4 ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S3.E1" title="In 3.1 Temporal encoder ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">1</span></a>). The resultant <math id="S3.SS1.p6.4.m4.1" class="ltx_Math" alttext="\rho(\cdot)" display="inline"><semantics id="S3.SS1.p6.4.m4.1a"><mrow id="S3.SS1.p6.4.m4.1.2" xref="S3.SS1.p6.4.m4.1.2.cmml"><mi id="S3.SS1.p6.4.m4.1.2.2" xref="S3.SS1.p6.4.m4.1.2.2.cmml">ρ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.4.m4.1.2.1" xref="S3.SS1.p6.4.m4.1.2.1.cmml">​</mo><mrow id="S3.SS1.p6.4.m4.1.2.3.2" xref="S3.SS1.p6.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS1.p6.4.m4.1.2.3.2.1" xref="S3.SS1.p6.4.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p6.4.m4.1.1" xref="S3.SS1.p6.4.m4.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p6.4.m4.1.2.3.2.2" xref="S3.SS1.p6.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.4.m4.1b"><apply id="S3.SS1.p6.4.m4.1.2.cmml" xref="S3.SS1.p6.4.m4.1.2"><times id="S3.SS1.p6.4.m4.1.2.1.cmml" xref="S3.SS1.p6.4.m4.1.2.1"></times><ci id="S3.SS1.p6.4.m4.1.2.2.cmml" xref="S3.SS1.p6.4.m4.1.2.2">𝜌</ci><ci id="S3.SS1.p6.4.m4.1.1.cmml" xref="S3.SS1.p6.4.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.4.m4.1c">\rho(\cdot)</annotation></semantics></math> is then normalized through the softmax operation, which is called the MoCA map. By jointly considering the characteristics of the NSSM and the attention map, the MoCA map can reveal the non-local context relations related to the human motion of the input video in a more appropriate range. To this end, the non-local context response <math id="S3.SS1.p6.5.m5.1" class="ltx_Math" alttext="{\mathbf{Y}}\in{\mathbb{R}}^{T\times\frac{2048}{m}}" display="inline"><semantics id="S3.SS1.p6.5.m5.1a"><mrow id="S3.SS1.p6.5.m5.1.1" xref="S3.SS1.p6.5.m5.1.1.cmml"><mi id="S3.SS1.p6.5.m5.1.1.2" xref="S3.SS1.p6.5.m5.1.1.2.cmml">𝐘</mi><mo id="S3.SS1.p6.5.m5.1.1.1" xref="S3.SS1.p6.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS1.p6.5.m5.1.1.3" xref="S3.SS1.p6.5.m5.1.1.3.cmml"><mi id="S3.SS1.p6.5.m5.1.1.3.2" xref="S3.SS1.p6.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p6.5.m5.1.1.3.3" xref="S3.SS1.p6.5.m5.1.1.3.3.cmml"><mi id="S3.SS1.p6.5.m5.1.1.3.3.2" xref="S3.SS1.p6.5.m5.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p6.5.m5.1.1.3.3.1" xref="S3.SS1.p6.5.m5.1.1.3.3.1.cmml">×</mo><mfrac id="S3.SS1.p6.5.m5.1.1.3.3.3" xref="S3.SS1.p6.5.m5.1.1.3.3.3.cmml"><mn id="S3.SS1.p6.5.m5.1.1.3.3.3.2" xref="S3.SS1.p6.5.m5.1.1.3.3.3.2.cmml">2048</mn><mi id="S3.SS1.p6.5.m5.1.1.3.3.3.3" xref="S3.SS1.p6.5.m5.1.1.3.3.3.3.cmml">m</mi></mfrac></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.5.m5.1b"><apply id="S3.SS1.p6.5.m5.1.1.cmml" xref="S3.SS1.p6.5.m5.1.1"><in id="S3.SS1.p6.5.m5.1.1.1.cmml" xref="S3.SS1.p6.5.m5.1.1.1"></in><ci id="S3.SS1.p6.5.m5.1.1.2.cmml" xref="S3.SS1.p6.5.m5.1.1.2">𝐘</ci><apply id="S3.SS1.p6.5.m5.1.1.3.cmml" xref="S3.SS1.p6.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p6.5.m5.1.1.3.1.cmml" xref="S3.SS1.p6.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.p6.5.m5.1.1.3.2.cmml" xref="S3.SS1.p6.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS1.p6.5.m5.1.1.3.3.cmml" xref="S3.SS1.p6.5.m5.1.1.3.3"><times id="S3.SS1.p6.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.p6.5.m5.1.1.3.3.1"></times><ci id="S3.SS1.p6.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.p6.5.m5.1.1.3.3.2">𝑇</ci><apply id="S3.SS1.p6.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.p6.5.m5.1.1.3.3.3"><divide id="S3.SS1.p6.5.m5.1.1.3.3.3.1.cmml" xref="S3.SS1.p6.5.m5.1.1.3.3.3"></divide><cn type="integer" id="S3.SS1.p6.5.m5.1.1.3.3.3.2.cmml" xref="S3.SS1.p6.5.m5.1.1.3.3.3.2">2048</cn><ci id="S3.SS1.p6.5.m5.1.1.3.3.3.3.cmml" xref="S3.SS1.p6.5.m5.1.1.3.3.3.3">𝑚</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.5.m5.1c">{\mathbf{Y}}\in{\mathbb{R}}^{T\times\frac{2048}{m}}</annotation></semantics></math> can be calculated from the linear combination between the matrices resulted from <math id="S3.SS1.p6.6.m6.1" class="ltx_Math" alttext="\rho(\cdot)" display="inline"><semantics id="S3.SS1.p6.6.m6.1a"><mrow id="S3.SS1.p6.6.m6.1.2" xref="S3.SS1.p6.6.m6.1.2.cmml"><mi id="S3.SS1.p6.6.m6.1.2.2" xref="S3.SS1.p6.6.m6.1.2.2.cmml">ρ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.6.m6.1.2.1" xref="S3.SS1.p6.6.m6.1.2.1.cmml">​</mo><mrow id="S3.SS1.p6.6.m6.1.2.3.2" xref="S3.SS1.p6.6.m6.1.2.cmml"><mo stretchy="false" id="S3.SS1.p6.6.m6.1.2.3.2.1" xref="S3.SS1.p6.6.m6.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p6.6.m6.1.1" xref="S3.SS1.p6.6.m6.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p6.6.m6.1.2.3.2.2" xref="S3.SS1.p6.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.6.m6.1b"><apply id="S3.SS1.p6.6.m6.1.2.cmml" xref="S3.SS1.p6.6.m6.1.2"><times id="S3.SS1.p6.6.m6.1.2.1.cmml" xref="S3.SS1.p6.6.m6.1.2.1"></times><ci id="S3.SS1.p6.6.m6.1.2.2.cmml" xref="S3.SS1.p6.6.m6.1.2.2">𝜌</ci><ci id="S3.SS1.p6.6.m6.1.1.cmml" xref="S3.SS1.p6.6.m6.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.6.m6.1c">\rho(\cdot)</annotation></semantics></math> and <math id="S3.SS1.p6.7.m7.1" class="ltx_Math" alttext="g(\cdot)" display="inline"><semantics id="S3.SS1.p6.7.m7.1a"><mrow id="S3.SS1.p6.7.m7.1.2" xref="S3.SS1.p6.7.m7.1.2.cmml"><mi id="S3.SS1.p6.7.m7.1.2.2" xref="S3.SS1.p6.7.m7.1.2.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.7.m7.1.2.1" xref="S3.SS1.p6.7.m7.1.2.1.cmml">​</mo><mrow id="S3.SS1.p6.7.m7.1.2.3.2" xref="S3.SS1.p6.7.m7.1.2.cmml"><mo stretchy="false" id="S3.SS1.p6.7.m7.1.2.3.2.1" xref="S3.SS1.p6.7.m7.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p6.7.m7.1.1" xref="S3.SS1.p6.7.m7.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p6.7.m7.1.2.3.2.2" xref="S3.SS1.p6.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.7.m7.1b"><apply id="S3.SS1.p6.7.m7.1.2.cmml" xref="S3.SS1.p6.7.m7.1.2"><times id="S3.SS1.p6.7.m7.1.2.1.cmml" xref="S3.SS1.p6.7.m7.1.2.1"></times><ci id="S3.SS1.p6.7.m7.1.2.2.cmml" xref="S3.SS1.p6.7.m7.1.2.2">𝑔</ci><ci id="S3.SS1.p6.7.m7.1.1.cmml" xref="S3.SS1.p6.7.m7.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.7.m7.1c">g(\cdot)</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">Finally, as in the design of the non-local block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we use residual connection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to generate the output temporal feature representation sequence <math id="S3.SS1.p7.1.m1.1" class="ltx_Math" alttext="{\mathbf{Z}}\in{\mathbb{R}}^{T\times 2048}" display="inline"><semantics id="S3.SS1.p7.1.m1.1a"><mrow id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml"><mi id="S3.SS1.p7.1.m1.1.1.2" xref="S3.SS1.p7.1.m1.1.1.2.cmml">𝐙</mi><mo id="S3.SS1.p7.1.m1.1.1.1" xref="S3.SS1.p7.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS1.p7.1.m1.1.1.3" xref="S3.SS1.p7.1.m1.1.1.3.cmml"><mi id="S3.SS1.p7.1.m1.1.1.3.2" xref="S3.SS1.p7.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p7.1.m1.1.1.3.3" xref="S3.SS1.p7.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p7.1.m1.1.1.3.3.2" xref="S3.SS1.p7.1.m1.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p7.1.m1.1.1.3.3.1" xref="S3.SS1.p7.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p7.1.m1.1.1.3.3.3" xref="S3.SS1.p7.1.m1.1.1.3.3.3.cmml">2048</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><apply id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1"><in id="S3.SS1.p7.1.m1.1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1.1"></in><ci id="S3.SS1.p7.1.m1.1.1.2.cmml" xref="S3.SS1.p7.1.m1.1.1.2">𝐙</ci><apply id="S3.SS1.p7.1.m1.1.1.3.cmml" xref="S3.SS1.p7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p7.1.m1.1.1.3.1.cmml" xref="S3.SS1.p7.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p7.1.m1.1.1.3.2.cmml" xref="S3.SS1.p7.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS1.p7.1.m1.1.1.3.3.cmml" xref="S3.SS1.p7.1.m1.1.1.3.3"><times id="S3.SS1.p7.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p7.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p7.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p7.1.m1.1.1.3.3.2">𝑇</ci><cn type="integer" id="S3.SS1.p7.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p7.1.m1.1.1.3.3.3">2048</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">{\mathbf{Z}}\in{\mathbb{R}}^{T\times 2048}</annotation></semantics></math> (see Figure <a href="#S3.F4" title="Figure 4 ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) in the MoCA module as follows:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.1" class="ltx_Math" alttext="{\mathbf{Z}}={\mathbf{Y}}{\mathbf{W}}_{z}+{\mathbf{X}},\vspace{-7pt}" display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml">𝐙</mi><mo id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.3.cmml"><msub id="S3.E6.m1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.3.2.cmml"><mi id="S3.E6.m1.1.1.1.1.3.2.2" xref="S3.E6.m1.1.1.1.1.3.2.2.cmml">𝐘𝐖</mi><mi id="S3.E6.m1.1.1.1.1.3.2.3" xref="S3.E6.m1.1.1.1.1.3.2.3.cmml">z</mi></msub><mo id="S3.E6.m1.1.1.1.1.3.1" xref="S3.E6.m1.1.1.1.1.3.1.cmml">+</mo><mi id="S3.E6.m1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.3.3.cmml">𝐗</mi></mrow></mrow><mo id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><eq id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"></eq><ci id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2">𝐙</ci><apply id="S3.E6.m1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3"><plus id="S3.E6.m1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.1"></plus><apply id="S3.E6.m1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.2">𝐘𝐖</ci><ci id="S3.E6.m1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3">𝑧</ci></apply><ci id="S3.E6.m1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3">𝐗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">{\mathbf{Z}}={\mathbf{Y}}{\mathbf{W}}_{z}+{\mathbf{X}},\vspace{-7pt}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p7.11" class="ltx_p">where <math id="S3.SS1.p7.2.m1.1" class="ltx_Math" alttext="{\mathbf{W}}_{z}" display="inline"><semantics id="S3.SS1.p7.2.m1.1a"><msub id="S3.SS1.p7.2.m1.1.1" xref="S3.SS1.p7.2.m1.1.1.cmml"><mi id="S3.SS1.p7.2.m1.1.1.2" xref="S3.SS1.p7.2.m1.1.1.2.cmml">𝐖</mi><mi id="S3.SS1.p7.2.m1.1.1.3" xref="S3.SS1.p7.2.m1.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.2.m1.1b"><apply id="S3.SS1.p7.2.m1.1.1.cmml" xref="S3.SS1.p7.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.2.m1.1.1.1.cmml" xref="S3.SS1.p7.2.m1.1.1">subscript</csymbol><ci id="S3.SS1.p7.2.m1.1.1.2.cmml" xref="S3.SS1.p7.2.m1.1.1.2">𝐖</ci><ci id="S3.SS1.p7.2.m1.1.1.3.cmml" xref="S3.SS1.p7.2.m1.1.1.3">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.2.m1.1c">{\mathbf{W}}_{z}</annotation></semantics></math> is a learnable weight matrix implemented by using the convolutional operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, and the number of channels in <math id="S3.SS1.p7.3.m2.1" class="ltx_Math" alttext="{\mathbf{W}}_{z}" display="inline"><semantics id="S3.SS1.p7.3.m2.1a"><msub id="S3.SS1.p7.3.m2.1.1" xref="S3.SS1.p7.3.m2.1.1.cmml"><mi id="S3.SS1.p7.3.m2.1.1.2" xref="S3.SS1.p7.3.m2.1.1.2.cmml">𝐖</mi><mi id="S3.SS1.p7.3.m2.1.1.3" xref="S3.SS1.p7.3.m2.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.3.m2.1b"><apply id="S3.SS1.p7.3.m2.1.1.cmml" xref="S3.SS1.p7.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.3.m2.1.1.1.cmml" xref="S3.SS1.p7.3.m2.1.1">subscript</csymbol><ci id="S3.SS1.p7.3.m2.1.1.2.cmml" xref="S3.SS1.p7.3.m2.1.1.2">𝐖</ci><ci id="S3.SS1.p7.3.m2.1.1.3.cmml" xref="S3.SS1.p7.3.m2.1.1.3">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.3.m2.1c">{\mathbf{W}}_{z}</annotation></semantics></math> is scaled up to match the number of channels (<em id="S3.SS1.p7.11.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p7.11.2" class="ltx_text"></span>, <math id="S3.SS1.p7.4.m3.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S3.SS1.p7.4.m3.1a"><mn id="S3.SS1.p7.4.m3.1.1" xref="S3.SS1.p7.4.m3.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.4.m3.1b"><cn type="integer" id="S3.SS1.p7.4.m3.1.1.cmml" xref="S3.SS1.p7.4.m3.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.4.m3.1c">2048</annotation></semantics></math>) in <math id="S3.SS1.p7.5.m4.1" class="ltx_Math" alttext="{\mathbf{X}}" display="inline"><semantics id="S3.SS1.p7.5.m4.1a"><mi id="S3.SS1.p7.5.m4.1.1" xref="S3.SS1.p7.5.m4.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.5.m4.1b"><ci id="S3.SS1.p7.5.m4.1.1.cmml" xref="S3.SS1.p7.5.m4.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.5.m4.1c">{\mathbf{X}}</annotation></semantics></math>. “<math id="S3.SS1.p7.6.m5.1" class="ltx_Math" alttext="+{\mathbf{X}}" display="inline"><semantics id="S3.SS1.p7.6.m5.1a"><mrow id="S3.SS1.p7.6.m5.1.1" xref="S3.SS1.p7.6.m5.1.1.cmml"><mo id="S3.SS1.p7.6.m5.1.1a" xref="S3.SS1.p7.6.m5.1.1.cmml">+</mo><mi id="S3.SS1.p7.6.m5.1.1.2" xref="S3.SS1.p7.6.m5.1.1.2.cmml">𝐗</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.6.m5.1b"><apply id="S3.SS1.p7.6.m5.1.1.cmml" xref="S3.SS1.p7.6.m5.1.1"><plus id="S3.SS1.p7.6.m5.1.1.1.cmml" xref="S3.SS1.p7.6.m5.1.1"></plus><ci id="S3.SS1.p7.6.m5.1.1.2.cmml" xref="S3.SS1.p7.6.m5.1.1.2">𝐗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.6.m5.1c">+{\mathbf{X}}</annotation></semantics></math>” denotes a residual connection. The residual connection allows us to insert the MoCA module into any pre-trained network, without breaking its initial behavior (<em id="S3.SS1.p7.11.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.p7.11.4" class="ltx_text"></span>, if <math id="S3.SS1.p7.7.m6.1" class="ltx_Math" alttext="{\mathbf{W}}_{z}" display="inline"><semantics id="S3.SS1.p7.7.m6.1a"><msub id="S3.SS1.p7.7.m6.1.1" xref="S3.SS1.p7.7.m6.1.1.cmml"><mi id="S3.SS1.p7.7.m6.1.1.2" xref="S3.SS1.p7.7.m6.1.1.2.cmml">𝐖</mi><mi id="S3.SS1.p7.7.m6.1.1.3" xref="S3.SS1.p7.7.m6.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.7.m6.1b"><apply id="S3.SS1.p7.7.m6.1.1.cmml" xref="S3.SS1.p7.7.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.7.m6.1.1.1.cmml" xref="S3.SS1.p7.7.m6.1.1">subscript</csymbol><ci id="S3.SS1.p7.7.m6.1.1.2.cmml" xref="S3.SS1.p7.7.m6.1.1.2">𝐖</ci><ci id="S3.SS1.p7.7.m6.1.1.3.cmml" xref="S3.SS1.p7.7.m6.1.1.3">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.7.m6.1c">{\mathbf{W}}_{z}</annotation></semantics></math> is initialized as zero). As a result, by further considering the non-local context response <math id="S3.SS1.p7.8.m7.1" class="ltx_Math" alttext="{\mathbf{Y}}" display="inline"><semantics id="S3.SS1.p7.8.m7.1a"><mi id="S3.SS1.p7.8.m7.1.1" xref="S3.SS1.p7.8.m7.1.1.cmml">𝐘</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.8.m7.1b"><ci id="S3.SS1.p7.8.m7.1.1.cmml" xref="S3.SS1.p7.8.m7.1.1">𝐘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.8.m7.1c">{\mathbf{Y}}</annotation></semantics></math>, <math id="S3.SS1.p7.9.m8.1" class="ltx_Math" alttext="{\mathbf{Z}}" display="inline"><semantics id="S3.SS1.p7.9.m8.1a"><mi id="S3.SS1.p7.9.m8.1.1" xref="S3.SS1.p7.9.m8.1.1.cmml">𝐙</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.9.m8.1b"><ci id="S3.SS1.p7.9.m8.1.1.cmml" xref="S3.SS1.p7.9.m8.1.1">𝐙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.9.m8.1c">{\mathbf{Z}}</annotation></semantics></math> will contain rich temporal information, so <math id="S3.SS1.p7.10.m9.1" class="ltx_Math" alttext="{\mathbf{Z}}" display="inline"><semantics id="S3.SS1.p7.10.m9.1a"><mi id="S3.SS1.p7.10.m9.1.1" xref="S3.SS1.p7.10.m9.1.1.cmml">𝐙</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.10.m9.1b"><ci id="S3.SS1.p7.10.m9.1.1.cmml" xref="S3.SS1.p7.10.m9.1.1">𝐙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.10.m9.1c">{\mathbf{Z}}</annotation></semantics></math> can be regarded as enhanced <math id="S3.SS1.p7.11.m10.1" class="ltx_Math" alttext="{\mathbf{X}}" display="inline"><semantics id="S3.SS1.p7.11.m10.1a"><mi id="S3.SS1.p7.11.m10.1.1" xref="S3.SS1.p7.11.m10.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.11.m10.1b"><ci id="S3.SS1.p7.11.m10.1.1.cmml" xref="S3.SS1.p7.11.m10.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.11.m10.1c">{\mathbf{X}}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Temporal feature integration</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">Given the temporal feature representation sequence <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="{\mathbf{Z}}\in{\mathbb{R}}^{T\times 2048}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">𝐙</mi><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.3.2" xref="S3.SS2.p1.1.m1.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.1.3.3.1" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS2.p1.1.m1.1.1.3.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.3.cmml">2048</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><in id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></in><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝐙</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3"><times id="S3.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.2">𝑇</ci><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.3">2048</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">{\mathbf{Z}}\in{\mathbb{R}}^{T\times 2048}</annotation></semantics></math>, the goal of the HAFI module is to refine the temporal feature of the current frame <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="{\mathbf{z}}_{t}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">𝐳</mi><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝐳</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">{\mathbf{z}}_{t}</annotation></semantics></math> by integrating the adjacent temporal features observed from past and future frames to strengthen their temporal correlation and obtain better pose and shape estimation, as shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.8" class="ltx_p"><span id="S3.SS2.p2.8.1" class="ltx_text ltx_font_bold">HAFI Module.</span>
Specifically, we use <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="T/2" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">T</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">/</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><divide id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></divide><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑇</ci><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">T/2</annotation></semantics></math> adjacent frames (<em id="S3.SS2.p2.8.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p2.8.3" class="ltx_text"></span>, <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\{{\mathbf{z}}_{t\pm\frac{T}{4}}\}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">{</mo><msub id="S3.SS2.p2.2.m2.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.2.cmml">𝐳</mi><mrow id="S3.SS2.p2.2.m2.1.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.1.1.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.1.1.3.2.cmml">t</mi><mo id="S3.SS2.p2.2.m2.1.1.1.1.3.1" xref="S3.SS2.p2.2.m2.1.1.1.1.3.1.cmml">±</mo><mfrac id="S3.SS2.p2.2.m2.1.1.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.1.1.3.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1.3.3.2" xref="S3.SS2.p2.2.m2.1.1.1.1.3.3.2.cmml">T</mi><mn id="S3.SS2.p2.2.m2.1.1.1.1.3.3.3" xref="S3.SS2.p2.2.m2.1.1.1.1.3.3.3.cmml">4</mn></mfrac></mrow></msub><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><set id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1"><apply id="S3.SS2.p2.2.m2.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.2">𝐳</ci><apply id="S3.SS2.p2.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.3"><csymbol cd="latexml" id="S3.SS2.p2.2.m2.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.3.1">plus-or-minus</csymbol><ci id="S3.SS2.p2.2.m2.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.3.2">𝑡</ci><apply id="S3.SS2.p2.2.m2.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.3.3"><divide id="S3.SS2.p2.2.m2.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.3.3"></divide><ci id="S3.SS2.p2.2.m2.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.3.3.2">𝑇</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.3.3.3">4</cn></apply></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\{{\mathbf{z}}_{t\pm\frac{T}{4}}\}</annotation></semantics></math>) to refine the temporal feature of the current frame <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="{\mathbf{z}}_{t}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">𝐳</mi><mi id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝐳</ci><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">{\mathbf{z}}_{t}</annotation></semantics></math> in a hierarchical attentive integration manner, as shown in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.1 Temporal encoder ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
For each branch in the HAFI module, we consider the temporal features of three adjacent frames as a group (adjacent frames between groups do not overlap), and resize them from <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mn id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><cn type="integer" id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">2048</annotation></semantics></math> dimensions to <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mn id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><cn type="integer" id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">256</annotation></semantics></math> dimensions respectively through a shared fully connected (FC) layer to reduce computational complexity. The resized temporal features are concatenated (<math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="{\mathbf{z}}^{\mathrm{concat}}\in{\mathbb{R}}^{768}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><msup id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2.2" xref="S3.SS2.p2.6.m6.1.1.2.2.cmml">𝐳</mi><mi id="S3.SS2.p2.6.m6.1.1.2.3" xref="S3.SS2.p2.6.m6.1.1.2.3.cmml">concat</mi></msup><mo id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml"><mi id="S3.SS2.p2.6.m6.1.1.3.2" xref="S3.SS2.p2.6.m6.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p2.6.m6.1.1.3.3" xref="S3.SS2.p2.6.m6.1.1.3.3.cmml">768</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><in id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></in><apply id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.2.1.cmml" xref="S3.SS2.p2.6.m6.1.1.2">superscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2.2">𝐳</ci><ci id="S3.SS2.p2.6.m6.1.1.2.3.cmml" xref="S3.SS2.p2.6.m6.1.1.2.3">concat</ci></apply><apply id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.3.1.cmml" xref="S3.SS2.p2.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.3.2.cmml" xref="S3.SS2.p2.6.m6.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS2.p2.6.m6.1.1.3.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3.3">768</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">{\mathbf{z}}^{\mathrm{concat}}\in{\mathbb{R}}^{768}</annotation></semantics></math>) and passed to three FC layers and a softmax activation to calculate the attention values <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="\mathbf{a}=\{a_{k}\}_{k=1}^{3}" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mrow id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml">𝐚</mi><mo id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">=</mo><msubsup id="S3.SS2.p2.7.m7.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.cmml"><mrow id="S3.SS2.p2.7.m7.1.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p2.7.m7.1.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.1.3.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.1.3.2" xref="S3.SS2.p2.7.m7.1.1.1.1.3.2.cmml">k</mi><mo id="S3.SS2.p2.7.m7.1.1.1.1.3.1" xref="S3.SS2.p2.7.m7.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p2.7.m7.1.1.1.1.3.3" xref="S3.SS2.p2.7.m7.1.1.1.1.3.3.cmml">1</mn></mrow><mn id="S3.SS2.p2.7.m7.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.3.cmml">3</mn></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><eq id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2"></eq><ci id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">𝐚</ci><apply id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1">superscript</csymbol><apply id="S3.SS2.p2.7.m7.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1">subscript</csymbol><set id="S3.SS2.p2.7.m7.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1"><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2">𝑎</ci><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.3">𝑘</ci></apply></set><apply id="S3.SS2.p2.7.m7.1.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.3"><eq id="S3.SS2.p2.7.m7.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.3.1"></eq><ci id="S3.SS2.p2.7.m7.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.3.2">𝑘</ci><cn type="integer" id="S3.SS2.p2.7.m7.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.3.3">1</cn></apply></apply><cn type="integer" id="S3.SS2.p2.7.m7.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\mathbf{a}=\{a_{k}\}_{k=1}^{3}</annotation></semantics></math> by exploring the dependencies among them. Then, the attention value is weighted back to each corresponding frame to amplify the contribution of important frames in the temporal feature integration to obtain the aggregated temporal feature (see Figure <a href="#S3.F5" title="Figure 5 ‣ 3.1 Temporal encoder ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). The aggregated temporal features produced by the bottom branches will be passed to the upper layer and integrated in the same way to produce the final refined <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="{\mathbf{z}}_{t}" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><msub id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml"><mi id="S3.SS2.p2.8.m8.1.1.2" xref="S3.SS2.p2.8.m8.1.1.2.cmml">𝐳</mi><mi id="S3.SS2.p2.8.m8.1.1.3" xref="S3.SS2.p2.8.m8.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><apply id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2">𝐳</ci><ci id="S3.SS2.p2.8.m8.1.1.3.cmml" xref="S3.SS2.p2.8.m8.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">{\mathbf{z}}_{t}</annotation></semantics></math>. By gradually integrating temporal features in adjacent frames to strengthen temporal correlation, it will provide opportunities for the SMPL parameter regressor to learn to estimate accurate and temporally coherent 3D human pose and shape.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">In this work, like Kocabas <em id="S3.SS2.p3.2.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS2.p3.2.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, we use the SMPL parameter regressor proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> as our regressor to estimate pose, shape, and camera parameters <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\Theta_{t}\in{\mathbb{R}}^{85}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><msub id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S3.SS2.p3.1.m1.1.1.2.2" xref="S3.SS2.p3.1.m1.1.1.2.2.cmml">Θ</mi><mi id="S3.SS2.p3.1.m1.1.1.2.3" xref="S3.SS2.p3.1.m1.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p3.1.m1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.cmml">85</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><in id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></in><apply id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2.2">Θ</ci><ci id="S3.SS2.p3.1.m1.1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.1.1.2.3">𝑡</ci></apply><apply id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3">85</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\Theta_{t}\in{\mathbb{R}}^{85}</annotation></semantics></math> according to each refined <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="{\mathbf{z}}_{t}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">𝐳</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝐳</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">{\mathbf{z}}_{t}</annotation></semantics></math> (see Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). In the training phase, we initialize the SMPL parameter regressor with pre-trained weights from HMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Loss functions</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">In terms of MPS-Net training, for each estimated <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\Theta_{t}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">Θ</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">Θ</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\Theta_{t}</annotation></semantics></math>, following the method proposed by Kocabas <em id="S3.SS3.p1.3.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS3.p1.3.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, we impose <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\mathrm{2}}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">ℒ</mi><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">ℒ</ci><cn type="integer" id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\mathcal{L}_{\mathrm{2}}</annotation></semantics></math> loss between the estimated and ground-truth SMPL parameters and 3D/2D joint coordinates to supervise MPS-Net to generate reasonable real-world poses. The 3D joint coordinates are obtained by forwarding the estimated SMPL parameters to the SMPL model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and the 2D joint coordinates are obtained through the 2D projection of the 3D joints using the predicted camera parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. In addition, like Kocabas <em id="S3.SS3.p1.3.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS3.p1.3.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, we also apply adversarial loss <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{\mathrm{adv}}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><msub id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">ℒ</mi><mi id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">adv</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">ℒ</ci><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">adv</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\mathcal{L}_{\mathrm{adv}}</annotation></semantics></math>, <em id="S3.SS3.p1.3.5" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS3.p1.3.6" class="ltx_text"></span>, using the AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> dataset to train a discriminator to distinguish between real human motion and those generated by MPS-Net’s SMPL parameter regressor to encourage the generation of reasonable 3D human motion.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.10.10" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.10.10.11.1" class="ltx_tr">
<th id="S3.T1.10.10.11.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="S3.T1.10.10.11.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">3DPW</th>
<th id="S3.T1.10.10.11.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">MPI-INF-3DHP</th>
<th id="S3.T1.10.10.11.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3">Human3.6M</th>
<th id="S3.T1.10.10.11.1.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt">Number of</th>
</tr>
<tr id="S3.T1.10.10.10" class="ltx_tr">
<th id="S3.T1.10.10.10.11" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">Method</th>
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">PA-MPJPE <math id="S3.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">MPJPE <math id="S3.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.2.2.2.2.m1.1a"><mo stretchy="false" id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.3.3.3.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">MPVPE <math id="S3.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.3.3.3.3.m1.1a"><mo stretchy="false" id="S3.T1.3.3.3.3.m1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.1b"><ci id="S3.T1.3.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.4.4.4.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">ACC-ERR <math id="S3.T1.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.4.4.4.4.m1.1a"><mo stretchy="false" id="S3.T1.4.4.4.4.m1.1.1" xref="S3.T1.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.4.m1.1b"><ci id="S3.T1.4.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.5.5.5.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">PA-MPJPE <math id="S3.T1.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.5.5.5.5.m1.1a"><mo stretchy="false" id="S3.T1.5.5.5.5.m1.1.1" xref="S3.T1.5.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.5.m1.1b"><ci id="S3.T1.5.5.5.5.m1.1.1.cmml" xref="S3.T1.5.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.6.6.6.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">MPJPE <math id="S3.T1.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.6.6.6.6.m1.1a"><mo stretchy="false" id="S3.T1.6.6.6.6.m1.1.1" xref="S3.T1.6.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.6.m1.1b"><ci id="S3.T1.6.6.6.6.m1.1.1.cmml" xref="S3.T1.6.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.7.7.7.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">ACC-ERR <math id="S3.T1.7.7.7.7.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.7.7.7.7.m1.1a"><mo stretchy="false" id="S3.T1.7.7.7.7.m1.1.1" xref="S3.T1.7.7.7.7.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.7.m1.1b"><ci id="S3.T1.7.7.7.7.m1.1.1.cmml" xref="S3.T1.7.7.7.7.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.7.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.8.8.8.8" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">PA-MPJPE <math id="S3.T1.8.8.8.8.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.8.8.8.8.m1.1a"><mo stretchy="false" id="S3.T1.8.8.8.8.m1.1.1" xref="S3.T1.8.8.8.8.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.8.m1.1b"><ci id="S3.T1.8.8.8.8.m1.1.1.cmml" xref="S3.T1.8.8.8.8.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.8.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.9.9.9.9" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">MPJPE <math id="S3.T1.9.9.9.9.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.9.9.9.9.m1.1a"><mo stretchy="false" id="S3.T1.9.9.9.9.m1.1.1" xref="S3.T1.9.9.9.9.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.9.m1.1b"><ci id="S3.T1.9.9.9.9.m1.1.1.cmml" xref="S3.T1.9.9.9.9.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.9.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.10.10.10.10" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">ACC-ERR <math id="S3.T1.10.10.10.10.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.10.10.10.10.m1.1a"><mo stretchy="false" id="S3.T1.10.10.10.10.m1.1.1" xref="S3.T1.10.10.10.10.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.10.10.m1.1b"><ci id="S3.T1.10.10.10.10.m1.1.1.cmml" xref="S3.T1.10.10.10.10.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.10.10.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.10.10.10.12" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column">Input Frames</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.10.10.12.1" class="ltx_tr">
<td id="S3.T1.10.10.12.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">VIBE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</td>
<td id="S3.T1.10.10.12.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">57.6</td>
<td id="S3.T1.10.10.12.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">91.9</td>
<td id="S3.T1.10.10.12.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">-</td>
<td id="S3.T1.10.10.12.1.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t">25.4</td>
<td id="S3.T1.10.10.12.1.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">68.9</td>
<td id="S3.T1.10.10.12.1.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">103.9</td>
<td id="S3.T1.10.10.12.1.8" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t">27.3</td>
<td id="S3.T1.10.10.12.1.9" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">53.3</td>
<td id="S3.T1.10.10.12.1.10" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">78.0</td>
<td id="S3.T1.10.10.12.1.11" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t">27.3</td>
<td id="S3.T1.10.10.12.1.12" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t"><span id="S3.T1.10.10.12.1.12.1" class="ltx_text ltx_font_bold">16</span></td>
</tr>
<tr id="S3.T1.10.10.13.2" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S3.T1.10.10.13.2.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.10.10.13.2.1.1" class="ltx_text" style="background-color:#E6E6E6;">MEVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite></span></td>
<td id="S3.T1.10.10.13.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S3.T1.10.10.13.2.2.1" class="ltx_text" style="background-color:#E6E6E6;">54.7</span></td>
<td id="S3.T1.10.10.13.2.3" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S3.T1.10.10.13.2.3.1" class="ltx_text" style="background-color:#E6E6E6;">86.9</span></td>
<td id="S3.T1.10.10.13.2.4" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S3.T1.10.10.13.2.4.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
<td id="S3.T1.10.10.13.2.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r"><span id="S3.T1.10.10.13.2.5.1" class="ltx_text" style="background-color:#E6E6E6;">11.6</span></td>
<td id="S3.T1.10.10.13.2.6" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S3.T1.10.10.13.2.6.1" class="ltx_text" style="background-color:#E6E6E6;">65.4</span></td>
<td id="S3.T1.10.10.13.2.7" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S3.T1.10.10.13.2.7.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">96.4</span></td>
<td id="S3.T1.10.10.13.2.8" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r"><span id="S3.T1.10.10.13.2.8.1" class="ltx_text" style="background-color:#E6E6E6;">11.1</span></td>
<td id="S3.T1.10.10.13.2.9" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S3.T1.10.10.13.2.9.1" class="ltx_text" style="background-color:#E6E6E6;">53.2</span></td>
<td id="S3.T1.10.10.13.2.10" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S3.T1.10.10.13.2.10.1" class="ltx_text" style="background-color:#E6E6E6;">76.0</span></td>
<td id="S3.T1.10.10.13.2.11" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r"><span id="S3.T1.10.10.13.2.11.1" class="ltx_text" style="background-color:#E6E6E6;">15.3</span></td>
<td id="S3.T1.10.10.13.2.12" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S3.T1.10.10.13.2.12.1" class="ltx_text" style="background-color:#E6E6E6;">90</span></td>
</tr>
<tr id="S3.T1.10.10.14.3" class="ltx_tr">
<td id="S3.T1.10.10.14.3.1" class="ltx_td ltx_align_left ltx_border_r">TCMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S3.T1.10.10.14.3.2" class="ltx_td ltx_nopad_l ltx_align_center">52.7</td>
<td id="S3.T1.10.10.14.3.3" class="ltx_td ltx_nopad_l ltx_align_center">86.5</td>
<td id="S3.T1.10.10.14.3.4" class="ltx_td ltx_nopad_l ltx_align_center">103.2</td>
<td id="S3.T1.10.10.14.3.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r"><span id="S3.T1.10.10.14.3.5.1" class="ltx_text ltx_font_bold">6.8</span></td>
<td id="S3.T1.10.10.14.3.6" class="ltx_td ltx_nopad_l ltx_align_center">63.5</td>
<td id="S3.T1.10.10.14.3.7" class="ltx_td ltx_nopad_l ltx_align_center">97.6</td>
<td id="S3.T1.10.10.14.3.8" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r"><span id="S3.T1.10.10.14.3.8.1" class="ltx_text ltx_font_bold">8.5</span></td>
<td id="S3.T1.10.10.14.3.9" class="ltx_td ltx_nopad_l ltx_align_center">52.0</td>
<td id="S3.T1.10.10.14.3.10" class="ltx_td ltx_nopad_l ltx_align_center">73.6</td>
<td id="S3.T1.10.10.14.3.11" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r">3.9</td>
<td id="S3.T1.10.10.14.3.12" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S3.T1.10.10.14.3.12.1" class="ltx_text ltx_font_bold">16</span></td>
</tr>
<tr id="S3.T1.10.10.15.4" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S3.T1.10.10.15.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S3.T1.10.10.15.4.1.1" class="ltx_text" style="background-color:#E6E6E6;">MPS-Net (Ours)</span></td>
<td id="S3.T1.10.10.15.4.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S3.T1.10.10.15.4.2.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">52.1</span></td>
<td id="S3.T1.10.10.15.4.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S3.T1.10.10.15.4.3.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">84.3</span></td>
<td id="S3.T1.10.10.15.4.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S3.T1.10.10.15.4.4.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">99.7</span></td>
<td id="S3.T1.10.10.15.4.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T1.10.10.15.4.5.1" class="ltx_text" style="background-color:#E6E6E6;">7.4</span></td>
<td id="S3.T1.10.10.15.4.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S3.T1.10.10.15.4.6.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">62.8</span></td>
<td id="S3.T1.10.10.15.4.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S3.T1.10.10.15.4.7.1" class="ltx_text" style="background-color:#E6E6E6;">96.7</span></td>
<td id="S3.T1.10.10.15.4.8" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T1.10.10.15.4.8.1" class="ltx_text" style="background-color:#E6E6E6;">9.6</span></td>
<td id="S3.T1.10.10.15.4.9" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S3.T1.10.10.15.4.9.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">47.4</span></td>
<td id="S3.T1.10.10.15.4.10" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S3.T1.10.10.15.4.10.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">69.4</span></td>
<td id="S3.T1.10.10.15.4.11" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T1.10.10.15.4.11.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">3.6</span></td>
<td id="S3.T1.10.10.15.4.12" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S3.T1.10.10.15.4.12.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">16</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.14.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.15.2" class="ltx_text" style="font-size:90%;">Evaluation of state-of-the-art video-based methods on 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> datasets. Following Choi <em id="S3.T1.15.2.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.T1.15.2.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, all methods are trained on the training set including 3DPW, but do not use the Human3.6M SMPL parameters obtained from Mosh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The number of input frames follows the original protocol of each method.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Implementation details</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.12" class="ltx_p">Following the previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we set <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="T=16" display="inline"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">T</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><eq id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></eq><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝑇</ci><cn type="integer" id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">T=16</annotation></semantics></math> as the sequence length.
We use ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> pre-trained by Kolotouros <em id="S4.p1.12.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.p1.12.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to serve as our static feature extractor. The static feature extractor is fixed and outputs a <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S4.p1.2.m2.1a"><mn id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><cn type="integer" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">2048</annotation></semantics></math>-dimensional feature for each frame, <em id="S4.p1.12.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.p1.12.4" class="ltx_text"></span>, <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="{\mathbf{x}}_{t}\in{\mathbb{R}}^{2048}" display="inline"><semantics id="S4.p1.3.m3.1a"><mrow id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml"><msub id="S4.p1.3.m3.1.1.2" xref="S4.p1.3.m3.1.1.2.cmml"><mi id="S4.p1.3.m3.1.1.2.2" xref="S4.p1.3.m3.1.1.2.2.cmml">𝐱</mi><mi id="S4.p1.3.m3.1.1.2.3" xref="S4.p1.3.m3.1.1.2.3.cmml">t</mi></msub><mo id="S4.p1.3.m3.1.1.1" xref="S4.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S4.p1.3.m3.1.1.3" xref="S4.p1.3.m3.1.1.3.cmml"><mi id="S4.p1.3.m3.1.1.3.2" xref="S4.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mn id="S4.p1.3.m3.1.1.3.3" xref="S4.p1.3.m3.1.1.3.3.cmml">2048</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><apply id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1"><in id="S4.p1.3.m3.1.1.1.cmml" xref="S4.p1.3.m3.1.1.1"></in><apply id="S4.p1.3.m3.1.1.2.cmml" xref="S4.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.p1.3.m3.1.1.2.1.cmml" xref="S4.p1.3.m3.1.1.2">subscript</csymbol><ci id="S4.p1.3.m3.1.1.2.2.cmml" xref="S4.p1.3.m3.1.1.2.2">𝐱</ci><ci id="S4.p1.3.m3.1.1.2.3.cmml" xref="S4.p1.3.m3.1.1.2.3">𝑡</ci></apply><apply id="S4.p1.3.m3.1.1.3.cmml" xref="S4.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.p1.3.m3.1.1.3.1.cmml" xref="S4.p1.3.m3.1.1.3">superscript</csymbol><ci id="S4.p1.3.m3.1.1.3.2.cmml" xref="S4.p1.3.m3.1.1.3.2">ℝ</ci><cn type="integer" id="S4.p1.3.m3.1.1.3.3.cmml" xref="S4.p1.3.m3.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">{\mathbf{x}}_{t}\in{\mathbb{R}}^{2048}</annotation></semantics></math>. The SMPL parameter regressor has two FC layers, each with <math id="S4.p1.4.m4.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S4.p1.4.m4.1a"><mn id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><cn type="integer" id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">1024</annotation></semantics></math> neurons, and followed an output layer to output <math id="S4.p1.5.m5.1" class="ltx_Math" alttext="85" display="inline"><semantics id="S4.p1.5.m5.1a"><mn id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml">85</mn><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.1b"><cn type="integer" id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1">85</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.1c">85</annotation></semantics></math> pose, shape, and camera parameters <math id="S4.p1.6.m6.1" class="ltx_Math" alttext="\Theta_{t}" display="inline"><semantics id="S4.p1.6.m6.1a"><msub id="S4.p1.6.m6.1.1" xref="S4.p1.6.m6.1.1.cmml"><mi mathvariant="normal" id="S4.p1.6.m6.1.1.2" xref="S4.p1.6.m6.1.1.2.cmml">Θ</mi><mi id="S4.p1.6.m6.1.1.3" xref="S4.p1.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.6.m6.1b"><apply id="S4.p1.6.m6.1.1.cmml" xref="S4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p1.6.m6.1.1.1.cmml" xref="S4.p1.6.m6.1.1">subscript</csymbol><ci id="S4.p1.6.m6.1.1.2.cmml" xref="S4.p1.6.m6.1.1.2">Θ</ci><ci id="S4.p1.6.m6.1.1.3.cmml" xref="S4.p1.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.6.m6.1c">\Theta_{t}</annotation></semantics></math> for each frame <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. The discriminator architecture we use is the same as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The parameters of MPS-Net and discriminator are optimized by the Adam solver <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> at a learning rate of <math id="S4.p1.7.m7.1" class="ltx_Math" alttext="5\times 10^{-5}" display="inline"><semantics id="S4.p1.7.m7.1a"><mrow id="S4.p1.7.m7.1.1" xref="S4.p1.7.m7.1.1.cmml"><mn id="S4.p1.7.m7.1.1.2" xref="S4.p1.7.m7.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p1.7.m7.1.1.1" xref="S4.p1.7.m7.1.1.1.cmml">×</mo><msup id="S4.p1.7.m7.1.1.3" xref="S4.p1.7.m7.1.1.3.cmml"><mn id="S4.p1.7.m7.1.1.3.2" xref="S4.p1.7.m7.1.1.3.2.cmml">10</mn><mrow id="S4.p1.7.m7.1.1.3.3" xref="S4.p1.7.m7.1.1.3.3.cmml"><mo id="S4.p1.7.m7.1.1.3.3a" xref="S4.p1.7.m7.1.1.3.3.cmml">−</mo><mn id="S4.p1.7.m7.1.1.3.3.2" xref="S4.p1.7.m7.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.7.m7.1b"><apply id="S4.p1.7.m7.1.1.cmml" xref="S4.p1.7.m7.1.1"><times id="S4.p1.7.m7.1.1.1.cmml" xref="S4.p1.7.m7.1.1.1"></times><cn type="integer" id="S4.p1.7.m7.1.1.2.cmml" xref="S4.p1.7.m7.1.1.2">5</cn><apply id="S4.p1.7.m7.1.1.3.cmml" xref="S4.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.p1.7.m7.1.1.3.1.cmml" xref="S4.p1.7.m7.1.1.3">superscript</csymbol><cn type="integer" id="S4.p1.7.m7.1.1.3.2.cmml" xref="S4.p1.7.m7.1.1.3.2">10</cn><apply id="S4.p1.7.m7.1.1.3.3.cmml" xref="S4.p1.7.m7.1.1.3.3"><minus id="S4.p1.7.m7.1.1.3.3.1.cmml" xref="S4.p1.7.m7.1.1.3.3"></minus><cn type="integer" id="S4.p1.7.m7.1.1.3.3.2.cmml" xref="S4.p1.7.m7.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.7.m7.1c">5\times 10^{-5}</annotation></semantics></math> and <math id="S4.p1.8.m8.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S4.p1.8.m8.1a"><mrow id="S4.p1.8.m8.1.1" xref="S4.p1.8.m8.1.1.cmml"><mn id="S4.p1.8.m8.1.1.2" xref="S4.p1.8.m8.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p1.8.m8.1.1.1" xref="S4.p1.8.m8.1.1.1.cmml">×</mo><msup id="S4.p1.8.m8.1.1.3" xref="S4.p1.8.m8.1.1.3.cmml"><mn id="S4.p1.8.m8.1.1.3.2" xref="S4.p1.8.m8.1.1.3.2.cmml">10</mn><mrow id="S4.p1.8.m8.1.1.3.3" xref="S4.p1.8.m8.1.1.3.3.cmml"><mo id="S4.p1.8.m8.1.1.3.3a" xref="S4.p1.8.m8.1.1.3.3.cmml">−</mo><mn id="S4.p1.8.m8.1.1.3.3.2" xref="S4.p1.8.m8.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.8.m8.1b"><apply id="S4.p1.8.m8.1.1.cmml" xref="S4.p1.8.m8.1.1"><times id="S4.p1.8.m8.1.1.1.cmml" xref="S4.p1.8.m8.1.1.1"></times><cn type="integer" id="S4.p1.8.m8.1.1.2.cmml" xref="S4.p1.8.m8.1.1.2">1</cn><apply id="S4.p1.8.m8.1.1.3.cmml" xref="S4.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S4.p1.8.m8.1.1.3.1.cmml" xref="S4.p1.8.m8.1.1.3">superscript</csymbol><cn type="integer" id="S4.p1.8.m8.1.1.3.2.cmml" xref="S4.p1.8.m8.1.1.3.2">10</cn><apply id="S4.p1.8.m8.1.1.3.3.cmml" xref="S4.p1.8.m8.1.1.3.3"><minus id="S4.p1.8.m8.1.1.3.3.1.cmml" xref="S4.p1.8.m8.1.1.3.3"></minus><cn type="integer" id="S4.p1.8.m8.1.1.3.3.2.cmml" xref="S4.p1.8.m8.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.8.m8.1c">1\times 10^{-4}</annotation></semantics></math>, respectively. The mini-batch size is set to <math id="S4.p1.9.m9.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.p1.9.m9.1a"><mn id="S4.p1.9.m9.1.1" xref="S4.p1.9.m9.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.p1.9.m9.1b"><cn type="integer" id="S4.p1.9.m9.1.1.cmml" xref="S4.p1.9.m9.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.9.m9.1c">32</annotation></semantics></math>. During training, if the performance does not improve within <math id="S4.p1.10.m10.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.p1.10.m10.1a"><mn id="S4.p1.10.m10.1.1" xref="S4.p1.10.m10.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.p1.10.m10.1b"><cn type="integer" id="S4.p1.10.m10.1.1.cmml" xref="S4.p1.10.m10.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.10.m10.1c">5</annotation></semantics></math> epochs, the learning rate of both the MPS-Net and the discriminator will be reduced by a factor of <math id="S4.p1.11.m11.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.p1.11.m11.1a"><mn id="S4.p1.11.m11.1.1" xref="S4.p1.11.m11.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.p1.11.m11.1b"><cn type="integer" id="S4.p1.11.m11.1.1.cmml" xref="S4.p1.11.m11.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.11.m11.1c">10</annotation></semantics></math>. We use an NVIDIA Titan RTX GPU to train the entire network for <math id="S4.p1.12.m12.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.p1.12.m12.1a"><mn id="S4.p1.12.m12.1.1" xref="S4.p1.12.m12.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.p1.12.m12.1b"><cn type="integer" id="S4.p1.12.m12.1.1.cmml" xref="S4.p1.12.m12.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.12.m12.1c">30</annotation></semantics></math> epochs. PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> is used for code implementation.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We first illustrate the datasets used for training and evaluation and the evaluation metrics. Then, we compare our MPS-Net against other state-of-the-art video-based methods and single image-based methods to demonstrate its advantages in addressing 3D human pose and shape estimation. We also provide an ablation study to confirm the effectiveness of each module in MPS-Net. Finally, we visualize some examples to show the qualitative evaluation results.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Datasets.</span> Following the previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we adopt batches of mixed 3D and 2D datasets for training. For 3D datasets, we use 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> for training, where 3DPW and AMASS provide SMPL parameter annotations, while MPI-INF-3DHP and Human3.6M include 3D joint annotations. For 2D datasets, we use PoseTrack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and InstaVariety <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> for training, where PoseTrack provides ground-truth 2D joints, while InstaVariety includes pseudo ground-truth 2D joints annotated using a 2D keypoint detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In terms of evaluation, the 3DPW, MPI-INF-3DHP, and Human3.6M datasets are used. Among them, Human3.6M is an indoor dataset, while 3DPW and MPI-INF-3DHP contain challenging outdoor videos.
More detailed settings are in the supplementary material.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.2" class="ltx_p"><span id="S5.p3.2.1" class="ltx_text ltx_font_bold">Evaluation metrics.</span> For the evaluation, four standard metrics are used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, including the mean per joint position error (MPJPE), the Procrustes-aligned mean per joint position error (PA-MPJPE), the mean per vertex position error (MPVPE), and the acceleration error (ACC-ERR). Among them, MPJPE, PA-MPJPE, and MPVPE are mainly used to express the accuracy of the estimated 3D human pose and shape (measured in millimeter (<math id="S5.p3.1.m1.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S5.p3.1.m1.1a"><mrow id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml"><mi id="S5.p3.1.m1.1.1.2" xref="S5.p3.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1" xref="S5.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.3" xref="S5.p3.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><apply id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1"><times id="S5.p3.1.m1.1.1.1.cmml" xref="S5.p3.1.m1.1.1.1"></times><ci id="S5.p3.1.m1.1.1.2.cmml" xref="S5.p3.1.m1.1.1.2">𝑚</ci><ci id="S5.p3.1.m1.1.1.3.cmml" xref="S5.p3.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">mm</annotation></semantics></math>)), and ACC-ERR (<math id="S5.p3.2.m2.1" class="ltx_Math" alttext="mm/s^{2}" display="inline"><semantics id="S5.p3.2.m2.1a"><mrow id="S5.p3.2.m2.1.1" xref="S5.p3.2.m2.1.1.cmml"><mrow id="S5.p3.2.m2.1.1.2" xref="S5.p3.2.m2.1.1.2.cmml"><mi id="S5.p3.2.m2.1.1.2.2" xref="S5.p3.2.m2.1.1.2.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.p3.2.m2.1.1.2.1" xref="S5.p3.2.m2.1.1.2.1.cmml">​</mo><mi id="S5.p3.2.m2.1.1.2.3" xref="S5.p3.2.m2.1.1.2.3.cmml">m</mi></mrow><mo id="S5.p3.2.m2.1.1.1" xref="S5.p3.2.m2.1.1.1.cmml">/</mo><msup id="S5.p3.2.m2.1.1.3" xref="S5.p3.2.m2.1.1.3.cmml"><mi id="S5.p3.2.m2.1.1.3.2" xref="S5.p3.2.m2.1.1.3.2.cmml">s</mi><mn id="S5.p3.2.m2.1.1.3.3" xref="S5.p3.2.m2.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.2.m2.1b"><apply id="S5.p3.2.m2.1.1.cmml" xref="S5.p3.2.m2.1.1"><divide id="S5.p3.2.m2.1.1.1.cmml" xref="S5.p3.2.m2.1.1.1"></divide><apply id="S5.p3.2.m2.1.1.2.cmml" xref="S5.p3.2.m2.1.1.2"><times id="S5.p3.2.m2.1.1.2.1.cmml" xref="S5.p3.2.m2.1.1.2.1"></times><ci id="S5.p3.2.m2.1.1.2.2.cmml" xref="S5.p3.2.m2.1.1.2.2">𝑚</ci><ci id="S5.p3.2.m2.1.1.2.3.cmml" xref="S5.p3.2.m2.1.1.2.3">𝑚</ci></apply><apply id="S5.p3.2.m2.1.1.3.cmml" xref="S5.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.p3.2.m2.1.1.3.1.cmml" xref="S5.p3.2.m2.1.1.3">superscript</csymbol><ci id="S5.p3.2.m2.1.1.3.2.cmml" xref="S5.p3.2.m2.1.1.3.2">𝑠</ci><cn type="integer" id="S5.p3.2.m2.1.1.3.3.cmml" xref="S5.p3.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.2.m2.1c">mm/s^{2}</annotation></semantics></math>) is used to express the smoothness and temporal coherence of 3D human motion. A detailed description of each metric is included in the supplementary material.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison with state-of-the-art methods</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Video-based methods.</span> Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Loss functions ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the performance comparison between our MPS-Net and the state-of-the-art video-based methods on the 3DPW, MPI-INF-3DHP, and Human3.6M datasets. Following TCMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, all methods are trained on the training set including 3DPW, but do not use the Human3.6M SMPL parameters obtained from Mosh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> for supervision. Because the SMPL parameters from Mosh have been removed from public access due to legal issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The values of the comparison method are from TCMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, but we validated them independently.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.2.1.1" class="ltx_tr">
<th id="S5.T2.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S5.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">#Parameters (M)</th>
<th id="S5.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FLOPs (G)</th>
<th id="S5.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Model Size (MB)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.2.2.1" class="ltx_tr">
<th id="S5.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">VIBE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<td id="S5.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">72.43</td>
<td id="S5.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.1.3.1" class="ltx_text ltx_font_bold">4.17</span></td>
<td id="S5.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">776</td>
</tr>
<tr id="S5.T2.2.3.2" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S5.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T2.2.3.2.1.1" class="ltx_text" style="background-color:#E6E6E6;">MEVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite></span></th>
<td id="S5.T2.2.3.2.2" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.2.1" class="ltx_text" style="background-color:#E6E6E6;">85.72</span></td>
<td id="S5.T2.2.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.3.1" class="ltx_text" style="background-color:#E6E6E6;">4.46</span></td>
<td id="S5.T2.2.3.2.4" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.4.1" class="ltx_text" style="background-color:#E6E6E6;">858.8</span></td>
</tr>
<tr id="S5.T2.2.4.3" class="ltx_tr">
<th id="S5.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TCMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S5.T2.2.4.3.2" class="ltx_td ltx_align_center">108.89</td>
<td id="S5.T2.2.4.3.3" class="ltx_td ltx_align_center">4.99</td>
<td id="S5.T2.2.4.3.4" class="ltx_td ltx_align_center">1073</td>
</tr>
<tr id="S5.T2.2.5.4" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S5.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S5.T2.2.5.4.1.1" class="ltx_text" style="background-color:#E6E6E6;">MPS-Net (Ours)</span></th>
<td id="S5.T2.2.5.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.5.4.2.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">39.63</span></td>
<td id="S5.T2.2.5.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.5.4.3.1" class="ltx_text" style="background-color:#E6E6E6;">4.45</span></td>
<td id="S5.T2.2.5.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.5.4.4.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">331</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.4.2" class="ltx_text" style="font-size:90%;">Comparison of the number of network parameters, FLOPs, and model size.</span></figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.4.4.5.1" class="ltx_tr">
<th id="S5.T3.4.4.5.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S5.T3.4.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">3DPW</th>
</tr>
<tr id="S5.T3.4.4.4" class="ltx_tr">
<th id="S5.T3.4.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r">Method</th>
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">PA-MPJPE <math id="S5.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S5.T3.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">MPJPE <math id="S5.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.2.2.2.2.m1.1a"><mo stretchy="false" id="S5.T3.2.2.2.2.m1.1.1" xref="S5.T3.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S5.T3.3.3.3.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">MPVPE <math id="S5.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.3.3.3.3.m1.1a"><mo stretchy="false" id="S5.T3.3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.m1.1b"><ci id="S5.T3.3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S5.T3.4.4.4.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">ACC-ERR <math id="S5.T3.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.4.4.4.4.m1.1a"><mo stretchy="false" id="S5.T3.4.4.4.4.m1.1.1" xref="S5.T3.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.4.m1.1b"><ci id="S5.T3.4.4.4.4.m1.1.1.cmml" xref="S5.T3.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.4.4.6.1" class="ltx_tr">
<th id="S5.T3.4.4.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MPS-Net</th>
<td id="S5.T3.4.4.6.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" rowspan="2"><span id="S5.T3.4.4.6.1.2.1" class="ltx_text">54.1</span></td>
<td id="S5.T3.4.4.6.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" rowspan="2"><span id="S5.T3.4.4.6.1.3.1" class="ltx_text">87.6</span></td>
<td id="S5.T3.4.4.6.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" rowspan="2"><span id="S5.T3.4.4.6.1.4.1" class="ltx_text">103.1</span></td>
<td id="S5.T3.4.4.6.1.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" rowspan="2"><span id="S5.T3.4.4.6.1.5.1" class="ltx_text">24.1</span></td>
</tr>
<tr id="S5.T3.4.4.7.2" class="ltx_tr">
<th id="S5.T3.4.4.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">- only Non-local <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</th>
</tr>
<tr id="S5.T3.4.4.8.3" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S5.T3.4.4.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T3.4.4.8.3.1.1" class="ltx_text" style="background-color:#E6E6E6;">MPS-Net</span></th>
<td id="S5.T3.4.4.8.3.2" class="ltx_td ltx_nopad_l"></td>
<td id="S5.T3.4.4.8.3.3" class="ltx_td ltx_nopad_l"></td>
<td id="S5.T3.4.4.8.3.4" class="ltx_td ltx_nopad_l"></td>
<td id="S5.T3.4.4.8.3.5" class="ltx_td ltx_nopad_l"></td>
</tr>
<tr id="S5.T3.4.4.9.4" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S5.T3.4.4.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T3.4.4.9.4.1.1" class="ltx_text" style="background-color:#E6E6E6;">- only MoCA</span></th>
<td id="S5.T3.4.4.9.4.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S5.T3.4.4.9.4.2.1" class="ltx_text" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">53.0</span></td>
<td id="S5.T3.4.4.9.4.3" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S5.T3.4.4.9.4.3.1" class="ltx_text" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">86.7</span></td>
<td id="S5.T3.4.4.9.4.4" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S5.T3.4.4.9.4.4.1" class="ltx_text" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">102.2</span></td>
<td id="S5.T3.4.4.9.4.5" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S5.T3.4.4.9.4.5.1" class="ltx_text" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">23.5</span></td>
</tr>
<tr id="S5.T3.4.4.10.5" class="ltx_tr">
<th id="S5.T3.4.4.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MPS-Net</th>
<td id="S5.T3.4.4.10.5.2" class="ltx_td ltx_nopad_l ltx_align_center" rowspan="2"><span id="S5.T3.4.4.10.5.2.1" class="ltx_text">52.4</span></td>
<td id="S5.T3.4.4.10.5.3" class="ltx_td ltx_nopad_l ltx_align_center" rowspan="2"><span id="S5.T3.4.4.10.5.3.1" class="ltx_text">86.0</span></td>
<td id="S5.T3.4.4.10.5.4" class="ltx_td ltx_nopad_l ltx_align_center" rowspan="2"><span id="S5.T3.4.4.10.5.4.1" class="ltx_text">101.5</span></td>
<td id="S5.T3.4.4.10.5.5" class="ltx_td ltx_nopad_l ltx_align_center" rowspan="2"><span id="S5.T3.4.4.10.5.5.1" class="ltx_text">10.5</span></td>
</tr>
<tr id="S5.T3.4.4.11.6" class="ltx_tr">
<th id="S5.T3.4.4.11.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">- MoCA + TF-intgr. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
</tr>
<tr id="S5.T3.4.4.12.7" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S5.T3.4.4.12.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T3.4.4.12.7.1.1" class="ltx_text" style="background-color:#E6E6E6;">MPS-Net (Ours)</span></th>
<td id="S5.T3.4.4.12.7.2" class="ltx_td ltx_nopad_l"></td>
<td id="S5.T3.4.4.12.7.3" class="ltx_td ltx_nopad_l"></td>
<td id="S5.T3.4.4.12.7.4" class="ltx_td ltx_nopad_l"></td>
<td id="S5.T3.4.4.12.7.5" class="ltx_td ltx_nopad_l"></td>
</tr>
<tr id="S5.T3.4.4.13.8" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S5.T3.4.4.13.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S5.T3.4.4.13.8.1.1" class="ltx_text" style="background-color:#E6E6E6;">- MoCA + HAFI</span></th>
<td id="S5.T3.4.4.13.8.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S5.T3.4.4.13.8.2.1" class="ltx_text ltx_font_bold" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">52.1</span></td>
<td id="S5.T3.4.4.13.8.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S5.T3.4.4.13.8.3.1" class="ltx_text ltx_font_bold" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">84.3</span></td>
<td id="S5.T3.4.4.13.8.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S5.T3.4.4.13.8.4.1" class="ltx_text ltx_font_bold" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">99.7</span></td>
<td id="S5.T3.4.4.13.8.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S5.T3.4.4.13.8.5.1" class="ltx_text ltx_font_bold" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">7.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.6.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.7.2" class="ltx_text" style="font-size:90%;">Ablation study for different modules of the MPS-Net on the 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> dataset. The training and evaluation settings are the same as the experiments on the 3DPW dataset in Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Loss functions ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</span></figcaption>
</figure>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.4.4.5.1" class="ltx_tr">
<th id="S5.T4.4.4.5.1.1" class="ltx_td ltx_nopad_r ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T4.4.4.5.1.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T4.4.4.5.1.3" class="ltx_td ltx_nopad_l ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<td id="S5.T4.4.4.5.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">3DPW</td>
</tr>
<tr id="S5.T4.4.4.4" class="ltx_tr">
<th id="S5.T4.4.4.4.5" class="ltx_td ltx_nopad_r ltx_th ltx_th_row"></th>
<th id="S5.T4.4.4.4.6" class="ltx_td ltx_nopad_l ltx_th ltx_th_row"></th>
<th id="S5.T4.4.4.4.7" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r">Method</th>
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">PA-MPJPE <math id="S5.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T4.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">MPJPE <math id="S5.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.2.2.2.2.m1.1a"><mo stretchy="false" id="S5.T4.2.2.2.2.m1.1.1" xref="S5.T4.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.2.m1.1b"><ci id="S5.T4.2.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.3.3.3.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">MPVPE <math id="S5.T4.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.3.3.3.3.m1.1a"><mo stretchy="false" id="S5.T4.3.3.3.3.m1.1.1" xref="S5.T4.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.3.m1.1b"><ci id="S5.T4.3.3.3.3.m1.1.1.cmml" xref="S5.T4.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.4.4.4.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">ACC-ERR <math id="S5.T4.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.4.4.4.4.m1.1a"><mo stretchy="false" id="S5.T4.4.4.4.4.m1.1.1" xref="S5.T4.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.4.m1.1b"><ci id="S5.T4.4.4.4.4.m1.1.1.cmml" xref="S5.T4.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T4.4.4.6.2" class="ltx_tr">
<th id="S5.T4.4.4.6.2.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S5.T4.4.4.6.2.1.1" class="ltx_text">
<span id="S5.T4.4.4.6.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:53.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:53.4pt;transform:translate(-22.25pt,-21.28pt) rotate(-90deg) ;">
<span id="S5.T4.4.4.6.2.1.1.1.1" class="ltx_p">single image</span>
</span></span></span></th>
<th id="S5.T4.4.4.6.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S5.T4.4.4.6.2.2.1" class="ltx_text">
<span id="S5.T4.4.4.6.2.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:27.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:27.8pt;transform:translate(-10.44pt,-10.44pt) rotate(-90deg) ;">
<span id="S5.T4.4.4.6.2.2.1.1.1" class="ltx_p">-based</span>
</span></span></span></th>
<th id="S5.T4.4.4.6.2.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">HMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<td id="S5.T4.4.4.6.2.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">76.7</td>
<td id="S5.T4.4.4.6.2.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">130.0</td>
<td id="S5.T4.4.4.6.2.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">-</td>
<td id="S5.T4.4.4.6.2.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">37.4</td>
</tr>
<tr id="S5.T4.4.4.7.3" class="ltx_tr">
<th id="S5.T4.4.4.7.3.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.7.3.1.1" class="ltx_text" style="background-color:#E6E6E6;">GraphCMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span></th>
<td id="S5.T4.4.4.7.3.2" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.7.3.2.1" class="ltx_text" style="background-color:#E6E6E6;">70.2</span></td>
<td id="S5.T4.4.4.7.3.3" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.7.3.3.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
<td id="S5.T4.4.4.7.3.4" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.7.3.4.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
<td id="S5.T4.4.4.7.3.5" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.7.3.5.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
</tr>
<tr id="S5.T4.4.4.8.4" class="ltx_tr">
<th id="S5.T4.4.4.8.4.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r">SPIN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</th>
<td id="S5.T4.4.4.8.4.2" class="ltx_td ltx_nopad_l ltx_align_center">59.2</td>
<td id="S5.T4.4.4.8.4.3" class="ltx_td ltx_nopad_l ltx_align_center">96.9</td>
<td id="S5.T4.4.4.8.4.4" class="ltx_td ltx_nopad_l ltx_align_center">116.4</td>
<td id="S5.T4.4.4.8.4.5" class="ltx_td ltx_nopad_l ltx_align_center">29.8</td>
</tr>
<tr id="S5.T4.4.4.9.5" class="ltx_tr">
<th id="S5.T4.4.4.9.5.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.9.5.1.1" class="ltx_text" style="background-color:#E6E6E6;">PyMAF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite></span></th>
<td id="S5.T4.4.4.9.5.2" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.9.5.2.1" class="ltx_text" style="background-color:#E6E6E6;">58.9</span></td>
<td id="S5.T4.4.4.9.5.3" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.9.5.3.1" class="ltx_text" style="background-color:#E6E6E6;">92.8</span></td>
<td id="S5.T4.4.4.9.5.4" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.9.5.4.1" class="ltx_text" style="background-color:#E6E6E6;">110.1</span></td>
<td id="S5.T4.4.4.9.5.5" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.9.5.5.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
</tr>
<tr id="S5.T4.4.4.10.6" class="ltx_tr">
<th id="S5.T4.4.4.10.6.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r">I2L-MeshNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</th>
<td id="S5.T4.4.4.10.6.2" class="ltx_td ltx_nopad_l ltx_align_center">57.7</td>
<td id="S5.T4.4.4.10.6.3" class="ltx_td ltx_nopad_l ltx_align_center">93.2</td>
<td id="S5.T4.4.4.10.6.4" class="ltx_td ltx_nopad_l ltx_align_center">110.1</td>
<td id="S5.T4.4.4.10.6.5" class="ltx_td ltx_nopad_l ltx_align_center">30.9</td>
</tr>
<tr id="S5.T4.4.4.11.7" class="ltx_tr">
<th id="S5.T4.4.4.11.7.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="6"><span id="S5.T4.4.4.11.7.1.1" class="ltx_text">
<span id="S5.T4.4.4.11.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:50.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:50.9pt;transform:translate(-21.97pt,-21.97pt) rotate(-90deg) ;">
<span id="S5.T4.4.4.11.7.1.1.1.1" class="ltx_p">video-based</span>
</span></span></span></th>
<th id="S5.T4.4.4.11.7.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_row ltx_border_t"></th>
<th id="S5.T4.4.4.11.7.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.11.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">HMMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite></span></th>
<td id="S5.T4.4.4.11.7.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.11.7.4.1" class="ltx_text" style="background-color:#E6E6E6;">72.6</span></td>
<td id="S5.T4.4.4.11.7.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.11.7.5.1" class="ltx_text" style="background-color:#E6E6E6;">116.5</span></td>
<td id="S5.T4.4.4.11.7.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.11.7.6.1" class="ltx_text" style="background-color:#E6E6E6;">139.3</span></td>
<td id="S5.T4.4.4.11.7.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.11.7.7.1" class="ltx_text" style="background-color:#E6E6E6;">15.2</span></td>
</tr>
<tr id="S5.T4.4.4.12.8" class="ltx_tr">
<th id="S5.T4.4.4.12.8.1" class="ltx_td ltx_nopad_l ltx_th ltx_th_row"></th>
<th id="S5.T4.4.4.12.8.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r">Doersch <em id="S5.T4.4.4.12.8.2.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S5.T4.4.4.12.8.2.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<td id="S5.T4.4.4.12.8.3" class="ltx_td ltx_nopad_l ltx_align_center">74.7</td>
<td id="S5.T4.4.4.12.8.4" class="ltx_td ltx_nopad_l ltx_align_center">-</td>
<td id="S5.T4.4.4.12.8.5" class="ltx_td ltx_nopad_l ltx_align_center">-</td>
<td id="S5.T4.4.4.12.8.6" class="ltx_td ltx_nopad_l ltx_align_center">-</td>
</tr>
<tr id="S5.T4.4.4.13.9" class="ltx_tr">
<th id="S5.T4.4.4.13.9.1" class="ltx_td ltx_nopad_l ltx_th ltx_th_row"></th>
<th id="S5.T4.4.4.13.9.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.13.9.2.1" class="ltx_text" style="background-color:#E6E6E6;">Sun <em id="S5.T4.4.4.13.9.2.1.1" class="ltx_emph ltx_font_italic" style="background-color:#E6E6E6;">et al</em>.<span id="S5.T4.4.4.13.9.2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite></span></th>
<td id="S5.T4.4.4.13.9.3" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.13.9.3.1" class="ltx_text" style="background-color:#E6E6E6;">69.5</span></td>
<td id="S5.T4.4.4.13.9.4" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.13.9.4.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
<td id="S5.T4.4.4.13.9.5" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.13.9.5.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
<td id="S5.T4.4.4.13.9.6" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.13.9.6.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
</tr>
<tr id="S5.T4.4.4.14.10" class="ltx_tr">
<th id="S5.T4.4.4.14.10.1" class="ltx_td ltx_nopad_l ltx_th ltx_th_row"></th>
<th id="S5.T4.4.4.14.10.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r">VIBE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<td id="S5.T4.4.4.14.10.3" class="ltx_td ltx_nopad_l ltx_align_center">56.5</td>
<td id="S5.T4.4.4.14.10.4" class="ltx_td ltx_nopad_l ltx_align_center">93.5</td>
<td id="S5.T4.4.4.14.10.5" class="ltx_td ltx_nopad_l ltx_align_center">113.4</td>
<td id="S5.T4.4.4.14.10.6" class="ltx_td ltx_nopad_l ltx_align_center">27.1</td>
</tr>
<tr id="S5.T4.4.4.15.11" class="ltx_tr">
<th id="S5.T4.4.4.15.11.1" class="ltx_td ltx_nopad_l ltx_th ltx_th_row"></th>
<th id="S5.T4.4.4.15.11.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.15.11.2.1" class="ltx_text" style="background-color:#E6E6E6;">TCMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span></th>
<td id="S5.T4.4.4.15.11.3" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.15.11.3.1" class="ltx_text" style="background-color:#E6E6E6;">55.8</span></td>
<td id="S5.T4.4.4.15.11.4" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.15.11.4.1" class="ltx_text" style="background-color:#E6E6E6;">95.0</span></td>
<td id="S5.T4.4.4.15.11.5" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.15.11.5.1" class="ltx_text" style="background-color:#E6E6E6;">111.3</span></td>
<td id="S5.T4.4.4.15.11.6" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.4.4.15.11.6.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">6.7</span></td>
</tr>
<tr id="S5.T4.4.4.16.12" class="ltx_tr">
<th id="S5.T4.4.4.16.12.1" class="ltx_td ltx_nopad_l ltx_th ltx_th_row ltx_border_bb"></th>
<th id="S5.T4.4.4.16.12.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">MPS-Net (Ours)</th>
<td id="S5.T4.4.4.16.12.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S5.T4.4.4.16.12.3.1" class="ltx_text ltx_font_bold">54.0</span></td>
<td id="S5.T4.4.4.16.12.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S5.T4.4.4.16.12.4.1" class="ltx_text ltx_font_bold">91.6</span></td>
<td id="S5.T4.4.4.16.12.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S5.T4.4.4.16.12.5.1" class="ltx_text ltx_font_bold">109.6</span></td>
<td id="S5.T4.4.4.16.12.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">7.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.6.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.7.2" class="ltx_text" style="font-size:90%;">Evaluation of state-of-the-art single image-based and video-based methods on the 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> dataset. All methods do not use 3DPW for training.</span></figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.2" class="ltx_p">The results in Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Loss functions ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> show that our MPS-Net outperforms the existing video-based methods in almost all metrics and datasets. This demonstrates that by capturing the motion continuity dependencies and integrating temporal features from adjacent past and future, performance can indeed be improved. Although TCMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> has also made great progress, it is limited by the ability of recurrent operation (<em id="S5.SS1.p2.2.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS1.p2.2.2" class="ltx_text"></span>, GRU) to capture non-local context relations in the action sequence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, thereby reducing the accuracy of the estimated 3D human pose and shape (<em id="S5.SS1.p2.2.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS1.p2.2.4" class="ltx_text"></span>, PA-MPJPE, MPJPE, and MPVPE are higher than MPS-Net). In addition, the number of network parameters and model size of TCMR are also about <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mn id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><cn type="integer" id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">3</annotation></semantics></math> times that of MPS-Net (see Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), which is relatively heavy. Regarding MEVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, as shown in Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Loss functions ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, MEVA requires at least <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="90" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mn id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><cn type="integer" id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">90</annotation></semantics></math> input frames, which means it cannot be trained and tested on short videos. This greatly reduces the value in practical applications. Overall, our MPS-Net can effectively estimate accurate (lower PA-MPJPE, MPJPE, and MPVPE) and smooth (lower ACC-ERR) 3D human pose and shape from a video, and is relatively lightweight (fewer network parameters). The comparisons on the three datasets also show the strong generalization property of our MPS-Net.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Ablation analysis.</span> To analyze the effectiveness of the MoCA and HAFI modules in MPS-Net, we conduct ablation studies on MPS-Net under the challenging in-the-wild 3DPW dataset. Specifically, we evaluate the impact on MPS-Net by replacing the MoCA module with the non-local block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, considering only the MoCA module (without using HAFI), and replacing the HAFI module with the temporal feature integration scheme proposed by Choi <em id="S5.SS1.p3.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S5.SS1.p3.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. For performance comparison, it is obvious from Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> that the proposed MoCA module (<em id="S5.SS1.p3.1.4" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS1.p3.1.5" class="ltx_text"></span>, MPS-Net-only MoCA) is superior to non-local block (<em id="S5.SS1.p3.1.6" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS1.p3.1.7" class="ltx_text"></span>, MPS-Net-only Non-local) in all metrics. The results confirm that by further introducing the a <em id="S5.SS1.p3.1.8" class="ltx_emph ltx_font_italic">priori</em> knowledge of NSSM to guide self-attention learning, the MoCA module can indeed improve 3D human pose and shape estimation. On the other hand, the results also show that our HAFI module (<em id="S5.SS1.p3.1.9" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS1.p3.1.10" class="ltx_text"></span>, MPS-Net-MoCA+HAFI) outperforms the temporal feature integration scheme (<em id="S5.SS1.p3.1.11" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS1.p3.1.12" class="ltx_text"></span>, MPS-Net-MoCA+TF-intgr.), which demonstrates that the gradual integration of adjacent features through a hierarchical attentive integration manner can indeed strengthen temporal correlation and make the generated 3D human motion smoother (<em id="S5.SS1.p3.1.13" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS1.p3.1.14" class="ltx_text"></span>, lower ACC-ERR). Overall, the ablation analysis confirmed the effectiveness of the proposed MoCA and HAFI modules.
</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Single image-based and video-based methods.</span> We further compare our MPS-Net with the methods including single image-based methods on the challenging in-the-wild 3DPW dataset. Notice that a number of previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> did not use the 3DPW training set to train their models, so in the comparison in Table <a href="#S5.T4" title="Table 4 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, all methods are not trained on 3DPW.</p>
</div>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.F6.2" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\animategraphics</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F6.3" class="ltx_p ltx_figure_panel ltx_align_center">[autoplay,
loop,
width=]15./Fig6gif2png/1361


</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.5.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.6.2" class="ltx_text" style="font-size:90%;">Qualitative comparison of TCMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (left) and our MPS-Net (right) on the challenging in-the-wild 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> dataset (the 1st and 2nd clips) and MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> dataset (the 3rd clip). <span id="S5.F6.6.2.1" class="ltx_text ltx_font_italic">This is an embedded video, please use Adobe Acrobat (standalone version) to view it. Note that the video animation has been slowed down for better viewing.</span></span></figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2203.08534/assets/Fig7-1.jpg" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="233" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2203.08534/assets/Fig7-2.jpg" id="S5.F7.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="242" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">Qualitative results of MPS-Net on the challenging in-the-wild 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> dataset and MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> dataset. For each sequence, the top row shows input images, the middle row shows the estimated body mesh from the camera view, and the bottom row shows the estimated mesh from an alternate viewpoint.</span></figcaption>
</figure>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.8" class="ltx_p">Similar to the results in Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Loss functions ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the results in Table <a href="#S5.T4" title="Table 4 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrate that our MPS-Net performs favorably against existing single image-based and video-based methods on the PA-MPJPE, MPJPE, and MPVPE evaluation metrics. Although TCMR achieves the lowest ACC-ERR, it tends to be overly smooth, thereby sacrificing the accuracy of pose and shape estimation. Specifically, when TCMR reduces ACC-ERR <math id="S5.SS1.p5.1.m1.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S5.SS1.p5.1.m1.1a"><mn id="S5.SS1.p5.1.m1.1.1" xref="S5.SS1.p5.1.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.1.m1.1b"><cn type="float" id="S5.SS1.p5.1.m1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.1.m1.1c">0.8</annotation></semantics></math> <math id="S5.SS1.p5.2.m2.1" class="ltx_Math" alttext="mm/s^{2}" display="inline"><semantics id="S5.SS1.p5.2.m2.1a"><mrow id="S5.SS1.p5.2.m2.1.1" xref="S5.SS1.p5.2.m2.1.1.cmml"><mrow id="S5.SS1.p5.2.m2.1.1.2" xref="S5.SS1.p5.2.m2.1.1.2.cmml"><mi id="S5.SS1.p5.2.m2.1.1.2.2" xref="S5.SS1.p5.2.m2.1.1.2.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p5.2.m2.1.1.2.1" xref="S5.SS1.p5.2.m2.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p5.2.m2.1.1.2.3" xref="S5.SS1.p5.2.m2.1.1.2.3.cmml">m</mi></mrow><mo id="S5.SS1.p5.2.m2.1.1.1" xref="S5.SS1.p5.2.m2.1.1.1.cmml">/</mo><msup id="S5.SS1.p5.2.m2.1.1.3" xref="S5.SS1.p5.2.m2.1.1.3.cmml"><mi id="S5.SS1.p5.2.m2.1.1.3.2" xref="S5.SS1.p5.2.m2.1.1.3.2.cmml">s</mi><mn id="S5.SS1.p5.2.m2.1.1.3.3" xref="S5.SS1.p5.2.m2.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.2.m2.1b"><apply id="S5.SS1.p5.2.m2.1.1.cmml" xref="S5.SS1.p5.2.m2.1.1"><divide id="S5.SS1.p5.2.m2.1.1.1.cmml" xref="S5.SS1.p5.2.m2.1.1.1"></divide><apply id="S5.SS1.p5.2.m2.1.1.2.cmml" xref="S5.SS1.p5.2.m2.1.1.2"><times id="S5.SS1.p5.2.m2.1.1.2.1.cmml" xref="S5.SS1.p5.2.m2.1.1.2.1"></times><ci id="S5.SS1.p5.2.m2.1.1.2.2.cmml" xref="S5.SS1.p5.2.m2.1.1.2.2">𝑚</ci><ci id="S5.SS1.p5.2.m2.1.1.2.3.cmml" xref="S5.SS1.p5.2.m2.1.1.2.3">𝑚</ci></apply><apply id="S5.SS1.p5.2.m2.1.1.3.cmml" xref="S5.SS1.p5.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p5.2.m2.1.1.3.1.cmml" xref="S5.SS1.p5.2.m2.1.1.3">superscript</csymbol><ci id="S5.SS1.p5.2.m2.1.1.3.2.cmml" xref="S5.SS1.p5.2.m2.1.1.3.2">𝑠</ci><cn type="integer" id="S5.SS1.p5.2.m2.1.1.3.3.cmml" xref="S5.SS1.p5.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.2.m2.1c">mm/s^{2}</annotation></semantics></math> compared to MPS-Net, MPS-Net reduces PA-MPJPE, MPJPE, and MPVPE by <math id="S5.SS1.p5.3.m3.1" class="ltx_Math" alttext="1.8" display="inline"><semantics id="S5.SS1.p5.3.m3.1a"><mn id="S5.SS1.p5.3.m3.1.1" xref="S5.SS1.p5.3.m3.1.1.cmml">1.8</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.3.m3.1b"><cn type="float" id="S5.SS1.p5.3.m3.1.1.cmml" xref="S5.SS1.p5.3.m3.1.1">1.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.3.m3.1c">1.8</annotation></semantics></math> <math id="S5.SS1.p5.4.m4.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S5.SS1.p5.4.m4.1a"><mrow id="S5.SS1.p5.4.m4.1.1" xref="S5.SS1.p5.4.m4.1.1.cmml"><mi id="S5.SS1.p5.4.m4.1.1.2" xref="S5.SS1.p5.4.m4.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p5.4.m4.1.1.1" xref="S5.SS1.p5.4.m4.1.1.1.cmml">​</mo><mi id="S5.SS1.p5.4.m4.1.1.3" xref="S5.SS1.p5.4.m4.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.4.m4.1b"><apply id="S5.SS1.p5.4.m4.1.1.cmml" xref="S5.SS1.p5.4.m4.1.1"><times id="S5.SS1.p5.4.m4.1.1.1.cmml" xref="S5.SS1.p5.4.m4.1.1.1"></times><ci id="S5.SS1.p5.4.m4.1.1.2.cmml" xref="S5.SS1.p5.4.m4.1.1.2">𝑚</ci><ci id="S5.SS1.p5.4.m4.1.1.3.cmml" xref="S5.SS1.p5.4.m4.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.4.m4.1c">mm</annotation></semantics></math>, <math id="S5.SS1.p5.5.m5.1" class="ltx_Math" alttext="3.4" display="inline"><semantics id="S5.SS1.p5.5.m5.1a"><mn id="S5.SS1.p5.5.m5.1.1" xref="S5.SS1.p5.5.m5.1.1.cmml">3.4</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.5.m5.1b"><cn type="float" id="S5.SS1.p5.5.m5.1.1.cmml" xref="S5.SS1.p5.5.m5.1.1">3.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.5.m5.1c">3.4</annotation></semantics></math> <math id="S5.SS1.p5.6.m6.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S5.SS1.p5.6.m6.1a"><mrow id="S5.SS1.p5.6.m6.1.1" xref="S5.SS1.p5.6.m6.1.1.cmml"><mi id="S5.SS1.p5.6.m6.1.1.2" xref="S5.SS1.p5.6.m6.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p5.6.m6.1.1.1" xref="S5.SS1.p5.6.m6.1.1.1.cmml">​</mo><mi id="S5.SS1.p5.6.m6.1.1.3" xref="S5.SS1.p5.6.m6.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.6.m6.1b"><apply id="S5.SS1.p5.6.m6.1.1.cmml" xref="S5.SS1.p5.6.m6.1.1"><times id="S5.SS1.p5.6.m6.1.1.1.cmml" xref="S5.SS1.p5.6.m6.1.1.1"></times><ci id="S5.SS1.p5.6.m6.1.1.2.cmml" xref="S5.SS1.p5.6.m6.1.1.2">𝑚</ci><ci id="S5.SS1.p5.6.m6.1.1.3.cmml" xref="S5.SS1.p5.6.m6.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.6.m6.1c">mm</annotation></semantics></math>, and <math id="S5.SS1.p5.7.m7.1" class="ltx_Math" alttext="1.7" display="inline"><semantics id="S5.SS1.p5.7.m7.1a"><mn id="S5.SS1.p5.7.m7.1.1" xref="S5.SS1.p5.7.m7.1.1.cmml">1.7</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.7.m7.1b"><cn type="float" id="S5.SS1.p5.7.m7.1.1.cmml" xref="S5.SS1.p5.7.m7.1.1">1.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.7.m7.1c">1.7</annotation></semantics></math> <math id="S5.SS1.p5.8.m8.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S5.SS1.p5.8.m8.1a"><mrow id="S5.SS1.p5.8.m8.1.1" xref="S5.SS1.p5.8.m8.1.1.cmml"><mi id="S5.SS1.p5.8.m8.1.1.2" xref="S5.SS1.p5.8.m8.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p5.8.m8.1.1.1" xref="S5.SS1.p5.8.m8.1.1.1.cmml">​</mo><mi id="S5.SS1.p5.8.m8.1.1.3" xref="S5.SS1.p5.8.m8.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.8.m8.1b"><apply id="S5.SS1.p5.8.m8.1.1.cmml" xref="S5.SS1.p5.8.m8.1.1"><times id="S5.SS1.p5.8.m8.1.1.1.cmml" xref="S5.SS1.p5.8.m8.1.1.1"></times><ci id="S5.SS1.p5.8.m8.1.1.2.cmml" xref="S5.SS1.p5.8.m8.1.1.2">𝑚</ci><ci id="S5.SS1.p5.8.m8.1.1.3.cmml" xref="S5.SS1.p5.8.m8.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.8.m8.1c">mm</annotation></semantics></math>, respectively. Table <a href="#S5.T4" title="Table 4 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> further confirms the importance of considering temporal information in consecutive frames, <em id="S5.SS1.p5.8.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS1.p5.8.2" class="ltx_text"></span>, compared with single-image-based methods, video-based methods have lower ACC-ERR. In summary, MPS-Net achieves a better balance in the accuracy and smoothness of 3D human pose and shape estimation.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Qualitative evaluation</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We present 1) visual comparisons with the TCMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, 2) visual effects of MPS-Net in alternative viewpoints, and 3) visual results of the learned human motion continuity.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Visual comparisons with the TCMR.</span> The qualitative comparison between TCMR and our MPS-Net on the 3DPW and MPI-INF-3DHP datasets is shown in Figure <a href="#S5.F6" title="Figure 6 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. From the results, we observe that the 3D human pose and shape estimated by MPS-Net can fit the input images well, especially on the limbs. TCMR seems to be too focused on generating smooth 3D human motion, so the estimated pose has relatively small changes from frame to frame, which limits its ability to fit the input images.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Visual effects of MPS-Net in alternative viewpoints.</span> We visualize the 3D human body estimated by MPS-Net from different viewpoints in Figure <a href="#S5.F7" title="Figure 7 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The results show that MPS-Net can estimate the correct global body rotation. This is quantitatively demonstrated by the improvements in the PA-MPJPE, MPJPE, and MPVPE (see Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Loss functions ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Visual results of the learned human motion continuity.</span> We use a relatively extreme example to show the continuity of human motion learned by MPS-Net. In this example, we randomly downloaded two pictures with different poses from the Internet, and copied the pictures multiple times to form a sequence. Then, we send the sequence to VIBE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and MPS-Net for 3D human pose and shape estimation. As shown in Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2 Qualitative evaluation ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, compared with VIBE, it is obvious from the estimation results that our MPS-Net produces a transition effect between pose exchanges, and this transition conforms to the continuity of human kinematics. It demonstrates that MPS-Net has indeed learned the continuity of human motion, and explains why MPS-Net can achieve lower ACC-ERR in the benchmark (action) datasets (see Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Loss functions ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). This result is also similar to using a 3D motion predictor to estimate reasonable human motion in-betweening of two key frames <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In contrast, VIBE relies too much on the features of the current frame, making it unable to truly learn the continuity of human motion. Thus, its ACC-ERR is still high (see Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Loss functions ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">For more results and video demos can be found at <a target="_blank" href="https://mps-net.github.io/MPS-Net/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mps-net.github.io/MPS-Net/</a>.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2203.08534/assets/Fig8.jpg" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.3.2" class="ltx_text" style="font-size:90%;">An example of visualization of the VIBE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and our MPS-Net on the continuity of human motion.</span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We propose the MPS-Net for estimating 3D human pose and shape from monocular video. The main contributions of this work lie in the design of the MoCA and HAFI modules. The former leverages visual cues observed from human motion to adaptively recalibrate the range that needs attention in the sequence to capture the motion continuity dependencies, and the later allows our model to strengthen temporal correlation and refine feature representation for producing temporally coherent estimates. Compared with existing methods, the integration of MoCA and HAFI modules demonstrates the advantages of our MPS-Net in achieving the state-of-the-art 3D human pose and shape estimation.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgment:</span>
This work was supported in part by MOST under grants 110-2221-E-001-016-MY3, 110-2634-F-007-027 and 110-2634-F-002-050, and Academia Sinica under grant AS-TP-111-M02.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton
Milan, Juergen Gall, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">PoseTrack: A benchmark for human pose estimation and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Neural machine translation by jointly learning to align and
translate.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Antoni Buades, Bartomeu Coll, and Jean-Michel Morel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">A non-local algorithm for image denoising.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2005.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Realtime multi-person 2D pose estimation using part affinity
fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Ding-Jie Chen, He-Yen Hsieh, and Tyng-Luh Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Adaptive image transformer for one-shot object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Beyond static features for temporally consistent 3D human pose and
shape from a video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">BERT: Pre-training of deep bidirectional transformers for language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NAACL-HLT</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Carl Doersch and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Sim2real transfer learning for 3D human pose estimation: Motion to
the rescue.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Foote.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Visualizing music and audio using self-similarity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Multimedia</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 1999.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Georgios Georgakis, Ren Li, Srikrishna Karanam, Terrence Chen, Jana Košecká, and Ziyan Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Hierarchical kinematic human mesh recovery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Félix G. Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Robust motion in-betweening.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM ToG</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 39(4):60:1–60:12, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, and Tyng-Luh Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">One-shot object detection with co-attention and co-excitation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Squeeze-and-excitation networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE TPAMI</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 42(8):2011–2023, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Human3.6M: Large scale datasets and predictive methods for 3D
human sensing in natural environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE TPAMI</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 36(7):1325–1339, 2014.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">End-to-end recovery of human shape and pose.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Learning 3D human dynamics from video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Diederik P. Kingma and Jimmy Lei Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Adam: a method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Muhammed Kocabas, Nikos Athanasiou, and Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">VIBE: Video inference for human body pose and shape estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Nikos Kolotouros, G. Pavlakos, Michael J. Black, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Learning to reconstruct 3D human pose and shape via model-fitting
in the loop.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Nikos Kolotouros, G. Pavlakos, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Convolutional mesh regression for single-image human shape
reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Matthew Loper, Naureen Mahmood, and Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">MoSh: Motion and shape capture from sparse markers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM ToG</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 33(6):220:1–220:13, 2014.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J.
Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">SMPL: a skinned multi-person linear model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM ToG</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 34(6):248:1–248:16, 2015.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Zhengyi Luo, S. Alireza Golestaneh, and Kris M. Kitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">3D human motion estimation via motion compression and refinement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACCV</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and
Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">AMASS: Archive of motion capture as surface shapes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko,
Weipeng Xu, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Monocular 3D human pose estimation in the wild using improved CNN
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">3DV</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Gyeongsik Moon and Kyoung Mu Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">I2L-MeshNet: Image-to-lixel prediction network for accurate
3D human pose and mesh estimation from a single RGB image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter V. Gehler, and Bernt
Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Neural Body Fitting: Unifying deep learning and model-based human
pose and shape estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">3DV</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Automatic differentiation in PyTorch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS Workshop on Autodiff</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
G. Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Learning to estimate 3D human pose and shape from a single color
image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Improving language understanding by generative pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">Technical report, OpenAI, 2018.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Abhijit Guha Roy, Nassir Navab, and Christian Wachinger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Recalibrating fully convolutional networks with spatial and channel
‘squeeze &amp; excitation’ blocks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE T-MI</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 38(2):540–549, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu, and Tao Mei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Human mesh recovery from monocular images via a skeleton-disentangled
representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Gül Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin Yumer, Ivan
Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">BodyNet: Volumetric inference of 3D human body shapes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Timo von Marcard, Roberto Henschel, Michael J. Black, Bodo Rosenhahn, and
Gerard Pons-Moll.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Recovering accurate 3D human pose in the wild using IMUs and a
moving camera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Non-local neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">CBAM: Convolutional block attention module.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Adams Wei Yu, David Dohan, Minh-Thang Luong, R. Zhao, Kai Chen, Mohammad
Norouzi, and Quoc V. Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">QANet: Combining local convolution with global self-attention for
reading comprehension.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang,
and Zhenan Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">PyMAF: 3D human pose and shape regression with pyramidal mesh
alignment feedback loop.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p ltx_align_center"><span id="p2.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Supplementary Material</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Datasets</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.4" class="ltx_p"><span id="S7.p1.4.1" class="ltx_text ltx_font_bold">3DPW.</span> 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> is mainly captured from outdoors and in-the-wild. It combines a hand-held camera and a set of inertial measurement unit (IMU) sensors attached at the body limbs to calculate the near ground-truth SMPL parameters. It contains a total of <math id="S7.p1.1.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S7.p1.1.m1.1a"><mn id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><cn type="integer" id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">60</annotation></semantics></math> videos of different lengths. We use the official split to train and test the model, where the training, validation, and test sets are composed of <math id="S7.p1.2.m2.1" class="ltx_Math" alttext="24" display="inline"><semantics id="S7.p1.2.m2.1a"><mn id="S7.p1.2.m2.1.1" xref="S7.p1.2.m2.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S7.p1.2.m2.1b"><cn type="integer" id="S7.p1.2.m2.1.1.cmml" xref="S7.p1.2.m2.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.2.m2.1c">24</annotation></semantics></math>, <math id="S7.p1.3.m3.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S7.p1.3.m3.1a"><mn id="S7.p1.3.m3.1.1" xref="S7.p1.3.m3.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S7.p1.3.m3.1b"><cn type="integer" id="S7.p1.3.m3.1.1.cmml" xref="S7.p1.3.m3.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.3.m3.1c">12</annotation></semantics></math>, and <math id="S7.p1.4.m4.1" class="ltx_Math" alttext="24" display="inline"><semantics id="S7.p1.4.m4.1a"><mn id="S7.p1.4.m4.1.1" xref="S7.p1.4.m4.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S7.p1.4.m4.1b"><cn type="integer" id="S7.p1.4.m4.1.1.cmml" xref="S7.p1.4.m4.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.4.m4.1c">24</annotation></semantics></math> videos, respectively. In addition, we report MPVPE on 3DPW because it is the only dataset that contains ground-truth 3D shape annotations among the datasets we used.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.4" class="ltx_p"><span id="S7.p2.4.1" class="ltx_text ltx_font_bold">MPI-INF-3DHP.</span> MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> is a dataset consisting of both constrained indoor and complex outdoor scenes. It is captured using a multi-view camera setting with a markerless motion capture system, and the 3D joint annotation is calculated through the multiview method. Following existing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we use the official split to train and test the model. The training set contains <math id="S7.p2.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S7.p2.1.m1.1a"><mn id="S7.p2.1.m1.1.1" xref="S7.p2.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S7.p2.1.m1.1b"><cn type="integer" id="S7.p2.1.m1.1.1.cmml" xref="S7.p2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.m1.1c">8</annotation></semantics></math> subjects, each of which has <math id="S7.p2.2.m2.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S7.p2.2.m2.1a"><mn id="S7.p2.2.m2.1.1" xref="S7.p2.2.m2.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S7.p2.2.m2.1b"><cn type="integer" id="S7.p2.2.m2.1.1.cmml" xref="S7.p2.2.m2.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.2.m2.1c">16</annotation></semantics></math> videos, and the test set contains <math id="S7.p2.3.m3.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S7.p2.3.m3.1a"><mn id="S7.p2.3.m3.1.1" xref="S7.p2.3.m3.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S7.p2.3.m3.1b"><cn type="integer" id="S7.p2.3.m3.1.1.cmml" xref="S7.p2.3.m3.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.3.m3.1c">6</annotation></semantics></math> subjects, performing <math id="S7.p2.4.m4.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S7.p2.4.m4.1a"><mn id="S7.p2.4.m4.1.1" xref="S7.p2.4.m4.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S7.p2.4.m4.1b"><cn type="integer" id="S7.p2.4.m4.1.1.cmml" xref="S7.p2.4.m4.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.4.m4.1c">7</annotation></semantics></math> actions in both indoor and outdoor environments.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.4" class="ltx_p"><span id="S7.p3.4.1" class="ltx_text ltx_font_bold">Human3.6M.</span> Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> is one of the largest motion capture datasets, which contains <math id="S7.p3.1.m1.1" class="ltx_Math" alttext="3.6" display="inline"><semantics id="S7.p3.1.m1.1a"><mn id="S7.p3.1.m1.1.1" xref="S7.p3.1.m1.1.1.cmml">3.6</mn><annotation-xml encoding="MathML-Content" id="S7.p3.1.m1.1b"><cn type="float" id="S7.p3.1.m1.1.1.cmml" xref="S7.p3.1.m1.1.1">3.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.1.m1.1c">3.6</annotation></semantics></math> million video frames and corresponding 3D joint annotations. This dataset is collected in an indoor and controlled environment. Same as existing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we train the model on <math id="S7.p3.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S7.p3.2.m2.1a"><mn id="S7.p3.2.m2.1.1" xref="S7.p3.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S7.p3.2.m2.1b"><cn type="integer" id="S7.p3.2.m2.1.1.cmml" xref="S7.p3.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.2.m2.1c">5</annotation></semantics></math> subjects (<em id="S7.p3.4.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S7.p3.4.3" class="ltx_text"></span>, S1, S5, S6, S7, and S8) and evaluate it on <math id="S7.p3.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S7.p3.3.m3.1a"><mn id="S7.p3.3.m3.1.1" xref="S7.p3.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S7.p3.3.m3.1b"><cn type="integer" id="S7.p3.3.m3.1.1.cmml" xref="S7.p3.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.3.m3.1c">2</annotation></semantics></math> subjects (<em id="S7.p3.4.4" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S7.p3.4.5" class="ltx_text"></span>, S9 and S11). We subsampled the dataset to <math id="S7.p3.4.m4.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S7.p3.4.m4.1a"><mn id="S7.p3.4.m4.1.1" xref="S7.p3.4.m4.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S7.p3.4.m4.1b"><cn type="integer" id="S7.p3.4.m4.1.1.cmml" xref="S7.p3.4.m4.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.4.m4.1c">25</annotation></semantics></math> frames per second (fps) for training and testing.</p>
</div>
<div id="S7.p4" class="ltx_para ltx_noindent">
<p id="S7.p4.4" class="ltx_p"><span id="S7.p4.4.1" class="ltx_text ltx_font_bold">AMASS.</span> AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> is a large-scale human motion sequence database that unifies <math id="S7.p4.1.m1.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S7.p4.1.m1.1a"><mn id="S7.p4.1.m1.1.1" xref="S7.p4.1.m1.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S7.p4.1.m1.1b"><cn type="integer" id="S7.p4.1.m1.1.1.cmml" xref="S7.p4.1.m1.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p4.1.m1.1c">15</annotation></semantics></math> existing motion capture (mocap) datasets by representing them within a common framework and parameterization. These motion sequences are annotated with Mosh++ to generate SMPL parameters. AMASS has a total of <math id="S7.p4.2.m2.1" class="ltx_Math" alttext="42" display="inline"><semantics id="S7.p4.2.m2.1a"><mn id="S7.p4.2.m2.1.1" xref="S7.p4.2.m2.1.1.cmml">42</mn><annotation-xml encoding="MathML-Content" id="S7.p4.2.m2.1b"><cn type="integer" id="S7.p4.2.m2.1.1.cmml" xref="S7.p4.2.m2.1.1">42</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p4.2.m2.1c">42</annotation></semantics></math> hours of mocap, <math id="S7.p4.3.m3.1" class="ltx_Math" alttext="346" display="inline"><semantics id="S7.p4.3.m3.1a"><mn id="S7.p4.3.m3.1.1" xref="S7.p4.3.m3.1.1.cmml">346</mn><annotation-xml encoding="MathML-Content" id="S7.p4.3.m3.1b"><cn type="integer" id="S7.p4.3.m3.1.1.cmml" xref="S7.p4.3.m3.1.1">346</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p4.3.m3.1c">346</annotation></semantics></math> subjects, and <math id="S7.p4.4.m4.2" class="ltx_Math" alttext="11,451" display="inline"><semantics id="S7.p4.4.m4.2a"><mrow id="S7.p4.4.m4.2.3.2" xref="S7.p4.4.m4.2.3.1.cmml"><mn id="S7.p4.4.m4.1.1" xref="S7.p4.4.m4.1.1.cmml">11</mn><mo id="S7.p4.4.m4.2.3.2.1" xref="S7.p4.4.m4.2.3.1.cmml">,</mo><mn id="S7.p4.4.m4.2.2" xref="S7.p4.4.m4.2.2.cmml">451</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p4.4.m4.2b"><list id="S7.p4.4.m4.2.3.1.cmml" xref="S7.p4.4.m4.2.3.2"><cn type="integer" id="S7.p4.4.m4.1.1.cmml" xref="S7.p4.4.m4.1.1">11</cn><cn type="integer" id="S7.p4.4.m4.2.2.cmml" xref="S7.p4.4.m4.2.2">451</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.p4.4.m4.2c">11,451</annotation></semantics></math> human motions. Following the setting of the existing method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, we use this database to train our MPS-Net.</p>
</div>
<div id="S7.p5" class="ltx_para ltx_noindent">
<p id="S7.p5.5" class="ltx_p"><span id="S7.p5.5.1" class="ltx_text ltx_font_bold">PoseTrack.</span> PoseTrack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is a 2D benchmark dataset for video-based multi-person pose estimation and articulated tracking. It contains a total of <math id="S7.p5.1.m1.2" class="ltx_Math" alttext="1,337" display="inline"><semantics id="S7.p5.1.m1.2a"><mrow id="S7.p5.1.m1.2.3.2" xref="S7.p5.1.m1.2.3.1.cmml"><mn id="S7.p5.1.m1.1.1" xref="S7.p5.1.m1.1.1.cmml">1</mn><mo id="S7.p5.1.m1.2.3.2.1" xref="S7.p5.1.m1.2.3.1.cmml">,</mo><mn id="S7.p5.1.m1.2.2" xref="S7.p5.1.m1.2.2.cmml">337</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p5.1.m1.2b"><list id="S7.p5.1.m1.2.3.1.cmml" xref="S7.p5.1.m1.2.3.2"><cn type="integer" id="S7.p5.1.m1.1.1.cmml" xref="S7.p5.1.m1.1.1">1</cn><cn type="integer" id="S7.p5.1.m1.2.2.cmml" xref="S7.p5.1.m1.2.2">337</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.p5.1.m1.2c">1,337</annotation></semantics></math> videos, divided into <math id="S7.p5.2.m2.1" class="ltx_Math" alttext="792" display="inline"><semantics id="S7.p5.2.m2.1a"><mn id="S7.p5.2.m2.1.1" xref="S7.p5.2.m2.1.1.cmml">792</mn><annotation-xml encoding="MathML-Content" id="S7.p5.2.m2.1b"><cn type="integer" id="S7.p5.2.m2.1.1.cmml" xref="S7.p5.2.m2.1.1">792</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p5.2.m2.1c">792</annotation></semantics></math>, <math id="S7.p5.3.m3.1" class="ltx_Math" alttext="170" display="inline"><semantics id="S7.p5.3.m3.1a"><mn id="S7.p5.3.m3.1.1" xref="S7.p5.3.m3.1.1.cmml">170</mn><annotation-xml encoding="MathML-Content" id="S7.p5.3.m3.1b"><cn type="integer" id="S7.p5.3.m3.1.1.cmml" xref="S7.p5.3.m3.1.1">170</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p5.3.m3.1c">170</annotation></semantics></math>, and <math id="S7.p5.4.m4.1" class="ltx_Math" alttext="375" display="inline"><semantics id="S7.p5.4.m4.1a"><mn id="S7.p5.4.m4.1.1" xref="S7.p5.4.m4.1.1.cmml">375</mn><annotation-xml encoding="MathML-Content" id="S7.p5.4.m4.1b"><cn type="integer" id="S7.p5.4.m4.1.1.cmml" xref="S7.p5.4.m4.1.1">375</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p5.4.m4.1c">375</annotation></semantics></math> videos for training, validation, and testing. Each person instance in the video is annotated with <math id="S7.p5.5.m5.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S7.p5.5.m5.1a"><mn id="S7.p5.5.m5.1.1" xref="S7.p5.5.m5.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S7.p5.5.m5.1b"><cn type="integer" id="S7.p5.5.m5.1.1.cmml" xref="S7.p5.5.m5.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p5.5.m5.1c">15</annotation></semantics></math> keypoints. Same as existing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we use the training set for model training.</p>
</div>
<div id="S7.p6" class="ltx_para ltx_noindent">
<p id="S7.p6.2" class="ltx_p"><span id="S7.p6.2.1" class="ltx_text ltx_font_bold">InstaVariety.</span> InstaVariety <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is a 2D benchmark dataset captured from Instagram using 84 hashtags. There are <math id="S7.p6.1.m1.2" class="ltx_Math" alttext="28,272" display="inline"><semantics id="S7.p6.1.m1.2a"><mrow id="S7.p6.1.m1.2.3.2" xref="S7.p6.1.m1.2.3.1.cmml"><mn id="S7.p6.1.m1.1.1" xref="S7.p6.1.m1.1.1.cmml">28</mn><mo id="S7.p6.1.m1.2.3.2.1" xref="S7.p6.1.m1.2.3.1.cmml">,</mo><mn id="S7.p6.1.m1.2.2" xref="S7.p6.1.m1.2.2.cmml">272</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p6.1.m1.2b"><list id="S7.p6.1.m1.2.3.1.cmml" xref="S7.p6.1.m1.2.3.2"><cn type="integer" id="S7.p6.1.m1.1.1.cmml" xref="S7.p6.1.m1.1.1">28</cn><cn type="integer" id="S7.p6.1.m1.2.2.cmml" xref="S7.p6.1.m1.2.2">272</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.p6.1.m1.2c">28,272</annotation></semantics></math> videos in total, with an average length of <math id="S7.p6.2.m2.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S7.p6.2.m2.1a"><mn id="S7.p6.2.m2.1.1" xref="S7.p6.2.m2.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S7.p6.2.m2.1b"><cn type="integer" id="S7.p6.2.m2.1.1.cmml" xref="S7.p6.2.m2.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p6.2.m2.1c">6</annotation></semantics></math> seconds, and OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is used to acquire pseudo ground-truth 2D joint annotations. Same as existing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we adopt this dataset for training.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Evaluation metrics</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Four standard evaluation metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> are considered, including MPJPE, PA-MPJPE, MPVPE, and ACC-ERR. Specifically, MPJPE is calculated as the mean of the Euclidean distance between the ground-truth and the estimated 3D joint positions after aligning the pelvis joint on the ground truth location. PA-MPJPE is calculated similarly to MPJPE, but after the estimated pose is rigidly aligned with the ground-truth pose. MPVPE is calculated as the mean of the Euclidean distance between the ground truth and the estimated 3D mesh vertices (output by the SMPL model). ACC-ERR is measured as the mean difference between the ground-truth and the estimated 3D acceleration for every joint.</p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Ablation study of HAFI module</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">To analyze the effect of the number of frames per group in the HAFI module, we conduct ablation studies on MPS-Net with different HAFI module settings under the 3DPW dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. The results in Table <a href="#S9.T5" title="Table 5 ‣ 9 Ablation study of HAFI module ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> indicate that considering the temporal features of three adjacent frames as a group can enable our MPS-Net to achieve the best 3D human pose and shape estimation. Therefore, for all experiments (<em id="S9.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S9.p1.1.2" class="ltx_text"></span>, Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Loss functions ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> to Table <a href="#S5.T4" title="Table 4 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), the HAFI module defaults to three frames as a group.</p>
</div>
<figure id="S9.T5" class="ltx_table">
<table id="S9.T5.4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S9.T5.4.4.5.1" class="ltx_tr">
<th id="S9.T5.4.4.5.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S9.T5.4.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">3DPW</th>
</tr>
<tr id="S9.T5.4.4.4" class="ltx_tr">
<th id="S9.T5.4.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r">Method</th>
<th id="S9.T5.1.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">PA-MPJPE <math id="S9.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S9.T5.1.1.1.1.m1.1a"><mo stretchy="false" id="S9.T5.1.1.1.1.m1.1.1" xref="S9.T5.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S9.T5.1.1.1.1.m1.1b"><ci id="S9.T5.1.1.1.1.m1.1.1.cmml" xref="S9.T5.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.T5.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S9.T5.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">MPJPE <math id="S9.T5.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S9.T5.2.2.2.2.m1.1a"><mo stretchy="false" id="S9.T5.2.2.2.2.m1.1.1" xref="S9.T5.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S9.T5.2.2.2.2.m1.1b"><ci id="S9.T5.2.2.2.2.m1.1.1.cmml" xref="S9.T5.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.T5.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S9.T5.3.3.3.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">MPVPE <math id="S9.T5.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S9.T5.3.3.3.3.m1.1a"><mo stretchy="false" id="S9.T5.3.3.3.3.m1.1.1" xref="S9.T5.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S9.T5.3.3.3.3.m1.1b"><ci id="S9.T5.3.3.3.3.m1.1.1.cmml" xref="S9.T5.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.T5.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S9.T5.4.4.4.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">ACC-ERR <math id="S9.T5.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S9.T5.4.4.4.4.m1.1a"><mo stretchy="false" id="S9.T5.4.4.4.4.m1.1.1" xref="S9.T5.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S9.T5.4.4.4.4.m1.1b"><ci id="S9.T5.4.4.4.4.m1.1.1.cmml" xref="S9.T5.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.T5.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S9.T5.4.4.6.1" class="ltx_tr">
<th id="S9.T5.4.4.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MPS-Net</th>
<td id="S9.T5.4.4.6.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" rowspan="2"><span id="S9.T5.4.4.6.1.2.1" class="ltx_text">52.6</span></td>
<td id="S9.T5.4.4.6.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" rowspan="2"><span id="S9.T5.4.4.6.1.3.1" class="ltx_text">85.4</span></td>
<td id="S9.T5.4.4.6.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" rowspan="2"><span id="S9.T5.4.4.6.1.4.1" class="ltx_text">101.0</span></td>
<td id="S9.T5.4.4.6.1.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" rowspan="2"><span id="S9.T5.4.4.6.1.5.1" class="ltx_text">7.8</span></td>
</tr>
<tr id="S9.T5.4.4.7.2" class="ltx_tr">
<th id="S9.T5.4.4.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">(HAFI, 2 frames/group)</th>
</tr>
<tr id="S9.T5.4.4.8.3" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S9.T5.4.4.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S9.T5.4.4.8.3.1.1" class="ltx_text" style="background-color:#E6E6E6;">MPS-Net</span></th>
<td id="S9.T5.4.4.8.3.2" class="ltx_td ltx_nopad_l"></td>
<td id="S9.T5.4.4.8.3.3" class="ltx_td ltx_nopad_l"></td>
<td id="S9.T5.4.4.8.3.4" class="ltx_td ltx_nopad_l"></td>
<td id="S9.T5.4.4.8.3.5" class="ltx_td ltx_nopad_l"></td>
</tr>
<tr id="S9.T5.4.4.9.4" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S9.T5.4.4.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S9.T5.4.4.9.4.1.1" class="ltx_text" style="background-color:#E6E6E6;">(HAFI, 3 frames/group)</span></th>
<td id="S9.T5.4.4.9.4.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S9.T5.4.4.9.4.2.1" class="ltx_text ltx_font_bold" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">52.1</span></td>
<td id="S9.T5.4.4.9.4.3" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S9.T5.4.4.9.4.3.1" class="ltx_text ltx_font_bold" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">84.3</span></td>
<td id="S9.T5.4.4.9.4.4" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S9.T5.4.4.9.4.4.1" class="ltx_text ltx_font_bold" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">99.7</span></td>
<td id="S9.T5.4.4.9.4.5" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S9.T5.4.4.9.4.5.1" class="ltx_text ltx_font_bold" style="position:relative; bottom:6.0pt;background-color:#E6E6E6;">7.4</span></td>
</tr>
<tr id="S9.T5.4.4.10.5" class="ltx_tr">
<th id="S9.T5.4.4.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MPS-Net</th>
<td id="S9.T5.4.4.10.5.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" rowspan="2"><span id="S9.T5.4.4.10.5.2.1" class="ltx_text">52.5</span></td>
<td id="S9.T5.4.4.10.5.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" rowspan="2"><span id="S9.T5.4.4.10.5.3.1" class="ltx_text">85.9</span></td>
<td id="S9.T5.4.4.10.5.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" rowspan="2"><span id="S9.T5.4.4.10.5.4.1" class="ltx_text">101.2</span></td>
<td id="S9.T5.4.4.10.5.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" rowspan="2"><span id="S9.T5.4.4.10.5.5.1" class="ltx_text">7.6</span></td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr id="S9.T5.4.4.11.1" class="ltx_tr">
<th id="S9.T5.4.4.11.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">(HAFI, 4 frames/group)</th>
</tr>
</tfoot>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S9.T5.6.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S9.T5.7.2" class="ltx_text" style="font-size:90%;">Effect of the number of frames per group in the HAFI module. The training and evaluation settings are the same as the experiments on the 3DPW dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> in Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Loss functions ‣ 3 Method ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</span></figcaption>
</figure>
<div id="S9.p2" class="ltx_para">
<p id="S9.p2.1" class="ltx_p">On the other hand, we further add the results considering only the HAFI module on MPS-Net (called MPS-Net-only HAFI) to supplement the ablation experiments in Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, with results of 54.0, 87.6, 103.5, and 7.5, respectively, for PA-MPJPE, MPJPE, MPVPE and ACC-ERR. Compared with MPS-Net-only MoCA (see Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), MPS-Net-only HAFI yields better ACC-ERR but worse on PA-MPJPE, MPJPE and MPVPE. However, by coupling MoCA with HAFI, MPS-Net (Ours) achieves the best result.</p>
</div>
<figure id="S9.F9" class="ltx_figure"><img src="/html/2203.08534/assets/Fig9.png" id="S9.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="474" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S9.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S9.F9.3.2" class="ltx_text" style="font-size:90%;">Visual comparison of 3D human pose and shape estimation of MoCA module and non-local block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> on the 3DPW dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. Where the attention map is generated from the non-local operation, and the MoCA map is generated from the MoCA operation. In the attention and MoCA maps, red indicates a higher attention value, and blue indicates a lower one. The results demonstrate that the MoCA map generated by our MoCA operation can indeed allow the MPS-Net to focus attention on a more appropriate range of action sequence to improve the estimation results.</span></figcaption>
</figure>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Qualitative comparison between MoCA module and non-local block</h2>

<div id="S10.p1" class="ltx_para">
<p id="S10.p1.1" class="ltx_p">To further verify whether the proposed MoCA operation can improve the 3D human pose and shape estimation by introducing NSSM to recalibrate the attention map generated by the non-local operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we conduct the following qualitative experiments. Specifically, we visualize the 3D human pose and shape estimation resulted from the methods of MPS-Net-only Non-local and MPS-Net-only MoCA in Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, respectively. The results can be seen from the two examples in Figure <a href="#S9.F9" title="Figure 9 ‣ 9 Ablation study of HAFI module ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> that the MoCA map generated by our MoCA operation can indeed allow the MPS-Net to focus attention on a more appropriate range of action sequence, thereby improving the accuracy of 3D human pose and shape estimation. On the contrary, the attention map generated by non-local operation is often unstable, and it is easy to focus attention on less correlated frames and ignore the continuity of human motion in the action sequence, which reduces the accuracy of estimation. Such a result is quantitatively demonstrated by the improvement of MPS-Net-only MoCA in the MPJPE, PA-MPJPE, and MPVPE errors (see Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div id="S10.p2" class="ltx_para">
<p id="S10.p2.1" class="ltx_p">On the other hand, we also add the results considering only the setting of NSSM on MPS-Net (called MPS-Net-only NSSM) to supplement the ablation experiments in Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, with results of 53.3, 88.0, 104.1, and 26.0, respectively, for PA-MPJPE, MPJPE, MPVPE and ACC-ERR. These (cf. Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Comparison with state-of-the-art methods ‣ 5 Experiments ‣ Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) indicate that MPS-Net-only NSSM and MPS-Net-only Non-local are complementary, and their fusion, <em id="S10.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S10.p2.1.2" class="ltx_text"></span>, MPS-Net-only MoCA, can further achieve better results.</p>
</div>
</section>
<section id="S11" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>Effect of motion speed and input fps</h2>

<div id="S11.p1" class="ltx_para">
<p id="S11.p1.1" class="ltx_p">To demonstrate how motion speed and frame rate influence MPS-Net performance, we conducted experiments with various demo videos (<em id="S11.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S11.p1.1.2" class="ltx_text"></span>, fast-moving dancing or general walking video) at different frame rates, from 24fps to 60fps, and found that it has little impact on MPS-Net. Frame rate info for each demo video is available on our demo website.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2203.08533" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2203.08534" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2203.08534">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2203.08534" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2203.08535" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 06:37:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
