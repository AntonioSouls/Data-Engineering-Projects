<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.12310] Fully Sparse Fusion for 3D Object Detection</title><meta property="og:description" content="Currently prevalent multimodal 3D detection methods are built upon LiDAR-based detectors that usually use dense Bird’s-Eye-View (BEV) feature maps.
However, the cost of such BEV feature maps is quadratic to the detecti…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fully Sparse Fusion for 3D Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fully Sparse Fusion for 3D Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.12310">

<!--Generated on Thu Feb 29 13:19:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fully Sparse Fusion for 3D Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yingyan Li 
<br class="ltx_break">   
CASIA
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lue Fan 
<br class="ltx_break">   
CASIA
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Liu 
<br class="ltx_break">   
CASIA
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zehao Huang 
<br class="ltx_break">   
TuSimple
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuntao Chen 
<br class="ltx_break">CAIR,HKISI,CAS
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Naiyan Wang 
<br class="ltx_break">TuSimple
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhaoxiang Zhang 
<br class="ltx_break">CASIA
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tieniu Tan 
<br class="ltx_break">CASIA
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{liyingyan2021, fanlue2019, liuyang2022, zhaoxiang.zhang}@ia.ac.cn </span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{zehaohuang18, chenyuntao08, winsty}@gmail.com, tnt@nlpr.ia.ac.cn
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">Currently prevalent multimodal 3D detection methods are built upon LiDAR-based detectors that usually use dense Bird’s-Eye-View (BEV) feature maps.
However, the cost of such BEV feature maps is quadratic to the detection range, making it not suitable for long-range detection.
Fully sparse architecture is gaining attention as they are highly efficient in long-range perception.
In this paper, we study how to effectively leverage image modality in the emerging fully sparse architecture.
Particularly, utilizing instance queries, our framework integrates the well-studied 2D instance segmentation into the LiDAR side, which is parallel to the 3D instance segmentation part in the fully sparse detector.
This design achieves a uniform query-based fusion framework in both the 2D and 3D sides while maintaining the fully sparse characteristic.
Extensive experiments showcase state-of-the-art results on the widely used nuScenes dataset and the long-range Argoverse 2 dataset.
Notably, the inference speed of the proposed method under the long-range LiDAR perception setting is 2.7 <math id="id1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation></semantics></math> faster than that of other state-of-the-art multimodal 3D detection methods.
Code will be released at <a target="_blank" href="https://github.com/BraveGroup/FullySparseFusion" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/BraveGroup/FullySparseFusion</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2304.12310/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="231" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison between dense fusion and sparse fusion.
Dense fusion methods rely on LiDAR BEV feature maps for final prediction.
In contrast, our sparse fusion framework unites two modalities at the instance level, requiring no dense feature map.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Autonomous driving systems heavily rely on 3D object detection.
Currently, LiDARs and cameras are the two main sensors used for perception.
LiDAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> offers precise spatial positioning but struggles to recognize small and distant objects due to its sparsity nature.
On the other hand, cameras <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> provide a wealth of semantic information but lack direct depth information.
Combining this two sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> leads to improved detection precision and makes the system more robust to different scenarios.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Currently, the predominant multi-modal methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> rely on dense LiDAR detectors.
Dense LiDAR detectors imply the detectors construct dense Bird’s-Eye-View (BEV) feature maps for prediction.
The size of these BEV feature maps increases quadratically with perception range, which leads to unaffordable costs in long-range detection.
To address this issue, several voxel-based fully sparse detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> have emerged, largely extending the perception range of the LiDAR side.
As a representative, FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> demonstrates impressive efficiency and efficacy.
However, incorporating multi-modal input needs a deliberate design for fully sparse structure.
How to develop an effective and efficient multi-modal fully sparse detector is untapped.
In this paper, we focus on extending the emerging fully sparse architecture into the domain of multi-modal detection.
</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">A couple of options could be adopted to fuse image information into sparse detectors.
The most straightforward one is point-wise painting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> with image features or semantic masks, which does not involve the dense BEV feature maps.
However, the straightforward painting only utilizes semantic information, overlooking the instance-level information, which is important for the detection task.
Fortunately, like other emerging detectors, FSD also follows the query-based paradigm, where the queries exactly correspond to 3D instances.
This enlightens us to also construct the 2D instance queries to make the fusion unified as a query-based paradigm.
In particular, we take the freebie from the well-studied 2D instance segmentation.
The 2D instance masks are viewed as 2D queries, which are lifted to 3D space by gathering the corresponding point cloud in the frustum.
In this way, these queries generated from images can be aligned with the queries from the LiDAR side, establishing a unified multi-modal input for further processing.
As Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows, our design unites two structures from different modalities without incorporating any other feature-level view transformations, fulfilling our initial motivation of maintaining the fully sparse property.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In addition to the unified structure, our method also offers a good supplement for FSD, in which the 3D instance segmentation plays an essential but error-prone role.
Without image information, the 3D instance segmentation itself is prone to miss non-discriminative foreground objects or mix crowded objects.
Parallel to the 3D part, the queries generated from the 2D instance mask provide rich semantic information and strong instance-level hints, effectively alleviating such errors.
This feature solely achieves a significant performance boost from the LiDAR-only version.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We summarize our contributions as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a Fully Sparse Fusion (FSF) framework that first introduces the fully sparse architecture into multi-modal detection. FSF effectively unifies the two modalities within a sparse framework based on a query-based paradigm by integrating the well-studied 2D instance segmentation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The FSF achieves state-of-the-art performance on two large-scale datasets, nuScenes and Argoverse 2.
Notably, FSF showcases superior efficiency in long-range detection (Argoverse 2) over other state-of-the-art multimodal 3D detectors.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Camera-based 3D detection</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In the early years of camera-based 3D detection, the focus is mainly on monocular 3D detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
However, monocular 3D detection ignores the relationships between cameras.
To this end, more and more researchers are engaged in multi-view 3D detection.
Multi-view 3D detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> contains two main types of approaches: BEV-based and learnable-query-based.
BEV-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> lift 2D features to 3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> or using transformer-like structures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> to construct BEV feature map for prediction.
On the contrary, learnable-query-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> predict 3D bounding boxes from learnable queries.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>LiDAR-based 3D detection</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Although camera-based 3D detection methods achieve great results, their performance is still inferior to LiDAR-based 3D detection methods.
LiDAR-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> usually convert irregular LiDAR points into regular space for feature extraction.
PointPillar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> and 3DFCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> employ dense convolution to extract features.
However, utilizing dense convolution introduces heavy computational cost.
To address this issue, SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> applies sparse convolution to 3D detection and inspires an increasing number of methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> adopting sparse convolution for feature extraction.
However, even with sparse convolution, most methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> still require a dense feature map to alleviate the so-called “center feature missing” issue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
Researchers have been dedicated to developing fully sparse methods since point clouds are naturally sparse in 3D space.
The pioneering works in this domain are PointNet-like methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
Also, range view <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> requires no BEV feature map, though their performance is inferior.
Recently, a fully sparse 3D detection algorithm named FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> has been proposed.
FSD has demonstrated faster speed and higher accuracy than previous methods.
Our work is based on FSD.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Multi-modal 3D detection</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The multimodal 3D detection approaches can be classified into two types, namely dense and sparse, depending on whether it relies on the dense BEV feature map.
As the representation of dense frameworks, BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> fuses the BEV feature maps generated from camera and LiDAR modalities.
Another line of dense fusion frameworks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> relies on the BEV feature map to generate proposals.
The sparse approach, on the other hand, does not rely on dense feature maps.
The most popular approaches are the point-level methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, which project LiDAR points onto the image plane to obtain image semantic cues. PointPainting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> paints semantic scores, while PointAugmenting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> paints features.
Also, there are plenty of instance-level fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> methods.
MVP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> uses instance information to produce virtual points to enrich point clouds.
FrustumPointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and FrustumConvnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> utilize frustums to generate proposals.
These methods work without relying on dense feature maps.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2304.12310/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of our framework. First, Bi-Modal Query Generation in §<a href="#S4.SS2" title="4.2 Bi-modal Query Generation ‣ 4 Methodology ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> generates queries from two modalities. Then, these queries are aligned by the Query Alignment module in §<a href="#S4.SS3" title="4.3 Bi-modal Query Refinement ‣ 4 Methodology ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
Finally, the results are predicted by these aligned query features.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Preliminary: Fully Sparse 3D Detector</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We first briefly introduce the emerging fully sparse 3D object detector FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which is adopted as our LiDAR-only baseline.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">3D Instance Segmentation</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">Given LiDAR point clouds, FSD first performs 3D instance segmentation to produce 3D instances.
Specifically, FSD extracts voxel features from the point cloud using a sparse voxel encoder in the beginning.
Then the voxel features are mapped into their included points to build the point features.
Using these point features, FSD classifies the points and only retains the foreground points.
The retained foreground points predict their corresponding object centers, making the points belonging to the same object closer to each other.
This process is called Voting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
After that, a <em id="S3.SS0.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">Connected Components Labeling (CCL)</em> algorithm is leveraged to obtain 3D instances by connecting the voted centers to form point clusters.
The obtained point clusters are viewed as 3D instances.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Sparse Prediction</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">Given these instances, a point-based instance feature extractor, named Sparse Instance Recognition (SIR) module is utilized for feature extraction.
The input of SIR is the feature of every point in the instance.
Then a VFE-like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> structure is used to produce high-quality instance features.
Finally, 3D bounding boxes are produced by a simple MLP head.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p2.1" class="ltx_p">The structure mentioned above does not involve any dense BEV feature maps.
Thus it can be easily extended to long-range scenarios. However, how to equip the fully sparse architecture with rich image semantics is still under investigation. In the following section, we will describe our Fully Sparse Fusion (FSF) framework which first introduces the fully sparse architecture into multi-modal detection.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Overall Architecture</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The overall architecture of FSF is shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.3 Multi-modal 3D detection ‣ 2 Related Work ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The framework could be roughly divided into two parts: the Bi-Modal Query Generation module in §<a href="#S4.SS2.SSS0.Px2" title="Camera Queries ‣ 4.2 Bi-modal Query Generation ‣ 4 Methodology ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> and the Bi-Modal Query Refinement module in §<a href="#S4.SS3" title="4.3 Bi-modal Query Refinement ‣ 4 Methodology ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
The former generates queries specific to each modality.
Afterwards, the latter aligns and refines the queries from different modalities to predict high-quality detection results.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Bi-modal Query Generation</h3>

<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LiDAR Queries</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">For the LiDAR modality, we adopt the 3D instance segmentation method in FSD to produce 3D instances.
We refer these instances as LiDAR queries, and we define the position of a query as its centroid.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Camera Queries</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">3D instance segmentation is subjected to the sparsity of the point cloud and the lack of semantic information.
The segmentation is prone to miss objects when the point cloud is too sparse.
Moreover, in the scene where the instances are too crowded, it is easy to cluster multiple objects into one cluster as shown in Fig. <a href="#S4.F3" title="Figure 3 ‣ Camera Queries ‣ 4.2 Bi-modal Query Generation ‣ 4 Methodology ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
To tackle this problem, we utilize 2D instance segmentation to address the limitations of 3D instance segmentation.
Specifically, we use the instance masks from well-studied 2D instance segmentation to generate initial frustums.
We then crop the points within frustums to form point clusters.
Note that frustums might overlap with each other, so we make copies of those points falling into the overlaps regions of each frustum.
These point clusters can also be viewed as 3D instances, but they have totally different shapes from the ones from the LiDAR side.
We will discuss the detailed solution in §<a href="#S4.SS3" title="4.3 Bi-modal Query Refinement ‣ 4 Methodology ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
For now, we name such coarse 3D instances generated from frustums as camera queries.
We define the center of a camera query as the weighted average of all points in this instance.
The weights are the classification scores from 3D instance segmentation.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2304.12310/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>(a): 3D instance segmentation is prone to ignore distant objects. (b): It is hard for 3D segmentation to separate the overlapped objects in a crowded scene. On the contrary, it is easy to handle these cases via 2D instance segmentation.</figcaption>
</figure>
<div id="S4.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p2.1" class="ltx_p">So far, we obtain two sets of queries from LiDAR and camera, respectively.
They are in a consistent form that is in fact the 3D instance, facilitating a unified query feature extraction and prediction pipeline.
In addition, the two kinds of queries are complementary to each other due to the different nature of their sources.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Bi-modal Query Refinement</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Query Alignment</span>

Although these two kinds of queries are in a consistent form, they might contain point clusters with totally different shapes.
To unify these two kinds of queries, we must first align their shapes to simplify the pipeline and enable more effective feature learning.
To this end, we propose to predict reference boxes from these queries.
Then the reference boxes are used to correct the misaligned point cloud shape by further cropping points.
Particularly, we apply SIR in FSD, a VFE-like module, on both of the two kinds of point clusters (i.e., queries) to extract the cluster features.
Eventually, we use the extracted cluster features to predict the reference boxes.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Query Prediction</span>
So far, the queries generated from the two modalities are aligned in the form of reference 3D bounding boxes.
These boxes are utilized to crop points and then extract the box features by another SIR module.
The extracted box features are then used for the final classification and bounding box regression.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Query Label Assignment</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">FSD develops a simple yet effective assignment strategy, which regards queries falling into GT boxes as positive, and we name this strategy <em id="S4.SS4.p1.1.1" class="ltx_emph ltx_font_italic">query-in-box</em> assignment.
However, this strategy could hardly work for camera queries for two reasons.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">The positions (i.e., cluster centroids) of camera queries are prone to have larger errors along the depth direction as Fig. <a href="#S4.F4" title="Figure 4 ‣ 3D Round ‣ 4.4 Query Label Assignment ‣ 4 Methodology ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows.
The large error may be caused by overlapped foreground objects or background points in the frustum.
After the query alignment, the reference boxes from camera queries have smaller variances but still are likely to be out of the GT boxes.
In this circumstance, query-in-box assignment may cause a considerable amount of camera queries to be mistakenly assigned as negative.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Due to the potential error of 2D instance segmentation and camera calibrations, a single frustum may contain multiple GTs, causing ambiguity in the assignment.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">To address these two issues, we propose a 3D/2D two-round assignment.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.10.10" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:447.2pt;height:230.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-108.1pt,55.7pt) scale(0.674098906966791,0.674098906966791) ;">
<table id="S4.T1.10.10.10" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.10.10.10.11" class="ltx_tr">
<td id="S4.T1.10.10.10.11.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt ltx_border_t" rowspan="2"><span id="S4.T1.10.10.10.11.1.1" class="ltx_text">Method</span></td>
<td id="S4.T1.10.10.10.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" rowspan="2"><span id="S4.T1.10.10.10.11.2.1" class="ltx_text">Modality</span></td>
<td id="S4.T1.10.10.10.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" colspan="2"><span id="S4.T1.10.10.10.11.3.1" class="ltx_text ltx_font_italic">val</span></td>
<td id="S4.T1.10.10.10.11.4" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" colspan="7"><span id="S4.T1.10.10.10.11.4.1" class="ltx_text ltx_font_italic">test</span></td>
</tr>
<tr id="S4.T1.9.9.9.9" class="ltx_tr">
<td id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_center">mAP <math id="S4.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.2.2.2.2.2.1" class="ltx_text" style="background-color:#ECECEC;">NDS <math id="S4.T1.2.2.2.2.2.1.m1.1" class="ltx_Math" style="background-color:#ECECEC;" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.2.2.1.m1.1a"><mo mathbackground="#ECECEC" stretchy="false" id="S4.T1.2.2.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.2.1.m1.1b"><ci id="S4.T1.2.2.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S4.T1.3.3.3.3.3" class="ltx_td ltx_align_center">mAP <math id="S4.T1.3.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.3.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.3.3.m1.1.1" xref="S4.T1.3.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.4.4.4.4.4" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.4.4.4.4.4.1" class="ltx_text" style="background-color:#ECECEC;">NDS <math id="S4.T1.4.4.4.4.4.1.m1.1" class="ltx_Math" style="background-color:#ECECEC;" alttext="\uparrow" display="inline"><semantics id="S4.T1.4.4.4.4.4.1.m1.1a"><mo mathbackground="#ECECEC" stretchy="false" id="S4.T1.4.4.4.4.4.1.m1.1.1" xref="S4.T1.4.4.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.4.1.m1.1b"><ci id="S4.T1.4.4.4.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S4.T1.5.5.5.5.5" class="ltx_td ltx_align_center">mATE <math id="S4.T1.5.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.5.5.5.5.5.m1.1a"><mo stretchy="false" id="S4.T1.5.5.5.5.5.m1.1.1" xref="S4.T1.5.5.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.5.5.m1.1b"><ci id="S4.T1.5.5.5.5.5.m1.1.1.cmml" xref="S4.T1.5.5.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T1.6.6.6.6.6" class="ltx_td ltx_align_center">mASE <math id="S4.T1.6.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.6.6.6.6.6.m1.1a"><mo stretchy="false" id="S4.T1.6.6.6.6.6.m1.1.1" xref="S4.T1.6.6.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.6.6.m1.1b"><ci id="S4.T1.6.6.6.6.6.m1.1.1.cmml" xref="S4.T1.6.6.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T1.7.7.7.7.7" class="ltx_td ltx_align_center">mAOE <math id="S4.T1.7.7.7.7.7.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.7.7.7.7.7.m1.1a"><mo stretchy="false" id="S4.T1.7.7.7.7.7.m1.1.1" xref="S4.T1.7.7.7.7.7.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.7.7.m1.1b"><ci id="S4.T1.7.7.7.7.7.m1.1.1.cmml" xref="S4.T1.7.7.7.7.7.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.7.7.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T1.8.8.8.8.8" class="ltx_td ltx_align_center">mAVE<math id="S4.T1.8.8.8.8.8.m1.1" class="ltx_Math" alttext="~{}\downarrow" display="inline"><semantics id="S4.T1.8.8.8.8.8.m1.1a"><mo stretchy="false" id="S4.T1.8.8.8.8.8.m1.1.1" xref="S4.T1.8.8.8.8.8.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.8.8.m1.1b"><ci id="S4.T1.8.8.8.8.8.m1.1.1.cmml" xref="S4.T1.8.8.8.8.8.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.8.8.m1.1c">~{}\downarrow</annotation></semantics></math>
</td>
<td id="S4.T1.9.9.9.9.9" class="ltx_td ltx_align_center">mAAE<math id="S4.T1.9.9.9.9.9.m1.1" class="ltx_Math" alttext="~{}\downarrow" display="inline"><semantics id="S4.T1.9.9.9.9.9.m1.1a"><mo stretchy="false" id="S4.T1.9.9.9.9.9.m1.1.1" xref="S4.T1.9.9.9.9.9.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.9.9.9.m1.1b"><ci id="S4.T1.9.9.9.9.9.m1.1.1.cmml" xref="S4.T1.9.9.9.9.9.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.9.9.9.m1.1c">~{}\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.10.10.10.12" class="ltx_tr">
<td id="S4.T1.10.10.10.12.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">DETR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</td>
<td id="S4.T1.10.10.10.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">C</td>
<td id="S4.T1.10.10.10.12.3" class="ltx_td ltx_align_center ltx_border_t">34.9</td>
<td id="S4.T1.10.10.10.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.12.4.1" class="ltx_text" style="background-color:#ECECEC;">43.4</span></td>
<td id="S4.T1.10.10.10.12.5" class="ltx_td ltx_align_center ltx_border_t">41.2</td>
<td id="S4.T1.10.10.10.12.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.12.6.1" class="ltx_text" style="background-color:#ECECEC;">47.9</span></td>
<td id="S4.T1.10.10.10.12.7" class="ltx_td ltx_align_center ltx_border_t">0.641</td>
<td id="S4.T1.10.10.10.12.8" class="ltx_td ltx_align_center ltx_border_t">0.255</td>
<td id="S4.T1.10.10.10.12.9" class="ltx_td ltx_align_center ltx_border_t">0.394</td>
<td id="S4.T1.10.10.10.12.10" class="ltx_td ltx_align_center ltx_border_t">0.845</td>
<td id="S4.T1.10.10.10.12.11" class="ltx_td ltx_align_center ltx_border_t">0.133</td>
</tr>
<tr id="S4.T1.10.10.10.13" class="ltx_tr">
<td id="S4.T1.10.10.10.13.1" class="ltx_td ltx_align_left ltx_border_r">BEVDet4D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T1.10.10.10.13.2" class="ltx_td ltx_align_center ltx_border_r">C</td>
<td id="S4.T1.10.10.10.13.3" class="ltx_td ltx_align_center">42.1</td>
<td id="S4.T1.10.10.10.13.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.13.4.1" class="ltx_text" style="background-color:#ECECEC;">54.5</span></td>
<td id="S4.T1.10.10.10.13.5" class="ltx_td ltx_align_center">45.1</td>
<td id="S4.T1.10.10.10.13.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.13.6.1" class="ltx_text" style="background-color:#ECECEC;">56.9</span></td>
<td id="S4.T1.10.10.10.13.7" class="ltx_td ltx_align_center">0.511</td>
<td id="S4.T1.10.10.10.13.8" class="ltx_td ltx_align_center">0.241</td>
<td id="S4.T1.10.10.10.13.9" class="ltx_td ltx_align_center">0.386</td>
<td id="S4.T1.10.10.10.13.10" class="ltx_td ltx_align_center">0.301</td>
<td id="S4.T1.10.10.10.13.11" class="ltx_td ltx_align_center">0.121</td>
</tr>
<tr id="S4.T1.10.10.10.14" class="ltx_tr">
<td id="S4.T1.10.10.10.14.1" class="ltx_td ltx_align_left ltx_border_r">BEVFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S4.T1.10.10.10.14.2" class="ltx_td ltx_align_center ltx_border_r">C</td>
<td id="S4.T1.10.10.10.14.3" class="ltx_td ltx_align_center">41.6</td>
<td id="S4.T1.10.10.10.14.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.14.4.1" class="ltx_text" style="background-color:#ECECEC;">51.7</span></td>
<td id="S4.T1.10.10.10.14.5" class="ltx_td ltx_align_center">48.1</td>
<td id="S4.T1.10.10.10.14.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.14.6.1" class="ltx_text" style="background-color:#ECECEC;">56.9</span></td>
<td id="S4.T1.10.10.10.14.7" class="ltx_td ltx_align_center">0.582</td>
<td id="S4.T1.10.10.10.14.8" class="ltx_td ltx_align_center">0.256</td>
<td id="S4.T1.10.10.10.14.9" class="ltx_td ltx_align_center">0.375</td>
<td id="S4.T1.10.10.10.14.10" class="ltx_td ltx_align_center">0.378</td>
<td id="S4.T1.10.10.10.14.11" class="ltx_td ltx_align_center">0.126</td>
</tr>
<tr id="S4.T1.10.10.10.15" class="ltx_tr">
<td id="S4.T1.10.10.10.15.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</td>
<td id="S4.T1.10.10.10.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L</td>
<td id="S4.T1.10.10.10.15.3" class="ltx_td ltx_align_center ltx_border_t">52.6</td>
<td id="S4.T1.10.10.10.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.15.4.1" class="ltx_text" style="background-color:#ECECEC;">63.0</span></td>
<td id="S4.T1.10.10.10.15.5" class="ltx_td ltx_align_center ltx_border_t">52.8</td>
<td id="S4.T1.10.10.10.15.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.15.6.1" class="ltx_text" style="background-color:#ECECEC;">63.3</span></td>
<td id="S4.T1.10.10.10.15.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.10.10.10.15.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.10.10.10.15.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.10.10.10.15.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.10.10.10.15.11" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T1.10.10.10.16" class="ltx_tr">
<td id="S4.T1.10.10.10.16.1" class="ltx_td ltx_align_left ltx_border_r">CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S4.T1.10.10.10.16.2" class="ltx_td ltx_align_center ltx_border_r">L</td>
<td id="S4.T1.10.10.10.16.3" class="ltx_td ltx_align_center">59.6</td>
<td id="S4.T1.10.10.10.16.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.16.4.1" class="ltx_text" style="background-color:#ECECEC;">66.8</span></td>
<td id="S4.T1.10.10.10.16.5" class="ltx_td ltx_align_center">60.3</td>
<td id="S4.T1.10.10.10.16.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.16.6.1" class="ltx_text" style="background-color:#ECECEC;">67.3</span></td>
<td id="S4.T1.10.10.10.16.7" class="ltx_td ltx_align_center">0.262</td>
<td id="S4.T1.10.10.10.16.8" class="ltx_td ltx_align_center">0.239</td>
<td id="S4.T1.10.10.10.16.9" class="ltx_td ltx_align_center">0.361</td>
<td id="S4.T1.10.10.10.16.10" class="ltx_td ltx_align_center">0.288</td>
<td id="S4.T1.10.10.10.16.11" class="ltx_td ltx_align_center">0.136</td>
</tr>
<tr id="S4.T1.10.10.10.10" class="ltx_tr">
<td id="S4.T1.10.10.10.10.1" class="ltx_td ltx_align_left ltx_border_r">FSD<sup id="S4.T1.10.10.10.10.1.1" class="ltx_sup">∗</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S4.T1.10.10.10.10.2" class="ltx_td ltx_align_center ltx_border_r">L</td>
<td id="S4.T1.10.10.10.10.3" class="ltx_td ltx_align_center">62.5</td>
<td id="S4.T1.10.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.10.4.1" class="ltx_text" style="background-color:#ECECEC;">68.7</span></td>
<td id="S4.T1.10.10.10.10.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.10.10.10.10.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.10.6.1" class="ltx_text" style="background-color:#ECECEC;">-</span></td>
<td id="S4.T1.10.10.10.10.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.10.10.10.10.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.10.10.10.10.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.10.10.10.10.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.10.10.10.10.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.10.10.10.17" class="ltx_tr">
<td id="S4.T1.10.10.10.17.1" class="ltx_td ltx_align_left ltx_border_r">VoxelNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S4.T1.10.10.10.17.2" class="ltx_td ltx_align_center ltx_border_r">L</td>
<td id="S4.T1.10.10.10.17.3" class="ltx_td ltx_align_center">63.5</td>
<td id="S4.T1.10.10.10.17.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.17.4.1" class="ltx_text" style="background-color:#ECECEC;">68.7</span></td>
<td id="S4.T1.10.10.10.17.5" class="ltx_td ltx_align_center">64.5</td>
<td id="S4.T1.10.10.10.17.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.17.6.1" class="ltx_text" style="background-color:#ECECEC;">70.0</span></td>
<td id="S4.T1.10.10.10.17.7" class="ltx_td ltx_align_center">0.268</td>
<td id="S4.T1.10.10.10.17.8" class="ltx_td ltx_align_center">0.238</td>
<td id="S4.T1.10.10.10.17.9" class="ltx_td ltx_align_center">0.377</td>
<td id="S4.T1.10.10.10.17.10" class="ltx_td ltx_align_center">0.219</td>
<td id="S4.T1.10.10.10.17.11" class="ltx_td ltx_align_center">0.127</td>
</tr>
<tr id="S4.T1.10.10.10.18" class="ltx_tr">
<td id="S4.T1.10.10.10.18.1" class="ltx_td ltx_align_left ltx_border_r">TransFusion-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S4.T1.10.10.10.18.2" class="ltx_td ltx_align_center ltx_border_r">L</td>
<td id="S4.T1.10.10.10.18.3" class="ltx_td ltx_align_center">64.9</td>
<td id="S4.T1.10.10.10.18.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.18.4.1" class="ltx_text" style="background-color:#ECECEC;">69.9</span></td>
<td id="S4.T1.10.10.10.18.5" class="ltx_td ltx_align_center">65.5</td>
<td id="S4.T1.10.10.10.18.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.18.6.1" class="ltx_text" style="background-color:#ECECEC;">70.2</span></td>
<td id="S4.T1.10.10.10.18.7" class="ltx_td ltx_align_center">0.256</td>
<td id="S4.T1.10.10.10.18.8" class="ltx_td ltx_align_center">0.240</td>
<td id="S4.T1.10.10.10.18.9" class="ltx_td ltx_align_center">0.351</td>
<td id="S4.T1.10.10.10.18.10" class="ltx_td ltx_align_center">0.278</td>
<td id="S4.T1.10.10.10.18.11" class="ltx_td ltx_align_center">0.129</td>
</tr>
<tr id="S4.T1.10.10.10.19" class="ltx_tr">
<td id="S4.T1.10.10.10.19.1" class="ltx_td ltx_align_left ltx_border_r">LargeKernel3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S4.T1.10.10.10.19.2" class="ltx_td ltx_align_center ltx_border_r">L</td>
<td id="S4.T1.10.10.10.19.3" class="ltx_td ltx_align_center">63.3</td>
<td id="S4.T1.10.10.10.19.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.19.4.1" class="ltx_text" style="background-color:#ECECEC;">69.1</span></td>
<td id="S4.T1.10.10.10.19.5" class="ltx_td ltx_align_center">65.3</td>
<td id="S4.T1.10.10.10.19.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.19.6.1" class="ltx_text" style="background-color:#ECECEC;">70.5</span></td>
<td id="S4.T1.10.10.10.19.7" class="ltx_td ltx_align_center">0.261</td>
<td id="S4.T1.10.10.10.19.8" class="ltx_td ltx_align_center">0.236</td>
<td id="S4.T1.10.10.10.19.9" class="ltx_td ltx_align_center">0.319</td>
<td id="S4.T1.10.10.10.19.10" class="ltx_td ltx_align_center">0.268</td>
<td id="S4.T1.10.10.10.19.11" class="ltx_td ltx_align_center">0.133</td>
</tr>
<tr id="S4.T1.10.10.10.20" class="ltx_tr">
<td id="S4.T1.10.10.10.20.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">FUTR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S4.T1.10.10.10.20.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">C+L</td>
<td id="S4.T1.10.10.10.20.3" class="ltx_td ltx_align_center ltx_border_t">64.5</td>
<td id="S4.T1.10.10.10.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.20.4.1" class="ltx_text" style="background-color:#ECECEC;">68.3</span></td>
<td id="S4.T1.10.10.10.20.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.10.10.10.20.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.20.6.1" class="ltx_text" style="background-color:#ECECEC;">-</span></td>
<td id="S4.T1.10.10.10.20.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.10.10.10.20.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.10.10.10.20.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.10.10.10.20.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.10.10.10.20.11" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T1.10.10.10.21" class="ltx_tr">
<td id="S4.T1.10.10.10.21.1" class="ltx_td ltx_align_left ltx_border_r">MVP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S4.T1.10.10.10.21.2" class="ltx_td ltx_align_center ltx_border_r">C+L</td>
<td id="S4.T1.10.10.10.21.3" class="ltx_td ltx_align_center">67.1</td>
<td id="S4.T1.10.10.10.21.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.21.4.1" class="ltx_text" style="background-color:#ECECEC;">70.8</span></td>
<td id="S4.T1.10.10.10.21.5" class="ltx_td ltx_align_center">66.4</td>
<td id="S4.T1.10.10.10.21.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.21.6.1" class="ltx_text" style="background-color:#ECECEC;">70.5</span></td>
<td id="S4.T1.10.10.10.21.7" class="ltx_td ltx_align_center">0.263</td>
<td id="S4.T1.10.10.10.21.8" class="ltx_td ltx_align_center">0.238</td>
<td id="S4.T1.10.10.10.21.9" class="ltx_td ltx_align_center">0.321</td>
<td id="S4.T1.10.10.10.21.10" class="ltx_td ltx_align_center">0.313</td>
<td id="S4.T1.10.10.10.21.11" class="ltx_td ltx_align_center">0.134</td>
</tr>
<tr id="S4.T1.10.10.10.22" class="ltx_tr">
<td id="S4.T1.10.10.10.22.1" class="ltx_td ltx_align_left ltx_border_r">PointAugmenting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T1.10.10.10.22.2" class="ltx_td ltx_align_center ltx_border_r">C+L</td>
<td id="S4.T1.10.10.10.22.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.10.10.10.22.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.22.4.1" class="ltx_text" style="background-color:#ECECEC;">-</span></td>
<td id="S4.T1.10.10.10.22.5" class="ltx_td ltx_align_center">66.8</td>
<td id="S4.T1.10.10.10.22.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.22.6.1" class="ltx_text" style="background-color:#ECECEC;">71.0</span></td>
<td id="S4.T1.10.10.10.22.7" class="ltx_td ltx_align_center">0.254</td>
<td id="S4.T1.10.10.10.22.8" class="ltx_td ltx_align_center">0.236</td>
<td id="S4.T1.10.10.10.22.9" class="ltx_td ltx_align_center">0.362</td>
<td id="S4.T1.10.10.10.22.10" class="ltx_td ltx_align_center">0.266</td>
<td id="S4.T1.10.10.10.22.11" class="ltx_td ltx_align_center">0.123</td>
</tr>
<tr id="S4.T1.10.10.10.23" class="ltx_tr">
<td id="S4.T1.10.10.10.23.1" class="ltx_td ltx_align_left ltx_border_r">TransFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S4.T1.10.10.10.23.2" class="ltx_td ltx_align_center ltx_border_r">C+L</td>
<td id="S4.T1.10.10.10.23.3" class="ltx_td ltx_align_center">67.5</td>
<td id="S4.T1.10.10.10.23.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.23.4.1" class="ltx_text" style="background-color:#ECECEC;">71.3</span></td>
<td id="S4.T1.10.10.10.23.5" class="ltx_td ltx_align_center">68.9</td>
<td id="S4.T1.10.10.10.23.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.23.6.1" class="ltx_text" style="background-color:#ECECEC;">71.6</span></td>
<td id="S4.T1.10.10.10.23.7" class="ltx_td ltx_align_center">0.259</td>
<td id="S4.T1.10.10.10.23.8" class="ltx_td ltx_align_center">0.243</td>
<td id="S4.T1.10.10.10.23.9" class="ltx_td ltx_align_center">0.359</td>
<td id="S4.T1.10.10.10.23.10" class="ltx_td ltx_align_center">0.288</td>
<td id="S4.T1.10.10.10.23.11" class="ltx_td ltx_align_center">0.127</td>
</tr>
<tr id="S4.T1.10.10.10.24" class="ltx_tr">
<td id="S4.T1.10.10.10.24.1" class="ltx_td ltx_align_left ltx_border_r">BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S4.T1.10.10.10.24.2" class="ltx_td ltx_align_center ltx_border_r">C+L</td>
<td id="S4.T1.10.10.10.24.3" class="ltx_td ltx_align_center">67.9</td>
<td id="S4.T1.10.10.10.24.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.24.4.1" class="ltx_text" style="background-color:#ECECEC;">71.0</span></td>
<td id="S4.T1.10.10.10.24.5" class="ltx_td ltx_align_center">69.2</td>
<td id="S4.T1.10.10.10.24.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.24.6.1" class="ltx_text" style="background-color:#ECECEC;">71.8</span></td>
<td id="S4.T1.10.10.10.24.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.10.10.10.24.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.10.10.10.24.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.10.10.10.24.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.10.10.10.24.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.10.10.10.25" class="ltx_tr">
<td id="S4.T1.10.10.10.25.1" class="ltx_td ltx_align_left ltx_border_r">BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S4.T1.10.10.10.25.2" class="ltx_td ltx_align_center ltx_border_r">C+L</td>
<td id="S4.T1.10.10.10.25.3" class="ltx_td ltx_align_center">68.5</td>
<td id="S4.T1.10.10.10.25.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.25.4.1" class="ltx_text" style="background-color:#ECECEC;">71.4</span></td>
<td id="S4.T1.10.10.10.25.5" class="ltx_td ltx_align_center">70.2</td>
<td id="S4.T1.10.10.10.25.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.25.6.1" class="ltx_text" style="background-color:#ECECEC;">72.9</span></td>
<td id="S4.T1.10.10.10.25.7" class="ltx_td ltx_align_center">0.261</td>
<td id="S4.T1.10.10.10.25.8" class="ltx_td ltx_align_center">0.239</td>
<td id="S4.T1.10.10.10.25.9" class="ltx_td ltx_align_center">0.329</td>
<td id="S4.T1.10.10.10.25.10" class="ltx_td ltx_align_center">0.260</td>
<td id="S4.T1.10.10.10.25.11" class="ltx_td ltx_align_center">0.134</td>
</tr>
<tr id="S4.T1.10.10.10.26" class="ltx_tr">
<td id="S4.T1.10.10.10.26.1" class="ltx_td ltx_align_left ltx_border_r">DeepInteraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S4.T1.10.10.10.26.2" class="ltx_td ltx_align_center ltx_border_r">C+L</td>
<td id="S4.T1.10.10.10.26.3" class="ltx_td ltx_align_center">69.9</td>
<td id="S4.T1.10.10.10.26.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.26.4.1" class="ltx_text" style="background-color:#ECECEC;">72.6</span></td>
<td id="S4.T1.10.10.10.26.5" class="ltx_td ltx_align_center"><span id="S4.T1.10.10.10.26.5.1" class="ltx_text ltx_font_bold">70.8</span></td>
<td id="S4.T1.10.10.10.26.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.26.6.1" class="ltx_text" style="background-color:#ECECEC;">73.4</span></td>
<td id="S4.T1.10.10.10.26.7" class="ltx_td ltx_align_center">0.257</td>
<td id="S4.T1.10.10.10.26.8" class="ltx_td ltx_align_center">0.240</td>
<td id="S4.T1.10.10.10.26.9" class="ltx_td ltx_align_center">0.325</td>
<td id="S4.T1.10.10.10.26.10" class="ltx_td ltx_align_center">0.245</td>
<td id="S4.T1.10.10.10.26.11" class="ltx_td ltx_align_center">0.128</td>
</tr>
<tr id="S4.T1.10.10.10.27" class="ltx_tr">
<td id="S4.T1.10.10.10.27.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_r">FSF (Ours)</td>
<td id="S4.T1.10.10.10.27.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">C+L</td>
<td id="S4.T1.10.10.10.27.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b"><span id="S4.T1.10.10.10.27.3.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S4.T1.10.10.10.27.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.27.4.1" class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">72.7</span></td>
<td id="S4.T1.10.10.10.27.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">70.6</td>
<td id="S4.T1.10.10.10.27.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" style="background-color:#ECECEC;"><span id="S4.T1.10.10.10.27.6.1" class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">74.0</span></td>
<td id="S4.T1.10.10.10.27.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b"><span id="S4.T1.10.10.10.27.7.1" class="ltx_text ltx_font_bold">0.246</span></td>
<td id="S4.T1.10.10.10.27.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b"><span id="S4.T1.10.10.10.27.8.1" class="ltx_text ltx_font_bold">0.234</span></td>
<td id="S4.T1.10.10.10.27.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b"><span id="S4.T1.10.10.10.27.9.1" class="ltx_text ltx_font_bold">0.318</span></td>
<td id="S4.T1.10.10.10.27.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b"><span id="S4.T1.10.10.10.27.10.1" class="ltx_text ltx_font_bold">0.211</span></td>
<td id="S4.T1.10.10.10.27.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b"><span id="S4.T1.10.10.10.27.11.1" class="ltx_text ltx_font_bold">0.123</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Comparison with state-of-the-art methods on the nuScenes dataset.
We do not use test-time augmentation or model ensemble.
<sup id="S4.T1.15.1" class="ltx_sup">∗</sup>: Reimplemented by us.
We mark the official benchmark of nuScenes as <span id="S4.T1.16.2" class="ltx_text" style="background-color:#ECECEC;">Gray</span>.
C: camera modality.
L: LiDAR modality.
</figcaption>
</figure>
<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">3D Round</h4>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.1" class="ltx_p">We first give higher priority to the assignment in 3D space.
Particularly, for all queries, we first straightforwardly assign GTs to these queries by the query-in-box strategy in 3D space.
This round not only makes our method consistent with the original LiDAR-only FSD, but also assigns accurate labels to the major part of camera queries.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2304.12310/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The motivation of 3D/2D two-round assignment.
The center of a camera query is hard to fall into GT boxes as (a) shows.
However, it is easy to assign this camera query to the corresponding GT on the 2D plane as (b) demonstrates.</figcaption>
</figure>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">2D Round</h4>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">Complementary to the 3D round, we assign labels in 2D image space for another round.
This round is designed for those unassigned camera queries.
In practice, for each camera query, we have a 2D bounding box produced by 2D instance segmentation.
We calculate the IoU between this 2D box and the projected 3D GTs.
Then we follow the commonly-used “max IoU” strategy to assign labels.
Specifically, for each ground-truth (GT) box, the “max IoU” assignment method compares this GT box with all the camera query boxes.
If the IoU between a query box and this GT box is above a certain threshold, this camera query will be assigned to the GT.
Eventually, after these two rounds, those samples still not associated with any GTs are assigned as negative.</p>
</div>
<div id="S4.SS4.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p2.1" class="ltx_p">To summarize, in our framework, we assign labels to queries in two rounds.
The first one is to assign labels to the initially generated queries by query-in-box strategy.
The second one is to assign labels to reference boxes via 2D IoU.
For the Query Generation module and the Query Refinement module, we adopt the same two-round strategy for assignments.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Detection Head and Loss</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Our framework comprises a total of three heads.
These three heads are dedicated to generating LiDAR reference bounding boxes, camera reference bounding boxes, and final bounding boxes, respectively.
The structures of these heads are the same.
Each head is divided into two branches, namely regression branch and classification branch.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.12" class="ltx_p">The regression branch takes each query’s feature as input, outputs <math id="S4.SS5.p2.1.m1.1" class="ltx_math_unparsed" alttext="(\Delta x" display="inline"><semantics id="S4.SS5.p2.1.m1.1a"><mrow id="S4.SS5.p2.1.m1.1b"><mo stretchy="false" id="S4.SS5.p2.1.m1.1.1">(</mo><mi mathvariant="normal" id="S4.SS5.p2.1.m1.1.2">Δ</mi><mi id="S4.SS5.p2.1.m1.1.3">x</mi></mrow><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">(\Delta x</annotation></semantics></math>, <math id="S4.SS5.p2.2.m2.1" class="ltx_Math" alttext="\Delta y" display="inline"><semantics id="S4.SS5.p2.2.m2.1a"><mrow id="S4.SS5.p2.2.m2.1.1" xref="S4.SS5.p2.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.SS5.p2.2.m2.1.1.2" xref="S4.SS5.p2.2.m2.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p2.2.m2.1.1.1" xref="S4.SS5.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS5.p2.2.m2.1.1.3" xref="S4.SS5.p2.2.m2.1.1.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><apply id="S4.SS5.p2.2.m2.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1"><times id="S4.SS5.p2.2.m2.1.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1.1"></times><ci id="S4.SS5.p2.2.m2.1.1.2.cmml" xref="S4.SS5.p2.2.m2.1.1.2">Δ</ci><ci id="S4.SS5.p2.2.m2.1.1.3.cmml" xref="S4.SS5.p2.2.m2.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">\Delta y</annotation></semantics></math>, <math id="S4.SS5.p2.3.m3.1" class="ltx_Math" alttext="\Delta z" display="inline"><semantics id="S4.SS5.p2.3.m3.1a"><mrow id="S4.SS5.p2.3.m3.1.1" xref="S4.SS5.p2.3.m3.1.1.cmml"><mi mathvariant="normal" id="S4.SS5.p2.3.m3.1.1.2" xref="S4.SS5.p2.3.m3.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p2.3.m3.1.1.1" xref="S4.SS5.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS5.p2.3.m3.1.1.3" xref="S4.SS5.p2.3.m3.1.1.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.3.m3.1b"><apply id="S4.SS5.p2.3.m3.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1"><times id="S4.SS5.p2.3.m3.1.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1.1"></times><ci id="S4.SS5.p2.3.m3.1.1.2.cmml" xref="S4.SS5.p2.3.m3.1.1.2">Δ</ci><ci id="S4.SS5.p2.3.m3.1.1.3.cmml" xref="S4.SS5.p2.3.m3.1.1.3">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.3.m3.1c">\Delta z</annotation></semantics></math>, <math id="S4.SS5.p2.4.m4.1" class="ltx_Math" alttext="\log{w}" display="inline"><semantics id="S4.SS5.p2.4.m4.1a"><mrow id="S4.SS5.p2.4.m4.1.1" xref="S4.SS5.p2.4.m4.1.1.cmml"><mi id="S4.SS5.p2.4.m4.1.1.1" xref="S4.SS5.p2.4.m4.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S4.SS5.p2.4.m4.1.1a" xref="S4.SS5.p2.4.m4.1.1.cmml">⁡</mo><mi id="S4.SS5.p2.4.m4.1.1.2" xref="S4.SS5.p2.4.m4.1.1.2.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.4.m4.1b"><apply id="S4.SS5.p2.4.m4.1.1.cmml" xref="S4.SS5.p2.4.m4.1.1"><log id="S4.SS5.p2.4.m4.1.1.1.cmml" xref="S4.SS5.p2.4.m4.1.1.1"></log><ci id="S4.SS5.p2.4.m4.1.1.2.cmml" xref="S4.SS5.p2.4.m4.1.1.2">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.4.m4.1c">\log{w}</annotation></semantics></math>, <math id="S4.SS5.p2.5.m5.1" class="ltx_Math" alttext="\log{l}" display="inline"><semantics id="S4.SS5.p2.5.m5.1a"><mrow id="S4.SS5.p2.5.m5.1.1" xref="S4.SS5.p2.5.m5.1.1.cmml"><mi id="S4.SS5.p2.5.m5.1.1.1" xref="S4.SS5.p2.5.m5.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S4.SS5.p2.5.m5.1.1a" xref="S4.SS5.p2.5.m5.1.1.cmml">⁡</mo><mi id="S4.SS5.p2.5.m5.1.1.2" xref="S4.SS5.p2.5.m5.1.1.2.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.5.m5.1b"><apply id="S4.SS5.p2.5.m5.1.1.cmml" xref="S4.SS5.p2.5.m5.1.1"><log id="S4.SS5.p2.5.m5.1.1.1.cmml" xref="S4.SS5.p2.5.m5.1.1.1"></log><ci id="S4.SS5.p2.5.m5.1.1.2.cmml" xref="S4.SS5.p2.5.m5.1.1.2">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.5.m5.1c">\log{l}</annotation></semantics></math>, <math id="S4.SS5.p2.6.m6.1" class="ltx_Math" alttext="\log{h}" display="inline"><semantics id="S4.SS5.p2.6.m6.1a"><mrow id="S4.SS5.p2.6.m6.1.1" xref="S4.SS5.p2.6.m6.1.1.cmml"><mi id="S4.SS5.p2.6.m6.1.1.1" xref="S4.SS5.p2.6.m6.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S4.SS5.p2.6.m6.1.1a" xref="S4.SS5.p2.6.m6.1.1.cmml">⁡</mo><mi id="S4.SS5.p2.6.m6.1.1.2" xref="S4.SS5.p2.6.m6.1.1.2.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.6.m6.1b"><apply id="S4.SS5.p2.6.m6.1.1.cmml" xref="S4.SS5.p2.6.m6.1.1"><log id="S4.SS5.p2.6.m6.1.1.1.cmml" xref="S4.SS5.p2.6.m6.1.1.1"></log><ci id="S4.SS5.p2.6.m6.1.1.2.cmml" xref="S4.SS5.p2.6.m6.1.1.2">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.6.m6.1c">\log{h}</annotation></semantics></math>, <math id="S4.SS5.p2.7.m7.1" class="ltx_Math" alttext="\sin{r_{y}}" display="inline"><semantics id="S4.SS5.p2.7.m7.1a"><mrow id="S4.SS5.p2.7.m7.1.1" xref="S4.SS5.p2.7.m7.1.1.cmml"><mi id="S4.SS5.p2.7.m7.1.1.1" xref="S4.SS5.p2.7.m7.1.1.1.cmml">sin</mi><mo lspace="0.167em" id="S4.SS5.p2.7.m7.1.1a" xref="S4.SS5.p2.7.m7.1.1.cmml">⁡</mo><msub id="S4.SS5.p2.7.m7.1.1.2" xref="S4.SS5.p2.7.m7.1.1.2.cmml"><mi id="S4.SS5.p2.7.m7.1.1.2.2" xref="S4.SS5.p2.7.m7.1.1.2.2.cmml">r</mi><mi id="S4.SS5.p2.7.m7.1.1.2.3" xref="S4.SS5.p2.7.m7.1.1.2.3.cmml">y</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.7.m7.1b"><apply id="S4.SS5.p2.7.m7.1.1.cmml" xref="S4.SS5.p2.7.m7.1.1"><sin id="S4.SS5.p2.7.m7.1.1.1.cmml" xref="S4.SS5.p2.7.m7.1.1.1"></sin><apply id="S4.SS5.p2.7.m7.1.1.2.cmml" xref="S4.SS5.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S4.SS5.p2.7.m7.1.1.2.1.cmml" xref="S4.SS5.p2.7.m7.1.1.2">subscript</csymbol><ci id="S4.SS5.p2.7.m7.1.1.2.2.cmml" xref="S4.SS5.p2.7.m7.1.1.2.2">𝑟</ci><ci id="S4.SS5.p2.7.m7.1.1.2.3.cmml" xref="S4.SS5.p2.7.m7.1.1.2.3">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.7.m7.1c">\sin{r_{y}}</annotation></semantics></math>, <math id="S4.SS5.p2.8.m8.1" class="ltx_math_unparsed" alttext="\cos{r_{y}})" display="inline"><semantics id="S4.SS5.p2.8.m8.1a"><mrow id="S4.SS5.p2.8.m8.1b"><mi id="S4.SS5.p2.8.m8.1.1">cos</mi><msub id="S4.SS5.p2.8.m8.1.2"><mi id="S4.SS5.p2.8.m8.1.2.2">r</mi><mi id="S4.SS5.p2.8.m8.1.2.3">y</mi></msub><mo stretchy="false" id="S4.SS5.p2.8.m8.1.3">)</mo></mrow><annotation encoding="application/x-tex" id="S4.SS5.p2.8.m8.1c">\cos{r_{y}})</annotation></semantics></math>.
<math id="S4.SS5.p2.9.m9.3" class="ltx_Math" alttext="(\Delta x,\Delta y,\Delta z)" display="inline"><semantics id="S4.SS5.p2.9.m9.3a"><mrow id="S4.SS5.p2.9.m9.3.3.3" xref="S4.SS5.p2.9.m9.3.3.4.cmml"><mo stretchy="false" id="S4.SS5.p2.9.m9.3.3.3.4" xref="S4.SS5.p2.9.m9.3.3.4.cmml">(</mo><mrow id="S4.SS5.p2.9.m9.1.1.1.1" xref="S4.SS5.p2.9.m9.1.1.1.1.cmml"><mi mathvariant="normal" id="S4.SS5.p2.9.m9.1.1.1.1.2" xref="S4.SS5.p2.9.m9.1.1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p2.9.m9.1.1.1.1.1" xref="S4.SS5.p2.9.m9.1.1.1.1.1.cmml">​</mo><mi id="S4.SS5.p2.9.m9.1.1.1.1.3" xref="S4.SS5.p2.9.m9.1.1.1.1.3.cmml">x</mi></mrow><mo id="S4.SS5.p2.9.m9.3.3.3.5" xref="S4.SS5.p2.9.m9.3.3.4.cmml">,</mo><mrow id="S4.SS5.p2.9.m9.2.2.2.2" xref="S4.SS5.p2.9.m9.2.2.2.2.cmml"><mi mathvariant="normal" id="S4.SS5.p2.9.m9.2.2.2.2.2" xref="S4.SS5.p2.9.m9.2.2.2.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p2.9.m9.2.2.2.2.1" xref="S4.SS5.p2.9.m9.2.2.2.2.1.cmml">​</mo><mi id="S4.SS5.p2.9.m9.2.2.2.2.3" xref="S4.SS5.p2.9.m9.2.2.2.2.3.cmml">y</mi></mrow><mo id="S4.SS5.p2.9.m9.3.3.3.6" xref="S4.SS5.p2.9.m9.3.3.4.cmml">,</mo><mrow id="S4.SS5.p2.9.m9.3.3.3.3" xref="S4.SS5.p2.9.m9.3.3.3.3.cmml"><mi mathvariant="normal" id="S4.SS5.p2.9.m9.3.3.3.3.2" xref="S4.SS5.p2.9.m9.3.3.3.3.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p2.9.m9.3.3.3.3.1" xref="S4.SS5.p2.9.m9.3.3.3.3.1.cmml">​</mo><mi id="S4.SS5.p2.9.m9.3.3.3.3.3" xref="S4.SS5.p2.9.m9.3.3.3.3.3.cmml">z</mi></mrow><mo stretchy="false" id="S4.SS5.p2.9.m9.3.3.3.7" xref="S4.SS5.p2.9.m9.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.9.m9.3b"><vector id="S4.SS5.p2.9.m9.3.3.4.cmml" xref="S4.SS5.p2.9.m9.3.3.3"><apply id="S4.SS5.p2.9.m9.1.1.1.1.cmml" xref="S4.SS5.p2.9.m9.1.1.1.1"><times id="S4.SS5.p2.9.m9.1.1.1.1.1.cmml" xref="S4.SS5.p2.9.m9.1.1.1.1.1"></times><ci id="S4.SS5.p2.9.m9.1.1.1.1.2.cmml" xref="S4.SS5.p2.9.m9.1.1.1.1.2">Δ</ci><ci id="S4.SS5.p2.9.m9.1.1.1.1.3.cmml" xref="S4.SS5.p2.9.m9.1.1.1.1.3">𝑥</ci></apply><apply id="S4.SS5.p2.9.m9.2.2.2.2.cmml" xref="S4.SS5.p2.9.m9.2.2.2.2"><times id="S4.SS5.p2.9.m9.2.2.2.2.1.cmml" xref="S4.SS5.p2.9.m9.2.2.2.2.1"></times><ci id="S4.SS5.p2.9.m9.2.2.2.2.2.cmml" xref="S4.SS5.p2.9.m9.2.2.2.2.2">Δ</ci><ci id="S4.SS5.p2.9.m9.2.2.2.2.3.cmml" xref="S4.SS5.p2.9.m9.2.2.2.2.3">𝑦</ci></apply><apply id="S4.SS5.p2.9.m9.3.3.3.3.cmml" xref="S4.SS5.p2.9.m9.3.3.3.3"><times id="S4.SS5.p2.9.m9.3.3.3.3.1.cmml" xref="S4.SS5.p2.9.m9.3.3.3.3.1"></times><ci id="S4.SS5.p2.9.m9.3.3.3.3.2.cmml" xref="S4.SS5.p2.9.m9.3.3.3.3.2">Δ</ci><ci id="S4.SS5.p2.9.m9.3.3.3.3.3.cmml" xref="S4.SS5.p2.9.m9.3.3.3.3.3">𝑧</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.9.m9.3c">(\Delta x,\Delta y,\Delta z)</annotation></semantics></math> means the predicted offset from query’s center.
<math id="S4.SS5.p2.10.m10.3" class="ltx_Math" alttext="(w,l,h)" display="inline"><semantics id="S4.SS5.p2.10.m10.3a"><mrow id="S4.SS5.p2.10.m10.3.4.2" xref="S4.SS5.p2.10.m10.3.4.1.cmml"><mo stretchy="false" id="S4.SS5.p2.10.m10.3.4.2.1" xref="S4.SS5.p2.10.m10.3.4.1.cmml">(</mo><mi id="S4.SS5.p2.10.m10.1.1" xref="S4.SS5.p2.10.m10.1.1.cmml">w</mi><mo id="S4.SS5.p2.10.m10.3.4.2.2" xref="S4.SS5.p2.10.m10.3.4.1.cmml">,</mo><mi id="S4.SS5.p2.10.m10.2.2" xref="S4.SS5.p2.10.m10.2.2.cmml">l</mi><mo id="S4.SS5.p2.10.m10.3.4.2.3" xref="S4.SS5.p2.10.m10.3.4.1.cmml">,</mo><mi id="S4.SS5.p2.10.m10.3.3" xref="S4.SS5.p2.10.m10.3.3.cmml">h</mi><mo stretchy="false" id="S4.SS5.p2.10.m10.3.4.2.4" xref="S4.SS5.p2.10.m10.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.10.m10.3b"><vector id="S4.SS5.p2.10.m10.3.4.1.cmml" xref="S4.SS5.p2.10.m10.3.4.2"><ci id="S4.SS5.p2.10.m10.1.1.cmml" xref="S4.SS5.p2.10.m10.1.1">𝑤</ci><ci id="S4.SS5.p2.10.m10.2.2.cmml" xref="S4.SS5.p2.10.m10.2.2">𝑙</ci><ci id="S4.SS5.p2.10.m10.3.3.cmml" xref="S4.SS5.p2.10.m10.3.3">ℎ</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.10.m10.3c">(w,l,h)</annotation></semantics></math> means the dimension of the predicted box.
<math id="S4.SS5.p2.11.m11.1" class="ltx_Math" alttext="r_{y}" display="inline"><semantics id="S4.SS5.p2.11.m11.1a"><msub id="S4.SS5.p2.11.m11.1.1" xref="S4.SS5.p2.11.m11.1.1.cmml"><mi id="S4.SS5.p2.11.m11.1.1.2" xref="S4.SS5.p2.11.m11.1.1.2.cmml">r</mi><mi id="S4.SS5.p2.11.m11.1.1.3" xref="S4.SS5.p2.11.m11.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.11.m11.1b"><apply id="S4.SS5.p2.11.m11.1.1.cmml" xref="S4.SS5.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS5.p2.11.m11.1.1.1.cmml" xref="S4.SS5.p2.11.m11.1.1">subscript</csymbol><ci id="S4.SS5.p2.11.m11.1.1.2.cmml" xref="S4.SS5.p2.11.m11.1.1.2">𝑟</ci><ci id="S4.SS5.p2.11.m11.1.1.3.cmml" xref="S4.SS5.p2.11.m11.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.11.m11.1c">r_{y}</annotation></semantics></math> is the heading angle in the yaw direction.
We use <math id="S4.SS5.p2.12.m12.1" class="ltx_Math" alttext="L1" display="inline"><semantics id="S4.SS5.p2.12.m12.1a"><mrow id="S4.SS5.p2.12.m12.1.1" xref="S4.SS5.p2.12.m12.1.1.cmml"><mi id="S4.SS5.p2.12.m12.1.1.2" xref="S4.SS5.p2.12.m12.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p2.12.m12.1.1.1" xref="S4.SS5.p2.12.m12.1.1.1.cmml">​</mo><mn id="S4.SS5.p2.12.m12.1.1.3" xref="S4.SS5.p2.12.m12.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.12.m12.1b"><apply id="S4.SS5.p2.12.m12.1.1.cmml" xref="S4.SS5.p2.12.m12.1.1"><times id="S4.SS5.p2.12.m12.1.1.1.cmml" xref="S4.SS5.p2.12.m12.1.1.1"></times><ci id="S4.SS5.p2.12.m12.1.1.2.cmml" xref="S4.SS5.p2.12.m12.1.1.2">𝐿</ci><cn type="integer" id="S4.SS5.p2.12.m12.1.1.3.cmml" xref="S4.SS5.p2.12.m12.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.12.m12.1c">L1</annotation></semantics></math> loss for regression branches and focal loss for classification branches, respectively.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.6" class="ltx_p">Formally, we have regression loss <math id="S4.SS5.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{L}^{Li}_{reg}" display="inline"><semantics id="S4.SS5.p3.1.m1.1a"><msubsup id="S4.SS5.p3.1.m1.1.1" xref="S4.SS5.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p3.1.m1.1.1.2.2" xref="S4.SS5.p3.1.m1.1.1.2.2.cmml">ℒ</mi><mrow id="S4.SS5.p3.1.m1.1.1.3" xref="S4.SS5.p3.1.m1.1.1.3.cmml"><mi id="S4.SS5.p3.1.m1.1.1.3.2" xref="S4.SS5.p3.1.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.1.m1.1.1.3.1" xref="S4.SS5.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.1.m1.1.1.3.3" xref="S4.SS5.p3.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.1.m1.1.1.3.1a" xref="S4.SS5.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.1.m1.1.1.3.4" xref="S4.SS5.p3.1.m1.1.1.3.4.cmml">g</mi></mrow><mrow id="S4.SS5.p3.1.m1.1.1.2.3" xref="S4.SS5.p3.1.m1.1.1.2.3.cmml"><mi id="S4.SS5.p3.1.m1.1.1.2.3.2" xref="S4.SS5.p3.1.m1.1.1.2.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.1.m1.1.1.2.3.1" xref="S4.SS5.p3.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p3.1.m1.1.1.2.3.3" xref="S4.SS5.p3.1.m1.1.1.2.3.3.cmml">i</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.1.m1.1b"><apply id="S4.SS5.p3.1.m1.1.1.cmml" xref="S4.SS5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.1.m1.1.1.1.cmml" xref="S4.SS5.p3.1.m1.1.1">subscript</csymbol><apply id="S4.SS5.p3.1.m1.1.1.2.cmml" xref="S4.SS5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.1.m1.1.1.2.1.cmml" xref="S4.SS5.p3.1.m1.1.1">superscript</csymbol><ci id="S4.SS5.p3.1.m1.1.1.2.2.cmml" xref="S4.SS5.p3.1.m1.1.1.2.2">ℒ</ci><apply id="S4.SS5.p3.1.m1.1.1.2.3.cmml" xref="S4.SS5.p3.1.m1.1.1.2.3"><times id="S4.SS5.p3.1.m1.1.1.2.3.1.cmml" xref="S4.SS5.p3.1.m1.1.1.2.3.1"></times><ci id="S4.SS5.p3.1.m1.1.1.2.3.2.cmml" xref="S4.SS5.p3.1.m1.1.1.2.3.2">𝐿</ci><ci id="S4.SS5.p3.1.m1.1.1.2.3.3.cmml" xref="S4.SS5.p3.1.m1.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S4.SS5.p3.1.m1.1.1.3.cmml" xref="S4.SS5.p3.1.m1.1.1.3"><times id="S4.SS5.p3.1.m1.1.1.3.1.cmml" xref="S4.SS5.p3.1.m1.1.1.3.1"></times><ci id="S4.SS5.p3.1.m1.1.1.3.2.cmml" xref="S4.SS5.p3.1.m1.1.1.3.2">𝑟</ci><ci id="S4.SS5.p3.1.m1.1.1.3.3.cmml" xref="S4.SS5.p3.1.m1.1.1.3.3">𝑒</ci><ci id="S4.SS5.p3.1.m1.1.1.3.4.cmml" xref="S4.SS5.p3.1.m1.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.1.m1.1c">\mathcal{L}^{Li}_{reg}</annotation></semantics></math>, <math id="S4.SS5.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{L}^{Cam}_{reg}" display="inline"><semantics id="S4.SS5.p3.2.m2.1a"><msubsup id="S4.SS5.p3.2.m2.1.1" xref="S4.SS5.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p3.2.m2.1.1.2.2" xref="S4.SS5.p3.2.m2.1.1.2.2.cmml">ℒ</mi><mrow id="S4.SS5.p3.2.m2.1.1.3" xref="S4.SS5.p3.2.m2.1.1.3.cmml"><mi id="S4.SS5.p3.2.m2.1.1.3.2" xref="S4.SS5.p3.2.m2.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.2.m2.1.1.3.1" xref="S4.SS5.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.2.m2.1.1.3.3" xref="S4.SS5.p3.2.m2.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.2.m2.1.1.3.1a" xref="S4.SS5.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.2.m2.1.1.3.4" xref="S4.SS5.p3.2.m2.1.1.3.4.cmml">g</mi></mrow><mrow id="S4.SS5.p3.2.m2.1.1.2.3" xref="S4.SS5.p3.2.m2.1.1.2.3.cmml"><mi id="S4.SS5.p3.2.m2.1.1.2.3.2" xref="S4.SS5.p3.2.m2.1.1.2.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.2.m2.1.1.2.3.1" xref="S4.SS5.p3.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p3.2.m2.1.1.2.3.3" xref="S4.SS5.p3.2.m2.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.2.m2.1.1.2.3.1a" xref="S4.SS5.p3.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p3.2.m2.1.1.2.3.4" xref="S4.SS5.p3.2.m2.1.1.2.3.4.cmml">m</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.2.m2.1b"><apply id="S4.SS5.p3.2.m2.1.1.cmml" xref="S4.SS5.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.2.m2.1.1.1.cmml" xref="S4.SS5.p3.2.m2.1.1">subscript</csymbol><apply id="S4.SS5.p3.2.m2.1.1.2.cmml" xref="S4.SS5.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.2.m2.1.1.2.1.cmml" xref="S4.SS5.p3.2.m2.1.1">superscript</csymbol><ci id="S4.SS5.p3.2.m2.1.1.2.2.cmml" xref="S4.SS5.p3.2.m2.1.1.2.2">ℒ</ci><apply id="S4.SS5.p3.2.m2.1.1.2.3.cmml" xref="S4.SS5.p3.2.m2.1.1.2.3"><times id="S4.SS5.p3.2.m2.1.1.2.3.1.cmml" xref="S4.SS5.p3.2.m2.1.1.2.3.1"></times><ci id="S4.SS5.p3.2.m2.1.1.2.3.2.cmml" xref="S4.SS5.p3.2.m2.1.1.2.3.2">𝐶</ci><ci id="S4.SS5.p3.2.m2.1.1.2.3.3.cmml" xref="S4.SS5.p3.2.m2.1.1.2.3.3">𝑎</ci><ci id="S4.SS5.p3.2.m2.1.1.2.3.4.cmml" xref="S4.SS5.p3.2.m2.1.1.2.3.4">𝑚</ci></apply></apply><apply id="S4.SS5.p3.2.m2.1.1.3.cmml" xref="S4.SS5.p3.2.m2.1.1.3"><times id="S4.SS5.p3.2.m2.1.1.3.1.cmml" xref="S4.SS5.p3.2.m2.1.1.3.1"></times><ci id="S4.SS5.p3.2.m2.1.1.3.2.cmml" xref="S4.SS5.p3.2.m2.1.1.3.2">𝑟</ci><ci id="S4.SS5.p3.2.m2.1.1.3.3.cmml" xref="S4.SS5.p3.2.m2.1.1.3.3">𝑒</ci><ci id="S4.SS5.p3.2.m2.1.1.3.4.cmml" xref="S4.SS5.p3.2.m2.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.2.m2.1c">\mathcal{L}^{Cam}_{reg}</annotation></semantics></math>, <math id="S4.SS5.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{L}^{Ref}_{reg}" display="inline"><semantics id="S4.SS5.p3.3.m3.1a"><msubsup id="S4.SS5.p3.3.m3.1.1" xref="S4.SS5.p3.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p3.3.m3.1.1.2.2" xref="S4.SS5.p3.3.m3.1.1.2.2.cmml">ℒ</mi><mrow id="S4.SS5.p3.3.m3.1.1.3" xref="S4.SS5.p3.3.m3.1.1.3.cmml"><mi id="S4.SS5.p3.3.m3.1.1.3.2" xref="S4.SS5.p3.3.m3.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.3.m3.1.1.3.1" xref="S4.SS5.p3.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.3.m3.1.1.3.3" xref="S4.SS5.p3.3.m3.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.3.m3.1.1.3.1a" xref="S4.SS5.p3.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.3.m3.1.1.3.4" xref="S4.SS5.p3.3.m3.1.1.3.4.cmml">g</mi></mrow><mrow id="S4.SS5.p3.3.m3.1.1.2.3" xref="S4.SS5.p3.3.m3.1.1.2.3.cmml"><mi id="S4.SS5.p3.3.m3.1.1.2.3.2" xref="S4.SS5.p3.3.m3.1.1.2.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.3.m3.1.1.2.3.1" xref="S4.SS5.p3.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p3.3.m3.1.1.2.3.3" xref="S4.SS5.p3.3.m3.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.3.m3.1.1.2.3.1a" xref="S4.SS5.p3.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p3.3.m3.1.1.2.3.4" xref="S4.SS5.p3.3.m3.1.1.2.3.4.cmml">f</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.3.m3.1b"><apply id="S4.SS5.p3.3.m3.1.1.cmml" xref="S4.SS5.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.3.m3.1.1.1.cmml" xref="S4.SS5.p3.3.m3.1.1">subscript</csymbol><apply id="S4.SS5.p3.3.m3.1.1.2.cmml" xref="S4.SS5.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.3.m3.1.1.2.1.cmml" xref="S4.SS5.p3.3.m3.1.1">superscript</csymbol><ci id="S4.SS5.p3.3.m3.1.1.2.2.cmml" xref="S4.SS5.p3.3.m3.1.1.2.2">ℒ</ci><apply id="S4.SS5.p3.3.m3.1.1.2.3.cmml" xref="S4.SS5.p3.3.m3.1.1.2.3"><times id="S4.SS5.p3.3.m3.1.1.2.3.1.cmml" xref="S4.SS5.p3.3.m3.1.1.2.3.1"></times><ci id="S4.SS5.p3.3.m3.1.1.2.3.2.cmml" xref="S4.SS5.p3.3.m3.1.1.2.3.2">𝑅</ci><ci id="S4.SS5.p3.3.m3.1.1.2.3.3.cmml" xref="S4.SS5.p3.3.m3.1.1.2.3.3">𝑒</ci><ci id="S4.SS5.p3.3.m3.1.1.2.3.4.cmml" xref="S4.SS5.p3.3.m3.1.1.2.3.4">𝑓</ci></apply></apply><apply id="S4.SS5.p3.3.m3.1.1.3.cmml" xref="S4.SS5.p3.3.m3.1.1.3"><times id="S4.SS5.p3.3.m3.1.1.3.1.cmml" xref="S4.SS5.p3.3.m3.1.1.3.1"></times><ci id="S4.SS5.p3.3.m3.1.1.3.2.cmml" xref="S4.SS5.p3.3.m3.1.1.3.2">𝑟</ci><ci id="S4.SS5.p3.3.m3.1.1.3.3.cmml" xref="S4.SS5.p3.3.m3.1.1.3.3">𝑒</ci><ci id="S4.SS5.p3.3.m3.1.1.3.4.cmml" xref="S4.SS5.p3.3.m3.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.3.m3.1c">\mathcal{L}^{Ref}_{reg}</annotation></semantics></math> and classification loss
<math id="S4.SS5.p3.4.m4.1" class="ltx_Math" alttext="\mathcal{L}^{Li}_{cls}" display="inline"><semantics id="S4.SS5.p3.4.m4.1a"><msubsup id="S4.SS5.p3.4.m4.1.1" xref="S4.SS5.p3.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p3.4.m4.1.1.2.2" xref="S4.SS5.p3.4.m4.1.1.2.2.cmml">ℒ</mi><mrow id="S4.SS5.p3.4.m4.1.1.3" xref="S4.SS5.p3.4.m4.1.1.3.cmml"><mi id="S4.SS5.p3.4.m4.1.1.3.2" xref="S4.SS5.p3.4.m4.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.4.m4.1.1.3.1" xref="S4.SS5.p3.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.4.m4.1.1.3.3" xref="S4.SS5.p3.4.m4.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.4.m4.1.1.3.1a" xref="S4.SS5.p3.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.4.m4.1.1.3.4" xref="S4.SS5.p3.4.m4.1.1.3.4.cmml">s</mi></mrow><mrow id="S4.SS5.p3.4.m4.1.1.2.3" xref="S4.SS5.p3.4.m4.1.1.2.3.cmml"><mi id="S4.SS5.p3.4.m4.1.1.2.3.2" xref="S4.SS5.p3.4.m4.1.1.2.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.4.m4.1.1.2.3.1" xref="S4.SS5.p3.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p3.4.m4.1.1.2.3.3" xref="S4.SS5.p3.4.m4.1.1.2.3.3.cmml">i</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.4.m4.1b"><apply id="S4.SS5.p3.4.m4.1.1.cmml" xref="S4.SS5.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.4.m4.1.1.1.cmml" xref="S4.SS5.p3.4.m4.1.1">subscript</csymbol><apply id="S4.SS5.p3.4.m4.1.1.2.cmml" xref="S4.SS5.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.4.m4.1.1.2.1.cmml" xref="S4.SS5.p3.4.m4.1.1">superscript</csymbol><ci id="S4.SS5.p3.4.m4.1.1.2.2.cmml" xref="S4.SS5.p3.4.m4.1.1.2.2">ℒ</ci><apply id="S4.SS5.p3.4.m4.1.1.2.3.cmml" xref="S4.SS5.p3.4.m4.1.1.2.3"><times id="S4.SS5.p3.4.m4.1.1.2.3.1.cmml" xref="S4.SS5.p3.4.m4.1.1.2.3.1"></times><ci id="S4.SS5.p3.4.m4.1.1.2.3.2.cmml" xref="S4.SS5.p3.4.m4.1.1.2.3.2">𝐿</ci><ci id="S4.SS5.p3.4.m4.1.1.2.3.3.cmml" xref="S4.SS5.p3.4.m4.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S4.SS5.p3.4.m4.1.1.3.cmml" xref="S4.SS5.p3.4.m4.1.1.3"><times id="S4.SS5.p3.4.m4.1.1.3.1.cmml" xref="S4.SS5.p3.4.m4.1.1.3.1"></times><ci id="S4.SS5.p3.4.m4.1.1.3.2.cmml" xref="S4.SS5.p3.4.m4.1.1.3.2">𝑐</ci><ci id="S4.SS5.p3.4.m4.1.1.3.3.cmml" xref="S4.SS5.p3.4.m4.1.1.3.3">𝑙</ci><ci id="S4.SS5.p3.4.m4.1.1.3.4.cmml" xref="S4.SS5.p3.4.m4.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.4.m4.1c">\mathcal{L}^{Li}_{cls}</annotation></semantics></math>, <math id="S4.SS5.p3.5.m5.1" class="ltx_Math" alttext="\mathcal{L}^{Cam}_{cls}" display="inline"><semantics id="S4.SS5.p3.5.m5.1a"><msubsup id="S4.SS5.p3.5.m5.1.1" xref="S4.SS5.p3.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p3.5.m5.1.1.2.2" xref="S4.SS5.p3.5.m5.1.1.2.2.cmml">ℒ</mi><mrow id="S4.SS5.p3.5.m5.1.1.3" xref="S4.SS5.p3.5.m5.1.1.3.cmml"><mi id="S4.SS5.p3.5.m5.1.1.3.2" xref="S4.SS5.p3.5.m5.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.5.m5.1.1.3.1" xref="S4.SS5.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.5.m5.1.1.3.3" xref="S4.SS5.p3.5.m5.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.5.m5.1.1.3.1a" xref="S4.SS5.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.5.m5.1.1.3.4" xref="S4.SS5.p3.5.m5.1.1.3.4.cmml">s</mi></mrow><mrow id="S4.SS5.p3.5.m5.1.1.2.3" xref="S4.SS5.p3.5.m5.1.1.2.3.cmml"><mi id="S4.SS5.p3.5.m5.1.1.2.3.2" xref="S4.SS5.p3.5.m5.1.1.2.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.5.m5.1.1.2.3.1" xref="S4.SS5.p3.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p3.5.m5.1.1.2.3.3" xref="S4.SS5.p3.5.m5.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.5.m5.1.1.2.3.1a" xref="S4.SS5.p3.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p3.5.m5.1.1.2.3.4" xref="S4.SS5.p3.5.m5.1.1.2.3.4.cmml">m</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.5.m5.1b"><apply id="S4.SS5.p3.5.m5.1.1.cmml" xref="S4.SS5.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.5.m5.1.1.1.cmml" xref="S4.SS5.p3.5.m5.1.1">subscript</csymbol><apply id="S4.SS5.p3.5.m5.1.1.2.cmml" xref="S4.SS5.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.5.m5.1.1.2.1.cmml" xref="S4.SS5.p3.5.m5.1.1">superscript</csymbol><ci id="S4.SS5.p3.5.m5.1.1.2.2.cmml" xref="S4.SS5.p3.5.m5.1.1.2.2">ℒ</ci><apply id="S4.SS5.p3.5.m5.1.1.2.3.cmml" xref="S4.SS5.p3.5.m5.1.1.2.3"><times id="S4.SS5.p3.5.m5.1.1.2.3.1.cmml" xref="S4.SS5.p3.5.m5.1.1.2.3.1"></times><ci id="S4.SS5.p3.5.m5.1.1.2.3.2.cmml" xref="S4.SS5.p3.5.m5.1.1.2.3.2">𝐶</ci><ci id="S4.SS5.p3.5.m5.1.1.2.3.3.cmml" xref="S4.SS5.p3.5.m5.1.1.2.3.3">𝑎</ci><ci id="S4.SS5.p3.5.m5.1.1.2.3.4.cmml" xref="S4.SS5.p3.5.m5.1.1.2.3.4">𝑚</ci></apply></apply><apply id="S4.SS5.p3.5.m5.1.1.3.cmml" xref="S4.SS5.p3.5.m5.1.1.3"><times id="S4.SS5.p3.5.m5.1.1.3.1.cmml" xref="S4.SS5.p3.5.m5.1.1.3.1"></times><ci id="S4.SS5.p3.5.m5.1.1.3.2.cmml" xref="S4.SS5.p3.5.m5.1.1.3.2">𝑐</ci><ci id="S4.SS5.p3.5.m5.1.1.3.3.cmml" xref="S4.SS5.p3.5.m5.1.1.3.3">𝑙</ci><ci id="S4.SS5.p3.5.m5.1.1.3.4.cmml" xref="S4.SS5.p3.5.m5.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.5.m5.1c">\mathcal{L}^{Cam}_{cls}</annotation></semantics></math>, <math id="S4.SS5.p3.6.m6.1" class="ltx_Math" alttext="\mathcal{L}^{Ref}_{cls}" display="inline"><semantics id="S4.SS5.p3.6.m6.1a"><msubsup id="S4.SS5.p3.6.m6.1.1" xref="S4.SS5.p3.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p3.6.m6.1.1.2.2" xref="S4.SS5.p3.6.m6.1.1.2.2.cmml">ℒ</mi><mrow id="S4.SS5.p3.6.m6.1.1.3" xref="S4.SS5.p3.6.m6.1.1.3.cmml"><mi id="S4.SS5.p3.6.m6.1.1.3.2" xref="S4.SS5.p3.6.m6.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.6.m6.1.1.3.1" xref="S4.SS5.p3.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.6.m6.1.1.3.3" xref="S4.SS5.p3.6.m6.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.6.m6.1.1.3.1a" xref="S4.SS5.p3.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p3.6.m6.1.1.3.4" xref="S4.SS5.p3.6.m6.1.1.3.4.cmml">s</mi></mrow><mrow id="S4.SS5.p3.6.m6.1.1.2.3" xref="S4.SS5.p3.6.m6.1.1.2.3.cmml"><mi id="S4.SS5.p3.6.m6.1.1.2.3.2" xref="S4.SS5.p3.6.m6.1.1.2.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.6.m6.1.1.2.3.1" xref="S4.SS5.p3.6.m6.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p3.6.m6.1.1.2.3.3" xref="S4.SS5.p3.6.m6.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.6.m6.1.1.2.3.1a" xref="S4.SS5.p3.6.m6.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p3.6.m6.1.1.2.3.4" xref="S4.SS5.p3.6.m6.1.1.2.3.4.cmml">f</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.6.m6.1b"><apply id="S4.SS5.p3.6.m6.1.1.cmml" xref="S4.SS5.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.6.m6.1.1.1.cmml" xref="S4.SS5.p3.6.m6.1.1">subscript</csymbol><apply id="S4.SS5.p3.6.m6.1.1.2.cmml" xref="S4.SS5.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.6.m6.1.1.2.1.cmml" xref="S4.SS5.p3.6.m6.1.1">superscript</csymbol><ci id="S4.SS5.p3.6.m6.1.1.2.2.cmml" xref="S4.SS5.p3.6.m6.1.1.2.2">ℒ</ci><apply id="S4.SS5.p3.6.m6.1.1.2.3.cmml" xref="S4.SS5.p3.6.m6.1.1.2.3"><times id="S4.SS5.p3.6.m6.1.1.2.3.1.cmml" xref="S4.SS5.p3.6.m6.1.1.2.3.1"></times><ci id="S4.SS5.p3.6.m6.1.1.2.3.2.cmml" xref="S4.SS5.p3.6.m6.1.1.2.3.2">𝑅</ci><ci id="S4.SS5.p3.6.m6.1.1.2.3.3.cmml" xref="S4.SS5.p3.6.m6.1.1.2.3.3">𝑒</ci><ci id="S4.SS5.p3.6.m6.1.1.2.3.4.cmml" xref="S4.SS5.p3.6.m6.1.1.2.3.4">𝑓</ci></apply></apply><apply id="S4.SS5.p3.6.m6.1.1.3.cmml" xref="S4.SS5.p3.6.m6.1.1.3"><times id="S4.SS5.p3.6.m6.1.1.3.1.cmml" xref="S4.SS5.p3.6.m6.1.1.3.1"></times><ci id="S4.SS5.p3.6.m6.1.1.3.2.cmml" xref="S4.SS5.p3.6.m6.1.1.3.2">𝑐</ci><ci id="S4.SS5.p3.6.m6.1.1.3.3.cmml" xref="S4.SS5.p3.6.m6.1.1.3.3">𝑙</ci><ci id="S4.SS5.p3.6.m6.1.1.3.4.cmml" xref="S4.SS5.p3.6.m6.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.6.m6.1c">\mathcal{L}^{Ref}_{cls}</annotation></semantics></math>
for LiDAR reference bounding boxes generation, camera reference bounding boxes generation and refined boxes generation, respectively.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.3" class="ltx_p">In total, we have</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.37" class="ltx_Math" alttext="\begin{split}\mathcal{L}_{total}=\mathcal{L}^{Li}_{seg}+\mathcal{L}^{Li}_{vote}+\mathcal{L}^{Li}_{cls}+\mathcal{L}^{Li}_{reg}\\
+\mathcal{L}^{Cam}_{cls}+\mathcal{L}^{Cam}_{reg}+\mathcal{L}^{Ref}_{cls}+\mathcal{L}^{Ref}_{reg},\end{split}" display="block"><semantics id="S4.E1.m1.37a"><mtable displaystyle="true" rowspacing="0pt" id="S4.E1.m1.37.37.2"><mtr id="S4.E1.m1.37.37.2a"><mtd class="ltx_align_right" columnalign="right" id="S4.E1.m1.37.37.2b"><mrow id="S4.E1.m1.18.18.18.18.18"><msub id="S4.E1.m1.18.18.18.18.18.19"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml">ℒ</mi><mrow id="S4.E1.m1.2.2.2.2.2.2.1" xref="S4.E1.m1.2.2.2.2.2.2.1.cmml"><mi id="S4.E1.m1.2.2.2.2.2.2.1.2" xref="S4.E1.m1.2.2.2.2.2.2.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2.2.2.2.1.1" xref="S4.E1.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S4.E1.m1.2.2.2.2.2.2.1.3" xref="S4.E1.m1.2.2.2.2.2.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2.2.2.2.1.1a" xref="S4.E1.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S4.E1.m1.2.2.2.2.2.2.1.4" xref="S4.E1.m1.2.2.2.2.2.2.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2.2.2.2.1.1b" xref="S4.E1.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S4.E1.m1.2.2.2.2.2.2.1.5" xref="S4.E1.m1.2.2.2.2.2.2.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2.2.2.2.1.1c" xref="S4.E1.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S4.E1.m1.2.2.2.2.2.2.1.6" xref="S4.E1.m1.2.2.2.2.2.2.1.6.cmml">l</mi></mrow></msub><mo id="S4.E1.m1.3.3.3.3.3.3" xref="S4.E1.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S4.E1.m1.18.18.18.18.18.20"><msubsup id="S4.E1.m1.18.18.18.18.18.20.1"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.4.4.4.4.4.4" xref="S4.E1.m1.4.4.4.4.4.4.cmml">ℒ</mi><mrow id="S4.E1.m1.6.6.6.6.6.6.1" xref="S4.E1.m1.6.6.6.6.6.6.1.cmml"><mi id="S4.E1.m1.6.6.6.6.6.6.1.2" xref="S4.E1.m1.6.6.6.6.6.6.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.6.6.6.6.6.6.1.1" xref="S4.E1.m1.6.6.6.6.6.6.1.1.cmml">​</mo><mi id="S4.E1.m1.6.6.6.6.6.6.1.3" xref="S4.E1.m1.6.6.6.6.6.6.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.6.6.6.6.6.6.1.1a" xref="S4.E1.m1.6.6.6.6.6.6.1.1.cmml">​</mo><mi id="S4.E1.m1.6.6.6.6.6.6.1.4" xref="S4.E1.m1.6.6.6.6.6.6.1.4.cmml">g</mi></mrow><mrow id="S4.E1.m1.5.5.5.5.5.5.1" xref="S4.E1.m1.5.5.5.5.5.5.1.cmml"><mi id="S4.E1.m1.5.5.5.5.5.5.1.2" xref="S4.E1.m1.5.5.5.5.5.5.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.5.5.5.5.5.5.1.1" xref="S4.E1.m1.5.5.5.5.5.5.1.1.cmml">​</mo><mi id="S4.E1.m1.5.5.5.5.5.5.1.3" xref="S4.E1.m1.5.5.5.5.5.5.1.3.cmml">i</mi></mrow></msubsup><mo id="S4.E1.m1.7.7.7.7.7.7" xref="S4.E1.m1.7.7.7.7.7.7.cmml">+</mo><msubsup id="S4.E1.m1.18.18.18.18.18.20.2"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.8.8.8.8.8.8" xref="S4.E1.m1.8.8.8.8.8.8.cmml">ℒ</mi><mrow id="S4.E1.m1.10.10.10.10.10.10.1" xref="S4.E1.m1.10.10.10.10.10.10.1.cmml"><mi id="S4.E1.m1.10.10.10.10.10.10.1.2" xref="S4.E1.m1.10.10.10.10.10.10.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.10.10.10.10.10.10.1.1" xref="S4.E1.m1.10.10.10.10.10.10.1.1.cmml">​</mo><mi id="S4.E1.m1.10.10.10.10.10.10.1.3" xref="S4.E1.m1.10.10.10.10.10.10.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.10.10.10.10.10.10.1.1a" xref="S4.E1.m1.10.10.10.10.10.10.1.1.cmml">​</mo><mi id="S4.E1.m1.10.10.10.10.10.10.1.4" xref="S4.E1.m1.10.10.10.10.10.10.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.10.10.10.10.10.10.1.1b" xref="S4.E1.m1.10.10.10.10.10.10.1.1.cmml">​</mo><mi id="S4.E1.m1.10.10.10.10.10.10.1.5" xref="S4.E1.m1.10.10.10.10.10.10.1.5.cmml">e</mi></mrow><mrow id="S4.E1.m1.9.9.9.9.9.9.1" xref="S4.E1.m1.9.9.9.9.9.9.1.cmml"><mi id="S4.E1.m1.9.9.9.9.9.9.1.2" xref="S4.E1.m1.9.9.9.9.9.9.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.9.9.9.9.9.9.1.1" xref="S4.E1.m1.9.9.9.9.9.9.1.1.cmml">​</mo><mi id="S4.E1.m1.9.9.9.9.9.9.1.3" xref="S4.E1.m1.9.9.9.9.9.9.1.3.cmml">i</mi></mrow></msubsup><mo id="S4.E1.m1.7.7.7.7.7.7a" xref="S4.E1.m1.7.7.7.7.7.7.cmml">+</mo><msubsup id="S4.E1.m1.18.18.18.18.18.20.3"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.12.12.12.12.12.12" xref="S4.E1.m1.12.12.12.12.12.12.cmml">ℒ</mi><mrow id="S4.E1.m1.14.14.14.14.14.14.1" xref="S4.E1.m1.14.14.14.14.14.14.1.cmml"><mi id="S4.E1.m1.14.14.14.14.14.14.1.2" xref="S4.E1.m1.14.14.14.14.14.14.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.14.14.14.14.14.14.1.1" xref="S4.E1.m1.14.14.14.14.14.14.1.1.cmml">​</mo><mi id="S4.E1.m1.14.14.14.14.14.14.1.3" xref="S4.E1.m1.14.14.14.14.14.14.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.14.14.14.14.14.14.1.1a" xref="S4.E1.m1.14.14.14.14.14.14.1.1.cmml">​</mo><mi id="S4.E1.m1.14.14.14.14.14.14.1.4" xref="S4.E1.m1.14.14.14.14.14.14.1.4.cmml">s</mi></mrow><mrow id="S4.E1.m1.13.13.13.13.13.13.1" xref="S4.E1.m1.13.13.13.13.13.13.1.cmml"><mi id="S4.E1.m1.13.13.13.13.13.13.1.2" xref="S4.E1.m1.13.13.13.13.13.13.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.13.13.13.13.13.13.1.1" xref="S4.E1.m1.13.13.13.13.13.13.1.1.cmml">​</mo><mi id="S4.E1.m1.13.13.13.13.13.13.1.3" xref="S4.E1.m1.13.13.13.13.13.13.1.3.cmml">i</mi></mrow></msubsup><mo id="S4.E1.m1.7.7.7.7.7.7b" xref="S4.E1.m1.7.7.7.7.7.7.cmml">+</mo><msubsup id="S4.E1.m1.18.18.18.18.18.20.4"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.16.16.16.16.16.16" xref="S4.E1.m1.16.16.16.16.16.16.cmml">ℒ</mi><mrow id="S4.E1.m1.18.18.18.18.18.18.1" xref="S4.E1.m1.18.18.18.18.18.18.1.cmml"><mi id="S4.E1.m1.18.18.18.18.18.18.1.2" xref="S4.E1.m1.18.18.18.18.18.18.1.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.18.18.18.18.18.18.1.1" xref="S4.E1.m1.18.18.18.18.18.18.1.1.cmml">​</mo><mi id="S4.E1.m1.18.18.18.18.18.18.1.3" xref="S4.E1.m1.18.18.18.18.18.18.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.18.18.18.18.18.18.1.1a" xref="S4.E1.m1.18.18.18.18.18.18.1.1.cmml">​</mo><mi id="S4.E1.m1.18.18.18.18.18.18.1.4" xref="S4.E1.m1.18.18.18.18.18.18.1.4.cmml">g</mi></mrow><mrow id="S4.E1.m1.17.17.17.17.17.17.1" xref="S4.E1.m1.17.17.17.17.17.17.1.cmml"><mi id="S4.E1.m1.17.17.17.17.17.17.1.2" xref="S4.E1.m1.17.17.17.17.17.17.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.17.17.17.17.17.17.1.1" xref="S4.E1.m1.17.17.17.17.17.17.1.1.cmml">​</mo><mi id="S4.E1.m1.17.17.17.17.17.17.1.3" xref="S4.E1.m1.17.17.17.17.17.17.1.3.cmml">i</mi></mrow></msubsup></mrow></mrow></mtd></mtr><mtr id="S4.E1.m1.37.37.2c"><mtd class="ltx_align_right" columnalign="right" id="S4.E1.m1.37.37.2d"><mrow id="S4.E1.m1.37.37.2.36.18.18.18"><mrow id="S4.E1.m1.37.37.2.36.18.18.18.1"><mrow id="S4.E1.m1.37.37.2.36.18.18.18.1.1"><mo id="S4.E1.m1.37.37.2.36.18.18.18.1.1a" xref="S4.E1.m1.36.36.1.1.1.cmml">+</mo><msubsup id="S4.E1.m1.37.37.2.36.18.18.18.1.1.1"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.20.20.20.2.2.2" xref="S4.E1.m1.20.20.20.2.2.2.cmml">ℒ</mi><mrow id="S4.E1.m1.22.22.22.4.4.4.1" xref="S4.E1.m1.22.22.22.4.4.4.1.cmml"><mi id="S4.E1.m1.22.22.22.4.4.4.1.2" xref="S4.E1.m1.22.22.22.4.4.4.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.22.22.22.4.4.4.1.1" xref="S4.E1.m1.22.22.22.4.4.4.1.1.cmml">​</mo><mi id="S4.E1.m1.22.22.22.4.4.4.1.3" xref="S4.E1.m1.22.22.22.4.4.4.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.22.22.22.4.4.4.1.1a" xref="S4.E1.m1.22.22.22.4.4.4.1.1.cmml">​</mo><mi id="S4.E1.m1.22.22.22.4.4.4.1.4" xref="S4.E1.m1.22.22.22.4.4.4.1.4.cmml">s</mi></mrow><mrow id="S4.E1.m1.21.21.21.3.3.3.1" xref="S4.E1.m1.21.21.21.3.3.3.1.cmml"><mi id="S4.E1.m1.21.21.21.3.3.3.1.2" xref="S4.E1.m1.21.21.21.3.3.3.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.21.21.21.3.3.3.1.1" xref="S4.E1.m1.21.21.21.3.3.3.1.1.cmml">​</mo><mi id="S4.E1.m1.21.21.21.3.3.3.1.3" xref="S4.E1.m1.21.21.21.3.3.3.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.21.21.21.3.3.3.1.1a" xref="S4.E1.m1.21.21.21.3.3.3.1.1.cmml">​</mo><mi id="S4.E1.m1.21.21.21.3.3.3.1.4" xref="S4.E1.m1.21.21.21.3.3.3.1.4.cmml">m</mi></mrow></msubsup></mrow><mo id="S4.E1.m1.23.23.23.5.5.5" xref="S4.E1.m1.36.36.1.1.1.cmml">+</mo><msubsup id="S4.E1.m1.37.37.2.36.18.18.18.1.2"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.24.24.24.6.6.6" xref="S4.E1.m1.24.24.24.6.6.6.cmml">ℒ</mi><mrow id="S4.E1.m1.26.26.26.8.8.8.1" xref="S4.E1.m1.26.26.26.8.8.8.1.cmml"><mi id="S4.E1.m1.26.26.26.8.8.8.1.2" xref="S4.E1.m1.26.26.26.8.8.8.1.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.26.26.26.8.8.8.1.1" xref="S4.E1.m1.26.26.26.8.8.8.1.1.cmml">​</mo><mi id="S4.E1.m1.26.26.26.8.8.8.1.3" xref="S4.E1.m1.26.26.26.8.8.8.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.26.26.26.8.8.8.1.1a" xref="S4.E1.m1.26.26.26.8.8.8.1.1.cmml">​</mo><mi id="S4.E1.m1.26.26.26.8.8.8.1.4" xref="S4.E1.m1.26.26.26.8.8.8.1.4.cmml">g</mi></mrow><mrow id="S4.E1.m1.25.25.25.7.7.7.1" xref="S4.E1.m1.25.25.25.7.7.7.1.cmml"><mi id="S4.E1.m1.25.25.25.7.7.7.1.2" xref="S4.E1.m1.25.25.25.7.7.7.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.25.25.25.7.7.7.1.1" xref="S4.E1.m1.25.25.25.7.7.7.1.1.cmml">​</mo><mi id="S4.E1.m1.25.25.25.7.7.7.1.3" xref="S4.E1.m1.25.25.25.7.7.7.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.25.25.25.7.7.7.1.1a" xref="S4.E1.m1.25.25.25.7.7.7.1.1.cmml">​</mo><mi id="S4.E1.m1.25.25.25.7.7.7.1.4" xref="S4.E1.m1.25.25.25.7.7.7.1.4.cmml">m</mi></mrow></msubsup><mo id="S4.E1.m1.23.23.23.5.5.5a" xref="S4.E1.m1.36.36.1.1.1.cmml">+</mo><msubsup id="S4.E1.m1.37.37.2.36.18.18.18.1.3"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.28.28.28.10.10.10" xref="S4.E1.m1.28.28.28.10.10.10.cmml">ℒ</mi><mrow id="S4.E1.m1.30.30.30.12.12.12.1" xref="S4.E1.m1.30.30.30.12.12.12.1.cmml"><mi id="S4.E1.m1.30.30.30.12.12.12.1.2" xref="S4.E1.m1.30.30.30.12.12.12.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.30.30.30.12.12.12.1.1" xref="S4.E1.m1.30.30.30.12.12.12.1.1.cmml">​</mo><mi id="S4.E1.m1.30.30.30.12.12.12.1.3" xref="S4.E1.m1.30.30.30.12.12.12.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.30.30.30.12.12.12.1.1a" xref="S4.E1.m1.30.30.30.12.12.12.1.1.cmml">​</mo><mi id="S4.E1.m1.30.30.30.12.12.12.1.4" xref="S4.E1.m1.30.30.30.12.12.12.1.4.cmml">s</mi></mrow><mrow id="S4.E1.m1.29.29.29.11.11.11.1" xref="S4.E1.m1.29.29.29.11.11.11.1.cmml"><mi id="S4.E1.m1.29.29.29.11.11.11.1.2" xref="S4.E1.m1.29.29.29.11.11.11.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.29.29.29.11.11.11.1.1" xref="S4.E1.m1.29.29.29.11.11.11.1.1.cmml">​</mo><mi id="S4.E1.m1.29.29.29.11.11.11.1.3" xref="S4.E1.m1.29.29.29.11.11.11.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.29.29.29.11.11.11.1.1a" xref="S4.E1.m1.29.29.29.11.11.11.1.1.cmml">​</mo><mi id="S4.E1.m1.29.29.29.11.11.11.1.4" xref="S4.E1.m1.29.29.29.11.11.11.1.4.cmml">f</mi></mrow></msubsup><mo id="S4.E1.m1.23.23.23.5.5.5b" xref="S4.E1.m1.36.36.1.1.1.cmml">+</mo><msubsup id="S4.E1.m1.37.37.2.36.18.18.18.1.4"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.32.32.32.14.14.14" xref="S4.E1.m1.32.32.32.14.14.14.cmml">ℒ</mi><mrow id="S4.E1.m1.34.34.34.16.16.16.1" xref="S4.E1.m1.34.34.34.16.16.16.1.cmml"><mi id="S4.E1.m1.34.34.34.16.16.16.1.2" xref="S4.E1.m1.34.34.34.16.16.16.1.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.34.34.34.16.16.16.1.1" xref="S4.E1.m1.34.34.34.16.16.16.1.1.cmml">​</mo><mi id="S4.E1.m1.34.34.34.16.16.16.1.3" xref="S4.E1.m1.34.34.34.16.16.16.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.34.34.34.16.16.16.1.1a" xref="S4.E1.m1.34.34.34.16.16.16.1.1.cmml">​</mo><mi id="S4.E1.m1.34.34.34.16.16.16.1.4" xref="S4.E1.m1.34.34.34.16.16.16.1.4.cmml">g</mi></mrow><mrow id="S4.E1.m1.33.33.33.15.15.15.1" xref="S4.E1.m1.33.33.33.15.15.15.1.cmml"><mi id="S4.E1.m1.33.33.33.15.15.15.1.2" xref="S4.E1.m1.33.33.33.15.15.15.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.33.33.33.15.15.15.1.1" xref="S4.E1.m1.33.33.33.15.15.15.1.1.cmml">​</mo><mi id="S4.E1.m1.33.33.33.15.15.15.1.3" xref="S4.E1.m1.33.33.33.15.15.15.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.33.33.33.15.15.15.1.1a" xref="S4.E1.m1.33.33.33.15.15.15.1.1.cmml">​</mo><mi id="S4.E1.m1.33.33.33.15.15.15.1.4" xref="S4.E1.m1.33.33.33.15.15.15.1.4.cmml">f</mi></mrow></msubsup></mrow><mo id="S4.E1.m1.35.35.35.17.17.17" xref="S4.E1.m1.36.36.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S4.E1.m1.37b"><apply id="S4.E1.m1.36.36.1.1.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><eq id="S4.E1.m1.3.3.3.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3.3.3"></eq><apply id="S4.E1.m1.36.36.1.1.1.2.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.2.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1">ℒ</ci><apply id="S4.E1.m1.2.2.2.2.2.2.1.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1"><times id="S4.E1.m1.2.2.2.2.2.2.1.1.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.1"></times><ci id="S4.E1.m1.2.2.2.2.2.2.1.2.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.2">𝑡</ci><ci id="S4.E1.m1.2.2.2.2.2.2.1.3.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.3">𝑜</ci><ci id="S4.E1.m1.2.2.2.2.2.2.1.4.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.4">𝑡</ci><ci id="S4.E1.m1.2.2.2.2.2.2.1.5.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.5">𝑎</ci><ci id="S4.E1.m1.2.2.2.2.2.2.1.6.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.6">𝑙</ci></apply></apply><apply id="S4.E1.m1.36.36.1.1.1.3.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><plus id="S4.E1.m1.7.7.7.7.7.7.cmml" xref="S4.E1.m1.7.7.7.7.7.7"></plus><apply id="S4.E1.m1.36.36.1.1.1.3.2.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.2.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">subscript</csymbol><apply id="S4.E1.m1.36.36.1.1.1.3.2.2.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.2.2.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">superscript</csymbol><ci id="S4.E1.m1.4.4.4.4.4.4.cmml" xref="S4.E1.m1.4.4.4.4.4.4">ℒ</ci><apply id="S4.E1.m1.5.5.5.5.5.5.1.cmml" xref="S4.E1.m1.5.5.5.5.5.5.1"><times id="S4.E1.m1.5.5.5.5.5.5.1.1.cmml" xref="S4.E1.m1.5.5.5.5.5.5.1.1"></times><ci id="S4.E1.m1.5.5.5.5.5.5.1.2.cmml" xref="S4.E1.m1.5.5.5.5.5.5.1.2">𝐿</ci><ci id="S4.E1.m1.5.5.5.5.5.5.1.3.cmml" xref="S4.E1.m1.5.5.5.5.5.5.1.3">𝑖</ci></apply></apply><apply id="S4.E1.m1.6.6.6.6.6.6.1.cmml" xref="S4.E1.m1.6.6.6.6.6.6.1"><times id="S4.E1.m1.6.6.6.6.6.6.1.1.cmml" xref="S4.E1.m1.6.6.6.6.6.6.1.1"></times><ci id="S4.E1.m1.6.6.6.6.6.6.1.2.cmml" xref="S4.E1.m1.6.6.6.6.6.6.1.2">𝑠</ci><ci id="S4.E1.m1.6.6.6.6.6.6.1.3.cmml" xref="S4.E1.m1.6.6.6.6.6.6.1.3">𝑒</ci><ci id="S4.E1.m1.6.6.6.6.6.6.1.4.cmml" xref="S4.E1.m1.6.6.6.6.6.6.1.4">𝑔</ci></apply></apply><apply id="S4.E1.m1.36.36.1.1.1.3.3.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.3.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">subscript</csymbol><apply id="S4.E1.m1.36.36.1.1.1.3.3.2.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.3.2.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">superscript</csymbol><ci id="S4.E1.m1.8.8.8.8.8.8.cmml" xref="S4.E1.m1.8.8.8.8.8.8">ℒ</ci><apply id="S4.E1.m1.9.9.9.9.9.9.1.cmml" xref="S4.E1.m1.9.9.9.9.9.9.1"><times id="S4.E1.m1.9.9.9.9.9.9.1.1.cmml" xref="S4.E1.m1.9.9.9.9.9.9.1.1"></times><ci id="S4.E1.m1.9.9.9.9.9.9.1.2.cmml" xref="S4.E1.m1.9.9.9.9.9.9.1.2">𝐿</ci><ci id="S4.E1.m1.9.9.9.9.9.9.1.3.cmml" xref="S4.E1.m1.9.9.9.9.9.9.1.3">𝑖</ci></apply></apply><apply id="S4.E1.m1.10.10.10.10.10.10.1.cmml" xref="S4.E1.m1.10.10.10.10.10.10.1"><times id="S4.E1.m1.10.10.10.10.10.10.1.1.cmml" xref="S4.E1.m1.10.10.10.10.10.10.1.1"></times><ci id="S4.E1.m1.10.10.10.10.10.10.1.2.cmml" xref="S4.E1.m1.10.10.10.10.10.10.1.2">𝑣</ci><ci id="S4.E1.m1.10.10.10.10.10.10.1.3.cmml" xref="S4.E1.m1.10.10.10.10.10.10.1.3">𝑜</ci><ci id="S4.E1.m1.10.10.10.10.10.10.1.4.cmml" xref="S4.E1.m1.10.10.10.10.10.10.1.4">𝑡</ci><ci id="S4.E1.m1.10.10.10.10.10.10.1.5.cmml" xref="S4.E1.m1.10.10.10.10.10.10.1.5">𝑒</ci></apply></apply><apply id="S4.E1.m1.36.36.1.1.1.3.4.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.4.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">subscript</csymbol><apply id="S4.E1.m1.36.36.1.1.1.3.4.2.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.4.2.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">superscript</csymbol><ci id="S4.E1.m1.12.12.12.12.12.12.cmml" xref="S4.E1.m1.12.12.12.12.12.12">ℒ</ci><apply id="S4.E1.m1.13.13.13.13.13.13.1.cmml" xref="S4.E1.m1.13.13.13.13.13.13.1"><times id="S4.E1.m1.13.13.13.13.13.13.1.1.cmml" xref="S4.E1.m1.13.13.13.13.13.13.1.1"></times><ci id="S4.E1.m1.13.13.13.13.13.13.1.2.cmml" xref="S4.E1.m1.13.13.13.13.13.13.1.2">𝐿</ci><ci id="S4.E1.m1.13.13.13.13.13.13.1.3.cmml" xref="S4.E1.m1.13.13.13.13.13.13.1.3">𝑖</ci></apply></apply><apply id="S4.E1.m1.14.14.14.14.14.14.1.cmml" xref="S4.E1.m1.14.14.14.14.14.14.1"><times id="S4.E1.m1.14.14.14.14.14.14.1.1.cmml" xref="S4.E1.m1.14.14.14.14.14.14.1.1"></times><ci id="S4.E1.m1.14.14.14.14.14.14.1.2.cmml" xref="S4.E1.m1.14.14.14.14.14.14.1.2">𝑐</ci><ci id="S4.E1.m1.14.14.14.14.14.14.1.3.cmml" xref="S4.E1.m1.14.14.14.14.14.14.1.3">𝑙</ci><ci id="S4.E1.m1.14.14.14.14.14.14.1.4.cmml" xref="S4.E1.m1.14.14.14.14.14.14.1.4">𝑠</ci></apply></apply><apply id="S4.E1.m1.36.36.1.1.1.3.5.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.5.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">subscript</csymbol><apply id="S4.E1.m1.36.36.1.1.1.3.5.2.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.5.2.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">superscript</csymbol><ci id="S4.E1.m1.16.16.16.16.16.16.cmml" xref="S4.E1.m1.16.16.16.16.16.16">ℒ</ci><apply id="S4.E1.m1.17.17.17.17.17.17.1.cmml" xref="S4.E1.m1.17.17.17.17.17.17.1"><times id="S4.E1.m1.17.17.17.17.17.17.1.1.cmml" xref="S4.E1.m1.17.17.17.17.17.17.1.1"></times><ci id="S4.E1.m1.17.17.17.17.17.17.1.2.cmml" xref="S4.E1.m1.17.17.17.17.17.17.1.2">𝐿</ci><ci id="S4.E1.m1.17.17.17.17.17.17.1.3.cmml" xref="S4.E1.m1.17.17.17.17.17.17.1.3">𝑖</ci></apply></apply><apply id="S4.E1.m1.18.18.18.18.18.18.1.cmml" xref="S4.E1.m1.18.18.18.18.18.18.1"><times id="S4.E1.m1.18.18.18.18.18.18.1.1.cmml" xref="S4.E1.m1.18.18.18.18.18.18.1.1"></times><ci id="S4.E1.m1.18.18.18.18.18.18.1.2.cmml" xref="S4.E1.m1.18.18.18.18.18.18.1.2">𝑟</ci><ci id="S4.E1.m1.18.18.18.18.18.18.1.3.cmml" xref="S4.E1.m1.18.18.18.18.18.18.1.3">𝑒</ci><ci id="S4.E1.m1.18.18.18.18.18.18.1.4.cmml" xref="S4.E1.m1.18.18.18.18.18.18.1.4">𝑔</ci></apply></apply><apply id="S4.E1.m1.36.36.1.1.1.3.6.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.6.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">subscript</csymbol><apply id="S4.E1.m1.36.36.1.1.1.3.6.2.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.6.2.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">superscript</csymbol><ci id="S4.E1.m1.20.20.20.2.2.2.cmml" xref="S4.E1.m1.20.20.20.2.2.2">ℒ</ci><apply id="S4.E1.m1.21.21.21.3.3.3.1.cmml" xref="S4.E1.m1.21.21.21.3.3.3.1"><times id="S4.E1.m1.21.21.21.3.3.3.1.1.cmml" xref="S4.E1.m1.21.21.21.3.3.3.1.1"></times><ci id="S4.E1.m1.21.21.21.3.3.3.1.2.cmml" xref="S4.E1.m1.21.21.21.3.3.3.1.2">𝐶</ci><ci id="S4.E1.m1.21.21.21.3.3.3.1.3.cmml" xref="S4.E1.m1.21.21.21.3.3.3.1.3">𝑎</ci><ci id="S4.E1.m1.21.21.21.3.3.3.1.4.cmml" xref="S4.E1.m1.21.21.21.3.3.3.1.4">𝑚</ci></apply></apply><apply id="S4.E1.m1.22.22.22.4.4.4.1.cmml" xref="S4.E1.m1.22.22.22.4.4.4.1"><times id="S4.E1.m1.22.22.22.4.4.4.1.1.cmml" xref="S4.E1.m1.22.22.22.4.4.4.1.1"></times><ci id="S4.E1.m1.22.22.22.4.4.4.1.2.cmml" xref="S4.E1.m1.22.22.22.4.4.4.1.2">𝑐</ci><ci id="S4.E1.m1.22.22.22.4.4.4.1.3.cmml" xref="S4.E1.m1.22.22.22.4.4.4.1.3">𝑙</ci><ci id="S4.E1.m1.22.22.22.4.4.4.1.4.cmml" xref="S4.E1.m1.22.22.22.4.4.4.1.4">𝑠</ci></apply></apply><apply id="S4.E1.m1.36.36.1.1.1.3.7.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.7.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">subscript</csymbol><apply id="S4.E1.m1.36.36.1.1.1.3.7.2.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.7.2.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">superscript</csymbol><ci id="S4.E1.m1.24.24.24.6.6.6.cmml" xref="S4.E1.m1.24.24.24.6.6.6">ℒ</ci><apply id="S4.E1.m1.25.25.25.7.7.7.1.cmml" xref="S4.E1.m1.25.25.25.7.7.7.1"><times id="S4.E1.m1.25.25.25.7.7.7.1.1.cmml" xref="S4.E1.m1.25.25.25.7.7.7.1.1"></times><ci id="S4.E1.m1.25.25.25.7.7.7.1.2.cmml" xref="S4.E1.m1.25.25.25.7.7.7.1.2">𝐶</ci><ci id="S4.E1.m1.25.25.25.7.7.7.1.3.cmml" xref="S4.E1.m1.25.25.25.7.7.7.1.3">𝑎</ci><ci id="S4.E1.m1.25.25.25.7.7.7.1.4.cmml" xref="S4.E1.m1.25.25.25.7.7.7.1.4">𝑚</ci></apply></apply><apply id="S4.E1.m1.26.26.26.8.8.8.1.cmml" xref="S4.E1.m1.26.26.26.8.8.8.1"><times id="S4.E1.m1.26.26.26.8.8.8.1.1.cmml" xref="S4.E1.m1.26.26.26.8.8.8.1.1"></times><ci id="S4.E1.m1.26.26.26.8.8.8.1.2.cmml" xref="S4.E1.m1.26.26.26.8.8.8.1.2">𝑟</ci><ci id="S4.E1.m1.26.26.26.8.8.8.1.3.cmml" xref="S4.E1.m1.26.26.26.8.8.8.1.3">𝑒</ci><ci id="S4.E1.m1.26.26.26.8.8.8.1.4.cmml" xref="S4.E1.m1.26.26.26.8.8.8.1.4">𝑔</ci></apply></apply><apply id="S4.E1.m1.36.36.1.1.1.3.8.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.8.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">subscript</csymbol><apply id="S4.E1.m1.36.36.1.1.1.3.8.2.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.8.2.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">superscript</csymbol><ci id="S4.E1.m1.28.28.28.10.10.10.cmml" xref="S4.E1.m1.28.28.28.10.10.10">ℒ</ci><apply id="S4.E1.m1.29.29.29.11.11.11.1.cmml" xref="S4.E1.m1.29.29.29.11.11.11.1"><times id="S4.E1.m1.29.29.29.11.11.11.1.1.cmml" xref="S4.E1.m1.29.29.29.11.11.11.1.1"></times><ci id="S4.E1.m1.29.29.29.11.11.11.1.2.cmml" xref="S4.E1.m1.29.29.29.11.11.11.1.2">𝑅</ci><ci id="S4.E1.m1.29.29.29.11.11.11.1.3.cmml" xref="S4.E1.m1.29.29.29.11.11.11.1.3">𝑒</ci><ci id="S4.E1.m1.29.29.29.11.11.11.1.4.cmml" xref="S4.E1.m1.29.29.29.11.11.11.1.4">𝑓</ci></apply></apply><apply id="S4.E1.m1.30.30.30.12.12.12.1.cmml" xref="S4.E1.m1.30.30.30.12.12.12.1"><times id="S4.E1.m1.30.30.30.12.12.12.1.1.cmml" xref="S4.E1.m1.30.30.30.12.12.12.1.1"></times><ci id="S4.E1.m1.30.30.30.12.12.12.1.2.cmml" xref="S4.E1.m1.30.30.30.12.12.12.1.2">𝑐</ci><ci id="S4.E1.m1.30.30.30.12.12.12.1.3.cmml" xref="S4.E1.m1.30.30.30.12.12.12.1.3">𝑙</ci><ci id="S4.E1.m1.30.30.30.12.12.12.1.4.cmml" xref="S4.E1.m1.30.30.30.12.12.12.1.4">𝑠</ci></apply></apply><apply id="S4.E1.m1.36.36.1.1.1.3.9.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.9.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">subscript</csymbol><apply id="S4.E1.m1.36.36.1.1.1.3.9.2.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a"><csymbol cd="ambiguous" id="S4.E1.m1.36.36.1.1.1.3.9.2.1.cmml" xref="S4.E1.m1.37.37.2.36.18.18.18.1.1a">superscript</csymbol><ci id="S4.E1.m1.32.32.32.14.14.14.cmml" xref="S4.E1.m1.32.32.32.14.14.14">ℒ</ci><apply id="S4.E1.m1.33.33.33.15.15.15.1.cmml" xref="S4.E1.m1.33.33.33.15.15.15.1"><times id="S4.E1.m1.33.33.33.15.15.15.1.1.cmml" xref="S4.E1.m1.33.33.33.15.15.15.1.1"></times><ci id="S4.E1.m1.33.33.33.15.15.15.1.2.cmml" xref="S4.E1.m1.33.33.33.15.15.15.1.2">𝑅</ci><ci id="S4.E1.m1.33.33.33.15.15.15.1.3.cmml" xref="S4.E1.m1.33.33.33.15.15.15.1.3">𝑒</ci><ci id="S4.E1.m1.33.33.33.15.15.15.1.4.cmml" xref="S4.E1.m1.33.33.33.15.15.15.1.4">𝑓</ci></apply></apply><apply id="S4.E1.m1.34.34.34.16.16.16.1.cmml" xref="S4.E1.m1.34.34.34.16.16.16.1"><times id="S4.E1.m1.34.34.34.16.16.16.1.1.cmml" xref="S4.E1.m1.34.34.34.16.16.16.1.1"></times><ci id="S4.E1.m1.34.34.34.16.16.16.1.2.cmml" xref="S4.E1.m1.34.34.34.16.16.16.1.2">𝑟</ci><ci id="S4.E1.m1.34.34.34.16.16.16.1.3.cmml" xref="S4.E1.m1.34.34.34.16.16.16.1.3">𝑒</ci><ci id="S4.E1.m1.34.34.34.16.16.16.1.4.cmml" xref="S4.E1.m1.34.34.34.16.16.16.1.4">𝑔</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.37c">\begin{split}\mathcal{L}_{total}=\mathcal{L}^{Li}_{seg}+\mathcal{L}^{Li}_{vote}+\mathcal{L}^{Li}_{cls}+\mathcal{L}^{Li}_{reg}\\
+\mathcal{L}^{Cam}_{cls}+\mathcal{L}^{Cam}_{reg}+\mathcal{L}^{Ref}_{cls}+\mathcal{L}^{Ref}_{reg},\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS5.p4.2" class="ltx_p">where <math id="S4.SS5.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{L}^{Li}_{seg}" display="inline"><semantics id="S4.SS5.p4.1.m1.1a"><msubsup id="S4.SS5.p4.1.m1.1.1" xref="S4.SS5.p4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p4.1.m1.1.1.2.2" xref="S4.SS5.p4.1.m1.1.1.2.2.cmml">ℒ</mi><mrow id="S4.SS5.p4.1.m1.1.1.3" xref="S4.SS5.p4.1.m1.1.1.3.cmml"><mi id="S4.SS5.p4.1.m1.1.1.3.2" xref="S4.SS5.p4.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p4.1.m1.1.1.3.1" xref="S4.SS5.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p4.1.m1.1.1.3.3" xref="S4.SS5.p4.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p4.1.m1.1.1.3.1a" xref="S4.SS5.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p4.1.m1.1.1.3.4" xref="S4.SS5.p4.1.m1.1.1.3.4.cmml">g</mi></mrow><mrow id="S4.SS5.p4.1.m1.1.1.2.3" xref="S4.SS5.p4.1.m1.1.1.2.3.cmml"><mi id="S4.SS5.p4.1.m1.1.1.2.3.2" xref="S4.SS5.p4.1.m1.1.1.2.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p4.1.m1.1.1.2.3.1" xref="S4.SS5.p4.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p4.1.m1.1.1.2.3.3" xref="S4.SS5.p4.1.m1.1.1.2.3.3.cmml">i</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.1.m1.1b"><apply id="S4.SS5.p4.1.m1.1.1.cmml" xref="S4.SS5.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p4.1.m1.1.1.1.cmml" xref="S4.SS5.p4.1.m1.1.1">subscript</csymbol><apply id="S4.SS5.p4.1.m1.1.1.2.cmml" xref="S4.SS5.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p4.1.m1.1.1.2.1.cmml" xref="S4.SS5.p4.1.m1.1.1">superscript</csymbol><ci id="S4.SS5.p4.1.m1.1.1.2.2.cmml" xref="S4.SS5.p4.1.m1.1.1.2.2">ℒ</ci><apply id="S4.SS5.p4.1.m1.1.1.2.3.cmml" xref="S4.SS5.p4.1.m1.1.1.2.3"><times id="S4.SS5.p4.1.m1.1.1.2.3.1.cmml" xref="S4.SS5.p4.1.m1.1.1.2.3.1"></times><ci id="S4.SS5.p4.1.m1.1.1.2.3.2.cmml" xref="S4.SS5.p4.1.m1.1.1.2.3.2">𝐿</ci><ci id="S4.SS5.p4.1.m1.1.1.2.3.3.cmml" xref="S4.SS5.p4.1.m1.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S4.SS5.p4.1.m1.1.1.3.cmml" xref="S4.SS5.p4.1.m1.1.1.3"><times id="S4.SS5.p4.1.m1.1.1.3.1.cmml" xref="S4.SS5.p4.1.m1.1.1.3.1"></times><ci id="S4.SS5.p4.1.m1.1.1.3.2.cmml" xref="S4.SS5.p4.1.m1.1.1.3.2">𝑠</ci><ci id="S4.SS5.p4.1.m1.1.1.3.3.cmml" xref="S4.SS5.p4.1.m1.1.1.3.3">𝑒</ci><ci id="S4.SS5.p4.1.m1.1.1.3.4.cmml" xref="S4.SS5.p4.1.m1.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.1.m1.1c">\mathcal{L}^{Li}_{seg}</annotation></semantics></math> and <math id="S4.SS5.p4.2.m2.1" class="ltx_Math" alttext="\mathcal{L}^{Li}_{vote}" display="inline"><semantics id="S4.SS5.p4.2.m2.1a"><msubsup id="S4.SS5.p4.2.m2.1.1" xref="S4.SS5.p4.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p4.2.m2.1.1.2.2" xref="S4.SS5.p4.2.m2.1.1.2.2.cmml">ℒ</mi><mrow id="S4.SS5.p4.2.m2.1.1.3" xref="S4.SS5.p4.2.m2.1.1.3.cmml"><mi id="S4.SS5.p4.2.m2.1.1.3.2" xref="S4.SS5.p4.2.m2.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p4.2.m2.1.1.3.1" xref="S4.SS5.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p4.2.m2.1.1.3.3" xref="S4.SS5.p4.2.m2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p4.2.m2.1.1.3.1a" xref="S4.SS5.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p4.2.m2.1.1.3.4" xref="S4.SS5.p4.2.m2.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p4.2.m2.1.1.3.1b" xref="S4.SS5.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p4.2.m2.1.1.3.5" xref="S4.SS5.p4.2.m2.1.1.3.5.cmml">e</mi></mrow><mrow id="S4.SS5.p4.2.m2.1.1.2.3" xref="S4.SS5.p4.2.m2.1.1.2.3.cmml"><mi id="S4.SS5.p4.2.m2.1.1.2.3.2" xref="S4.SS5.p4.2.m2.1.1.2.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p4.2.m2.1.1.2.3.1" xref="S4.SS5.p4.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S4.SS5.p4.2.m2.1.1.2.3.3" xref="S4.SS5.p4.2.m2.1.1.2.3.3.cmml">i</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.2.m2.1b"><apply id="S4.SS5.p4.2.m2.1.1.cmml" xref="S4.SS5.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS5.p4.2.m2.1.1.1.cmml" xref="S4.SS5.p4.2.m2.1.1">subscript</csymbol><apply id="S4.SS5.p4.2.m2.1.1.2.cmml" xref="S4.SS5.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS5.p4.2.m2.1.1.2.1.cmml" xref="S4.SS5.p4.2.m2.1.1">superscript</csymbol><ci id="S4.SS5.p4.2.m2.1.1.2.2.cmml" xref="S4.SS5.p4.2.m2.1.1.2.2">ℒ</ci><apply id="S4.SS5.p4.2.m2.1.1.2.3.cmml" xref="S4.SS5.p4.2.m2.1.1.2.3"><times id="S4.SS5.p4.2.m2.1.1.2.3.1.cmml" xref="S4.SS5.p4.2.m2.1.1.2.3.1"></times><ci id="S4.SS5.p4.2.m2.1.1.2.3.2.cmml" xref="S4.SS5.p4.2.m2.1.1.2.3.2">𝐿</ci><ci id="S4.SS5.p4.2.m2.1.1.2.3.3.cmml" xref="S4.SS5.p4.2.m2.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S4.SS5.p4.2.m2.1.1.3.cmml" xref="S4.SS5.p4.2.m2.1.1.3"><times id="S4.SS5.p4.2.m2.1.1.3.1.cmml" xref="S4.SS5.p4.2.m2.1.1.3.1"></times><ci id="S4.SS5.p4.2.m2.1.1.3.2.cmml" xref="S4.SS5.p4.2.m2.1.1.3.2">𝑣</ci><ci id="S4.SS5.p4.2.m2.1.1.3.3.cmml" xref="S4.SS5.p4.2.m2.1.1.3.3">𝑜</ci><ci id="S4.SS5.p4.2.m2.1.1.3.4.cmml" xref="S4.SS5.p4.2.m2.1.1.3.4">𝑡</ci><ci id="S4.SS5.p4.2.m2.1.1.3.5.cmml" xref="S4.SS5.p4.2.m2.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.2.m2.1c">\mathcal{L}^{Li}_{vote}</annotation></semantics></math> is the segmentation and voting loss of FSD.
We omit the weight factors for each loss for brevity.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Setup</h3>

<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:496.9pt;height:139pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-156.4pt,43.7pt) scale(0.613614736550868,0.613614736550868) ;">
<table id="S5.T2.2.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.2.2.2.3" class="ltx_tr">
<td id="S5.T2.2.2.2.3.1" class="ltx_td ltx_border_r ltx_border_tt ltx_border_t"></td>
<td id="S5.T2.2.2.2.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt ltx_border_t"><span id="S5.T2.2.2.2.3.2.1" class="ltx_text ltx_font_bold">Methods</span></td>
<td id="S5.T2.2.2.2.3.3" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:35.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:35.3pt;transform:translate(-13.26pt,-12.29pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.3.1.1" class="ltx_p">Average</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.4" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:31.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:31.1pt;transform:translate(-12.08pt,-12.08pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.4.1.1" class="ltx_p">Vehicle</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.5" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:16.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:16.6pt;transform:translate(-4.88pt,-4.88pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.5.1.1" class="ltx_p">Bus</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.6" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:46.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.1pt;transform:translate(-19.56pt,-19.56pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.6.1.1" class="ltx_p">Pedestrian</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.7" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.7.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:45.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.7pt;transform:translate(-19.39pt,-19.39pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.7.1.1" class="ltx_p">Box Truck</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.8" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:37.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.7pt;transform:translate(-15.38pt,-15.38pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.8.1.1" class="ltx_p">C-Barrel</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.9" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:54.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:54.5pt;transform:translate(-22.82pt,-21.85pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.9.1.1" class="ltx_p">Motorcyclist</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.10" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.10.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:45.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.4pt;transform:translate(-18.32pt,-17.35pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.10.1.1" class="ltx_p">MPC-Sign</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.11" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.11.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:48.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:48.4pt;transform:translate(-19.74pt,-18.76pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.11.1.1" class="ltx_p">Motorcycle</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.12" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.12.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:31.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:31.3pt;transform:translate(-11.18pt,-10.21pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.12.1.1" class="ltx_p">Bicycle</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.13" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.13.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:27.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:27.4pt;transform:translate(-10.29pt,-10.29pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.13.1.1" class="ltx_p">A-Bus</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.14" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.14.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:48.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:48.3pt;transform:translate(-20.65pt,-20.65pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.14.1.1" class="ltx_p">School Bus</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.15" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.15.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:46.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.4pt;transform:translate(-19.74pt,-19.74pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.15.1.1" class="ltx_p">Truck Cab</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.16" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.16.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:32.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.8pt;transform:translate(-12.97pt,-12.97pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.16.1.1" class="ltx_p">C-Cone</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.17" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.17.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:40.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:40.1pt;transform:translate(-16.56pt,-16.56pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.17.1.1" class="ltx_p">V-Trailer</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.18" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.18.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:32.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.1pt;transform:translate(-12.58pt,-12.58pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.18.1.1" class="ltx_p">Bollard</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.19" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.19.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:18.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:18.9pt;transform:translate(-5.06pt,-4.08pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.19.1.1" class="ltx_p">Sign</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.20" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.20.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:59.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:59.1pt;transform:translate(-25.08pt,-24.11pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.20.1.1" class="ltx_p">Large Vehicle</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.21" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.21.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:42.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.2pt;transform:translate(-16.72pt,-15.75pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.21.1.1" class="ltx_p">Stop Sign</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.22" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.22.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:32.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.3pt;transform:translate(-12.67pt,-12.67pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.22.1.1" class="ltx_p">Stroller</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.23" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">
<div id="S5.T2.2.2.2.3.23.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:37.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.4pt;transform:translate(-14.26pt,-13.29pt) rotate(-90deg) ;">
<p id="S5.T2.2.2.2.3.23.1.1" class="ltx_p">Bicyclist</p>
</span></div>
</td>
<td id="S5.T2.2.2.2.3.24" class="ltx_td ltx_border_tt ltx_border_t"></td>
</tr>
<tr id="S5.T2.2.2.2.4" class="ltx_tr">
<td id="S5.T2.2.2.2.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="4"><span id="S5.T2.2.2.2.4.1.1" class="ltx_text">mAP</span></td>
<td id="S5.T2.2.2.2.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">CenterPoint‡ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S5.T2.2.2.2.4.3" class="ltx_td ltx_align_center ltx_border_t">13.5</td>
<td id="S5.T2.2.2.2.4.4" class="ltx_td ltx_align_center ltx_border_t">61.0</td>
<td id="S5.T2.2.2.2.4.5" class="ltx_td ltx_align_center ltx_border_t">36.0</td>
<td id="S5.T2.2.2.2.4.6" class="ltx_td ltx_align_center ltx_border_t">33.0</td>
<td id="S5.T2.2.2.2.4.7" class="ltx_td ltx_align_center ltx_border_t">26.0</td>
<td id="S5.T2.2.2.2.4.8" class="ltx_td ltx_align_center ltx_border_t">22.5</td>
<td id="S5.T2.2.2.2.4.9" class="ltx_td ltx_align_center ltx_border_t">16.0</td>
<td id="S5.T2.2.2.2.4.10" class="ltx_td ltx_align_center ltx_border_t">16.0</td>
<td id="S5.T2.2.2.2.4.11" class="ltx_td ltx_align_center ltx_border_t">12.5</td>
<td id="S5.T2.2.2.2.4.12" class="ltx_td ltx_align_center ltx_border_t">9.5</td>
<td id="S5.T2.2.2.2.4.13" class="ltx_td ltx_align_center ltx_border_t">8.5</td>
<td id="S5.T2.2.2.2.4.14" class="ltx_td ltx_align_center ltx_border_t">7.5</td>
<td id="S5.T2.2.2.2.4.15" class="ltx_td ltx_align_center ltx_border_t">8.0</td>
<td id="S5.T2.2.2.2.4.16" class="ltx_td ltx_align_center ltx_border_t">8.0</td>
<td id="S5.T2.2.2.2.4.17" class="ltx_td ltx_align_center ltx_border_t">7.0</td>
<td id="S5.T2.2.2.2.4.18" class="ltx_td ltx_align_center ltx_border_t">25.0</td>
<td id="S5.T2.2.2.2.4.19" class="ltx_td ltx_align_center ltx_border_t">6.5</td>
<td id="S5.T2.2.2.2.4.20" class="ltx_td ltx_align_center ltx_border_t">3.0</td>
<td id="S5.T2.2.2.2.4.21" class="ltx_td ltx_align_center ltx_border_t">28.0</td>
<td id="S5.T2.2.2.2.4.22" class="ltx_td ltx_align_center ltx_border_t">2.0</td>
<td id="S5.T2.2.2.2.4.23" class="ltx_td ltx_align_center ltx_border_t">14</td>
<td id="S5.T2.2.2.2.4.24" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r">CenterPoint<sup id="S5.T2.1.1.1.1.1.1" class="ltx_sup">∗</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S5.T2.1.1.1.1.2" class="ltx_td ltx_align_center">22.0</td>
<td id="S5.T2.1.1.1.1.3" class="ltx_td ltx_align_center">67.6</td>
<td id="S5.T2.1.1.1.1.4" class="ltx_td ltx_align_center">38.9</td>
<td id="S5.T2.1.1.1.1.5" class="ltx_td ltx_align_center">46.5</td>
<td id="S5.T2.1.1.1.1.6" class="ltx_td ltx_align_center">40.1</td>
<td id="S5.T2.1.1.1.1.7" class="ltx_td ltx_align_center">32.2</td>
<td id="S5.T2.1.1.1.1.8" class="ltx_td ltx_align_center">28.6</td>
<td id="S5.T2.1.1.1.1.9" class="ltx_td ltx_align_center">27.4</td>
<td id="S5.T2.1.1.1.1.10" class="ltx_td ltx_align_center">33.4</td>
<td id="S5.T2.1.1.1.1.11" class="ltx_td ltx_align_center">24.5</td>
<td id="S5.T2.1.1.1.1.12" class="ltx_td ltx_align_center">8.7</td>
<td id="S5.T2.1.1.1.1.13" class="ltx_td ltx_align_center">25.8</td>
<td id="S5.T2.1.1.1.1.14" class="ltx_td ltx_align_center">22.6</td>
<td id="S5.T2.1.1.1.1.15" class="ltx_td ltx_align_center">29.5</td>
<td id="S5.T2.1.1.1.1.16" class="ltx_td ltx_align_center">22.4</td>
<td id="S5.T2.1.1.1.1.17" class="ltx_td ltx_align_center">37.4</td>
<td id="S5.T2.1.1.1.1.18" class="ltx_td ltx_align_center">6.3</td>
<td id="S5.T2.1.1.1.1.19" class="ltx_td ltx_align_center">3.9</td>
<td id="S5.T2.1.1.1.1.20" class="ltx_td ltx_align_center">16.9</td>
<td id="S5.T2.1.1.1.1.21" class="ltx_td ltx_align_center">0.5</td>
<td id="S5.T2.1.1.1.1.22" class="ltx_td ltx_align_center">20.1</td>
<td id="S5.T2.1.1.1.1.23" class="ltx_td"></td>
</tr>
<tr id="S5.T2.2.2.2.5" class="ltx_tr">
<td id="S5.T2.2.2.2.5.1" class="ltx_td ltx_align_left ltx_border_r">FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S5.T2.2.2.2.5.2" class="ltx_td ltx_align_center">28.2</td>
<td id="S5.T2.2.2.2.5.3" class="ltx_td ltx_align_center">68.1</td>
<td id="S5.T2.2.2.2.5.4" class="ltx_td ltx_align_center">40.9</td>
<td id="S5.T2.2.2.2.5.5" class="ltx_td ltx_align_center">59.0</td>
<td id="S5.T2.2.2.2.5.6" class="ltx_td ltx_align_center">38.5</td>
<td id="S5.T2.2.2.2.5.7" class="ltx_td ltx_align_center">42.6</td>
<td id="S5.T2.2.2.2.5.8" class="ltx_td ltx_align_center">39.7</td>
<td id="S5.T2.2.2.2.5.9" class="ltx_td ltx_align_center">26.2</td>
<td id="S5.T2.2.2.2.5.10" class="ltx_td ltx_align_center">49.0</td>
<td id="S5.T2.2.2.2.5.11" class="ltx_td ltx_align_center">38.6</td>
<td id="S5.T2.2.2.2.5.12" class="ltx_td ltx_align_center">20.4</td>
<td id="S5.T2.2.2.2.5.13" class="ltx_td ltx_align_center">30.5</td>
<td id="S5.T2.2.2.2.5.14" class="ltx_td ltx_align_center">14.8</td>
<td id="S5.T2.2.2.2.5.15" class="ltx_td ltx_align_center">41.2</td>
<td id="S5.T2.2.2.2.5.16" class="ltx_td ltx_align_center">26.9</td>
<td id="S5.T2.2.2.2.5.17" class="ltx_td ltx_align_center">41.8</td>
<td id="S5.T2.2.2.2.5.18" class="ltx_td ltx_align_center">11.9</td>
<td id="S5.T2.2.2.2.5.19" class="ltx_td ltx_align_center">5.9</td>
<td id="S5.T2.2.2.2.5.20" class="ltx_td ltx_align_center">29.0</td>
<td id="S5.T2.2.2.2.5.21" class="ltx_td ltx_align_center">13.8</td>
<td id="S5.T2.2.2.2.5.22" class="ltx_td ltx_align_center">33.4</td>
<td id="S5.T2.2.2.2.5.23" class="ltx_td"></td>
</tr>
<tr id="S5.T2.2.2.2.6" class="ltx_tr">
<td id="S5.T2.2.2.2.6.1" class="ltx_td ltx_align_left ltx_border_r">VoxelNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S5.T2.2.2.2.6.2" class="ltx_td ltx_align_center">30.5</td>
<td id="S5.T2.2.2.2.6.3" class="ltx_td ltx_align_center">72.0</td>
<td id="S5.T2.2.2.2.6.4" class="ltx_td ltx_align_center">39.7</td>
<td id="S5.T2.2.2.2.6.5" class="ltx_td ltx_align_center">63.2</td>
<td id="S5.T2.2.2.2.6.6" class="ltx_td ltx_align_center">39.7</td>
<td id="S5.T2.2.2.2.6.7" class="ltx_td ltx_align_center">64.5</td>
<td id="S5.T2.2.2.2.6.8" class="ltx_td ltx_align_center">46.0</td>
<td id="S5.T2.2.2.2.6.9" class="ltx_td ltx_align_center">34.8</td>
<td id="S5.T2.2.2.2.6.10" class="ltx_td ltx_align_center">44.9</td>
<td id="S5.T2.2.2.2.6.11" class="ltx_td ltx_align_center">40.7</td>
<td id="S5.T2.2.2.2.6.12" class="ltx_td ltx_align_center">21.0</td>
<td id="S5.T2.2.2.2.6.13" class="ltx_td ltx_align_center">27.0</td>
<td id="S5.T2.2.2.2.6.14" class="ltx_td ltx_align_center">18.4</td>
<td id="S5.T2.2.2.2.6.15" class="ltx_td ltx_align_center">44.5</td>
<td id="S5.T2.2.2.2.6.16" class="ltx_td ltx_align_center">22.2</td>
<td id="S5.T2.2.2.2.6.17" class="ltx_td ltx_align_center">53.7</td>
<td id="S5.T2.2.2.2.6.18" class="ltx_td ltx_align_center">15.6</td>
<td id="S5.T2.2.2.2.6.19" class="ltx_td ltx_align_center">7.3</td>
<td id="S5.T2.2.2.2.6.20" class="ltx_td ltx_align_center">40.1</td>
<td id="S5.T2.2.2.2.6.21" class="ltx_td ltx_align_center">11.1</td>
<td id="S5.T2.2.2.2.6.22" class="ltx_td ltx_align_center">34.9</td>
<td id="S5.T2.2.2.2.6.23" class="ltx_td"></td>
</tr>
<tr id="S5.T2.2.2.2.7" class="ltx_tr">
<td id="S5.T2.2.2.2.7.1" class="ltx_td ltx_border_r"></td>
<td id="S5.T2.2.2.2.7.2" class="ltx_td ltx_align_left ltx_border_r">FSF (Ours)</td>
<td id="S5.T2.2.2.2.7.3" class="ltx_td ltx_align_center"><span id="S5.T2.2.2.2.7.3.1" class="ltx_text ltx_font_bold">33.2</span></td>
<td id="S5.T2.2.2.2.7.4" class="ltx_td ltx_align_center">70.8</td>
<td id="S5.T2.2.2.2.7.5" class="ltx_td ltx_align_center">44.1</td>
<td id="S5.T2.2.2.2.7.6" class="ltx_td ltx_align_center">60.8</td>
<td id="S5.T2.2.2.2.7.7" class="ltx_td ltx_align_center">40.2</td>
<td id="S5.T2.2.2.2.7.8" class="ltx_td ltx_align_center">50.9</td>
<td id="S5.T2.2.2.2.7.9" class="ltx_td ltx_align_center">48.9</td>
<td id="S5.T2.2.2.2.7.10" class="ltx_td ltx_align_center">28.3</td>
<td id="S5.T2.2.2.2.7.11" class="ltx_td ltx_align_center">60.9</td>
<td id="S5.T2.2.2.2.7.12" class="ltx_td ltx_align_center">47.6</td>
<td id="S5.T2.2.2.2.7.13" class="ltx_td ltx_align_center">22.7</td>
<td id="S5.T2.2.2.2.7.14" class="ltx_td ltx_align_center">36.1</td>
<td id="S5.T2.2.2.2.7.15" class="ltx_td ltx_align_center">26.7</td>
<td id="S5.T2.2.2.2.7.16" class="ltx_td ltx_align_center">51.7</td>
<td id="S5.T2.2.2.2.7.17" class="ltx_td ltx_align_center">28.1</td>
<td id="S5.T2.2.2.2.7.18" class="ltx_td ltx_align_center">41.1</td>
<td id="S5.T2.2.2.2.7.19" class="ltx_td ltx_align_center">12.2</td>
<td id="S5.T2.2.2.2.7.20" class="ltx_td ltx_align_center">6.8</td>
<td id="S5.T2.2.2.2.7.21" class="ltx_td ltx_align_center">27.7</td>
<td id="S5.T2.2.2.2.7.22" class="ltx_td ltx_align_center">25.0</td>
<td id="S5.T2.2.2.2.7.23" class="ltx_td ltx_align_center">41.6</td>
<td id="S5.T2.2.2.2.7.24" class="ltx_td"></td>
</tr>
<tr id="S5.T2.2.2.2.2" class="ltx_tr">
<td id="S5.T2.2.2.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="3"><span id="S5.T2.2.2.2.2.2.1" class="ltx_text">CDS</span></td>
<td id="S5.T2.2.2.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">CenterPoint<sup id="S5.T2.2.2.2.2.1.1" class="ltx_sup">∗</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S5.T2.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">17.6</td>
<td id="S5.T2.2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">57.2</td>
<td id="S5.T2.2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t">32.0</td>
<td id="S5.T2.2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">35.7</td>
<td id="S5.T2.2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t">31.0</td>
<td id="S5.T2.2.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t">25.6</td>
<td id="S5.T2.2.2.2.2.9" class="ltx_td ltx_align_center ltx_border_t">22.2</td>
<td id="S5.T2.2.2.2.2.10" class="ltx_td ltx_align_center ltx_border_t">19.1</td>
<td id="S5.T2.2.2.2.2.11" class="ltx_td ltx_align_center ltx_border_t">28.2</td>
<td id="S5.T2.2.2.2.2.12" class="ltx_td ltx_align_center ltx_border_t">19.6</td>
<td id="S5.T2.2.2.2.2.13" class="ltx_td ltx_align_center ltx_border_t">6.8</td>
<td id="S5.T2.2.2.2.2.14" class="ltx_td ltx_align_center ltx_border_t">22.5</td>
<td id="S5.T2.2.2.2.2.15" class="ltx_td ltx_align_center ltx_border_t">17.4</td>
<td id="S5.T2.2.2.2.2.16" class="ltx_td ltx_align_center ltx_border_t">22.4</td>
<td id="S5.T2.2.2.2.2.17" class="ltx_td ltx_align_center ltx_border_t">17.2</td>
<td id="S5.T2.2.2.2.2.18" class="ltx_td ltx_align_center ltx_border_t">28.9</td>
<td id="S5.T2.2.2.2.2.19" class="ltx_td ltx_align_center ltx_border_t">4.8</td>
<td id="S5.T2.2.2.2.2.20" class="ltx_td ltx_align_center ltx_border_t">3.0</td>
<td id="S5.T2.2.2.2.2.21" class="ltx_td ltx_align_center ltx_border_t">13.2</td>
<td id="S5.T2.2.2.2.2.22" class="ltx_td ltx_align_center ltx_border_t">0.4</td>
<td id="S5.T2.2.2.2.2.23" class="ltx_td ltx_align_center ltx_border_t">16.7</td>
<td id="S5.T2.2.2.2.2.24" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T2.2.2.2.8" class="ltx_tr">
<td id="S5.T2.2.2.2.8.1" class="ltx_td ltx_align_left ltx_border_r">FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S5.T2.2.2.2.8.2" class="ltx_td ltx_align_center">22.7</td>
<td id="S5.T2.2.2.2.8.3" class="ltx_td ltx_align_center">57.7</td>
<td id="S5.T2.2.2.2.8.4" class="ltx_td ltx_align_center">34.2</td>
<td id="S5.T2.2.2.2.8.5" class="ltx_td ltx_align_center">47.5</td>
<td id="S5.T2.2.2.2.8.6" class="ltx_td ltx_align_center">31.7</td>
<td id="S5.T2.2.2.2.8.7" class="ltx_td ltx_align_center">34.4</td>
<td id="S5.T2.2.2.2.8.8" class="ltx_td ltx_align_center">32.3</td>
<td id="S5.T2.2.2.2.8.9" class="ltx_td ltx_align_center">18.0</td>
<td id="S5.T2.2.2.2.8.10" class="ltx_td ltx_align_center">41.4</td>
<td id="S5.T2.2.2.2.8.11" class="ltx_td ltx_align_center">32.0</td>
<td id="S5.T2.2.2.2.8.12" class="ltx_td ltx_align_center">15.9</td>
<td id="S5.T2.2.2.2.8.13" class="ltx_td ltx_align_center">26.1</td>
<td id="S5.T2.2.2.2.8.14" class="ltx_td ltx_align_center">11.0</td>
<td id="S5.T2.2.2.2.8.15" class="ltx_td ltx_align_center">30.7</td>
<td id="S5.T2.2.2.2.8.16" class="ltx_td ltx_align_center">20.5</td>
<td id="S5.T2.2.2.2.8.17" class="ltx_td ltx_align_center">30.9</td>
<td id="S5.T2.2.2.2.8.18" class="ltx_td ltx_align_center">9.5</td>
<td id="S5.T2.2.2.2.8.19" class="ltx_td ltx_align_center">4.4</td>
<td id="S5.T2.2.2.2.8.20" class="ltx_td ltx_align_center">23.4</td>
<td id="S5.T2.2.2.2.8.21" class="ltx_td ltx_align_center">11.5</td>
<td id="S5.T2.2.2.2.8.22" class="ltx_td ltx_align_center">28.0</td>
<td id="S5.T2.2.2.2.8.23" class="ltx_td"></td>
</tr>
<tr id="S5.T2.2.2.2.9" class="ltx_tr">
<td id="S5.T2.2.2.2.9.1" class="ltx_td ltx_align_left ltx_border_r">VoxelNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S5.T2.2.2.2.9.2" class="ltx_td ltx_align_center">23.0</td>
<td id="S5.T2.2.2.2.9.3" class="ltx_td ltx_align_center">57.7</td>
<td id="S5.T2.2.2.2.9.4" class="ltx_td ltx_align_center">30.3</td>
<td id="S5.T2.2.2.2.9.5" class="ltx_td ltx_align_center">45.5</td>
<td id="S5.T2.2.2.2.9.6" class="ltx_td ltx_align_center">31.6</td>
<td id="S5.T2.2.2.2.9.7" class="ltx_td ltx_align_center">50.5</td>
<td id="S5.T2.2.2.2.9.8" class="ltx_td ltx_align_center">33.8</td>
<td id="S5.T2.2.2.2.9.9" class="ltx_td ltx_align_center">25.1</td>
<td id="S5.T2.2.2.2.9.10" class="ltx_td ltx_align_center">34.3</td>
<td id="S5.T2.2.2.2.9.11" class="ltx_td ltx_align_center">30.5</td>
<td id="S5.T2.2.2.2.9.12" class="ltx_td ltx_align_center">15.5</td>
<td id="S5.T2.2.2.2.9.13" class="ltx_td ltx_align_center">22.2</td>
<td id="S5.T2.2.2.2.9.14" class="ltx_td ltx_align_center">13.6</td>
<td id="S5.T2.2.2.2.9.15" class="ltx_td ltx_align_center">32.5</td>
<td id="S5.T2.2.2.2.9.16" class="ltx_td ltx_align_center">15.1</td>
<td id="S5.T2.2.2.2.9.17" class="ltx_td ltx_align_center">38.4</td>
<td id="S5.T2.2.2.2.9.18" class="ltx_td ltx_align_center">11.8</td>
<td id="S5.T2.2.2.2.9.19" class="ltx_td ltx_align_center">5.2</td>
<td id="S5.T2.2.2.2.9.20" class="ltx_td ltx_align_center">30.0</td>
<td id="S5.T2.2.2.2.9.21" class="ltx_td ltx_align_center">8.9</td>
<td id="S5.T2.2.2.2.9.22" class="ltx_td ltx_align_center">25.7</td>
<td id="S5.T2.2.2.2.9.23" class="ltx_td"></td>
</tr>
<tr id="S5.T2.2.2.2.10" class="ltx_tr">
<td id="S5.T2.2.2.2.10.1" class="ltx_td ltx_border_bb ltx_border_b ltx_border_r"></td>
<td id="S5.T2.2.2.2.10.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_r">FSF (Ours)</td>
<td id="S5.T2.2.2.2.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b"><span id="S5.T2.2.2.2.10.3.1" class="ltx_text ltx_font_bold">25.5</span></td>
<td id="S5.T2.2.2.2.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">59.6</td>
<td id="S5.T2.2.2.2.10.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">35.6</td>
<td id="S5.T2.2.2.2.10.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">48.5</td>
<td id="S5.T2.2.2.2.10.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">32.1</td>
<td id="S5.T2.2.2.2.10.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">40.1</td>
<td id="S5.T2.2.2.2.10.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">35.9</td>
<td id="S5.T2.2.2.2.10.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">19.1</td>
<td id="S5.T2.2.2.2.10.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">48.9</td>
<td id="S5.T2.2.2.2.10.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">37.2</td>
<td id="S5.T2.2.2.2.10.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">17.2</td>
<td id="S5.T2.2.2.2.10.14" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">29.5</td>
<td id="S5.T2.2.2.2.10.15" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">19.6</td>
<td id="S5.T2.2.2.2.10.16" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">37.3</td>
<td id="S5.T2.2.2.2.10.17" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">21.0</td>
<td id="S5.T2.2.2.2.10.18" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">29.9</td>
<td id="S5.T2.2.2.2.10.19" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">9.2</td>
<td id="S5.T2.2.2.2.10.20" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">4.9</td>
<td id="S5.T2.2.2.2.10.21" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">21.8</td>
<td id="S5.T2.2.2.2.10.22" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">18.5</td>
<td id="S5.T2.2.2.2.10.23" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">32.0</td>
<td id="S5.T2.2.2.2.10.24" class="ltx_td ltx_border_bb ltx_border_b"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>
Comparison with state-of-the-art methods on Argoverse 2 validation split.
C-Barrel: construction barrel.
MPC-Sign: mobile pedestrian crossing sign.
A-Bus: articulated bus.
C-Cone: construction cone.
V-Trailer: vehicular trailer.
‡: provided by authors of AV2 dataset.
<sup id="S5.T2.6.1" class="ltx_sup">∗</sup>: reimplemented by FSD.
Some categories are excluded from the table due to the limited number of instances they contain.
However, the average results consider all categories, even those that are omitted.
</figcaption>
</figure>
<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Dataset: nuScenes</h4>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p1.1" class="ltx_p">Our experiment is mainly conducted on the nuScenes dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, which provides a total of 1000 scenes.
Each scene is about 20 seconds long and contains collected data from 32-beam LiDAR along with 6 cameras.
The effective detection range of nuScenes covers a <math id="S5.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="108m\times 108m" display="inline"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mrow id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml"><mrow id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.cmml"><mn id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.2" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.2.cmml">108</mn><mo lspace="0em" rspace="0em" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.1.cmml">​</mo><mi id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.3" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">×</mo><mn id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.3" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.3.cmml">108</mn></mrow><mo lspace="0em" rspace="0em" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1"><times id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1"></times><apply id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2"><times id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.1"></times><apply id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2"><times id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.1"></times><cn type="integer" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.2.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.2">108</cn><ci id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.3.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.3">𝑚</ci></apply><cn type="integer" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.3.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.3">108</cn></apply><ci id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">108m\times 108m</annotation></semantics></math> area, and annotations are divided into 10 categories.
In order to measure the performance in this dataset, the official metrics of nuScenes, Mean Average Precision (mAP) and nuScenes Detection Score (NDS) are utilized.
The NDS is the weighted average of mAP, mATE (translation error), mASE (scale error), mAOE (orientation error), mAVE (velocity error), and mAAE (attribute error).</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Dataset: Argoverse 2</h4>

<div id="S5.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p1.2" class="ltx_p">To demonstrate our superiority in long-range detection, we proceed to conduct experiments on the Argoverse 2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, abbreviated as AV2.
The AV2 contains 150k annotated frames, 5<math id="S5.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.SSS0.Px2.p1.1.m1.1a"><mo id="S5.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.1.m1.1b"><times id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.1.m1.1c">\times</annotation></semantics></math> larger than nuScenes.
AV2 employs two 32-beam LiDARs to form a 64-beam LiDAR, combined with 7 surrounding cameras.
The valid detection distance of AV2 is 200m (covering <math id="S5.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="400m\times 400m" display="inline"><semantics id="S5.SS1.SSS0.Px2.p1.2.m2.1a"><mrow id="S5.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mrow id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml"><mrow id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.cmml"><mn id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.2" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.2.cmml">400</mn><mo lspace="0em" rspace="0em" id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.1" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.1.cmml">​</mo><mi id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.3" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.1" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.1.cmml">×</mo><mn id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.3" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.cmml">400</mn></mrow><mo lspace="0em" rspace="0em" id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1"><times id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1"></times><apply id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2"><times id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.1"></times><apply id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2"><times id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.1"></times><cn type="integer" id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.2.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.2">400</cn><ci id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.3.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.3">𝑚</ci></apply><cn type="integer" id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.3">400</cn></apply><ci id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.2.m2.1c">400m\times 400m</annotation></semantics></math> area).
In addition, AV2 contains 30 classes, exhibiting challenging long-tail distribution.
As far as metrics are concerned, in addition to the Mean Average Precision (mAP), AV2 provides a Composite Detection Score (CDS) benchmark, which takes mAP, mATE, mASE, and mAOE into account.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Model</h4>

<div id="S5.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px3.p1.1" class="ltx_p">As for LiDAR, following FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, we adopt a sparse-convolution-based U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> as the LiDAR backbone.
The voxel size is set to <math id="S5.SS1.SSS0.Px3.p1.1.m1.3" class="ltx_Math" alttext="[0.2m,0.2m,0.2m]" display="inline"><semantics id="S5.SS1.SSS0.Px3.p1.1.m1.3a"><mrow id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.4.cmml"><mo stretchy="false" id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.4" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.4.cmml">[</mo><mrow id="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.cmml"><mn id="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.2" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.2.cmml">0.2</mn><mo lspace="0em" rspace="0em" id="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.1" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.3" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.3.cmml">m</mi></mrow><mo id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.5" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.4.cmml">,</mo><mrow id="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2" xref="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.cmml"><mn id="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.2" xref="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.2.cmml">0.2</mn><mo lspace="0em" rspace="0em" id="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.1" xref="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.3" xref="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.3.cmml">m</mi></mrow><mo id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.6" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.4.cmml">,</mo><mrow id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.cmml"><mn id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.2" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.2.cmml">0.2</mn><mo lspace="0em" rspace="0em" id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.1" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.cmml">m</mi></mrow><mo stretchy="false" id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.7" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.4.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px3.p1.1.m1.3b"><list id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.4.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3"><apply id="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1"><times id="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.1.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.1"></times><cn type="float" id="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.2.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.2">0.2</cn><ci id="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.3.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.3">𝑚</ci></apply><apply id="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2"><times id="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.1.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.1"></times><cn type="float" id="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.2.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.2">0.2</cn><ci id="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.3.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.3">𝑚</ci></apply><apply id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3"><times id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.1.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.1"></times><cn type="float" id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.2.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.2">0.2</cn><ci id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.3.3.3">𝑚</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px3.p1.1.m1.3c">[0.2m,0.2m,0.2m]</annotation></semantics></math>.
With respect to 2D instance segmentation, we use HybridTaskCascade (HTC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> pre-trained on nuImages.
For the SIR module that utilized for instance feature extraction, we use the same setting as FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Training Scheme</h4>

<div id="S5.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px4.p1.2" class="ltx_p">Next, we present the training strategy.
Consistent with previous methods<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, we first train the LiDAR-based detector.
We pre-train FSD on nuScenes for 20 epochs with CopyPaste augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.
Next, initializing from FSD, we train our model for 6 epochs with CBGS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.
The optimizer is AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.
We adopt the one-cycle learning rate policy, with a maximum learning rate of <math id="S5.SS1.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S5.SS1.SSS0.Px4.p1.1.m1.1a"><msup id="S5.SS1.SSS0.Px4.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1.cmml"><mn id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.2" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml"><mo id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3a" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml">−</mo><mn id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3.2" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px4.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1.2">10</cn><apply id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3"><minus id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3.1.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3"></minus><cn type="integer" id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px4.p1.1.m1.1c">10^{-4}</annotation></semantics></math> and a weight decay of <math id="S5.SS1.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="10^{-2}" display="inline"><semantics id="S5.SS1.SSS0.Px4.p1.2.m2.1a"><msup id="S5.SS1.SSS0.Px4.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px4.p1.2.m2.1.1.cmml"><mn id="S5.SS1.SSS0.Px4.p1.2.m2.1.1.2" xref="S5.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml">10</mn><mrow id="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3" xref="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml"><mo id="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3a" xref="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml">−</mo><mn id="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3.2" xref="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3.2.cmml">2</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px4.p1.2.m2.1b"><apply id="S5.SS1.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS0.Px4.p1.2.m2.1.1">superscript</csymbol><cn type="integer" id="S5.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S5.SS1.SSS0.Px4.p1.2.m2.1.1.2">10</cn><apply id="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3"><minus id="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3.1.cmml" xref="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3"></minus><cn type="integer" id="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3.2.cmml" xref="S5.SS1.SSS0.Px4.p1.2.m2.1.1.3.2">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px4.p1.2.m2.1c">10^{-2}</annotation></semantics></math>. The model is trained on eight NVIDIA RTX 3090 with batch size of 8.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparison with State-of-the-art Methods</h3>

<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">nuScenes</h4>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">As shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.4 Query Label Assignment ‣ 4 Methodology ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, FSF surpasses all previous multi-modal 3D detection methods in the official NDS metric.
It is noteworthy that state-of-the-art BEVFusion and DeepInteraction have inferior performance to FSF, even they <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> utilize a stronger LiDAR-only detector (TransFusion-L).</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Argoverse 2</h4>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p1.1" class="ltx_p">AV2 has a much larger perception range than nuScenes, making it a suitable testbed to demonstrate our superiority in long-range fusion.
The results in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Setup ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> show that FSF consistently outperforms previous LiDAR-based state-of-the-art methods by large margins.
Notably, we observe an 8 to 10 mAP increase in categories with relatively small sizes, such as Traffic Cone, Motorcycle, and Bicycle.
Although previous multi-modal methods (e.g., BEVFusion) achieve competitive results on nuScenes, the training cost of their dense BEV feature maps are not affordable on AV2 dataset. As a result, we omit their results on AV2.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Alternatives to FSF</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">As mentioned in §<a href="#S1" title="1 Introduction ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, a couple of fusion methods can be adapted to fully sparse architecture.
So, in this section, we reproduce these alternatives and conduct a comprehensive performance comparison between them and FSF.</p>
</div>
<section id="S5.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Point Painting</h4>

<div id="S5.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px1.p1.1" class="ltx_p">The most straightforward approach is using PointPainting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, where the 2D semantic scores are painted onto each point.
The results in Table <a href="#S5.T3" title="Table 3 ‣ Discussion: why is FSF superior to the alternatives? ‣ 5.3 Alternatives to FSF ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show simple point painting could obtain considerable improvement but is largely inferior to FSF.</p>
</div>
</section>
<section id="S5.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Feature Painting</h4>

<div id="S5.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px2.p1.1" class="ltx_p">Feature painting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> alternatively paints image features to the point cloud.
Features contain rich information than semantic masks.
To reproduce a fair feature painting baseline with FSD, we utilize the multi-level image features from FPN of the HTC.
The points are then decorated with these image features.
Compared with score painting, feature painting has an improvement of 0.4 mAP.</p>
</div>
</section>
<section id="S5.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Virtual Point</h4>

<div id="S5.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px3.p1.1" class="ltx_p">Similar to FSF, MVP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> utilizes 2D instance segmentation to augment point clouds by adding additional virtual points on each instance.
Then the augmented point clouds are sent into a LiDAR-based detector.
We use the official code to generate virtual points.
Table <a href="#S5.T3" title="Table 3 ‣ Discussion: why is FSF superior to the alternatives? ‣ 5.3 Alternatives to FSF ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrates that adopting MVP achieves better performance than the painting method above, but is still worse than ours.</p>
</div>
</section>
<section id="S5.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Discussion: why is FSF superior to the alternatives?</h4>

<div id="S5.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px4.p1.1" class="ltx_p">Although all three alternatives achieve considerable improvement on the LiDAR baseline, they are significantly worse than FSF.
We owe the superior performance of FSF to two aspects:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">FSF not only enriches semantic information but also explicitly leverages instance information to correct the mistaken 3D instance segmentation, which might mix the crowded instances together as Fig. <a href="#S4.F3" title="Figure 3 ‣ Camera Queries ‣ 4.2 Bi-modal Query Generation ‣ 4 Methodology ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">We align camera queries and LiDAR queries in a consistent form, so the prediction head could be shared between these two modalities, unleashing the potential of camera queries.</p>
</div>
</li>
</ul>
</div>
<figure id="S5.T3" class="ltx_table">
<div id="S5.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:173.4pt;height:55.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-81.3pt,26.1pt) scale(0.516092436642858,0.516092436642858) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Method</td>
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Modality</td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">mAP</td>
<td id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">NDS</td>
</tr>
<tr id="S5.T3.1.1.2" class="ltx_tr">
<td id="S5.T3.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S5.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L</td>
<td id="S5.T3.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">62.5</td>
<td id="S5.T3.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">68.7</td>
</tr>
<tr id="S5.T3.1.1.3" class="ltx_tr">
<td id="S5.T3.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">FSD + PointPainting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S5.T3.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L+C</td>
<td id="S5.T3.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">66.9</td>
<td id="S5.T3.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">70.9</td>
</tr>
<tr id="S5.T3.1.1.4" class="ltx_tr">
<td id="S5.T3.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r">FSD + Feature Painting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S5.T3.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r">L+C</td>
<td id="S5.T3.1.1.4.3" class="ltx_td ltx_align_center">67.3</td>
<td id="S5.T3.1.1.4.4" class="ltx_td ltx_align_center">71.2</td>
</tr>
<tr id="S5.T3.1.1.5" class="ltx_tr">
<td id="S5.T3.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r">FSD + MVP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S5.T3.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r">L+C</td>
<td id="S5.T3.1.1.5.3" class="ltx_td ltx_align_center">67.6</td>
<td id="S5.T3.1.1.5.4" class="ltx_td ltx_align_center">71.5</td>
</tr>
<tr id="S5.T3.1.1.6" class="ltx_tr">
<td id="S5.T3.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">FSF</td>
<td id="S5.T3.1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">L+C</td>
<td id="S5.T3.1.1.6.3" class="ltx_td ltx_align_center ltx_border_bb">70.4</td>
<td id="S5.T3.1.1.6.4" class="ltx_td ltx_align_center ltx_border_bb">72.7</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Different fusion strategies for sparse detection architecture.
PointPainting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>: painting semantic scores.
Feature Painting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>: painting the image features.
MVP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>: employing virtual points to enhance point clouds. </figcaption>
</figure>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Long Range Fusion</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">FSF achieves state-of-the-art performance on Argoverse 2 dataset, which has a very large perception range (<math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="400m\times 400m" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><mrow id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mrow id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml"><mrow id="S5.SS4.p1.1.m1.1.1.2.2" xref="S5.SS4.p1.1.m1.1.1.2.2.cmml"><mn id="S5.SS4.p1.1.m1.1.1.2.2.2" xref="S5.SS4.p1.1.m1.1.1.2.2.2.cmml">400</mn><mo lspace="0em" rspace="0em" id="S5.SS4.p1.1.m1.1.1.2.2.1" xref="S5.SS4.p1.1.m1.1.1.2.2.1.cmml">​</mo><mi id="S5.SS4.p1.1.m1.1.1.2.2.3" xref="S5.SS4.p1.1.m1.1.1.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.p1.1.m1.1.1.2.1" xref="S5.SS4.p1.1.m1.1.1.2.1.cmml">×</mo><mn id="S5.SS4.p1.1.m1.1.1.2.3" xref="S5.SS4.p1.1.m1.1.1.2.3.cmml">400</mn></mrow><mo lspace="0em" rspace="0em" id="S5.SS4.p1.1.m1.1.1.1" xref="S5.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.1.m1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><times id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1"></times><apply id="S5.SS4.p1.1.m1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2"><times id="S5.SS4.p1.1.m1.1.1.2.1.cmml" xref="S5.SS4.p1.1.m1.1.1.2.1"></times><apply id="S5.SS4.p1.1.m1.1.1.2.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2.2"><times id="S5.SS4.p1.1.m1.1.1.2.2.1.cmml" xref="S5.SS4.p1.1.m1.1.1.2.2.1"></times><cn type="integer" id="S5.SS4.p1.1.m1.1.1.2.2.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2.2.2">400</cn><ci id="S5.SS4.p1.1.m1.1.1.2.2.3.cmml" xref="S5.SS4.p1.1.m1.1.1.2.2.3">𝑚</ci></apply><cn type="integer" id="S5.SS4.p1.1.m1.1.1.2.3.cmml" xref="S5.SS4.p1.1.m1.1.1.2.3">400</cn></apply><ci id="S5.SS4.p1.1.m1.1.1.3.cmml" xref="S5.SS4.p1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">400m\times 400m</annotation></semantics></math>).
Here we emphasize the superiority of FSF in long-range fusion by evaluating the inference latency and range-conditioned performance.
The latency is tested on RTX 3090 with batch size 1. Our code is based on MMDetection3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and the IO time is excluded.</p>
</div>
<section id="S5.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Latency and Memory Footprint</h4>

<div id="S5.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS4.SSS0.Px1.p1.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 ‣ Latency and Memory Footprint ‣ 5.4 Long Range Fusion ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows their computational and memory cost.
FSF shows remarkable efficiency advantages over other state-of-the-art multi-modal methods since FSF does not incorporate any dense BEV feature maps.
In contrast, previous art TransFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> not only applies 2D convolutions on dense BEV feature maps, but also adopts global cross attention between queries and the whole BEV feature maps, leading to unacceptable latency.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<div id="S5.T4.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:190.8pt;height:60pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-76.3pt,24.0pt) scale(0.555557377220348,0.555557377220348) ;">
<table id="S5.T4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.3.1.1" class="ltx_tr">
<td id="S5.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Method</td>
<td id="S5.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Modality</td>
<td id="S5.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Latency (ms)</td>
<td id="S5.T4.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Memory (GB)</td>
</tr>
<tr id="S5.T4.3.1.2" class="ltx_tr">
<td id="S5.T4.3.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">TransFusion-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S5.T4.3.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L</td>
<td id="S5.T4.3.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">320</td>
<td id="S5.T4.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">14.5</td>
</tr>
<tr id="S5.T4.3.1.3" class="ltx_tr">
<td id="S5.T4.3.1.3.1" class="ltx_td ltx_align_left ltx_border_r">CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S5.T4.3.1.3.2" class="ltx_td ltx_align_center ltx_border_r">L</td>
<td id="S5.T4.3.1.3.3" class="ltx_td ltx_align_center ltx_border_r">232</td>
<td id="S5.T4.3.1.3.4" class="ltx_td ltx_align_center">8.1</td>
</tr>
<tr id="S5.T4.3.1.4" class="ltx_tr">
<td id="S5.T4.3.1.4.1" class="ltx_td ltx_align_left ltx_border_r">FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S5.T4.3.1.4.2" class="ltx_td ltx_align_center ltx_border_r">L</td>
<td id="S5.T4.3.1.4.3" class="ltx_td ltx_align_center ltx_border_r">97</td>
<td id="S5.T4.3.1.4.4" class="ltx_td ltx_align_center">3.8</td>
</tr>
<tr id="S5.T4.3.1.5" class="ltx_tr">
<td id="S5.T4.3.1.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">TransFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S5.T4.3.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">C+L</td>
<td id="S5.T4.3.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">384</td>
<td id="S5.T4.3.1.5.4" class="ltx_td ltx_align_center ltx_border_t">17.3</td>
</tr>
<tr id="S5.T4.3.1.6" class="ltx_tr">
<td id="S5.T4.3.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Ours</td>
<td id="S5.T4.3.1.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">C+L</td>
<td id="S5.T4.3.1.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">141</td>
<td id="S5.T4.3.1.6.4" class="ltx_td ltx_align_center ltx_border_bb">6.9</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>The inference latency and memory footprint of Argoverse 2 dataset, which covers <math id="S5.T4.2.m1.1" class="ltx_Math" alttext="400m\times 400m" display="inline"><semantics id="S5.T4.2.m1.1b"><mrow id="S5.T4.2.m1.1.1" xref="S5.T4.2.m1.1.1.cmml"><mrow id="S5.T4.2.m1.1.1.2" xref="S5.T4.2.m1.1.1.2.cmml"><mrow id="S5.T4.2.m1.1.1.2.2" xref="S5.T4.2.m1.1.1.2.2.cmml"><mn id="S5.T4.2.m1.1.1.2.2.2" xref="S5.T4.2.m1.1.1.2.2.2.cmml">400</mn><mo lspace="0em" rspace="0em" id="S5.T4.2.m1.1.1.2.2.1" xref="S5.T4.2.m1.1.1.2.2.1.cmml">​</mo><mi id="S5.T4.2.m1.1.1.2.2.3" xref="S5.T4.2.m1.1.1.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S5.T4.2.m1.1.1.2.1" xref="S5.T4.2.m1.1.1.2.1.cmml">×</mo><mn id="S5.T4.2.m1.1.1.2.3" xref="S5.T4.2.m1.1.1.2.3.cmml">400</mn></mrow><mo lspace="0em" rspace="0em" id="S5.T4.2.m1.1.1.1" xref="S5.T4.2.m1.1.1.1.cmml">​</mo><mi id="S5.T4.2.m1.1.1.3" xref="S5.T4.2.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.2.m1.1c"><apply id="S5.T4.2.m1.1.1.cmml" xref="S5.T4.2.m1.1.1"><times id="S5.T4.2.m1.1.1.1.cmml" xref="S5.T4.2.m1.1.1.1"></times><apply id="S5.T4.2.m1.1.1.2.cmml" xref="S5.T4.2.m1.1.1.2"><times id="S5.T4.2.m1.1.1.2.1.cmml" xref="S5.T4.2.m1.1.1.2.1"></times><apply id="S5.T4.2.m1.1.1.2.2.cmml" xref="S5.T4.2.m1.1.1.2.2"><times id="S5.T4.2.m1.1.1.2.2.1.cmml" xref="S5.T4.2.m1.1.1.2.2.1"></times><cn type="integer" id="S5.T4.2.m1.1.1.2.2.2.cmml" xref="S5.T4.2.m1.1.1.2.2.2">400</cn><ci id="S5.T4.2.m1.1.1.2.2.3.cmml" xref="S5.T4.2.m1.1.1.2.2.3">𝑚</ci></apply><cn type="integer" id="S5.T4.2.m1.1.1.2.3.cmml" xref="S5.T4.2.m1.1.1.2.3">400</cn></apply><ci id="S5.T4.2.m1.1.1.3.cmml" xref="S5.T4.2.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.m1.1d">400m\times 400m</annotation></semantics></math> area. We are even faster and use less memory than the BEV feature map based LiDAR-only methods.</figcaption>
</figure>
<div id="S5.SS4.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS4.SSS0.Px1.p2.1" class="ltx_p">In addition, as evidenced in Table <a href="#S5.T4" title="Table 4 ‣ Latency and Memory Footprint ‣ 5.4 Long Range Fusion ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, despite the need to process seven images, our approach remains faster and requires less memory footprint than LiDAR-only methods like CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Range-conditioned Performance</h4>

<div id="S5.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS4.SSS0.Px2.p1.1" class="ltx_p">Table <a href="#S5.T5" title="Table 5 ‣ Range-conditioned Performance ‣ 5.4 Long Range Fusion ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrates the performance conditioned on different perception ranges.
The image is dense and of high resolution, making it helpful for detecting distant objects.
As shown in Table <a href="#S5.T5" title="Table 5 ‣ Range-conditioned Performance ‣ 5.4 Long Range Fusion ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, in the range spanning from 50m to 100m, we observe a remarkable improvement in mAP specifically for small objects, such as motorcyclists and bicyclists.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<div id="S5.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:40.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-83.9pt,15.7pt) scale(0.563589252867009,0.563589252867009) ;">
<table id="S5.T5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<td id="S5.T5.1.1.1.1" class="ltx_td ltx_border_r ltx_border_tt" rowspan="2"></td>
<td id="S5.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T5.1.1.1.2.1" class="ltx_text"><span id="S5.T5.1.1.1.2.1.1" class="ltx_text"></span> <span id="S5.T5.1.1.1.2.1.2" class="ltx_text">
<span id="S5.T5.1.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T5.1.1.1.2.1.2.1.1" class="ltx_tr">
<span id="S5.T5.1.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Overall</span></span>
<span id="S5.T5.1.1.1.2.1.2.1.2" class="ltx_tr">
<span id="S5.T5.1.1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">mAP</span></span>
</span></span> <span id="S5.T5.1.1.1.2.1.3" class="ltx_text"></span></span></td>
<td id="S5.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4">0m-50m</td>
<td id="S5.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">50m-100m</td>
</tr>
<tr id="S5.T5.1.1.2" class="ltx_tr">
<td id="S5.T5.1.1.2.1" class="ltx_td ltx_align_center">Avg.</td>
<td id="S5.T5.1.1.2.2" class="ltx_td ltx_align_center">Motor.</td>
<td id="S5.T5.1.1.2.3" class="ltx_td ltx_align_center">Bicyc.</td>
<td id="S5.T5.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r">C.B.</td>
<td id="S5.T5.1.1.2.5" class="ltx_td ltx_align_center">Avg.</td>
<td id="S5.T5.1.1.2.6" class="ltx_td ltx_align_center">Motor.</td>
<td id="S5.T5.1.1.2.7" class="ltx_td ltx_align_center">Bicyc.</td>
<td id="S5.T5.1.1.2.8" class="ltx_td ltx_align_center">C.B.</td>
</tr>
<tr id="S5.T5.1.1.3" class="ltx_tr">
<td id="S5.T5.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">FSD</td>
<td id="S5.T5.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28.1</td>
<td id="S5.T5.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">41.6</td>
<td id="S5.T5.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">57.3</td>
<td id="S5.T5.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">57.4</td>
<td id="S5.T5.1.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.1</td>
<td id="S5.T5.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">10.9</td>
<td id="S5.T5.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">8.8</td>
<td id="S5.T5.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t">17.0</td>
<td id="S5.T5.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t">13.8</td>
</tr>
<tr id="S5.T5.1.1.4" class="ltx_tr">
<td id="S5.T5.1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">FSF</td>
<td id="S5.T5.1.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">33.2</td>
<td id="S5.T5.1.1.4.3" class="ltx_td ltx_align_center ltx_border_bb">45.3</td>
<td id="S5.T5.1.1.4.4" class="ltx_td ltx_align_center ltx_border_bb">57.9</td>
<td id="S5.T5.1.1.4.5" class="ltx_td ltx_align_center ltx_border_bb">65.1</td>
<td id="S5.T5.1.1.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">73.7</td>
<td id="S5.T5.1.1.4.7" class="ltx_td ltx_align_center ltx_border_bb">17.2</td>
<td id="S5.T5.1.1.4.8" class="ltx_td ltx_align_center ltx_border_bb">36.4</td>
<td id="S5.T5.1.1.4.9" class="ltx_td ltx_align_center ltx_border_bb">31.1</td>
<td id="S5.T5.1.1.4.10" class="ltx_td ltx_align_center ltx_border_bb">25.2</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance of different ranges on Argoverse 2. FSF significantly improves the performance on faraway objects compared to the FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
Avg.: the average mAP of all classes.
Motor.: Motorcyclist.
Bicyc.: Bicyclist.
C.B.: Construction Barrel.
</figcaption>
</figure>
</section>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Ablation Studies</h3>

<section id="S5.SS5.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Bi-modal Queries</h4>

<figure id="S5.T6" class="ltx_table">
<div id="S5.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:496.9pt;height:66pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-90.3pt,12.0pt) scale(0.733516498616855,0.733516498616855) ;">
<table id="S5.T6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.1.1" class="ltx_tr">
<td id="S5.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Camera Queries</td>
<td id="S5.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">LiDAR Queries</td>
<td id="S5.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Query Alignment</td>
<td id="S5.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">NDS</td>
<td id="S5.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">mAP</td>
<td id="S5.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Car</td>
<td id="S5.T6.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">Truck</td>
<td id="S5.T6.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt">Bus</td>
<td id="S5.T6.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt">Trailer</td>
<td id="S5.T6.1.1.1.10" class="ltx_td ltx_align_center ltx_border_tt">C.V.</td>
<td id="S5.T6.1.1.1.11" class="ltx_td ltx_align_center ltx_border_tt">Ped.</td>
<td id="S5.T6.1.1.1.12" class="ltx_td ltx_align_center ltx_border_tt">Motor.</td>
<td id="S5.T6.1.1.1.13" class="ltx_td ltx_align_center ltx_border_tt">Bicyc.</td>
<td id="S5.T6.1.1.1.14" class="ltx_td ltx_align_center ltx_border_tt">T.C.</td>
<td id="S5.T6.1.1.1.15" class="ltx_td ltx_align_center ltx_border_tt">Barrier</td>
</tr>
<tr id="S5.T6.1.1.2" class="ltx_tr">
<td id="S5.T6.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S5.T6.1.1.2.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T6.1.1.2.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T6.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">68.8</td>
<td id="S5.T6.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">63.1</td>
<td id="S5.T6.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">80.5</td>
<td id="S5.T6.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">54.3</td>
<td id="S5.T6.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t">72.1</td>
<td id="S5.T6.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t">34.6</td>
<td id="S5.T6.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span id="S5.T6.1.1.2.10.1" class="ltx_text" style="background-color:#ECECEC;">29.0</span></td>
<td id="S5.T6.1.1.2.11" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span id="S5.T6.1.1.2.11.1" class="ltx_text" style="background-color:#ECECEC;">85.4</span></td>
<td id="S5.T6.1.1.2.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span id="S5.T6.1.1.2.12.1" class="ltx_text" style="background-color:#ECECEC;">72.3</span></td>
<td id="S5.T6.1.1.2.13" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span id="S5.T6.1.1.2.13.1" class="ltx_text" style="background-color:#ECECEC;">68.8</span></td>
<td id="S5.T6.1.1.2.14" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span id="S5.T6.1.1.2.14.1" class="ltx_text" style="background-color:#ECECEC;">78.0</span></td>
<td id="S5.T6.1.1.2.15" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span id="S5.T6.1.1.2.15.1" class="ltx_text" style="background-color:#ECECEC;">64.0</span></td>
</tr>
<tr id="S5.T6.1.1.3" class="ltx_tr">
<td id="S5.T6.1.1.3.1" class="ltx_td"></td>
<td id="S5.T6.1.1.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T6.1.1.3.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T6.1.1.3.4" class="ltx_td ltx_align_center">68.7</td>
<td id="S5.T6.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r">62.5</td>
<td id="S5.T6.1.1.3.6" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S5.T6.1.1.3.6.1" class="ltx_text" style="background-color:#ECECEC;">83.9</span></td>
<td id="S5.T6.1.1.3.7" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S5.T6.1.1.3.7.1" class="ltx_text" style="background-color:#ECECEC;">56.4</span></td>
<td id="S5.T6.1.1.3.8" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S5.T6.1.1.3.8.1" class="ltx_text" style="background-color:#ECECEC;">73.4</span></td>
<td id="S5.T6.1.1.3.9" class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span id="S5.T6.1.1.3.9.1" class="ltx_text" style="background-color:#ECECEC;">41.4</span></td>
<td id="S5.T6.1.1.3.10" class="ltx_td ltx_align_center">27.4</td>
<td id="S5.T6.1.1.3.11" class="ltx_td ltx_align_center">84.3</td>
<td id="S5.T6.1.1.3.12" class="ltx_td ltx_align_center">69.5</td>
<td id="S5.T6.1.1.3.13" class="ltx_td ltx_align_center">55.6</td>
<td id="S5.T6.1.1.3.14" class="ltx_td ltx_align_center">72.4</td>
<td id="S5.T6.1.1.3.15" class="ltx_td ltx_align_center">60.7</td>
</tr>
<tr id="S5.T6.1.1.4" class="ltx_tr">
<td id="S5.T6.1.1.4.1" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T6.1.1.4.2" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T6.1.1.4.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T6.1.1.4.4" class="ltx_td ltx_align_center">71.5</td>
<td id="S5.T6.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r">68.6</td>
<td id="S5.T6.1.1.4.6" class="ltx_td ltx_align_center">85.2</td>
<td id="S5.T6.1.1.4.7" class="ltx_td ltx_align_center">60.2</td>
<td id="S5.T6.1.1.4.8" class="ltx_td ltx_align_center">75.0</td>
<td id="S5.T6.1.1.4.9" class="ltx_td ltx_align_center">41.5</td>
<td id="S5.T6.1.1.4.10" class="ltx_td ltx_align_center">32.9</td>
<td id="S5.T6.1.1.4.11" class="ltx_td ltx_align_center">87.4</td>
<td id="S5.T6.1.1.4.12" class="ltx_td ltx_align_center">77.2</td>
<td id="S5.T6.1.1.4.13" class="ltx_td ltx_align_center">72.0</td>
<td id="S5.T6.1.1.4.14" class="ltx_td ltx_align_center">80.1</td>
<td id="S5.T6.1.1.4.15" class="ltx_td ltx_align_center">74.1</td>
</tr>
<tr id="S5.T6.1.1.5" class="ltx_tr">
<td id="S5.T6.1.1.5.1" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S5.T6.1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S5.T6.1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S5.T6.1.1.5.4" class="ltx_td ltx_align_center ltx_border_bb">72.7</td>
<td id="S5.T6.1.1.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">70.4</td>
<td id="S5.T6.1.1.5.6" class="ltx_td ltx_align_center ltx_border_bb">86.1</td>
<td id="S5.T6.1.1.5.7" class="ltx_td ltx_align_center ltx_border_bb">62.5</td>
<td id="S5.T6.1.1.5.8" class="ltx_td ltx_align_center ltx_border_bb">76.8</td>
<td id="S5.T6.1.1.5.9" class="ltx_td ltx_align_center ltx_border_bb">44.8</td>
<td id="S5.T6.1.1.5.10" class="ltx_td ltx_align_center ltx_border_bb">34.4</td>
<td id="S5.T6.1.1.5.11" class="ltx_td ltx_align_center ltx_border_bb">88.6</td>
<td id="S5.T6.1.1.5.12" class="ltx_td ltx_align_center ltx_border_bb">78.7</td>
<td id="S5.T6.1.1.5.13" class="ltx_td ltx_align_center ltx_border_bb">73.7</td>
<td id="S5.T6.1.1.5.14" class="ltx_td ltx_align_center ltx_border_bb">82.6</td>
<td id="S5.T6.1.1.5.15" class="ltx_td ltx_align_center ltx_border_bb">75.5</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Ablation study of the Query Generation and Query Alignment.
C.V.: Construction Vehicle. Ped.: Pedestrian. Motor.: Motorcycle. Bicyc.: Bicycle. T.C: Traffic Cone.
<span id="S5.T6.3.1" class="ltx_text" style="background-color:#ECECEC;">Gray Cell</span>: By comparing LiDAR queries and camera queries, we highlight the results in which one performed better than the other.
</figcaption>
</figure>
<div id="S5.SS5.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS5.SSS0.Px1.p1.1" class="ltx_p">Although the form of two kinds of queries is unified by query alignment, they still have different natures since they come from two different modalities.
To reveal the pros and cons of the two kinds of queries, we remove each of them from FSF for ablation.
There are several intriguing findings in Table <a href="#S5.T6" title="Table 6 ‣ Bi-modal Queries ‣ 5.5 Ablation Studies ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p">Camera queries are better in the categories with relatively small sizes. Especially, camera queries show significant advantages over LiDAR queries in <em id="S5.I2.i1.p1.1.1" class="ltx_emph ltx_font_italic">Motorcycle</em>, <em id="S5.I2.i1.p1.1.2" class="ltx_emph ltx_font_italic">Bicycle</em>, and <em id="S5.I2.i1.p1.1.3" class="ltx_emph ltx_font_italic">Traffic cone</em>. In contrast, LiDAR queries are better in the categories with large sizes, such as <em id="S5.I2.i1.p1.1.4" class="ltx_emph ltx_font_italic">Car</em>, <em id="S5.I2.i1.p1.1.5" class="ltx_emph ltx_font_italic">Bus</em>, and <em id="S5.I2.i1.p1.1.6" class="ltx_emph ltx_font_italic">Trailer</em>.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p">Their differences make them complementary to each other.
Combining two kinds of queries, the performance improves in all categories.
It is noteworthy that the performance is also significantly improved in some classes where LiDAR queries and camera queries have similar performance, such as <em id="S5.I2.i2.p1.1.1" class="ltx_emph ltx_font_italic">Bus</em> and <em id="S5.I2.i2.p1.1.2" class="ltx_emph ltx_font_italic">Pedestrian</em>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS5.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Query Alignment</h4>

<div id="S5.SS5.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS5.SSS0.Px2.p1.1" class="ltx_p">To verify the effectiveness of our query alignment method, we design a model without query alignment, which we refer to as FSF-M (FSF-Misaligned).
Specifically, to properly remove the query alignment while maintaining fairness, we still predict a reference box based on the initial camera queries but do not use the reference box to correct the shape of the point cluster.
Thus, the only difference between FSF-M and FSF is that they have different cluster shapes in camera queries.
Table <a href="#S5.T6" title="Table 6 ‣ Bi-modal Queries ‣ 5.5 Ablation Studies ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the comparison between FSF-M (the third row) and FSF , where query alignment gives us a 1.8 mAP boost.
Without the alignment, the performance degrades in all categories consistently, which indicates the query alignment has an essential impact on the performance.</p>
</div>
</section>
<section id="S5.SS5.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Two-round Assignment</h4>

<div id="S5.SS5.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS5.SSS0.Px3.p1.1" class="ltx_p">Another essential difference between FSF and the LiDAR-only FSD is the proposed two-around assignment.
To verify its effectiveness, we make comparisons between our strategy with two commonly used strategies: <span id="S5.SS5.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_bold">one-to-one assignment</span> and <span id="S5.SS5.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_bold">query-in-box assignment</span>, and the results are shown in Table <a href="#S5.T7" title="Table 7 ‣ Two-round Assignment ‣ 5.5 Ablation Studies ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
For the one-to-one assignment, we adopt the implementation from DETR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and add a self-attention module between queries following the convention.
For the query-in-box assignment, we use the same hyperparameters with our 3D round.
In the two experiments, all queries are treated equally.
We reach two conclusions as follows.</p>
<ul id="S5.I3" class="ltx_itemize">
<li id="S5.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I3.i1.p1" class="ltx_para">
<p id="S5.I3.i1.p1.1" class="ltx_p">The model adopting one-to-one assignment fails to converge.
This is because a considerable number of camera queries tightly overlap with LiDAR queries.
It confuses the model learning if we only choose one as positive.</p>
</div>
</li>
<li id="S5.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I3.i2.p1" class="ltx_para">
<p id="S5.I3.i2.p1.1" class="ltx_p">The model using query-in-box assignment normally converges but shows an inferior performance to our default setting.
We owe it to that the centroids of frustum have large variances along the depth direction, and are unlikely to accurately fall into 3D ground-truth boxes as Fig. <a href="#S4.F4" title="Figure 4 ‣ 3D Round ‣ 4.4 Query Label Assignment ‣ 4 Methodology ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows.</p>
</div>
</li>
</ul>
</div>
<figure id="S5.T7" class="ltx_table">
<div id="S5.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:190.8pt;height:53.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.5pt,9.3pt) scale(0.740386785185876,0.740386785185876) ;">
<table id="S5.T7.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.1.1" class="ltx_tr">
<td id="S5.T7.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Assignment</td>
<td id="S5.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">mAP</td>
<td id="S5.T7.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">NDS</td>
</tr>
<tr id="S5.T7.1.1.2" class="ltx_tr">
<td id="S5.T7.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">One-to-one</td>
<td id="S5.T7.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Not converged</td>
<td id="S5.T7.1.1.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">Not converged</td>
</tr>
<tr id="S5.T7.1.1.3" class="ltx_tr">
<td id="S5.T7.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r">Query-in-Box only</td>
<td id="S5.T7.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r">68.7</td>
<td id="S5.T7.1.1.3.3" class="ltx_td ltx_nopad_r ltx_align_center">71.9</td>
</tr>
<tr id="S5.T7.1.1.4" class="ltx_tr">
<td id="S5.T7.1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Two-round Assignment</td>
<td id="S5.T7.1.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T7.1.1.4.2.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S5.T7.1.1.4.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S5.T7.1.1.4.3.1" class="ltx_text ltx_font_bold">72.7</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Ablation study of different assignment methods. This experiment reveals the importance of our two-round assignment.</figcaption>
</figure>
</section>
<section id="S5.SS5.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Robustness against 2D Predictions</h4>

<div id="S5.SS5.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS5.SSS0.Px4.p1.1" class="ltx_p">It would be a major concern whether FSF is robust to the 2D sides.
To answer this question, we change the default HTC to a basic Mask RCNN whose 2D Mask AP on nuImages is only 38.4.
As Table <a href="#S5.T8" title="Table 8 ‣ Robustness against 2D Predictions ‣ 5.5 Ablation Studies ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows, although the 2D performance has a significant drop after degrading the 2D part, the 3D performance of FSF only has 1 mAP drop.
Besides, we also conduct an experiment that degrades 2D masks to 2D boxes during producing frustum.
The result shows that downgrading the masks to boxes has minimal impact on 3D detection.
It is reasonable since the camera queries go through further refinement in FSF , which mitigates the impact of inaccurate 2D results.
</p>
</div>
<figure id="S5.T8" class="ltx_table">
<div id="S5.T8.2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:57.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-61.4pt,16.3pt) scale(0.638328478472992,0.638328478472992) ;">
<table id="S5.T8.2.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.2.2.2.3" class="ltx_tr">
<td id="S5.T8.2.2.2.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T8.2.2.2.3.1.1" class="ltx_text">Method</span></td>
<td id="S5.T8.2.2.2.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">2D in nuImage</td>
<td id="S5.T8.2.2.2.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T8.2.2.2.3.3.1" class="ltx_text"><span id="S5.T8.2.2.2.3.3.1.1" class="ltx_text"></span> <span id="S5.T8.2.2.2.3.3.1.2" class="ltx_text">
<span id="S5.T8.2.2.2.3.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T8.2.2.2.3.3.1.2.1.1" class="ltx_tr">
<span id="S5.T8.2.2.2.3.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Mask/Box</span></span>
</span></span> <span id="S5.T8.2.2.2.3.3.1.3" class="ltx_text"></span></span></td>
<td id="S5.T8.2.2.2.3.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">3D in nuScenes</td>
</tr>
<tr id="S5.T8.2.2.2.2" class="ltx_tr">
<td id="S5.T8.1.1.1.1.1" class="ltx_td ltx_align_center">Mask<sup id="S5.T8.1.1.1.1.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S5.T8.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">Box<sup id="S5.T8.2.2.2.2.2.1" class="ltx_sup">∗</sup>
</td>
<td id="S5.T8.2.2.2.2.3" class="ltx_td ltx_align_center">mAP</td>
<td id="S5.T8.2.2.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center">NDS</td>
</tr>
<tr id="S5.T8.2.2.2.4" class="ltx_tr">
<td id="S5.T8.2.2.2.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Mask RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S5.T8.2.2.2.4.2" class="ltx_td ltx_align_center ltx_border_t">38.4</td>
<td id="S5.T8.2.2.2.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.8</td>
<td id="S5.T8.2.2.2.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mask</td>
<td id="S5.T8.2.2.2.4.5" class="ltx_td ltx_align_center ltx_border_t">69.4</td>
<td id="S5.T8.2.2.2.4.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">72.3</td>
</tr>
<tr id="S5.T8.2.2.2.5" class="ltx_tr">
<td id="S5.T8.2.2.2.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T8.2.2.2.5.1.1" class="ltx_text">HTC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></span></td>
<td id="S5.T8.2.2.2.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S5.T8.2.2.2.5.2.1" class="ltx_text">46.4</span></td>
<td id="S5.T8.2.2.2.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T8.2.2.2.5.3.1" class="ltx_text">57.3</span></td>
<td id="S5.T8.2.2.2.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Box</td>
<td id="S5.T8.2.2.2.5.5" class="ltx_td ltx_align_center ltx_border_t">69.8</td>
<td id="S5.T8.2.2.2.5.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">72.5</td>
</tr>
<tr id="S5.T8.2.2.2.6" class="ltx_tr">
<td id="S5.T8.2.2.2.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Mask</td>
<td id="S5.T8.2.2.2.6.2" class="ltx_td ltx_align_center ltx_border_bb">70.4</td>
<td id="S5.T8.2.2.2.6.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">72.7</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Ablation study of different 2D sides.
Mask<sup id="S5.T8.9.1" class="ltx_sup">∗</sup>: Mask mAP.
Box<sup id="S5.T8.10.2" class="ltx_sup">∗</sup>: Box mAP.
The result reveals that our method is robust against the 2D predictions.</figcaption>
</figure>
<figure id="S5.T9" class="ltx_table">
<div id="S5.T9.1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:33.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-68.2pt,10.4pt) scale(0.61390181371342,0.61390181371342) ;">
<table id="S5.T9.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T9.1.1.1.2" class="ltx_tr">
<td id="S5.T9.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Image features</td>
<td id="S5.T9.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt">mAP</td>
<td id="S5.T9.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">NDS</td>
<td id="S5.T9.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt">mATE</td>
<td id="S5.T9.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_tt">mASE</td>
<td id="S5.T9.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_tt">mAOE</td>
<td id="S5.T9.1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_tt">mAVE</td>
<td id="S5.T9.1.1.1.2.8" class="ltx_td ltx_align_center ltx_border_tt">mAAE</td>
</tr>
<tr id="S5.T9.1.1.1.1" class="ltx_tr">
<td id="S5.T9.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T9.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T9.1.1.1.1.1.m1.1a"><mo id="S5.T9.1.1.1.1.1.m1.1.1" xref="S5.T9.1.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T9.1.1.1.1.1.m1.1b"><times id="S5.T9.1.1.1.1.1.m1.1.1.cmml" xref="S5.T9.1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.1.1.1.1.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S5.T9.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">70.4</td>
<td id="S5.T9.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.7</td>
<td id="S5.T9.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">0.277</td>
<td id="S5.T9.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">0.247</td>
<td id="S5.T9.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">0.317</td>
<td id="S5.T9.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">0.218</td>
<td id="S5.T9.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t">0.186</td>
</tr>
<tr id="S5.T9.1.1.1.3" class="ltx_tr">
<td id="S5.T9.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S5.T9.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_bb">70.7</td>
<td id="S5.T9.1.1.1.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">72.9</td>
<td id="S5.T9.1.1.1.3.4" class="ltx_td ltx_align_center ltx_border_bb">0.274</td>
<td id="S5.T9.1.1.1.3.5" class="ltx_td ltx_align_center ltx_border_bb">0.247</td>
<td id="S5.T9.1.1.1.3.6" class="ltx_td ltx_align_center ltx_border_bb">0.307</td>
<td id="S5.T9.1.1.1.3.7" class="ltx_td ltx_align_center ltx_border_bb">0.223</td>
<td id="S5.T9.1.1.1.3.8" class="ltx_td ltx_align_center ltx_border_bb">0.187</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Ablation study of using the image feature.
Utilizing image features does not result in a significant improvement.
As a result, we make it an optional choice.</figcaption>
</figure>
</section>
<section id="S5.SS5.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Does the image feature help more?</h4>

<div id="S5.SS5.SSS0.Px5.p1" class="ltx_para">
<p id="S5.SS5.SSS0.Px5.p1.1" class="ltx_p">To answer this question, we conduct an experiment that adopts the same setting as §<a href="#S5.SS3" title="5.3 Alternatives to FSF ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>, where each point is painted with the FPN’s multi-layered features.
As shown in Table <a href="#S5.T9" title="Table 9 ‣ Robustness against 2D Predictions ‣ 5.5 Ablation Studies ‣ 5 Experiments ‣ Fully Sparse Fusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, using the image feature results in a marginal increase of 0.3 mAP in our approach.
Therefore, we make it an optional choice to utilize the image feature or not, since it will introduce additional costs.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We propose a Fully Sparse Fusion (FSF) framework, a state-of-the-art 3D object detector that utilizes a Bi-Modal Query Generator for joint 2D and 3D instance segmentation.
FSF then incorporates a Query Refinement module that effectively aligns and refines queries.
Our framework achieves state-of-the-art performance on two large-scale datasets, nuScenes and Argoverse 2.
Additionally, FSF significantly reduces latency and memory usage in long-range detection.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and
Chiew-Lan Tai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">TransFusion: Robust lidar-camera fusion for 3d object detection with
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Garrick Brazil and Xiaoming Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">M3D-RPN: Monocular 3d region proposal network for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">nuScenes: A multimodal dataset for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun,
Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Hybrid task cascade for instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Multi-View 3D Object Detection Network for Autonomous Driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Xuanyao Chen, Tianyuan Zhang, Yue Wang, Yilun Wang, and Hang Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">FUTR3D: A unified sensor fusion framework for 3d detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.10642</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Focal Sparse Convolutional Networks for 3D Object Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Jianhui Liu, Xiaojuan Qi, Xiangyu Zhang, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Scaling up kernels in 3D CNNs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.10555</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and
Tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinghong Jiang, Feng Zhao,
Bolei Zhou, and Hang Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Autoalign: Pixel-instance feature aggregation for multi-modal 3d
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Zhiyu Chong, Xinzhu Ma, Hong Zhang, Yuxin Yue, Haojie Li, Zhihui Wang, and
Wanli Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">MonoDistill: Learning spatial features for monocular 3D object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
MMDetection3D Contributors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">MMDetection3D: OpenMMLab Next-generation Platform for General 3D
Object Detection.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/open-mmlab/mmdetection3d" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/open-mmlab/mmdetection3d</a><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan
Wang, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Embracing Single Stride 3D Object Detector with Sparse Transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Lue Fan, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Fully Sparse 3D Object Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Lue Fan, Xuan Xiong, Feng Wang, Naiyan Wang, and ZhaoXiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">RangeDet: In Defense of Range View for LiDAR-Based 3D Object
Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Lue Fan, Yuxue Yang, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Super sparse 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.02562</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Xiaoyang Guo, Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Liga-stereo: Learning Lidar Geometry Aware Representations for
Stereo-based 3D Detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Mask R-CNN.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Junjie Huang and Guan Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">BEVDet4D: Exploit temporal cues in multi-camera 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.17054</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Tengteng Huang, Zhe Liu, Xiwu Chen, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">EPNet: Enhancing point features with image semantics for 3D object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven L Waslander.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Joint 3D Proposal Generation and Object Detection from View
Aggregation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IROS</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar
Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">PointPillars: Fast Encoders for Object Detection from Point Clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Bo Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">3D Fully Convolutional Network for Vehicle Detection in Point
Cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IROS</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Peiliang Li, Xiaozhi Chen, and Shaojie Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Stereo R-CNN based 3d Object Detection for Autonomous Driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Yingyan Li, Yuntao Chen, Jiawei He, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Densely Constrained Depth Estimator for Monocular 3D Object
Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran Wang, Yukang Shi,
Jianjian Sun, and Zeming Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">BevDepth: Acquisition of reliable depth for multi-view 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng,
Junyang Shen, Bo Wu, Yifeng Lu, Denny Zhou, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object
Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Zhichao Li, Feng Wang, and Naiyan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">LiDAR R-CNN: An Efficient and Universal 3D Object Detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao,
and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">BevFormer: Learning bird’s-eye-view representation from
multi-camera images via spatiotemporal transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia, Zhiwei Lin, Yongtao
Wang, Tao Tang, Bing Wang, and Zhi Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">BEVFusion: A simple and robust lidar-camera fusion framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">PETR: Position embedding transformation for multi-view 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus,
and Song Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye
View Representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Zhijian Liu, Xinyu Yang, Haotian Tang, Shang Yang, and Song Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">FlatFormer: Flattened Window Attention for Efficient Point Cloud
Transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Zongdai Liu, Dingfu Zhou, Feixiang Lu, Jin Fang, and Liangjun Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">AutoShape: Real-time shape-aware monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Decoupled Weight Decay Regularization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1711.05101</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Yan Lu, Xinzhu Ma, Lei Yang, Tianzhu Zhang, Yating Liu, Qi Chu, Junjie Yan, and
Wanli Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Geometry uncertainty projection network for monocular 3D object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Su Pang, Daniel Morris, and Hayder Radha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">CLOCs: Camera-LiDAR object candidates fusion for 3D object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IROS</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Jonah Philion and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Lift, Splat, Shoot: Encoding images from arbitrary camera rigs by
implicitly unprojecting to 3D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Deep Hough Voting for 3D Object Detection in Point Clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Frustum PointNets for 3D Object Detection from RGB-D Data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">PointNet: Deep Learning on Point Sets for 3D Classification and
Segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a
Metric Space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Cody Reading, Ali Harakeh, Julia Chae, and Steven L Waslander.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Categorical depth distribution network for monocular 3D object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and
Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object
Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">PointRCNN: 3D Object Proposal Generation and Detection from Point
Cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">From Points to Parts: 3D Object Detection from Point Cloud with
Part-aware and Part-aggregation Network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Zhi Tian, Xiangxiang Chu, Xiaoming Wang, Xiaolin Wei, and Chunhua Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Fully convolutional one-stage 3D object detection on LiDAR range
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Sourabh Vora, Alex H Lang, Bassam Helou, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">PointPainting: Sequential Fusion for 3D Object Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Chunwei Wang, Chao Ma, Ming Zhu, and Xiaokang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Pointaugmenting: Cross-modal augmentation for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Yuqi Wang, Yuntao Chen, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D
Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">2023.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao,
and Justin Solomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">DETR3D: 3D object detection from multi-view images via 3d-to-2d
queries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRL</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Zitian Wang, Zehao Huang, Jiahui Fu, Naiyan Wang, and Si Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Object as Query: Equipping Any 2D Object Detector with 3D Detection
Ability.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.02364</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Zhixin Wang and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Frustum ConvNet: Sliding frustums to aggregate local point-wise
features for amodal 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IROS</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh,
Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett,
Jhony Kaesemodel Pontes, Deva Ramanan, Peter Carr, and James Hays.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Argoverse 2: Next Generation Datasets for Self-Driving Perception
and Forecasting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS Datasets and Benchmarks 2021</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Xu, Dingfu Zhou, Jin Fang, Junbo Yin, Zhou Bin, and Liangjun Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">FusionPainting: Multimodal fusion with adaptive attention for 3D
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ITSC</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Yan Yan, Yuxing Mao, and Bo Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">SECOND: Sparsely Embedded Convolutional Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, 18(10), 2018.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang,
Gao Huang, Hongyang Li, Yu Qiao, Lewei Lu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">BEVFormer v2: Adapting Modern Image Backbones to Bird’s-Eye-View
Recognition via Perspective Supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">2023.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Zeyu Yang, Jiaqi Chen, Zhenwei Miao, Wei Li, Xiatian Zhu, and Li Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">DeepInteraction: 3D object detection via modality interaction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">3DSSD: Point-based 3D Single Stage Object Detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Center-based 3D Object Detection and Tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Multimodal virtual point 3d detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Jin Hyeok Yoo, Yecheol Kim, Jisong Kim, and Jun Won Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">3D-CVF: Generating joint camera and lidar features using cross-view
spatial feature fusion for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Yunpeng Zhang, Jiwen Lu, and Jie Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Objects are different: Flexible monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib63.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Hui Zhou, Xinge Zhu, Xiao Song, Yuexin Ma, Zhe Wang, Hongsheng Li, and Dahua
Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR
Semantic Segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Yin Zhou and Oncel Tuzel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object
Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib65.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">Class-balanced grouping and sampling for point cloud 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.09492</span><span id="bib.bib66.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.12309" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.12310" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.12310">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.12310" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.12311" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 13:19:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
