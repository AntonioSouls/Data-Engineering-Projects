<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.13876] Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images</title><meta property="og:description" content="Object detection in Remote Sensing Images (RSI) is a critical task for numerous applications in Earth Observation (EO). Unlike general object detection, object detection in RSI has specific challenges: 1) the scarcity …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.13876">

<!--Generated on Tue Feb 27 21:29:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multimodal Transformer Using Cross-Channel attention for 
<br class="ltx_break">Object Detection in Remote Sensing Images</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Object detection in Remote Sensing Images (RSI) is a critical task for numerous applications in Earth Observation (EO). Unlike general object detection, object detection in RSI has specific challenges: 1) the scarcity of labeled data in RSI compared to general object detection datasets, and 2) the small objects
presented in a high-resolution image with a vast background. To address these challenges, we propose a multimodal transformer exploring multi-source remote sensing data for object detection. Instead of directly combining the multimodal input through a channel-wise concatenation, which ignores the heterogeneity of different modalities, we propose a cross-channel attention module. This module learns the relationship between different channels, enabling the construction of a coherent multimodal input by aligning the different modalities at the early stage. We also introduce a new architecture based on the Swin transformer that incorporates convolution layers in non-shifting blocks while maintaining fixed dimensions,
allowing for the generation of fine-to-coarse representations with a favorable accuracy-computation trad-off.
The extensive experiments prove the effectiveness of the proposed multimodal fusion module and architecture, demonstrating their applicability to multimodal aerial imagery.
</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
Multimodal transformer, cross-channel attention, convolutional shifting window, object detection, remote sensing imagery</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Object detection in Remote Sensing Images (RSI) including aerial images is a critical task that allows us to identify and locate objects of interest in satellite or aerial images. It has numerous applications for Earth Observation (EO) such as environmental monitoring, climate change, urban planning, and military surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. All of these tasks have been explored in the past few decades using data from single sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, e.g., Hyperspectral Image (HSI) instruments or RGB sensors from satellites or airplanes. Additionally, there are specific challenges facing object detection in RSI
: 1) the scarcity of labeled data in RSI compared to general object detection datasets, and 2) the small objects presented in a high-resolution image with a vast background. Nowadays, the largest public dataset DOTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> for object detection in RSI comprises only 188K instances, a notable contrast to the general object detection dataset COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, which has 1.5 million instances. The detection accuracy and the generalization capacity of models have been limited by this data scarcity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Moreover, the diminished size of objects in aerial images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and the unique top-down perspective intrinsic to aerial observations make it difficult to achieve high accuracy.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Given the growing availability of multi-source remote sensing data and aerial images, embracing multimodal learning using multi-sensor data such as HSI, RGB, Infrared (IR), Light Detection and Ranging (LiDAR) has become imperative to tackle the above challenges. Multi-source data can not only augment the volume and diversity of the visual data but also provide complementary semantic knowledge between different modalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Although recent research, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> have demonstrated that fusing different modalities can significantly improve performance with good efficiency, they often combine the different modalities through channel-wise concatenation as a multimodal input, overlooking the inherent differences between images from different modalities acquired by different sensors. To address this issue, we propose a cross-channel attention module that can thoroughly explore the relationships between channels from different or the same modalities, allowing us to align the different modalities at the early stage and then construct a coherent multimodal input using the learned features (see Fig <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.13876/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="215" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Combining multimodal inputs using cross-channel attention instead of simple channel-wise concatenation.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recently, Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and its variant Swin transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> have achieved impressive performance on image classification compared to Convolutional Neural Networks (CNNs).
Inspired by these works, we also propose an architecture stacking ViT blocks with shifting window attention of varying resolutions. Particularly, we introduce a convolutional layer in the Feed-Forward Network (FFN) to enhance the network to capture local information and facilitate the integration of neighboring patches across different windows, referred to as the convolutional-shifting window in this work. This approach empowers the model to detect the small object by leveraging the hierarchical features generated in a fine-to-coarse manner.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To summarize, the main contributions of this work are: 1) We introduce a new cross-channel attention module that allows for the early alignment of different modalities by learning the relationship between different channels;
2) We propose the convolutional-shifting window which incorporates convolutional layers in FFN to learn the hierarchical features in a fine-to-coarse manner enhancing the detection of small objects; 3) The extensive experiments demonstrate the superiority of the proposed approach, highlighting its applicability for object detection using multimodal aerial imagery.
</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Multimodal object detection</span>. Manish et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> proposed a real-time framework for object detection in multimodal remote sensing imaging by conducting mid-level fusion from RGB and IR images. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> proposed the CNN-based SuperYOLO and they investigated diverse fusion strategies including pixel-level fusion, intermediate-level fusion, and early-stage modal fusion. Their findings suggest that pixel-level fusion stands out as the most efficient approach, excelling in terms of both detection performance and computational efficiency.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Window-based Transformer/CNN</span>.
The landscape of computer vision has undergone a substantial transformation with the emergence of ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, showcasing advancements across a broad spectrum of visual tasks.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> enhanced the traditional ViTs by introducing hierarchical architectures and localized windows. These enhancements have found practical applications in single-modal aerial image object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Drawing from this body of work, more recent approaches endeavor to combine the strengths of CNNs with ViTs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. This fusion capitalizes on the respective advantages of both CNNs and ViTs, thus offering promising prospects for computer vision applications such as classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and Face Presentation Attack Detection (PAD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed method</h2>

<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" style="width:314.0pt;"><img src="/html/2310.13876/assets/x2.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.1.1.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>(a) The overall architecture based on Swin-like backbone for multimodal object detection in RSI;
(b) Convolutional-shifting window module.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" style="width:172.2pt;"><img src="/html/2310.13876/assets/x3.png" id="S3.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="538" height="365" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1.1" class="ltx_text ltx_font_bold">Fig. 3</span>: </span>Cross-channel attention module for RGB and IR images multimodal fusion.</figcaption>
</figure>
</div>
</div>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overall architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Proposed method ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the proposed architecture
is composed mainly of the proposed cross-channel attention module, the feature extraction backbone consisting of three
Swin-like blocks based on the proposed convolutional-shifting window, and a YOLO-based detection head as used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multimodal fusion by cross-channel attention</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.13" class="ltx_p">In this work, the cross-channel attention module is designed to facilitate the multimodal fusion of the IR and RGB images including in the VEDAI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. As shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Proposed method ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the proposed cross-channel attention approach addresses the interactions within the RGB’s three channels and between the channels of RGB and IR’s one channel. For instance, the interactions between R and G channels, G and B channels, B and IR channels, and R and IR channels. Before calculating the cross-channel attention, each channel has been partitioned into 4<math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mo id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><times id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\times</annotation></semantics></math>4 patches. Inspired by the Swin transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, we calculate the cross-channel attention based on a window containing <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="M\times M" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">M</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></times><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑀</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">M\times M</annotation></semantics></math> patches instead of the single patch as the conventional self-attention approach. Consequently, the queries <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="{Q}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">{Q}</annotation></semantics></math>, keys <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="{K}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">{K}</annotation></semantics></math>, and values <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="{V}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">{V}</annotation></semantics></math> used for calculating attentions are obtained from each window as illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Proposed method ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Rather than the conventional self-attention considering the interactions between patches within the same image, the cross-channel attention using the query <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="{Q}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">{Q}</annotation></semantics></math>, and key <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="{K}" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">{K}</annotation></semantics></math> and value <math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="{V}" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><mi id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><ci id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">{V}</annotation></semantics></math> from two different channels (i.e., images). For instance, we use the query <math id="S3.SS2.p1.9.m9.1" class="ltx_Math" alttext="{Q}" display="inline"><semantics id="S3.SS2.p1.9.m9.1a"><mi id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><ci id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">{Q}</annotation></semantics></math> from channel R, and the key <math id="S3.SS2.p1.10.m10.1" class="ltx_Math" alttext="{K}" display="inline"><semantics id="S3.SS2.p1.10.m10.1a"><mi id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><ci id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">{K}</annotation></semantics></math> and value <math id="S3.SS2.p1.11.m11.1" class="ltx_Math" alttext="{V}" display="inline"><semantics id="S3.SS2.p1.11.m11.1a"><mi id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b"><ci id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">{V}</annotation></semantics></math> from channel G to calculate cross-channel attention between R and G. Then the cross-attention feature maps <math id="S3.SS2.p1.12.m12.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p1.12.m12.1a"><mi id="S3.SS2.p1.12.m12.1.1" xref="S3.SS2.p1.12.m12.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m12.1b"><ci id="S3.SS2.p1.12.m12.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m12.1c">C</annotation></semantics></math> is given by the equation (<a href="#S3.E1" title="In 3.2 Multimodal fusion by cross-channel attention ‣ 3 Proposed method ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) using the obtained <math id="S3.SS2.p1.13.m13.1" class="ltx_Math" alttext="{Q/K/V}" display="inline"><semantics id="S3.SS2.p1.13.m13.1a"><mrow id="S3.SS2.p1.13.m13.1.1" xref="S3.SS2.p1.13.m13.1.1.cmml"><mi id="S3.SS2.p1.13.m13.1.1.2" xref="S3.SS2.p1.13.m13.1.1.2.cmml">Q</mi><mo id="S3.SS2.p1.13.m13.1.1.1" xref="S3.SS2.p1.13.m13.1.1.1.cmml">/</mo><mi id="S3.SS2.p1.13.m13.1.1.3" xref="S3.SS2.p1.13.m13.1.1.3.cmml">K</mi><mo id="S3.SS2.p1.13.m13.1.1.1a" xref="S3.SS2.p1.13.m13.1.1.1.cmml">/</mo><mi id="S3.SS2.p1.13.m13.1.1.4" xref="S3.SS2.p1.13.m13.1.1.4.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m13.1b"><apply id="S3.SS2.p1.13.m13.1.1.cmml" xref="S3.SS2.p1.13.m13.1.1"><divide id="S3.SS2.p1.13.m13.1.1.1.cmml" xref="S3.SS2.p1.13.m13.1.1.1"></divide><ci id="S3.SS2.p1.13.m13.1.1.2.cmml" xref="S3.SS2.p1.13.m13.1.1.2">𝑄</ci><ci id="S3.SS2.p1.13.m13.1.1.3.cmml" xref="S3.SS2.p1.13.m13.1.1.3">𝐾</ci><ci id="S3.SS2.p1.13.m13.1.1.4.cmml" xref="S3.SS2.p1.13.m13.1.1.4">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m13.1c">{Q/K/V}</annotation></semantics></math> from two different channels.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="C_{ij}=\text{softmax}\left(\frac{Q_{i}K_{j}^{T}}{\sqrt{d_{k}}}\right)V_{j}" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.2" xref="S3.E1.m1.1.2.cmml"><msub id="S3.E1.m1.1.2.2" xref="S3.E1.m1.1.2.2.cmml"><mi id="S3.E1.m1.1.2.2.2" xref="S3.E1.m1.1.2.2.2.cmml">C</mi><mrow id="S3.E1.m1.1.2.2.3" xref="S3.E1.m1.1.2.2.3.cmml"><mi id="S3.E1.m1.1.2.2.3.2" xref="S3.E1.m1.1.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.2.3.1" xref="S3.E1.m1.1.2.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.2.2.3.3" xref="S3.E1.m1.1.2.2.3.3.cmml">j</mi></mrow></msub><mo id="S3.E1.m1.1.2.1" xref="S3.E1.m1.1.2.1.cmml">=</mo><mrow id="S3.E1.m1.1.2.3" xref="S3.E1.m1.1.2.3.cmml"><mtext id="S3.E1.m1.1.2.3.2" xref="S3.E1.m1.1.2.3.2a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.3.1" xref="S3.E1.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E1.m1.1.2.3.3.2" xref="S3.E1.m1.1.1.cmml"><mo id="S3.E1.m1.1.2.3.3.2.1" xref="S3.E1.m1.1.1.cmml">(</mo><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><msub id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml"><mi id="S3.E1.m1.1.1.2.2.2" xref="S3.E1.m1.1.1.2.2.2.cmml">Q</mi><mi id="S3.E1.m1.1.1.2.2.3" xref="S3.E1.m1.1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.2.1" xref="S3.E1.m1.1.1.2.1.cmml">​</mo><msubsup id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.2.3.2.2" xref="S3.E1.m1.1.1.2.3.2.2.cmml">K</mi><mi id="S3.E1.m1.1.1.2.3.2.3" xref="S3.E1.m1.1.1.2.3.2.3.cmml">j</mi><mi id="S3.E1.m1.1.1.2.3.3" xref="S3.E1.m1.1.1.2.3.3.cmml">T</mi></msubsup></mrow><msqrt id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">d</mi><mi id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo id="S3.E1.m1.1.2.3.3.2.2" xref="S3.E1.m1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.3.1a" xref="S3.E1.m1.1.2.3.1.cmml">​</mo><msub id="S3.E1.m1.1.2.3.4" xref="S3.E1.m1.1.2.3.4.cmml"><mi id="S3.E1.m1.1.2.3.4.2" xref="S3.E1.m1.1.2.3.4.2.cmml">V</mi><mi id="S3.E1.m1.1.2.3.4.3" xref="S3.E1.m1.1.2.3.4.3.cmml">j</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.2.cmml" xref="S3.E1.m1.1.2"><eq id="S3.E1.m1.1.2.1.cmml" xref="S3.E1.m1.1.2.1"></eq><apply id="S3.E1.m1.1.2.2.cmml" xref="S3.E1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.2.1.cmml" xref="S3.E1.m1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.1.2.2.2.cmml" xref="S3.E1.m1.1.2.2.2">𝐶</ci><apply id="S3.E1.m1.1.2.2.3.cmml" xref="S3.E1.m1.1.2.2.3"><times id="S3.E1.m1.1.2.2.3.1.cmml" xref="S3.E1.m1.1.2.2.3.1"></times><ci id="S3.E1.m1.1.2.2.3.2.cmml" xref="S3.E1.m1.1.2.2.3.2">𝑖</ci><ci id="S3.E1.m1.1.2.2.3.3.cmml" xref="S3.E1.m1.1.2.2.3.3">𝑗</ci></apply></apply><apply id="S3.E1.m1.1.2.3.cmml" xref="S3.E1.m1.1.2.3"><times id="S3.E1.m1.1.2.3.1.cmml" xref="S3.E1.m1.1.2.3.1"></times><ci id="S3.E1.m1.1.2.3.2a.cmml" xref="S3.E1.m1.1.2.3.2"><mtext id="S3.E1.m1.1.2.3.2.cmml" xref="S3.E1.m1.1.2.3.2">softmax</mtext></ci><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.2.3.3.2"><divide id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.2.3.3.2"></divide><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><times id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2.1"></times><apply id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.2.2.2">𝑄</ci><ci id="S3.E1.m1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.2.2.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.2.3">superscript</csymbol><apply id="S3.E1.m1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.3.2.1.cmml" xref="S3.E1.m1.1.1.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.2.3.2.2.cmml" xref="S3.E1.m1.1.1.2.3.2.2">𝐾</ci><ci id="S3.E1.m1.1.1.2.3.2.3.cmml" xref="S3.E1.m1.1.1.2.3.2.3">𝑗</ci></apply><ci id="S3.E1.m1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><root id="S3.E1.m1.1.1.3a.cmml" xref="S3.E1.m1.1.1.3"></root><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">𝑑</ci><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply><apply id="S3.E1.m1.1.2.3.4.cmml" xref="S3.E1.m1.1.2.3.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.4.1.cmml" xref="S3.E1.m1.1.2.3.4">subscript</csymbol><ci id="S3.E1.m1.1.2.3.4.2.cmml" xref="S3.E1.m1.1.2.3.4.2">𝑉</ci><ci id="S3.E1.m1.1.2.3.4.3.cmml" xref="S3.E1.m1.1.2.3.4.3">𝑗</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">C_{ij}=\text{softmax}\left(\frac{Q_{i}K_{j}^{T}}{\sqrt{d_{k}}}\right)V_{j}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.9" class="ltx_p">where <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="C_{ij}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">C</mi><mrow id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.3.1" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝐶</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><times id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.1"></times><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">𝑖</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">C_{ij}</annotation></semantics></math> is the output of cross-channel attention between channel <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">i</annotation></semantics></math> and <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">j</annotation></semantics></math>, <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="Q_{i}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">Q</mi><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">𝑄</ci><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">Q_{i}</annotation></semantics></math> is the query coming from channel <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">i</annotation></semantics></math>, <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="K_{j}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">K</mi><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝐾</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">K_{j}</annotation></semantics></math>, and <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="V_{j}" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><msub id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">V</mi><mi id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2">𝑉</ci><ci id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">V_{j}</annotation></semantics></math> represents key and value originating from channel <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mi id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><ci id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">j</annotation></semantics></math>, and <math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="d_{k}" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><msub id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml"><mi id="S3.SS2.p2.9.m9.1.1.2" xref="S3.SS2.p2.9.m9.1.1.2.cmml">d</mi><mi id="S3.SS2.p2.9.m9.1.1.3" xref="S3.SS2.p2.9.m9.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><apply id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.p2.9.m9.1.1.2">𝑑</ci><ci id="S3.SS2.p2.9.m9.1.1.3.cmml" xref="S3.SS2.p2.9.m9.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">d_{k}</annotation></semantics></math> denotes the dimension of key. To enhance the multimodal fusion, we also concatenate the raw windows from RGB/IR channels and the obtained cross-channel attention feature maps to generate the final multimodal input for further processing. Note that this method can be easily applied to multiple modalities involving different channels such as RGB images, IR, HSI and LiDAR.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Convolutional-shifting window-based backbone</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Since the objects in RSI are often small and densely packed into a few pixels, we have modified the Swin-like backbone with a higher number of blocks in the initial stage where the resolution remains high, while progressively reducing the number of blocks in later stages decreasing the resolution by a factor of 2. This backbone enables us to learn hierarchical multi-resolution features in a fine-to-coarse manner to detect small objects (see Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Proposed method ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a)).</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">A well-recognized limitation of window-based Vision Transformers is their segregation of neighboring patches across different windows. To address this challenge, the Swin Transformer introduces a shifting window mechanism, albeit restricted to only half of its blocks. In our approach, we seek to enhance connectivity across all blocks and imbue the architecture with a heightened sense of locality. To achieve this, we introduce an extra convolutional layer positioned between two Fully Connected (FC) layers within the FFN while keeping the dimension fixed (the orange block as shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Proposed method ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b)). This augmentation not only promotes greater coherence but also enhances the network’s perception of spatial proximity.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.4" class="ltx_p">All the experiments are performed using the VEDAI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The dataset has been divided into 10 folds for cross-validation evaluation. Following the same protocol as SuperYOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, we use the first folder for the ablation studies, and all 10 folders for overall evaluation comparing with the state-of-the-art methods. We considered eight classes in the dataset and ignored classes that have under 50 instances in the dataset. The standard Stochastic Gradient Descent (SGD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> is used to optimize the network with a momentum of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="0.937" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">0.937</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="float" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">0.937</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">0.937</annotation></semantics></math> and weight decay of <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="0.0005" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">0.0005</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><cn type="float" id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">0.0005</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">0.0005</annotation></semantics></math>. The models were trained for 300 epochs using Nvidia A100 GPUs.We used the standard detection loss (combining localization, classification, and confidence losses) to train the model and evaluate the performance using <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="mAP_{50}" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.1a" xref="S4.SS1.p1.3.m3.1.1.1.cmml">​</mo><msub id="S4.SS1.p1.3.m3.1.1.4" xref="S4.SS1.p1.3.m3.1.1.4.cmml"><mi id="S4.SS1.p1.3.m3.1.1.4.2" xref="S4.SS1.p1.3.m3.1.1.4.2.cmml">P</mi><mn id="S4.SS1.p1.3.m3.1.1.4.3" xref="S4.SS1.p1.3.m3.1.1.4.3.cmml">50</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><times id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1"></times><ci id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">𝑚</ci><ci id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3">𝐴</ci><apply id="S4.SS1.p1.3.m3.1.1.4.cmml" xref="S4.SS1.p1.3.m3.1.1.4"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.1.1.4.1.cmml" xref="S4.SS1.p1.3.m3.1.1.4">subscript</csymbol><ci id="S4.SS1.p1.3.m3.1.1.4.2.cmml" xref="S4.SS1.p1.3.m3.1.1.4.2">𝑃</ci><cn type="integer" id="S4.SS1.p1.3.m3.1.1.4.3.cmml" xref="S4.SS1.p1.3.m3.1.1.4.3">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">mAP_{50}</annotation></semantics></math>, i.e., detection metric of mean Average Precision at IOU (Intersection over Union) <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mo id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><eq id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">=</annotation></semantics></math> 0.5 for all categories.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.2.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.3.1" class="ltx_text ltx_font_bold">Car</span></td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.4.1" class="ltx_text ltx_font_bold">Pickup</span></td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.5.1" class="ltx_text ltx_font_bold">Camping</span></td>
<td id="S4.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.6.1" class="ltx_text ltx_font_bold">Truck</span></td>
<td id="S4.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.7.1" class="ltx_text ltx_font_bold">Other</span></td>
<td id="S4.T1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.8.1" class="ltx_text ltx_font_bold">Tractor</span></td>
<td id="S4.T1.1.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.9.1" class="ltx_text ltx_font_bold">Boat</span></td>
<td id="S4.T1.1.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.10.1" class="ltx_text ltx_font_bold">Van</span></td>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\textbf{mAP}_{\textbf{50}}" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><msub id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S4.T1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.m1.1.1.2a.cmml">mAP</mtext><mtext class="ltx_mathvariant_bold" id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3a.cmml">50</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.1.1.1.m1.1.1.2a.cmml" xref="S4.T1.1.1.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2">mAP</mtext></ci><ci id="S4.T1.1.1.1.m1.1.1.3a.cmml" xref="S4.T1.1.1.1.m1.1.1.3"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="S4.T1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3">50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\textbf{mAP}_{\textbf{50}}</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.1.2" class="ltx_tr">
<td id="S4.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</td>
<td id="S4.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.57</td>
<td id="S4.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.68</td>
<td id="S4.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.13</td>
<td id="S4.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.96</td>
<td id="S4.T1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">43.04</td>
<td id="S4.T1.1.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.24</td>
<td id="S4.T1.1.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">37.10</td>
<td id="S4.T1.1.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">58.29</td>
<td id="S4.T1.1.2.10" class="ltx_td ltx_align_center ltx_border_t">61.26</td>
</tr>
<tr id="S4.T1.1.3" class="ltx_tr">
<td id="S4.T1.1.3.1" class="ltx_td ltx_align_center ltx_border_r">YOLOv4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S4.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_r">85.46</td>
<td id="S4.T1.1.3.3" class="ltx_td ltx_align_center ltx_border_r">72.84</td>
<td id="S4.T1.1.3.4" class="ltx_td ltx_align_center ltx_border_r">72.38</td>
<td id="S4.T1.1.3.5" class="ltx_td ltx_align_center ltx_border_r">62.82</td>
<td id="S4.T1.1.3.6" class="ltx_td ltx_align_center ltx_border_r">48.94</td>
<td id="S4.T1.1.3.7" class="ltx_td ltx_align_center ltx_border_r">68.99</td>
<td id="S4.T1.1.3.8" class="ltx_td ltx_align_center ltx_border_r">34.28</td>
<td id="S4.T1.1.3.9" class="ltx_td ltx_align_center ltx_border_r">54.66</td>
<td id="S4.T1.1.3.10" class="ltx_td ltx_align_center">62.55</td>
</tr>
<tr id="S4.T1.1.4" class="ltx_tr">
<td id="S4.T1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S4.T1.1.4.2" class="ltx_td ltx_align_center ltx_border_r">84.33</td>
<td id="S4.T1.1.4.3" class="ltx_td ltx_align_center ltx_border_r">72.95</td>
<td id="S4.T1.1.4.4" class="ltx_td ltx_align_center ltx_border_r">70.09</td>
<td id="S4.T1.1.4.5" class="ltx_td ltx_align_center ltx_border_r">61.15</td>
<td id="S4.T1.1.4.6" class="ltx_td ltx_align_center ltx_border_r">49.94</td>
<td id="S4.T1.1.4.7" class="ltx_td ltx_align_center ltx_border_r">67.35</td>
<td id="S4.T1.1.4.8" class="ltx_td ltx_align_center ltx_border_r">38.71</td>
<td id="S4.T1.1.4.9" class="ltx_td ltx_align_center ltx_border_r">56.65</td>
<td id="S4.T1.1.4.10" class="ltx_td ltx_align_center">62.65</td>
</tr>
<tr id="S4.T1.1.5" class="ltx_tr">
<td id="S4.T1.1.5.1" class="ltx_td ltx_align_center ltx_border_r">YOLOrs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S4.T1.1.5.2" class="ltx_td ltx_align_center ltx_border_r">84.15</td>
<td id="S4.T1.1.5.3" class="ltx_td ltx_align_center ltx_border_r">78.27</td>
<td id="S4.T1.1.5.4" class="ltx_td ltx_align_center ltx_border_r">68.81</td>
<td id="S4.T1.1.5.5" class="ltx_td ltx_align_center ltx_border_r">52.60</td>
<td id="S4.T1.1.5.6" class="ltx_td ltx_align_center ltx_border_r">46.75</td>
<td id="S4.T1.1.5.7" class="ltx_td ltx_align_center ltx_border_r">67.88</td>
<td id="S4.T1.1.5.8" class="ltx_td ltx_align_center ltx_border_r">21.47</td>
<td id="S4.T1.1.5.9" class="ltx_td ltx_align_center ltx_border_r">57.91</td>
<td id="S4.T1.1.5.10" class="ltx_td ltx_align_center">59.73</td>
</tr>
<tr id="S4.T1.1.6" class="ltx_tr">
<td id="S4.T1.1.6.1" class="ltx_td ltx_align_center ltx_border_r">YOLO-Fine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</td>
<td id="S4.T1.1.6.2" class="ltx_td ltx_align_center ltx_border_r">79.68</td>
<td id="S4.T1.1.6.3" class="ltx_td ltx_align_center ltx_border_r">74.49</td>
<td id="S4.T1.1.6.4" class="ltx_td ltx_align_center ltx_border_r">77.09</td>
<td id="S4.T1.1.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.6.5.1" class="ltx_text ltx_font_bold">80.97</span></td>
<td id="S4.T1.1.6.6" class="ltx_td ltx_align_center ltx_border_r">37.33</td>
<td id="S4.T1.1.6.7" class="ltx_td ltx_align_center ltx_border_r">70.65</td>
<td id="S4.T1.1.6.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.6.8.1" class="ltx_text ltx_font_bold">60.84</span></td>
<td id="S4.T1.1.6.9" class="ltx_td ltx_align_center ltx_border_r">63.56</td>
<td id="S4.T1.1.6.10" class="ltx_td ltx_align_center">68.83</td>
</tr>
<tr id="S4.T1.1.7" class="ltx_tr">
<td id="S4.T1.1.7.1" class="ltx_td ltx_align_center ltx_border_r">SuperYOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S4.T1.1.7.2" class="ltx_td ltx_align_center ltx_border_r">89.30</td>
<td id="S4.T1.1.7.3" class="ltx_td ltx_align_center ltx_border_r">81.48</td>
<td id="S4.T1.1.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.7.4.1" class="ltx_text ltx_font_bold">79.22</span></td>
<td id="S4.T1.1.7.5" class="ltx_td ltx_align_center ltx_border_r">67.27</td>
<td id="S4.T1.1.7.6" class="ltx_td ltx_align_center ltx_border_r">54.29</td>
<td id="S4.T1.1.7.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.7.7.1" class="ltx_text ltx_font_bold">78.88</span></td>
<td id="S4.T1.1.7.8" class="ltx_td ltx_align_center ltx_border_r">55.95</td>
<td id="S4.T1.1.7.9" class="ltx_td ltx_align_center ltx_border_r">71.41</td>
<td id="S4.T1.1.7.10" class="ltx_td ltx_align_center">72.22</td>
</tr>
<tr id="S4.T1.1.8" class="ltx_tr">
<td id="S4.T1.1.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.1.8.1.1" class="ltx_text ltx_font_bold">Ours</span></td>
<td id="S4.T1.1.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.1.8.2.1" class="ltx_text ltx_font_bold">89.13</span></td>
<td id="S4.T1.1.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.1.8.3.1" class="ltx_text ltx_font_bold">82.70</span></td>
<td id="S4.T1.1.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">76.38</td>
<td id="S4.T1.1.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">61.57</td>
<td id="S4.T1.1.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.1.8.6.1" class="ltx_text ltx_font_bold">56.32</span></td>
<td id="S4.T1.1.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">77.94</td>
<td id="S4.T1.1.8.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">60.36</td>
<td id="S4.T1.1.8.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.1.8.9.1" class="ltx_text ltx_font_bold">75.84</span></td>
<td id="S4.T1.1.8.10" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.1.8.10.1" class="ltx_text ltx_font_bold">72.53</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.5.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Class-wise mean Average Precision <math id="S4.T1.3.m1.1" class="ltx_Math" alttext="mAP_{50}" display="inline"><semantics id="S4.T1.3.m1.1b"><mrow id="S4.T1.3.m1.1.1" xref="S4.T1.3.m1.1.1.cmml"><mi id="S4.T1.3.m1.1.1.2" xref="S4.T1.3.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.m1.1.1.1" xref="S4.T1.3.m1.1.1.1.cmml">​</mo><mi id="S4.T1.3.m1.1.1.3" xref="S4.T1.3.m1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.m1.1.1.1b" xref="S4.T1.3.m1.1.1.1.cmml">​</mo><msub id="S4.T1.3.m1.1.1.4" xref="S4.T1.3.m1.1.1.4.cmml"><mi id="S4.T1.3.m1.1.1.4.2" xref="S4.T1.3.m1.1.1.4.2.cmml">P</mi><mn id="S4.T1.3.m1.1.1.4.3" xref="S4.T1.3.m1.1.1.4.3.cmml">50</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.m1.1c"><apply id="S4.T1.3.m1.1.1.cmml" xref="S4.T1.3.m1.1.1"><times id="S4.T1.3.m1.1.1.1.cmml" xref="S4.T1.3.m1.1.1.1"></times><ci id="S4.T1.3.m1.1.1.2.cmml" xref="S4.T1.3.m1.1.1.2">𝑚</ci><ci id="S4.T1.3.m1.1.1.3.cmml" xref="S4.T1.3.m1.1.1.3">𝐴</ci><apply id="S4.T1.3.m1.1.1.4.cmml" xref="S4.T1.3.m1.1.1.4"><csymbol cd="ambiguous" id="S4.T1.3.m1.1.1.4.1.cmml" xref="S4.T1.3.m1.1.1.4">subscript</csymbol><ci id="S4.T1.3.m1.1.1.4.2.cmml" xref="S4.T1.3.m1.1.1.4.2">𝑃</ci><cn type="integer" id="S4.T1.3.m1.1.1.4.3.cmml" xref="S4.T1.3.m1.1.1.4.3">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.m1.1d">mAP_{50}</annotation></semantics></math> for our proposed method comparing to the state-of-art on VEDAI Dataset.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Cross-channel attention</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We verified the effectiveness of our proposed Cross-Channel (CC) attention both on CNN-based and ViT-based architectures such as SuperYOLO’s backbone and our proposed Swin-like backbone as shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Cross-channel attention ‣ 4 Experiments ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For the SuperYOLO’s backbone, CC attention outperforms the Pixel-level fusion and the Multimodal Feature-level (MF) fusion used in SuperYOLO. For the ViT-based backbone, the CC attention outperforms the RGB-IR concatenation by 3.3% and improves by 15% and  8% compared to using only IR or RGB images respectively. The results obtained from the different architectures highlight the effectiveness of the proposed cross-channel attention module for multimodal fusion.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T2.1.1.2.1" class="ltx_text ltx_font_bold">Architecture</span></td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.3.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\textbf{mAP}_{\textbf{50}}" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.2a.cmml">mAP</mtext><mtext class="ltx_mathvariant_bold" id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3a.cmml">50</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.m1.1.1.2a.cmml" xref="S4.T2.1.1.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S4.T2.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2">mAP</mtext></ci><ci id="S4.T2.1.1.1.m1.1.1.3a.cmml" xref="S4.T2.1.1.1.m1.1.1.3"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="S4.T2.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3">50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\textbf{mAP}_{\textbf{50}}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T2.1.2.1.1" class="ltx_text"><span id="S4.T2.1.2.1.1.1" class="ltx_text"></span> <span id="S4.T2.1.2.1.1.2" class="ltx_text">
<span id="S4.T2.1.2.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.2.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.2.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">CNN-based</span></span>
<span id="S4.T2.1.2.1.1.2.1.2" class="ltx_tr">
<span id="S4.T2.1.2.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(SuperYOLO)</span></span>
</span></span> <span id="S4.T2.1.2.1.1.3" class="ltx_text"></span></span></td>
<td id="S4.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Pixel fusion</td>
<td id="S4.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">76.90</td>
</tr>
<tr id="S4.T2.1.3" class="ltx_tr">
<td id="S4.T2.1.3.1" class="ltx_td ltx_align_center ltx_border_r">MF fusion</td>
<td id="S4.T2.1.3.2" class="ltx_td ltx_align_center">77.73</td>
</tr>
<tr id="S4.T2.1.4" class="ltx_tr">
<td id="S4.T2.1.4.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.4.1.1" class="ltx_text ltx_font_bold">CC attention</span></td>
<td id="S4.T2.1.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.2.1" class="ltx_text ltx_font_bold">77.9</span></td>
</tr>
<tr id="S4.T2.1.5" class="ltx_tr">
<td id="S4.T2.1.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T2.1.5.1.1" class="ltx_text"><span id="S4.T2.1.5.1.1.1" class="ltx_text"></span> <span id="S4.T2.1.5.1.1.2" class="ltx_text">
<span id="S4.T2.1.5.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.5.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.5.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">ViT-based</span></span>
<span id="S4.T2.1.5.1.1.2.1.2" class="ltx_tr">
<span id="S4.T2.1.5.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(Ours)</span></span>
</span></span> <span id="S4.T2.1.5.1.1.3" class="ltx_text"></span></span></td>
<td id="S4.T2.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">IR</td>
<td id="S4.T2.1.5.3" class="ltx_td ltx_align_center ltx_border_t">63.79</td>
</tr>
<tr id="S4.T2.1.6" class="ltx_tr">
<td id="S4.T2.1.6.1" class="ltx_td ltx_align_center ltx_border_r">RGB</td>
<td id="S4.T2.1.6.2" class="ltx_td ltx_align_center">70.55</td>
</tr>
<tr id="S4.T2.1.7" class="ltx_tr">
<td id="S4.T2.1.7.1" class="ltx_td ltx_align_center ltx_border_r">RGB-IR concatenation</td>
<td id="S4.T2.1.7.2" class="ltx_td ltx_align_center">74.23</td>
</tr>
<tr id="S4.T2.1.8" class="ltx_tr">
<td id="S4.T2.1.8.1" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S4.T2.1.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.1.8.2.1" class="ltx_text ltx_font_bold">RGB-IR CC attention</span></td>
<td id="S4.T2.1.8.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.8.3.1" class="ltx_text ltx_font_bold">78.53</span></td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>The cross-channel attention based on CNN-based and ViT-based backbones on VEDAI dataset (Fold-1).</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison of different backbones</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We compare different backbones for multi-modal object detection in Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Comparison of different backbones ‣ 4 Experiments ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Specifically, we compare our proposed backbone with the original Swin Transformer and the backbone of SuperYOLO (i.e., CSP-Darknet as used in YOLOs) with and without the use of the Super-Resolution (SR) module. As shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Comparison of different backbones ‣ 4 Experiments ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the original Swin Transformer obtained the lowest score due to the overfitting issue on the small dataset. However, our proposed backbone achieves the best result showing the effectiveness of the modification.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.2.1" class="ltx_text ltx_font_bold">Backbone</span></td>
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="\textbf{mAP}_{\textbf{50}}" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><msub id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S4.T3.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.m1.1.1.2a.cmml">mAP</mtext><mtext class="ltx_mathvariant_bold" id="S4.T3.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.m1.1.1.3a.cmml">50</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T3.1.1.1.m1.1.1.2a.cmml" xref="S4.T3.1.1.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S4.T3.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.m1.1.1.2">mAP</mtext></ci><ci id="S4.T3.1.1.1.m1.1.1.3a.cmml" xref="S4.T3.1.1.1.m1.1.1.3"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="S4.T3.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.m1.1.1.3">50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\textbf{mAP}_{\textbf{50}}</annotation></semantics></math></td>
</tr>
<tr id="S4.T3.1.2" class="ltx_tr">
<td id="S4.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SuperYOLO (with SR)</td>
<td id="S4.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t">76.63</td>
</tr>
<tr id="S4.T3.1.3" class="ltx_tr">
<td id="S4.T3.1.3.1" class="ltx_td ltx_align_center ltx_border_r">SuperYOLO (w/o SR)</td>
<td id="S4.T3.1.3.2" class="ltx_td ltx_align_center">77.73</td>
</tr>
<tr id="S4.T3.1.4" class="ltx_tr">
<td id="S4.T3.1.4.1" class="ltx_td ltx_align_center ltx_border_r">SuperYOLO (w/o SR) with CC attention)</td>
<td id="S4.T3.1.4.2" class="ltx_td ltx_align_center">77.9</td>
</tr>
<tr id="S4.T3.1.5" class="ltx_tr">
<td id="S4.T3.1.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Swin transformer</td>
<td id="S4.T3.1.5.2" class="ltx_td ltx_align_center ltx_border_t">67.27</td>
</tr>
<tr id="S4.T3.1.6" class="ltx_tr">
<td id="S4.T3.1.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.1.6.1.1" class="ltx_text ltx_font_bold">Ours</span></td>
<td id="S4.T3.1.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T3.1.6.2.1" class="ltx_text ltx_font_bold">78.53</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>The comparisons of different backbones using Fold-1 of VEDAI dataset</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Convolutional-shifting window</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.4" class="ltx_p">Figure <a href="#S4.F4" title="Figure 4 ‣ 4.4 Convolutional-shifting window ‣ 4 Experiments ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (left) demonstrates the effectiveness of the proposed convolutional-shifting window. When adding a convolution layer inside the FFN in non-shifting blocks at stage 1, the model outperforms the FFN without convolution by 4.5% in terms of the <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="mAP_{50}" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.1.m1.1.1.1a" xref="S4.SS4.p1.1.m1.1.1.1.cmml">​</mo><msub id="S4.SS4.p1.1.m1.1.1.4" xref="S4.SS4.p1.1.m1.1.1.4.cmml"><mi id="S4.SS4.p1.1.m1.1.1.4.2" xref="S4.SS4.p1.1.m1.1.1.4.2.cmml">P</mi><mn id="S4.SS4.p1.1.m1.1.1.4.3" xref="S4.SS4.p1.1.m1.1.1.4.3.cmml">50</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><times id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1"></times><ci id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">𝑚</ci><ci id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">𝐴</ci><apply id="S4.SS4.p1.1.m1.1.1.4.cmml" xref="S4.SS4.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.SS4.p1.1.m1.1.1.4.1.cmml" xref="S4.SS4.p1.1.m1.1.1.4">subscript</csymbol><ci id="S4.SS4.p1.1.m1.1.1.4.2.cmml" xref="S4.SS4.p1.1.m1.1.1.4.2">𝑃</ci><cn type="integer" id="S4.SS4.p1.1.m1.1.1.4.3.cmml" xref="S4.SS4.p1.1.m1.1.1.4.3">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">mAP_{50}</annotation></semantics></math>. Furthermore, introducing convolution at stages 1 and 2 gains 5.2% improvement. These results indicate the effectiveness of using convolution in FFN. We also investigate the impact of shifting size for convolutional-shifting windows,
which shows that a smaller shifting size of 2 performs <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="2.4\%" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mrow id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mn id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">2.4</mn><mo id="S4.SS4.p1.2.m2.1.1.1" xref="S4.SS4.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">2.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">2.4\%</annotation></semantics></math> better than the shifting size of 4 (<math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="75.75\%" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><mrow id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mn id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">75.75</mn><mo id="S4.SS4.p1.3.m3.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p1.3.m3.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2">75.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">75.75\%</annotation></semantics></math> v.s.<math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="73.34\%" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><mrow id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mn id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">73.34</mn><mo id="S4.SS4.p1.4.m4.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.2">73.34</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">73.34\%</annotation></semantics></math>) indicating the fine-grained details captured from neighboring patches are more important to detect small objects.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:101.3pt;"><img src="/html/2310.13876/assets/x4.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="339" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:126.6pt;"><img src="/html/2310.13876/assets/x5.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="228" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text ltx_font_bold">Fig. 4</span>: </span>The figure on the left shows the effect of the convolution in FFN at the 1st and the 2nd stages; the right one shows the impact of window size of the cross-channel attention. </figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Window-size in cross-channel attention</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">The right of Figure <a href="#S4.F4" title="Figure 4 ‣ 4.4 Convolutional-shifting window ‣ 4 Experiments ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the impact of the window size in cross-channel attention. We can see that the cross-channel attention with a window size of 1 (including one patch) performs the best in terms of <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="mAP_{50}" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mrow id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml"><mi id="S4.SS5.p1.1.m1.1.1.2" xref="S4.SS5.p1.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p1.1.m1.1.1.1" xref="S4.SS5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS5.p1.1.m1.1.1.3" xref="S4.SS5.p1.1.m1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p1.1.m1.1.1.1a" xref="S4.SS5.p1.1.m1.1.1.1.cmml">​</mo><msub id="S4.SS5.p1.1.m1.1.1.4" xref="S4.SS5.p1.1.m1.1.1.4.cmml"><mi id="S4.SS5.p1.1.m1.1.1.4.2" xref="S4.SS5.p1.1.m1.1.1.4.2.cmml">P</mi><mn id="S4.SS5.p1.1.m1.1.1.4.3" xref="S4.SS5.p1.1.m1.1.1.4.3.cmml">50</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><apply id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"><times id="S4.SS5.p1.1.m1.1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1.1"></times><ci id="S4.SS5.p1.1.m1.1.1.2.cmml" xref="S4.SS5.p1.1.m1.1.1.2">𝑚</ci><ci id="S4.SS5.p1.1.m1.1.1.3.cmml" xref="S4.SS5.p1.1.m1.1.1.3">𝐴</ci><apply id="S4.SS5.p1.1.m1.1.1.4.cmml" xref="S4.SS5.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.SS5.p1.1.m1.1.1.4.1.cmml" xref="S4.SS5.p1.1.m1.1.1.4">subscript</csymbol><ci id="S4.SS5.p1.1.m1.1.1.4.2.cmml" xref="S4.SS5.p1.1.m1.1.1.4.2">𝑃</ci><cn type="integer" id="S4.SS5.p1.1.m1.1.1.4.3.cmml" xref="S4.SS5.p1.1.m1.1.1.4.3">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">mAP_{50}</annotation></semantics></math>. Interestingly, increasing the window size does not lead to the improvement. This shows that the small window focusing on local region information is more pertinent for detecting small objects in RSI.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Overall performance</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">The overall comparison of our model with SuperYOLO is shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Experimental setup ‣ 4 Experiments ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Although the proposed ViT-based model is not pre-trained, it achieves competitive results and outperforms the state-of-the-art CNN model by 0.3%. Additionally, our method outperforms SuperYOLO in detecting difficult classes with the least number of instances in the training set, namely the Boat, Van, and Other classes. Fig <a href="#S4.F5" title="Figure 5 ‣ 4.6 Overall performance ‣ 4 Experiments ‣ Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows a visual comparison of our method and SuperYOLO for two different scenes where only our method has successfully detected and correctly classified the objects.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2310.13876/assets/x6.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="110" height="78" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text ltx_font_bold">Fig. 5</span>: </span>Visual results using our method and SuperYOLO.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper introduces a new cross-channel attention module that allows for aligning different modalities by learning the relationship between different channels at the early stage instead of combining multimodal inputs using simple channel-wise concatenation. Furthermore, the convolutional-shifting window which incorporates convolutional layers in FFN is proposed to learn the hierarchical features in a fine-to-coarse manner enhancing the detection of small objects. The extensive experiments demonstrate the superiority of the proposed approach, highlighting its applicability for object detection using multimodal aerial imagery.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Muhammad Ahmad, Sidrah Shabbir, and et. al,

</span>
<span class="ltx_bibblock">“Hyperspectral image classification—traditional to deep models: A survey for future prospects,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE journal of selected topics in applied earth observations and remote sensing</span>, vol. 15, pp. 968–999, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Danfeng Hong, Wei He, and et. al,

</span>
<span class="ltx_bibblock">“Interpretable hyperspectral artificial intelligence: When nonconvex modeling meets hyperspectral remote sensing,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE Geoscience and Remote Sensing Magazine</span>, vol. 9, no. 2, pp. 52–87, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jian Ding, Nan Xue, and et. al,

</span>
<span class="ltx_bibblock">“Object detection in aerial images: A large-scale benchmark and challenges,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>, vol. 44, no. 11, pp. 7778–7796, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, and et. al,

</span>
<span class="ltx_bibblock">“Microsoft coco: Common objects in context,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</span>. Springer, 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Xian Sun, Bing Wang, and et. al,

</span>
<span class="ltx_bibblock">“Research progress on few-shot learning for remote sensing image interpretation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</span>, vol. 14, pp. 2387–2402, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chang Xu, Jinwang Wang, and et. al,

</span>
<span class="ltx_bibblock">“Detecting tiny objects in aerial images: A normalized wasserstein distance and a new benchmark,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">ISPRS Journal of Photogrammetry and Remote Sensing</span>, vol. 190, pp. 79–93, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Luis Gómez-Chova, Devis Tuia, and et. al,

</span>
<span class="ltx_bibblock">“Multimodal classification of remote sensing images: A review and future directions,”

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, vol. 103, no. 9, pp. 1560–1584, 2015.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Jiaqing Zhang, Jie Lei, and et al.,

</span>
<span class="ltx_bibblock">“Superyolo: Super resolution assisted object detection in multimodal remote sensing imagery,”

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</span>, vol. 61, pp. 1–15, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Manish Sharma, Mayur Dhanaraj, and et. al,

</span>
<span class="ltx_bibblock">“Yolors: Object detection in multimodal remote sensing imagery,”

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</span>, vol. 14, pp. 1497–1508, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, and et. al,

</span>
<span class="ltx_bibblock">“An image is worth 16x16 words: Transformers for image recognition at scale,”

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.11929</span>, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, and et. al,

</span>
<span class="ltx_bibblock">“Swin transformer: Hierarchical vision transformer using shifted windows,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF ICCV</span>, 2021, pp. 10012–10022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Yuhui Yuan, Rao Fu, and et. al,

</span>
<span class="ltx_bibblock">“HRformer: High-resolution vision transformer for dense predict,”

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, vol. 34, pp. 7281–7293, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Haoqi Fan, Bo Xiong, and et. al,

</span>
<span class="ltx_bibblock">“Multiscale vision transformers,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF ICCV</span>, 2021, pp. 6824–6835.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Xiangkai Xu, Zhejun Feng, and et. al,

</span>
<span class="ltx_bibblock">“An improved swin transformer-based model for remote sensing object detection and instance segmentation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Remote Sensing</span>, vol. 13, no. 23, pp. 4779, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Xuan Cao, Yanwei Zhang, and et. al,

</span>
<span class="ltx_bibblock">“Swin-transformer-based yolov5 for small-object detection in remote sensing images,”

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 23, no. 7, pp. 3634, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Haiping Wu, Bin Xiao, and et. al,

</span>
<span class="ltx_bibblock">“Cvt: Introducing convolutions to vision transformers,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF ICCV</span>, 2021, pp. 22–31.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Zuheng Ming, Zitong Yu, and et. al,

</span>
<span class="ltx_bibblock">“Vitranspad: video transformer using convolution and self-attention for face presentation attack detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">2022 IEEE International Conference on Image Processing (ICIP)</span>. IEEE, 2022, pp. 4248–4252.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Sebastien Razakarivony and Frederic Jurie,

</span>
<span class="ltx_bibblock">“Vehicle detection in aerial imagery: A small target detection benchmark,”

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Journal of Visual Communication and Image Representation</span>, vol. 34, pp. 187–203, 2016.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Herbert Robbins and Sutton Monro,

</span>
<span class="ltx_bibblock">“A stochastic approximation method,”

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">The annals of mathematical statistics</span>, pp. 400–407, 1951.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Joseph Redmon and Ali Farhadi,

</span>
<span class="ltx_bibblock">“Yolov3: An incremental improvement,”

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.02767</span>, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Alexey Bochkovskiy, Chien-Yao Wang, and et. al,

</span>
<span class="ltx_bibblock">“Yolov4: Optimal speed and accuracy of object detection,”

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.10934</span>, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
G. J. et al,

</span>
<span class="ltx_bibblock">“ultralyticsyolo:v5-v5.0,”

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">https://github.com/ultralytics/yolov5</span>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Minh-Tan Pham, Luc Courtrai, and et. al,

</span>
<span class="ltx_bibblock">“Yolo-fine: One-stage detector of small objects under various backgrounds in remote sensing images,”

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Remote Sensing</span>, vol. 12, no. 15, pp. 2501, 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.13874" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.13876" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.13876">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.13876" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.13877" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 21:29:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
