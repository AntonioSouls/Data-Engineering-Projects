<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.08811] PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects</title><meta property="og:description" content="Object pose estimation is crucial for robotic applications and augmented reality. Beyond instance level 6D object pose estimation methods, estimating category-level pose and shape has become a promising trend. As such,…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.08811">

<!--Generated on Mon Mar 11 13:05:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation
<br class="ltx_break">with Photometrically Challenging Objects</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Pengyuan Wang<sup id="id15.13.id1" class="ltx_sup"><span id="id15.13.id1.1" class="ltx_text ltx_font_italic">∗1</span></sup>,
HyunJun Jung<sup id="id16.14.id2" class="ltx_sup"><span id="id16.14.id2.1" class="ltx_text ltx_font_italic">∗1</span></sup>,
Yitong Li<sup id="id17.15.id3" class="ltx_sup"><span id="id17.15.id3.1" class="ltx_text ltx_font_italic">1</span></sup>,
Siyuan Shen<sup id="id18.16.id4" class="ltx_sup"><span id="id18.16.id4.1" class="ltx_text ltx_font_italic">1</span></sup>,
Rahul Parthasarathy Srikanth<sup id="id19.17.id5" class="ltx_sup"><span id="id19.17.id5.1" class="ltx_text ltx_font_italic">1</span></sup>,
<br class="ltx_break">Lorenzo Garattoni<sup id="id20.18.id6" class="ltx_sup"><span id="id20.18.id6.1" class="ltx_text ltx_font_italic">2</span></sup>,
Sven Meier<sup id="id21.19.id7" class="ltx_sup"><span id="id21.19.id7.1" class="ltx_text ltx_font_italic">2</span></sup>,
Nassir Navab<sup id="id22.20.id8" class="ltx_sup"><span id="id22.20.id8.1" class="ltx_text ltx_font_italic">1</span></sup>,
Benjamin Busam<sup id="id23.21.id9" class="ltx_sup"><span id="id23.21.id9.1" class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"><sup id="id24.22.id10" class="ltx_sup"><span id="id24.22.id10.1" class="ltx_text ltx_font_italic">∗</span></sup> Equal Contribution  <sup id="id25.23.id11" class="ltx_sup">1</sup> Technical University of Munich  <sup id="id26.24.id12" class="ltx_sup">2</sup> Toyota Motor Europe
<br class="ltx_break"><span id="id27.25.id13" class="ltx_text ltx_font_typewriter" style="font-size:90%;">pengyuan.wang@tum.de</span>
 <span id="id28.26.id14" class="ltx_text ltx_font_typewriter" style="font-size:90%;">hyunjun.jung@tum.de</span>
 <span id="id29.27.id15" class="ltx_text ltx_font_typewriter" style="font-size:90%;">b.busam@tum.de</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id30.id1" class="ltx_p">Object pose estimation is crucial for robotic applications and augmented reality. Beyond instance level 6D object pose estimation methods, estimating category-level pose and shape has become a promising trend. As such, a new research field needs to be supported by well-designed datasets. To provide a benchmark with high-quality ground truth annotations to the community, we introduce a multimodal dataset for category-level object pose estimation with photometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high quality 3D models of household objects over 8 categories including highly reflective, transparent and symmetric objects.
We developed a novel robot-supported multi-modal (RGB, depth, polarisation) data acquisition and annotation process. It ensures sub-millimeter accuracy of the pose for opaque textured, shiny and transparent objects, no motion blur and perfect camera synchronisation.</p>
<p id="id31.id2" class="ltx_p">To set a benchmark for our dataset, state-of-the-art RGB-D and monocular RGB methods are evaluated on the challenging scenes of PhoCaL.</p>
</div>
<div id="id14" class="ltx_logical-block">
<div id="id14.p1" class="ltx_para">
<img src="/html/2205.08811/assets/images/_teaser_bbox.png" id="id13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="151" alt="[Uncaptioned image]">
</div>
<figure id="S0.F1" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.3.2" class="ltx_text" style="font-size:90%;">PhoCaL comprises 60 high quality 3D models of household object in 8 categories with different photometric complexity. The selected objects include challenging texture-less, occluded, symmetric, reflective and transparent objects. Our robotic-induced pose annotation pipeline provides highly accurate 6D pose labels even for objects that are hard to capture by modern RGBD sensors. The figure shows RGB, 3D bounding boxes and rendered Normalized Object Coordinate Space (NOCS) map for 4 example scenes.
</span></figcaption>
</figure>
</div>
<figure id="S0.F2" class="ltx_figure"><img src="/html/2205.08811/assets/images/_train_test.png" id="S0.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="198" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S0.F2.3.2" class="ltx_text" style="font-size:90%;">Our dataset comprises 60 household objects among 8 object categories. The training and test split is depicted here.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Vision systems interacting with their environment need to estimate the position and orientation of objects in space, which highlights why 6D object pose estimation is an important task for robotic applications. Even though there have been great advances in the field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, instance-level 6D pose methods require pre-scanned object models and support limited number of objects. Category-level object pose estimation  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> scales better to the needs of real operating environments. However, photometrically challenging objects such as shiny, e.g. metallic, and transparent, e.g. glass, objects are very common in our daily life and little work has been done to estimate their 6D poses within practical accuracy on a category-level. The difficulty arises from two aspects: first, it is difficult to annotate 6D pose ground truth for photometrically challenging objects since no texture can be used to determine key points; second, commonly used depth sensors fail to return the correct depth information, as structured light and stereo method often fail to correctly interpret reflection and refraction artefacts. As a consequence, RGB-D methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> do not work reliably with photometrically challenging objects.
We introduce PhoCaL, a class-level dataset of photometrically challenging objects with high-quality ground-truth annotations.
The dataset provides multi-modal data such as RGB, depth and polarization which enables investigation into object’s surface reflectance properties.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">We obtain highly accurate ground truth poses with a novel method using a collaborative robot arm in gravity compensated mode and a calibrated mechanical tip. In order to annotate the 6D pose of transparent and non-textured objects, a specially designed tip is mounted on the robot arm. With the calibrated tip, the positions of pre-defined points on the object surface are acquired on the real object and matched to a scan thereof. Using this method, the object pose can be determined with an order of magnitude more accuracy than previous methods. For transparent and textureless objects, topographic key points are used instead of textural ones. The points gathered in this way are then matched to the object model in a final ICP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> step to yield an accurate fit.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The camera to robot end-effector transformation is needed to obtain the object poses in camera coordinates. Typically, hand-eye calibration approaches solve this by visually estimating the marker position and optimizing for the transformation between camera and end-effector. To minimize the error propagation and obtain highly accurate ground truth labels, we instead used the end-effector tip of the arm in gravity-compensated mode to measure the position of 12 points on a ChArUco <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> board. This allows us to use the robot’s accurate position system to obtain both object poses and camera poses for image sequences.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Beyond photometrically challenging categories and high-quality annotations, multi-modal input is another highlight of PhoCaL. As the active depth sensors fail on metallic and transparent surfaces, we include an additional passive sensor modality in the form of a polarization camera. It provides valuable information on object surfaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. In our setup, we designed and 3D printed a rig that holds multiple cameras, each mounted on it and carefully calibrated. During recording, a pre-defined trajectory is repeated by the robot arm. The robot arm stops when capturing images from all cameras, which avoids motion blur and diminished effects from imperfect synchronization.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In summary, our main contributions are:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">PhoCaL</span>, a <span id="S1.I1.i1.p1.1.2" class="ltx_text ltx_font_bold">multi-modal</span> (RGBD + RGBP) <span id="S1.I1.i1.p1.1.3" class="ltx_text ltx_font_bold">dataset for category-level object pose estimation</span>. The dataset comprises 60 high-quality 3D models of household objects including symmetric, transparent and reflective objects in 8 categories with 24 sequences featuring occlusion, partial visibility and clutter.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We introduce a new and <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">highly accurate pose annotation method using a robotic manipulator</span> that allows for sub-millimeter precision 6D pose annotations of photometrically challenging objects even with reflective or transparent surfaces.</p>
</div>
</li>
</ol>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.36" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.36.37.1" class="ltx_tr">
<td id="S1.T1.36.37.1.1" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt"><span id="S1.T1.36.37.1.1.1" class="ltx_text" style="font-size:90%;">Dataset</span></td>
<td id="S1.T1.36.37.1.2" class="ltx_td ltx_align_center ltx_border_tt">
<div id="S1.T1.36.37.1.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:19.8pt;vertical-align:-6.8pt;"><span class="ltx_transformed_inner" style="width:19.8pt;transform:translate(-6.83pt,0pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.2.1.1" class="ltx_p"><span id="S1.T1.36.37.1.2.1.1.1" class="ltx_text" style="font-size:90%;">RGB</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.3" class="ltx_td ltx_align_center ltx_border_tt">
<div id="S1.T1.36.37.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.0pt;height:24.3pt;vertical-align:-9.9pt;"><span class="ltx_transformed_inner" style="width:24.4pt;transform:translate(-8.19pt,2.63pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.3.1.1" class="ltx_p"><span id="S1.T1.36.37.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Depth</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<div id="S1.T1.36.37.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:47pt;vertical-align:-20.4pt;"><span class="ltx_transformed_inner" style="width:47.0pt;transform:translate(-20.35pt,0pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.4.1.1" class="ltx_p"><span id="S1.T1.36.37.1.4.1.1.1" class="ltx_text" style="font-size:90%;">Polarisation</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.5" class="ltx_td ltx_align_center ltx_border_tt">
<div id="S1.T1.36.37.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:17.6pt;vertical-align:-5.7pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.69pt,0pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.5.1.1" class="ltx_p"><span id="S1.T1.36.37.1.5.1.1.1" class="ltx_text" style="font-size:90%;">Real</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.6" class="ltx_td ltx_align_center ltx_border_tt">
<div id="S1.T1.36.37.1.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:44.5pt;vertical-align:-19.1pt;"><span class="ltx_transformed_inner" style="width:44.5pt;transform:translate(-19.13pt,0pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.6.1.1" class="ltx_p"><span id="S1.T1.36.37.1.6.1.1.1" class="ltx_text" style="font-size:90%;">Multi-View</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<div id="S1.T1.36.37.1.7.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:47.4pt;vertical-align:-20.6pt;"><span class="ltx_transformed_inner" style="width:47.4pt;transform:translate(-20.59pt,0pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.7.1.1" class="ltx_p"><span id="S1.T1.36.37.1.7.1.1.1" class="ltx_text" style="font-size:90%;">Robotic GT</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.8" class="ltx_td ltx_align_center ltx_border_tt">
<div id="S1.T1.36.37.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:38.1pt;vertical-align:-15.9pt;"><span class="ltx_transformed_inner" style="width:38.1pt;transform:translate(-15.9pt,0pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.8.1.1" class="ltx_p"><span id="S1.T1.36.37.1.8.1.1.1" class="ltx_text" style="font-size:90%;">Occlusion</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.9" class="ltx_td ltx_align_center ltx_border_tt">
<div id="S1.T1.36.37.1.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.9pt;height:40.6pt;vertical-align:-18.1pt;"><span class="ltx_transformed_inner" style="width:40.5pt;transform:translate(-16.31pt,2.63pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.9.1.1" class="ltx_p"><span id="S1.T1.36.37.1.9.1.1.1" class="ltx_text" style="font-size:90%;">Symmetry</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.10" class="ltx_td ltx_align_center ltx_border_tt">
<div id="S1.T1.36.37.1.10.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.9pt;height:47.6pt;vertical-align:-21.6pt;"><span class="ltx_transformed_inner" style="width:47.6pt;transform:translate(-19.85pt,2.63pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.10.1.1" class="ltx_p"><span id="S1.T1.36.37.1.10.1.1.1" class="ltx_text" style="font-size:90%;">Transparent</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<div id="S1.T1.36.37.1.11.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:38.4pt;vertical-align:-16.1pt;"><span class="ltx_transformed_inner" style="width:38.4pt;transform:translate(-16.06pt,0pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.11.1.1" class="ltx_p"><span id="S1.T1.36.37.1.11.1.1.1" class="ltx_text" style="font-size:90%;">Reflective</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.12" class="ltx_td ltx_align_right ltx_border_tt">
<div id="S1.T1.36.37.1.12.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.9pt;height:41pt;vertical-align:-18.3pt;"><span class="ltx_transformed_inner" style="width:41.1pt;transform:translate(-16.59pt,2.63pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.12.1.1" class="ltx_p"><span id="S1.T1.36.37.1.12.1.1.1" class="ltx_text" style="font-size:90%;">Categories</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.13" class="ltx_td ltx_align_right ltx_border_tt">
<div id="S1.T1.36.37.1.13.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.0pt;height:30.3pt;vertical-align:-12.9pt;"><span class="ltx_transformed_inner" style="width:30.3pt;transform:translate(-11.15pt,2.63pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.13.1.1" class="ltx_p"><span id="S1.T1.36.37.1.13.1.1.1" class="ltx_text" style="font-size:90%;">Objects</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.14" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">
<div id="S1.T1.36.37.1.14.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.9pt;height:39.4pt;vertical-align:-17.5pt;"><span class="ltx_transformed_inner" style="width:39.3pt;transform:translate(-15.7pt,2.63pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.14.1.1" class="ltx_p"><span id="S1.T1.36.37.1.14.1.1.1" class="ltx_text" style="font-size:90%;">Sequences</span></p>
</span></div>
</td>
<td id="S1.T1.36.37.1.15" class="ltx_td ltx_align_left ltx_border_tt">
<div id="S1.T1.36.37.1.15.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:28.7pt;vertical-align:-11.3pt;"><span class="ltx_transformed_inner" style="width:28.7pt;transform:translate(-11.26pt,0pt) rotate(-90deg) ;">
<p id="S1.T1.36.37.1.15.1.1" class="ltx_p"><span id="S1.T1.36.37.1.15.1.1.1" class="ltx_text" style="font-size:90%;">License</span></p>
</span></div>
</td>
</tr>
<tr id="S1.T1.2.2" class="ltx_tr">
<td id="S1.T1.2.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">FAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S1.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.2.2.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S1.T1.2.2.7" class="ltx_td ltx_border_t"></td>
<td id="S1.T1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.2.2.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S1.T1.2.2.10" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.2.2.11" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.2.2.12" class="ltx_td ltx_border_t"></td>
<td id="S1.T1.2.2.13" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S1.T1.2.2.14" class="ltx_td ltx_align_right ltx_border_t">–</td>
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_right ltx_border_t"><math id="S1.T1.1.1.1.m1.1" class="ltx_Math" alttext="21" display="inline"><semantics id="S1.T1.1.1.1.m1.1a"><mn id="S1.T1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.m1.1.1.cmml">21</mn><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.m1.1b"><cn type="integer" id="S1.T1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.m1.1.1">21</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.m1.1c">21</annotation></semantics></math></td>
<td id="S1.T1.2.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">
<math id="S1.T1.2.2.2.m1.1" class="ltx_Math" alttext="&gt;1" display="inline"><semantics id="S1.T1.2.2.2.m1.1a"><mrow id="S1.T1.2.2.2.m1.1.1" xref="S1.T1.2.2.2.m1.1.1.cmml"><mi id="S1.T1.2.2.2.m1.1.1.2" xref="S1.T1.2.2.2.m1.1.1.2.cmml"></mi><mo id="S1.T1.2.2.2.m1.1.1.1" xref="S1.T1.2.2.2.m1.1.1.1.cmml">&gt;</mo><mn id="S1.T1.2.2.2.m1.1.1.3" xref="S1.T1.2.2.2.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.2.m1.1b"><apply id="S1.T1.2.2.2.m1.1.1.cmml" xref="S1.T1.2.2.2.m1.1.1"><gt id="S1.T1.2.2.2.m1.1.1.1.cmml" xref="S1.T1.2.2.2.m1.1.1.1"></gt><csymbol cd="latexml" id="S1.T1.2.2.2.m1.1.1.2.cmml" xref="S1.T1.2.2.2.m1.1.1.2">absent</csymbol><cn type="integer" id="S1.T1.2.2.2.m1.1.1.3.cmml" xref="S1.T1.2.2.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.2.m1.1c">&gt;1</annotation></semantics></math>k</td>
<td id="S1.T1.2.2.15" class="ltx_td ltx_align_left ltx_border_t">CC BY-NC-SA 4.0</td>
</tr>
<tr id="S1.T1.3.3" class="ltx_tr">
<td id="S1.T1.3.3.2" class="ltx_td ltx_align_right ltx_border_r">BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S1.T1.3.3.3" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.3.3.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.3.3.5" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.3.3.6" class="ltx_td"></td>
<td id="S1.T1.3.3.7" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.3.3.8" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.3.3.9" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.3.3.10" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.3.3.11" class="ltx_td"></td>
<td id="S1.T1.3.3.12" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.3.3.13" class="ltx_td ltx_align_right">–</td>
<td id="S1.T1.3.3.14" class="ltx_td ltx_align_right">–</td>
<td id="S1.T1.3.3.1" class="ltx_td ltx_align_right ltx_border_r">
<math id="S1.T1.3.3.1.m1.1" class="ltx_Math" alttext="&gt;1" display="inline"><semantics id="S1.T1.3.3.1.m1.1a"><mrow id="S1.T1.3.3.1.m1.1.1" xref="S1.T1.3.3.1.m1.1.1.cmml"><mi id="S1.T1.3.3.1.m1.1.1.2" xref="S1.T1.3.3.1.m1.1.1.2.cmml"></mi><mo id="S1.T1.3.3.1.m1.1.1.1" xref="S1.T1.3.3.1.m1.1.1.1.cmml">&gt;</mo><mn id="S1.T1.3.3.1.m1.1.1.3" xref="S1.T1.3.3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.3.3.1.m1.1b"><apply id="S1.T1.3.3.1.m1.1.1.cmml" xref="S1.T1.3.3.1.m1.1.1"><gt id="S1.T1.3.3.1.m1.1.1.1.cmml" xref="S1.T1.3.3.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S1.T1.3.3.1.m1.1.1.2.cmml" xref="S1.T1.3.3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S1.T1.3.3.1.m1.1.1.3.cmml" xref="S1.T1.3.3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.3.1.m1.1c">&gt;1</annotation></semantics></math>k</td>
<td id="S1.T1.3.3.15" class="ltx_td ltx_align_left">GNU GPL 3.0</td>
</tr>
<tr id="S1.T1.5.5" class="ltx_tr">
<td id="S1.T1.5.5.3" class="ltx_td ltx_align_right ltx_border_r">LabelFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S1.T1.5.5.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.5.5.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.5.5.6" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.5.5.7" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.5.5.8" class="ltx_td"></td>
<td id="S1.T1.5.5.9" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.5.5.10" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.5.5.11" class="ltx_td"></td>
<td id="S1.T1.5.5.12" class="ltx_td"></td>
<td id="S1.T1.5.5.13" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.5.5.14" class="ltx_td ltx_align_right">–</td>
<td id="S1.T1.4.4.1" class="ltx_td ltx_align_right"><math id="S1.T1.4.4.1.m1.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S1.T1.4.4.1.m1.1a"><mn id="S1.T1.4.4.1.m1.1.1" xref="S1.T1.4.4.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S1.T1.4.4.1.m1.1b"><cn type="integer" id="S1.T1.4.4.1.m1.1.1.cmml" xref="S1.T1.4.4.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.4.4.1.m1.1c">12</annotation></semantics></math></td>
<td id="S1.T1.5.5.2" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.5.5.2.m1.1" class="ltx_Math" alttext="138" display="inline"><semantics id="S1.T1.5.5.2.m1.1a"><mn id="S1.T1.5.5.2.m1.1.1" xref="S1.T1.5.5.2.m1.1.1.cmml">138</mn><annotation-xml encoding="MathML-Content" id="S1.T1.5.5.2.m1.1b"><cn type="integer" id="S1.T1.5.5.2.m1.1.1.cmml" xref="S1.T1.5.5.2.m1.1.1">138</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.5.5.2.m1.1c">138</annotation></semantics></math></td>
<td id="S1.T1.5.5.15" class="ltx_td ltx_align_left">BSD 3-Clause</td>
</tr>
<tr id="S1.T1.7.7" class="ltx_tr">
<td id="S1.T1.7.7.3" class="ltx_td ltx_align_right ltx_border_r">Toyota Light <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S1.T1.7.7.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.7.7.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.7.7.6" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.7.7.7" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.7.7.8" class="ltx_td"></td>
<td id="S1.T1.7.7.9" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.7.7.10" class="ltx_td"></td>
<td id="S1.T1.7.7.11" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.7.7.12" class="ltx_td"></td>
<td id="S1.T1.7.7.13" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.7.7.14" class="ltx_td ltx_align_right">–</td>
<td id="S1.T1.6.6.1" class="ltx_td ltx_align_right"><math id="S1.T1.6.6.1.m1.1" class="ltx_Math" alttext="21" display="inline"><semantics id="S1.T1.6.6.1.m1.1a"><mn id="S1.T1.6.6.1.m1.1.1" xref="S1.T1.6.6.1.m1.1.1.cmml">21</mn><annotation-xml encoding="MathML-Content" id="S1.T1.6.6.1.m1.1b"><cn type="integer" id="S1.T1.6.6.1.m1.1.1.cmml" xref="S1.T1.6.6.1.m1.1.1">21</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.6.6.1.m1.1c">21</annotation></semantics></math></td>
<td id="S1.T1.7.7.2" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.7.7.2.m1.1" class="ltx_Math" alttext="21" display="inline"><semantics id="S1.T1.7.7.2.m1.1a"><mn id="S1.T1.7.7.2.m1.1.1" xref="S1.T1.7.7.2.m1.1.1.cmml">21</mn><annotation-xml encoding="MathML-Content" id="S1.T1.7.7.2.m1.1b"><cn type="integer" id="S1.T1.7.7.2.m1.1.1.cmml" xref="S1.T1.7.7.2.m1.1.1">21</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.7.7.2.m1.1c">21</annotation></semantics></math></td>
<td id="S1.T1.7.7.15" class="ltx_td ltx_align_left">MIT</td>
</tr>
<tr id="S1.T1.9.9" class="ltx_tr">
<td id="S1.T1.9.9.3" class="ltx_td ltx_align_right ltx_border_r">YCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S1.T1.9.9.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.9.9.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.9.9.6" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.9.9.7" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.9.9.8" class="ltx_td"></td>
<td id="S1.T1.9.9.9" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.9.9.10" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.9.9.11" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.9.9.12" class="ltx_td"></td>
<td id="S1.T1.9.9.13" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.9.9.14" class="ltx_td ltx_align_right">–</td>
<td id="S1.T1.8.8.1" class="ltx_td ltx_align_right"><math id="S1.T1.8.8.1.m1.1" class="ltx_Math" alttext="21" display="inline"><semantics id="S1.T1.8.8.1.m1.1a"><mn id="S1.T1.8.8.1.m1.1.1" xref="S1.T1.8.8.1.m1.1.1.cmml">21</mn><annotation-xml encoding="MathML-Content" id="S1.T1.8.8.1.m1.1b"><cn type="integer" id="S1.T1.8.8.1.m1.1.1.cmml" xref="S1.T1.8.8.1.m1.1.1">21</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.8.8.1.m1.1c">21</annotation></semantics></math></td>
<td id="S1.T1.9.9.2" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.9.9.2.m1.1" class="ltx_Math" alttext="92" display="inline"><semantics id="S1.T1.9.9.2.m1.1a"><mn id="S1.T1.9.9.2.m1.1.1" xref="S1.T1.9.9.2.m1.1.1.cmml">92</mn><annotation-xml encoding="MathML-Content" id="S1.T1.9.9.2.m1.1b"><cn type="integer" id="S1.T1.9.9.2.m1.1.1.cmml" xref="S1.T1.9.9.2.m1.1.1">92</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.9.9.2.m1.1c">92</annotation></semantics></math></td>
<td id="S1.T1.9.9.15" class="ltx_td ltx_align_left">MIT</td>
</tr>
<tr id="S1.T1.11.11" class="ltx_tr">
<td id="S1.T1.11.11.3" class="ltx_td ltx_align_right ltx_border_r">Linemod <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S1.T1.11.11.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.11.11.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.11.11.6" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.11.11.7" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.11.11.8" class="ltx_td"></td>
<td id="S1.T1.11.11.9" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.11.11.10" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.11.11.11" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.11.11.12" class="ltx_td"></td>
<td id="S1.T1.11.11.13" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.11.11.14" class="ltx_td ltx_align_right">–</td>
<td id="S1.T1.10.10.1" class="ltx_td ltx_align_right"><math id="S1.T1.10.10.1.m1.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S1.T1.10.10.1.m1.1a"><mn id="S1.T1.10.10.1.m1.1.1" xref="S1.T1.10.10.1.m1.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S1.T1.10.10.1.m1.1b"><cn type="integer" id="S1.T1.10.10.1.m1.1.1.cmml" xref="S1.T1.10.10.1.m1.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.10.10.1.m1.1c">15</annotation></semantics></math></td>
<td id="S1.T1.11.11.2" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.11.11.2.m1.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S1.T1.11.11.2.m1.1a"><mn id="S1.T1.11.11.2.m1.1.1" xref="S1.T1.11.11.2.m1.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S1.T1.11.11.2.m1.1b"><cn type="integer" id="S1.T1.11.11.2.m1.1.1.cmml" xref="S1.T1.11.11.2.m1.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.11.11.2.m1.1c">15</annotation></semantics></math></td>
<td id="S1.T1.11.11.15" class="ltx_td ltx_align_left">CC BY 4.0</td>
</tr>
<tr id="S1.T1.13.13" class="ltx_tr">
<td id="S1.T1.13.13.3" class="ltx_td ltx_align_right ltx_border_r">GraspNet-1Billion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S1.T1.13.13.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.13.13.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.13.13.6" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.13.13.7" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.13.13.8" class="ltx_td"></td>
<td id="S1.T1.13.13.9" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.13.13.10" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.13.13.11" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.13.13.12" class="ltx_td"></td>
<td id="S1.T1.13.13.13" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.13.13.14" class="ltx_td ltx_align_right">–</td>
<td id="S1.T1.12.12.1" class="ltx_td ltx_align_right"><math id="S1.T1.12.12.1.m1.1" class="ltx_Math" alttext="88" display="inline"><semantics id="S1.T1.12.12.1.m1.1a"><mn id="S1.T1.12.12.1.m1.1.1" xref="S1.T1.12.12.1.m1.1.1.cmml">88</mn><annotation-xml encoding="MathML-Content" id="S1.T1.12.12.1.m1.1b"><cn type="integer" id="S1.T1.12.12.1.m1.1.1.cmml" xref="S1.T1.12.12.1.m1.1.1">88</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.12.12.1.m1.1c">88</annotation></semantics></math></td>
<td id="S1.T1.13.13.2" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.13.13.2.m1.1" class="ltx_Math" alttext="190" display="inline"><semantics id="S1.T1.13.13.2.m1.1a"><mn id="S1.T1.13.13.2.m1.1.1" xref="S1.T1.13.13.2.m1.1.1.cmml">190</mn><annotation-xml encoding="MathML-Content" id="S1.T1.13.13.2.m1.1b"><cn type="integer" id="S1.T1.13.13.2.m1.1.1.cmml" xref="S1.T1.13.13.2.m1.1.1">190</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.13.13.2.m1.1c">190</annotation></semantics></math></td>
<td id="S1.T1.13.13.15" class="ltx_td ltx_align_left">CC BY-NC-SA 4.0</td>
</tr>
<tr id="S1.T1.15.15" class="ltx_tr">
<td id="S1.T1.15.15.3" class="ltx_td ltx_align_right ltx_border_r">T-LESS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</td>
<td id="S1.T1.15.15.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.15.15.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.15.15.6" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.15.15.7" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.15.15.8" class="ltx_td"></td>
<td id="S1.T1.15.15.9" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.15.15.10" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.15.15.11" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.15.15.12" class="ltx_td"></td>
<td id="S1.T1.15.15.13" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.15.15.14" class="ltx_td ltx_align_right">–</td>
<td id="S1.T1.14.14.1" class="ltx_td ltx_align_right"><math id="S1.T1.14.14.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S1.T1.14.14.1.m1.1a"><mn id="S1.T1.14.14.1.m1.1.1" xref="S1.T1.14.14.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S1.T1.14.14.1.m1.1b"><cn type="integer" id="S1.T1.14.14.1.m1.1.1.cmml" xref="S1.T1.14.14.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.14.14.1.m1.1c">30</annotation></semantics></math></td>
<td id="S1.T1.15.15.2" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.15.15.2.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S1.T1.15.15.2.m1.1a"><mn id="S1.T1.15.15.2.m1.1.1" xref="S1.T1.15.15.2.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S1.T1.15.15.2.m1.1b"><cn type="integer" id="S1.T1.15.15.2.m1.1.1.cmml" xref="S1.T1.15.15.2.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.15.15.2.m1.1c">20</annotation></semantics></math></td>
<td id="S1.T1.15.15.15" class="ltx_td ltx_align_left">CC BY 4.0</td>
</tr>
<tr id="S1.T1.17.17" class="ltx_tr">
<td id="S1.T1.17.17.3" class="ltx_td ltx_align_right ltx_border_r">HomebrewedDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</td>
<td id="S1.T1.17.17.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.17.17.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.17.17.6" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.17.17.7" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.17.17.8" class="ltx_td"></td>
<td id="S1.T1.17.17.9" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.17.17.10" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.17.17.11" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.17.17.12" class="ltx_td"></td>
<td id="S1.T1.17.17.13" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.17.17.14" class="ltx_td ltx_align_right">–</td>
<td id="S1.T1.16.16.1" class="ltx_td ltx_align_right"><math id="S1.T1.16.16.1.m1.1" class="ltx_Math" alttext="33" display="inline"><semantics id="S1.T1.16.16.1.m1.1a"><mn id="S1.T1.16.16.1.m1.1.1" xref="S1.T1.16.16.1.m1.1.1.cmml">33</mn><annotation-xml encoding="MathML-Content" id="S1.T1.16.16.1.m1.1b"><cn type="integer" id="S1.T1.16.16.1.m1.1.1.cmml" xref="S1.T1.16.16.1.m1.1.1">33</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.16.16.1.m1.1c">33</annotation></semantics></math></td>
<td id="S1.T1.17.17.2" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.17.17.2.m1.1" class="ltx_Math" alttext="13" display="inline"><semantics id="S1.T1.17.17.2.m1.1a"><mn id="S1.T1.17.17.2.m1.1.1" xref="S1.T1.17.17.2.m1.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S1.T1.17.17.2.m1.1b"><cn type="integer" id="S1.T1.17.17.2.m1.1.1.cmml" xref="S1.T1.17.17.2.m1.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.17.17.2.m1.1c">13</annotation></semantics></math></td>
<td id="S1.T1.17.17.15" class="ltx_td ltx_align_left">CC0 1.0 Universal</td>
</tr>
<tr id="S1.T1.19.19" class="ltx_tr">
<td id="S1.T1.19.19.3" class="ltx_td ltx_align_right ltx_border_r">ITODD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S1.T1.19.19.4" class="ltx_td"></td>
<td id="S1.T1.19.19.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.19.19.6" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.19.19.7" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.19.19.8" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.19.19.9" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.19.19.10" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.19.19.11" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.19.19.12" class="ltx_td"></td>
<td id="S1.T1.19.19.13" class="ltx_td ltx_align_center ltx_border_r">(✓)</td>
<td id="S1.T1.19.19.14" class="ltx_td ltx_align_right">–</td>
<td id="S1.T1.18.18.1" class="ltx_td ltx_align_right"><math id="S1.T1.18.18.1.m1.1" class="ltx_Math" alttext="28" display="inline"><semantics id="S1.T1.18.18.1.m1.1a"><mn id="S1.T1.18.18.1.m1.1.1" xref="S1.T1.18.18.1.m1.1.1.cmml">28</mn><annotation-xml encoding="MathML-Content" id="S1.T1.18.18.1.m1.1b"><cn type="integer" id="S1.T1.18.18.1.m1.1.1.cmml" xref="S1.T1.18.18.1.m1.1.1">28</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.18.18.1.m1.1c">28</annotation></semantics></math></td>
<td id="S1.T1.19.19.2" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.19.19.2.m1.1" class="ltx_Math" alttext="800" display="inline"><semantics id="S1.T1.19.19.2.m1.1a"><mn id="S1.T1.19.19.2.m1.1.1" xref="S1.T1.19.19.2.m1.1.1.cmml">800</mn><annotation-xml encoding="MathML-Content" id="S1.T1.19.19.2.m1.1b"><cn type="integer" id="S1.T1.19.19.2.m1.1.1.cmml" xref="S1.T1.19.19.2.m1.1.1">800</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.19.19.2.m1.1c">800</annotation></semantics></math></td>
<td id="S1.T1.19.19.15" class="ltx_td ltx_align_left">CC BY-NC-SA 4.0</td>
</tr>
<tr id="S1.T1.21.21" class="ltx_tr">
<td id="S1.T1.21.21.3" class="ltx_td ltx_align_right ltx_border_r">StereoOBJ-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S1.T1.21.21.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.21.21.5" class="ltx_td"></td>
<td id="S1.T1.21.21.6" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.21.21.7" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.21.21.8" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.21.21.9" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.21.21.10" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.21.21.11" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.21.21.12" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.21.21.13" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.21.21.14" class="ltx_td ltx_align_right">–</td>
<td id="S1.T1.20.20.1" class="ltx_td ltx_align_right"><math id="S1.T1.20.20.1.m1.1" class="ltx_Math" alttext="18" display="inline"><semantics id="S1.T1.20.20.1.m1.1a"><mn id="S1.T1.20.20.1.m1.1.1" xref="S1.T1.20.20.1.m1.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S1.T1.20.20.1.m1.1b"><cn type="integer" id="S1.T1.20.20.1.m1.1.1.cmml" xref="S1.T1.20.20.1.m1.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.20.20.1.m1.1c">18</annotation></semantics></math></td>
<td id="S1.T1.21.21.2" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.21.21.2.m1.1" class="ltx_Math" alttext="183" display="inline"><semantics id="S1.T1.21.21.2.m1.1a"><mn id="S1.T1.21.21.2.m1.1.1" xref="S1.T1.21.21.2.m1.1.1.cmml">183</mn><annotation-xml encoding="MathML-Content" id="S1.T1.21.21.2.m1.1b"><cn type="integer" id="S1.T1.21.21.2.m1.1.1.cmml" xref="S1.T1.21.21.2.m1.1.1">183</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.21.21.2.m1.1c">183</annotation></semantics></math></td>
<td id="S1.T1.21.21.15" class="ltx_td ltx_align_left">Not (yet) released</td>
</tr>
<tr id="S1.T1.24.24" class="ltx_tr">
<td id="S1.T1.24.24.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">kPAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S1.T1.24.24.5" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.24.24.6" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.24.24.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S1.T1.24.24.8" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.24.24.9" class="ltx_td ltx_border_t"></td>
<td id="S1.T1.24.24.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S1.T1.24.24.11" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.24.24.12" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.24.24.13" class="ltx_td ltx_border_t"></td>
<td id="S1.T1.24.24.14" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S1.T1.22.22.1" class="ltx_td ltx_align_right ltx_border_t"><math id="S1.T1.22.22.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S1.T1.22.22.1.m1.1a"><mn id="S1.T1.22.22.1.m1.1.1" xref="S1.T1.22.22.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S1.T1.22.22.1.m1.1b"><cn type="integer" id="S1.T1.22.22.1.m1.1.1.cmml" xref="S1.T1.22.22.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.22.22.1.m1.1c">2</annotation></semantics></math></td>
<td id="S1.T1.23.23.2" class="ltx_td ltx_align_right ltx_border_t"><math id="S1.T1.23.23.2.m1.1" class="ltx_Math" alttext="91" display="inline"><semantics id="S1.T1.23.23.2.m1.1a"><mn id="S1.T1.23.23.2.m1.1.1" xref="S1.T1.23.23.2.m1.1.1.cmml">91</mn><annotation-xml encoding="MathML-Content" id="S1.T1.23.23.2.m1.1b"><cn type="integer" id="S1.T1.23.23.2.m1.1.1.cmml" xref="S1.T1.23.23.2.m1.1.1">91</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.23.23.2.m1.1c">91</annotation></semantics></math></td>
<td id="S1.T1.24.24.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S1.T1.24.24.3.m1.1" class="ltx_Math" alttext="362" display="inline"><semantics id="S1.T1.24.24.3.m1.1a"><mn id="S1.T1.24.24.3.m1.1.1" xref="S1.T1.24.24.3.m1.1.1.cmml">362</mn><annotation-xml encoding="MathML-Content" id="S1.T1.24.24.3.m1.1b"><cn type="integer" id="S1.T1.24.24.3.m1.1.1.cmml" xref="S1.T1.24.24.3.m1.1.1">362</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.24.24.3.m1.1c">362</annotation></semantics></math></td>
<td id="S1.T1.24.24.15" class="ltx_td ltx_align_left ltx_border_t">MIT</td>
</tr>
<tr id="S1.T1.27.27" class="ltx_tr">
<td id="S1.T1.27.27.4" class="ltx_td ltx_align_right ltx_border_r">CAMERA25 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</td>
<td id="S1.T1.27.27.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.27.27.6" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.27.27.7" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.27.27.8" class="ltx_td ltx_align_center">(✓)</td>
<td id="S1.T1.27.27.9" class="ltx_td"></td>
<td id="S1.T1.27.27.10" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.27.27.11" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.27.27.12" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.27.27.13" class="ltx_td"></td>
<td id="S1.T1.27.27.14" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.25.25.1" class="ltx_td ltx_align_right"><math id="S1.T1.25.25.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S1.T1.25.25.1.m1.1a"><mn id="S1.T1.25.25.1.m1.1.1" xref="S1.T1.25.25.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S1.T1.25.25.1.m1.1b"><cn type="integer" id="S1.T1.25.25.1.m1.1.1.cmml" xref="S1.T1.25.25.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.25.25.1.m1.1c">6</annotation></semantics></math></td>
<td id="S1.T1.26.26.2" class="ltx_td ltx_align_right"><math id="S1.T1.26.26.2.m1.1" class="ltx_Math" alttext="42" display="inline"><semantics id="S1.T1.26.26.2.m1.1a"><mn id="S1.T1.26.26.2.m1.1.1" xref="S1.T1.26.26.2.m1.1.1.cmml">42</mn><annotation-xml encoding="MathML-Content" id="S1.T1.26.26.2.m1.1b"><cn type="integer" id="S1.T1.26.26.2.m1.1.1.cmml" xref="S1.T1.26.26.2.m1.1.1">42</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.26.26.2.m1.1c">42</annotation></semantics></math></td>
<td id="S1.T1.27.27.3" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.27.27.3.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S1.T1.27.27.3.m1.1a"><mn id="S1.T1.27.27.3.m1.1.1" xref="S1.T1.27.27.3.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S1.T1.27.27.3.m1.1b"><cn type="integer" id="S1.T1.27.27.3.m1.1.1.cmml" xref="S1.T1.27.27.3.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.27.27.3.m1.1c">30</annotation></semantics></math></td>
<td id="S1.T1.27.27.15" class="ltx_td ltx_align_left">MIT</td>
</tr>
<tr id="S1.T1.30.30" class="ltx_tr">
<td id="S1.T1.30.30.4" class="ltx_td ltx_align_right ltx_border_r">REAL275 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</td>
<td id="S1.T1.30.30.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.30.30.6" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.30.30.7" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.30.30.8" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.30.30.9" class="ltx_td"></td>
<td id="S1.T1.30.30.10" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.30.30.11" class="ltx_td"></td>
<td id="S1.T1.30.30.12" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.30.30.13" class="ltx_td"></td>
<td id="S1.T1.30.30.14" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.28.28.1" class="ltx_td ltx_align_right"><math id="S1.T1.28.28.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S1.T1.28.28.1.m1.1a"><mn id="S1.T1.28.28.1.m1.1.1" xref="S1.T1.28.28.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S1.T1.28.28.1.m1.1b"><cn type="integer" id="S1.T1.28.28.1.m1.1.1.cmml" xref="S1.T1.28.28.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.28.28.1.m1.1c">6</annotation></semantics></math></td>
<td id="S1.T1.29.29.2" class="ltx_td ltx_align_right"><math id="S1.T1.29.29.2.m1.1" class="ltx_Math" alttext="42" display="inline"><semantics id="S1.T1.29.29.2.m1.1a"><mn id="S1.T1.29.29.2.m1.1.1" xref="S1.T1.29.29.2.m1.1.1.cmml">42</mn><annotation-xml encoding="MathML-Content" id="S1.T1.29.29.2.m1.1b"><cn type="integer" id="S1.T1.29.29.2.m1.1.1.cmml" xref="S1.T1.29.29.2.m1.1.1">42</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.29.29.2.m1.1c">42</annotation></semantics></math></td>
<td id="S1.T1.30.30.3" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.30.30.3.m1.1" class="ltx_Math" alttext="13" display="inline"><semantics id="S1.T1.30.30.3.m1.1a"><mn id="S1.T1.30.30.3.m1.1.1" xref="S1.T1.30.30.3.m1.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S1.T1.30.30.3.m1.1b"><cn type="integer" id="S1.T1.30.30.3.m1.1.1.cmml" xref="S1.T1.30.30.3.m1.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.30.30.3.m1.1c">13</annotation></semantics></math></td>
<td id="S1.T1.30.30.15" class="ltx_td ltx_align_left">MIT</td>
</tr>
<tr id="S1.T1.33.33" class="ltx_tr">
<td id="S1.T1.33.33.4" class="ltx_td ltx_align_right ltx_border_r">TOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S1.T1.33.33.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.33.33.6" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.33.33.7" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.33.33.8" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.33.33.9" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.33.33.10" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.33.33.11" class="ltx_td"></td>
<td id="S1.T1.33.33.12" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.33.33.13" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.33.33.14" class="ltx_td ltx_border_r"></td>
<td id="S1.T1.31.31.1" class="ltx_td ltx_align_right"><math id="S1.T1.31.31.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S1.T1.31.31.1.m1.1a"><mn id="S1.T1.31.31.1.m1.1.1" xref="S1.T1.31.31.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S1.T1.31.31.1.m1.1b"><cn type="integer" id="S1.T1.31.31.1.m1.1.1.cmml" xref="S1.T1.31.31.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.31.31.1.m1.1c">3</annotation></semantics></math></td>
<td id="S1.T1.32.32.2" class="ltx_td ltx_align_right"><math id="S1.T1.32.32.2.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S1.T1.32.32.2.m1.1a"><mn id="S1.T1.32.32.2.m1.1.1" xref="S1.T1.32.32.2.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S1.T1.32.32.2.m1.1b"><cn type="integer" id="S1.T1.32.32.2.m1.1.1.cmml" xref="S1.T1.32.32.2.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.32.32.2.m1.1c">20</annotation></semantics></math></td>
<td id="S1.T1.33.33.3" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.33.33.3.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S1.T1.33.33.3.m1.1a"><mn id="S1.T1.33.33.3.m1.1.1" xref="S1.T1.33.33.3.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S1.T1.33.33.3.m1.1b"><cn type="integer" id="S1.T1.33.33.3.m1.1.1.cmml" xref="S1.T1.33.33.3.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.33.33.3.m1.1c">10</annotation></semantics></math></td>
<td id="S1.T1.33.33.15" class="ltx_td ltx_align_left">CC BY 4.0</td>
</tr>
<tr id="S1.T1.36.36" class="ltx_tr">
<td id="S1.T1.36.36.4" class="ltx_td ltx_align_right ltx_border_r">Ours (PhoCaL)</td>
<td id="S1.T1.36.36.5" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.36.36.6" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.36.36.7" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.36.36.8" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.36.36.9" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.36.36.10" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.36.36.11" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.36.36.12" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.36.36.13" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.36.36.14" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.34.34.1" class="ltx_td ltx_align_right"><math id="S1.T1.34.34.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S1.T1.34.34.1.m1.1a"><mn id="S1.T1.34.34.1.m1.1.1" xref="S1.T1.34.34.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S1.T1.34.34.1.m1.1b"><cn type="integer" id="S1.T1.34.34.1.m1.1.1.cmml" xref="S1.T1.34.34.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.34.34.1.m1.1c">8</annotation></semantics></math></td>
<td id="S1.T1.35.35.2" class="ltx_td ltx_align_right"><math id="S1.T1.35.35.2.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S1.T1.35.35.2.m1.1a"><mn id="S1.T1.35.35.2.m1.1.1" xref="S1.T1.35.35.2.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S1.T1.35.35.2.m1.1b"><cn type="integer" id="S1.T1.35.35.2.m1.1.1.cmml" xref="S1.T1.35.35.2.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.35.35.2.m1.1c">60</annotation></semantics></math></td>
<td id="S1.T1.36.36.3" class="ltx_td ltx_align_right ltx_border_r"><math id="S1.T1.36.36.3.m1.1" class="ltx_Math" alttext="24" display="inline"><semantics id="S1.T1.36.36.3.m1.1a"><mn id="S1.T1.36.36.3.m1.1.1" xref="S1.T1.36.36.3.m1.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S1.T1.36.36.3.m1.1b"><cn type="integer" id="S1.T1.36.36.3.m1.1.1.cmml" xref="S1.T1.36.36.3.m1.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.36.36.3.m1.1c">24</annotation></semantics></math></td>
<td id="S1.T1.36.36.15" class="ltx_td ltx_align_left">CC BY 4.0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.38.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.39.2" class="ltx_text" style="font-size:90%;">Overview of pose estimation datasets. The upper part shows instance-level datasets while the lower part includes category-level setups. PhoCaL is the only dataset that includes both photometrically challenging objects with high quality (robotic) pose annotations and all three modalities, RGB, depth, and polarisation.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work &amp; Current Challenges</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Standardized datasets are used in the field of object pose and shape estimation to quantify and compare contributions and advances in the field. These datasets generally fall in two domains: instance-level datasets, where the 3D model of the object is known a priori; and category-level datasets, where the exact CAD model is unknown. Tab. <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides an overview of related datasets in both domains.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Instance-level 6D Object Pose Dataset</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">One of the earliest, most widely used publicly available datasets for instance level pose estimation is LineMOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and its occlusion extension LM-Occlusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
Their data was acquired using a PrimeSense RGB-D Carmine sensor and a marker board was used to keep track of the relative sensor pose. While undoubtedly pioneering this field, the 3D model quality is now outdated and the leader boards on these datasets have become saturated. HomebrewedDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> accounts for the latter shortcoming by providing high quality 3D models scanned with a structured light sensor. Including three models from LineMOD, they add 30 more toy, household and industrial objects. Different illumination conditions and occlusions make the scenes more challenging. Other datasets also include household objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> or focus on industrial parts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> with low texture for which it is also possible to manually design or retrieve accurate CAD models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The BOP 6D pose benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> includes a summary of these datasets with standardized metrics in a common format.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">While the datasets mentioned so far provide individual frames, the YCB-Video dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> also includes video sequences of 21 household objects. While YCB uses LabelFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for semi-manual frame annotation and pose propagation through the sequence, Garon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> leverage tiny markers on the object to estimate the poses in their videos directly at the cost of synthetic data cleaning afterwards. The advent of photo realistic rendering further enables a branch of works that leverages training on purely synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Although this circumvents the cumbersome pose labelling process, it introduces a domain gap between synthetic data for evaluations and real-world appearances faced in the final applications.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Category-level Object Poses and Datasets</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In real-world applications, a 3D model is not always available, but pose information is still required. Detection of such objects under these conditions has classically been tackled using 3D geometric primitives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">While these methods consider outdoor scenes for which kitti <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> provides 3D bounding box annotations, they lack object shape comparison and the information is often too inaccurate for robotic grasping tasks. The pioneering work of NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> was the first category-level method that could detect object pose and shape in indoor environments. Further investigations consider correspondence-free methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> where a deep generative model learns a canonical shape space from RGBD and a method to estimate pose and shape for fully unseen objects is also proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, albeit this method requires a reference image for latent code generation. CPS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> demonstrates how to estimate pose and metric shapes at category level, using only a monocular view. The extension CPS++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> further utilizes synthetic data and a domain transfer approach using self-supervised refinement with a differentiable renderer from RGBD data without annotations. SGPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> explores shape priors to estimate the object pose. DualPoseNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> leverages spherical fusion for better encoding of the object information.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">We leverage the standard RGBD method NOCS and the strong state-of-the-art RGB method CPS to set the baselines on our new dataset. While task-specific datasets for general object detection exist for robot grasping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, methods for category-level pose estimation are typically tested on NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> data. The NOCS objects comprise various categories, but do not contain photometric challenges often present in everyday objects such as reflectance and transparency.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Photometric Challenges and Multimodalities</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">While texture-less objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> were initially challenging for pose estimation, transparency presents an even bigger hurdle. While the problem is not new, previous methods have addressed this using RGB stereo without a 3D model to identify grasping points only <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Rotational object symmetry can be leveraged by contour fitting for transparent object reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> using template matching.
ClearGrasp <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> proposes a method for geometry estimation of transparent objects based on RGBD. However, this method passes over the transparent regions from the depth map and predicts depth from RGB in these areas instead.
Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> investigate instance- and category-level pose estimation from stereo imagery. Since their depth sensing fails on transparent objects, they use an opaque object twin as proxy to establish ground truth depth. More recently StereOBJ-1M proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> a large dataset including transparent and translucent objects with specular reflections and symmetry. However, at the time of this writing it is not yet available for download.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">For 2D object detection, information from multiple orthogonal sensor modalities such as polarisation (RGBP) can help for transparent object segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. This modality can provide information in regions were depth sensors fail. Their inherent connection with surface normals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> can also make them attractive for pose estimation of photometrically challenging objects.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2205.08811/assets/x1.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="190" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Limitations of RGBD sensors. The depth for photometrically challenging objects is difficult to measure with a commodity depth sensor. The intel RealSense D515 LiDAR ToF sensor used here is affected by reflections that lead to invalid (1) incorrect (2) distance estimates. Moreover, the glassware becomes invisible to the sensor (3) and causes noise (4).
</span></figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Ground Truth Pose Annotation</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Manual annotation of 6D pose is difficult and extremely time-consuming. Therefore, most datasets rely on semi-manual processes for ground truth annotation. The data from a depth sensor, if available, is often used to register the 3D model and manual adjustments are applied to visually refine the pose for this one frame. Relative camera motion is typically calculated using visual markers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to propagate the pose information through a sequence of images. The use of depth sensors for ICP-based alignment of pose labels reduces labour and improves fully-manual annotation quality. However, depth maps from RGBD sensors are erroneous or invalid for photometrically challenging objects with high reflectance and translucent or transparent surfaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. An examples is shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.3 Photometric Challenges and Multimodalities ‣ 2 Related Work &amp; Current Challenges ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Ensuring high quality of pose labels over a series of images is difficult and errors accumulate as the examples in Fig. <a href="#S2.F4" title="Figure 4 ‣ 2.4 Ground Truth Pose Annotation ‣ 2 Related Work &amp; Current Challenges ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> show. This equally affects depth-based refinement strategies of 6D pose pipelines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
We propose a mechanical measurement process using a robotic manipulator to circumvent this issue and allow for high precision labels that omits the error propagation of relative camera pose retrieval from images.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2205.08811/assets/x2.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="139" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.3.2" class="ltx_text" style="font-size:90%;">Annotation quality for poses in datasets Linemod <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> (projected green silhouette, left) and YCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> (rendered overlay, right) together with its correction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> (right).</span></figcaption>
</figure>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2205.08811/assets/x3.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S2.F5.3.2" class="ltx_text" style="font-size:90%;">Overview of dataset acquisition pipeline. (a): 3D models are extracted with a structured light scanner. (b): Pivot calibration calibrates a tipping tool to robot coordinates. (c): 6D poses are annotated using the tool and manual movements of the robot. (d): The camera trajectory is saved. (e): Dataset is recorded automatically following the planned trajectory.</span></figcaption>
</figure>
<figure id="S2.F6" class="ltx_figure"><img src="/html/2205.08811/assets/x4.png" id="S2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="160" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S2.F6.3.2" class="ltx_text" style="font-size:90%;">Overview of hand-eye-calibration and its evaluation. (a): shows the marker-to-robot calibration. (b): illustrates camera-to-robot hand-eye calibration. (c) depicts our accuracy evaluation.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset Acquisition Pipeline</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our dataset features multiple object classes including photometrically challenging classes such as objects with reflective surfaces or transparent material. It also provides multi-modal sensor data with highly accurate 6D pose annotation. This section describes our dataset acquisition pipeline as shown in Fig. <a href="#S2.F5" title="Figure 5 ‣ 2.4 Ground Truth Pose Annotation ‣ 2 Related Work &amp; Current Challenges ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Objects Model Acquisition</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">To represent a cross section of common household objects, we selected eight common categories for our category-level 6D object pose dataset: bottle, box, can, cup, remote, teapot, cutlery, glassware. All object models are scanned using an EinScan-SP 3D Scanner (SHINING 3D Tech. Co., Ltd., Hangzhou, China). The scanner is a structured light stereo system with a single shot accuracy of <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\leq 0.05~{}\text{mm}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml"></mi><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">≤</mo><mrow id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mn id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">0.05</mn><mo lspace="0.330em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.3.1" xref="S3.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mtext id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3a.cmml">mm</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><leq id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></leq><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">absent</csymbol><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><times id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.1"></times><cn type="float" id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">0.05</cn><ci id="S3.SS1.p1.1.m1.1.1.3.3a.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3"><mtext id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3">mm</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\leq 0.05~{}\text{mm}</annotation></semantics></math> in a scanning volume of <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="1200\times 1200\times 1200~{}\text{mm}^{3}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mrow id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml"><mn id="S3.SS1.p1.2.m2.1.1.2.2" xref="S3.SS1.p1.2.m2.1.1.2.2.cmml">1200</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.2.1" xref="S3.SS1.p1.2.m2.1.1.2.1.cmml">×</mo><mn id="S3.SS1.p1.2.m2.1.1.2.3" xref="S3.SS1.p1.2.m2.1.1.2.3.cmml">1200</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.2.1a" xref="S3.SS1.p1.2.m2.1.1.2.1.cmml">×</mo><mn id="S3.SS1.p1.2.m2.1.1.2.4" xref="S3.SS1.p1.2.m2.1.1.2.4.cmml">1200</mn></mrow><mo lspace="0.330em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">​</mo><msup id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml"><mtext id="S3.SS1.p1.2.m2.1.1.3.2" xref="S3.SS1.p1.2.m2.1.1.3.2a.cmml">mm</mtext><mn id="S3.SS1.p1.2.m2.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><apply id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2"><times id="S3.SS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1.2.1"></times><cn type="integer" id="S3.SS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2.2">1200</cn><cn type="integer" id="S3.SS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3">1200</cn><cn type="integer" id="S3.SS1.p1.2.m2.1.1.2.4.cmml" xref="S3.SS1.p1.2.m2.1.1.2.4">1200</cn></apply><apply id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.3.2a.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2"><mtext id="S3.SS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2">mm</mtext></ci><cn type="integer" id="S3.SS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">1200\times 1200\times 1200~{}\text{mm}^{3}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The models from the first six categories are provided as textured obj files. Since the cutlery and glassware objects are photometrically challenging with their highly reflective and transparent surfaces, we apply a self-vanishing 3D scanning spray (AESUB Blue, Aesub, Recklinghausen, Germany) to make the objects temporarily opaque for scanning. We scan the object and provide an obj file without texture. The spray sublimes after approx. <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn type="integer" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">4</annotation></semantics></math> h.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Scene Acquistion Setup</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.5" class="ltx_p">For each scene, 5-8 objects are placed on the table with the random background. We use a KUKA LBR iiwa 7 R800 (KUKA Roboter GmbH, Augsburg, Germany) 7 DoF robotic arm that guarantees a positional reproducibility of <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\pm 0.1" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mo id="S3.SS2.p1.1.m1.1.1a" xref="S3.SS2.p1.1.m1.1.1.cmml">±</mo><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\pm 0.1</annotation></semantics></math> mm.
The vision system comprises a Phoenix 5.0 MP Polarization camera (IMX264MZR/MYR) with Sony IMX264MYR CMOS (Color) Polarsens (i.e. PHX050S1-QC) (LUCID Vision Labs, Inc., Richmond B.C., Canada) with a Universe Compact lens with C-Mount 5MP 2/3” 6mm f/2.0 (Universe, New York, USA).
As depth camera, the Time-of-Flight (ToF) sensor Intel<sup id="S3.SS2.p1.5.1" class="ltx_sup">®</sup>RealSense™LiDAR L515 is used, which acquires depth images at a resolution of 1024x768 pixels in an operating range between 25 cm and 9 m with a field-of-view of 70°x 55°and an accuracy of <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="5\pm 2.5" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">5</mn><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">±</mo><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">2.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1">plus-or-minus</csymbol><cn type="integer" id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">5</cn><cn type="float" id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">2.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">5\pm 2.5</annotation></semantics></math> mm at <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn type="integer" id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">1</annotation></semantics></math> m distance up to <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="14\pm 15.5" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mn id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">14</mn><mo id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml">±</mo><mn id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">15.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1">plus-or-minus</csymbol><cn type="integer" id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">14</cn><cn type="float" id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">15.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">14\pm 15.5</annotation></semantics></math> mm at <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mn id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><cn type="integer" id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">9</annotation></semantics></math> m distance.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Tip Calibration</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.4" class="ltx_p">We use a rigid, pointy metallic tip to obtain the coordinate position of selected points on the object. Tip calibration is therefore essential to ensure the accuracy of the system. The rig attached to the robot’s end-effector consists of custom 3D printed mount which holds the tool-tip rigidly. The pivot calibration is performed as shown in Fig. <a href="#S3.F8" title="Figure 8 ‣ 3.4 6D Pose Annotation ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (left), where the tip point is placed in a fixed position, while only the robot end-effector position is changed. We collect data from N such tip positions with corresponding end-effector poses, <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="{}_{i}{T_{e}^{b}}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mmultiscripts id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2.2.2" xref="S3.SS3.p1.1.m1.1.1.2.2.2.cmml">T</mi><mi id="S3.SS3.p1.1.m1.1.1.2.2.3" xref="S3.SS3.p1.1.m1.1.1.2.2.3.cmml">e</mi><mi id="S3.SS3.p1.1.m1.1.1.2.3" xref="S3.SS3.p1.1.m1.1.1.2.3.cmml">b</mi><mprescripts id="S3.SS3.p1.1.m1.1.1a" xref="S3.SS3.p1.1.m1.1.1.cmml"></mprescripts><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">i</mi><mrow id="S3.SS3.p1.1.m1.1.1b" xref="S3.SS3.p1.1.m1.1.1.cmml"></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><apply id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1">superscript</csymbol><apply id="S3.SS3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.2.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2.2.2">𝑇</ci><ci id="S3.SS3.p1.1.m1.1.1.2.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.2.2.3">𝑒</ci></apply><ci id="S3.SS3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.2.3">𝑏</ci></apply><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">{}_{i}{T_{e}^{b}}</annotation></semantics></math>, which contain rotation <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="{}_{i}{R_{e}^{b}}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mmultiscripts id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2.2.2" xref="S3.SS3.p1.2.m2.1.1.2.2.2.cmml">R</mi><mi id="S3.SS3.p1.2.m2.1.1.2.2.3" xref="S3.SS3.p1.2.m2.1.1.2.2.3.cmml">e</mi><mi id="S3.SS3.p1.2.m2.1.1.2.3" xref="S3.SS3.p1.2.m2.1.1.2.3.cmml">b</mi><mprescripts id="S3.SS3.p1.2.m2.1.1a" xref="S3.SS3.p1.2.m2.1.1.cmml"></mprescripts><mi id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">i</mi><mrow id="S3.SS3.p1.2.m2.1.1b" xref="S3.SS3.p1.2.m2.1.1.cmml"></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><apply id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1">superscript</csymbol><apply id="S3.SS3.p1.2.m2.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.2.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2.2">𝑅</ci><ci id="S3.SS3.p1.2.m2.1.1.2.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2.3">𝑒</ci></apply><ci id="S3.SS3.p1.2.m2.1.1.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3">𝑏</ci></apply><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">{}_{i}{R_{e}^{b}}</annotation></semantics></math> and translation <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="{}_{i}{t_{e}^{b}}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mmultiscripts id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2.2.2" xref="S3.SS3.p1.3.m3.1.1.2.2.2.cmml">t</mi><mi id="S3.SS3.p1.3.m3.1.1.2.2.3" xref="S3.SS3.p1.3.m3.1.1.2.2.3.cmml">e</mi><mi id="S3.SS3.p1.3.m3.1.1.2.3" xref="S3.SS3.p1.3.m3.1.1.2.3.cmml">b</mi><mprescripts id="S3.SS3.p1.3.m3.1.1a" xref="S3.SS3.p1.3.m3.1.1.cmml"></mprescripts><mi id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">i</mi><mrow id="S3.SS3.p1.3.m3.1.1b" xref="S3.SS3.p1.3.m3.1.1.cmml"></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><apply id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.2.1.cmml" xref="S3.SS3.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS3.p1.3.m3.1.1.2.2.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.2.2.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.2.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2.2.2">𝑡</ci><ci id="S3.SS3.p1.3.m3.1.1.2.2.3.cmml" xref="S3.SS3.p1.3.m3.1.1.2.2.3">𝑒</ci></apply><ci id="S3.SS3.p1.3.m3.1.1.2.3.cmml" xref="S3.SS3.p1.3.m3.1.1.2.3">𝑏</ci></apply><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">{}_{i}{t_{e}^{b}}</annotation></semantics></math>, the final translation <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="{t_{t}^{e}}" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><msubsup id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2.2" xref="S3.SS3.p1.4.m4.1.1.2.2.cmml">t</mi><mi id="S3.SS3.p1.4.m4.1.1.2.3" xref="S3.SS3.p1.4.m4.1.1.2.3.cmml">t</mi><mi id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml">e</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">superscript</csymbol><apply id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.2.1.cmml" xref="S3.SS3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.2.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2.2">𝑡</ci><ci id="S3.SS3.p1.4.m4.1.1.2.3.cmml" xref="S3.SS3.p1.4.m4.1.1.2.3">𝑡</ci></apply><ci id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">{t_{t}^{e}}</annotation></semantics></math> of the end-effector is calculated as follows:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="t_{t}^{e}=\begin{bmatrix}{}_{1}{R_{e}^{b}}-{}_{2}{R_{e}^{b}}\\
{}_{2}{R_{e}^{b}}-{}_{3}{R_{e}^{b}}\\
\vdots\\
{}_{n}{R_{e}^{b}}-{}_{1}{R_{e}^{b}}\\
\end{bmatrix}^{\dagger}\cdot\begin{bmatrix}{}_{1}{t_{e}^{b}}-{}_{2}{t_{e}^{b}}\\
{}_{2}{t_{e}^{b}}-{}_{3}{t_{e}^{b}}\\
\vdots\\
{}_{n}{t_{e}^{b}}-{}_{1}{t_{e}^{b}}\\
\end{bmatrix}" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.3" xref="S3.E1.m1.2.3.cmml"><msubsup id="S3.E1.m1.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><mi id="S3.E1.m1.2.3.2.2.2" xref="S3.E1.m1.2.3.2.2.2.cmml">t</mi><mi id="S3.E1.m1.2.3.2.2.3" xref="S3.E1.m1.2.3.2.2.3.cmml">t</mi><mi id="S3.E1.m1.2.3.2.3" xref="S3.E1.m1.2.3.2.3.cmml">e</mi></msubsup><mo id="S3.E1.m1.2.3.1" xref="S3.E1.m1.2.3.1.cmml">=</mo><mrow id="S3.E1.m1.2.3.3" xref="S3.E1.m1.2.3.3.cmml"><msup id="S3.E1.m1.2.3.3.2" xref="S3.E1.m1.2.3.3.2.cmml"><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.2.1.cmml">[</mo><mtable displaystyle="true" rowspacing="0pt" id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mtr id="S3.E1.m1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1b" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mmultiscripts id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.cmml">R</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml">e</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.1.1.1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"></mprescripts><mn id="S3.E1.m1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">1</mn><mrow id="S3.E1.m1.1.1.1.1.1.1.1.2b" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"></mrow></mmultiscripts><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mmultiscripts id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.2.cmml">R</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.3.cmml">e</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.1.1.1.1.1.1.1.3a" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"></mprescripts><mn id="S3.E1.m1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml">2</mn><mrow id="S3.E1.m1.1.1.1.1.1.1.1.3b" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"></mrow></mmultiscripts></mrow></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1c" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1d" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.2.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.cmml"><mmultiscripts id="S3.E1.m1.1.1.1.1.2.1.1.2" xref="S3.E1.m1.1.1.1.1.2.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.1.1.2.2.2.2.cmml">R</mi><mi id="S3.E1.m1.1.1.1.1.2.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.1.1.2.2.2.3.cmml">e</mi><mi id="S3.E1.m1.1.1.1.1.2.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.2.1.1.2.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.1.1.1.1.2.1.1.2a" xref="S3.E1.m1.1.1.1.1.2.1.1.2.cmml"></mprescripts><mn id="S3.E1.m1.1.1.1.1.2.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.1.1.2.3.cmml">2</mn><mrow id="S3.E1.m1.1.1.1.1.2.1.1.2b" xref="S3.E1.m1.1.1.1.1.2.1.1.2.cmml"></mrow></mmultiscripts><mo id="S3.E1.m1.1.1.1.1.2.1.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.cmml">−</mo><mmultiscripts id="S3.E1.m1.1.1.1.1.2.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.1.3.2.2.2" xref="S3.E1.m1.1.1.1.1.2.1.1.3.2.2.2.cmml">R</mi><mi id="S3.E1.m1.1.1.1.1.2.1.1.3.2.2.3" xref="S3.E1.m1.1.1.1.1.2.1.1.3.2.2.3.cmml">e</mi><mi id="S3.E1.m1.1.1.1.1.2.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.2.1.1.3.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.1.1.1.1.2.1.1.3a" xref="S3.E1.m1.1.1.1.1.2.1.1.3.cmml"></mprescripts><mn id="S3.E1.m1.1.1.1.1.2.1.1.3.3" xref="S3.E1.m1.1.1.1.1.2.1.1.3.3.cmml">3</mn><mrow id="S3.E1.m1.1.1.1.1.2.1.1.3b" xref="S3.E1.m1.1.1.1.1.2.1.1.3.cmml"></mrow></mmultiscripts></mrow></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1e" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1f" xref="S3.E1.m1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E1.m1.1.1.1.1.3.1.1" xref="S3.E1.m1.1.1.1.1.3.1.1.cmml">⋮</mi></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1g" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1h" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.4.1.1" xref="S3.E1.m1.1.1.1.1.4.1.1.cmml"><mmultiscripts id="S3.E1.m1.1.1.1.1.4.1.1.2" xref="S3.E1.m1.1.1.1.1.4.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.4.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.4.1.1.2.2.2.2.cmml">R</mi><mi id="S3.E1.m1.1.1.1.1.4.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.4.1.1.2.2.2.3.cmml">e</mi><mi id="S3.E1.m1.1.1.1.1.4.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.4.1.1.2.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.1.1.1.1.4.1.1.2a" xref="S3.E1.m1.1.1.1.1.4.1.1.2.cmml"></mprescripts><mi id="S3.E1.m1.1.1.1.1.4.1.1.2.3" xref="S3.E1.m1.1.1.1.1.4.1.1.2.3.cmml">n</mi><mrow id="S3.E1.m1.1.1.1.1.4.1.1.2b" xref="S3.E1.m1.1.1.1.1.4.1.1.2.cmml"></mrow></mmultiscripts><mo id="S3.E1.m1.1.1.1.1.4.1.1.1" xref="S3.E1.m1.1.1.1.1.4.1.1.1.cmml">−</mo><mmultiscripts id="S3.E1.m1.1.1.1.1.4.1.1.3" xref="S3.E1.m1.1.1.1.1.4.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.4.1.1.3.2.2.2" xref="S3.E1.m1.1.1.1.1.4.1.1.3.2.2.2.cmml">R</mi><mi id="S3.E1.m1.1.1.1.1.4.1.1.3.2.2.3" xref="S3.E1.m1.1.1.1.1.4.1.1.3.2.2.3.cmml">e</mi><mi id="S3.E1.m1.1.1.1.1.4.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.4.1.1.3.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.1.1.1.1.4.1.1.3a" xref="S3.E1.m1.1.1.1.1.4.1.1.3.cmml"></mprescripts><mn id="S3.E1.m1.1.1.1.1.4.1.1.3.3" xref="S3.E1.m1.1.1.1.1.4.1.1.3.3.cmml">1</mn><mrow id="S3.E1.m1.1.1.1.1.4.1.1.3b" xref="S3.E1.m1.1.1.1.1.4.1.1.3.cmml"></mrow></mmultiscripts></mrow></mtd></mtr></mtable><mo rspace="0.055em" id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.2.1.cmml">]</mo></mrow><mo id="S3.E1.m1.2.3.3.2.2" xref="S3.E1.m1.2.3.3.2.2.cmml">†</mo></msup><mo rspace="0.222em" id="S3.E1.m1.2.3.3.1" xref="S3.E1.m1.2.3.3.1.cmml">⋅</mo><mrow id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.2.cmml"><mo id="S3.E1.m1.2.2.3.1" xref="S3.E1.m1.2.2.2.1.cmml">[</mo><mtable displaystyle="true" rowspacing="0pt" id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mtr id="S3.E1.m1.2.2.1.1a" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1b" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml"><mmultiscripts id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.2.2.2.2.cmml">t</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.2.2.1.1.1.1.1.2.2.2.3.cmml">e</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.1.1.1.2.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.2.2.1.1.1.1.1.2a" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml"></mprescripts><mn id="S3.E1.m1.2.2.1.1.1.1.1.2.3" xref="S3.E1.m1.2.2.1.1.1.1.1.2.3.cmml">1</mn><mrow id="S3.E1.m1.2.2.1.1.1.1.1.2b" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml"></mrow></mmultiscripts><mo id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">−</mo><mmultiscripts id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.3.2.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.3.2.2.2.cmml">t</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.3.2.2.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.2.2.3.cmml">e</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.3.2.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.2.2.1.1.1.1.1.3a" xref="S3.E1.m1.2.2.1.1.1.1.1.3.cmml"></mprescripts><mn id="S3.E1.m1.2.2.1.1.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.3.cmml">2</mn><mrow id="S3.E1.m1.2.2.1.1.1.1.1.3b" xref="S3.E1.m1.2.2.1.1.1.1.1.3.cmml"></mrow></mmultiscripts></mrow></mtd></mtr><mtr id="S3.E1.m1.2.2.1.1c" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1d" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.2.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.cmml"><mmultiscripts id="S3.E1.m1.2.2.1.1.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.1.1.2.2.2.2.cmml">t</mi><mi id="S3.E1.m1.2.2.1.1.2.1.1.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.1.1.2.2.2.3.cmml">e</mi><mi id="S3.E1.m1.2.2.1.1.2.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.2.1.1.2.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.2.2.1.1.2.1.1.2a" xref="S3.E1.m1.2.2.1.1.2.1.1.2.cmml"></mprescripts><mn id="S3.E1.m1.2.2.1.1.2.1.1.2.3" xref="S3.E1.m1.2.2.1.1.2.1.1.2.3.cmml">2</mn><mrow id="S3.E1.m1.2.2.1.1.2.1.1.2b" xref="S3.E1.m1.2.2.1.1.2.1.1.2.cmml"></mrow></mmultiscripts><mo id="S3.E1.m1.2.2.1.1.2.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.cmml">−</mo><mmultiscripts id="S3.E1.m1.2.2.1.1.2.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.3.2.2.2" xref="S3.E1.m1.2.2.1.1.2.1.1.3.2.2.2.cmml">t</mi><mi id="S3.E1.m1.2.2.1.1.2.1.1.3.2.2.3" xref="S3.E1.m1.2.2.1.1.2.1.1.3.2.2.3.cmml">e</mi><mi id="S3.E1.m1.2.2.1.1.2.1.1.3.2.3" xref="S3.E1.m1.2.2.1.1.2.1.1.3.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.2.2.1.1.2.1.1.3a" xref="S3.E1.m1.2.2.1.1.2.1.1.3.cmml"></mprescripts><mn id="S3.E1.m1.2.2.1.1.2.1.1.3.3" xref="S3.E1.m1.2.2.1.1.2.1.1.3.3.cmml">3</mn><mrow id="S3.E1.m1.2.2.1.1.2.1.1.3b" xref="S3.E1.m1.2.2.1.1.2.1.1.3.cmml"></mrow></mmultiscripts></mrow></mtd></mtr><mtr id="S3.E1.m1.2.2.1.1e" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1f" xref="S3.E1.m1.2.2.1.1.cmml"><mi mathvariant="normal" id="S3.E1.m1.2.2.1.1.3.1.1" xref="S3.E1.m1.2.2.1.1.3.1.1.cmml">⋮</mi></mtd></mtr><mtr id="S3.E1.m1.2.2.1.1g" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1h" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.4.1.1" xref="S3.E1.m1.2.2.1.1.4.1.1.cmml"><mmultiscripts id="S3.E1.m1.2.2.1.1.4.1.1.2" xref="S3.E1.m1.2.2.1.1.4.1.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.4.1.1.2.2.2.2" xref="S3.E1.m1.2.2.1.1.4.1.1.2.2.2.2.cmml">t</mi><mi id="S3.E1.m1.2.2.1.1.4.1.1.2.2.2.3" xref="S3.E1.m1.2.2.1.1.4.1.1.2.2.2.3.cmml">e</mi><mi id="S3.E1.m1.2.2.1.1.4.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.4.1.1.2.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.2.2.1.1.4.1.1.2a" xref="S3.E1.m1.2.2.1.1.4.1.1.2.cmml"></mprescripts><mi id="S3.E1.m1.2.2.1.1.4.1.1.2.3" xref="S3.E1.m1.2.2.1.1.4.1.1.2.3.cmml">n</mi><mrow id="S3.E1.m1.2.2.1.1.4.1.1.2b" xref="S3.E1.m1.2.2.1.1.4.1.1.2.cmml"></mrow></mmultiscripts><mo id="S3.E1.m1.2.2.1.1.4.1.1.1" xref="S3.E1.m1.2.2.1.1.4.1.1.1.cmml">−</mo><mmultiscripts id="S3.E1.m1.2.2.1.1.4.1.1.3" xref="S3.E1.m1.2.2.1.1.4.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.4.1.1.3.2.2.2" xref="S3.E1.m1.2.2.1.1.4.1.1.3.2.2.2.cmml">t</mi><mi id="S3.E1.m1.2.2.1.1.4.1.1.3.2.2.3" xref="S3.E1.m1.2.2.1.1.4.1.1.3.2.2.3.cmml">e</mi><mi id="S3.E1.m1.2.2.1.1.4.1.1.3.2.3" xref="S3.E1.m1.2.2.1.1.4.1.1.3.2.3.cmml">b</mi><mprescripts id="S3.E1.m1.2.2.1.1.4.1.1.3a" xref="S3.E1.m1.2.2.1.1.4.1.1.3.cmml"></mprescripts><mn id="S3.E1.m1.2.2.1.1.4.1.1.3.3" xref="S3.E1.m1.2.2.1.1.4.1.1.3.3.cmml">1</mn><mrow id="S3.E1.m1.2.2.1.1.4.1.1.3b" xref="S3.E1.m1.2.2.1.1.4.1.1.3.cmml"></mrow></mmultiscripts></mrow></mtd></mtr></mtable><mo id="S3.E1.m1.2.2.3.2" xref="S3.E1.m1.2.2.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.3.cmml" xref="S3.E1.m1.2.3"><eq id="S3.E1.m1.2.3.1.cmml" xref="S3.E1.m1.2.3.1"></eq><apply id="S3.E1.m1.2.3.2.cmml" xref="S3.E1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.1.cmml" xref="S3.E1.m1.2.3.2">superscript</csymbol><apply id="S3.E1.m1.2.3.2.2.cmml" xref="S3.E1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.2.1.cmml" xref="S3.E1.m1.2.3.2">subscript</csymbol><ci id="S3.E1.m1.2.3.2.2.2.cmml" xref="S3.E1.m1.2.3.2.2.2">𝑡</ci><ci id="S3.E1.m1.2.3.2.2.3.cmml" xref="S3.E1.m1.2.3.2.2.3">𝑡</ci></apply><ci id="S3.E1.m1.2.3.2.3.cmml" xref="S3.E1.m1.2.3.2.3">𝑒</ci></apply><apply id="S3.E1.m1.2.3.3.cmml" xref="S3.E1.m1.2.3.3"><ci id="S3.E1.m1.2.3.3.1.cmml" xref="S3.E1.m1.2.3.3.1">⋅</ci><apply id="S3.E1.m1.2.3.3.2.cmml" xref="S3.E1.m1.2.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.2.1.cmml" xref="S3.E1.m1.2.3.3.2">superscript</csymbol><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="latexml" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.3.1">matrix</csymbol><matrix id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><matrixrow id="S3.E1.m1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2">𝑅</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.3">𝑏</ci></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3">1</cn></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.2">𝑅</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.3">𝑏</ci></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3">2</cn></apply></apply></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.2.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1"><minus id="S3.E1.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.2.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2.2.2.2">𝑅</ci><ci id="S3.E1.m1.1.1.1.1.2.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.1.1.1.1.2.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2.2.3">𝑏</ci></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.2.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.2.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.1.2.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3.2.2.2">𝑅</ci><ci id="S3.E1.m1.1.1.1.1.2.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.1.1.1.1.2.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3.2.3">𝑏</ci></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.2.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.3.3">3</cn></apply></apply></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1c.cmml" xref="S3.E1.m1.1.1.1.1"><ci id="S3.E1.m1.1.1.1.1.3.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1">⋮</ci></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1d.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.4.1.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1"><minus id="S3.E1.m1.1.1.1.1.4.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.4.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.2">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.4.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.4.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.1.1.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.4.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.2.2.2.2">𝑅</ci><ci id="S3.E1.m1.1.1.1.1.4.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.2.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.1.1.1.1.4.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.2.2.3">𝑏</ci></apply><ci id="S3.E1.m1.1.1.1.1.4.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.2.3">𝑛</ci></apply><apply id="S3.E1.m1.1.1.1.1.4.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.3">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.4.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.3">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.4.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.4.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.3.2.2.2">𝑅</ci><ci id="S3.E1.m1.1.1.1.1.4.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.3.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.1.1.1.1.4.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.3.2.3">𝑏</ci></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.4.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.4.1.1.3.3">1</cn></apply></apply></matrixrow></matrix></apply><ci id="S3.E1.m1.2.3.3.2.2.cmml" xref="S3.E1.m1.2.3.3.2.2">†</ci></apply><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.3"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.3.1">matrix</csymbol><matrix id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><matrixrow id="S3.E1.m1.2.2.1.1a.cmml" xref="S3.E1.m1.2.2.1.1"><apply id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><minus id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2.2.2.2">𝑡</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.2.2.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2.2.3">𝑏</ci></apply><cn type="integer" id="S3.E1.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2.3">1</cn></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.2.2.2">𝑡</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.2.2.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.2.3">𝑏</ci></apply><cn type="integer" id="S3.E1.m1.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.3">2</cn></apply></apply></matrixrow><matrixrow id="S3.E1.m1.2.2.1.1b.cmml" xref="S3.E1.m1.2.2.1.1"><apply id="S3.E1.m1.2.2.1.1.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1"><minus id="S3.E1.m1.2.2.1.1.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1"></minus><apply id="S3.E1.m1.2.2.1.1.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.2.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2.2.2.2">𝑡</ci><ci id="S3.E1.m1.2.2.1.1.2.1.1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.2.2.1.1.2.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2.2.3">𝑏</ci></apply><cn type="integer" id="S3.E1.m1.2.2.1.1.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2.3">2</cn></apply><apply id="S3.E1.m1.2.2.1.1.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.3">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.3">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.2.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.3.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.3.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.3.2.2.2">𝑡</ci><ci id="S3.E1.m1.2.2.1.1.2.1.1.3.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.3.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.2.2.1.1.2.1.1.3.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.3.2.3">𝑏</ci></apply><cn type="integer" id="S3.E1.m1.2.2.1.1.2.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.3.3">3</cn></apply></apply></matrixrow><matrixrow id="S3.E1.m1.2.2.1.1c.cmml" xref="S3.E1.m1.2.2.1.1"><ci id="S3.E1.m1.2.2.1.1.3.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1.1">⋮</ci></matrixrow><matrixrow id="S3.E1.m1.2.2.1.1d.cmml" xref="S3.E1.m1.2.2.1.1"><apply id="S3.E1.m1.2.2.1.1.4.1.1.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1"><minus id="S3.E1.m1.2.2.1.1.4.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.1"></minus><apply id="S3.E1.m1.2.2.1.1.4.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.2">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.4.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.1.1.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.2">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.4.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.1.1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.4.1.1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.2.2.2.2">𝑡</ci><ci id="S3.E1.m1.2.2.1.1.4.1.1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.2.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.2.2.1.1.4.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.2.2.3">𝑏</ci></apply><ci id="S3.E1.m1.2.2.1.1.4.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.2.3">𝑛</ci></apply><apply id="S3.E1.m1.2.2.1.1.4.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.3">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.4.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.3">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.4.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.1.1.3.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.4.1.1.3.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.3.2.2.2">𝑡</ci><ci id="S3.E1.m1.2.2.1.1.4.1.1.3.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.3.2.2.3">𝑒</ci></apply><ci id="S3.E1.m1.2.2.1.1.4.1.1.3.2.3.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.3.2.3">𝑏</ci></apply><cn type="integer" id="S3.E1.m1.2.2.1.1.4.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.4.1.1.3.3">1</cn></apply></apply></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">t_{t}^{e}=\begin{bmatrix}{}_{1}{R_{e}^{b}}-{}_{2}{R_{e}^{b}}\\
{}_{2}{R_{e}^{b}}-{}_{3}{R_{e}^{b}}\\
\vdots\\
{}_{n}{R_{e}^{b}}-{}_{1}{R_{e}^{b}}\\
\end{bmatrix}^{\dagger}\cdot\begin{bmatrix}{}_{1}{t_{e}^{b}}-{}_{2}{t_{e}^{b}}\\
{}_{2}{t_{e}^{b}}-{}_{3}{t_{e}^{b}}\\
\vdots\\
{}_{n}{t_{e}^{b}}-{}_{1}{t_{e}^{b}}\\
\end{bmatrix}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.2" class="ltx_p">where <sup id="S3.SS3.p2.2.1" class="ltx_sup"><span id="S3.SS3.p2.2.1.1" class="ltx_text ltx_font_italic">†</span></sup> denotes the pseudo-inverse.
We evaluate the tip calibration by calculating the variance of each tip location at the pivot point. The variance of the tip location in our setup is <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\varepsilon=0.057~{}\text{mm}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">ε</mi><mo id="S3.SS3.p2.2.m2.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mn id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml">0.057</mn><mo lspace="0.330em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.3.1" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">​</mo><mtext id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3a.cmml">mm</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><eq id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1"></eq><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝜀</ci><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><times id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.1"></times><cn type="float" id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2">0.057</cn><ci id="S3.SS3.p2.2.m2.1.1.3.3a.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3"><mtext id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3">mm</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\varepsilon=0.057~{}\text{mm}</annotation></semantics></math>.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2205.08811/assets/x5.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.3.2" class="ltx_text" style="font-size:90%;">Example of annotation quality before and after ICP based refinement on the textureless object. (a) Initial pose of mesh overlaid with measured surface points (red dots) shows error in initial pose (red arrow). (b) After the ICP, refined pose matches with the surface points properly (blue arrow). (c) Shows improvement in 6D pose annotation. Rendering of the mesh with initial pose (d) and refined pose (e) shows a significant difference in quality.</span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>6D Pose Annotation</h3>

<figure id="S3.F8" class="ltx_figure"><img src="/html/2205.08811/assets/x6.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="265" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.3.2" class="ltx_text" style="font-size:90%;">Tip calibration (left) with its pivot point (red). Tip measuring points of object surface (right) and its correspondence on the object’s model mesh (blue).</span></figcaption>
</figure>
<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Annotating the precise 6D pose of the objects is a challenging task as mentioned in Sec. <a href="#S2.SS4" title="2.4 Ground Truth Pose Annotation ‣ 2 Related Work &amp; Current Challenges ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>. Here, we utilize the robot accuracy and its reproducible encoders to annotate the object pose. Our annotation steps are as follows: first, we attach the tool tip on the robot’s end-effector and measure several keypoints along with 20-30 surface points of the given object by hand guiding the end-effector while the robot is in gravity compensation mode (Fig <a href="#S2.F5" title="Figure 5 ‣ 2.4 Ground Truth Pose Annotation ‣ 2 Related Work &amp; Current Challenges ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (c), Fig <a href="#S3.F8" title="Figure 8 ‣ 3.4 6D Pose Annotation ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (right)). Then, corresponding keypoints are manually picked on the object model’s mesh to obtain the initial pose of the respective objects (Fig <a href="#S3.F8" title="Figure 8 ‣ 3.4 6D Pose Annotation ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (right) (blue)). Finally, ICP is applied to align the dense mesh points of the object and the measured sparse surface points as the refinement step for the initial object poses.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.4" class="ltx_p">To evaluate the refinement performance, 25 points on a specific area of the object surface are picked and uniformly distributed noise of <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mo id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><csymbol cd="latexml" id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\pm</annotation></semantics></math>0.2mm is added to simulate the measurement noise. We then apply a small perturbation of random translation errors of range <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mo id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><csymbol cd="latexml" id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">\pm</annotation></semantics></math>2mm in x,y,z and a rotation error about a random axis with an angle of up to 4 degrees to the object pose to simulate the error introduced by the point correspondences. Thereafter, we apply ICP between the picked surface points and the perturbed mesh to refine the pose. We test this pipeline with 3 selected objects with 5 different random perturbation before applying ICP to recover the initial pose. The pose error is measured in translation and rotational distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> after the refinement and it gives an average RMSE of <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="0.20\,\text{mm}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mrow id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mn id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">0.20</mn><mo lspace="0.170em" rspace="0em" id="S3.SS4.p2.3.m3.1.1.1" xref="S3.SS4.p2.3.m3.1.1.1.cmml">​</mo><mtext id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3a.cmml">mm</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><times id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1.1"></times><cn type="float" id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">0.20</cn><ci id="S3.SS4.p2.3.m3.1.1.3a.cmml" xref="S3.SS4.p2.3.m3.1.1.3"><mtext id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">mm</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">0.20\,\text{mm}</annotation></semantics></math> in translation and <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="0.38^{\circ}" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><msup id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mn id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">0.38</mn><mo id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">superscript</csymbol><cn type="float" id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">0.38</cn><compose id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">0.38^{\circ}</annotation></semantics></math> in rotation.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">It is observed that ICP improves the annotation in real life scenario particularly on textureless objects, where it is difficult to find exact correspondence from the mesh. An extreme example of annotated poses before and after ICP on the textureless objects is shown in Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.3 Tip Calibration ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Hand-Eye Calibration</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.3" class="ltx_p">Traditional hand-eye calibration, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> requires detection of the marker from the camera in various positions to obtain an accurate calibration result. The transformation from camera to end-effector is difficult to estimate as the marker transformation to robot base is unknown and both have to be jointly estimated. In our case, however, the marker position can be accurately measured with the robot tip. Considering this fact, we measure 12 selected points on the marker board and calculate <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="T_{Marker\to RB}" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><msub id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">T</mi><mrow id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml"><mrow id="S3.SS5.p1.1.m1.1.1.3.2" xref="S3.SS5.p1.1.m1.1.1.3.2.cmml"><mi id="S3.SS5.p1.1.m1.1.1.3.2.2" xref="S3.SS5.p1.1.m1.1.1.3.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.1.m1.1.1.3.2.1" xref="S3.SS5.p1.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p1.1.m1.1.1.3.2.3" xref="S3.SS5.p1.1.m1.1.1.3.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.1.m1.1.1.3.2.1a" xref="S3.SS5.p1.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p1.1.m1.1.1.3.2.4" xref="S3.SS5.p1.1.m1.1.1.3.2.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.1.m1.1.1.3.2.1b" xref="S3.SS5.p1.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p1.1.m1.1.1.3.2.5" xref="S3.SS5.p1.1.m1.1.1.3.2.5.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.1.m1.1.1.3.2.1c" xref="S3.SS5.p1.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p1.1.m1.1.1.3.2.6" xref="S3.SS5.p1.1.m1.1.1.3.2.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.1.m1.1.1.3.2.1d" xref="S3.SS5.p1.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p1.1.m1.1.1.3.2.7" xref="S3.SS5.p1.1.m1.1.1.3.2.7.cmml">r</mi></mrow><mo stretchy="false" id="S3.SS5.p1.1.m1.1.1.3.1" xref="S3.SS5.p1.1.m1.1.1.3.1.cmml">→</mo><mrow id="S3.SS5.p1.1.m1.1.1.3.3" xref="S3.SS5.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS5.p1.1.m1.1.1.3.3.2" xref="S3.SS5.p1.1.m1.1.1.3.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.1.m1.1.1.3.3.1" xref="S3.SS5.p1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS5.p1.1.m1.1.1.3.3.3" xref="S3.SS5.p1.1.m1.1.1.3.3.3.cmml">B</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">𝑇</ci><apply id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3"><ci id="S3.SS5.p1.1.m1.1.1.3.1.cmml" xref="S3.SS5.p1.1.m1.1.1.3.1">→</ci><apply id="S3.SS5.p1.1.m1.1.1.3.2.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2"><times id="S3.SS5.p1.1.m1.1.1.3.2.1.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.1"></times><ci id="S3.SS5.p1.1.m1.1.1.3.2.2.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.2">𝑀</ci><ci id="S3.SS5.p1.1.m1.1.1.3.2.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.3">𝑎</ci><ci id="S3.SS5.p1.1.m1.1.1.3.2.4.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.4">𝑟</ci><ci id="S3.SS5.p1.1.m1.1.1.3.2.5.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.5">𝑘</ci><ci id="S3.SS5.p1.1.m1.1.1.3.2.6.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.6">𝑒</ci><ci id="S3.SS5.p1.1.m1.1.1.3.2.7.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.7">𝑟</ci></apply><apply id="S3.SS5.p1.1.m1.1.1.3.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3.3"><times id="S3.SS5.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS5.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS5.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS5.p1.1.m1.1.1.3.3.2">𝑅</ci><ci id="S3.SS5.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3.3.3">𝐵</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">T_{Marker\to RB}</annotation></semantics></math> (Fig. <a href="#S2.F6" title="Figure 6 ‣ 2.4 Ground Truth Pose Annotation ‣ 2 Related Work &amp; Current Challenges ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (a)) to link the end-effector pose to the camera frame. From <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="T_{Marker\to RB}" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><msub id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">T</mi><mrow id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml"><mrow id="S3.SS5.p1.2.m2.1.1.3.2" xref="S3.SS5.p1.2.m2.1.1.3.2.cmml"><mi id="S3.SS5.p1.2.m2.1.1.3.2.2" xref="S3.SS5.p1.2.m2.1.1.3.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.2.m2.1.1.3.2.1" xref="S3.SS5.p1.2.m2.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p1.2.m2.1.1.3.2.3" xref="S3.SS5.p1.2.m2.1.1.3.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.2.m2.1.1.3.2.1a" xref="S3.SS5.p1.2.m2.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p1.2.m2.1.1.3.2.4" xref="S3.SS5.p1.2.m2.1.1.3.2.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.2.m2.1.1.3.2.1b" xref="S3.SS5.p1.2.m2.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p1.2.m2.1.1.3.2.5" xref="S3.SS5.p1.2.m2.1.1.3.2.5.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.2.m2.1.1.3.2.1c" xref="S3.SS5.p1.2.m2.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p1.2.m2.1.1.3.2.6" xref="S3.SS5.p1.2.m2.1.1.3.2.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.2.m2.1.1.3.2.1d" xref="S3.SS5.p1.2.m2.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p1.2.m2.1.1.3.2.7" xref="S3.SS5.p1.2.m2.1.1.3.2.7.cmml">r</mi></mrow><mo stretchy="false" id="S3.SS5.p1.2.m2.1.1.3.1" xref="S3.SS5.p1.2.m2.1.1.3.1.cmml">→</mo><mrow id="S3.SS5.p1.2.m2.1.1.3.3" xref="S3.SS5.p1.2.m2.1.1.3.3.cmml"><mi id="S3.SS5.p1.2.m2.1.1.3.3.2" xref="S3.SS5.p1.2.m2.1.1.3.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.2.m2.1.1.3.3.1" xref="S3.SS5.p1.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S3.SS5.p1.2.m2.1.1.3.3.3" xref="S3.SS5.p1.2.m2.1.1.3.3.3.cmml">B</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2">𝑇</ci><apply id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3"><ci id="S3.SS5.p1.2.m2.1.1.3.1.cmml" xref="S3.SS5.p1.2.m2.1.1.3.1">→</ci><apply id="S3.SS5.p1.2.m2.1.1.3.2.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2"><times id="S3.SS5.p1.2.m2.1.1.3.2.1.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2.1"></times><ci id="S3.SS5.p1.2.m2.1.1.3.2.2.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2.2">𝑀</ci><ci id="S3.SS5.p1.2.m2.1.1.3.2.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2.3">𝑎</ci><ci id="S3.SS5.p1.2.m2.1.1.3.2.4.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2.4">𝑟</ci><ci id="S3.SS5.p1.2.m2.1.1.3.2.5.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2.5">𝑘</ci><ci id="S3.SS5.p1.2.m2.1.1.3.2.6.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2.6">𝑒</ci><ci id="S3.SS5.p1.2.m2.1.1.3.2.7.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2.7">𝑟</ci></apply><apply id="S3.SS5.p1.2.m2.1.1.3.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3.3"><times id="S3.SS5.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS5.p1.2.m2.1.1.3.3.1"></times><ci id="S3.SS5.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS5.p1.2.m2.1.1.3.3.2">𝑅</ci><ci id="S3.SS5.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3.3.3">𝐵</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">T_{Marker\to RB}</annotation></semantics></math>, the <math id="S3.SS5.p1.3.m3.1" class="ltx_Math" alttext="T_{handeye}" display="inline"><semantics id="S3.SS5.p1.3.m3.1a"><msub id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml"><mi id="S3.SS5.p1.3.m3.1.1.2" xref="S3.SS5.p1.3.m3.1.1.2.cmml">T</mi><mrow id="S3.SS5.p1.3.m3.1.1.3" xref="S3.SS5.p1.3.m3.1.1.3.cmml"><mi id="S3.SS5.p1.3.m3.1.1.3.2" xref="S3.SS5.p1.3.m3.1.1.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.3.m3.1.1.3.1" xref="S3.SS5.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p1.3.m3.1.1.3.3" xref="S3.SS5.p1.3.m3.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.3.m3.1.1.3.1a" xref="S3.SS5.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p1.3.m3.1.1.3.4" xref="S3.SS5.p1.3.m3.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.3.m3.1.1.3.1b" xref="S3.SS5.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p1.3.m3.1.1.3.5" xref="S3.SS5.p1.3.m3.1.1.3.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.3.m3.1.1.3.1c" xref="S3.SS5.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p1.3.m3.1.1.3.6" xref="S3.SS5.p1.3.m3.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.3.m3.1.1.3.1d" xref="S3.SS5.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p1.3.m3.1.1.3.7" xref="S3.SS5.p1.3.m3.1.1.3.7.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.3.m3.1.1.3.1e" xref="S3.SS5.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p1.3.m3.1.1.3.8" xref="S3.SS5.p1.3.m3.1.1.3.8.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><apply id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.3.m3.1.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p1.3.m3.1.1.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2">𝑇</ci><apply id="S3.SS5.p1.3.m3.1.1.3.cmml" xref="S3.SS5.p1.3.m3.1.1.3"><times id="S3.SS5.p1.3.m3.1.1.3.1.cmml" xref="S3.SS5.p1.3.m3.1.1.3.1"></times><ci id="S3.SS5.p1.3.m3.1.1.3.2.cmml" xref="S3.SS5.p1.3.m3.1.1.3.2">ℎ</ci><ci id="S3.SS5.p1.3.m3.1.1.3.3.cmml" xref="S3.SS5.p1.3.m3.1.1.3.3">𝑎</ci><ci id="S3.SS5.p1.3.m3.1.1.3.4.cmml" xref="S3.SS5.p1.3.m3.1.1.3.4">𝑛</ci><ci id="S3.SS5.p1.3.m3.1.1.3.5.cmml" xref="S3.SS5.p1.3.m3.1.1.3.5">𝑑</ci><ci id="S3.SS5.p1.3.m3.1.1.3.6.cmml" xref="S3.SS5.p1.3.m3.1.1.3.6">𝑒</ci><ci id="S3.SS5.p1.3.m3.1.1.3.7.cmml" xref="S3.SS5.p1.3.m3.1.1.3.7">𝑦</ci><ci id="S3.SS5.p1.3.m3.1.1.3.8.cmml" xref="S3.SS5.p1.3.m3.1.1.3.8">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">T_{handeye}</annotation></semantics></math> is calculated as shown in Fig. <a href="#S2.F6" title="Figure 6 ‣ 2.4 Ground Truth Pose Annotation ‣ 2 Related Work &amp; Current Challenges ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (b).</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.9" class="ltx_p">The overall accuracy of the entire procedure is measured as shown in Fig. <a href="#S2.F6" title="Figure 6 ‣ 2.4 Ground Truth Pose Annotation ‣ 2 Related Work &amp; Current Challenges ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (c). <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="T_{Marker\to cam}" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><msub id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml"><mi id="S3.SS5.p2.1.m1.1.1.2" xref="S3.SS5.p2.1.m1.1.1.2.cmml">T</mi><mrow id="S3.SS5.p2.1.m1.1.1.3" xref="S3.SS5.p2.1.m1.1.1.3.cmml"><mrow id="S3.SS5.p2.1.m1.1.1.3.2" xref="S3.SS5.p2.1.m1.1.1.3.2.cmml"><mi id="S3.SS5.p2.1.m1.1.1.3.2.2" xref="S3.SS5.p2.1.m1.1.1.3.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.2.1" xref="S3.SS5.p2.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p2.1.m1.1.1.3.2.3" xref="S3.SS5.p2.1.m1.1.1.3.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.2.1a" xref="S3.SS5.p2.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p2.1.m1.1.1.3.2.4" xref="S3.SS5.p2.1.m1.1.1.3.2.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.2.1b" xref="S3.SS5.p2.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p2.1.m1.1.1.3.2.5" xref="S3.SS5.p2.1.m1.1.1.3.2.5.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.2.1c" xref="S3.SS5.p2.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p2.1.m1.1.1.3.2.6" xref="S3.SS5.p2.1.m1.1.1.3.2.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.2.1d" xref="S3.SS5.p2.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p2.1.m1.1.1.3.2.7" xref="S3.SS5.p2.1.m1.1.1.3.2.7.cmml">r</mi></mrow><mo stretchy="false" id="S3.SS5.p2.1.m1.1.1.3.1" xref="S3.SS5.p2.1.m1.1.1.3.1.cmml">→</mo><mrow id="S3.SS5.p2.1.m1.1.1.3.3" xref="S3.SS5.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS5.p2.1.m1.1.1.3.3.2" xref="S3.SS5.p2.1.m1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.3.1" xref="S3.SS5.p2.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS5.p2.1.m1.1.1.3.3.3" xref="S3.SS5.p2.1.m1.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.3.1a" xref="S3.SS5.p2.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS5.p2.1.m1.1.1.3.3.4" xref="S3.SS5.p2.1.m1.1.1.3.3.4.cmml">m</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p2.1.m1.1.1.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2">𝑇</ci><apply id="S3.SS5.p2.1.m1.1.1.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3"><ci id="S3.SS5.p2.1.m1.1.1.3.1.cmml" xref="S3.SS5.p2.1.m1.1.1.3.1">→</ci><apply id="S3.SS5.p2.1.m1.1.1.3.2.cmml" xref="S3.SS5.p2.1.m1.1.1.3.2"><times id="S3.SS5.p2.1.m1.1.1.3.2.1.cmml" xref="S3.SS5.p2.1.m1.1.1.3.2.1"></times><ci id="S3.SS5.p2.1.m1.1.1.3.2.2.cmml" xref="S3.SS5.p2.1.m1.1.1.3.2.2">𝑀</ci><ci id="S3.SS5.p2.1.m1.1.1.3.2.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3.2.3">𝑎</ci><ci id="S3.SS5.p2.1.m1.1.1.3.2.4.cmml" xref="S3.SS5.p2.1.m1.1.1.3.2.4">𝑟</ci><ci id="S3.SS5.p2.1.m1.1.1.3.2.5.cmml" xref="S3.SS5.p2.1.m1.1.1.3.2.5">𝑘</ci><ci id="S3.SS5.p2.1.m1.1.1.3.2.6.cmml" xref="S3.SS5.p2.1.m1.1.1.3.2.6">𝑒</ci><ci id="S3.SS5.p2.1.m1.1.1.3.2.7.cmml" xref="S3.SS5.p2.1.m1.1.1.3.2.7">𝑟</ci></apply><apply id="S3.SS5.p2.1.m1.1.1.3.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3.3"><times id="S3.SS5.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS5.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS5.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS5.p2.1.m1.1.1.3.3.2">𝑐</ci><ci id="S3.SS5.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3.3.3">𝑎</ci><ci id="S3.SS5.p2.1.m1.1.1.3.3.4.cmml" xref="S3.SS5.p2.1.m1.1.1.3.3.4">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">T_{Marker\to cam}</annotation></semantics></math> is formed by applying <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="T_{handeye}" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><msub id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml"><mi id="S3.SS5.p2.2.m2.1.1.2" xref="S3.SS5.p2.2.m2.1.1.2.cmml">T</mi><mrow id="S3.SS5.p2.2.m2.1.1.3" xref="S3.SS5.p2.2.m2.1.1.3.cmml"><mi id="S3.SS5.p2.2.m2.1.1.3.2" xref="S3.SS5.p2.2.m2.1.1.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.3.1" xref="S3.SS5.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.2.m2.1.1.3.3" xref="S3.SS5.p2.2.m2.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.3.1a" xref="S3.SS5.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.2.m2.1.1.3.4" xref="S3.SS5.p2.2.m2.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.3.1b" xref="S3.SS5.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.2.m2.1.1.3.5" xref="S3.SS5.p2.2.m2.1.1.3.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.3.1c" xref="S3.SS5.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.2.m2.1.1.3.6" xref="S3.SS5.p2.2.m2.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.3.1d" xref="S3.SS5.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.2.m2.1.1.3.7" xref="S3.SS5.p2.2.m2.1.1.3.7.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.3.1e" xref="S3.SS5.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.2.m2.1.1.3.8" xref="S3.SS5.p2.2.m2.1.1.3.8.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><apply id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.2.m2.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p2.2.m2.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2">𝑇</ci><apply id="S3.SS5.p2.2.m2.1.1.3.cmml" xref="S3.SS5.p2.2.m2.1.1.3"><times id="S3.SS5.p2.2.m2.1.1.3.1.cmml" xref="S3.SS5.p2.2.m2.1.1.3.1"></times><ci id="S3.SS5.p2.2.m2.1.1.3.2.cmml" xref="S3.SS5.p2.2.m2.1.1.3.2">ℎ</ci><ci id="S3.SS5.p2.2.m2.1.1.3.3.cmml" xref="S3.SS5.p2.2.m2.1.1.3.3">𝑎</ci><ci id="S3.SS5.p2.2.m2.1.1.3.4.cmml" xref="S3.SS5.p2.2.m2.1.1.3.4">𝑛</ci><ci id="S3.SS5.p2.2.m2.1.1.3.5.cmml" xref="S3.SS5.p2.2.m2.1.1.3.5">𝑑</ci><ci id="S3.SS5.p2.2.m2.1.1.3.6.cmml" xref="S3.SS5.p2.2.m2.1.1.3.6">𝑒</ci><ci id="S3.SS5.p2.2.m2.1.1.3.7.cmml" xref="S3.SS5.p2.2.m2.1.1.3.7">𝑦</ci><ci id="S3.SS5.p2.2.m2.1.1.3.8.cmml" xref="S3.SS5.p2.2.m2.1.1.3.8">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">T_{handeye}</annotation></semantics></math> and multiply <math id="S3.SS5.p2.3.m3.1" class="ltx_Math" alttext="T_{marker\to cam}" display="inline"><semantics id="S3.SS5.p2.3.m3.1a"><msub id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml"><mi id="S3.SS5.p2.3.m3.1.1.2" xref="S3.SS5.p2.3.m3.1.1.2.cmml">T</mi><mrow id="S3.SS5.p2.3.m3.1.1.3" xref="S3.SS5.p2.3.m3.1.1.3.cmml"><mrow id="S3.SS5.p2.3.m3.1.1.3.2" xref="S3.SS5.p2.3.m3.1.1.3.2.cmml"><mi id="S3.SS5.p2.3.m3.1.1.3.2.2" xref="S3.SS5.p2.3.m3.1.1.3.2.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.3.m3.1.1.3.2.1" xref="S3.SS5.p2.3.m3.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p2.3.m3.1.1.3.2.3" xref="S3.SS5.p2.3.m3.1.1.3.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.3.m3.1.1.3.2.1a" xref="S3.SS5.p2.3.m3.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p2.3.m3.1.1.3.2.4" xref="S3.SS5.p2.3.m3.1.1.3.2.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.3.m3.1.1.3.2.1b" xref="S3.SS5.p2.3.m3.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p2.3.m3.1.1.3.2.5" xref="S3.SS5.p2.3.m3.1.1.3.2.5.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.3.m3.1.1.3.2.1c" xref="S3.SS5.p2.3.m3.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p2.3.m3.1.1.3.2.6" xref="S3.SS5.p2.3.m3.1.1.3.2.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.3.m3.1.1.3.2.1d" xref="S3.SS5.p2.3.m3.1.1.3.2.1.cmml">​</mo><mi id="S3.SS5.p2.3.m3.1.1.3.2.7" xref="S3.SS5.p2.3.m3.1.1.3.2.7.cmml">r</mi></mrow><mo stretchy="false" id="S3.SS5.p2.3.m3.1.1.3.1" xref="S3.SS5.p2.3.m3.1.1.3.1.cmml">→</mo><mrow id="S3.SS5.p2.3.m3.1.1.3.3" xref="S3.SS5.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS5.p2.3.m3.1.1.3.3.2" xref="S3.SS5.p2.3.m3.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.3.m3.1.1.3.3.1" xref="S3.SS5.p2.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS5.p2.3.m3.1.1.3.3.3" xref="S3.SS5.p2.3.m3.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.3.m3.1.1.3.3.1a" xref="S3.SS5.p2.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS5.p2.3.m3.1.1.3.3.4" xref="S3.SS5.p2.3.m3.1.1.3.3.4.cmml">m</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><apply id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.3.m3.1.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p2.3.m3.1.1.2.cmml" xref="S3.SS5.p2.3.m3.1.1.2">𝑇</ci><apply id="S3.SS5.p2.3.m3.1.1.3.cmml" xref="S3.SS5.p2.3.m3.1.1.3"><ci id="S3.SS5.p2.3.m3.1.1.3.1.cmml" xref="S3.SS5.p2.3.m3.1.1.3.1">→</ci><apply id="S3.SS5.p2.3.m3.1.1.3.2.cmml" xref="S3.SS5.p2.3.m3.1.1.3.2"><times id="S3.SS5.p2.3.m3.1.1.3.2.1.cmml" xref="S3.SS5.p2.3.m3.1.1.3.2.1"></times><ci id="S3.SS5.p2.3.m3.1.1.3.2.2.cmml" xref="S3.SS5.p2.3.m3.1.1.3.2.2">𝑚</ci><ci id="S3.SS5.p2.3.m3.1.1.3.2.3.cmml" xref="S3.SS5.p2.3.m3.1.1.3.2.3">𝑎</ci><ci id="S3.SS5.p2.3.m3.1.1.3.2.4.cmml" xref="S3.SS5.p2.3.m3.1.1.3.2.4">𝑟</ci><ci id="S3.SS5.p2.3.m3.1.1.3.2.5.cmml" xref="S3.SS5.p2.3.m3.1.1.3.2.5">𝑘</ci><ci id="S3.SS5.p2.3.m3.1.1.3.2.6.cmml" xref="S3.SS5.p2.3.m3.1.1.3.2.6">𝑒</ci><ci id="S3.SS5.p2.3.m3.1.1.3.2.7.cmml" xref="S3.SS5.p2.3.m3.1.1.3.2.7">𝑟</ci></apply><apply id="S3.SS5.p2.3.m3.1.1.3.3.cmml" xref="S3.SS5.p2.3.m3.1.1.3.3"><times id="S3.SS5.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS5.p2.3.m3.1.1.3.3.1"></times><ci id="S3.SS5.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS5.p2.3.m3.1.1.3.3.2">𝑐</ci><ci id="S3.SS5.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS5.p2.3.m3.1.1.3.3.3">𝑎</ci><ci id="S3.SS5.p2.3.m3.1.1.3.3.4.cmml" xref="S3.SS5.p2.3.m3.1.1.3.3.4">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">T_{marker\to cam}</annotation></semantics></math> of <math id="S3.SS5.p2.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS5.p2.4.m4.1a"><mi id="S3.SS5.p2.4.m4.1.1" xref="S3.SS5.p2.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.4.m4.1b"><ci id="S3.SS5.p2.4.m4.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.4.m4.1c">n</annotation></semantics></math> different views to transform the 12 points from the marker board to the robot base (<math id="S3.SS5.p2.5.m5.1" class="ltx_Math" alttext="{P_{transformed}}_{n}" display="inline"><semantics id="S3.SS5.p2.5.m5.1a"><mmultiscripts id="S3.SS5.p2.5.m5.1.1" xref="S3.SS5.p2.5.m5.1.1.cmml"><mi id="S3.SS5.p2.5.m5.1.1.2.2" xref="S3.SS5.p2.5.m5.1.1.2.2.cmml">P</mi><mrow id="S3.SS5.p2.5.m5.1.1.2.3" xref="S3.SS5.p2.5.m5.1.1.2.3.cmml"><mi id="S3.SS5.p2.5.m5.1.1.2.3.2" xref="S3.SS5.p2.5.m5.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.5.m5.1.1.2.3.1" xref="S3.SS5.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p2.5.m5.1.1.2.3.3" xref="S3.SS5.p2.5.m5.1.1.2.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.5.m5.1.1.2.3.1a" xref="S3.SS5.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p2.5.m5.1.1.2.3.4" xref="S3.SS5.p2.5.m5.1.1.2.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.5.m5.1.1.2.3.1b" xref="S3.SS5.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p2.5.m5.1.1.2.3.5" xref="S3.SS5.p2.5.m5.1.1.2.3.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.5.m5.1.1.2.3.1c" xref="S3.SS5.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p2.5.m5.1.1.2.3.6" xref="S3.SS5.p2.5.m5.1.1.2.3.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.5.m5.1.1.2.3.1d" xref="S3.SS5.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p2.5.m5.1.1.2.3.7" xref="S3.SS5.p2.5.m5.1.1.2.3.7.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.5.m5.1.1.2.3.1e" xref="S3.SS5.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p2.5.m5.1.1.2.3.8" xref="S3.SS5.p2.5.m5.1.1.2.3.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.5.m5.1.1.2.3.1f" xref="S3.SS5.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p2.5.m5.1.1.2.3.9" xref="S3.SS5.p2.5.m5.1.1.2.3.9.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.5.m5.1.1.2.3.1g" xref="S3.SS5.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p2.5.m5.1.1.2.3.10" xref="S3.SS5.p2.5.m5.1.1.2.3.10.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.5.m5.1.1.2.3.1h" xref="S3.SS5.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p2.5.m5.1.1.2.3.11" xref="S3.SS5.p2.5.m5.1.1.2.3.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.5.m5.1.1.2.3.1i" xref="S3.SS5.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p2.5.m5.1.1.2.3.12" xref="S3.SS5.p2.5.m5.1.1.2.3.12.cmml">d</mi></mrow><mrow id="S3.SS5.p2.5.m5.1.1a" xref="S3.SS5.p2.5.m5.1.1.cmml"></mrow><mi id="S3.SS5.p2.5.m5.1.1.3" xref="S3.SS5.p2.5.m5.1.1.3.cmml">n</mi><mrow id="S3.SS5.p2.5.m5.1.1b" xref="S3.SS5.p2.5.m5.1.1.cmml"></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.5.m5.1b"><apply id="S3.SS5.p2.5.m5.1.1.cmml" xref="S3.SS5.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.5.m5.1.1.1.cmml" xref="S3.SS5.p2.5.m5.1.1">subscript</csymbol><apply id="S3.SS5.p2.5.m5.1.1.2.cmml" xref="S3.SS5.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.5.m5.1.1.2.1.cmml" xref="S3.SS5.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS5.p2.5.m5.1.1.2.2.cmml" xref="S3.SS5.p2.5.m5.1.1.2.2">𝑃</ci><apply id="S3.SS5.p2.5.m5.1.1.2.3.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3"><times id="S3.SS5.p2.5.m5.1.1.2.3.1.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.1"></times><ci id="S3.SS5.p2.5.m5.1.1.2.3.2.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.2">𝑡</ci><ci id="S3.SS5.p2.5.m5.1.1.2.3.3.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.3">𝑟</ci><ci id="S3.SS5.p2.5.m5.1.1.2.3.4.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.4">𝑎</ci><ci id="S3.SS5.p2.5.m5.1.1.2.3.5.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.5">𝑛</ci><ci id="S3.SS5.p2.5.m5.1.1.2.3.6.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.6">𝑠</ci><ci id="S3.SS5.p2.5.m5.1.1.2.3.7.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.7">𝑓</ci><ci id="S3.SS5.p2.5.m5.1.1.2.3.8.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.8">𝑜</ci><ci id="S3.SS5.p2.5.m5.1.1.2.3.9.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.9">𝑟</ci><ci id="S3.SS5.p2.5.m5.1.1.2.3.10.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.10">𝑚</ci><ci id="S3.SS5.p2.5.m5.1.1.2.3.11.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.11">𝑒</ci><ci id="S3.SS5.p2.5.m5.1.1.2.3.12.cmml" xref="S3.SS5.p2.5.m5.1.1.2.3.12">𝑑</ci></apply></apply><ci id="S3.SS5.p2.5.m5.1.1.3.cmml" xref="S3.SS5.p2.5.m5.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.5.m5.1c">{P_{transformed}}_{n}</annotation></semantics></math>). RMSE is calculated by comparing the result to <math id="S3.SS5.p2.6.m6.1" class="ltx_Math" alttext="P_{measured}" display="inline"><semantics id="S3.SS5.p2.6.m6.1a"><msub id="S3.SS5.p2.6.m6.1.1" xref="S3.SS5.p2.6.m6.1.1.cmml"><mi id="S3.SS5.p2.6.m6.1.1.2" xref="S3.SS5.p2.6.m6.1.1.2.cmml">P</mi><mrow id="S3.SS5.p2.6.m6.1.1.3" xref="S3.SS5.p2.6.m6.1.1.3.cmml"><mi id="S3.SS5.p2.6.m6.1.1.3.2" xref="S3.SS5.p2.6.m6.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.6.m6.1.1.3.1" xref="S3.SS5.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.6.m6.1.1.3.3" xref="S3.SS5.p2.6.m6.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.6.m6.1.1.3.1a" xref="S3.SS5.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.6.m6.1.1.3.4" xref="S3.SS5.p2.6.m6.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.6.m6.1.1.3.1b" xref="S3.SS5.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.6.m6.1.1.3.5" xref="S3.SS5.p2.6.m6.1.1.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.6.m6.1.1.3.1c" xref="S3.SS5.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.6.m6.1.1.3.6" xref="S3.SS5.p2.6.m6.1.1.3.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.6.m6.1.1.3.1d" xref="S3.SS5.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.6.m6.1.1.3.7" xref="S3.SS5.p2.6.m6.1.1.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.6.m6.1.1.3.1e" xref="S3.SS5.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.6.m6.1.1.3.8" xref="S3.SS5.p2.6.m6.1.1.3.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.6.m6.1.1.3.1f" xref="S3.SS5.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.6.m6.1.1.3.9" xref="S3.SS5.p2.6.m6.1.1.3.9.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.6.m6.1b"><apply id="S3.SS5.p2.6.m6.1.1.cmml" xref="S3.SS5.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.6.m6.1.1.1.cmml" xref="S3.SS5.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS5.p2.6.m6.1.1.2.cmml" xref="S3.SS5.p2.6.m6.1.1.2">𝑃</ci><apply id="S3.SS5.p2.6.m6.1.1.3.cmml" xref="S3.SS5.p2.6.m6.1.1.3"><times id="S3.SS5.p2.6.m6.1.1.3.1.cmml" xref="S3.SS5.p2.6.m6.1.1.3.1"></times><ci id="S3.SS5.p2.6.m6.1.1.3.2.cmml" xref="S3.SS5.p2.6.m6.1.1.3.2">𝑚</ci><ci id="S3.SS5.p2.6.m6.1.1.3.3.cmml" xref="S3.SS5.p2.6.m6.1.1.3.3">𝑒</ci><ci id="S3.SS5.p2.6.m6.1.1.3.4.cmml" xref="S3.SS5.p2.6.m6.1.1.3.4">𝑎</ci><ci id="S3.SS5.p2.6.m6.1.1.3.5.cmml" xref="S3.SS5.p2.6.m6.1.1.3.5">𝑠</ci><ci id="S3.SS5.p2.6.m6.1.1.3.6.cmml" xref="S3.SS5.p2.6.m6.1.1.3.6">𝑢</ci><ci id="S3.SS5.p2.6.m6.1.1.3.7.cmml" xref="S3.SS5.p2.6.m6.1.1.3.7">𝑟</ci><ci id="S3.SS5.p2.6.m6.1.1.3.8.cmml" xref="S3.SS5.p2.6.m6.1.1.3.8">𝑒</ci><ci id="S3.SS5.p2.6.m6.1.1.3.9.cmml" xref="S3.SS5.p2.6.m6.1.1.3.9">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.6.m6.1c">P_{measured}</annotation></semantics></math>. We evaluate our hand-eye calibrations in one of our scenes on both RGBD and Polarization camera with the mentioned approach with <math id="S3.SS5.p2.7.m7.1" class="ltx_Math" alttext="n=10" display="inline"><semantics id="S3.SS5.p2.7.m7.1a"><mrow id="S3.SS5.p2.7.m7.1.1" xref="S3.SS5.p2.7.m7.1.1.cmml"><mi id="S3.SS5.p2.7.m7.1.1.2" xref="S3.SS5.p2.7.m7.1.1.2.cmml">n</mi><mo id="S3.SS5.p2.7.m7.1.1.1" xref="S3.SS5.p2.7.m7.1.1.1.cmml">=</mo><mn id="S3.SS5.p2.7.m7.1.1.3" xref="S3.SS5.p2.7.m7.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.7.m7.1b"><apply id="S3.SS5.p2.7.m7.1.1.cmml" xref="S3.SS5.p2.7.m7.1.1"><eq id="S3.SS5.p2.7.m7.1.1.1.cmml" xref="S3.SS5.p2.7.m7.1.1.1"></eq><ci id="S3.SS5.p2.7.m7.1.1.2.cmml" xref="S3.SS5.p2.7.m7.1.1.2">𝑛</ci><cn type="integer" id="S3.SS5.p2.7.m7.1.1.3.cmml" xref="S3.SS5.p2.7.m7.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.7.m7.1c">n=10</annotation></semantics></math> and obtained <math id="S3.SS5.p2.8.m8.1" class="ltx_Math" alttext="\text{RMSE}_{\text{RGBD}}=0.89\,\text{mm}" display="inline"><semantics id="S3.SS5.p2.8.m8.1a"><mrow id="S3.SS5.p2.8.m8.1.1" xref="S3.SS5.p2.8.m8.1.1.cmml"><msub id="S3.SS5.p2.8.m8.1.1.2" xref="S3.SS5.p2.8.m8.1.1.2.cmml"><mtext id="S3.SS5.p2.8.m8.1.1.2.2" xref="S3.SS5.p2.8.m8.1.1.2.2a.cmml">RMSE</mtext><mtext id="S3.SS5.p2.8.m8.1.1.2.3" xref="S3.SS5.p2.8.m8.1.1.2.3a.cmml">RGBD</mtext></msub><mo id="S3.SS5.p2.8.m8.1.1.1" xref="S3.SS5.p2.8.m8.1.1.1.cmml">=</mo><mrow id="S3.SS5.p2.8.m8.1.1.3" xref="S3.SS5.p2.8.m8.1.1.3.cmml"><mn id="S3.SS5.p2.8.m8.1.1.3.2" xref="S3.SS5.p2.8.m8.1.1.3.2.cmml">0.89</mn><mo lspace="0.170em" rspace="0em" id="S3.SS5.p2.8.m8.1.1.3.1" xref="S3.SS5.p2.8.m8.1.1.3.1.cmml">​</mo><mtext id="S3.SS5.p2.8.m8.1.1.3.3" xref="S3.SS5.p2.8.m8.1.1.3.3a.cmml">mm</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.8.m8.1b"><apply id="S3.SS5.p2.8.m8.1.1.cmml" xref="S3.SS5.p2.8.m8.1.1"><eq id="S3.SS5.p2.8.m8.1.1.1.cmml" xref="S3.SS5.p2.8.m8.1.1.1"></eq><apply id="S3.SS5.p2.8.m8.1.1.2.cmml" xref="S3.SS5.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.p2.8.m8.1.1.2.1.cmml" xref="S3.SS5.p2.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS5.p2.8.m8.1.1.2.2a.cmml" xref="S3.SS5.p2.8.m8.1.1.2.2"><mtext id="S3.SS5.p2.8.m8.1.1.2.2.cmml" xref="S3.SS5.p2.8.m8.1.1.2.2">RMSE</mtext></ci><ci id="S3.SS5.p2.8.m8.1.1.2.3a.cmml" xref="S3.SS5.p2.8.m8.1.1.2.3"><mtext mathsize="70%" id="S3.SS5.p2.8.m8.1.1.2.3.cmml" xref="S3.SS5.p2.8.m8.1.1.2.3">RGBD</mtext></ci></apply><apply id="S3.SS5.p2.8.m8.1.1.3.cmml" xref="S3.SS5.p2.8.m8.1.1.3"><times id="S3.SS5.p2.8.m8.1.1.3.1.cmml" xref="S3.SS5.p2.8.m8.1.1.3.1"></times><cn type="float" id="S3.SS5.p2.8.m8.1.1.3.2.cmml" xref="S3.SS5.p2.8.m8.1.1.3.2">0.89</cn><ci id="S3.SS5.p2.8.m8.1.1.3.3a.cmml" xref="S3.SS5.p2.8.m8.1.1.3.3"><mtext id="S3.SS5.p2.8.m8.1.1.3.3.cmml" xref="S3.SS5.p2.8.m8.1.1.3.3">mm</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.8.m8.1c">\text{RMSE}_{\text{RGBD}}=0.89\,\text{mm}</annotation></semantics></math> and <math id="S3.SS5.p2.9.m9.1" class="ltx_Math" alttext="\text{RMSE}_{\text{Polarization}}=0.83\,\text{mm}" display="inline"><semantics id="S3.SS5.p2.9.m9.1a"><mrow id="S3.SS5.p2.9.m9.1.1" xref="S3.SS5.p2.9.m9.1.1.cmml"><msub id="S3.SS5.p2.9.m9.1.1.2" xref="S3.SS5.p2.9.m9.1.1.2.cmml"><mtext id="S3.SS5.p2.9.m9.1.1.2.2" xref="S3.SS5.p2.9.m9.1.1.2.2a.cmml">RMSE</mtext><mtext id="S3.SS5.p2.9.m9.1.1.2.3" xref="S3.SS5.p2.9.m9.1.1.2.3a.cmml">Polarization</mtext></msub><mo id="S3.SS5.p2.9.m9.1.1.1" xref="S3.SS5.p2.9.m9.1.1.1.cmml">=</mo><mrow id="S3.SS5.p2.9.m9.1.1.3" xref="S3.SS5.p2.9.m9.1.1.3.cmml"><mn id="S3.SS5.p2.9.m9.1.1.3.2" xref="S3.SS5.p2.9.m9.1.1.3.2.cmml">0.83</mn><mo lspace="0.170em" rspace="0em" id="S3.SS5.p2.9.m9.1.1.3.1" xref="S3.SS5.p2.9.m9.1.1.3.1.cmml">​</mo><mtext id="S3.SS5.p2.9.m9.1.1.3.3" xref="S3.SS5.p2.9.m9.1.1.3.3a.cmml">mm</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.9.m9.1b"><apply id="S3.SS5.p2.9.m9.1.1.cmml" xref="S3.SS5.p2.9.m9.1.1"><eq id="S3.SS5.p2.9.m9.1.1.1.cmml" xref="S3.SS5.p2.9.m9.1.1.1"></eq><apply id="S3.SS5.p2.9.m9.1.1.2.cmml" xref="S3.SS5.p2.9.m9.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.p2.9.m9.1.1.2.1.cmml" xref="S3.SS5.p2.9.m9.1.1.2">subscript</csymbol><ci id="S3.SS5.p2.9.m9.1.1.2.2a.cmml" xref="S3.SS5.p2.9.m9.1.1.2.2"><mtext id="S3.SS5.p2.9.m9.1.1.2.2.cmml" xref="S3.SS5.p2.9.m9.1.1.2.2">RMSE</mtext></ci><ci id="S3.SS5.p2.9.m9.1.1.2.3a.cmml" xref="S3.SS5.p2.9.m9.1.1.2.3"><mtext mathsize="70%" id="S3.SS5.p2.9.m9.1.1.2.3.cmml" xref="S3.SS5.p2.9.m9.1.1.2.3">Polarization</mtext></ci></apply><apply id="S3.SS5.p2.9.m9.1.1.3.cmml" xref="S3.SS5.p2.9.m9.1.1.3"><times id="S3.SS5.p2.9.m9.1.1.3.1.cmml" xref="S3.SS5.p2.9.m9.1.1.3.1"></times><cn type="float" id="S3.SS5.p2.9.m9.1.1.3.2.cmml" xref="S3.SS5.p2.9.m9.1.1.3.2">0.83</cn><ci id="S3.SS5.p2.9.m9.1.1.3.3a.cmml" xref="S3.SS5.p2.9.m9.1.1.3.3"><mtext id="S3.SS5.p2.9.m9.1.1.3.3.cmml" xref="S3.SS5.p2.9.m9.1.1.3.3">mm</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.9.m9.1c">\text{RMSE}_{\text{Polarization}}=0.83\,\text{mm}</annotation></semantics></math>
across all the view points. This calibration is performed procedure for all cameras before recording each scene as shown in Fig. <a href="#S3.F9" title="Figure 9 ‣ 3.5 Hand-Eye Calibration ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2205.08811/assets/x7.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S3.F9.3.2" class="ltx_text" style="font-size:90%;">Measuring the marker points for the calibration on the scene (left) and detected marker from one of the cameras (right)</span></figcaption>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Synchronized Robot Pose Capture with Images</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">RGBD and polarization cameras are used for the data acquisition. A specially designed and 3D printed rig is used to mount both cameras tightly on the end-effector. The trajectory of all joints of the robot is recorded by manually moving the end-effector while the robot arm is in gravity compensated mode. Thereafter, we record the images of the scene by replaying the joint trajectory while stopping the robot every 5-7 joint positions to capture the images and the robot pose (approx 10-15 fps). This ensures no motion blur and camera synchronization artefacts are recorded while reproducing the original hand-held camera trajectory.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Evaluation of Overall Annotation Quality</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">We evaluate overall annotation quality of our dataset by running simulated data acquisition with two measured error statistics : ICP error (Sec <a href="#S3.SS4" title="3.4 6D Pose Annotation ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>) and hand-eye calibration error (Sec <a href="#S3.SS5" title="3.5 Hand-Eye Calibration ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>). For both RGBD and Polarization camera, setup from one of the scenes is used including the objects and the trajectories. The acquisition is simulated twice, with and without the aforementioned error. In the end, RMSE error is calculated pointwise in mm between the acquisitions. We averaged the error per object and per each frame in the trajectories.</p>
</div>
<div id="S3.SS7.p2" class="ltx_para">
<p id="S3.SS7.p2.1" class="ltx_p">RMSE error for RGBD camera is 0.84 mm and for polarization camera is 0.76 mm. Detailed description of this procedure is attached in the supplementary material. The annotation quality in comparison with other dataset acquisition principles is shown in Tab. <a href="#S3.T2" title="Table 2 ‣ 3.7 Evaluation of Overall Annotation Quality ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:30.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-85.6pt,11.9pt) scale(0.558779314131255,0.558779314131255) ;">
<table id="S3.T2.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.4.4.5.1" class="ltx_tr">
<th id="S3.T2.4.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T2.4.4.5.1.1.1" class="ltx_text" style="font-size:80%;">Dataset</span></th>
<th id="S3.T2.4.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.4.4.5.1.2.1" class="ltx_text" style="font-size:80%;">RGBD Dataset</span></th>
<th id="S3.T2.4.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T2.4.4.5.1.3.1" class="ltx_text" style="font-size:80%;">TOD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.4.4.5.1.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S3.T2.4.4.5.1.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S3.T2.4.4.5.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T2.4.4.5.1.4.1" class="ltx_text" style="font-size:80%;">StereOBJ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.4.4.5.1.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S3.T2.4.4.5.1.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S3.T2.4.4.5.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.4.4.5.1.5.1" class="ltx_text" style="font-size:80%;">Ours</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.4.4.6.1" class="ltx_tr">
<th id="S3.T2.4.4.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T2.4.4.6.1.1.1" class="ltx_text" style="font-size:80%;">3D Labeling</span></th>
<td id="S3.T2.4.4.6.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.4.4.6.1.2.1" class="ltx_text" style="font-size:80%;">Depth Map</span></td>
<td id="S3.T2.4.4.6.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S3.T2.4.4.6.1.3.1" class="ltx_text" style="font-size:80%;">Multi-View</span></td>
<td id="S3.T2.4.4.6.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.4.4.6.1.4.1" class="ltx_text" style="font-size:80%;">Robot</span></td>
</tr>
<tr id="S3.T2.4.4.4" class="ltx_tr">
<th id="S3.T2.4.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S3.T2.4.4.4.5.1" class="ltx_text" style="font-size:80%;">Point RMSE</span></th>
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_bb">
<math id="S3.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\geq 17" display="inline"><semantics id="S3.T2.1.1.1.1.m1.1a"><mrow id="S3.T2.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.cmml"><mi id="S3.T2.1.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.1.m1.1.1.2.cmml"></mi><mo mathsize="80%" id="S3.T2.1.1.1.1.m1.1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.1.cmml">≥</mo><mn mathsize="80%" id="S3.T2.1.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.1.m1.1.1.3.cmml">17</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1"><geq id="S3.T2.1.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1.1"></geq><csymbol cd="latexml" id="S3.T2.1.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.T2.1.1.1.1.m1.1.1.3.cmml" xref="S3.T2.1.1.1.1.m1.1.1.3">17</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\geq 17</annotation></semantics></math><span id="S3.T2.1.1.1.1.1" class="ltx_text" style="font-size:80%;">mm</span>
</td>
<td id="S3.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_bb">
<math id="S3.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="3.4" display="inline"><semantics id="S3.T2.2.2.2.2.m1.1a"><mn mathsize="80%" id="S3.T2.2.2.2.2.m1.1.1" xref="S3.T2.2.2.2.2.m1.1.1.cmml">3.4</mn><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.2.m1.1b"><cn type="float" id="S3.T2.2.2.2.2.m1.1.1.cmml" xref="S3.T2.2.2.2.2.m1.1.1">3.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.2.m1.1c">3.4</annotation></semantics></math><span id="S3.T2.2.2.2.2.1" class="ltx_text" style="font-size:80%;">mm</span>
</td>
<td id="S3.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_bb">
<math id="S3.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="2.3" display="inline"><semantics id="S3.T2.3.3.3.3.m1.1a"><mn mathsize="80%" id="S3.T2.3.3.3.3.m1.1.1" xref="S3.T2.3.3.3.3.m1.1.1.cmml">2.3</mn><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.3.m1.1b"><cn type="float" id="S3.T2.3.3.3.3.m1.1.1.cmml" xref="S3.T2.3.3.3.3.m1.1.1">2.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.3.m1.1c">2.3</annotation></semantics></math><span id="S3.T2.3.3.3.3.1" class="ltx_text" style="font-size:80%;">mm</span>
</td>
<td id="S3.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb">
<math id="S3.T2.4.4.4.4.m1.1" class="ltx_Math" alttext="0.80" display="inline"><semantics id="S3.T2.4.4.4.4.m1.1a"><mn mathsize="80%" id="S3.T2.4.4.4.4.m1.1.1" xref="S3.T2.4.4.4.4.m1.1.1.cmml">0.80</mn><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.4.4.m1.1b"><cn type="float" id="S3.T2.4.4.4.4.m1.1.1.cmml" xref="S3.T2.4.4.4.4.m1.1.1">0.80</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.4.4.m1.1c">0.80</annotation></semantics></math><span id="S3.T2.4.4.4.4.1" class="ltx_text" style="font-size:80%;">mm</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.11.1.1" class="ltx_text" style="font-size:113%;">Table 2</span>: </span><span id="S3.T2.12.2" class="ltx_text" style="font-size:113%;">Comparison of pose annotation quality for different dataset setups. The error for RGBD is exemplified with the standard deviation of the Microsoft Azure Kinect <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</span></figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:496.9pt;height:34.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-139.6pt,9.7pt) scale(0.640162785744312,0.640162785744312) ;">
<table id="S3.T3.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.2.2.2" class="ltx_tr">
<td id="S3.T3.2.2.2.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">3D<sub id="S3.T3.2.2.2.2.1" class="ltx_sub"><span id="S3.T3.2.2.2.2.1.1" class="ltx_text ltx_font_italic">25</span></sub> / 3D<sub id="S3.T3.2.2.2.2.2" class="ltx_sub"><span id="S3.T3.2.2.2.2.2.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="S3.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">Bottle</td>
<td id="S3.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">Box</td>
<td id="S3.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t">Can</td>
<td id="S3.T3.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">Cup</td>
<td id="S3.T3.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t">Remote</td>
<td id="S3.T3.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t">Teapot</td>
<td id="S3.T3.2.2.2.9" class="ltx_td ltx_align_center ltx_border_t">Cutlery</td>
<td id="S3.T3.2.2.2.10" class="ltx_td ltx_align_center ltx_border_t">Glassware</td>
<td id="S3.T3.2.2.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mean</td>
</tr>
<tr id="S3.T3.2.2.3.1" class="ltx_tr">
<td id="S3.T3.2.2.3.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</td>
<td id="S3.T3.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T3.2.2.3.1.2.1" class="ltx_text ltx_font_bold">91.17</span> / 0.65</td>
<td id="S3.T3.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">16.10 / 0.01</td>
<td id="S3.T3.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T3.2.2.3.1.4.1" class="ltx_text ltx_font_bold">85.44</span> / <span id="S3.T3.2.2.3.1.4.2" class="ltx_text ltx_font_bold">23.01</span>
</td>
<td id="S3.T3.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">51.83 / 1.48</td>
<td id="S3.T3.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T3.2.2.3.1.6.1" class="ltx_text ltx_font_bold">93.26</span> / <span id="S3.T3.2.2.3.1.6.2" class="ltx_text ltx_font_bold">86.05</span>
</td>
<td id="S3.T3.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.00 / 0.00</td>
<td id="S3.T3.2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_t">4.89 / 0.01</td>
<td id="S3.T3.2.2.3.1.9" class="ltx_td ltx_align_center ltx_border_t">4.00 / 0.06</td>
<td id="S3.T3.2.2.3.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">43.34 / 13.91</td>
</tr>
<tr id="S3.T3.2.2.4.2" class="ltx_tr">
<td id="S3.T3.2.2.4.2.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_t">CPS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S3.T3.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">80.08 / <span id="S3.T3.2.2.4.2.2.1" class="ltx_text ltx_font_bold">40.30</span>
</td>
<td id="S3.T3.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S3.T3.2.2.4.2.3.1" class="ltx_text ltx_font_bold">31.68</span> / <span id="S3.T3.2.2.4.2.3.2" class="ltx_text ltx_font_bold">28.18</span>
</td>
<td id="S3.T3.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">68.96 / 6.69</td>
<td id="S3.T3.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S3.T3.2.2.4.2.5.1" class="ltx_text ltx_font_bold">81.60</span> / <span id="S3.T3.2.2.4.2.5.2" class="ltx_text ltx_font_bold">70.24</span>
</td>
<td id="S3.T3.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">86.30 / 37.08</td>
<td id="S3.T3.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S3.T3.2.2.4.2.7.1" class="ltx_text ltx_font_bold">67.43</span> / <span id="S3.T3.2.2.4.2.7.2" class="ltx_text ltx_font_bold">4.31</span>
</td>
<td id="S3.T3.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S3.T3.2.2.4.2.8.1" class="ltx_text ltx_font_bold">44.00</span> / <span id="S3.T3.2.2.4.2.8.2" class="ltx_text ltx_font_bold">24.95</span>
</td>
<td id="S3.T3.2.2.4.2.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S3.T3.2.2.4.2.9.1" class="ltx_text ltx_font_bold">30.33</span> / <span id="S3.T3.2.2.4.2.9.2" class="ltx_text ltx_font_bold">17.74</span>
</td>
<td id="S3.T3.2.2.4.2.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T3.2.2.4.2.10.1" class="ltx_text ltx_font_bold">61.30</span> / <span id="S3.T3.2.2.4.2.10.2" class="ltx_text ltx_font_bold">28.69</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S3.T3.5.2" class="ltx_text" style="font-size:90%;">Class-wise evaluation of 3D IoU for NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and CPS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> on test split of known objects.</span></figcaption>
</figure>
<figure id="S3.T4" class="ltx_table">
<div id="S3.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:496.9pt;height:41.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-77.9pt,6.4pt) scale(0.761269561789405,0.761269561789405) ;">
<table id="S3.T4.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.2.2.2" class="ltx_tr">
<td id="S3.T4.2.2.2.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">3D<sub id="S3.T4.2.2.2.2.1" class="ltx_sub"><span id="S3.T4.2.2.2.2.1.1" class="ltx_text ltx_font_italic">25</span></sub> / 3D<sub id="S3.T4.2.2.2.2.2" class="ltx_sub"><span id="S3.T4.2.2.2.2.2.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="S3.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">Bottle</td>
<td id="S3.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">Box</td>
<td id="S3.T4.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t">Can</td>
<td id="S3.T4.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">Cup</td>
<td id="S3.T4.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t">Remote</td>
<td id="S3.T4.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t">Teapot</td>
<td id="S3.T4.2.2.2.9" class="ltx_td ltx_align_center ltx_border_t">Cutlery</td>
<td id="S3.T4.2.2.2.10" class="ltx_td ltx_align_center ltx_border_t">Glassware</td>
<td id="S3.T4.2.2.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mean</td>
</tr>
<tr id="S3.T4.2.2.3.1" class="ltx_tr">
<td id="S3.T4.2.2.3.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">Experiment 1</td>
<td id="S3.T4.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.2.2.3.1.2.1" class="ltx_text ltx_font_bold">91.17</span> / 0.65</td>
<td id="S3.T4.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">16.10 / <span id="S3.T4.2.2.3.1.3.1" class="ltx_text ltx_font_bold">0.01</span>
</td>
<td id="S3.T4.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.2.2.3.1.4.1" class="ltx_text ltx_font_bold">85.44</span> / <span id="S3.T4.2.2.3.1.4.2" class="ltx_text ltx_font_bold">23.01</span>
</td>
<td id="S3.T4.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">51.83 / <span id="S3.T4.2.2.3.1.5.1" class="ltx_text ltx_font_bold">1.48</span>
</td>
<td id="S3.T4.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.2.2.3.1.6.1" class="ltx_text ltx_font_bold">93.26</span> / <span id="S3.T4.2.2.3.1.6.2" class="ltx_text ltx_font_bold">86.05</span>
</td>
<td id="S3.T4.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.00 / 0.00</td>
<td id="S3.T4.2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_t">4.89 / <span id="S3.T4.2.2.3.1.8.1" class="ltx_text ltx_font_bold">0.01</span>
</td>
<td id="S3.T4.2.2.3.1.9" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.2.2.3.1.9.1" class="ltx_text ltx_font_bold">4.00</span> / <span id="S3.T4.2.2.3.1.9.2" class="ltx_text ltx_font_bold">0.06</span>
</td>
<td id="S3.T4.2.2.3.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T4.2.2.3.1.10.1" class="ltx_text ltx_font_bold">43.3</span> / <span id="S3.T4.2.2.3.1.10.2" class="ltx_text ltx_font_bold">13.91</span>
</td>
</tr>
<tr id="S3.T4.2.2.4.2" class="ltx_tr">
<td id="S3.T4.2.2.4.2.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_t">Experiment 2</td>
<td id="S3.T4.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">13.70 / <span id="S3.T4.2.2.4.2.2.1" class="ltx_text ltx_font_bold">1.28</span>
</td>
<td id="S3.T4.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S3.T4.2.2.4.2.3.1" class="ltx_text ltx_font_bold">27.74</span> / 0.00</td>
<td id="S3.T4.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">48.17 / 0.00</td>
<td id="S3.T4.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S3.T4.2.2.4.2.5.1" class="ltx_text ltx_font_bold">61.77</span> / 0.00</td>
<td id="S3.T4.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">8.35 / 0.00</td>
<td id="S3.T4.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S3.T4.2.2.4.2.7.1" class="ltx_text ltx_font_bold">4.90</span> / 0.00</td>
<td id="S3.T4.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S3.T4.2.2.4.2.8.1" class="ltx_text ltx_font_bold">16.10</span> / 0.00</td>
<td id="S3.T4.2.2.4.2.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.83 / 0.00</td>
<td id="S3.T4.2.2.4.2.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">22.70 / 0.17</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S3.T4.5.2" class="ltx_text" style="font-size:90%;">Class-wise evaluation of 3D IoU for NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> on seen (Experiment 1) and mostly unseen (Experiment 2) objects.</span></figcaption>
</figure>
<figure id="S3.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F10.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.08811/assets/images/_newplot1.png" id="S3.F10.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="608" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F10.sf1.3.2" class="ltx_text" style="font-size:90%;">NOCS result in the first experiment </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F10.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.08811/assets/images/_new_plot2_4_3.5_.png" id="S3.F10.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="608" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F10.sf2.3.2" class="ltx_text" style="font-size:90%;">CPS result in the first experiment </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F10.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.08811/assets/images/_newplot3.png" id="S3.F10.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="608" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F10.sf3.3.2" class="ltx_text" style="font-size:90%;">NOCS result in the second experiment </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S3.F10.3.2" class="ltx_text" style="font-size:90%;">Plots of average precision (AP) with respect to 3D IoU thresholds for each category. </span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Benchmarks and Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Both monocular (CPS) and RGB-D based (NOCS) category-level methods are considered for the baseline evaluation on the PhoCaL dataset. For the evaluation of NOCS, the normal object coordinate space maps are rendered for each training image and will be published together with the dataset. With the predicted normalized object shape from NOCS map, the depth information is used to lift 2D detection to 3D space using ICP. Considering the artifacts in the depth data from metallic and transparent objects in the dataset, along with the occlusion, the test sequences are very challenging for RGBD methods.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Similiar to NOCS, CPS first detects 2D bounding boxes. Then lifting modules for each class transform 2D image features to 6D pose and scales. Simultaneously the method also estimates the point cloud shape for the respective object class. CPS is trained on approximately 1000 object instance models for each category to learn a deep point cloud encoding of each class. The 2D detection and lifting modules are trained together for 100k steps with a learning rate of 1e-4, decaying to 1e-5 at 60k steps.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Pipeline</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Our dataset consists of 24 image sequences in total with training and testing split in each sequence. In our evaluation pipeline, the training split of the first 12 sequences are used to train the network. To have an evaluation on both the known and novel objects in each category, two experiments are designed. To evaluate on seen objects firstly, the network is trained on the training split of the first 12 sequences and tested on the testing split of the same sequences. To further evaluate the generalization ability of NOCS and CPS to novel objects in the same category, the same training split of the first 12 sequences is used, but we evaluate the result on the testing split of the latter 12 sequences, where objects are mostly unseen. With this way, generalization ability of the methods to novel objects in the category is emphasized, which is a common issue in real operating environments. The evaluation metric is the intersection over union (IoU) result with a threshold of 25% and 50%.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Result</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The 3D IoU at 25% and 50% evaluations of NOCS for the first experiment setup is shown in Tab. <a href="#S3.T3" title="Table 3 ‣ 3.7 Evaluation of Overall Annotation Quality ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The mean average precision (mAP) for 3D IoU at 25% is 43.34%. It is observed in the experiment that even if the segmentation and normalized object coordinate map predictions are accurate, the lifting from NOCS map to 6D space is sensitive to artifacts in depth maps. Since the objects are highly occluded in the PhoCaL dataset, and depth measurements are inaccurate because of cutlery and glassware categories, the method does not have a good performance on the dataset which indicates the drawbacks of RGBD methods in these photometrically challenging cases.
The average precision of each category with respect to 3D IoU threshold is plotted in Fig. <a href="#S3.F10.sf1" title="Figure 10(a) ‣ Figure 10 ‣ 3.7 Evaluation of Overall Annotation Quality ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(a)</span></a>. Note that the results of cutlery and glassware categories are among the worst three categories.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For comparison, the result of CPS is also listed in Tab. <a href="#S3.T3" title="Table 3 ‣ 3.7 Evaluation of Overall Annotation Quality ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. As can be seen from the table, CPS has a higher precision for cutlery and glassware categories. Monocular methods are not affected by artifacts in depth images, which explains the result from the experiment. CPS has a higher mAP of 61.30%, which means RGB has an advantage in dealing with photometrically challenging objects. The detailed APs for each category are plotted in Fig. <a href="#S3.F10.sf2" title="Figure 10(b) ‣ Figure 10 ‣ 3.7 Evaluation of Overall Annotation Quality ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(b)</span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">In addition, the NOCS evaluation on both experiments are compared in table <a href="#S3.T4" title="Table 4 ‣ 3.7 Evaluation of Overall Annotation Quality ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The evaluation result for the second experiment has a lower mAP for 3D IoU at 25% and 50% as expected, as most of the test objects are novel in the second experiment. Fig. <a href="#S3.F10.sf3" title="Figure 10(c) ‣ Figure 10 ‣ 3.7 Evaluation of Overall Annotation Quality ‣ 3 Dataset Acquisition Pipeline ‣ PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(c)</span></a> plots NOCS APs in the second experiment. In comparison to NOCS, the CPS result drops significantly in the second experiment and the 3D IoU at 25% is 4.3%. The result shows that pretraining with a large amount of synthetics images is necessary for monocular methods, to learn the correct lifting from 2D detection to 3D space without the help of depth images.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Limitations</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Even though the proposed pipeline for annotating the 6D pose ground truth is accurate, annotating the objects with deformable surface, such as empty boxes, poses a challenge during the surface measurement step in the workflow due to its light deformation which could deteriorate the quality of both initial pose and ICP based refinement. Moreover, the limited workspace of the robot constrains the view angles in the image sequences which is an issue the PhoCaL shares with other robotic acquisition setups.
The hand eye calibration of the camera plays a key role for the annotation quality. If the camera resolution is low, a good calibration result requires significantly more input images from different angles.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper we introduce the PhoCaL dataset, which contains photometrically challenging categories. High-quality 6D pose annotations are provided for all categories and multiple camera modalities, namely RGBD and RGBP. With our manipulator-driven annotation pipeline, we reach pose accuracy levels that are one order of magnitude more precise than previous vision-sensor-only pipelines even for photometrically complex objects. Moreover, baselines are provided for future works on category-level 6D pose on our dataset by evaluating both monocular and RGB-D methods. The evaluation shows the difficulty level of the dataset in particular for objects that include reflective and transparent surfaces. PhoCaL therefore constitutes a challenging dataset with accurate ground truth that can pave the way for future pose pipelines that are applicable to more realistic scenarios with everyday objects.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Gwon Hwan An, Siyeong Lee, Min-Woo Seo, Kugjin Yun, Won-Sik Cheong, and Suk-Ju
Kang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Charuco board-based omnidirectional camera calibration method.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Electronics</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 7(12):421, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Paul J Besl and Neil D McKay.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Method for registration of 3-d shapes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensor fusion IV: control paradigms and data structures</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">,
volume 1611, pages 586–606. International Society for Optics and Photonics,
1992.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Tolga Birdal, Benjamin Busam, Nassir Navab, Slobodan Ilic, and Peter Sturm.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">A minimalist approach to type-agnostic detection of quadrics in point
clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 3530–3540, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Tolga Birdal, Benjamin Busam, Nassir Navab, Slobodan Ilic, and Peter Sturm.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Generic primitive detection in point clouds using novel minimal
quadric fits.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">,
42(6):1333–1347, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton,
and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Learning 6d object pose estimation using 3d object coordinates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">,
pages 536–551. Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Yannick Bukschat and Marcus Vetter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Efficientpose: An efficient, accurate and scalable end-to-end 6d
multi object pose estimation approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2011.04307</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Busam, Hyun Jun Jung, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">I like to move it: 6d pose estimation as an action decision process.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.12678</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Berk Calli, Aaron Walsman, Arjun Singh, Siddhartha Srinivasa, Pieter Abbeel,
and Aaron M Dollar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Benchmarking in manipulation research: The ycb object and model set
and benchmarking protocols.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1502.03143</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Peter Carr, Yaser Sheikh, and Iain Matthews.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Monocular object detection using 3d geometric primitives.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">,
pages 864–878. Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Dengsheng Chen, Jun Li, Zheng Wang, and Kai Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Learning canonical shape space for category-level 6d object pose and
size estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 11973–11982, 2020.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Kai Chen and Qi Dou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Sgpa: Structure-guided prior adaptation for category-level 6d object
pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 2773–2782, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan,
Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Blenderproc.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.01911</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Andreas Doumanoglou, Rigas Kouskouridas, Sotiris Malassiotis, and Tae-Kyun Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Recovering 6d object pose and predicting next-best-view in the crowd.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 3583–3592, 2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Bertram Drost, Markus Ulrich, Paul Bergmann, Philipp Hartinger, and Carsten
Steger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Introducing mvtec itodd - a dataset for 3d object recognition in
industry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision Workshops</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, Oct 2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Graspnet-1billion: A large-scale benchmark for general object
grasping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 11444–11453, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Mathieu Garon, Denis Laurendeau, and Jean-François Lalonde.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">A framework for evaluating 6-dof object trackers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">,
pages 582–597, 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Mathieu Garon, Denis Laurendeau, and Jean-François Lalonde.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">A framework for evaluating 6-DOF object trackers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">,
2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Are we ready for autonomous driving? the kitti vision benchmark
suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 3354–3361. IEEE, 2012.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Stefan Hinterstoisser, Stefan Holzer, Cedric Cagniart, Slobodan Ilic, Kurt
Konolige, Nassir Navab, and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Multimodal templates for real-time detection of texture-less objects
in heavily cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 858–865. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Tomáš Hodan, Pavel Haluza, Štepán Obdržálek, Jiri
Matas, Manolis Lourakis, and Xenophon Zabulis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">T-less: An rgb-d dataset for 6d pose estimation of texture-less
objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE Winter Conference on Applications of Computer
Vision (WACV)</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 880–888. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl, Anders GlentBuch, Dirk
Kraft, Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Bop: Benchmark for 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">,
pages 19–34, 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Agastya Kalra, Vage Taamazyan, Supreeth Krishna Rao, Kartik Venkataraman,
Ramesh Raskar, and Achuta Kadambi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Deep polarization cues for transparent object segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 8602–8611, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Roman Kaskman, Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Homebreweddb: Rgb-d dataset for 6d pose estimation of 3d objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision Workshops</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great
again.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 1521–1529, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Jiehong Lin, Zewei Wei, Zhihao Li, Songcen Xu, Kui Jia, and Yuanqing Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Dualposenet: Category-level 6d object pose and size estimation using
dual pose network with refined learning of pose consistency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 3560–3569, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Xingyu Liu, Shun Iwase, and Kris M Kitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Stereobj-1m: Large-scale stereo image dataset for 6d object pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 10870–10879, 2021.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Xingyu Liu, Rico Jonschkowski, Anelia Angelova, and Kurt Konolige.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Keypose: Multi-view 3d labeling and keypoint estimation for
transparent objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 11602–11610, 2020.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Fabian Manhardt, Manuel Nickel, Sven Meier, Luca Minciullo, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Cps: Class-level 6d pose and shape estimation from monocular images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2003.05848</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Fabian Manhardt, Gu Wang, Benjamin Busam, Manuel Nickel, Sven Meier, Luca
Minciullo, Xiangyang Ji, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Cps++: Improving class-level 6d pose and shape estimation from
monocular images with self-supervised learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2003.05848</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Lucas Manuelli, Wei Gao, Peter Florence, and Russ Tedrake.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">kpam: Keypoint affordances for category-level robotic manipulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1903.06684</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Pat Marion, Peter R Florence, Lucas Manuelli, and Russ Tedrake.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Label fusion: A pipeline for generating ground truth labels for real
rgbd data of cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Robotics and Automation</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">,
pages 3235–3242. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Keunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Latentfusion: End-to-end differentiable reconstruction and rendering
for unseen object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 10710–10719, 2020.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Cody J Phillips, Matthieu Lecce, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Seeing glassware: from edge detection to pose estimation and shape
recovery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Robotics: Science and Systems</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, volume 3, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Colin Rennie, Rahul Shome, Kostas E Bekris, and Alberto F De Souza.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">A dataset for improved rgbd-based object detection and pose
estimation for warehouse pick-and-place.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 1(2):1179–1185, 2016.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Shreeyak Sajjan, Matthew Moore, Mike Pan, Ganesh Nagaraja, Johnny Lee, Andy
Zeng, and Shuran Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Clear grasp: 3d shape estimation of transparent objects for
manipulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Robotics and Automation</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">,
pages 3634–3642. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Ashutosh Saxena, Justin Driemeyer, and Andrew Y Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Robotic grasping of novel objects using vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The International Journal of Robotics Research</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 27(2):157–173,
2008.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Alykhan Tejani, Danhang Tang, Rigas Kouskouridas, and Tae-Kyun Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Latent-class hough forests for 3D object detection and pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">,
pages 462–477. Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Tremblay, Thang To, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Falling things: A synthetic dataset for 3d object detection and pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 2038–2041, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Roger Y Tsai, Reimar K Lenz, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">A new technique for fully autonomous and efficient 3 d robotics
hand/eye calibration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on robotics and automation</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 5(3):345–358,
1989.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and
Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Normalized object coordinate space for category-level 6d object pose
and size estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 2642–2651, 2019.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Posecnn: A convolutional neural network for 6d object pose estimation
in cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Robotics: Science and Systems</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Dpod: 6d pose object detector and refiner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 1941–1950, 2019.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Shihao Zou, Xinxin Zuo, Yiming Qian, Sen Wang, Chi Xu, Minglun Gong, and Li
Cheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">3d human shape reconstruction from a polarization image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 351–368.
Springer, 2020.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.08808" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.08811" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.08811">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.08811" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.08812" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 13:05:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
