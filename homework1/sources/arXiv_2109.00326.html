<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.00326] Category-Level Metric Scale Object Shape and Pose Estimation</title><meta property="og:description" content="Advances in deep learning recognition have led to accurate object detection with 2D images.
However, these 2D perception methods are insufficient for complete 3D world information.
Concurrently, advanced 3D shape estim…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Category-Level Metric Scale Object Shape and Pose Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Category-Level Metric Scale Object Shape and Pose Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.00326">

<!--Generated on Wed Mar  6 21:51:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Robot manipulation,  augmented reality,  object shape estimation,  object pose estimation.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Category-Level Metric Scale Object Shape and Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Taeyeop Lee<sup id="6.6.1" class="ltx_sup"><span id="6.6.1.1" class="ltx_text ltx_font_italic">1</span></sup>, Byeong-Uk Lee<sup id="7.7.2" class="ltx_sup"><span id="7.7.2.1" class="ltx_text ltx_font_italic">1</span></sup>, Myungchul Kim<sup id="8.8.3" class="ltx_sup"><span id="8.8.3.1" class="ltx_text ltx_font_italic">1</span></sup>, and In So Kweon <sup id="9.9.4" class="ltx_sup"><span id="9.9.4.1" class="ltx_text ltx_font_italic">1</span></sup>



</span><span class="ltx_author_notes">
Manuscript received: April 30, 2021; Revised July 27, 2021; Accepted August, 23, 2021.
This paper was recommended for publication by Editor Cesar Cadena Lerma upon evaluation of the Associate Editor and
Reviewers’ comments.
This work was supported by the Technology Innovation Program (10070171, Development of core technology for advanced locomotion/manipulation based on high-speed/power robot platform and robot intelligence) funded by the Ministry of Trade, Industry and Energy (MOTIE, Korea). <span id="10.10.1" class="ltx_text ltx_font_italic">(Corresponding author: I. S. Kweon.)</span><sup id="11.11.1" class="ltx_sup"><span id="11.11.1.1" class="ltx_text ltx_font_italic">1</span></sup>T. Lee, BU Lee, M. Kim and I. S. Kweon are with the Robotics and Computer Vision Laboratory, KAIST, Daejeon, Republic of Korea (e-mail: taeyeop.trevor@kaist.ac.kr; byeonguk.lee@kaist.ac.kr; gritycda@kaist.ac.kr; iskweon77@kaist.ac.kr).
Digital Object Identifier (DOI): see top of this page.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="12.1" class="ltx_p">Advances in deep learning recognition have led to accurate object detection with 2D images.
However, these 2D perception methods are insufficient for complete 3D world information.
Concurrently, advanced 3D shape estimation approaches focus on the shape itself, without considering metric scale.
These methods cannot determine the accurate location and orientation of objects. To tackle this problem, we propose a framework that jointly estimates a metric scale shape and pose from a single RGB image.
Our framework has two branches: the Metric Scale Object Shape branch (MSOS) and the Normalized Object Coordinate Space branch (NOCS).
The MSOS branch estimates the metric scale shape observed in the camera coordinates.
The NOCS branch predicts the normalized object coordinate space (NOCS) map and performs similarity transformation with the rendered depth map from a predicted metric scale mesh to obtain 6d pose and size.
Additionally, we introduce the Normalized Object Center Estimation (NOCE) to estimate the geometrically aligned distance from the camera to the object center.
We validated our method on both synthetic and real-world datasets to evaluate category-level object pose and shape.
</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Robot manipulation, augmented reality, object shape estimation, object pose estimation.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">3D object recognition is one of the crucial tasks in various robotics and computer vision applications, such as robot manipulation and augmented reality (AR).
To recognize and interact with objects in the 3D space, these applications require acquiring the types, shapes, sizes, locations, and orientations of the objects.
For example, for a robot to grasp a mug, it needs to know what kind of a mug it is (type), how it is shaped (shape), how big or small the mug is (size), where the mug is exactly lying (location), and in what direction the handle of the mug is pointing (orientation).
In augmented reality, these kinds of information also enable virtual interaction with the objects re-rendered from the real world.
The shape and size information can be referred to as a metric scale object shape.
The location and orientation of an object can be interpreted as a 6D object pose, with 3 degrees of freedom for each.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Many of the previous 6D object pose estimation methods have tried to solve instance-level pose estimation, by assuming that the 3D CAD model and size information of each object is known.
Recent RGB-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> have shown remarkable performances utilizing the Perspective-n-Point (PnP) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
However, it is not easy to have or obtain accurate 3D scans of every object, and this makes it difficult to apply these algorithms in general settings where the objects have never been seen before, and therefore have no 3D CAD models.
Also, it is inefficient to train a separate network each time a new object is observed.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address this problem, category-level 6D pose and size estimation approaches have been proposed to handle unseen objects.
Category-level pose estimation tasks assume that the object instances have not been previously seen, but their categories are still within known categories.
Wang <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p3.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> introduced a new representation called Normalized Object Coordinate Space (NOCS), to align different object instances within one category in a shared orientation.
By predicting a category-wise similar NOCS map, it can estimate the 6D pose of an unseen object without a 3D model.
To estimate an object pose and size information, previous category-level 6D pose estimation methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> have required depth information instead of 3D model information.
Chen <em id="S1.p3.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p3.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> proposed an algorithm that predicts the shape and the pose of an object without using depth information, but its RGB-based object shape estimation results lack object size information.
Moreover, current category-level pose estimation methods do not deal with metric scale object shapes in 3D space.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2109.00326/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.3.1" class="ltx_text ltx_font_bold">3D shape and 6D Pose estimation examples of our proposed framework.</span> Our framework takes an input image and predicts the metric scale 3D shape, 6D pose of the object.
</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2109.00326/assets/x2.png" id="S1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S1.F2.3.1" class="ltx_text ltx_font_bold">An overview of our framework.</span> Our framework first detects class label, bounding box and segmentation mask using the off-the-shelf instance segmentation method. The detected image patch goes into two branches: a metric scale object shape branch (MSOS) and a normalized object coordinate space branch (NOCS).
The MSOS branch estimates the shape of the metric scale objects and rendered depths by projecting the predicted metric scale object mesh onto the image plane.
The 6D object pose is obtained by similarity transformation using a rendered depth map and the predicted NOCS map.
</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we propose a CNN-based category-level metric scale object shape and pose estimation algorithm, utilizing only a single RGB image with none or very sparse depth information.
Our algorithm consists of two neural network branches, where one performs the metric scale mesh estimation and the other one estimates the NOCS map.
We replace the 3D CAD model or the depth information that was used in the previous 6D pose estimation approaches, with the metric scale mesh itself and the rendered depth map acquired by projecting the mesh onto the image plane.
With the obtained NOCS map and the rendered depth map, our network predicts the size and 6D pose of an object via similarity transformation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
Our category-level object shape, pose and size estimation algorithm had the best performances among RGB-based 6D pose estimation methods in the benchmark evaluation.
Also, without any additional training, our algorithm refines the scale and size prediction using only a few sparse depth inputs, and showed results comparable to the previous RGBD-based approaches in object size and pose estimation.
We additionally performed various ablation studies to show the effectiveness and validity of our design choices.</p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2109.00326/assets/x3.png" id="S1.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="124" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S1.F3.3.1" class="ltx_text ltx_font_bold">Illustration of the MSOS branch.</span>
Our metric scale object shape branch (MSOS) has two headers with a shared feature encoder.
The mesh header estimates a normalized object mesh.
The Z header estimates the object center and radius to lift the normalized object mesh to the metric scale object mesh. Graph convolution is used to refine the mesh.
</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">RELATED WORK</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Instance-level object pose estimation</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">With the increasing interest in 3D object recognition, various instance-level object pose estimation approaches have been introduced.
Instance-level pose estimation approaches can be broadly divided into two categories: correspondence-based methods and regression-based methods.
The regression-based methods directly infer the rotation and translation from input images.
The challenging part of these approaches is that non-linearity in the pose space makes training hard to generalize <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
To get an accurate pose, these methods require certain types of refinement processes such as ICP which are based on 3D CAD models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
Correspondence-based methods rely on point matching between the 2D image and a 3D model.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> regressed the corresponding 3D object coordinates for each image pixel in the masked region.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> detected the keypoints in the projected 3D points on the image and then solved a non-linear problem using the Perspective-n-Point (PnP) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
While all of the aforementioned instance-level works have shown impressive results, most of them require 3D CAD model information at both the training and inference times.
This dependency on 3D CAD models limits their applicability since storing and updating all objects at test time is time consuming and impractical for real-world scenarios.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Category-level object pose estimation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Some methods have focused on estimating the pose of unseen objects using known categories during training when no instance-specific 3D models are given at test time.
As a pioneering work, Wang <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> introduced a Normalized Object Coordinate Space (NOCS) for category-level 6D object pose estimation.
Wang <em id="S2.SS2.p1.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p1.1.4" class="ltx_text"></span> estimated NOCS, the shared canonical representation of object instances within a category, from the RGB image.
It indirectly estimates the pose and size of an object by matching their predicted NOCS map and observed depth map using non-linear solutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
Tian <em id="S2.SS2.p1.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p1.1.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> proposed a more advanced network that explicitly learns deformation from the shape prior to estimating the NOCS map.
Chen <em id="S2.SS2.p1.1.7" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p1.1.8" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> regressed the pose and size directly from the image and depth.
However, to reliably predict object size, these methods inevitably require depth information.
Recently, Chen <em id="S2.SS2.p1.1.9" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p1.1.10" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> proposed a generative model that generates the object appearance from various viewpoints.
Synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> estimates an object pose from an RGB image by matching the input and the generated appearance using an iterative alignment at inference time.
Therefore, this approach does not handle 3D object shapes and sizes.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Category-Level Metric Scale Object Shape and Pose Estimation</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Given an RGB image, our goal is to estimate the object shape, pose, and size of an object instance.
Fig. <a href="#S1.F2" title="Figure 2 ‣ I INTRODUCTION ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the overall pipeline of our framework.
Our framework detects the class label, bounding box, and segmentation using an off-the-shelf instance segmentation network.
The detected image patches are then used as inputs of the two branches: a metric scale object shape branch (MSOS) and a normalized object coordinate space branch (NOCS).
The MSOS branch estimates the object shape in the metric scale (Sec. <a href="#S3.SS1" title="III-A Metric Scale Object Shape branch (MSOS) ‣ III Category-Level Metric Scale Object Shape and Pose Estimation ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>) and renders 2D depth maps by projecting the predicted metric scale object shape onto the image plane (Sec. <a href="#S3.SS2" title="III-B Depth Rendering ‣ III Category-Level Metric Scale Object Shape and Pose Estimation ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>).
The rendered depth map and predicted NOCS map from the NOCS branch are used to estimate the pose and size of the object using the similarity transformation (Sec. <a href="#S3.SS3" title="III-C Pose Estimation ‣ III Category-Level Metric Scale Object Shape and Pose Estimation ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>).</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2109.00326/assets/x4.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S3.F4.3.1" class="ltx_text ltx_font_bold">Difference between point cloud and mesh representation.</span>
Compared to mesh representation, point cloud representation does not contain information on the object surface and invisible parts, therefore the shape is too sparse.
</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Metric Scale Object Shape branch (MSOS)</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Various methods have been developed for different 3D shape representations, such as voxels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, point clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and meshes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
In robotics and AR applications, we believe that dense and accurate 3D shape representation is necessary.
The accuracy of the detailed structure in a voxel representation is dependent on the voxel resolution in 3D space.
Point cloud representation may handle more details with less-quantized 3D points, but as shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ III Category-Level Metric Scale Object Shape and Pose Estimation ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, it does not contain object surface information and as a result, the shape is too sparse.
For these reasons, we chose the mesh representation, which is capable of showing both dense and accurate 3D shapes.
Also, mesh enables the depth rendering process described in Sec. <a href="#S3.SS2" title="III-B Depth Rendering ‣ III Category-Level Metric Scale Object Shape and Pose Estimation ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The Metric Scale Object Shape (MSOS) branch estimates the metric scale object mesh from the camera coordinate using a single image.
Fig. <a href="#S1.F3" title="Figure 3 ‣ I INTRODUCTION ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows details of the MSOS branch.
The MSOS branch consists of two jointly-trained headers with a shared encoder.
The mesh header estimates the normalized object mesh, and the Z header predicts <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">Z</annotation></semantics></math> information, which is the distance of the object along the z-axis from the camera center, to lift the normalized object shape to a metric scale object shape.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>Normalized Object Mesh Estimation (Mesh header)</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Instead of training a single network for each category <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, we aim to cover all object categories with a single network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
For this reason, while not being sensitive to any specific network baseline, we follow a similar scheme as the Mesh R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to estimate the normalized object mesh <math id="S3.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="M_{\text{norm}}" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><msub id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml">M</mi><mtext id="S3.SS1.SSS1.p1.1.m1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.1.1.3a.cmml">norm</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.2">𝑀</ci><ci id="S3.SS1.SSS1.p1.1.m1.1.1.3a.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3">norm</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">M_{\text{norm}}</annotation></semantics></math>.
The detected object in the instance segmentation stage is cropped using the bounding box and is resized to have the same patch size.
By taking this resized image patch as input, the mesh header can efficiently learn the object shape by utilizing just the relevant information.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.4.1.1" class="ltx_text">III-A</span>2 </span>Normalized Object Center Estimation (NOCE)</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.2" class="ltx_p">To transform the predicted normalized mesh into the camera coordinate, <em id="S3.SS1.SSS2.p1.2.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.SSS2.p1.2.2" class="ltx_text"></span> metric scale, we need the location and scale information of the object.
As shown in Fig. <a href="#S1.F3" title="Figure 3 ‣ I INTRODUCTION ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we represent the distance from the camera to the object center in the z-axis as object center <math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mi id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><ci id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">Z</annotation></semantics></math>, and we also define the radius of the metric scale object mesh as object radius <math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mi id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><ci id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">R</annotation></semantics></math>.
The object center handles the location of the metric mesh, and the object radius decides the scale/size.
We call our object center and object radius estimation branch the Z header since both values are predicted along the z-axis from the camera.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">Estimating the distance to the center of an object using only an RGB image is challenging.
The main reason is that the RGB image does not contain any information regarding 3D volume.
To tackle this problem, we implicitly embed the mesh prediction feature in the training process, using a shared feature encoder for the mesh header and Z header.
We believe that by jointly estimating object shape and object center, each header can be guided using the objective of the other header.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.3" class="ltx_p">As explained in Sec. <a href="#S3.SS1.SSS1" title="III-A1 Normalized Object Mesh Estimation (Mesh header) ‣ III-A Metric Scale Object Shape branch (MSOS) ‣ III Category-Level Metric Scale Object Shape and Pose Estimation ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>, using a cropped and resized image on the object region as input for our mesh header is beneficial for mesh quality and efficient inference.
However, when resizing the cropped image patch, the information in the detected bounding box is lost.
This is crucial since the size of the bounding box implies the distance from the camera to the object, Accordingly, resizing the image will cause ambiguity for on <math id="S3.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.SSS2.p3.1.m1.1a"><mi id="S3.SS1.SSS2.p3.1.m1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.1.m1.1b"><ci id="S3.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.1.m1.1c">Z</annotation></semantics></math> and <math id="S3.SS1.SSS2.p3.2.m2.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS2.p3.2.m2.1a"><mi id="S3.SS1.SSS2.p3.2.m2.1.1" xref="S3.SS1.SSS2.p3.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.2.m2.1b"><ci id="S3.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.2.m2.1c">R</annotation></semantics></math> prediction.
For example, as illustrated in Fig. <a href="#S3.F5" title="Figure 5 ‣ III-A2 Normalized Object Center Estimation (NOCE) ‣ III-A Metric Scale Object Shape branch (MSOS) ‣ III Category-Level Metric Scale Object Shape and Pose Estimation ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, upsampling the original image patch is similar to taking the camera closer to the scene, which alters the original <math id="S3.SS1.SSS2.p3.3.m3.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.SSS2.p3.3.m3.1a"><mi id="S3.SS1.SSS2.p3.3.m3.1.1" xref="S3.SS1.SSS2.p3.3.m3.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.3.m3.1b"><ci id="S3.SS1.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.3.m3.1c">Z</annotation></semantics></math> information.
Therefore, the two detected mugs appear to be the same size in the input image patch, while having a completely different object center.
This makes the object center prediction using a cropped and resized image patch an ill-posed problem.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2109.00326/assets/x5.png" id="S3.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="322" height="303" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S3.F5.3.1" class="ltx_text ltx_font_bold">Relationship between the size of the bounding box and the distance from the camera center.</span>
</figcaption>
</figure>
<div id="S3.SS1.SSS2.p4" class="ltx_para">
<p id="S3.SS1.SSS2.p4.3" class="ltx_p">To solve this problem, we propose the normalized object center estimation (NOCE).
The NOCE linearly scales the ground truth object center <math id="S3.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.SSS2.p4.1.m1.1a"><mi id="S3.SS1.SSS2.p4.1.m1.1.1" xref="S3.SS1.SSS2.p4.1.m1.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.1.m1.1b"><ci id="S3.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p4.1.m1.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.1.m1.1c">Z</annotation></semantics></math> by using the size information of the detected bounding box.
We compensate the resizing ratio <math id="S3.SS1.SSS2.p4.2.m2.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS1.SSS2.p4.2.m2.1a"><mi id="S3.SS1.SSS2.p4.2.m2.1.1" xref="S3.SS1.SSS2.p4.2.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.2.m2.1b"><ci id="S3.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.2.m2.1c">\tau</annotation></semantics></math> to the object center <math id="S3.SS1.SSS2.p4.3.m3.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.SSS2.p4.3.m3.1a"><mi id="S3.SS1.SSS2.p4.3.m3.1.1" xref="S3.SS1.SSS2.p4.3.m3.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.3.m3.1b"><ci id="S3.SS1.SSS2.p4.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.3.m3.1c">Z</annotation></semantics></math>, as</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="Z_{\text{NOCE}}=\frac{Z\times\tau}{{f}}," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml">Z</mi><mtext id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3a.cmml">NOCE</mtext></msub><mo id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml">Z</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.3.2.1" xref="S3.E1.m1.1.1.1.1.3.2.1.cmml">×</mo><mi id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml">τ</mi></mrow><mi id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml">f</mi></mfrac></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2">𝑍</ci><ci id="S3.E1.m1.1.1.1.1.2.3a.cmml" xref="S3.E1.m1.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3">NOCE</mtext></ci></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><divide id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3"></divide><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><times id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.1"></times><ci id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">𝑍</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3">𝜏</ci></apply><ci id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">Z_{\text{NOCE}}=\frac{Z\times\tau}{{f}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS2.p4.12" class="ltx_p">where <math id="S3.SS1.SSS2.p4.4.m1.1" class="ltx_Math" alttext="\tau=H_{o}/H_{\text{patch}}" display="inline"><semantics id="S3.SS1.SSS2.p4.4.m1.1a"><mrow id="S3.SS1.SSS2.p4.4.m1.1.1" xref="S3.SS1.SSS2.p4.4.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p4.4.m1.1.1.2" xref="S3.SS1.SSS2.p4.4.m1.1.1.2.cmml">τ</mi><mo id="S3.SS1.SSS2.p4.4.m1.1.1.1" xref="S3.SS1.SSS2.p4.4.m1.1.1.1.cmml">=</mo><mrow id="S3.SS1.SSS2.p4.4.m1.1.1.3" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.cmml"><msub id="S3.SS1.SSS2.p4.4.m1.1.1.3.2" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.2.cmml"><mi id="S3.SS1.SSS2.p4.4.m1.1.1.3.2.2" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.2.2.cmml">H</mi><mi id="S3.SS1.SSS2.p4.4.m1.1.1.3.2.3" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.2.3.cmml">o</mi></msub><mo id="S3.SS1.SSS2.p4.4.m1.1.1.3.1" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.1.cmml">/</mo><msub id="S3.SS1.SSS2.p4.4.m1.1.1.3.3" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.3.cmml"><mi id="S3.SS1.SSS2.p4.4.m1.1.1.3.3.2" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.3.2.cmml">H</mi><mtext id="S3.SS1.SSS2.p4.4.m1.1.1.3.3.3" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.3.3a.cmml">patch</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.4.m1.1b"><apply id="S3.SS1.SSS2.p4.4.m1.1.1.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1"><eq id="S3.SS1.SSS2.p4.4.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.1"></eq><ci id="S3.SS1.SSS2.p4.4.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.2">𝜏</ci><apply id="S3.SS1.SSS2.p4.4.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.3"><divide id="S3.SS1.SSS2.p4.4.m1.1.1.3.1.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.1"></divide><apply id="S3.SS1.SSS2.p4.4.m1.1.1.3.2.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.4.m1.1.1.3.2.1.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS1.SSS2.p4.4.m1.1.1.3.2.2.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.2.2">𝐻</ci><ci id="S3.SS1.SSS2.p4.4.m1.1.1.3.2.3.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.2.3">𝑜</ci></apply><apply id="S3.SS1.SSS2.p4.4.m1.1.1.3.3.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.4.m1.1.1.3.3.1.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS1.SSS2.p4.4.m1.1.1.3.3.2.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.3.2">𝐻</ci><ci id="S3.SS1.SSS2.p4.4.m1.1.1.3.3.3a.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.3.3"><mtext mathsize="70%" id="S3.SS1.SSS2.p4.4.m1.1.1.3.3.3.cmml" xref="S3.SS1.SSS2.p4.4.m1.1.1.3.3.3">patch</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.4.m1.1c">\tau=H_{o}/H_{\text{patch}}</annotation></semantics></math> while <math id="S3.SS1.SSS2.p4.5.m2.1" class="ltx_Math" alttext="H_{o}" display="inline"><semantics id="S3.SS1.SSS2.p4.5.m2.1a"><msub id="S3.SS1.SSS2.p4.5.m2.1.1" xref="S3.SS1.SSS2.p4.5.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p4.5.m2.1.1.2" xref="S3.SS1.SSS2.p4.5.m2.1.1.2.cmml">H</mi><mi id="S3.SS1.SSS2.p4.5.m2.1.1.3" xref="S3.SS1.SSS2.p4.5.m2.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.5.m2.1b"><apply id="S3.SS1.SSS2.p4.5.m2.1.1.cmml" xref="S3.SS1.SSS2.p4.5.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.5.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p4.5.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p4.5.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p4.5.m2.1.1.2">𝐻</ci><ci id="S3.SS1.SSS2.p4.5.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p4.5.m2.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.5.m2.1c">H_{o}</annotation></semantics></math> refers to the size of the original bounding box, <math id="S3.SS1.SSS2.p4.6.m3.1" class="ltx_Math" alttext="H_{\text{patch}}" display="inline"><semantics id="S3.SS1.SSS2.p4.6.m3.1a"><msub id="S3.SS1.SSS2.p4.6.m3.1.1" xref="S3.SS1.SSS2.p4.6.m3.1.1.cmml"><mi id="S3.SS1.SSS2.p4.6.m3.1.1.2" xref="S3.SS1.SSS2.p4.6.m3.1.1.2.cmml">H</mi><mtext id="S3.SS1.SSS2.p4.6.m3.1.1.3" xref="S3.SS1.SSS2.p4.6.m3.1.1.3a.cmml">patch</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.6.m3.1b"><apply id="S3.SS1.SSS2.p4.6.m3.1.1.cmml" xref="S3.SS1.SSS2.p4.6.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.6.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p4.6.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p4.6.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p4.6.m3.1.1.2">𝐻</ci><ci id="S3.SS1.SSS2.p4.6.m3.1.1.3a.cmml" xref="S3.SS1.SSS2.p4.6.m3.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS2.p4.6.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p4.6.m3.1.1.3">patch</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.6.m3.1c">H_{\text{patch}}</annotation></semantics></math> is the size of the network input image patch, and <math id="S3.SS1.SSS2.p4.7.m4.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS1.SSS2.p4.7.m4.1a"><mi id="S3.SS1.SSS2.p4.7.m4.1.1" xref="S3.SS1.SSS2.p4.7.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.7.m4.1b"><ci id="S3.SS1.SSS2.p4.7.m4.1.1.cmml" xref="S3.SS1.SSS2.p4.7.m4.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.7.m4.1c">f</annotation></semantics></math> denotes the camera focal length.
Instead of the original <math id="S3.SS1.SSS2.p4.8.m5.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.SSS2.p4.8.m5.1a"><mi id="S3.SS1.SSS2.p4.8.m5.1.1" xref="S3.SS1.SSS2.p4.8.m5.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.8.m5.1b"><ci id="S3.SS1.SSS2.p4.8.m5.1.1.cmml" xref="S3.SS1.SSS2.p4.8.m5.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.8.m5.1c">Z</annotation></semantics></math>, the normalized object center <math id="S3.SS1.SSS2.p4.9.m6.1" class="ltx_Math" alttext="Z_{\text{NOCE}}" display="inline"><semantics id="S3.SS1.SSS2.p4.9.m6.1a"><msub id="S3.SS1.SSS2.p4.9.m6.1.1" xref="S3.SS1.SSS2.p4.9.m6.1.1.cmml"><mi id="S3.SS1.SSS2.p4.9.m6.1.1.2" xref="S3.SS1.SSS2.p4.9.m6.1.1.2.cmml">Z</mi><mtext id="S3.SS1.SSS2.p4.9.m6.1.1.3" xref="S3.SS1.SSS2.p4.9.m6.1.1.3a.cmml">NOCE</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.9.m6.1b"><apply id="S3.SS1.SSS2.p4.9.m6.1.1.cmml" xref="S3.SS1.SSS2.p4.9.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.9.m6.1.1.1.cmml" xref="S3.SS1.SSS2.p4.9.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p4.9.m6.1.1.2.cmml" xref="S3.SS1.SSS2.p4.9.m6.1.1.2">𝑍</ci><ci id="S3.SS1.SSS2.p4.9.m6.1.1.3a.cmml" xref="S3.SS1.SSS2.p4.9.m6.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS2.p4.9.m6.1.1.3.cmml" xref="S3.SS1.SSS2.p4.9.m6.1.1.3">NOCE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.9.m6.1c">Z_{\text{NOCE}}</annotation></semantics></math> is used as ground truth during training.
By doing this, similar objects in similar scenes will have similar <math id="S3.SS1.SSS2.p4.10.m7.1" class="ltx_Math" alttext="Z_{\text{NOCE}}" display="inline"><semantics id="S3.SS1.SSS2.p4.10.m7.1a"><msub id="S3.SS1.SSS2.p4.10.m7.1.1" xref="S3.SS1.SSS2.p4.10.m7.1.1.cmml"><mi id="S3.SS1.SSS2.p4.10.m7.1.1.2" xref="S3.SS1.SSS2.p4.10.m7.1.1.2.cmml">Z</mi><mtext id="S3.SS1.SSS2.p4.10.m7.1.1.3" xref="S3.SS1.SSS2.p4.10.m7.1.1.3a.cmml">NOCE</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.10.m7.1b"><apply id="S3.SS1.SSS2.p4.10.m7.1.1.cmml" xref="S3.SS1.SSS2.p4.10.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.10.m7.1.1.1.cmml" xref="S3.SS1.SSS2.p4.10.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p4.10.m7.1.1.2.cmml" xref="S3.SS1.SSS2.p4.10.m7.1.1.2">𝑍</ci><ci id="S3.SS1.SSS2.p4.10.m7.1.1.3a.cmml" xref="S3.SS1.SSS2.p4.10.m7.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS2.p4.10.m7.1.1.3.cmml" xref="S3.SS1.SSS2.p4.10.m7.1.1.3">NOCE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.10.m7.1c">Z_{\text{NOCE}}</annotation></semantics></math> values, which significantly helps to improve the metric scale object shape and translation estimation.
At inference time, we divide the predicted <math id="S3.SS1.SSS2.p4.11.m8.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.SSS2.p4.11.m8.1a"><mi id="S3.SS1.SSS2.p4.11.m8.1.1" xref="S3.SS1.SSS2.p4.11.m8.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.11.m8.1b"><ci id="S3.SS1.SSS2.p4.11.m8.1.1.cmml" xref="S3.SS1.SSS2.p4.11.m8.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.11.m8.1c">Z</annotation></semantics></math> by <math id="S3.SS1.SSS2.p4.12.m9.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS1.SSS2.p4.12.m9.1a"><mi id="S3.SS1.SSS2.p4.12.m9.1.1" xref="S3.SS1.SSS2.p4.12.m9.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.12.m9.1b"><ci id="S3.SS1.SSS2.p4.12.m9.1.1.cmml" xref="S3.SS1.SSS2.p4.12.m9.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.12.m9.1c">\tau</annotation></semantics></math> to restore the original object center information.</p>
</div>
<div id="S3.SS1.SSS2.p5" class="ltx_para">
<p id="S3.SS1.SSS2.p5.4" class="ltx_p">The object radius <math id="S3.SS1.SSS2.p5.1.m1.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS2.p5.1.m1.1a"><mi id="S3.SS1.SSS2.p5.1.m1.1.1" xref="S3.SS1.SSS2.p5.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p5.1.m1.1b"><ci id="S3.SS1.SSS2.p5.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p5.1.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p5.1.m1.1c">R</annotation></semantics></math> is also jointly learned by our Z header.
In the same context as NOCE, the object radius <math id="S3.SS1.SSS2.p5.2.m2.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS2.p5.2.m2.1a"><mi id="S3.SS1.SSS2.p5.2.m2.1.1" xref="S3.SS1.SSS2.p5.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p5.2.m2.1b"><ci id="S3.SS1.SSS2.p5.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p5.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p5.2.m2.1c">R</annotation></semantics></math> is trained on the normalized scale.
The final prediction <math id="S3.SS1.SSS2.p5.3.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS2.p5.3.m3.1a"><mi id="S3.SS1.SSS2.p5.3.m3.1.1" xref="S3.SS1.SSS2.p5.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p5.3.m3.1b"><ci id="S3.SS1.SSS2.p5.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p5.3.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p5.3.m3.1c">R</annotation></semantics></math> is then multiplied by <math id="S3.SS1.SSS2.p5.4.m4.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.SSS2.p5.4.m4.1a"><mi id="S3.SS1.SSS2.p5.4.m4.1.1" xref="S3.SS1.SSS2.p5.4.m4.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p5.4.m4.1b"><ci id="S3.SS1.SSS2.p5.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p5.4.m4.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p5.4.m4.1c">Z</annotation></semantics></math> to restore the original object size.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS3.4.1.1" class="ltx_text">III-A</span>3 </span>Generating the Metric Scale Mesh</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.2" class="ltx_p">The final stage of our MSOS branch is to convert the predicted normalized mesh from the mesh header to metric scale mesh, using object center <math id="S3.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><mi id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.1b"><ci id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.1c">Z</annotation></semantics></math> and object radius <math id="S3.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><mi id="S3.SS1.SSS3.p1.2.m2.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.2.m2.1b"><ci id="S3.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.2.m2.1c">R</annotation></semantics></math> estimation values from the Z header, as in,</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="M_{\text{metric}}=f_{\text{scale}}(K,M_{\text{norm}},Z,R)." display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml"><msub id="S3.E2.m1.4.4.1.1.3" xref="S3.E2.m1.4.4.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.3.2" xref="S3.E2.m1.4.4.1.1.3.2.cmml">M</mi><mtext id="S3.E2.m1.4.4.1.1.3.3" xref="S3.E2.m1.4.4.1.1.3.3a.cmml">metric</mtext></msub><mo id="S3.E2.m1.4.4.1.1.2" xref="S3.E2.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.4.4.1.1.1" xref="S3.E2.m1.4.4.1.1.1.cmml"><msub id="S3.E2.m1.4.4.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.1.3.2.cmml">f</mi><mtext id="S3.E2.m1.4.4.1.1.1.3.3" xref="S3.E2.m1.4.4.1.1.1.3.3a.cmml">scale</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">K</mi><mo id="S3.E2.m1.4.4.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">,</mo><msub id="S3.E2.m1.4.4.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.cmml">M</mi><mtext id="S3.E2.m1.4.4.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3a.cmml">norm</mtext></msub><mo id="S3.E2.m1.4.4.1.1.1.1.1.4" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">Z</mi><mo id="S3.E2.m1.4.4.1.1.1.1.1.5" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">R</mi><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.1.1.6" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1"><eq id="S3.E2.m1.4.4.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2"></eq><apply id="S3.E2.m1.4.4.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2">𝑀</ci><ci id="S3.E2.m1.4.4.1.1.3.3a.cmml" xref="S3.E2.m1.4.4.1.1.3.3"><mtext mathsize="70%" id="S3.E2.m1.4.4.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.3.3">metric</mtext></ci></apply><apply id="S3.E2.m1.4.4.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1"><times id="S3.E2.m1.4.4.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.2"></times><apply id="S3.E2.m1.4.4.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.3.2">𝑓</ci><ci id="S3.E2.m1.4.4.1.1.1.3.3a.cmml" xref="S3.E2.m1.4.4.1.1.1.3.3"><mtext mathsize="70%" id="S3.E2.m1.4.4.1.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.1.3.3">scale</mtext></ci></apply><vector id="S3.E2.m1.4.4.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝐾</ci><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2">𝑀</ci><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3">norm</mtext></ci></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑍</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑅</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">M_{\text{metric}}=f_{\text{scale}}(K,M_{\text{norm}},Z,R).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS3.p1.8" class="ltx_p"><math id="S3.SS1.SSS3.p1.3.m1.1" class="ltx_Math" alttext="f_{\text{scale}}" display="inline"><semantics id="S3.SS1.SSS3.p1.3.m1.1a"><msub id="S3.SS1.SSS3.p1.3.m1.1.1" xref="S3.SS1.SSS3.p1.3.m1.1.1.cmml"><mi id="S3.SS1.SSS3.p1.3.m1.1.1.2" xref="S3.SS1.SSS3.p1.3.m1.1.1.2.cmml">f</mi><mtext id="S3.SS1.SSS3.p1.3.m1.1.1.3" xref="S3.SS1.SSS3.p1.3.m1.1.1.3a.cmml">scale</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.3.m1.1b"><apply id="S3.SS1.SSS3.p1.3.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.3.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p1.3.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p1.3.m1.1.1.2.cmml" xref="S3.SS1.SSS3.p1.3.m1.1.1.2">𝑓</ci><ci id="S3.SS1.SSS3.p1.3.m1.1.1.3a.cmml" xref="S3.SS1.SSS3.p1.3.m1.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS3.p1.3.m1.1.1.3.cmml" xref="S3.SS1.SSS3.p1.3.m1.1.1.3">scale</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.3.m1.1c">f_{\text{scale}}</annotation></semantics></math> transforms the normalized mesh to camera coordinate systems to make metric scale objects with the camera intrinsic <math id="S3.SS1.SSS3.p1.4.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.SSS3.p1.4.m2.1a"><mi id="S3.SS1.SSS3.p1.4.m2.1.1" xref="S3.SS1.SSS3.p1.4.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.4.m2.1b"><ci id="S3.SS1.SSS3.p1.4.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.4.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.4.m2.1c">K</annotation></semantics></math>.
Fig. <a href="#S1.F3" title="Figure 3 ‣ I INTRODUCTION ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the intuitive process of <math id="S3.SS1.SSS3.p1.5.m3.1" class="ltx_Math" alttext="f_{\text{scale}}" display="inline"><semantics id="S3.SS1.SSS3.p1.5.m3.1a"><msub id="S3.SS1.SSS3.p1.5.m3.1.1" xref="S3.SS1.SSS3.p1.5.m3.1.1.cmml"><mi id="S3.SS1.SSS3.p1.5.m3.1.1.2" xref="S3.SS1.SSS3.p1.5.m3.1.1.2.cmml">f</mi><mtext id="S3.SS1.SSS3.p1.5.m3.1.1.3" xref="S3.SS1.SSS3.p1.5.m3.1.1.3a.cmml">scale</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.5.m3.1b"><apply id="S3.SS1.SSS3.p1.5.m3.1.1.cmml" xref="S3.SS1.SSS3.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.5.m3.1.1.1.cmml" xref="S3.SS1.SSS3.p1.5.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p1.5.m3.1.1.2.cmml" xref="S3.SS1.SSS3.p1.5.m3.1.1.2">𝑓</ci><ci id="S3.SS1.SSS3.p1.5.m3.1.1.3a.cmml" xref="S3.SS1.SSS3.p1.5.m3.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS3.p1.5.m3.1.1.3.cmml" xref="S3.SS1.SSS3.p1.5.m3.1.1.3">scale</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.5.m3.1c">f_{\text{scale}}</annotation></semantics></math> and how it lifts the normalized object shape to metric scale object shape from a detected image patch.
Using the original 2D bounding box, the predicted <math id="S3.SS1.SSS3.p1.6.m4.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.SSS3.p1.6.m4.1a"><mi id="S3.SS1.SSS3.p1.6.m4.1.1" xref="S3.SS1.SSS3.p1.6.m4.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.6.m4.1b"><ci id="S3.SS1.SSS3.p1.6.m4.1.1.cmml" xref="S3.SS1.SSS3.p1.6.m4.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.6.m4.1c">Z</annotation></semantics></math> and <math id="S3.SS1.SSS3.p1.7.m5.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS3.p1.7.m5.1a"><mi id="S3.SS1.SSS3.p1.7.m5.1.1" xref="S3.SS1.SSS3.p1.7.m5.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.7.m5.1b"><ci id="S3.SS1.SSS3.p1.7.m5.1.1.cmml" xref="S3.SS1.SSS3.p1.7.m5.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.7.m5.1c">R</annotation></semantics></math>, <math id="S3.SS1.SSS3.p1.8.m6.1" class="ltx_Math" alttext="f_{\text{scale}}" display="inline"><semantics id="S3.SS1.SSS3.p1.8.m6.1a"><msub id="S3.SS1.SSS3.p1.8.m6.1.1" xref="S3.SS1.SSS3.p1.8.m6.1.1.cmml"><mi id="S3.SS1.SSS3.p1.8.m6.1.1.2" xref="S3.SS1.SSS3.p1.8.m6.1.1.2.cmml">f</mi><mtext id="S3.SS1.SSS3.p1.8.m6.1.1.3" xref="S3.SS1.SSS3.p1.8.m6.1.1.3a.cmml">scale</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.8.m6.1b"><apply id="S3.SS1.SSS3.p1.8.m6.1.1.cmml" xref="S3.SS1.SSS3.p1.8.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.8.m6.1.1.1.cmml" xref="S3.SS1.SSS3.p1.8.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p1.8.m6.1.1.2.cmml" xref="S3.SS1.SSS3.p1.8.m6.1.1.2">𝑓</ci><ci id="S3.SS1.SSS3.p1.8.m6.1.1.3a.cmml" xref="S3.SS1.SSS3.p1.8.m6.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS3.p1.8.m6.1.1.3.cmml" xref="S3.SS1.SSS3.p1.8.m6.1.1.3">scale</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.8.m6.1c">f_{\text{scale}}</annotation></semantics></math> obtains a 3D bounding box on the camera coordinate.
It then optimizes the normalized mesh into metric scale using keypoint correspondences between the normalized 3D bounding box and the estimated 3D bounding box.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2109.00326/assets/x6.png" id="S3.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S3.F6.3.1" class="ltx_text ltx_font_bold">Illustration of the depth rendering and refinement process.</span>
The MSOS branch can additionally get a rendered depth map by projecting the metric scale mesh to the image plane.
The rendered depth map and metric scale object mesh improve when very sparse depth is given by refining/re-scaling the object center and radius.
</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Depth Rendering</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">Previous NOCS-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> use the predicted NOCS map and observed depth map to estimate the pose by similarity transformation.
One of the naive approaches to replace the observed depth information in an RGB setup is to apply monocular depth estimation.
However, we choose to project our metric scale object mesh into the image plane to obtain a rendered depth map.
We believe that our metric scale object shape generates more qualified depth for pose estimation than directly regressing the depths.
We use the DIB-R renderer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, camera intrinsic <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">K</annotation></semantics></math>, and our predicted metric mesh <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="M_{\text{metric}}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">M</mi><mtext id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3a.cmml">metric</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑀</ci><ci id="S3.SS2.p1.2.m2.1.1.3a.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">metric</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">M_{\text{metric}}</annotation></semantics></math> for our depth rendering.
The rendered depth map is denoted as <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="D_{r}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">D</mi><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝐷</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">D_{r}</annotation></semantics></math>.
Directly regressing the depth information for each pixel makes it difficult to consider the local surface connection and overall 3D structure.
Our depth, rendered through the metric scale object surface, makes it possible to estimate the dense depth with each pixel relation using faces.
We compared the metric scale based depth and the direct regression depth in an ablation study (Sec. <a href="#S4.SS1" title="IV-A Ablation study ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.11" class="ltx_p">Inspired by Cartman, winner of the 2017 Amazon robotics challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, we considered a setup where very sparse depth information is given in the depth rendering process.
Note that our method is RGB-based, but can refine the rendered depth map using sparse depth input without additional training.
When the additional sparse depth map is given, we can correct the initially predicted <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">Z</annotation></semantics></math> and <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">R</annotation></semantics></math> to <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\tilde{Z}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mover accent="true" id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">Z</mi><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><ci id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1">~</ci><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝑍</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\tilde{Z}</annotation></semantics></math> and <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\tilde{R}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mover accent="true" id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">R</mi><mo id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><ci id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1">~</ci><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\tilde{R}</annotation></semantics></math>, using the difference between the observed sparse depth and the depth value of the matching pixel from our rendered depth map <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="D_{r}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><msub id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">D</mi><mi id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝐷</ci><ci id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">D_{r}</annotation></semantics></math>.
Then, we regenerate the metric scale object mesh <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="\tilde{M}_{\text{metric}}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mover accent="true" id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2.2" xref="S3.SS2.p2.6.m6.1.1.2.2.cmml">M</mi><mo id="S3.SS2.p2.6.m6.1.1.2.1" xref="S3.SS2.p2.6.m6.1.1.2.1.cmml">~</mo></mover><mtext id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3a.cmml">metric</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><apply id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2"><ci id="S3.SS2.p2.6.m6.1.1.2.1.cmml" xref="S3.SS2.p2.6.m6.1.1.2.1">~</ci><ci id="S3.SS2.p2.6.m6.1.1.2.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2.2">𝑀</ci></apply><ci id="S3.SS2.p2.6.m6.1.1.3a.cmml" xref="S3.SS2.p2.6.m6.1.1.3"><mtext mathsize="70%" id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">metric</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">\tilde{M}_{\text{metric}}</annotation></semantics></math> with the normalized mesh <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="M_{\text{norm}}" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><msub id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">M</mi><mtext id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3a.cmml">norm</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2">𝑀</ci><ci id="S3.SS2.p2.7.m7.1.1.3a.cmml" xref="S3.SS2.p2.7.m7.1.1.3"><mtext mathsize="70%" id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">norm</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">M_{\text{norm}}</annotation></semantics></math>, <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="\tilde{Z}" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mover accent="true" id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml"><mi id="S3.SS2.p2.8.m8.1.1.2" xref="S3.SS2.p2.8.m8.1.1.2.cmml">Z</mi><mo id="S3.SS2.p2.8.m8.1.1.1" xref="S3.SS2.p2.8.m8.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><apply id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1"><ci id="S3.SS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1.1">~</ci><ci id="S3.SS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2">𝑍</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">\tilde{Z}</annotation></semantics></math>, and <math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="\tilde{R}" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><mover accent="true" id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml"><mi id="S3.SS2.p2.9.m9.1.1.2" xref="S3.SS2.p2.9.m9.1.1.2.cmml">R</mi><mo id="S3.SS2.p2.9.m9.1.1.1" xref="S3.SS2.p2.9.m9.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><apply id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1"><ci id="S3.SS2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1.1">~</ci><ci id="S3.SS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.p2.9.m9.1.1.2">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">\tilde{R}</annotation></semantics></math>.
Using <math id="S3.SS2.p2.10.m10.1" class="ltx_Math" alttext="\tilde{M}_{\text{metric}}" display="inline"><semantics id="S3.SS2.p2.10.m10.1a"><msub id="S3.SS2.p2.10.m10.1.1" xref="S3.SS2.p2.10.m10.1.1.cmml"><mover accent="true" id="S3.SS2.p2.10.m10.1.1.2" xref="S3.SS2.p2.10.m10.1.1.2.cmml"><mi id="S3.SS2.p2.10.m10.1.1.2.2" xref="S3.SS2.p2.10.m10.1.1.2.2.cmml">M</mi><mo id="S3.SS2.p2.10.m10.1.1.2.1" xref="S3.SS2.p2.10.m10.1.1.2.1.cmml">~</mo></mover><mtext id="S3.SS2.p2.10.m10.1.1.3" xref="S3.SS2.p2.10.m10.1.1.3a.cmml">metric</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m10.1b"><apply id="S3.SS2.p2.10.m10.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.10.m10.1.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1">subscript</csymbol><apply id="S3.SS2.p2.10.m10.1.1.2.cmml" xref="S3.SS2.p2.10.m10.1.1.2"><ci id="S3.SS2.p2.10.m10.1.1.2.1.cmml" xref="S3.SS2.p2.10.m10.1.1.2.1">~</ci><ci id="S3.SS2.p2.10.m10.1.1.2.2.cmml" xref="S3.SS2.p2.10.m10.1.1.2.2">𝑀</ci></apply><ci id="S3.SS2.p2.10.m10.1.1.3a.cmml" xref="S3.SS2.p2.10.m10.1.1.3"><mtext mathsize="70%" id="S3.SS2.p2.10.m10.1.1.3.cmml" xref="S3.SS2.p2.10.m10.1.1.3">metric</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m10.1c">\tilde{M}_{\text{metric}}</annotation></semantics></math> and the DIB-R renderer mentioned before, we get the refined/re-scaled rendered depth map <math id="S3.SS2.p2.11.m11.1" class="ltx_Math" alttext="\tilde{D}_{r}" display="inline"><semantics id="S3.SS2.p2.11.m11.1a"><msub id="S3.SS2.p2.11.m11.1.1" xref="S3.SS2.p2.11.m11.1.1.cmml"><mover accent="true" id="S3.SS2.p2.11.m11.1.1.2" xref="S3.SS2.p2.11.m11.1.1.2.cmml"><mi id="S3.SS2.p2.11.m11.1.1.2.2" xref="S3.SS2.p2.11.m11.1.1.2.2.cmml">D</mi><mo id="S3.SS2.p2.11.m11.1.1.2.1" xref="S3.SS2.p2.11.m11.1.1.2.1.cmml">~</mo></mover><mi id="S3.SS2.p2.11.m11.1.1.3" xref="S3.SS2.p2.11.m11.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m11.1b"><apply id="S3.SS2.p2.11.m11.1.1.cmml" xref="S3.SS2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.11.m11.1.1.1.cmml" xref="S3.SS2.p2.11.m11.1.1">subscript</csymbol><apply id="S3.SS2.p2.11.m11.1.1.2.cmml" xref="S3.SS2.p2.11.m11.1.1.2"><ci id="S3.SS2.p2.11.m11.1.1.2.1.cmml" xref="S3.SS2.p2.11.m11.1.1.2.1">~</ci><ci id="S3.SS2.p2.11.m11.1.1.2.2.cmml" xref="S3.SS2.p2.11.m11.1.1.2.2">𝐷</ci></apply><ci id="S3.SS2.p2.11.m11.1.1.3.cmml" xref="S3.SS2.p2.11.m11.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m11.1c">\tilde{D}_{r}</annotation></semantics></math>.
Specifically, we experimented on a setup where we used the depth value of only one pixel per object. We represent this setup as RGB-OD.
Fig. <a href="#S3.F6" title="Figure 6 ‣ III-A3 Generating the Metric Scale Mesh ‣ III-A Metric Scale Object Shape branch (MSOS) ‣ III Category-Level Metric Scale Object Shape and Pose Estimation ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the modifying process.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Pose Estimation</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.4" class="ltx_p">In the category-level pose estimation, Wang <em id="S3.SS3.p1.4.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS3.p1.4.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and Tian <em id="S3.SS3.p1.4.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS3.p1.4.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> showed effective pose and size estimation using a normalized object coordinate space (NOCS) map.
The NOCS has the advantage of representing different object instances in the same category to the unified object coordinate space.
Existing NOCS based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> estimate the similarity transformation parameters (rotation, translation, and size) by matching the pixel location of observed depth <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="D_{o}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝐷</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">D_{o}</annotation></semantics></math> and the predicted NOCS map.
Compared to these approaches, we used the depth map <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="D_{r}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">D</mi><mi id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝐷</ci><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">D_{r}</annotation></semantics></math> rendered from our predicted metric scale mesh in the MSOS branch.
Given the rendered depth <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="D_{r}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><msub id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">D</mi><mi id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">𝐷</ci><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">D_{r}</annotation></semantics></math> and NOCS <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="N_{c}" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><msub id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml">N</mi><mi id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2">𝑁</ci><ci id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">N_{c}</annotation></semantics></math>, the object rotation, translation and size are estimated via the Umeyama algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and RANSAC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Loss</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.6" class="ltx_p">To train our framework, we introduced our loss functions in the MSOS branch.

The MSOS consists of two main headers: the mesh header and Z-header.
To train the mesh header, we adopted voxel loss <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\zeta_{\text{voxel}}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">ζ</mi><mtext id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3a.cmml">voxel</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝜁</ci><ci id="S3.SS4.p1.1.m1.1.1.3a.cmml" xref="S3.SS4.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">voxel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\zeta_{\text{voxel}}</annotation></semantics></math> and normalized mesh loss <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="\zeta_{M_{\text{norm}}}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><msub id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">ζ</mi><msub id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml"><mi id="S3.SS4.p1.2.m2.1.1.3.2" xref="S3.SS4.p1.2.m2.1.1.3.2.cmml">M</mi><mtext id="S3.SS4.p1.2.m2.1.1.3.3" xref="S3.SS4.p1.2.m2.1.1.3.3a.cmml">norm</mtext></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">𝜁</ci><apply id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.3.1.cmml" xref="S3.SS4.p1.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.3.2.cmml" xref="S3.SS4.p1.2.m2.1.1.3.2">𝑀</ci><ci id="S3.SS4.p1.2.m2.1.1.3.3a.cmml" xref="S3.SS4.p1.2.m2.1.1.3.3"><mtext mathsize="50%" id="S3.SS4.p1.2.m2.1.1.3.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3.3">norm</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\zeta_{M_{\text{norm}}}</annotation></semantics></math> from Mesh R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, where voxel loss computes the binary cross entropy (BCE), measured on a quantized voxel grid, and the normalized mesh loss is a combination of the chamfer distance between the predicted and ground truth point clouds, the inner product between the predicted and ground truth surface normal of the mesh, and the shape-regularizing edge loss.
For Z header, we applied radius loss <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="\zeta_{R}" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><msub id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml">ζ</mi><mi id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">𝜁</ci><ci id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\zeta_{R}</annotation></semantics></math> and our proposed normalized object center estimation (NOCE) loss <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="\zeta_{Z_{\text{NOCE}}}" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">ζ</mi><msub id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml"><mi id="S3.SS4.p1.4.m4.1.1.3.2" xref="S3.SS4.p1.4.m4.1.1.3.2.cmml">Z</mi><mtext id="S3.SS4.p1.4.m4.1.1.3.3" xref="S3.SS4.p1.4.m4.1.1.3.3a.cmml">NOCE</mtext></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">𝜁</ci><apply id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.3.1.cmml" xref="S3.SS4.p1.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.3.2.cmml" xref="S3.SS4.p1.4.m4.1.1.3.2">𝑍</ci><ci id="S3.SS4.p1.4.m4.1.1.3.3a.cmml" xref="S3.SS4.p1.4.m4.1.1.3.3"><mtext mathsize="50%" id="S3.SS4.p1.4.m4.1.1.3.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3.3">NOCE</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\zeta_{Z_{\text{NOCE}}}</annotation></semantics></math>, which are simple <math id="S3.SS4.p1.5.m5.1" class="ltx_Math" alttext="smooth" display="inline"><semantics id="S3.SS4.p1.5.m5.1a"><mrow id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml"><mi id="S3.SS4.p1.5.m5.1.1.2" xref="S3.SS4.p1.5.m5.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.5.m5.1.1.1" xref="S3.SS4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.5.m5.1.1.3" xref="S3.SS4.p1.5.m5.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.5.m5.1.1.1a" xref="S3.SS4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.5.m5.1.1.4" xref="S3.SS4.p1.5.m5.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.5.m5.1.1.1b" xref="S3.SS4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.5.m5.1.1.5" xref="S3.SS4.p1.5.m5.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.5.m5.1.1.1c" xref="S3.SS4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.5.m5.1.1.6" xref="S3.SS4.p1.5.m5.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.5.m5.1.1.1d" xref="S3.SS4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.5.m5.1.1.7" xref="S3.SS4.p1.5.m5.1.1.7.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1"><times id="S3.SS4.p1.5.m5.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1.1"></times><ci id="S3.SS4.p1.5.m5.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2">𝑠</ci><ci id="S3.SS4.p1.5.m5.1.1.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3">𝑚</ci><ci id="S3.SS4.p1.5.m5.1.1.4.cmml" xref="S3.SS4.p1.5.m5.1.1.4">𝑜</ci><ci id="S3.SS4.p1.5.m5.1.1.5.cmml" xref="S3.SS4.p1.5.m5.1.1.5">𝑜</ci><ci id="S3.SS4.p1.5.m5.1.1.6.cmml" xref="S3.SS4.p1.5.m5.1.1.6">𝑡</ci><ci id="S3.SS4.p1.5.m5.1.1.7.cmml" xref="S3.SS4.p1.5.m5.1.1.7">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">smooth</annotation></semantics></math> <math id="S3.SS4.p1.6.m6.1" class="ltx_Math" alttext="l_{1}" display="inline"><semantics id="S3.SS4.p1.6.m6.1a"><msub id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml"><mi id="S3.SS4.p1.6.m6.1.1.2" xref="S3.SS4.p1.6.m6.1.1.2.cmml">l</mi><mn id="S3.SS4.p1.6.m6.1.1.3" xref="S3.SS4.p1.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><apply id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p1.6.m6.1.1.2.cmml" xref="S3.SS4.p1.6.m6.1.1.2">𝑙</ci><cn type="integer" id="S3.SS4.p1.6.m6.1.1.3.cmml" xref="S3.SS4.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">l_{1}</annotation></semantics></math> losses.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.5" class="ltx_p">Finally, we defined the MSOS branch loss functions:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.25" class="ltx_Math" alttext="\begin{split}\zeta_{\text{MSOS}}=\lambda_{\text{voxel}}\zeta_{\text{voxel}}+\lambda_{M_{\text{norm}}}\zeta_{M_{\text{norm}}}\\
+\lambda_{R}\zeta_{R}+\lambda_{Z_{\text{NOCE}}}\zeta_{Z_{\text{NOCE}}},\end{split}" display="block"><semantics id="S3.E3.m1.25a"><mtable displaystyle="true" rowspacing="0pt" id="S3.E3.m1.25.25.2"><mtr id="S3.E3.m1.25.25.2a"><mtd class="ltx_align_right" columnalign="right" id="S3.E3.m1.25.25.2b"><mrow id="S3.E3.m1.12.12.12.12.12"><msub id="S3.E3.m1.12.12.12.12.12.13"><mi id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml">ζ</mi><mtext id="S3.E3.m1.2.2.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.2.2.1a.cmml">MSOS</mtext></msub><mo id="S3.E3.m1.3.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S3.E3.m1.12.12.12.12.12.14"><mrow id="S3.E3.m1.12.12.12.12.12.14.1"><msub id="S3.E3.m1.12.12.12.12.12.14.1.2"><mi id="S3.E3.m1.4.4.4.4.4.4" xref="S3.E3.m1.4.4.4.4.4.4.cmml">λ</mi><mtext id="S3.E3.m1.5.5.5.5.5.5.1" xref="S3.E3.m1.5.5.5.5.5.5.1a.cmml">voxel</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.12.12.12.12.12.14.1.1" xref="S3.E3.m1.24.24.1.1.1.cmml">​</mo><msub id="S3.E3.m1.12.12.12.12.12.14.1.3"><mi id="S3.E3.m1.6.6.6.6.6.6" xref="S3.E3.m1.6.6.6.6.6.6.cmml">ζ</mi><mtext id="S3.E3.m1.7.7.7.7.7.7.1" xref="S3.E3.m1.7.7.7.7.7.7.1a.cmml">voxel</mtext></msub></mrow><mo id="S3.E3.m1.8.8.8.8.8.8" xref="S3.E3.m1.8.8.8.8.8.8.cmml">+</mo><mrow id="S3.E3.m1.12.12.12.12.12.14.2"><msub id="S3.E3.m1.12.12.12.12.12.14.2.2"><mi id="S3.E3.m1.9.9.9.9.9.9" xref="S3.E3.m1.9.9.9.9.9.9.cmml">λ</mi><msub id="S3.E3.m1.10.10.10.10.10.10.1" xref="S3.E3.m1.10.10.10.10.10.10.1.cmml"><mi id="S3.E3.m1.10.10.10.10.10.10.1.2" xref="S3.E3.m1.10.10.10.10.10.10.1.2.cmml">M</mi><mtext id="S3.E3.m1.10.10.10.10.10.10.1.3" xref="S3.E3.m1.10.10.10.10.10.10.1.3a.cmml">norm</mtext></msub></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.12.12.12.12.12.14.2.1" xref="S3.E3.m1.24.24.1.1.1.cmml">​</mo><msub id="S3.E3.m1.12.12.12.12.12.14.2.3"><mi id="S3.E3.m1.11.11.11.11.11.11" xref="S3.E3.m1.11.11.11.11.11.11.cmml">ζ</mi><msub id="S3.E3.m1.12.12.12.12.12.12.1" xref="S3.E3.m1.12.12.12.12.12.12.1.cmml"><mi id="S3.E3.m1.12.12.12.12.12.12.1.2" xref="S3.E3.m1.12.12.12.12.12.12.1.2.cmml">M</mi><mtext id="S3.E3.m1.12.12.12.12.12.12.1.3" xref="S3.E3.m1.12.12.12.12.12.12.1.3a.cmml">norm</mtext></msub></msub></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E3.m1.25.25.2c"><mtd class="ltx_align_right" columnalign="right" id="S3.E3.m1.25.25.2d"><mrow id="S3.E3.m1.25.25.2.24.12.12.12"><mrow id="S3.E3.m1.25.25.2.24.12.12.12.1"><mrow id="S3.E3.m1.25.25.2.24.12.12.12.1.1"><mo id="S3.E3.m1.25.25.2.24.12.12.12.1.1a" xref="S3.E3.m1.24.24.1.1.1.cmml">+</mo><mrow id="S3.E3.m1.25.25.2.24.12.12.12.1.1.1"><msub id="S3.E3.m1.25.25.2.24.12.12.12.1.1.1.2"><mi id="S3.E3.m1.14.14.14.2.2.2" xref="S3.E3.m1.14.14.14.2.2.2.cmml">λ</mi><mi id="S3.E3.m1.15.15.15.3.3.3.1" xref="S3.E3.m1.15.15.15.3.3.3.1.cmml">R</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.25.25.2.24.12.12.12.1.1.1.1" xref="S3.E3.m1.24.24.1.1.1.cmml">​</mo><msub id="S3.E3.m1.25.25.2.24.12.12.12.1.1.1.3"><mi id="S3.E3.m1.16.16.16.4.4.4" xref="S3.E3.m1.16.16.16.4.4.4.cmml">ζ</mi><mi id="S3.E3.m1.17.17.17.5.5.5.1" xref="S3.E3.m1.17.17.17.5.5.5.1.cmml">R</mi></msub></mrow></mrow><mo id="S3.E3.m1.18.18.18.6.6.6" xref="S3.E3.m1.24.24.1.1.1.cmml">+</mo><mrow id="S3.E3.m1.25.25.2.24.12.12.12.1.2"><msub id="S3.E3.m1.25.25.2.24.12.12.12.1.2.2"><mi id="S3.E3.m1.19.19.19.7.7.7" xref="S3.E3.m1.19.19.19.7.7.7.cmml">λ</mi><msub id="S3.E3.m1.20.20.20.8.8.8.1" xref="S3.E3.m1.20.20.20.8.8.8.1.cmml"><mi id="S3.E3.m1.20.20.20.8.8.8.1.2" xref="S3.E3.m1.20.20.20.8.8.8.1.2.cmml">Z</mi><mtext id="S3.E3.m1.20.20.20.8.8.8.1.3" xref="S3.E3.m1.20.20.20.8.8.8.1.3a.cmml">NOCE</mtext></msub></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.25.25.2.24.12.12.12.1.2.1" xref="S3.E3.m1.24.24.1.1.1.cmml">​</mo><msub id="S3.E3.m1.25.25.2.24.12.12.12.1.2.3"><mi id="S3.E3.m1.21.21.21.9.9.9" xref="S3.E3.m1.21.21.21.9.9.9.cmml">ζ</mi><msub id="S3.E3.m1.22.22.22.10.10.10.1" xref="S3.E3.m1.22.22.22.10.10.10.1.cmml"><mi id="S3.E3.m1.22.22.22.10.10.10.1.2" xref="S3.E3.m1.22.22.22.10.10.10.1.2.cmml">Z</mi><mtext id="S3.E3.m1.22.22.22.10.10.10.1.3" xref="S3.E3.m1.22.22.22.10.10.10.1.3a.cmml">NOCE</mtext></msub></msub></mrow></mrow><mo id="S3.E3.m1.23.23.23.11.11.11" xref="S3.E3.m1.24.24.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E3.m1.25b"><apply id="S3.E3.m1.24.24.1.1.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><eq id="S3.E3.m1.3.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3.3"></eq><apply id="S3.E3.m1.24.24.1.1.1.2.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.24.24.1.1.1.2.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">𝜁</ci><ci id="S3.E3.m1.2.2.2.2.2.2.1a.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1"><mtext mathsize="70%" id="S3.E3.m1.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1">MSOS</mtext></ci></apply><apply id="S3.E3.m1.24.24.1.1.1.3.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><plus id="S3.E3.m1.8.8.8.8.8.8.cmml" xref="S3.E3.m1.8.8.8.8.8.8"></plus><apply id="S3.E3.m1.24.24.1.1.1.3.2.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><times id="S3.E3.m1.24.24.1.1.1.3.2.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"></times><apply id="S3.E3.m1.24.24.1.1.1.3.2.2.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.24.24.1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1">subscript</csymbol><ci id="S3.E3.m1.4.4.4.4.4.4.cmml" xref="S3.E3.m1.4.4.4.4.4.4">𝜆</ci><ci id="S3.E3.m1.5.5.5.5.5.5.1a.cmml" xref="S3.E3.m1.5.5.5.5.5.5.1"><mtext mathsize="70%" id="S3.E3.m1.5.5.5.5.5.5.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5.1">voxel</mtext></ci></apply><apply id="S3.E3.m1.24.24.1.1.1.3.2.3.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.24.24.1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1">subscript</csymbol><ci id="S3.E3.m1.6.6.6.6.6.6.cmml" xref="S3.E3.m1.6.6.6.6.6.6">𝜁</ci><ci id="S3.E3.m1.7.7.7.7.7.7.1a.cmml" xref="S3.E3.m1.7.7.7.7.7.7.1"><mtext mathsize="70%" id="S3.E3.m1.7.7.7.7.7.7.1.cmml" xref="S3.E3.m1.7.7.7.7.7.7.1">voxel</mtext></ci></apply></apply><apply id="S3.E3.m1.24.24.1.1.1.3.3.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><times id="S3.E3.m1.24.24.1.1.1.3.3.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"></times><apply id="S3.E3.m1.24.24.1.1.1.3.3.2.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.24.24.1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1">subscript</csymbol><ci id="S3.E3.m1.9.9.9.9.9.9.cmml" xref="S3.E3.m1.9.9.9.9.9.9">𝜆</ci><apply id="S3.E3.m1.10.10.10.10.10.10.1.cmml" xref="S3.E3.m1.10.10.10.10.10.10.1"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.10.10.10.10.1.1.cmml" xref="S3.E3.m1.10.10.10.10.10.10.1">subscript</csymbol><ci id="S3.E3.m1.10.10.10.10.10.10.1.2.cmml" xref="S3.E3.m1.10.10.10.10.10.10.1.2">𝑀</ci><ci id="S3.E3.m1.10.10.10.10.10.10.1.3a.cmml" xref="S3.E3.m1.10.10.10.10.10.10.1.3"><mtext mathsize="50%" id="S3.E3.m1.10.10.10.10.10.10.1.3.cmml" xref="S3.E3.m1.10.10.10.10.10.10.1.3">norm</mtext></ci></apply></apply><apply id="S3.E3.m1.24.24.1.1.1.3.3.3.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.24.24.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1">subscript</csymbol><ci id="S3.E3.m1.11.11.11.11.11.11.cmml" xref="S3.E3.m1.11.11.11.11.11.11">𝜁</ci><apply id="S3.E3.m1.12.12.12.12.12.12.1.cmml" xref="S3.E3.m1.12.12.12.12.12.12.1"><csymbol cd="ambiguous" id="S3.E3.m1.12.12.12.12.12.12.1.1.cmml" xref="S3.E3.m1.12.12.12.12.12.12.1">subscript</csymbol><ci id="S3.E3.m1.12.12.12.12.12.12.1.2.cmml" xref="S3.E3.m1.12.12.12.12.12.12.1.2">𝑀</ci><ci id="S3.E3.m1.12.12.12.12.12.12.1.3a.cmml" xref="S3.E3.m1.12.12.12.12.12.12.1.3"><mtext mathsize="50%" id="S3.E3.m1.12.12.12.12.12.12.1.3.cmml" xref="S3.E3.m1.12.12.12.12.12.12.1.3">norm</mtext></ci></apply></apply></apply><apply id="S3.E3.m1.24.24.1.1.1.3.4.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><times id="S3.E3.m1.24.24.1.1.1.3.4.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"></times><apply id="S3.E3.m1.24.24.1.1.1.3.4.2.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.24.24.1.1.1.3.4.2.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1">subscript</csymbol><ci id="S3.E3.m1.14.14.14.2.2.2.cmml" xref="S3.E3.m1.14.14.14.2.2.2">𝜆</ci><ci id="S3.E3.m1.15.15.15.3.3.3.1.cmml" xref="S3.E3.m1.15.15.15.3.3.3.1">𝑅</ci></apply><apply id="S3.E3.m1.24.24.1.1.1.3.4.3.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.24.24.1.1.1.3.4.3.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1">subscript</csymbol><ci id="S3.E3.m1.16.16.16.4.4.4.cmml" xref="S3.E3.m1.16.16.16.4.4.4">𝜁</ci><ci id="S3.E3.m1.17.17.17.5.5.5.1.cmml" xref="S3.E3.m1.17.17.17.5.5.5.1">𝑅</ci></apply></apply><apply id="S3.E3.m1.24.24.1.1.1.3.5.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><times id="S3.E3.m1.24.24.1.1.1.3.5.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"></times><apply id="S3.E3.m1.24.24.1.1.1.3.5.2.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.24.24.1.1.1.3.5.2.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1">subscript</csymbol><ci id="S3.E3.m1.19.19.19.7.7.7.cmml" xref="S3.E3.m1.19.19.19.7.7.7">𝜆</ci><apply id="S3.E3.m1.20.20.20.8.8.8.1.cmml" xref="S3.E3.m1.20.20.20.8.8.8.1"><csymbol cd="ambiguous" id="S3.E3.m1.20.20.20.8.8.8.1.1.cmml" xref="S3.E3.m1.20.20.20.8.8.8.1">subscript</csymbol><ci id="S3.E3.m1.20.20.20.8.8.8.1.2.cmml" xref="S3.E3.m1.20.20.20.8.8.8.1.2">𝑍</ci><ci id="S3.E3.m1.20.20.20.8.8.8.1.3a.cmml" xref="S3.E3.m1.20.20.20.8.8.8.1.3"><mtext mathsize="50%" id="S3.E3.m1.20.20.20.8.8.8.1.3.cmml" xref="S3.E3.m1.20.20.20.8.8.8.1.3">NOCE</mtext></ci></apply></apply><apply id="S3.E3.m1.24.24.1.1.1.3.5.3.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.24.24.1.1.1.3.5.3.1.cmml" xref="S3.E3.m1.12.12.12.12.12.14.1.1">subscript</csymbol><ci id="S3.E3.m1.21.21.21.9.9.9.cmml" xref="S3.E3.m1.21.21.21.9.9.9">𝜁</ci><apply id="S3.E3.m1.22.22.22.10.10.10.1.cmml" xref="S3.E3.m1.22.22.22.10.10.10.1"><csymbol cd="ambiguous" id="S3.E3.m1.22.22.22.10.10.10.1.1.cmml" xref="S3.E3.m1.22.22.22.10.10.10.1">subscript</csymbol><ci id="S3.E3.m1.22.22.22.10.10.10.1.2.cmml" xref="S3.E3.m1.22.22.22.10.10.10.1.2">𝑍</ci><ci id="S3.E3.m1.22.22.22.10.10.10.1.3a.cmml" xref="S3.E3.m1.22.22.22.10.10.10.1.3"><mtext mathsize="50%" id="S3.E3.m1.22.22.22.10.10.10.1.3.cmml" xref="S3.E3.m1.22.22.22.10.10.10.1.3">NOCE</mtext></ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.25c">\begin{split}\zeta_{\text{MSOS}}=\lambda_{\text{voxel}}\zeta_{\text{voxel}}+\lambda_{M_{\text{norm}}}\zeta_{M_{\text{norm}}}\\
+\lambda_{R}\zeta_{R}+\lambda_{Z_{\text{NOCE}}}\zeta_{Z_{\text{NOCE}}},\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p2.4" class="ltx_p">where <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="\lambda_{\text{voxel}}" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><msub id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">λ</mi><mtext id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3a.cmml">voxel</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">𝜆</ci><ci id="S3.SS4.p2.1.m1.1.1.3a.cmml" xref="S3.SS4.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">voxel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\lambda_{\text{voxel}}</annotation></semantics></math>, <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="\lambda_{M_{\text{norm}}}" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><msub id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">λ</mi><msub id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml"><mi id="S3.SS4.p2.2.m2.1.1.3.2" xref="S3.SS4.p2.2.m2.1.1.3.2.cmml">M</mi><mtext id="S3.SS4.p2.2.m2.1.1.3.3" xref="S3.SS4.p2.2.m2.1.1.3.3a.cmml">norm</mtext></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">𝜆</ci><apply id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.3.1.cmml" xref="S3.SS4.p2.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.3.2.cmml" xref="S3.SS4.p2.2.m2.1.1.3.2">𝑀</ci><ci id="S3.SS4.p2.2.m2.1.1.3.3a.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3"><mtext mathsize="50%" id="S3.SS4.p2.2.m2.1.1.3.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3">norm</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">\lambda_{M_{\text{norm}}}</annotation></semantics></math>, <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="\lambda_{R}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><msub id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">λ</mi><mi id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">𝜆</ci><ci id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">\lambda_{R}</annotation></semantics></math> and <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="\lambda_{Z_{\text{NOCE}}}" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><msub id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">λ</mi><msub id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml"><mi id="S3.SS4.p2.4.m4.1.1.3.2" xref="S3.SS4.p2.4.m4.1.1.3.2.cmml">Z</mi><mtext id="S3.SS4.p2.4.m4.1.1.3.3" xref="S3.SS4.p2.4.m4.1.1.3.3a.cmml">NOCE</mtext></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">𝜆</ci><apply id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.3.1.cmml" xref="S3.SS4.p2.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.3.2.cmml" xref="S3.SS4.p2.4.m4.1.1.3.2">𝑍</ci><ci id="S3.SS4.p2.4.m4.1.1.3.3a.cmml" xref="S3.SS4.p2.4.m4.1.1.3.3"><mtext mathsize="50%" id="S3.SS4.p2.4.m4.1.1.3.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3.3">NOCE</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">\lambda_{Z_{\text{NOCE}}}</annotation></semantics></math> are weighting parameters, which are set empirically.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Datasets.</span>
We evaluated our method on two standard benchmarks in the task of category-level object pose estimation: Context-Aware MixedEd ReAlity (CAMERA) and the REAL dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
The CAMERA dataset is generated by rendering and compositing synthetic objects into real images in a context-aware manner.
It comprises 300k synthetic images, where 25K images are for the evaluation.
The synthetic training set contains 1085 object instances selected from 6 different categories - bottle, bowl, camera, can, laptop and mug.
The evaluation set contains 184 different instances.
The REAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> dataset is complementary to the synthetic data.
It has 43,000 real-world images of 7 scenes for training and 2,750 real-world images of 6 scenes for evaluation.
It contains 42 unique objects with 6 categories. Each evaluation set is noted as CAMERA25 and REAL275.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.6" class="ltx_p"><span id="S4.p2.6.1" class="ltx_text ltx_font_bold">Metrics.</span>
We followed the previous pose and size evaluation metric from the NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, which evaluates the performance of 3D object detection and 6D pose estimation.
We report the average precision at different Intersection-Over-Union (IoU) thresholds for 3D object detection.
Threshold values of 25%, 50%, and 75% were used to evaluate the results.
For 6D object pose evaluation, the average precision was computed at rotation (<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="r^{\circ}" display="inline"><semantics id="S4.p2.1.m1.1a"><msup id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">r</mi><mo id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1">superscript</csymbol><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">𝑟</ci><compose id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">r^{\circ}</annotation></semantics></math>) and translation (<math id="S4.p2.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">t</annotation></semantics></math> cm) errors.
For example, the <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="10^{\circ}" display="inline"><semantics id="S4.p2.3.m3.1a"><msup id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mn id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">10</mn><mo id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">10</cn><compose id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">10^{\circ}</annotation></semantics></math> <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.p2.4.m4.1a"><mn id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><cn type="integer" id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">10</annotation></semantics></math>cm metric denotes the average precision of object instances where the error is less than <math id="S4.p2.5.m5.1" class="ltx_Math" alttext="10^{\circ}" display="inline"><semantics id="S4.p2.5.m5.1a"><msup id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml"><mn id="S4.p2.5.m5.1.1.2" xref="S4.p2.5.m5.1.1.2.cmml">10</mn><mo id="S4.p2.5.m5.1.1.3" xref="S4.p2.5.m5.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><apply id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p2.5.m5.1.1.1.cmml" xref="S4.p2.5.m5.1.1">superscript</csymbol><cn type="integer" id="S4.p2.5.m5.1.1.2.cmml" xref="S4.p2.5.m5.1.1.2">10</cn><compose id="S4.p2.5.m5.1.1.3.cmml" xref="S4.p2.5.m5.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">10^{\circ}</annotation></semantics></math> and <math id="S4.p2.6.m6.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.p2.6.m6.1a"><mn id="S4.p2.6.m6.1.1" xref="S4.p2.6.m6.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><cn type="integer" id="S4.p2.6.m6.1.1.cmml" xref="S4.p2.6.m6.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">10</annotation></semantics></math>cm.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.8" class="ltx_p">We used the chamfer distance (<math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\times 10^{-3}" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">×</mo><msup id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml"><mn id="S4.p3.1.m1.1.1.3.2" xref="S4.p3.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.p3.1.m1.1.1.3.3" xref="S4.p3.1.m1.1.1.3.3.cmml"><mo id="S4.p3.1.m1.1.1.3.3a" xref="S4.p3.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.p3.1.m1.1.1.3.3.2" xref="S4.p3.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><times id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></times><csymbol cd="latexml" id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">absent</csymbol><apply id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.3.1.cmml" xref="S4.p3.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.p3.1.m1.1.1.3.2.cmml" xref="S4.p3.1.m1.1.1.3.2">10</cn><apply id="S4.p3.1.m1.1.1.3.3.cmml" xref="S4.p3.1.m1.1.1.3.3"><minus id="S4.p3.1.m1.1.1.3.3.1.cmml" xref="S4.p3.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.p3.1.m1.1.1.3.3.2.cmml" xref="S4.p3.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\times 10^{-3}</annotation></semantics></math>) and normal consistency from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to evaluate our metric scale object shape.
To evaluate the depth, we adapted the standard evaluation metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
Let <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="\hat{D}_{i}" display="inline"><semantics id="S4.p3.2.m2.1a"><msub id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mover accent="true" id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml"><mi id="S4.p3.2.m2.1.1.2.2" xref="S4.p3.2.m2.1.1.2.2.cmml">D</mi><mo id="S4.p3.2.m2.1.1.2.1" xref="S4.p3.2.m2.1.1.2.1.cmml">^</mo></mover><mi id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1">subscript</csymbol><apply id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2"><ci id="S4.p3.2.m2.1.1.2.1.cmml" xref="S4.p3.2.m2.1.1.2.1">^</ci><ci id="S4.p3.2.m2.1.1.2.2.cmml" xref="S4.p3.2.m2.1.1.2.2">𝐷</ci></apply><ci id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">\hat{D}_{i}</annotation></semantics></math> denotes the ground truth depth at a pixel location <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.p3.3.m3.1a"><mi id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><ci id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">i</annotation></semantics></math> and <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S4.p3.4.m4.1a"><msub id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml"><mi id="S4.p3.4.m4.1.1.2" xref="S4.p3.4.m4.1.1.2.cmml">D</mi><mi id="S4.p3.4.m4.1.1.3" xref="S4.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><apply id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p3.4.m4.1.1.1.cmml" xref="S4.p3.4.m4.1.1">subscript</csymbol><ci id="S4.p3.4.m4.1.1.2.cmml" xref="S4.p3.4.m4.1.1.2">𝐷</ci><ci id="S4.p3.4.m4.1.1.3.cmml" xref="S4.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">D_{i}</annotation></semantics></math> denotes the estimated depth.
The depth evaluation metrics are specified as follows, <math id="S4.p3.5.m5.2" class="ltx_Math" alttext="RMSE:\frac{1}{\left|I\right|}\sum_{i\in{I}}\sqrt{(\hat{D}_{i}-D_{i})^{2}}" display="inline"><semantics id="S4.p3.5.m5.2a"><mrow id="S4.p3.5.m5.2.3" xref="S4.p3.5.m5.2.3.cmml"><mrow id="S4.p3.5.m5.2.3.2" xref="S4.p3.5.m5.2.3.2.cmml"><mi id="S4.p3.5.m5.2.3.2.2" xref="S4.p3.5.m5.2.3.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.2.3.2.1" xref="S4.p3.5.m5.2.3.2.1.cmml">​</mo><mi id="S4.p3.5.m5.2.3.2.3" xref="S4.p3.5.m5.2.3.2.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.2.3.2.1a" xref="S4.p3.5.m5.2.3.2.1.cmml">​</mo><mi id="S4.p3.5.m5.2.3.2.4" xref="S4.p3.5.m5.2.3.2.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.2.3.2.1b" xref="S4.p3.5.m5.2.3.2.1.cmml">​</mo><mi id="S4.p3.5.m5.2.3.2.5" xref="S4.p3.5.m5.2.3.2.5.cmml">E</mi></mrow><mo lspace="0.278em" rspace="0.278em" id="S4.p3.5.m5.2.3.1" xref="S4.p3.5.m5.2.3.1.cmml">:</mo><mrow id="S4.p3.5.m5.2.3.3" xref="S4.p3.5.m5.2.3.3.cmml"><mfrac id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml"><mn id="S4.p3.5.m5.1.1.3" xref="S4.p3.5.m5.1.1.3.cmml">1</mn><mrow id="S4.p3.5.m5.1.1.1.3" xref="S4.p3.5.m5.1.1.1.2.cmml"><mo id="S4.p3.5.m5.1.1.1.3.1" xref="S4.p3.5.m5.1.1.1.2.1.cmml">|</mo><mi id="S4.p3.5.m5.1.1.1.1" xref="S4.p3.5.m5.1.1.1.1.cmml">I</mi><mo id="S4.p3.5.m5.1.1.1.3.2" xref="S4.p3.5.m5.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.2.3.3.1" xref="S4.p3.5.m5.2.3.3.1.cmml">​</mo><mrow id="S4.p3.5.m5.2.3.3.2" xref="S4.p3.5.m5.2.3.3.2.cmml"><msub id="S4.p3.5.m5.2.3.3.2.1" xref="S4.p3.5.m5.2.3.3.2.1.cmml"><mo id="S4.p3.5.m5.2.3.3.2.1.2" xref="S4.p3.5.m5.2.3.3.2.1.2.cmml">∑</mo><mrow id="S4.p3.5.m5.2.3.3.2.1.3" xref="S4.p3.5.m5.2.3.3.2.1.3.cmml"><mi id="S4.p3.5.m5.2.3.3.2.1.3.2" xref="S4.p3.5.m5.2.3.3.2.1.3.2.cmml">i</mi><mo id="S4.p3.5.m5.2.3.3.2.1.3.1" xref="S4.p3.5.m5.2.3.3.2.1.3.1.cmml">∈</mo><mi id="S4.p3.5.m5.2.3.3.2.1.3.3" xref="S4.p3.5.m5.2.3.3.2.1.3.3.cmml">I</mi></mrow></msub><msqrt id="S4.p3.5.m5.2.2" xref="S4.p3.5.m5.2.2.cmml"><msup id="S4.p3.5.m5.2.2.1" xref="S4.p3.5.m5.2.2.1.cmml"><mrow id="S4.p3.5.m5.2.2.1.1.1" xref="S4.p3.5.m5.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.p3.5.m5.2.2.1.1.1.2" xref="S4.p3.5.m5.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.p3.5.m5.2.2.1.1.1.1" xref="S4.p3.5.m5.2.2.1.1.1.1.cmml"><msub id="S4.p3.5.m5.2.2.1.1.1.1.2" xref="S4.p3.5.m5.2.2.1.1.1.1.2.cmml"><mover accent="true" id="S4.p3.5.m5.2.2.1.1.1.1.2.2" xref="S4.p3.5.m5.2.2.1.1.1.1.2.2.cmml"><mi id="S4.p3.5.m5.2.2.1.1.1.1.2.2.2" xref="S4.p3.5.m5.2.2.1.1.1.1.2.2.2.cmml">D</mi><mo id="S4.p3.5.m5.2.2.1.1.1.1.2.2.1" xref="S4.p3.5.m5.2.2.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S4.p3.5.m5.2.2.1.1.1.1.2.3" xref="S4.p3.5.m5.2.2.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.p3.5.m5.2.2.1.1.1.1.1" xref="S4.p3.5.m5.2.2.1.1.1.1.1.cmml">−</mo><msub id="S4.p3.5.m5.2.2.1.1.1.1.3" xref="S4.p3.5.m5.2.2.1.1.1.1.3.cmml"><mi id="S4.p3.5.m5.2.2.1.1.1.1.3.2" xref="S4.p3.5.m5.2.2.1.1.1.1.3.2.cmml">D</mi><mi id="S4.p3.5.m5.2.2.1.1.1.1.3.3" xref="S4.p3.5.m5.2.2.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.p3.5.m5.2.2.1.1.1.3" xref="S4.p3.5.m5.2.2.1.1.1.1.cmml">)</mo></mrow><mn id="S4.p3.5.m5.2.2.1.3" xref="S4.p3.5.m5.2.2.1.3.cmml">2</mn></msup></msqrt></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.2b"><apply id="S4.p3.5.m5.2.3.cmml" xref="S4.p3.5.m5.2.3"><ci id="S4.p3.5.m5.2.3.1.cmml" xref="S4.p3.5.m5.2.3.1">:</ci><apply id="S4.p3.5.m5.2.3.2.cmml" xref="S4.p3.5.m5.2.3.2"><times id="S4.p3.5.m5.2.3.2.1.cmml" xref="S4.p3.5.m5.2.3.2.1"></times><ci id="S4.p3.5.m5.2.3.2.2.cmml" xref="S4.p3.5.m5.2.3.2.2">𝑅</ci><ci id="S4.p3.5.m5.2.3.2.3.cmml" xref="S4.p3.5.m5.2.3.2.3">𝑀</ci><ci id="S4.p3.5.m5.2.3.2.4.cmml" xref="S4.p3.5.m5.2.3.2.4">𝑆</ci><ci id="S4.p3.5.m5.2.3.2.5.cmml" xref="S4.p3.5.m5.2.3.2.5">𝐸</ci></apply><apply id="S4.p3.5.m5.2.3.3.cmml" xref="S4.p3.5.m5.2.3.3"><times id="S4.p3.5.m5.2.3.3.1.cmml" xref="S4.p3.5.m5.2.3.3.1"></times><apply id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1"><divide id="S4.p3.5.m5.1.1.2.cmml" xref="S4.p3.5.m5.1.1"></divide><cn type="integer" id="S4.p3.5.m5.1.1.3.cmml" xref="S4.p3.5.m5.1.1.3">1</cn><apply id="S4.p3.5.m5.1.1.1.2.cmml" xref="S4.p3.5.m5.1.1.1.3"><abs id="S4.p3.5.m5.1.1.1.2.1.cmml" xref="S4.p3.5.m5.1.1.1.3.1"></abs><ci id="S4.p3.5.m5.1.1.1.1.cmml" xref="S4.p3.5.m5.1.1.1.1">𝐼</ci></apply></apply><apply id="S4.p3.5.m5.2.3.3.2.cmml" xref="S4.p3.5.m5.2.3.3.2"><apply id="S4.p3.5.m5.2.3.3.2.1.cmml" xref="S4.p3.5.m5.2.3.3.2.1"><csymbol cd="ambiguous" id="S4.p3.5.m5.2.3.3.2.1.1.cmml" xref="S4.p3.5.m5.2.3.3.2.1">subscript</csymbol><sum id="S4.p3.5.m5.2.3.3.2.1.2.cmml" xref="S4.p3.5.m5.2.3.3.2.1.2"></sum><apply id="S4.p3.5.m5.2.3.3.2.1.3.cmml" xref="S4.p3.5.m5.2.3.3.2.1.3"><in id="S4.p3.5.m5.2.3.3.2.1.3.1.cmml" xref="S4.p3.5.m5.2.3.3.2.1.3.1"></in><ci id="S4.p3.5.m5.2.3.3.2.1.3.2.cmml" xref="S4.p3.5.m5.2.3.3.2.1.3.2">𝑖</ci><ci id="S4.p3.5.m5.2.3.3.2.1.3.3.cmml" xref="S4.p3.5.m5.2.3.3.2.1.3.3">𝐼</ci></apply></apply><apply id="S4.p3.5.m5.2.2.cmml" xref="S4.p3.5.m5.2.2"><root id="S4.p3.5.m5.2.2a.cmml" xref="S4.p3.5.m5.2.2"></root><apply id="S4.p3.5.m5.2.2.1.cmml" xref="S4.p3.5.m5.2.2.1"><csymbol cd="ambiguous" id="S4.p3.5.m5.2.2.1.2.cmml" xref="S4.p3.5.m5.2.2.1">superscript</csymbol><apply id="S4.p3.5.m5.2.2.1.1.1.1.cmml" xref="S4.p3.5.m5.2.2.1.1.1"><minus id="S4.p3.5.m5.2.2.1.1.1.1.1.cmml" xref="S4.p3.5.m5.2.2.1.1.1.1.1"></minus><apply id="S4.p3.5.m5.2.2.1.1.1.1.2.cmml" xref="S4.p3.5.m5.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.p3.5.m5.2.2.1.1.1.1.2.1.cmml" xref="S4.p3.5.m5.2.2.1.1.1.1.2">subscript</csymbol><apply id="S4.p3.5.m5.2.2.1.1.1.1.2.2.cmml" xref="S4.p3.5.m5.2.2.1.1.1.1.2.2"><ci id="S4.p3.5.m5.2.2.1.1.1.1.2.2.1.cmml" xref="S4.p3.5.m5.2.2.1.1.1.1.2.2.1">^</ci><ci id="S4.p3.5.m5.2.2.1.1.1.1.2.2.2.cmml" xref="S4.p3.5.m5.2.2.1.1.1.1.2.2.2">𝐷</ci></apply><ci id="S4.p3.5.m5.2.2.1.1.1.1.2.3.cmml" xref="S4.p3.5.m5.2.2.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.p3.5.m5.2.2.1.1.1.1.3.cmml" xref="S4.p3.5.m5.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p3.5.m5.2.2.1.1.1.1.3.1.cmml" xref="S4.p3.5.m5.2.2.1.1.1.1.3">subscript</csymbol><ci id="S4.p3.5.m5.2.2.1.1.1.1.3.2.cmml" xref="S4.p3.5.m5.2.2.1.1.1.1.3.2">𝐷</ci><ci id="S4.p3.5.m5.2.2.1.1.1.1.3.3.cmml" xref="S4.p3.5.m5.2.2.1.1.1.1.3.3">𝑖</ci></apply></apply><cn type="integer" id="S4.p3.5.m5.2.2.1.3.cmml" xref="S4.p3.5.m5.2.2.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.2c">RMSE:\frac{1}{\left|I\right|}\sum_{i\in{I}}\sqrt{(\hat{D}_{i}-D_{i})^{2}}</annotation></semantics></math>, <math id="S4.p3.6.m6.2" class="ltx_Math" alttext="REL:\frac{1}{\left|I\right|}\sum_{i\in{I}}\left|(\hat{D}_{i}-D_{i})/\hat{D}_{i}\right|" display="inline"><semantics id="S4.p3.6.m6.2a"><mrow id="S4.p3.6.m6.2.2" xref="S4.p3.6.m6.2.2.cmml"><mrow id="S4.p3.6.m6.2.2.3" xref="S4.p3.6.m6.2.2.3.cmml"><mi id="S4.p3.6.m6.2.2.3.2" xref="S4.p3.6.m6.2.2.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.p3.6.m6.2.2.3.1" xref="S4.p3.6.m6.2.2.3.1.cmml">​</mo><mi id="S4.p3.6.m6.2.2.3.3" xref="S4.p3.6.m6.2.2.3.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.p3.6.m6.2.2.3.1a" xref="S4.p3.6.m6.2.2.3.1.cmml">​</mo><mi id="S4.p3.6.m6.2.2.3.4" xref="S4.p3.6.m6.2.2.3.4.cmml">L</mi></mrow><mo lspace="0.278em" rspace="0.278em" id="S4.p3.6.m6.2.2.2" xref="S4.p3.6.m6.2.2.2.cmml">:</mo><mrow id="S4.p3.6.m6.2.2.1" xref="S4.p3.6.m6.2.2.1.cmml"><mfrac id="S4.p3.6.m6.1.1" xref="S4.p3.6.m6.1.1.cmml"><mn id="S4.p3.6.m6.1.1.3" xref="S4.p3.6.m6.1.1.3.cmml">1</mn><mrow id="S4.p3.6.m6.1.1.1.3" xref="S4.p3.6.m6.1.1.1.2.cmml"><mo id="S4.p3.6.m6.1.1.1.3.1" xref="S4.p3.6.m6.1.1.1.2.1.cmml">|</mo><mi id="S4.p3.6.m6.1.1.1.1" xref="S4.p3.6.m6.1.1.1.1.cmml">I</mi><mo id="S4.p3.6.m6.1.1.1.3.2" xref="S4.p3.6.m6.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.p3.6.m6.2.2.1.2" xref="S4.p3.6.m6.2.2.1.2.cmml">​</mo><mrow id="S4.p3.6.m6.2.2.1.1" xref="S4.p3.6.m6.2.2.1.1.cmml"><msub id="S4.p3.6.m6.2.2.1.1.2" xref="S4.p3.6.m6.2.2.1.1.2.cmml"><mo rspace="0em" id="S4.p3.6.m6.2.2.1.1.2.2" xref="S4.p3.6.m6.2.2.1.1.2.2.cmml">∑</mo><mrow id="S4.p3.6.m6.2.2.1.1.2.3" xref="S4.p3.6.m6.2.2.1.1.2.3.cmml"><mi id="S4.p3.6.m6.2.2.1.1.2.3.2" xref="S4.p3.6.m6.2.2.1.1.2.3.2.cmml">i</mi><mo id="S4.p3.6.m6.2.2.1.1.2.3.1" xref="S4.p3.6.m6.2.2.1.1.2.3.1.cmml">∈</mo><mi id="S4.p3.6.m6.2.2.1.1.2.3.3" xref="S4.p3.6.m6.2.2.1.1.2.3.3.cmml">I</mi></mrow></msub><mrow id="S4.p3.6.m6.2.2.1.1.1.1" xref="S4.p3.6.m6.2.2.1.1.1.2.cmml"><mo id="S4.p3.6.m6.2.2.1.1.1.1.2" xref="S4.p3.6.m6.2.2.1.1.1.2.1.cmml">|</mo><mrow id="S4.p3.6.m6.2.2.1.1.1.1.1" xref="S4.p3.6.m6.2.2.1.1.1.1.1.cmml"><mrow id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.2" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.cmml"><msub id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2.2" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml">D</mi><mo id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2.1" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.3" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.1" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3.2" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3.2.cmml">D</mi><mi id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3.3" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.3" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.p3.6.m6.2.2.1.1.1.1.1.2" xref="S4.p3.6.m6.2.2.1.1.1.1.1.2.cmml">/</mo><msub id="S4.p3.6.m6.2.2.1.1.1.1.1.3" xref="S4.p3.6.m6.2.2.1.1.1.1.1.3.cmml"><mover accent="true" id="S4.p3.6.m6.2.2.1.1.1.1.1.3.2" xref="S4.p3.6.m6.2.2.1.1.1.1.1.3.2.cmml"><mi id="S4.p3.6.m6.2.2.1.1.1.1.1.3.2.2" xref="S4.p3.6.m6.2.2.1.1.1.1.1.3.2.2.cmml">D</mi><mo id="S4.p3.6.m6.2.2.1.1.1.1.1.3.2.1" xref="S4.p3.6.m6.2.2.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S4.p3.6.m6.2.2.1.1.1.1.1.3.3" xref="S4.p3.6.m6.2.2.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S4.p3.6.m6.2.2.1.1.1.1.3" xref="S4.p3.6.m6.2.2.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.6.m6.2b"><apply id="S4.p3.6.m6.2.2.cmml" xref="S4.p3.6.m6.2.2"><ci id="S4.p3.6.m6.2.2.2.cmml" xref="S4.p3.6.m6.2.2.2">:</ci><apply id="S4.p3.6.m6.2.2.3.cmml" xref="S4.p3.6.m6.2.2.3"><times id="S4.p3.6.m6.2.2.3.1.cmml" xref="S4.p3.6.m6.2.2.3.1"></times><ci id="S4.p3.6.m6.2.2.3.2.cmml" xref="S4.p3.6.m6.2.2.3.2">𝑅</ci><ci id="S4.p3.6.m6.2.2.3.3.cmml" xref="S4.p3.6.m6.2.2.3.3">𝐸</ci><ci id="S4.p3.6.m6.2.2.3.4.cmml" xref="S4.p3.6.m6.2.2.3.4">𝐿</ci></apply><apply id="S4.p3.6.m6.2.2.1.cmml" xref="S4.p3.6.m6.2.2.1"><times id="S4.p3.6.m6.2.2.1.2.cmml" xref="S4.p3.6.m6.2.2.1.2"></times><apply id="S4.p3.6.m6.1.1.cmml" xref="S4.p3.6.m6.1.1"><divide id="S4.p3.6.m6.1.1.2.cmml" xref="S4.p3.6.m6.1.1"></divide><cn type="integer" id="S4.p3.6.m6.1.1.3.cmml" xref="S4.p3.6.m6.1.1.3">1</cn><apply id="S4.p3.6.m6.1.1.1.2.cmml" xref="S4.p3.6.m6.1.1.1.3"><abs id="S4.p3.6.m6.1.1.1.2.1.cmml" xref="S4.p3.6.m6.1.1.1.3.1"></abs><ci id="S4.p3.6.m6.1.1.1.1.cmml" xref="S4.p3.6.m6.1.1.1.1">𝐼</ci></apply></apply><apply id="S4.p3.6.m6.2.2.1.1.cmml" xref="S4.p3.6.m6.2.2.1.1"><apply id="S4.p3.6.m6.2.2.1.1.2.cmml" xref="S4.p3.6.m6.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.p3.6.m6.2.2.1.1.2.1.cmml" xref="S4.p3.6.m6.2.2.1.1.2">subscript</csymbol><sum id="S4.p3.6.m6.2.2.1.1.2.2.cmml" xref="S4.p3.6.m6.2.2.1.1.2.2"></sum><apply id="S4.p3.6.m6.2.2.1.1.2.3.cmml" xref="S4.p3.6.m6.2.2.1.1.2.3"><in id="S4.p3.6.m6.2.2.1.1.2.3.1.cmml" xref="S4.p3.6.m6.2.2.1.1.2.3.1"></in><ci id="S4.p3.6.m6.2.2.1.1.2.3.2.cmml" xref="S4.p3.6.m6.2.2.1.1.2.3.2">𝑖</ci><ci id="S4.p3.6.m6.2.2.1.1.2.3.3.cmml" xref="S4.p3.6.m6.2.2.1.1.2.3.3">𝐼</ci></apply></apply><apply id="S4.p3.6.m6.2.2.1.1.1.2.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1"><abs id="S4.p3.6.m6.2.2.1.1.1.2.1.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.2"></abs><apply id="S4.p3.6.m6.2.2.1.1.1.1.1.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1"><divide id="S4.p3.6.m6.2.2.1.1.1.1.1.2.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.2"></divide><apply id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1"><minus id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2"><ci id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.2.2">𝐷</ci></apply><ci id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3.2">𝐷</ci><ci id="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S4.p3.6.m6.2.2.1.1.1.1.1.3.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p3.6.m6.2.2.1.1.1.1.1.3.1.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.3">subscript</csymbol><apply id="S4.p3.6.m6.2.2.1.1.1.1.1.3.2.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.3.2"><ci id="S4.p3.6.m6.2.2.1.1.1.1.1.3.2.1.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.3.2.1">^</ci><ci id="S4.p3.6.m6.2.2.1.1.1.1.1.3.2.2.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.3.2.2">𝐷</ci></apply><ci id="S4.p3.6.m6.2.2.1.1.1.1.1.3.3.cmml" xref="S4.p3.6.m6.2.2.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.6.m6.2c">REL:\frac{1}{\left|I\right|}\sum_{i\in{I}}\left|(\hat{D}_{i}-D_{i})/\hat{D}_{i}\right|</annotation></semantics></math>, 
<br class="ltx_break"><math id="S4.p3.7.m7.1" class="ltx_Math" alttext="\delta_{\tau}:" display="inline"><semantics id="S4.p3.7.m7.1a"><mrow id="S4.p3.7.m7.1.1" xref="S4.p3.7.m7.1.1.cmml"><msub id="S4.p3.7.m7.1.1.2" xref="S4.p3.7.m7.1.1.2.cmml"><mi id="S4.p3.7.m7.1.1.2.2" xref="S4.p3.7.m7.1.1.2.2.cmml">δ</mi><mi id="S4.p3.7.m7.1.1.2.3" xref="S4.p3.7.m7.1.1.2.3.cmml">τ</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S4.p3.7.m7.1.1.1" xref="S4.p3.7.m7.1.1.1.cmml">:</mo><mi id="S4.p3.7.m7.1.1.3" xref="S4.p3.7.m7.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.7.m7.1b"><apply id="S4.p3.7.m7.1.1.cmml" xref="S4.p3.7.m7.1.1"><ci id="S4.p3.7.m7.1.1.1.cmml" xref="S4.p3.7.m7.1.1.1">:</ci><apply id="S4.p3.7.m7.1.1.2.cmml" xref="S4.p3.7.m7.1.1.2"><csymbol cd="ambiguous" id="S4.p3.7.m7.1.1.2.1.cmml" xref="S4.p3.7.m7.1.1.2">subscript</csymbol><ci id="S4.p3.7.m7.1.1.2.2.cmml" xref="S4.p3.7.m7.1.1.2.2">𝛿</ci><ci id="S4.p3.7.m7.1.1.2.3.cmml" xref="S4.p3.7.m7.1.1.2.3">𝜏</ci></apply><csymbol cd="latexml" id="S4.p3.7.m7.1.1.3.cmml" xref="S4.p3.7.m7.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.7.m7.1c">\delta_{\tau}:</annotation></semantics></math> percentage of pixels satisfying <math id="S4.p3.8.m8.2" class="ltx_Math" alttext="max(\frac{D_{i}}{\hat{D}_{i}},\frac{\hat{D}_{i}}{D_{i}})&lt;\tau" display="inline"><semantics id="S4.p3.8.m8.2a"><mrow id="S4.p3.8.m8.2.3" xref="S4.p3.8.m8.2.3.cmml"><mrow id="S4.p3.8.m8.2.3.2" xref="S4.p3.8.m8.2.3.2.cmml"><mi id="S4.p3.8.m8.2.3.2.2" xref="S4.p3.8.m8.2.3.2.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.p3.8.m8.2.3.2.1" xref="S4.p3.8.m8.2.3.2.1.cmml">​</mo><mi id="S4.p3.8.m8.2.3.2.3" xref="S4.p3.8.m8.2.3.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p3.8.m8.2.3.2.1a" xref="S4.p3.8.m8.2.3.2.1.cmml">​</mo><mi id="S4.p3.8.m8.2.3.2.4" xref="S4.p3.8.m8.2.3.2.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.p3.8.m8.2.3.2.1b" xref="S4.p3.8.m8.2.3.2.1.cmml">​</mo><mrow id="S4.p3.8.m8.2.3.2.5.2" xref="S4.p3.8.m8.2.3.2.5.1.cmml"><mo stretchy="false" id="S4.p3.8.m8.2.3.2.5.2.1" xref="S4.p3.8.m8.2.3.2.5.1.cmml">(</mo><mfrac id="S4.p3.8.m8.1.1" xref="S4.p3.8.m8.1.1.cmml"><msub id="S4.p3.8.m8.1.1.2" xref="S4.p3.8.m8.1.1.2.cmml"><mi id="S4.p3.8.m8.1.1.2.2" xref="S4.p3.8.m8.1.1.2.2.cmml">D</mi><mi id="S4.p3.8.m8.1.1.2.3" xref="S4.p3.8.m8.1.1.2.3.cmml">i</mi></msub><msub id="S4.p3.8.m8.1.1.3" xref="S4.p3.8.m8.1.1.3.cmml"><mover accent="true" id="S4.p3.8.m8.1.1.3.2" xref="S4.p3.8.m8.1.1.3.2.cmml"><mi id="S4.p3.8.m8.1.1.3.2.2" xref="S4.p3.8.m8.1.1.3.2.2.cmml">D</mi><mo id="S4.p3.8.m8.1.1.3.2.1" xref="S4.p3.8.m8.1.1.3.2.1.cmml">^</mo></mover><mi id="S4.p3.8.m8.1.1.3.3" xref="S4.p3.8.m8.1.1.3.3.cmml">i</mi></msub></mfrac><mo id="S4.p3.8.m8.2.3.2.5.2.2" xref="S4.p3.8.m8.2.3.2.5.1.cmml">,</mo><mfrac id="S4.p3.8.m8.2.2" xref="S4.p3.8.m8.2.2.cmml"><msub id="S4.p3.8.m8.2.2.2" xref="S4.p3.8.m8.2.2.2.cmml"><mover accent="true" id="S4.p3.8.m8.2.2.2.2" xref="S4.p3.8.m8.2.2.2.2.cmml"><mi id="S4.p3.8.m8.2.2.2.2.2" xref="S4.p3.8.m8.2.2.2.2.2.cmml">D</mi><mo id="S4.p3.8.m8.2.2.2.2.1" xref="S4.p3.8.m8.2.2.2.2.1.cmml">^</mo></mover><mi id="S4.p3.8.m8.2.2.2.3" xref="S4.p3.8.m8.2.2.2.3.cmml">i</mi></msub><msub id="S4.p3.8.m8.2.2.3" xref="S4.p3.8.m8.2.2.3.cmml"><mi id="S4.p3.8.m8.2.2.3.2" xref="S4.p3.8.m8.2.2.3.2.cmml">D</mi><mi id="S4.p3.8.m8.2.2.3.3" xref="S4.p3.8.m8.2.2.3.3.cmml">i</mi></msub></mfrac><mo stretchy="false" id="S4.p3.8.m8.2.3.2.5.2.3" xref="S4.p3.8.m8.2.3.2.5.1.cmml">)</mo></mrow></mrow><mo id="S4.p3.8.m8.2.3.1" xref="S4.p3.8.m8.2.3.1.cmml">&lt;</mo><mi id="S4.p3.8.m8.2.3.3" xref="S4.p3.8.m8.2.3.3.cmml">τ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.8.m8.2b"><apply id="S4.p3.8.m8.2.3.cmml" xref="S4.p3.8.m8.2.3"><lt id="S4.p3.8.m8.2.3.1.cmml" xref="S4.p3.8.m8.2.3.1"></lt><apply id="S4.p3.8.m8.2.3.2.cmml" xref="S4.p3.8.m8.2.3.2"><times id="S4.p3.8.m8.2.3.2.1.cmml" xref="S4.p3.8.m8.2.3.2.1"></times><ci id="S4.p3.8.m8.2.3.2.2.cmml" xref="S4.p3.8.m8.2.3.2.2">𝑚</ci><ci id="S4.p3.8.m8.2.3.2.3.cmml" xref="S4.p3.8.m8.2.3.2.3">𝑎</ci><ci id="S4.p3.8.m8.2.3.2.4.cmml" xref="S4.p3.8.m8.2.3.2.4">𝑥</ci><interval closure="open" id="S4.p3.8.m8.2.3.2.5.1.cmml" xref="S4.p3.8.m8.2.3.2.5.2"><apply id="S4.p3.8.m8.1.1.cmml" xref="S4.p3.8.m8.1.1"><divide id="S4.p3.8.m8.1.1.1.cmml" xref="S4.p3.8.m8.1.1"></divide><apply id="S4.p3.8.m8.1.1.2.cmml" xref="S4.p3.8.m8.1.1.2"><csymbol cd="ambiguous" id="S4.p3.8.m8.1.1.2.1.cmml" xref="S4.p3.8.m8.1.1.2">subscript</csymbol><ci id="S4.p3.8.m8.1.1.2.2.cmml" xref="S4.p3.8.m8.1.1.2.2">𝐷</ci><ci id="S4.p3.8.m8.1.1.2.3.cmml" xref="S4.p3.8.m8.1.1.2.3">𝑖</ci></apply><apply id="S4.p3.8.m8.1.1.3.cmml" xref="S4.p3.8.m8.1.1.3"><csymbol cd="ambiguous" id="S4.p3.8.m8.1.1.3.1.cmml" xref="S4.p3.8.m8.1.1.3">subscript</csymbol><apply id="S4.p3.8.m8.1.1.3.2.cmml" xref="S4.p3.8.m8.1.1.3.2"><ci id="S4.p3.8.m8.1.1.3.2.1.cmml" xref="S4.p3.8.m8.1.1.3.2.1">^</ci><ci id="S4.p3.8.m8.1.1.3.2.2.cmml" xref="S4.p3.8.m8.1.1.3.2.2">𝐷</ci></apply><ci id="S4.p3.8.m8.1.1.3.3.cmml" xref="S4.p3.8.m8.1.1.3.3">𝑖</ci></apply></apply><apply id="S4.p3.8.m8.2.2.cmml" xref="S4.p3.8.m8.2.2"><divide id="S4.p3.8.m8.2.2.1.cmml" xref="S4.p3.8.m8.2.2"></divide><apply id="S4.p3.8.m8.2.2.2.cmml" xref="S4.p3.8.m8.2.2.2"><csymbol cd="ambiguous" id="S4.p3.8.m8.2.2.2.1.cmml" xref="S4.p3.8.m8.2.2.2">subscript</csymbol><apply id="S4.p3.8.m8.2.2.2.2.cmml" xref="S4.p3.8.m8.2.2.2.2"><ci id="S4.p3.8.m8.2.2.2.2.1.cmml" xref="S4.p3.8.m8.2.2.2.2.1">^</ci><ci id="S4.p3.8.m8.2.2.2.2.2.cmml" xref="S4.p3.8.m8.2.2.2.2.2">𝐷</ci></apply><ci id="S4.p3.8.m8.2.2.2.3.cmml" xref="S4.p3.8.m8.2.2.2.3">𝑖</ci></apply><apply id="S4.p3.8.m8.2.2.3.cmml" xref="S4.p3.8.m8.2.2.3"><csymbol cd="ambiguous" id="S4.p3.8.m8.2.2.3.1.cmml" xref="S4.p3.8.m8.2.2.3">subscript</csymbol><ci id="S4.p3.8.m8.2.2.3.2.cmml" xref="S4.p3.8.m8.2.2.3.2">𝐷</ci><ci id="S4.p3.8.m8.2.2.3.3.cmml" xref="S4.p3.8.m8.2.2.3.3">𝑖</ci></apply></apply></interval></apply><ci id="S4.p3.8.m8.2.3.3.cmml" xref="S4.p3.8.m8.2.3.3">𝜏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.8.m8.2c">max(\frac{D_{i}}{\hat{D}_{i}},\frac{\hat{D}_{i}}{D_{i}})&lt;\tau</annotation></semantics></math>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:63pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-158.2pt,22.8pt) scale(0.578210495150872,0.578210495150872) ;">
<table id="S4.T1.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.3.3.4.1" class="ltx_tr">
<th id="S4.T1.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" rowspan="2"><span id="S4.T1.3.3.4.1.1.1" class="ltx_text">Depth</span></th>
<th id="S4.T1.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" rowspan="2"><span id="S4.T1.3.3.4.1.2.1" class="ltx_text">NOCE</span></th>
<th id="S4.T1.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" rowspan="2"><span id="S4.T1.3.3.4.1.3.1" class="ltx_text">Encoder</span></th>
<th id="S4.T1.3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">3D Shape</th>
<th id="S4.T1.3.3.4.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">2D Depth</th>
<th id="S4.T1.3.3.4.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="6">6D Pose and Size</th>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<th id="S4.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Chamfer ↓</th>
<th id="S4.T1.3.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Normal ↓</th>
<th id="S4.T1.3.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">RMSE ↓</th>
<th id="S4.T1.3.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Rel ↓</th>
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\delta_{1.25}" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><msub id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.1.m1.1.1.2.cmml">δ</mi><mn id="S4.T1.1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.1.m1.1.1.3.cmml">1.25</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.1.m1.1.1.2">𝛿</ci><cn type="float" id="S4.T1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.1.m1.1.1.3">1.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\delta_{1.25}</annotation></semantics></math> ↑</th>
<th id="S4.T1.3.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">3D IoU (25) ↑</th>
<th id="S4.T1.3.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">3D IoU (50) ↑</th>
<th id="S4.T1.3.3.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">3D IoU (75) ↑</th>
<th id="S4.T1.3.3.3.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">10 cm ↑</th>
<th id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="10^{\circ}" display="inline"><semantics id="S4.T1.2.2.2.2.m1.1a"><msup id="S4.T1.2.2.2.2.m1.1.1" xref="S4.T1.2.2.2.2.m1.1.1.cmml"><mn id="S4.T1.2.2.2.2.m1.1.1.2" xref="S4.T1.2.2.2.2.m1.1.1.2.cmml">10</mn><mo id="S4.T1.2.2.2.2.m1.1.1.3" xref="S4.T1.2.2.2.2.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><apply id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.2.2.2.2.m1.1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T1.2.2.2.2.m1.1.1.2.cmml" xref="S4.T1.2.2.2.2.m1.1.1.2">10</cn><compose id="S4.T1.2.2.2.2.m1.1.1.3.cmml" xref="S4.T1.2.2.2.2.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">10^{\circ}</annotation></semantics></math> ↑</th>
<th id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="10^{\circ}" display="inline"><semantics id="S4.T1.3.3.3.3.m1.1a"><msup id="S4.T1.3.3.3.3.m1.1.1" xref="S4.T1.3.3.3.3.m1.1.1.cmml"><mn id="S4.T1.3.3.3.3.m1.1.1.2" xref="S4.T1.3.3.3.3.m1.1.1.2.cmml">10</mn><mo id="S4.T1.3.3.3.3.m1.1.1.3" xref="S4.T1.3.3.3.3.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><apply id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.3.3.m1.1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T1.3.3.3.3.m1.1.1.2.cmml" xref="S4.T1.3.3.3.3.m1.1.1.2">10</cn><compose id="S4.T1.3.3.3.3.m1.1.1.3.cmml" xref="S4.T1.3.3.3.3.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">10^{\circ}</annotation></semantics></math> 10cm ↑</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.3.3.5.1" class="ltx_tr">
<td id="S4.T1.3.3.5.1.1" class="ltx_td ltx_align_center ltx_border_t">Regression</td>
<td id="S4.T1.3.3.5.1.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T1.3.3.5.1.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Sep.</td>
<td id="S4.T1.3.3.5.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.3.3.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T1.3.3.5.1.6" class="ltx_td ltx_align_center ltx_border_t">0.1700</td>
<td id="S4.T1.3.3.5.1.7" class="ltx_td ltx_align_center ltx_border_t">0.2143</td>
<td id="S4.T1.3.3.5.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.8274</td>
<td id="S4.T1.3.3.5.1.9" class="ltx_td ltx_align_center ltx_border_t">50.1</td>
<td id="S4.T1.3.3.5.1.10" class="ltx_td ltx_align_center ltx_border_t">15.4</td>
<td id="S4.T1.3.3.5.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.7</td>
<td id="S4.T1.3.3.5.1.12" class="ltx_td ltx_align_center ltx_border_t">18.9</td>
<td id="S4.T1.3.3.5.1.13" class="ltx_td ltx_align_center ltx_border_t">27.9</td>
<td id="S4.T1.3.3.5.1.14" class="ltx_td ltx_align_center ltx_border_t">6.1</td>
</tr>
<tr id="S4.T1.3.3.6.2" class="ltx_tr">
<td id="S4.T1.3.3.6.2.1" class="ltx_td ltx_align_center">Rendered</td>
<td id="S4.T1.3.3.6.2.2" class="ltx_td"></td>
<td id="S4.T1.3.3.6.2.3" class="ltx_td ltx_align_center ltx_border_rr">Sep.</td>
<td id="S4.T1.3.3.6.2.4" class="ltx_td ltx_align_center">40.08</td>
<td id="S4.T1.3.3.6.2.5" class="ltx_td ltx_align_center ltx_border_r">0.76</td>
<td id="S4.T1.3.3.6.2.6" class="ltx_td ltx_align_center">0.1309</td>
<td id="S4.T1.3.3.6.2.7" class="ltx_td ltx_align_center">0.0984</td>
<td id="S4.T1.3.3.6.2.8" class="ltx_td ltx_align_center ltx_border_r">0.9233</td>
<td id="S4.T1.3.3.6.2.9" class="ltx_td ltx_align_center">66.6</td>
<td id="S4.T1.3.3.6.2.10" class="ltx_td ltx_align_center">25.3</td>
<td id="S4.T1.3.3.6.2.11" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S4.T1.3.3.6.2.12" class="ltx_td ltx_align_center">24.8</td>
<td id="S4.T1.3.3.6.2.13" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.6.2.13.1" class="ltx_text ltx_font_bold">63.8</span></td>
<td id="S4.T1.3.3.6.2.14" class="ltx_td ltx_align_center">16.6</td>
</tr>
<tr id="S4.T1.3.3.7.3" class="ltx_tr">
<td id="S4.T1.3.3.7.3.1" class="ltx_td ltx_align_center">Rendered</td>
<td id="S4.T1.3.3.7.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T1.3.3.7.3.3" class="ltx_td ltx_align_center ltx_border_rr">Sep.</td>
<td id="S4.T1.3.3.7.3.4" class="ltx_td ltx_align_center">34.34</td>
<td id="S4.T1.3.3.7.3.5" class="ltx_td ltx_align_center ltx_border_r">0.82</td>
<td id="S4.T1.3.3.7.3.6" class="ltx_td ltx_align_center">0.1204</td>
<td id="S4.T1.3.3.7.3.7" class="ltx_td ltx_align_center">0.0914</td>
<td id="S4.T1.3.3.7.3.8" class="ltx_td ltx_align_center ltx_border_r">0.9376</td>
<td id="S4.T1.3.3.7.3.9" class="ltx_td ltx_align_center">71.9</td>
<td id="S4.T1.3.3.7.3.10" class="ltx_td ltx_align_center">29.2</td>
<td id="S4.T1.3.3.7.3.11" class="ltx_td ltx_align_center ltx_border_r">4.6</td>
<td id="S4.T1.3.3.7.3.12" class="ltx_td ltx_align_center">28.0</td>
<td id="S4.T1.3.3.7.3.13" class="ltx_td ltx_align_center">61.6</td>
<td id="S4.T1.3.3.7.3.14" class="ltx_td ltx_align_center">18.2</td>
</tr>
<tr id="S4.T1.3.3.8.4" class="ltx_tr">
<td id="S4.T1.3.3.8.4.1" class="ltx_td ltx_align_center ltx_border_b">Rendered</td>
<td id="S4.T1.3.3.8.4.2" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S4.T1.3.3.8.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">Shared</td>
<td id="S4.T1.3.3.8.4.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.3.3.8.4.4.1" class="ltx_text ltx_font_bold">31.44</span></td>
<td id="S4.T1.3.3.8.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.3.3.8.4.5.1" class="ltx_text ltx_font_bold">0.74</span></td>
<td id="S4.T1.3.3.8.4.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.3.3.8.4.6.1" class="ltx_text ltx_font_bold">0.1168</span></td>
<td id="S4.T1.3.3.8.4.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.3.3.8.4.7.1" class="ltx_text ltx_font_bold">0.0898</span></td>
<td id="S4.T1.3.3.8.4.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.3.3.8.4.8.1" class="ltx_text ltx_font_bold">0.9486</span></td>
<td id="S4.T1.3.3.8.4.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.3.3.8.4.9.1" class="ltx_text ltx_font_bold">75.4</span></td>
<td id="S4.T1.3.3.8.4.10" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.3.3.8.4.10.1" class="ltx_text ltx_font_bold">32.4</span></td>
<td id="S4.T1.3.3.8.4.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.3.3.8.4.11.1" class="ltx_text ltx_font_bold">5.1</span></td>
<td id="S4.T1.3.3.8.4.12" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.3.3.8.4.12.1" class="ltx_text ltx_font_bold">29.7</span></td>
<td id="S4.T1.3.3.8.4.13" class="ltx_td ltx_align_center ltx_border_b">60.8</td>
<td id="S4.T1.3.3.8.4.14" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.3.3.8.4.14.1" class="ltx_text ltx_font_bold">19.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>
<span id="S4.T1.5.1" class="ltx_text ltx_font_bold">Ablation study of our design choices.</span> We compared 4 variations of our framework on the CAMERA25 dataset. We evaluated performances on 3D shape, 2D Depth, and 6D pose and size.
</figcaption>
</figure>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.6" class="ltx_p"><span id="S4.p4.6.1" class="ltx_text ltx_font_bold">Implementation details.</span>
Our MSOS branch uses a 4-channeled input, with an RGB image and a predicted segmentation mask, where we resize the input to 192 <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p4.1.m1.1a"><mo id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><times id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\times</annotation></semantics></math> 192 pixels.
We used the shared encoder structure from Chen <em id="S4.p4.6.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.p4.6.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
Due to the memory limitation, we set the 12 channels for voxel predictions.
In the Z header, each object center and radius decoders comprise 4 convolutional layers with kernel size 3 <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p4.2.m2.1a"><mo id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><times id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">\times</annotation></semantics></math> 3 and 2 fully connected layers with ReLU activation.
We use the NOCS branch from off-the-shelf methods of NOCS predictor from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
Our overall pipeline was only trained on the CAMERA dataset.
We trained our MSOS branch from scratch, and we used the pretrained weights from the original paper for the NOCS branch.
We implemented our framework in PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and trained the MSOS branch using a single GPU (Titan Xp).
We empirically set the <math id="S4.p4.3.m3.1" class="ltx_Math" alttext="\lambda_{\text{voxel}}" display="inline"><semantics id="S4.p4.3.m3.1a"><msub id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml"><mi id="S4.p4.3.m3.1.1.2" xref="S4.p4.3.m3.1.1.2.cmml">λ</mi><mtext id="S4.p4.3.m3.1.1.3" xref="S4.p4.3.m3.1.1.3a.cmml">voxel</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><apply id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p4.3.m3.1.1.1.cmml" xref="S4.p4.3.m3.1.1">subscript</csymbol><ci id="S4.p4.3.m3.1.1.2.cmml" xref="S4.p4.3.m3.1.1.2">𝜆</ci><ci id="S4.p4.3.m3.1.1.3a.cmml" xref="S4.p4.3.m3.1.1.3"><mtext mathsize="70%" id="S4.p4.3.m3.1.1.3.cmml" xref="S4.p4.3.m3.1.1.3">voxel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">\lambda_{\text{voxel}}</annotation></semantics></math> = 3, <math id="S4.p4.4.m4.1" class="ltx_Math" alttext="\lambda_{M_{\text{norm}}}" display="inline"><semantics id="S4.p4.4.m4.1a"><msub id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml"><mi id="S4.p4.4.m4.1.1.2" xref="S4.p4.4.m4.1.1.2.cmml">λ</mi><msub id="S4.p4.4.m4.1.1.3" xref="S4.p4.4.m4.1.1.3.cmml"><mi id="S4.p4.4.m4.1.1.3.2" xref="S4.p4.4.m4.1.1.3.2.cmml">M</mi><mtext id="S4.p4.4.m4.1.1.3.3" xref="S4.p4.4.m4.1.1.3.3a.cmml">norm</mtext></msub></msub><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><apply id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p4.4.m4.1.1.1.cmml" xref="S4.p4.4.m4.1.1">subscript</csymbol><ci id="S4.p4.4.m4.1.1.2.cmml" xref="S4.p4.4.m4.1.1.2">𝜆</ci><apply id="S4.p4.4.m4.1.1.3.cmml" xref="S4.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.p4.4.m4.1.1.3.1.cmml" xref="S4.p4.4.m4.1.1.3">subscript</csymbol><ci id="S4.p4.4.m4.1.1.3.2.cmml" xref="S4.p4.4.m4.1.1.3.2">𝑀</ci><ci id="S4.p4.4.m4.1.1.3.3a.cmml" xref="S4.p4.4.m4.1.1.3.3"><mtext mathsize="50%" id="S4.p4.4.m4.1.1.3.3.cmml" xref="S4.p4.4.m4.1.1.3.3">norm</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">\lambda_{M_{\text{norm}}}</annotation></semantics></math> = 1, <math id="S4.p4.5.m5.1" class="ltx_Math" alttext="\lambda_{Z_{\text{NOCE}}}" display="inline"><semantics id="S4.p4.5.m5.1a"><msub id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml"><mi id="S4.p4.5.m5.1.1.2" xref="S4.p4.5.m5.1.1.2.cmml">λ</mi><msub id="S4.p4.5.m5.1.1.3" xref="S4.p4.5.m5.1.1.3.cmml"><mi id="S4.p4.5.m5.1.1.3.2" xref="S4.p4.5.m5.1.1.3.2.cmml">Z</mi><mtext id="S4.p4.5.m5.1.1.3.3" xref="S4.p4.5.m5.1.1.3.3a.cmml">NOCE</mtext></msub></msub><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.1b"><apply id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p4.5.m5.1.1.1.cmml" xref="S4.p4.5.m5.1.1">subscript</csymbol><ci id="S4.p4.5.m5.1.1.2.cmml" xref="S4.p4.5.m5.1.1.2">𝜆</ci><apply id="S4.p4.5.m5.1.1.3.cmml" xref="S4.p4.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.p4.5.m5.1.1.3.1.cmml" xref="S4.p4.5.m5.1.1.3">subscript</csymbol><ci id="S4.p4.5.m5.1.1.3.2.cmml" xref="S4.p4.5.m5.1.1.3.2">𝑍</ci><ci id="S4.p4.5.m5.1.1.3.3a.cmml" xref="S4.p4.5.m5.1.1.3.3"><mtext mathsize="50%" id="S4.p4.5.m5.1.1.3.3.cmml" xref="S4.p4.5.m5.1.1.3.3">NOCE</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.1c">\lambda_{Z_{\text{NOCE}}}</annotation></semantics></math> = 30, <math id="S4.p4.6.m6.1" class="ltx_Math" alttext="\lambda_{R}" display="inline"><semantics id="S4.p4.6.m6.1a"><msub id="S4.p4.6.m6.1.1" xref="S4.p4.6.m6.1.1.cmml"><mi id="S4.p4.6.m6.1.1.2" xref="S4.p4.6.m6.1.1.2.cmml">λ</mi><mi id="S4.p4.6.m6.1.1.3" xref="S4.p4.6.m6.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p4.6.m6.1b"><apply id="S4.p4.6.m6.1.1.cmml" xref="S4.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p4.6.m6.1.1.1.cmml" xref="S4.p4.6.m6.1.1">subscript</csymbol><ci id="S4.p4.6.m6.1.1.2.cmml" xref="S4.p4.6.m6.1.1.2">𝜆</ci><ci id="S4.p4.6.m6.1.1.3.cmml" xref="S4.p4.6.m6.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.6.m6.1c">\lambda_{R}</annotation></semantics></math> = 30.
We used the Adam optimizer with an initial learning rate of 0.0001 and the batch size of 16.
In each stage, we decreased the learning rate by a factor of 0.6, 0.3, 0.1, 0.01 for each 15k, 30k, 45k, 60k iteration.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Ablation study</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We performed an ablation study to evaluate our various design choices.
Both the training and testing were done on the CAMERA dataset.
Overall performances are summarized in Tab. <a href="#S4.T1" title="TABLE I ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Depth rendering.</span>
As mentioned in Sec. <a href="#S3.SS2" title="III-B Depth Rendering ‣ III Category-Level Metric Scale Object Shape and Pose Estimation ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>, we compare our rendered depth map with simple UNet-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> monocular depth estimation results.
Comparing the first two rows in Tab. <a href="#S4.T1" title="TABLE I ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, it is clear that obtaining the depth map using our depth rendering with the predicted metric scale mesh outperformed all the metrics compared to the UNet-based depth estimation.
Also, UNet does not predict the metric scale shape of the 3D object, thereby the results are marked as ‘-’ in the 3D shape metric.
In particular, the depth rendering results have a significant lead in the rotation evaluation.
We believe that our rendered depth contains the surface information between adjacent pixels based on face information from our predicted mesh.
These results demonstrate the superiority of our metric scale mesh based depth rendering approach over direct depth regression.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:87.8pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-113.1pt,28.4pt) scale(0.605401571623967,0.605401571623967) ;">
<table id="S4.T2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.2.2.3.1.1.1" class="ltx_text">Method</span></th>
<th id="S4.T2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.2.2.3.1.2.1" class="ltx_text">Input</span></th>
<td id="S4.T2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="6">6D Pose and Size</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">3D IoU (25) ↑</td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">3D IoU (50) ↑</td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3D IoU (75) ↑</td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">10 cm ↑</td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="10^{\circ}" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><msup id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mn id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">10</mn><mo id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">10</cn><compose id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">10^{\circ}</annotation></semantics></math> ↑</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="10^{\circ}" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><msup id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml"><mn id="S4.T2.2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.2.m1.1.1.2.cmml">10</mn><mo id="S4.T2.2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.2.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2">10</cn><compose id="S4.T2.2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.2.2.2.2.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">10^{\circ}</annotation></semantics></math> 10cm ↑</td>
</tr>
<tr id="S4.T2.2.2.4.2" class="ltx_tr">
<th id="S4.T2.2.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<th id="S4.T2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">RGB</th>
<td id="S4.T2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t">34.0</td>
<td id="S4.T2.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_t">14.2</td>
<td id="S4.T2.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_t">4.8</td>
</tr>
<tr id="S4.T2.2.2.5.3" class="ltx_tr">
<th id="S4.T2.2.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Ours</th>
<th id="S4.T2.2.2.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">RGB</th>
<td id="S4.T2.2.2.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.5.3.3.1" class="ltx_text ltx_font_bold">62</span></td>
<td id="S4.T2.2.2.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.5.3.4.1" class="ltx_text ltx_font_bold">23.4</span></td>
<td id="S4.T2.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.2.5.3.5.1" class="ltx_text ltx_font_bold">3.0</span></td>
<td id="S4.T2.2.2.5.3.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.5.3.6.1" class="ltx_text ltx_font_bold">39.5</span></td>
<td id="S4.T2.2.2.5.3.7" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.5.3.7.1" class="ltx_text ltx_font_bold">29.2</span></td>
<td id="S4.T2.2.2.5.3.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.5.3.8.1" class="ltx_text ltx_font_bold">9.6</span></td>
</tr>
<tr id="S4.T2.2.2.6.4" class="ltx_tr">
<th id="S4.T2.2.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</th>
<th id="S4.T2.2.2.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">RGB-D</th>
<td id="S4.T2.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.6.4.3.1" class="ltx_text ltx_font_bold">84.4</span></td>
<td id="S4.T2.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_t">76.9</td>
<td id="S4.T2.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.1</td>
<td id="S4.T2.2.2.6.4.6" class="ltx_td ltx_align_center ltx_border_t">97.9</td>
<td id="S4.T2.2.2.6.4.7" class="ltx_td ltx_align_center ltx_border_t">24.8</td>
<td id="S4.T2.2.2.6.4.8" class="ltx_td ltx_align_center ltx_border_t">24.3</td>
</tr>
<tr id="S4.T2.2.2.7.5" class="ltx_tr">
<th id="S4.T2.2.2.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Shape-Prior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<th id="S4.T2.2.2.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">RGB-D</th>
<td id="S4.T2.2.2.7.5.3" class="ltx_td ltx_align_center">83.4</td>
<td id="S4.T2.2.2.7.5.4" class="ltx_td ltx_align_center">77.4</td>
<td id="S4.T2.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.2.7.5.5.1" class="ltx_text ltx_font_bold">53.5</span></td>
<td id="S4.T2.2.2.7.5.6" class="ltx_td ltx_align_center">99.4</td>
<td id="S4.T2.2.2.7.5.7" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.7.5.7.1" class="ltx_text ltx_font_bold">57.5</span></td>
<td id="S4.T2.2.2.7.5.8" class="ltx_td ltx_align_center">56.2</td>
</tr>
<tr id="S4.T2.2.2.8.6" class="ltx_tr">
<th id="S4.T2.2.2.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CASS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</th>
<th id="S4.T2.2.2.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">RGB-D</th>
<td id="S4.T2.2.2.8.6.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.8.6.3.1" class="ltx_text ltx_font_bold">84.4</span></td>
<td id="S4.T2.2.2.8.6.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.8.6.4.1" class="ltx_text ltx_font_bold">79.3</span></td>
<td id="S4.T2.2.2.8.6.5" class="ltx_td ltx_border_r"></td>
<td id="S4.T2.2.2.8.6.6" class="ltx_td"></td>
<td id="S4.T2.2.2.8.6.7" class="ltx_td"></td>
<td id="S4.T2.2.2.8.6.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.8.6.8.1" class="ltx_text ltx_font_bold">58.3</span></td>
</tr>
<tr id="S4.T2.2.2.9.7" class="ltx_tr">
<th id="S4.T2.2.2.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Ours</th>
<th id="S4.T2.2.2.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">RGB-OD</th>
<td id="S4.T2.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_b">81.6</td>
<td id="S4.T2.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_b">68.1</td>
<td id="S4.T2.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">32.9</td>
<td id="S4.T2.2.2.9.7.6" class="ltx_td ltx_align_center ltx_border_b">96.7</td>
<td id="S4.T2.2.2.9.7.7" class="ltx_td ltx_align_center ltx_border_b">28.7</td>
<td id="S4.T2.2.2.9.7.8" class="ltx_td ltx_align_center ltx_border_b">26.5</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span><span id="S4.T2.4.1" class="ltx_text ltx_font_bold">Quantitative comparison with the state-of-the-art methods on the REAL275 dataset.</span>
Empty entries either could not be evaluated, or were not reported in the original paper.
</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:66.3pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-111.7pt,21.2pt) scale(0.608261776755678,0.608261776755678) ;">
<table id="S4.T3.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2.3.1" class="ltx_tr">
<th id="S4.T3.2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.2.2.3.1.1.1" class="ltx_text">Method</span></th>
<th id="S4.T3.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.2.2.3.1.2.1" class="ltx_text">Input</span></th>
<td id="S4.T3.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="6">6D Pose and Size</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">3D IoU (25) ↑</td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">3D IoU (50) ↑</td>
<td id="S4.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3D IoU (75) ↑</td>
<td id="S4.T3.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">10 cm ↑</td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="10^{\circ}" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><msup id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml"><mn id="S4.T3.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.m1.1.1.2.cmml">10</mn><mo id="S4.T3.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T3.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.1.m1.1.1.2">10</cn><compose id="S4.T3.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">10^{\circ}</annotation></semantics></math> ↑</td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="10^{\circ}" display="inline"><semantics id="S4.T3.2.2.2.2.m1.1a"><msup id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml"><mn id="S4.T3.2.2.2.2.m1.1.1.2" xref="S4.T3.2.2.2.2.m1.1.1.2.cmml">10</mn><mo id="S4.T3.2.2.2.2.m1.1.1.3" xref="S4.T3.2.2.2.2.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><apply id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.2.2.2.2.m1.1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T3.2.2.2.2.m1.1.1.2.cmml" xref="S4.T3.2.2.2.2.m1.1.1.2">10</cn><compose id="S4.T3.2.2.2.2.m1.1.1.3.cmml" xref="S4.T3.2.2.2.2.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">10^{\circ}</annotation></semantics></math> 10cm ↑</td>
</tr>
<tr id="S4.T3.2.2.4.2" class="ltx_tr">
<th id="S4.T3.2.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Ours</th>
<th id="S4.T3.2.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">RGB</th>
<td id="S4.T3.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.4.2.3.1" class="ltx_text ltx_font_bold">75.4</span></td>
<td id="S4.T3.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.4.2.4.1" class="ltx_text ltx_font_bold">32.4</span></td>
<td id="S4.T3.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.1</td>
<td id="S4.T3.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.4.2.6.1" class="ltx_text ltx_font_bold">29.7</span></td>
<td id="S4.T3.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_t">60.8</td>
<td id="S4.T3.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.4.2.8.1" class="ltx_text ltx_font_bold">19.2</span></td>
</tr>
<tr id="S4.T3.2.2.5.3" class="ltx_tr">
<th id="S4.T3.2.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</th>
<th id="S4.T3.2.2.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">RGB-D</th>
<td id="S4.T3.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">90.0</td>
<td id="S4.T3.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_t">87.8</td>
<td id="S4.T3.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.1</td>
<td id="S4.T3.2.2.5.3.6" class="ltx_td ltx_align_center ltx_border_t">99.0</td>
<td id="S4.T3.2.2.5.3.7" class="ltx_td ltx_align_center ltx_border_t">63.5</td>
<td id="S4.T3.2.2.5.3.8" class="ltx_td ltx_align_center ltx_border_t">63.1</td>
</tr>
<tr id="S4.T3.2.2.6.4" class="ltx_tr">
<th id="S4.T3.2.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Shape-Prior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<th id="S4.T3.2.2.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">RGB-D</th>
<td id="S4.T3.2.2.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.6.4.3.1" class="ltx_text ltx_font_bold">94.2</span></td>
<td id="S4.T3.2.2.6.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.6.4.4.1" class="ltx_text ltx_font_bold">93</span></td>
<td id="S4.T3.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.2.6.4.5.1" class="ltx_text ltx_font_bold">84.5</span></td>
<td id="S4.T3.2.2.6.4.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.6.4.6.1" class="ltx_text ltx_font_bold">99.4</span></td>
<td id="S4.T3.2.2.6.4.7" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.6.4.7.1" class="ltx_text ltx_font_bold">80.1</span></td>
<td id="S4.T3.2.2.6.4.8" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.6.4.8.1" class="ltx_text ltx_font_bold">79.8</span></td>
</tr>
<tr id="S4.T3.2.2.7.5" class="ltx_tr">
<th id="S4.T3.2.2.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Ours</th>
<th id="S4.T3.2.2.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">RGB-OD</th>
<td id="S4.T3.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_b">93.8</td>
<td id="S4.T3.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_b">90.8</td>
<td id="S4.T3.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">72.8</td>
<td id="S4.T3.2.2.7.5.6" class="ltx_td ltx_align_center ltx_border_b">96.6</td>
<td id="S4.T3.2.2.7.5.7" class="ltx_td ltx_align_center ltx_border_b">60.7</td>
<td id="S4.T3.2.2.7.5.8" class="ltx_td ltx_align_center ltx_border_b">58.9</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span><span id="S4.T3.4.1" class="ltx_text ltx_font_bold">Quantitative comparison with state-of-the-art methods on the CAMERA25 dataset.</span></figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.3" class="ltx_p"><span id="S4.SS1.p3.3.1" class="ltx_text ltx_font_bold">Object Center Estimation.</span>
As was explained in Sec. <a href="#S3.SS1.SSS2" title="III-A2 Normalized Object Center Estimation (NOCE) ‣ III-A Metric Scale Object Shape branch (MSOS) ‣ III Category-Level Metric Scale Object Shape and Pose Estimation ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>2</span></a>, we proposed a shared feature encoder training and normalized object center estimation (NOCE) technique for robust object center estimation.
To ablate the effectiveness of our design choices, we used the same NOCS map in the evaluation.
The second row in Tab. <a href="#S4.T1" title="TABLE I ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> does not use NOCE but directly estimates the object center <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">Z</annotation></semantics></math> from an image patch that has not been resized.
Tab. <a href="#S4.T1" title="TABLE I ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> shows that using NOCE and shared feature encoder improves their overall performance, respectively.
We want to focus more on the chamfer distance of the 3D shape evaluation and 3D IoUs and 10cm precision on the 6D pose and size evaluation, since these metrics are directly related to the object center <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mi id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">Z</annotation></semantics></math> and the object radius <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mi id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><ci id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">R</annotation></semantics></math> prediction.
In these evaluation categories, it is shown that NOCE significantly improves the performances, since it enables robust object center prediction.
By implicitly guiding the training process with the shared encoder for our mesh header and our Z header, it also helps to boost the accuracy in the aforementioned metrics.
Overall, among our 4 variations of object shape and pose estimation approaches, all three of our network designs resulted in the best performance.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Comparison with state-of-the-art</span>
</h3>

<figure id="S4.F7" class="ltx_figure"><img src="/html/2109.00326/assets/x7.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="S4.F7.3.1" class="ltx_text ltx_font_bold">Comparison with state-of-the-art RGB-D methods on thef CAMERA25 dataset.</span></figcaption>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.4.1.1" class="ltx_text">IV-B</span>1 </span>RGB based methods</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">To the best of our knowledge, Synthesis by Chen <em id="S4.SS2.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.SS2.SSS1.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is the only method in the RGB-based category-level pose estimation.
Since Synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> has no results using the CAMERA25 dataset, we only compared our method on the REAL275 dataset.
Note that Synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> cannot estimate the size of an object.
Tab. <a href="#S4.T2" title="TABLE II ‣ IV-A Ablation study ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows that our method achieved state-of-the-art results in the RGB-based category-level pose estimation.
Especially, our method achieved more than double the performance at <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="10^{\circ}" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><msup id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS2.SSS1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.2">10</cn><compose id="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">10^{\circ}</annotation></semantics></math> metric (29.2% compared to 14.2%).</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2109.00326/assets/x8.png" id="S4.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="341" height="407" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span id="S4.F8.3.1" class="ltx_text ltx_font_bold">Qualitative results on the REAL275 dataset.</span></figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2109.00326/assets/x9.png" id="S4.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="368" height="413" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="S4.F9.3.1" class="ltx_text ltx_font_bold">Qualitative examples of our predictions on CAMERA25.</span>
Our method jointly predicted the object shape with fine details, including bottle head, mug and thin laptop with accurate poses.
Our metric scale object shape branch (MSOS) reconstructs the object mesh well even for invisible parts.
</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.4.1.1" class="ltx_text">IV-B</span>2 </span>RGB-D based methods</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">We also compared our performance on the RGB-OD setup with RGB-D-based state-of-the-art methods on both the CAMERA25 and REAL275 datasets.
As mentioned earlier, our RGB-OD setup uses only one pixel depth value per object.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">The results on the REAL275 dataset are summarized in Tab. <a href="#S4.T2" title="TABLE II ‣ IV-A Ablation study ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, and the results on the CAMERA25 dataset are summarized in Tab. <a href="#S4.T3" title="TABLE III ‣ IV-A Ablation study ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.
In the evaluation of the REAL275 dataset, note that unlike the existing RGB-D methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, we trained only with the CAMERA dataset.
Both tables show that our RGB-OD results were significantly better than our results with the RGB-only setup.
While our RGB-based method still performed the best among RGB-only approaches, we can say that it is still a challenging problem to solve the object center estimation and object size estimation problem without using any reliable depth information.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.2" class="ltx_p">Comparing our RGB-OD results to other RGB-D approaches, ours performed comparably well in both datasets and even outperformed NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> on the 3D IoU metrics in the CAMERA25 dataset, and on the <math id="S4.SS2.SSS2.p3.1.m1.1" class="ltx_Math" alttext="10^{\circ}" display="inline"><semantics id="S4.SS2.SSS2.p3.1.m1.1a"><msup id="S4.SS2.SSS2.p3.1.m1.1.1" xref="S4.SS2.SSS2.p3.1.m1.1.1.cmml"><mn id="S4.SS2.SSS2.p3.1.m1.1.1.2" xref="S4.SS2.SSS2.p3.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS2.SSS2.p3.1.m1.1.1.3" xref="S4.SS2.SSS2.p3.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.1.m1.1b"><apply id="S4.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.SSS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1.2">10</cn><compose id="S4.SS2.SSS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.1.m1.1c">10^{\circ}</annotation></semantics></math> metric in the REAL275 dataset.
Also, Fig. <a href="#S4.F7" title="Figure 7 ‣ IV-B Comparison with state-of-the-art ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that our method achieved higher performance after around <math id="S4.SS2.SSS2.p3.2.m2.1" class="ltx_Math" alttext="12^{\circ}" display="inline"><semantics id="S4.SS2.SSS2.p3.2.m2.1a"><msup id="S4.SS2.SSS2.p3.2.m2.1.1" xref="S4.SS2.SSS2.p3.2.m2.1.1.cmml"><mn id="S4.SS2.SSS2.p3.2.m2.1.1.2" xref="S4.SS2.SSS2.p3.2.m2.1.1.2.cmml">12</mn><mo id="S4.SS2.SSS2.p3.2.m2.1.1.3" xref="S4.SS2.SSS2.p3.2.m2.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.2.m2.1b"><apply id="S4.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.SSS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.2">12</cn><compose id="S4.SS2.SSS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.2.m2.1c">12^{\circ}</annotation></semantics></math> threshold errors than NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> on the mAP of the rotation metric.
The results show that our proposed algorithm was powerful in the RGB-only comparison, and was also comparable to previous RGB-D methods while using only a single pixel depth value without any additional training.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS3.4.1.1" class="ltx_text">IV-B</span>3 </span>Qualitative results</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Fig. <a href="#S4.F8" title="Figure 8 ‣ IV-B1 RGB based methods ‣ IV-B Comparison with state-of-the-art ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and Fig. <a href="#S4.F9" title="Figure 9 ‣ IV-B1 RGB based methods ‣ IV-B Comparison with state-of-the-art ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> show examples of our predictions on both the REAL275 and CAMERA25 datasets.
The REAL275 consists of 6 scenarios and Fig. <a href="#S4.F8" title="Figure 8 ‣ IV-B1 RGB based methods ‣ IV-B Comparison with state-of-the-art ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> visualizes each scene of the 3D shape and 6D pose results.
It shows that our method estimated the 3D shape and 6D pose pretty well, even when the appearance and the position and orientation were different.
Fig. <a href="#S4.F9" title="Figure 9 ‣ IV-B1 RGB based methods ‣ IV-B Comparison with state-of-the-art ‣ IV Experiments ‣ Category-Level Metric Scale Object Shape and Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows that our method jointly predicted the object shape with fine details such as a bottle head, a mug, and a thin laptop with the accurate pose.
Our object prediction reconstructed well, even for invisible parts.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">CONCLUSIONS</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we proposed a framework for estimating the metric scale shape and 6D pose of a category-level object utilizing only a single RGB image with none or very spare depth information.
Our framework consists of two branches, where the Metric Scale Object Shape branch (MSOS) predicts the metric scale object mesh and the Normalized Object Coordinate Space branch (NOCS) estimates the normalized object coordinate space map.
In our MSOS branch, we propose the Normalized Object Center Estimation (NOCE) with a shared feature encoder to implicitly guide the geometrically aligned object center prediction using 3D volume information.
Using additional similarity transformation between the predicted metric scale mesh and NOCS map, our algorithm outputs metric scale mesh, 6d pose, and the size of an object.
Our results show that our method achieved the state-of-the-art performance for RGB-based methods and our RGB-OD approach was on par and sometimes better performance than the RGB-D methods.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We would like to thank Kiru Park for valuable comments and insightful discussions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Generative and discriminative voxel modeling with convolutional
neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1608.04236</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Dengsheng Chen, Jun Li, Zheng Wang, and Kai Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Learning canonical shape space for category-level 6d object pose and
size estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 11973–11982, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Rui Chen, Songfang Han, Jing Xu, and Hao Su.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Point-based multi-view stereo network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehtinen, Alec
Jacobson, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Learning to predict 3d objects with an interpolation-based
differentiable renderer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages
9609–9619, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Xu Chen, Zijian Dong, Jie Song, Andreas Geiger, and Otmar Hilliges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Category level object pose estimation via neural
analysis-by-synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">3d-r2n2: A unified approach for single and multi-view 3d object
reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 628–644.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Haoqiang Fan, Hao Su, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">A point set generation network for 3d object reconstruction from a
single image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 605–613, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Martin A Fischler and Robert C Bolles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Random sample consensus: a paradigm for model fitting with
applications to image analysis and automated cartography.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Communications of the ACM</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 24(6):381–395, 1981.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Georgia Gkioxari, Jitendra Malik, and Justin Johnson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Mesh r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 9785–9795, 2019.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu
Aubry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">A papier-mâché approach to learning 3d surface generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 216–224, 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Andrey Kurenkov, Jingwei Ji, Animesh Garg, Viraj Mehta, JunYoung Gwak,
Christopher Choy, and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Deformnet: Free-form deformation network for 3d shape reconstruction
from a single image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE Winter Conference on Applications of Computer
Vision</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 858–866. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Epnp: An accurate o (n) solution to the pnp problem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 81(2):155, 2009.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Zhigang Li, Gu Wang, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Cdpn: Coordinates-based disentangled pose network for real-time
rgb-based 6-dof object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 7678–7687, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Priyanka Mandikal, Navaneet KL, and R Venkatesh Babu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">3d-psrnet: Part segmented 3d point cloud reconstruction from a single
image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV) Workshops</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 0–0, 2018.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Douglas Morrison, Adam W Tow, Matt Mctaggart, R Smith, Norton Kelly-Boxall,
Sean Wade-Mccue, Jordan Erskine, Riccardo Grinover, Alec Gurman, T Hunn,
et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Cartman: The low-cost cartesian manipulator that won the amazon
robotics challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE International Conference on Robotics and Automation
(ICRA)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 7757–7764. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Junyi Pan, Xiaoguang Han, Weikai Chen, Jiapeng Tang, and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Deep mesh reconstruction from single rgb images via topology
modification networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 9964–9973, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Kiru Park, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 7668–7677, 2019.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Automatic differentiation in pytorch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Pvnet: Pixel-wise voting network for 6dof pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 4561–4570, 2019.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Mahdi Rad and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Bb8: A scalable, accurate, robust to partial occlusion method for
predicting the 3d poses of challenging objects without using depth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 3828–3836, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Medical image computing and
computer-assisted intervention</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 234–241. Springer, 2015.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Szymon Rusinkiewicz and Marc Levoy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Efficient variants of the icp algorithm.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings third international conference on 3-D digital
imaging and modeling</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 145–152. IEEE, 2001.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura Leal-Taixe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Understanding the limitations of cnn-based absolute camera pose
regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 3302–3312, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Indoor segmentation and support inference from rgbd images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 746–760.
Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Bugra Tekin, Sudipta N Sinha, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Real-time seamless single shot 6d object pose prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 292–301, 2018.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Meng Tian, Marcelo H Ang Jr, and Gim Hee Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Shape prior deformation for categorical 6d object pose and size
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, August 2020.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, and
Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Deep object pose estimation for semantic robotic grasping of
household objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1809.10790</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Shinji Umeyama.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Least-squares estimation of transformation parameters between two
point patterns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">,
13(04):376–380, 1991.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, and
Federico Tombari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Self6d: Self-supervised monocular 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The European Conference on Computer Vision (ECCV)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, August
2020.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and
Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Normalized object coordinate space for category-level 6d object pose
and size estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 2642–2651, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Pixel2mesh: Generating 3d mesh models from single rgb images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 52–67, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T Freeman, and Joshua B
Tenenbaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Learning a probabilistic latent space of object shapes via 3d
generative-adversarial modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1610.07584</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Dpod: 6d pose object detector and refiner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 1941–1950, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.00325" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.00326" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.00326">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.00326" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.00327" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 21:51:35 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
