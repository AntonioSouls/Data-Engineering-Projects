<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection</title>
<!--Generated on Thu Oct  3 20:34:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.03010v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S1" title="In MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S2" title="In MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3" title="In MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.SS1" title="In 3 Method â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Modality Masking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.SS2" title="In 3 Method â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Modality Projection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.SS3" title="In 3 Method â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Token and Dimension Variability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.SS4" title="In 3 Method â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Alignment Loss Objective</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4" title="In MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments and Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.SS1" title="In 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.SS2" title="In 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.SS3" title="In 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results on Multimodal Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.SS4" title="In 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Visualization of Predictions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.SS5" title="In 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Results on Multimodal Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.SS6" title="In 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Results on Multimodal Sentiment Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.SS7" title="In 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Ablation Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S5" title="In MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S1a" title="In MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S2a" title="In MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S2 </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S2.SS1" title="In S2 Implementation Details â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S2.1 </span>Multimodal segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S2.SS2" title="In S2 Implementation Details â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S2.2 </span>Multimodal classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S2.SS3" title="In S2 Implementation Details â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S2.3 </span>Multimodal sentiment analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S2.SS4" title="In S2 Implementation Details â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S2.4 </span>Reproducibility Statement</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3a" title="In MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S3 </span>Projected and Real Tokens Alignment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.SS1a" title="In S3 Projected and Real Tokens Alignment â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S3.1 </span>UPMC Food-101 dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.SS2a" title="In S3 Projected and Real Tokens Alignment â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S3.2 </span>CMU-MOSI dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4a" title="In MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S4 </span>Evaluation Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.SS1a" title="In S4 Evaluation Metrics â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S4.1 </span>Multimodal Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.SS2a" title="In S4 Evaluation Metrics â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S4.2 </span>Multimodal Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.SS3a" title="In S4 Evaluation Metrics â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S4.3 </span>Multimodal Sentiment Analysis</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Niki Nezakati<sup class="ltx_sup" id="id1.1.id1">1</sup>,
Md Kaykobad Reza<sup class="ltx_sup" id="id2.2.id2">1</sup>, Ameya Patil<sup class="ltx_sup" id="id3.3.id3">2</sup>,
Mashhour Solh<sup class="ltx_sup" id="id4.4.id4">2</sup>,
M. Salman Asif<sup class="ltx_sup" id="id5.5.id5">1</sup>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">Multimodal learning seeks to combine data from multiple input sources to enhance the performance of different downstream tasks. In real-world scenarios, performance can degrade substantially if some input modalities are missing. Existing methods that can handle missing modalities involve custom training or adaptation steps for each input modality combination. These approaches are either tied to specific modalities or become computationally expensive as the number of input modalities increases. In this paper, we propose <span class="ltx_text ltx_font_bold" id="id6.id1.1">M</span>asked <span class="ltx_text ltx_font_bold" id="id6.id1.2">M</span>odality <span class="ltx_text ltx_font_bold" id="id6.id1.3">P</span>rojection (MMP), a method designed to train a single model that is robust to any missing modality scenario. We achieve this by randomly masking a subset of modalities during training and learning to project available input modalities to estimate the tokens for the masked modalities. This approach enables the model to effectively learn to leverage the information from the available modalities to compensate for the missing ones, enhancing missing modality robustness. We conduct a series of experiments with various baseline models and datasets to assess the effectiveness of this strategy. Experiments demonstrate that our approach improves robustness to different missing modality scenarios, outperforming existing methods designed for missing modalities or specific modality combinations.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Multimodal learning (MML) <cite class="ltx_cite ltx_citemacro_citep">(BaltruÅ¡aitis, Ahuja, and Morency <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib2" title="">2018</a>; Xu, Zhu, and Clifton <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib45" title="">2023</a>)</cite> leverages information from multiple input sources to perform better on the underlying task. Incorporating knowledge from diverse input sources has proven to be very effective in enhancing model performance <cite class="ltx_cite ltx_citemacro_citep">(Huang etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib11" title="">2021</a>; Lu <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib22" title="">2024</a>)</cite>. Since these models are generally trained on all input modalities, they rely heavily on the presence of all modalities to perform optimally during test time. In the real-world scenarios, any subset of the modalities can be missing due to sensor malfunction, privacy concerns, or data acquisition constraints. Recent studies have shown that multimodal models show significant performance drop when a subset of input modalities is missing <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib23" title="">2022</a>; Lee etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib16" title="">2023</a>)</cite>. In this paper, we investigate the missing modality issue during test time and show that a single model trained in a robust manner can outperform existing baseline methods in different missing modality scenarios.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="352" id="S1.F1.g1" src="extracted/5897133/figures/mmp_arch.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture of the proposed MMP approach for training a single multimodal model that is robust to missing modalities. Input modalities are passed through embedding layers, generating tokens. For a masked modality <math alttext="i" class="ltx_Math" display="inline" id="S1.F1.2.m1.1"><semantics id="S1.F1.2.m1.1b"><mi id="S1.F1.2.m1.1.1" xref="S1.F1.2.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S1.F1.2.m1.1c"><ci id="S1.F1.2.m1.1.1.cmml" xref="S1.F1.2.m1.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.m1.1d">i</annotation><annotation encoding="application/x-llamapun" id="S1.F1.2.m1.1e">italic_i</annotation></semantics></math>, a projection function utilizes the tokens from the available modalities to generate projected tokens. These projected tokens are then passed to the masked modality branch.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">A number of approaches has been proposed to enhance missing modality robustness for different multimodal tasks. Some of these approaches include utilizing a robust training approach <cite class="ltx_cite ltx_citemacro_citep">(Neverova etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib27" title="">2015</a>; HussenÂ Abdelaziz etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib12" title="">2020</a>)</cite>, modality masking <cite class="ltx_cite ltx_citemacro_citep">(Bachmann etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib1" title="">2022</a>; Shin, Lee, and Kweon <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib31" title="">2023</a>)</cite>, or knowledge distillation <cite class="ltx_cite ltx_citemacro_citep">(Tarvainen and Valpola <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib33" title="">2017</a>; Maheshwari, Liu, and Kira <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib25" title="">2024</a>; Wu etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib43" title="">2024</a>)</cite> during training. These approaches force the model to perform better with available modalities either by masking inputs or from the help of a teacher model. Prompting-based methods learn to compensate for the missing modalities with learnable prompts <cite class="ltx_cite ltx_citemacro_citep">(Lee etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib16" title="">2023</a>; Jang, Wang, and Kim <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib13" title="">2024</a>)</cite>; however, they need to learn one set of prompts for each missing modality scenarios, which scales poorly as the number of input modalities increase. Missing modality imputation is another approach where generative networks like generative adversarial network (GAN) <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib46" title="">2018</a>; Sharma and Hamarneh <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib30" title="">2019</a>)</cite> and variational autoencoder (VAE) <cite class="ltx_cite ltx_citemacro_citep">(Dorent etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib7" title="">2019</a>)</cite> are used to generate missing modalities from available modalities. Training such generative networks to impute missing modality adds extra overhead to the overall process.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we propose <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">M</span>asked <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">M</span>odality <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">P</span>rojection (MMP), a method for training a single multimodal model that is robust to any missing modality scenario. Our approach leverages available modalities to compensate for the missing ones. In particular, we mask out a subset of modalities during training and utilize available modalities to predict the tokens for the masked ones. As illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">1</span></a>, our approach consists of two main stages: (1) modality masking: where a subset of modalities is masked out during training; and (2) modality projection: where available modalities are used to predict the tokens of the masked modalities, referred to as projected tokens. Additionally, we use alignment loss to align the projected tokens and the actual tokens during training. During inference, the projected tokens are used to substitute for the missing modalities.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our proposed MMP method can be integrated into any existing multimodal architecture. Moreover, we do not need to train or adapt the model for each missing modality scenario. Experimental results demonstrate that models trained with MMP show significant performance improvement in different missing modality scenarios. We conducted extensive experiments on three baseline models and five datasets, covering three tasks (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4" title="4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">4</span></a>). The results show that our method significantly improves performance when some modalities are entirely missing (compared to existing methods), while also maintaining competitive performance when all modalities are available. Performance of the MMP approach is also comparable or better than the models that are exclusively trained for each input modality combination.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our main contributions can be summarized as follows.</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Masked modality projection (MMP) is a novel approach to predict missing modality tokens from available modalities and enhance robustness to missing modalities.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">MMP provides significantly improved performance with missing modalities compared to models trained with all modalities. The performance is comparable or better than the networks trained for specific modality combinations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">MMP requires minimal change in the network, which makes it versatile and adaptable across various multimodal tasks, datasets, and models (as demonstrated by our experiments).</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Robust model design</span> is one approach to perform well on missing modality scenarios. <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib35" title="">2023</a>)</cite> designed a model to learn modality specific and modality shared features and impute missing modality from available ones. On the other hand, <cite class="ltx_cite ltx_citemacro_citet">Shin, Lee, and Kweon (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib31" title="">2023</a>)</cite> designed a robust framework based on modality masking and knowledge distillation for RGB-Thermal segmentation. <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib37" title="">2022</a>)</cite> proposed a method to dynamically detect and replace uninformative tokens with informative tokens. <cite class="ltx_cite ltx_citemacro_citet">Choi and Lee (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib5" title="">2019</a>); Fan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib8" title="">2023</a>); Lin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib18" title="">2023</a>)</cite> designed a robust fusion strategies to enhance model robustness in different missing modality situations. For MRI
missing modality task, <cite class="ltx_cite ltx_citemacro_citet">Karimijafarbigloo etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib14" title="">2024</a>)</cite> proposed a Transformer-based approach with adopted co-training strategy. However, these models and fusion strategies are generally designed for a specific tasks. So, it is non-trivial to generalize them for other multimodal tasks.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Robust training approach</span> can make models robust to missing modalities. Modality dropout augmentation has been applied by <cite class="ltx_cite ltx_citemacro_citet">Neverova etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib27" title="">2015</a>)</cite> for multimodal gesture recognition and <cite class="ltx_cite ltx_citemacro_citet">HussenÂ Abdelaziz etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib12" title="">2020</a>)</cite> for generating 3D facial animation. Modality masking based approaches also gained popularity. <cite class="ltx_cite ltx_citemacro_citet">Shin, Lee, and Kweon (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib31" title="">2023</a>)</cite> used complementary random masking and <cite class="ltx_cite ltx_citemacro_citet">Fan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib8" title="">2023</a>)</cite> used partial modality masking for training robust models. <cite class="ltx_cite ltx_citemacro_citet">Bachmann etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib1" title="">2022</a>)</cite> utilized masked autoencoders, <cite class="ltx_cite ltx_citemacro_citet">Ma etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib23" title="">2022</a>)</cite> utilized masked cross attention and <cite class="ltx_cite ltx_citemacro_citet">Hazarika etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib10" title="">2022</a>)</cite> used modality perturbation to make the underlying models robust to missing and corrupted modalities. Though these approaches improve model robustness in missing modality scenarios, they can not compensate the performance drop completely.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Model adaptation</span> is another approach to make models robust to missing scenarios. <cite class="ltx_cite ltx_citemacro_citet">Lee etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib16" title="">2023</a>)</cite> trained one set of learnable prompts for each modality combination and used those learned prompts when modalities got missing during test time. A followup study by <cite class="ltx_cite ltx_citemacro_citet">Jang, Wang, and Kim (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib13" title="">2024</a>)</cite> showed that learning one set of prompts for each input modality is sufficient for comparable performance. <cite class="ltx_cite ltx_citemacro_citet">Reza, Prater-Bennette, and Asif (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib29" title="">2023</a>)</cite> utilized parameter efficient adaptation to build a generic framework to make existing models robust. They validated the effectiveness of their framework on a number of multimodal tasks. The main disadvantage of these approaches is that they require one set of learnable parameters per modality combination.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Generation and knowledge distillation</span> based approaches are also used to enhance model robustness. GAN based generative models were used by <cite class="ltx_cite ltx_citemacro_citet">Yu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib46" title="">2018</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Sharma and Hamarneh (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib30" title="">2019</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib51" title="">2024</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Dorent etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib7" title="">2019</a>)</cite> used variational autoencoders to generate missing modality. <cite class="ltx_cite ltx_citemacro_citet">Qu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib28" title="">2024</a>)</cite> introduced a local diffusion shared-specific autoencoder to the handle missing modality issue in image classification. Studies by <cite class="ltx_cite ltx_citemacro_citet">Woo etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib41" title="">2023</a>)</cite> proposed ActionMAE to generate missing feature vectors for robust action recognition. Knowledge distillation also showed great promise in a number of tasks. <cite class="ltx_cite ltx_citemacro_citet">Shin, Lee, and Kweon (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib31" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Maheshwari, Liu, and Kira (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib25" title="">2024</a>)</cite> utilized knowledge distillation for multimodal segmentation tasks. <cite class="ltx_cite ltx_citemacro_citet">Wu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib43" title="">2024</a>)</cite> used a teacher-student distillation model to combine partially available visual information with auditory information. <cite class="ltx_cite ltx_citemacro_citet">Wei, Luo, and Luo (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib40" title="">2023</a>)</cite> presented a framework that uses a teacher network to transfer comprehensive multimodal information for improving multimodal learning with missing data. Apart from these approaches, policy learning <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib23" title="">2022</a>)</cite>, Bayesian meta-learning <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib24" title="">2021</a>)</cite> and weight-space ensembling <cite class="ltx_cite ltx_citemacro_citep">(Wortsman etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib42" title="">2022</a>)</cite> are also utilized for missing modality robustness. The main drawback of these approaches is that they need to train/utilize another model to generate missing modality or distill knowledge.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">In this paper, our goal is to train a single model that is robust to any missing modality scenario. Our method utilizes available input modalities to generate the tokens for the missing modalities. The model is trained end-to-end without the need for tuning or adapting it for any specific modality combination.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduce Masked Modality Projection (MMP), a novel approach for training a single multimodal model that is robust to missing modalities. In this method, a subset of modalities is randomly masked out during each training iteration. To address the absence of these modalities, we introduce projection functions that learn to map tokens from the available modalities to the missing modality tokens, which we refer to as projected tokens. These projected tokens are aligned with the actual tokens using an alignment loss objective. Finally, the projected tokens are passed to the corresponding branch for the masked modality.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Modality Masking</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Modality masking is a key component of our MMP approach. As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S2" title="2 Related Work â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">2</span></a>, modality dropout augmentation, which randomly zeros out modalities, has shown robustness to missing modalities.
We extend this idea by masking out all the tokens of a random subset of modalities during each training iteration. Instead of feeding zeros, we use available modalities to predict the tokens for the masked modalities. Specifically, at each iteration, a random subset of modalities are selected to be masked (i.e., they are not fed to the model). Our goal is to train the model to predict the tokens of the masked modalities using the available ones.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="338" id="S3.F2.g1" src="extracted/5897133/figures/projection.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Visualization of the modality projection approach. Available modality tokens are processed through cross-attention to update their aggregated tokens. These aggregated tokens are combined with those of the masked modality through another cross-attention step. The resulting cross-modal relationships are used to attend to the actual tokens of the available modalities. The final output tokens are passed through an MLP to generate the projected tokens of the masked modality.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Modality Projection</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">We propose a modality projection approach for masked modality <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_i</annotation></semantics></math>, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.F2" title="Figure 2 â€£ 3.1 Modality Masking â€£ 3 Method â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">2</span></a>. Suppose we have <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_M</annotation></semantics></math> distinct modalities given as input. The embedding layers generate tokens from each input modality as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{T}_{i}=\text{EmbeddingLayer}(\mathbf{I}_{i})," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">ğ“</mi><mi id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mtext id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3a.cmml">EmbeddingLayer</mtext><mo id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">ğˆ</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">ğ“</ci><ci id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.1.3a.cmml" xref="S3.E1.m1.1.1.1.1.1.3"><mtext id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">EmbeddingLayer</mtext></ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">ğˆ</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathbf{T}_{i}=\text{EmbeddingLayer}(\mathbf{I}_{i}),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">bold_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = EmbeddingLayer ( bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.10">where <math alttext="\mathbf{I}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m1.1"><semantics id="S3.SS2.p1.3.m1.1a"><msub id="S3.SS2.p1.3.m1.1.1" xref="S3.SS2.p1.3.m1.1.1.cmml"><mi id="S3.SS2.p1.3.m1.1.1.2" xref="S3.SS2.p1.3.m1.1.1.2.cmml">ğˆ</mi><mi id="S3.SS2.p1.3.m1.1.1.3" xref="S3.SS2.p1.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m1.1b"><apply id="S3.SS2.p1.3.m1.1.1.cmml" xref="S3.SS2.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m1.1.1.1.cmml" xref="S3.SS2.p1.3.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m1.1.1.2.cmml" xref="S3.SS2.p1.3.m1.1.1.2">ğˆ</ci><ci id="S3.SS2.p1.3.m1.1.1.3.cmml" xref="S3.SS2.p1.3.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m1.1c">\mathbf{I}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m1.1d">bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents input modality <math alttext="i\in\{1,2,\ldots,M\}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m2.4"><semantics id="S3.SS2.p1.4.m2.4a"><mrow id="S3.SS2.p1.4.m2.4.5" xref="S3.SS2.p1.4.m2.4.5.cmml"><mi id="S3.SS2.p1.4.m2.4.5.2" xref="S3.SS2.p1.4.m2.4.5.2.cmml">i</mi><mo id="S3.SS2.p1.4.m2.4.5.1" xref="S3.SS2.p1.4.m2.4.5.1.cmml">âˆˆ</mo><mrow id="S3.SS2.p1.4.m2.4.5.3.2" xref="S3.SS2.p1.4.m2.4.5.3.1.cmml"><mo id="S3.SS2.p1.4.m2.4.5.3.2.1" stretchy="false" xref="S3.SS2.p1.4.m2.4.5.3.1.cmml">{</mo><mn id="S3.SS2.p1.4.m2.1.1" xref="S3.SS2.p1.4.m2.1.1.cmml">1</mn><mo id="S3.SS2.p1.4.m2.4.5.3.2.2" xref="S3.SS2.p1.4.m2.4.5.3.1.cmml">,</mo><mn id="S3.SS2.p1.4.m2.2.2" xref="S3.SS2.p1.4.m2.2.2.cmml">2</mn><mo id="S3.SS2.p1.4.m2.4.5.3.2.3" xref="S3.SS2.p1.4.m2.4.5.3.1.cmml">,</mo><mi id="S3.SS2.p1.4.m2.3.3" mathvariant="normal" xref="S3.SS2.p1.4.m2.3.3.cmml">â€¦</mi><mo id="S3.SS2.p1.4.m2.4.5.3.2.4" xref="S3.SS2.p1.4.m2.4.5.3.1.cmml">,</mo><mi id="S3.SS2.p1.4.m2.4.4" xref="S3.SS2.p1.4.m2.4.4.cmml">M</mi><mo id="S3.SS2.p1.4.m2.4.5.3.2.5" stretchy="false" xref="S3.SS2.p1.4.m2.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m2.4b"><apply id="S3.SS2.p1.4.m2.4.5.cmml" xref="S3.SS2.p1.4.m2.4.5"><in id="S3.SS2.p1.4.m2.4.5.1.cmml" xref="S3.SS2.p1.4.m2.4.5.1"></in><ci id="S3.SS2.p1.4.m2.4.5.2.cmml" xref="S3.SS2.p1.4.m2.4.5.2">ğ‘–</ci><set id="S3.SS2.p1.4.m2.4.5.3.1.cmml" xref="S3.SS2.p1.4.m2.4.5.3.2"><cn id="S3.SS2.p1.4.m2.1.1.cmml" type="integer" xref="S3.SS2.p1.4.m2.1.1">1</cn><cn id="S3.SS2.p1.4.m2.2.2.cmml" type="integer" xref="S3.SS2.p1.4.m2.2.2">2</cn><ci id="S3.SS2.p1.4.m2.3.3.cmml" xref="S3.SS2.p1.4.m2.3.3">â€¦</ci><ci id="S3.SS2.p1.4.m2.4.4.cmml" xref="S3.SS2.p1.4.m2.4.4">ğ‘€</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m2.4c">i\in\{1,2,\ldots,M\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m2.4d">italic_i âˆˆ { 1 , 2 , â€¦ , italic_M }</annotation></semantics></math> and <math alttext="\mathbf{T}_{i}\in\mathbb{R}^{N\times d}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m3.1"><semantics id="S3.SS2.p1.5.m3.1a"><mrow id="S3.SS2.p1.5.m3.1.1" xref="S3.SS2.p1.5.m3.1.1.cmml"><msub id="S3.SS2.p1.5.m3.1.1.2" xref="S3.SS2.p1.5.m3.1.1.2.cmml"><mi id="S3.SS2.p1.5.m3.1.1.2.2" xref="S3.SS2.p1.5.m3.1.1.2.2.cmml">ğ“</mi><mi id="S3.SS2.p1.5.m3.1.1.2.3" xref="S3.SS2.p1.5.m3.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p1.5.m3.1.1.1" xref="S3.SS2.p1.5.m3.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p1.5.m3.1.1.3" xref="S3.SS2.p1.5.m3.1.1.3.cmml"><mi id="S3.SS2.p1.5.m3.1.1.3.2" xref="S3.SS2.p1.5.m3.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.p1.5.m3.1.1.3.3" xref="S3.SS2.p1.5.m3.1.1.3.3.cmml"><mi id="S3.SS2.p1.5.m3.1.1.3.3.2" xref="S3.SS2.p1.5.m3.1.1.3.3.2.cmml">N</mi><mo id="S3.SS2.p1.5.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.5.m3.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p1.5.m3.1.1.3.3.3" xref="S3.SS2.p1.5.m3.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m3.1b"><apply id="S3.SS2.p1.5.m3.1.1.cmml" xref="S3.SS2.p1.5.m3.1.1"><in id="S3.SS2.p1.5.m3.1.1.1.cmml" xref="S3.SS2.p1.5.m3.1.1.1"></in><apply id="S3.SS2.p1.5.m3.1.1.2.cmml" xref="S3.SS2.p1.5.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m3.1.1.2.1.cmml" xref="S3.SS2.p1.5.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.5.m3.1.1.2.2.cmml" xref="S3.SS2.p1.5.m3.1.1.2.2">ğ“</ci><ci id="S3.SS2.p1.5.m3.1.1.2.3.cmml" xref="S3.SS2.p1.5.m3.1.1.2.3">ğ‘–</ci></apply><apply id="S3.SS2.p1.5.m3.1.1.3.cmml" xref="S3.SS2.p1.5.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m3.1.1.3.1.cmml" xref="S3.SS2.p1.5.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.5.m3.1.1.3.2.cmml" xref="S3.SS2.p1.5.m3.1.1.3.2">â„</ci><apply id="S3.SS2.p1.5.m3.1.1.3.3.cmml" xref="S3.SS2.p1.5.m3.1.1.3.3"><times id="S3.SS2.p1.5.m3.1.1.3.3.1.cmml" xref="S3.SS2.p1.5.m3.1.1.3.3.1"></times><ci id="S3.SS2.p1.5.m3.1.1.3.3.2.cmml" xref="S3.SS2.p1.5.m3.1.1.3.3.2">ğ‘</ci><ci id="S3.SS2.p1.5.m3.1.1.3.3.3.cmml" xref="S3.SS2.p1.5.m3.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m3.1c">\mathbf{T}_{i}\in\mathbb{R}^{N\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m3.1d">bold_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_N Ã— italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> denotes tokens for modality <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m4.1"><semantics id="S3.SS2.p1.6.m4.1a"><mi id="S3.SS2.p1.6.m4.1.1" xref="S3.SS2.p1.6.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m4.1b"><ci id="S3.SS2.p1.6.m4.1.1.cmml" xref="S3.SS2.p1.6.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m4.1d">italic_i</annotation></semantics></math>, <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m5.1"><semantics id="S3.SS2.p1.7.m5.1a"><mi id="S3.SS2.p1.7.m5.1.1" xref="S3.SS2.p1.7.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m5.1b"><ci id="S3.SS2.p1.7.m5.1.1.cmml" xref="S3.SS2.p1.7.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m5.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m5.1d">italic_N</annotation></semantics></math> is the number of tokens, and <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m6.1"><semantics id="S3.SS2.p1.8.m6.1a"><mi id="S3.SS2.p1.8.m6.1.1" xref="S3.SS2.p1.8.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m6.1b"><ci id="S3.SS2.p1.8.m6.1.1.cmml" xref="S3.SS2.p1.8.m6.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m6.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m6.1d">italic_d</annotation></semantics></math> is the embedding dimension. For simplicity, here we assume <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.p1.9.m7.1"><semantics id="S3.SS2.p1.9.m7.1a"><mi id="S3.SS2.p1.9.m7.1.1" xref="S3.SS2.p1.9.m7.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m7.1b"><ci id="S3.SS2.p1.9.m7.1.1.cmml" xref="S3.SS2.p1.9.m7.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m7.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.9.m7.1d">italic_N</annotation></semantics></math> and <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.p1.10.m8.1"><semantics id="S3.SS2.p1.10.m8.1a"><mi id="S3.SS2.p1.10.m8.1.1" xref="S3.SS2.p1.10.m8.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m8.1b"><ci id="S3.SS2.p1.10.m8.1.1.cmml" xref="S3.SS2.p1.10.m8.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m8.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.10.m8.1d">italic_d</annotation></semantics></math> to be the same across modalities. We will discuss how our method can be extended to handle cases with varying numbers of tokens and embedding dimensions among modalities in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.SS3" title="3.3 Token and Dimension Variability â€£ 3 Method â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.7">We introduce the projection function <math alttext="\textit{{f}}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold-italic" id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2a.cmml">f</mtext><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2a.cmml" xref="S3.SS2.p2.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold-italic" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">f</mtext></ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\textit{{f}}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to predict the tokens of the masked modality <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_i</annotation></semantics></math>. This process begins by utilizing aggregated tokens. Inspired by <cite class="ltx_cite ltx_citemacro_citet">Mo and Morgado (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib26" title="">2024</a>)</cite>, aggregated tokens summarize the modality information into a compact representation; therefore, reducing the computational and storage complexity associated with numerous modality tokens. For each modality <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_i</annotation></semantics></math>, we use eight aggregated tokens <math alttext="\mathbf{\overline{T}}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mover accent="true" id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2.2" xref="S3.SS2.p2.4.m4.1.1.2.2.cmml">ğ“</mi><mo id="S3.SS2.p2.4.m4.1.1.2.1" xref="S3.SS2.p2.4.m4.1.1.2.1.cmml">Â¯</mo></mover><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><apply id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2"><ci id="S3.SS2.p2.4.m4.1.1.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.2.1">Â¯</ci><ci id="S3.SS2.p2.4.m4.1.1.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2.2">ğ“</ci></apply><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\mathbf{\overline{T}}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">overÂ¯ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, initialized randomly as learnable parameters. When modality <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_j</annotation></semantics></math> is available during an iteration, its aggregated tokens <math alttext="\overline{\mathbf{T}}_{j}" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mover accent="true" id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2.2" xref="S3.SS2.p2.6.m6.1.1.2.2.cmml">ğ“</mi><mo id="S3.SS2.p2.6.m6.1.1.2.1" xref="S3.SS2.p2.6.m6.1.1.2.1.cmml">Â¯</mo></mover><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><apply id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2"><ci id="S3.SS2.p2.6.m6.1.1.2.1.cmml" xref="S3.SS2.p2.6.m6.1.1.2.1">Â¯</ci><ci id="S3.SS2.p2.6.m6.1.1.2.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2.2">ğ“</ci></apply><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">\overline{\mathbf{T}}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">overÂ¯ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> are updated by attending to the actual modality tokens <math alttext="\mathbf{T}_{j}" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m7.1"><semantics id="S3.SS2.p2.7.m7.1a"><msub id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">ğ“</mi><mi id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2">ğ“</ci><ci id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\mathbf{T}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m7.1d">bold_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> using multi-head cross-attention. This process is represented as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S4.EGx1">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbf{\overline{T}}_{j}" class="ltx_Math" display="inline" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><msub id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mover accent="true" id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2.cmml">ğ“</mi><mo id="S3.E2.m1.1.1.2.1" xref="S3.E2.m1.1.1.2.1.cmml">Â¯</mo></mover><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><ci id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2.1">Â¯</ci><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">ğ“</ci></apply><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle\mathbf{\overline{T}}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">overÂ¯ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\text{CrossAttention}\left(\mathbf{\overline{T}}_{j},\mathbf{T}_%
{j}\mid j\in\mathcal{A}\right)," class="ltx_Math" display="inline" id="S3.E2.m2.1"><semantics id="S3.E2.m2.1a"><mrow id="S3.E2.m2.1.1.1" xref="S3.E2.m2.1.1.1.1.cmml"><mrow id="S3.E2.m2.1.1.1.1" xref="S3.E2.m2.1.1.1.1.cmml"><mi id="S3.E2.m2.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.3.cmml"></mi><mo id="S3.E2.m2.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m2.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.cmml"><mtext id="S3.E2.m2.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.3a.cmml">CrossAttention</mtext><mo id="S3.E2.m2.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m2.1.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m2.1.1.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m2.1.1.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.3.cmml"><msub id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğ“</mi><mo id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">Â¯</mo></mover><mi id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.cmml"><msub id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">ğ“</mi><mi id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">j</mi></msub><mo id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.1.cmml">âˆ£</mo><mi id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.3.cmml">j</mi></mrow></mrow><mo id="S3.E2.m2.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.3.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m2.1.1.1.1.1.1.1.1.4" xref="S3.E2.m2.1.1.1.1.1.1.1.1.4.cmml">ğ’œ</mi></mrow><mo id="S3.E2.m2.1.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m2.1.1.1.2" xref="S3.E2.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b"><apply id="S3.E2.m2.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1"><eq id="S3.E2.m2.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S3.E2.m2.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.3">absent</csymbol><apply id="S3.E2.m2.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1"><times id="S3.E2.m2.1.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.1.2"></times><ci id="S3.E2.m2.1.1.1.1.1.3a.cmml" xref="S3.E2.m2.1.1.1.1.1.3"><mtext id="S3.E2.m2.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.3">CrossAttention</mtext></ci><apply id="S3.E2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1"><in id="S3.E2.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.3"></in><list id="S3.E2.m2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2"><apply id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2"><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2.1">Â¯</ci><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ“</ci></apply><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘—</ci></apply><apply id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="latexml" id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.1">conditional</csymbol><apply id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2.2">ğ“</ci><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.2.3">ğ‘—</ci></apply><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.2.2.3">ğ‘—</ci></apply></list><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.4">ğ’œ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle=\text{CrossAttention}\left(\mathbf{\overline{T}}_{j},\mathbf{T}_%
{j}\mid j\in\mathcal{A}\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m2.1d">= CrossAttention ( overÂ¯ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , bold_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT âˆ£ italic_j âˆˆ caligraphic_A ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\text{softmax}\left(\frac{\mathbf{\overline{T}}_{j}\mathbf{W}_{%
\text{q}}\mathbf{W}_{\text{k}}^{\top}\mathbf{T}_{j}^{\top}}{\sqrt{d}}\right)%
\mathbf{T}_{j}\mathbf{W}_{\text{v}}," class="ltx_Math" display="inline" id="S3.Ex1.m1.2"><semantics id="S3.Ex1.m1.2a"><mrow id="S3.Ex1.m1.2.2.1" xref="S3.Ex1.m1.2.2.1.1.cmml"><mrow id="S3.Ex1.m1.2.2.1.1" xref="S3.Ex1.m1.2.2.1.1.cmml"><mi id="S3.Ex1.m1.2.2.1.1.2" xref="S3.Ex1.m1.2.2.1.1.2.cmml"></mi><mo id="S3.Ex1.m1.2.2.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m1.2.2.1.1.3" xref="S3.Ex1.m1.2.2.1.1.3.cmml"><mtext id="S3.Ex1.m1.2.2.1.1.3.2" xref="S3.Ex1.m1.2.2.1.1.3.2a.cmml">softmax</mtext><mo id="S3.Ex1.m1.2.2.1.1.3.1" xref="S3.Ex1.m1.2.2.1.1.3.1.cmml">â¢</mo><mrow id="S3.Ex1.m1.2.2.1.1.3.3.2" xref="S3.Ex1.m1.1.1.cmml"><mo id="S3.Ex1.m1.2.2.1.1.3.3.2.1" xref="S3.Ex1.m1.1.1.cmml">(</mo><mstyle displaystyle="true" id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><mfrac id="S3.Ex1.m1.1.1a" xref="S3.Ex1.m1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml"><msub id="S3.Ex1.m1.1.1.2.2" xref="S3.Ex1.m1.1.1.2.2.cmml"><mover accent="true" id="S3.Ex1.m1.1.1.2.2.2" xref="S3.Ex1.m1.1.1.2.2.2.cmml"><mi id="S3.Ex1.m1.1.1.2.2.2.2" xref="S3.Ex1.m1.1.1.2.2.2.2.cmml">ğ“</mi><mo id="S3.Ex1.m1.1.1.2.2.2.1" xref="S3.Ex1.m1.1.1.2.2.2.1.cmml">Â¯</mo></mover><mi id="S3.Ex1.m1.1.1.2.2.3" xref="S3.Ex1.m1.1.1.2.2.3.cmml">j</mi></msub><mo id="S3.Ex1.m1.1.1.2.1" xref="S3.Ex1.m1.1.1.2.1.cmml">â¢</mo><msub id="S3.Ex1.m1.1.1.2.3" xref="S3.Ex1.m1.1.1.2.3.cmml"><mi id="S3.Ex1.m1.1.1.2.3.2" xref="S3.Ex1.m1.1.1.2.3.2.cmml">ğ–</mi><mtext id="S3.Ex1.m1.1.1.2.3.3" xref="S3.Ex1.m1.1.1.2.3.3a.cmml">q</mtext></msub><mo id="S3.Ex1.m1.1.1.2.1a" xref="S3.Ex1.m1.1.1.2.1.cmml">â¢</mo><msubsup id="S3.Ex1.m1.1.1.2.4" xref="S3.Ex1.m1.1.1.2.4.cmml"><mi id="S3.Ex1.m1.1.1.2.4.2.2" xref="S3.Ex1.m1.1.1.2.4.2.2.cmml">ğ–</mi><mtext id="S3.Ex1.m1.1.1.2.4.2.3" xref="S3.Ex1.m1.1.1.2.4.2.3a.cmml">k</mtext><mo id="S3.Ex1.m1.1.1.2.4.3" xref="S3.Ex1.m1.1.1.2.4.3.cmml">âŠ¤</mo></msubsup><mo id="S3.Ex1.m1.1.1.2.1b" xref="S3.Ex1.m1.1.1.2.1.cmml">â¢</mo><msubsup id="S3.Ex1.m1.1.1.2.5" xref="S3.Ex1.m1.1.1.2.5.cmml"><mi id="S3.Ex1.m1.1.1.2.5.2.2" xref="S3.Ex1.m1.1.1.2.5.2.2.cmml">ğ“</mi><mi id="S3.Ex1.m1.1.1.2.5.2.3" xref="S3.Ex1.m1.1.1.2.5.2.3.cmml">j</mi><mo id="S3.Ex1.m1.1.1.2.5.3" xref="S3.Ex1.m1.1.1.2.5.3.cmml">âŠ¤</mo></msubsup></mrow><msqrt id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.3.2" xref="S3.Ex1.m1.1.1.3.2.cmml">d</mi></msqrt></mfrac></mstyle><mo id="S3.Ex1.m1.2.2.1.1.3.3.2.2" xref="S3.Ex1.m1.1.1.cmml">)</mo></mrow><mo id="S3.Ex1.m1.2.2.1.1.3.1a" xref="S3.Ex1.m1.2.2.1.1.3.1.cmml">â¢</mo><msub id="S3.Ex1.m1.2.2.1.1.3.4" xref="S3.Ex1.m1.2.2.1.1.3.4.cmml"><mi id="S3.Ex1.m1.2.2.1.1.3.4.2" xref="S3.Ex1.m1.2.2.1.1.3.4.2.cmml">ğ“</mi><mi id="S3.Ex1.m1.2.2.1.1.3.4.3" xref="S3.Ex1.m1.2.2.1.1.3.4.3.cmml">j</mi></msub><mo id="S3.Ex1.m1.2.2.1.1.3.1b" xref="S3.Ex1.m1.2.2.1.1.3.1.cmml">â¢</mo><msub id="S3.Ex1.m1.2.2.1.1.3.5" xref="S3.Ex1.m1.2.2.1.1.3.5.cmml"><mi id="S3.Ex1.m1.2.2.1.1.3.5.2" xref="S3.Ex1.m1.2.2.1.1.3.5.2.cmml">ğ–</mi><mtext id="S3.Ex1.m1.2.2.1.1.3.5.3" xref="S3.Ex1.m1.2.2.1.1.3.5.3a.cmml">v</mtext></msub></mrow></mrow><mo id="S3.Ex1.m1.2.2.1.2" xref="S3.Ex1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.2b"><apply id="S3.Ex1.m1.2.2.1.1.cmml" xref="S3.Ex1.m1.2.2.1"><eq id="S3.Ex1.m1.2.2.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1"></eq><csymbol cd="latexml" id="S3.Ex1.m1.2.2.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.2">absent</csymbol><apply id="S3.Ex1.m1.2.2.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.3"><times id="S3.Ex1.m1.2.2.1.1.3.1.cmml" xref="S3.Ex1.m1.2.2.1.1.3.1"></times><ci id="S3.Ex1.m1.2.2.1.1.3.2a.cmml" xref="S3.Ex1.m1.2.2.1.1.3.2"><mtext id="S3.Ex1.m1.2.2.1.1.3.2.cmml" xref="S3.Ex1.m1.2.2.1.1.3.2">softmax</mtext></ci><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.3.3.2"><divide id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.3.3.2"></divide><apply id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2"><times id="S3.Ex1.m1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.2.1"></times><apply id="S3.Ex1.m1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.2.2.1.cmml" xref="S3.Ex1.m1.1.1.2.2">subscript</csymbol><apply id="S3.Ex1.m1.1.1.2.2.2.cmml" xref="S3.Ex1.m1.1.1.2.2.2"><ci id="S3.Ex1.m1.1.1.2.2.2.1.cmml" xref="S3.Ex1.m1.1.1.2.2.2.1">Â¯</ci><ci id="S3.Ex1.m1.1.1.2.2.2.2.cmml" xref="S3.Ex1.m1.1.1.2.2.2.2">ğ“</ci></apply><ci id="S3.Ex1.m1.1.1.2.2.3.cmml" xref="S3.Ex1.m1.1.1.2.2.3">ğ‘—</ci></apply><apply id="S3.Ex1.m1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.2.3.1.cmml" xref="S3.Ex1.m1.1.1.2.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.2.3.2.cmml" xref="S3.Ex1.m1.1.1.2.3.2">ğ–</ci><ci id="S3.Ex1.m1.1.1.2.3.3a.cmml" xref="S3.Ex1.m1.1.1.2.3.3"><mtext id="S3.Ex1.m1.1.1.2.3.3.cmml" mathsize="70%" xref="S3.Ex1.m1.1.1.2.3.3">q</mtext></ci></apply><apply id="S3.Ex1.m1.1.1.2.4.cmml" xref="S3.Ex1.m1.1.1.2.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.2.4.1.cmml" xref="S3.Ex1.m1.1.1.2.4">superscript</csymbol><apply id="S3.Ex1.m1.1.1.2.4.2.cmml" xref="S3.Ex1.m1.1.1.2.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.2.4.2.1.cmml" xref="S3.Ex1.m1.1.1.2.4">subscript</csymbol><ci id="S3.Ex1.m1.1.1.2.4.2.2.cmml" xref="S3.Ex1.m1.1.1.2.4.2.2">ğ–</ci><ci id="S3.Ex1.m1.1.1.2.4.2.3a.cmml" xref="S3.Ex1.m1.1.1.2.4.2.3"><mtext id="S3.Ex1.m1.1.1.2.4.2.3.cmml" mathsize="70%" xref="S3.Ex1.m1.1.1.2.4.2.3">k</mtext></ci></apply><csymbol cd="latexml" id="S3.Ex1.m1.1.1.2.4.3.cmml" xref="S3.Ex1.m1.1.1.2.4.3">top</csymbol></apply><apply id="S3.Ex1.m1.1.1.2.5.cmml" xref="S3.Ex1.m1.1.1.2.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.2.5.1.cmml" xref="S3.Ex1.m1.1.1.2.5">superscript</csymbol><apply id="S3.Ex1.m1.1.1.2.5.2.cmml" xref="S3.Ex1.m1.1.1.2.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.2.5.2.1.cmml" xref="S3.Ex1.m1.1.1.2.5">subscript</csymbol><ci id="S3.Ex1.m1.1.1.2.5.2.2.cmml" xref="S3.Ex1.m1.1.1.2.5.2.2">ğ“</ci><ci id="S3.Ex1.m1.1.1.2.5.2.3.cmml" xref="S3.Ex1.m1.1.1.2.5.2.3">ğ‘—</ci></apply><csymbol cd="latexml" id="S3.Ex1.m1.1.1.2.5.3.cmml" xref="S3.Ex1.m1.1.1.2.5.3">top</csymbol></apply></apply><apply id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3"><root id="S3.Ex1.m1.1.1.3a.cmml" xref="S3.Ex1.m1.1.1.3"></root><ci id="S3.Ex1.m1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.3.2">ğ‘‘</ci></apply></apply><apply id="S3.Ex1.m1.2.2.1.1.3.4.cmml" xref="S3.Ex1.m1.2.2.1.1.3.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.3.4.1.cmml" xref="S3.Ex1.m1.2.2.1.1.3.4">subscript</csymbol><ci id="S3.Ex1.m1.2.2.1.1.3.4.2.cmml" xref="S3.Ex1.m1.2.2.1.1.3.4.2">ğ“</ci><ci id="S3.Ex1.m1.2.2.1.1.3.4.3.cmml" xref="S3.Ex1.m1.2.2.1.1.3.4.3">ğ‘—</ci></apply><apply id="S3.Ex1.m1.2.2.1.1.3.5.cmml" xref="S3.Ex1.m1.2.2.1.1.3.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.3.5.1.cmml" xref="S3.Ex1.m1.2.2.1.1.3.5">subscript</csymbol><ci id="S3.Ex1.m1.2.2.1.1.3.5.2.cmml" xref="S3.Ex1.m1.2.2.1.1.3.5.2">ğ–</ci><ci id="S3.Ex1.m1.2.2.1.1.3.5.3a.cmml" xref="S3.Ex1.m1.2.2.1.1.3.5.3"><mtext id="S3.Ex1.m1.2.2.1.1.3.5.3.cmml" mathsize="70%" xref="S3.Ex1.m1.2.2.1.1.3.5.3">v</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.2c">\displaystyle=\text{softmax}\left(\frac{\mathbf{\overline{T}}_{j}\mathbf{W}_{%
\text{q}}\mathbf{W}_{\text{k}}^{\top}\mathbf{T}_{j}^{\top}}{\sqrt{d}}\right)%
\mathbf{T}_{j}\mathbf{W}_{\text{v}},</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.2d">= softmax ( divide start_ARG overÂ¯ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT q end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT bold_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d end_ARG end_ARG ) bold_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT v end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p2.11">where <math alttext="\mathbf{T}_{j}\in\mathbb{R}^{N\times d}" class="ltx_Math" display="inline" id="S3.SS2.p2.8.m1.1"><semantics id="S3.SS2.p2.8.m1.1a"><mrow id="S3.SS2.p2.8.m1.1.1" xref="S3.SS2.p2.8.m1.1.1.cmml"><msub id="S3.SS2.p2.8.m1.1.1.2" xref="S3.SS2.p2.8.m1.1.1.2.cmml"><mi id="S3.SS2.p2.8.m1.1.1.2.2" xref="S3.SS2.p2.8.m1.1.1.2.2.cmml">ğ“</mi><mi id="S3.SS2.p2.8.m1.1.1.2.3" xref="S3.SS2.p2.8.m1.1.1.2.3.cmml">j</mi></msub><mo id="S3.SS2.p2.8.m1.1.1.1" xref="S3.SS2.p2.8.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p2.8.m1.1.1.3" xref="S3.SS2.p2.8.m1.1.1.3.cmml"><mi id="S3.SS2.p2.8.m1.1.1.3.2" xref="S3.SS2.p2.8.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.p2.8.m1.1.1.3.3" xref="S3.SS2.p2.8.m1.1.1.3.3.cmml"><mi id="S3.SS2.p2.8.m1.1.1.3.3.2" xref="S3.SS2.p2.8.m1.1.1.3.3.2.cmml">N</mi><mo id="S3.SS2.p2.8.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.8.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p2.8.m1.1.1.3.3.3" xref="S3.SS2.p2.8.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m1.1b"><apply id="S3.SS2.p2.8.m1.1.1.cmml" xref="S3.SS2.p2.8.m1.1.1"><in id="S3.SS2.p2.8.m1.1.1.1.cmml" xref="S3.SS2.p2.8.m1.1.1.1"></in><apply id="S3.SS2.p2.8.m1.1.1.2.cmml" xref="S3.SS2.p2.8.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m1.1.1.2.1.cmml" xref="S3.SS2.p2.8.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.8.m1.1.1.2.2.cmml" xref="S3.SS2.p2.8.m1.1.1.2.2">ğ“</ci><ci id="S3.SS2.p2.8.m1.1.1.2.3.cmml" xref="S3.SS2.p2.8.m1.1.1.2.3">ğ‘—</ci></apply><apply id="S3.SS2.p2.8.m1.1.1.3.cmml" xref="S3.SS2.p2.8.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m1.1.1.3.1.cmml" xref="S3.SS2.p2.8.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.8.m1.1.1.3.2.cmml" xref="S3.SS2.p2.8.m1.1.1.3.2">â„</ci><apply id="S3.SS2.p2.8.m1.1.1.3.3.cmml" xref="S3.SS2.p2.8.m1.1.1.3.3"><times id="S3.SS2.p2.8.m1.1.1.3.3.1.cmml" xref="S3.SS2.p2.8.m1.1.1.3.3.1"></times><ci id="S3.SS2.p2.8.m1.1.1.3.3.2.cmml" xref="S3.SS2.p2.8.m1.1.1.3.3.2">ğ‘</ci><ci id="S3.SS2.p2.8.m1.1.1.3.3.3.cmml" xref="S3.SS2.p2.8.m1.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m1.1c">\mathbf{T}_{j}\in\mathbb{R}^{N\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.8.m1.1d">bold_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_N Ã— italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{\overline{T}}_{j}\in\mathbb{R}^{8\times d}" class="ltx_Math" display="inline" id="S3.SS2.p2.9.m2.1"><semantics id="S3.SS2.p2.9.m2.1a"><mrow id="S3.SS2.p2.9.m2.1.1" xref="S3.SS2.p2.9.m2.1.1.cmml"><msub id="S3.SS2.p2.9.m2.1.1.2" xref="S3.SS2.p2.9.m2.1.1.2.cmml"><mover accent="true" id="S3.SS2.p2.9.m2.1.1.2.2" xref="S3.SS2.p2.9.m2.1.1.2.2.cmml"><mi id="S3.SS2.p2.9.m2.1.1.2.2.2" xref="S3.SS2.p2.9.m2.1.1.2.2.2.cmml">ğ“</mi><mo id="S3.SS2.p2.9.m2.1.1.2.2.1" xref="S3.SS2.p2.9.m2.1.1.2.2.1.cmml">Â¯</mo></mover><mi id="S3.SS2.p2.9.m2.1.1.2.3" xref="S3.SS2.p2.9.m2.1.1.2.3.cmml">j</mi></msub><mo id="S3.SS2.p2.9.m2.1.1.1" xref="S3.SS2.p2.9.m2.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p2.9.m2.1.1.3" xref="S3.SS2.p2.9.m2.1.1.3.cmml"><mi id="S3.SS2.p2.9.m2.1.1.3.2" xref="S3.SS2.p2.9.m2.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.p2.9.m2.1.1.3.3" xref="S3.SS2.p2.9.m2.1.1.3.3.cmml"><mn id="S3.SS2.p2.9.m2.1.1.3.3.2" xref="S3.SS2.p2.9.m2.1.1.3.3.2.cmml">8</mn><mo id="S3.SS2.p2.9.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.9.m2.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p2.9.m2.1.1.3.3.3" xref="S3.SS2.p2.9.m2.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m2.1b"><apply id="S3.SS2.p2.9.m2.1.1.cmml" xref="S3.SS2.p2.9.m2.1.1"><in id="S3.SS2.p2.9.m2.1.1.1.cmml" xref="S3.SS2.p2.9.m2.1.1.1"></in><apply id="S3.SS2.p2.9.m2.1.1.2.cmml" xref="S3.SS2.p2.9.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m2.1.1.2.1.cmml" xref="S3.SS2.p2.9.m2.1.1.2">subscript</csymbol><apply id="S3.SS2.p2.9.m2.1.1.2.2.cmml" xref="S3.SS2.p2.9.m2.1.1.2.2"><ci id="S3.SS2.p2.9.m2.1.1.2.2.1.cmml" xref="S3.SS2.p2.9.m2.1.1.2.2.1">Â¯</ci><ci id="S3.SS2.p2.9.m2.1.1.2.2.2.cmml" xref="S3.SS2.p2.9.m2.1.1.2.2.2">ğ“</ci></apply><ci id="S3.SS2.p2.9.m2.1.1.2.3.cmml" xref="S3.SS2.p2.9.m2.1.1.2.3">ğ‘—</ci></apply><apply id="S3.SS2.p2.9.m2.1.1.3.cmml" xref="S3.SS2.p2.9.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m2.1.1.3.1.cmml" xref="S3.SS2.p2.9.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.9.m2.1.1.3.2.cmml" xref="S3.SS2.p2.9.m2.1.1.3.2">â„</ci><apply id="S3.SS2.p2.9.m2.1.1.3.3.cmml" xref="S3.SS2.p2.9.m2.1.1.3.3"><times id="S3.SS2.p2.9.m2.1.1.3.3.1.cmml" xref="S3.SS2.p2.9.m2.1.1.3.3.1"></times><cn id="S3.SS2.p2.9.m2.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.p2.9.m2.1.1.3.3.2">8</cn><ci id="S3.SS2.p2.9.m2.1.1.3.3.3.cmml" xref="S3.SS2.p2.9.m2.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m2.1c">\mathbf{\overline{T}}_{j}\in\mathbb{R}^{8\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.9.m2.1d">overÂ¯ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT 8 Ã— italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> denote tokens and aggregated tokens of modality <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p2.10.m3.1"><semantics id="S3.SS2.p2.10.m3.1a"><mi id="S3.SS2.p2.10.m3.1.1" xref="S3.SS2.p2.10.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m3.1b"><ci id="S3.SS2.p2.10.m3.1.1.cmml" xref="S3.SS2.p2.10.m3.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.10.m3.1d">italic_j</annotation></semantics></math>, respectively, and <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S3.SS2.p2.11.m4.1"><semantics id="S3.SS2.p2.11.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.11.m4.1.1" xref="S3.SS2.p2.11.m4.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m4.1b"><ci id="S3.SS2.p2.11.m4.1.1.cmml" xref="S3.SS2.p2.11.m4.1.1">ğ’œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m4.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.11.m4.1d">caligraphic_A</annotation></semantics></math> is the set of available modalities.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.5"><math alttext="\mathbf{W}_{\text{q}}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">ğ–</mi><mtext id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3a.cmml">q</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">ğ–</ci><ci id="S3.SS2.p3.1.m1.1.1.3a.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><mtext id="S3.SS2.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p3.1.m1.1.1.3">q</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathbf{W}_{\text{q}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">bold_W start_POSTSUBSCRIPT q end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\mathbf{W}_{\text{k}}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">ğ–</mi><mtext id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3a.cmml">k</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ğ–</ci><ci id="S3.SS2.p3.2.m2.1.1.3a.cmml" xref="S3.SS2.p3.2.m2.1.1.3"><mtext id="S3.SS2.p3.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p3.2.m2.1.1.3">k</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\mathbf{W}_{\text{k}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">bold_W start_POSTSUBSCRIPT k end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\mathbf{W}_{\text{v}}" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><msub id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">ğ–</mi><mtext id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3a.cmml">v</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">ğ–</ci><ci id="S3.SS2.p3.3.m3.1.1.3a.cmml" xref="S3.SS2.p3.3.m3.1.1.3"><mtext id="S3.SS2.p3.3.m3.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p3.3.m3.1.1.3">v</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\mathbf{W}_{\text{v}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">bold_W start_POSTSUBSCRIPT v end_POSTSUBSCRIPT</annotation></semantics></math> are learnable weight matrices for the query, key, and value projections, respectively. The aggregated tokens are only updated when a modality is available; if modality <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">italic_i</annotation></semantics></math> is masked at an iteration, its aggregated tokens <math alttext="\mathbf{\overline{T}}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m5.1"><semantics id="S3.SS2.p3.5.m5.1a"><msub id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mover accent="true" id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml"><mi id="S3.SS2.p3.5.m5.1.1.2.2" xref="S3.SS2.p3.5.m5.1.1.2.2.cmml">ğ“</mi><mo id="S3.SS2.p3.5.m5.1.1.2.1" xref="S3.SS2.p3.5.m5.1.1.2.1.cmml">Â¯</mo></mover><mi id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">subscript</csymbol><apply id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2"><ci id="S3.SS2.p3.5.m5.1.1.2.1.cmml" xref="S3.SS2.p3.5.m5.1.1.2.1">Â¯</ci><ci id="S3.SS2.p3.5.m5.1.1.2.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2.2">ğ“</ci></apply><ci id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">\mathbf{\overline{T}}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.m5.1d">overÂ¯ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are left unchanged.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">When dealing with missing modalities, for each modality <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_i</annotation></semantics></math> that is missing (masked during training), cross-attention is performed between the aggregated tokens of the missing modality and the aggregated tokens of each available modality separately. This step captures the relationships between the missing modality and each available modality, allowing the model to approximate the missing modality information based on available data. Specifically</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{X}_{ij}=\text{CrossAttention}(\mathbf{\overline{T}}_{i},\mathbf{%
\overline{T}}_{j})," class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.4.cmml"><mi id="S3.E3.m1.1.1.1.1.4.2" xref="S3.E3.m1.1.1.1.1.4.2.cmml">ğ—</mi><mrow id="S3.E3.m1.1.1.1.1.4.3" xref="S3.E3.m1.1.1.1.1.4.3.cmml"><mi id="S3.E3.m1.1.1.1.1.4.3.2" xref="S3.E3.m1.1.1.1.1.4.3.2.cmml">i</mi><mo id="S3.E3.m1.1.1.1.1.4.3.1" xref="S3.E3.m1.1.1.1.1.4.3.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.1.1.4.3.3" xref="S3.E3.m1.1.1.1.1.4.3.3.cmml">j</mi></mrow></msub><mo id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><mtext id="S3.E3.m1.1.1.1.1.2.4" xref="S3.E3.m1.1.1.1.1.2.4a.cmml">CrossAttention</mtext><mo id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.cmml">â¢</mo><mrow id="S3.E3.m1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml"><mo id="S3.E3.m1.1.1.1.1.2.2.2.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml">ğ“</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml">Â¯</mo></mover><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E3.m1.1.1.1.1.2.2.2.4" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E3.m1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.2.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.2.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.2.cmml">ğ“</mi><mo id="S3.E3.m1.1.1.1.1.2.2.2.2.2.1" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.1.cmml">Â¯</mo></mover><mi id="S3.E3.m1.1.1.1.1.2.2.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.2.2.3.cmml">j</mi></msub><mo id="S3.E3.m1.1.1.1.1.2.2.2.5" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"></eq><apply id="S3.E3.m1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.4.1.cmml" xref="S3.E3.m1.1.1.1.1.4">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.4.2.cmml" xref="S3.E3.m1.1.1.1.1.4.2">ğ—</ci><apply id="S3.E3.m1.1.1.1.1.4.3.cmml" xref="S3.E3.m1.1.1.1.1.4.3"><times id="S3.E3.m1.1.1.1.1.4.3.1.cmml" xref="S3.E3.m1.1.1.1.1.4.3.1"></times><ci id="S3.E3.m1.1.1.1.1.4.3.2.cmml" xref="S3.E3.m1.1.1.1.1.4.3.2">ğ‘–</ci><ci id="S3.E3.m1.1.1.1.1.4.3.3.cmml" xref="S3.E3.m1.1.1.1.1.4.3.3">ğ‘—</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><times id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3"></times><ci id="S3.E3.m1.1.1.1.1.2.4a.cmml" xref="S3.E3.m1.1.1.1.1.2.4"><mtext id="S3.E3.m1.1.1.1.1.2.4.cmml" xref="S3.E3.m1.1.1.1.1.2.4">CrossAttention</mtext></ci><interval closure="open" id="S3.E3.m1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2"><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2"><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1">Â¯</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2">ğ“</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E3.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2"><ci id="S3.E3.m1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.1">Â¯</ci><ci id="S3.E3.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.2">ğ“</ci></apply><ci id="S3.E3.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.3">ğ‘—</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\mathbf{X}_{ij}=\text{CrossAttention}(\mathbf{\overline{T}}_{i},\mathbf{%
\overline{T}}_{j}),</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">bold_X start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = CrossAttention ( overÂ¯ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , overÂ¯ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p4.7">where <math alttext="\mathbf{\overline{T}}_{i},\mathbf{\overline{T}}_{j}" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m1.2"><semantics id="S3.SS2.p4.2.m1.2a"><mrow id="S3.SS2.p4.2.m1.2.2.2" xref="S3.SS2.p4.2.m1.2.2.3.cmml"><msub id="S3.SS2.p4.2.m1.1.1.1.1" xref="S3.SS2.p4.2.m1.1.1.1.1.cmml"><mover accent="true" id="S3.SS2.p4.2.m1.1.1.1.1.2" xref="S3.SS2.p4.2.m1.1.1.1.1.2.cmml"><mi id="S3.SS2.p4.2.m1.1.1.1.1.2.2" xref="S3.SS2.p4.2.m1.1.1.1.1.2.2.cmml">ğ“</mi><mo id="S3.SS2.p4.2.m1.1.1.1.1.2.1" xref="S3.SS2.p4.2.m1.1.1.1.1.2.1.cmml">Â¯</mo></mover><mi id="S3.SS2.p4.2.m1.1.1.1.1.3" xref="S3.SS2.p4.2.m1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p4.2.m1.2.2.2.3" xref="S3.SS2.p4.2.m1.2.2.3.cmml">,</mo><msub id="S3.SS2.p4.2.m1.2.2.2.2" xref="S3.SS2.p4.2.m1.2.2.2.2.cmml"><mover accent="true" id="S3.SS2.p4.2.m1.2.2.2.2.2" xref="S3.SS2.p4.2.m1.2.2.2.2.2.cmml"><mi id="S3.SS2.p4.2.m1.2.2.2.2.2.2" xref="S3.SS2.p4.2.m1.2.2.2.2.2.2.cmml">ğ“</mi><mo id="S3.SS2.p4.2.m1.2.2.2.2.2.1" xref="S3.SS2.p4.2.m1.2.2.2.2.2.1.cmml">Â¯</mo></mover><mi id="S3.SS2.p4.2.m1.2.2.2.2.3" xref="S3.SS2.p4.2.m1.2.2.2.2.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m1.2b"><list id="S3.SS2.p4.2.m1.2.2.3.cmml" xref="S3.SS2.p4.2.m1.2.2.2"><apply id="S3.SS2.p4.2.m1.1.1.1.1.cmml" xref="S3.SS2.p4.2.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m1.1.1.1.1.1.cmml" xref="S3.SS2.p4.2.m1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.p4.2.m1.1.1.1.1.2.cmml" xref="S3.SS2.p4.2.m1.1.1.1.1.2"><ci id="S3.SS2.p4.2.m1.1.1.1.1.2.1.cmml" xref="S3.SS2.p4.2.m1.1.1.1.1.2.1">Â¯</ci><ci id="S3.SS2.p4.2.m1.1.1.1.1.2.2.cmml" xref="S3.SS2.p4.2.m1.1.1.1.1.2.2">ğ“</ci></apply><ci id="S3.SS2.p4.2.m1.1.1.1.1.3.cmml" xref="S3.SS2.p4.2.m1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS2.p4.2.m1.2.2.2.2.cmml" xref="S3.SS2.p4.2.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m1.2.2.2.2.1.cmml" xref="S3.SS2.p4.2.m1.2.2.2.2">subscript</csymbol><apply id="S3.SS2.p4.2.m1.2.2.2.2.2.cmml" xref="S3.SS2.p4.2.m1.2.2.2.2.2"><ci id="S3.SS2.p4.2.m1.2.2.2.2.2.1.cmml" xref="S3.SS2.p4.2.m1.2.2.2.2.2.1">Â¯</ci><ci id="S3.SS2.p4.2.m1.2.2.2.2.2.2.cmml" xref="S3.SS2.p4.2.m1.2.2.2.2.2.2">ğ“</ci></apply><ci id="S3.SS2.p4.2.m1.2.2.2.2.3.cmml" xref="S3.SS2.p4.2.m1.2.2.2.2.3">ğ‘—</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m1.2c">\mathbf{\overline{T}}_{i},\mathbf{\overline{T}}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m1.2d">overÂ¯ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , overÂ¯ start_ARG bold_T end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> are the aggregated tokens of missing modality <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m2.1"><semantics id="S3.SS2.p4.3.m2.1a"><mi id="S3.SS2.p4.3.m2.1.1" xref="S3.SS2.p4.3.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m2.1b"><ci id="S3.SS2.p4.3.m2.1.1.cmml" xref="S3.SS2.p4.3.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m2.1d">italic_i</annotation></semantics></math>, available modality <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m3.1"><semantics id="S3.SS2.p4.4.m3.1a"><mi id="S3.SS2.p4.4.m3.1.1" xref="S3.SS2.p4.4.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m3.1b"><ci id="S3.SS2.p4.4.m3.1.1.cmml" xref="S3.SS2.p4.4.m3.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.4.m3.1d">italic_j</annotation></semantics></math>, respectively. <math alttext="\mathbf{X}_{ij}" class="ltx_Math" display="inline" id="S3.SS2.p4.5.m4.1"><semantics id="S3.SS2.p4.5.m4.1a"><msub id="S3.SS2.p4.5.m4.1.1" xref="S3.SS2.p4.5.m4.1.1.cmml"><mi id="S3.SS2.p4.5.m4.1.1.2" xref="S3.SS2.p4.5.m4.1.1.2.cmml">ğ—</mi><mrow id="S3.SS2.p4.5.m4.1.1.3" xref="S3.SS2.p4.5.m4.1.1.3.cmml"><mi id="S3.SS2.p4.5.m4.1.1.3.2" xref="S3.SS2.p4.5.m4.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p4.5.m4.1.1.3.1" xref="S3.SS2.p4.5.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p4.5.m4.1.1.3.3" xref="S3.SS2.p4.5.m4.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m4.1b"><apply id="S3.SS2.p4.5.m4.1.1.cmml" xref="S3.SS2.p4.5.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.5.m4.1.1.1.cmml" xref="S3.SS2.p4.5.m4.1.1">subscript</csymbol><ci id="S3.SS2.p4.5.m4.1.1.2.cmml" xref="S3.SS2.p4.5.m4.1.1.2">ğ—</ci><apply id="S3.SS2.p4.5.m4.1.1.3.cmml" xref="S3.SS2.p4.5.m4.1.1.3"><times id="S3.SS2.p4.5.m4.1.1.3.1.cmml" xref="S3.SS2.p4.5.m4.1.1.3.1"></times><ci id="S3.SS2.p4.5.m4.1.1.3.2.cmml" xref="S3.SS2.p4.5.m4.1.1.3.2">ğ‘–</ci><ci id="S3.SS2.p4.5.m4.1.1.3.3.cmml" xref="S3.SS2.p4.5.m4.1.1.3.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m4.1c">\mathbf{X}_{ij}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.5.m4.1d">bold_X start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> represents the attended tokens for available modality <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p4.6.m5.1"><semantics id="S3.SS2.p4.6.m5.1a"><mi id="S3.SS2.p4.6.m5.1.1" xref="S3.SS2.p4.6.m5.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m5.1b"><ci id="S3.SS2.p4.6.m5.1.1.cmml" xref="S3.SS2.p4.6.m5.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m5.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.6.m5.1d">italic_j</annotation></semantics></math> in relation to missing modality <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p4.7.m6.1"><semantics id="S3.SS2.p4.7.m6.1a"><mi id="S3.SS2.p4.7.m6.1.1" xref="S3.SS2.p4.7.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m6.1b"><ci id="S3.SS2.p4.7.m6.1.1.cmml" xref="S3.SS2.p4.7.m6.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m6.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.7.m6.1d">italic_i</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.3">We then utilize <math alttext="\mathbf{X}_{ij}" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><msub id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">ğ—</mi><mrow id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.3.2" xref="S3.SS2.p5.1.m1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p5.1.m1.1.1.3.1" xref="S3.SS2.p5.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p5.1.m1.1.1.3.3" xref="S3.SS2.p5.1.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">ğ—</ci><apply id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3"><times id="S3.SS2.p5.1.m1.1.1.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.3.1"></times><ci id="S3.SS2.p5.1.m1.1.1.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.3.2">ğ‘–</ci><ci id="S3.SS2.p5.1.m1.1.1.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\mathbf{X}_{ij}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">bold_X start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> in cross-attention with the original tokens <math alttext="\mathbf{T}_{j}" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><msub id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">ğ“</mi><mi id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">ğ“</ci><ci id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\mathbf{T}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">bold_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> of availble modality <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p5.3.m3.1"><semantics id="S3.SS2.p5.3.m3.1a"><mi id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><ci id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.3.m3.1d">italic_j</annotation></semantics></math> to ensure that the relational information is integrated with the specific features of each available modality. The refinement process is expressed as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{T}_{\text{attended}_{j}}=\text{CrossAttention}(\mathbf{T}_{j},\mathbf{%
X}_{ij})," class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.4.cmml"><mi id="S3.E4.m1.1.1.1.1.4.2" xref="S3.E4.m1.1.1.1.1.4.2.cmml">ğ“</mi><msub id="S3.E4.m1.1.1.1.1.4.3" xref="S3.E4.m1.1.1.1.1.4.3.cmml"><mtext id="S3.E4.m1.1.1.1.1.4.3.2" xref="S3.E4.m1.1.1.1.1.4.3.2a.cmml">attended</mtext><mi id="S3.E4.m1.1.1.1.1.4.3.3" xref="S3.E4.m1.1.1.1.1.4.3.3.cmml">j</mi></msub></msub><mo id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml"><mtext id="S3.E4.m1.1.1.1.1.2.4" xref="S3.E4.m1.1.1.1.1.2.4a.cmml">CrossAttention</mtext><mo id="S3.E4.m1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.2.3.cmml">â¢</mo><mrow id="S3.E4.m1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.2.2.3.cmml"><mo id="S3.E4.m1.1.1.1.1.2.2.2.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml">ğ“</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E4.m1.1.1.1.1.2.2.2.4" xref="S3.E4.m1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E4.m1.1.1.1.1.2.2.2.2" xref="S3.E4.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.2.2.2" xref="S3.E4.m1.1.1.1.1.2.2.2.2.2.cmml">ğ—</mi><mrow id="S3.E4.m1.1.1.1.1.2.2.2.2.3" xref="S3.E4.m1.1.1.1.1.2.2.2.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.2.2.3.2" xref="S3.E4.m1.1.1.1.1.2.2.2.2.3.2.cmml">i</mi><mo id="S3.E4.m1.1.1.1.1.2.2.2.2.3.1" xref="S3.E4.m1.1.1.1.1.2.2.2.2.3.1.cmml">â¢</mo><mi id="S3.E4.m1.1.1.1.1.2.2.2.2.3.3" xref="S3.E4.m1.1.1.1.1.2.2.2.2.3.3.cmml">j</mi></mrow></msub><mo id="S3.E4.m1.1.1.1.1.2.2.2.5" stretchy="false" xref="S3.E4.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"></eq><apply id="S3.E4.m1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.4.1.cmml" xref="S3.E4.m1.1.1.1.1.4">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.4.2.cmml" xref="S3.E4.m1.1.1.1.1.4.2">ğ“</ci><apply id="S3.E4.m1.1.1.1.1.4.3.cmml" xref="S3.E4.m1.1.1.1.1.4.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.4.3.1.cmml" xref="S3.E4.m1.1.1.1.1.4.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.4.3.2a.cmml" xref="S3.E4.m1.1.1.1.1.4.3.2"><mtext id="S3.E4.m1.1.1.1.1.4.3.2.cmml" mathsize="70%" xref="S3.E4.m1.1.1.1.1.4.3.2">attended</mtext></ci><ci id="S3.E4.m1.1.1.1.1.4.3.3.cmml" xref="S3.E4.m1.1.1.1.1.4.3.3">ğ‘—</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"><times id="S3.E4.m1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.3"></times><ci id="S3.E4.m1.1.1.1.1.2.4a.cmml" xref="S3.E4.m1.1.1.1.1.2.4"><mtext id="S3.E4.m1.1.1.1.1.2.4.cmml" xref="S3.E4.m1.1.1.1.1.2.4">CrossAttention</mtext></ci><interval closure="open" id="S3.E4.m1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2"><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2">ğ“</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3">ğ‘—</ci></apply><apply id="S3.E4.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2.2">ğ—</ci><apply id="S3.E4.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2.3"><times id="S3.E4.m1.1.1.1.1.2.2.2.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2.3.1"></times><ci id="S3.E4.m1.1.1.1.1.2.2.2.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2.3.2">ğ‘–</ci><ci id="S3.E4.m1.1.1.1.1.2.2.2.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2.3.3">ğ‘—</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\mathbf{T}_{\text{attended}_{j}}=\text{CrossAttention}(\mathbf{T}_{j},\mathbf{%
X}_{ij}),</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">bold_T start_POSTSUBSCRIPT attended start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT = CrossAttention ( bold_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , bold_X start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p5.6">where <math alttext="\mathbf{T}_{\text{attended}_{j}}" class="ltx_Math" display="inline" id="S3.SS2.p5.4.m1.1"><semantics id="S3.SS2.p5.4.m1.1a"><msub id="S3.SS2.p5.4.m1.1.1" xref="S3.SS2.p5.4.m1.1.1.cmml"><mi id="S3.SS2.p5.4.m1.1.1.2" xref="S3.SS2.p5.4.m1.1.1.2.cmml">ğ“</mi><msub id="S3.SS2.p5.4.m1.1.1.3" xref="S3.SS2.p5.4.m1.1.1.3.cmml"><mtext id="S3.SS2.p5.4.m1.1.1.3.2" xref="S3.SS2.p5.4.m1.1.1.3.2a.cmml">attended</mtext><mi id="S3.SS2.p5.4.m1.1.1.3.3" xref="S3.SS2.p5.4.m1.1.1.3.3.cmml">j</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m1.1b"><apply id="S3.SS2.p5.4.m1.1.1.cmml" xref="S3.SS2.p5.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m1.1.1.1.cmml" xref="S3.SS2.p5.4.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.4.m1.1.1.2.cmml" xref="S3.SS2.p5.4.m1.1.1.2">ğ“</ci><apply id="S3.SS2.p5.4.m1.1.1.3.cmml" xref="S3.SS2.p5.4.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m1.1.1.3.1.cmml" xref="S3.SS2.p5.4.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.4.m1.1.1.3.2a.cmml" xref="S3.SS2.p5.4.m1.1.1.3.2"><mtext id="S3.SS2.p5.4.m1.1.1.3.2.cmml" mathsize="70%" xref="S3.SS2.p5.4.m1.1.1.3.2">attended</mtext></ci><ci id="S3.SS2.p5.4.m1.1.1.3.3.cmml" xref="S3.SS2.p5.4.m1.1.1.3.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m1.1c">\mathbf{T}_{\text{attended}_{j}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.4.m1.1d">bold_T start_POSTSUBSCRIPT attended start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> denotes the refined attended tokens for available modality <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p5.5.m2.1"><semantics id="S3.SS2.p5.5.m2.1a"><mi id="S3.SS2.p5.5.m2.1.1" xref="S3.SS2.p5.5.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m2.1b"><ci id="S3.SS2.p5.5.m2.1.1.cmml" xref="S3.SS2.p5.5.m2.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m2.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.5.m2.1d">italic_j</annotation></semantics></math>. The refined attended tokens are then concatenated for each available modality <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p5.6.m3.1"><semantics id="S3.SS2.p5.6.m3.1a"><mi id="S3.SS2.p5.6.m3.1.1" xref="S3.SS2.p5.6.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m3.1b"><ci id="S3.SS2.p5.6.m3.1.1.cmml" xref="S3.SS2.p5.6.m3.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.6.m3.1d">italic_j</annotation></semantics></math></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{T}_{\text{available}}=\text{Concat}\left(\left\{\mathbf{T}_{\text{%
attended}_{j}}\mid j\in\mathcal{A}\right\}\right)," class="ltx_Math" display="block" id="S3.E5.m1.1"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.3.2.cmml">ğ“</mi><mtext id="S3.E5.m1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.3.3a.cmml">available</mtext></msub><mo id="S3.E5.m1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.cmml"><mtext id="S3.E5.m1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.3a.cmml">Concat</mtext><mo id="S3.E5.m1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.cmml"><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml">{</mo><msub id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ“</mi><msub id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mtext id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.2a.cmml">attended</mtext><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></msub></msub><mo fence="true" id="S3.E5.m1.1.1.1.1.1.1.1.1.2.4" lspace="0em" rspace="0em" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml">âˆ£</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">j</mi><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">ğ’œ</mi></mrow><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.2.5" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml">}</mo></mrow><mo id="S3.E5.m1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><eq id="S3.E5.m1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2"></eq><apply id="S3.E5.m1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.3.2">ğ“</ci><ci id="S3.E5.m1.1.1.1.1.3.3a.cmml" xref="S3.E5.m1.1.1.1.1.3.3"><mtext id="S3.E5.m1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S3.E5.m1.1.1.1.1.3.3">available</mtext></ci></apply><apply id="S3.E5.m1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><times id="S3.E5.m1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.2"></times><ci id="S3.E5.m1.1.1.1.1.1.3a.cmml" xref="S3.E5.m1.1.1.1.1.1.3"><mtext id="S3.E5.m1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.3">Concat</mtext></ci><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.3">conditional-set</csymbol><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.2">ğ“</ci><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.2a.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.2"><mtext id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" mathsize="70%" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.2">attended</mtext></ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘—</ci></apply></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2"><in id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.1"></in><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2">ğ‘—</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.3">ğ’œ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\mathbf{T}_{\text{available}}=\text{Concat}\left(\left\{\mathbf{T}_{\text{%
attended}_{j}}\mid j\in\mathcal{A}\right\}\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.1d">bold_T start_POSTSUBSCRIPT available end_POSTSUBSCRIPT = Concat ( { bold_T start_POSTSUBSCRIPT attended start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆ£ italic_j âˆˆ caligraphic_A } ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p5.10">where <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S3.SS2.p5.7.m1.1"><semantics id="S3.SS2.p5.7.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.7.m1.1.1" xref="S3.SS2.p5.7.m1.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m1.1b"><ci id="S3.SS2.p5.7.m1.1.1.cmml" xref="S3.SS2.p5.7.m1.1.1">ğ’œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m1.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.7.m1.1d">caligraphic_A</annotation></semantics></math> denotes the set of available modalities. The concatenated tokens <math alttext="\mathbf{T}_{\text{available}}" class="ltx_Math" display="inline" id="S3.SS2.p5.8.m2.1"><semantics id="S3.SS2.p5.8.m2.1a"><msub id="S3.SS2.p5.8.m2.1.1" xref="S3.SS2.p5.8.m2.1.1.cmml"><mi id="S3.SS2.p5.8.m2.1.1.2" xref="S3.SS2.p5.8.m2.1.1.2.cmml">ğ“</mi><mtext id="S3.SS2.p5.8.m2.1.1.3" xref="S3.SS2.p5.8.m2.1.1.3a.cmml">available</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.8.m2.1b"><apply id="S3.SS2.p5.8.m2.1.1.cmml" xref="S3.SS2.p5.8.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.8.m2.1.1.1.cmml" xref="S3.SS2.p5.8.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.8.m2.1.1.2.cmml" xref="S3.SS2.p5.8.m2.1.1.2">ğ“</ci><ci id="S3.SS2.p5.8.m2.1.1.3a.cmml" xref="S3.SS2.p5.8.m2.1.1.3"><mtext id="S3.SS2.p5.8.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p5.8.m2.1.1.3">available</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.8.m2.1c">\mathbf{T}_{\text{available}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.8.m2.1d">bold_T start_POSTSUBSCRIPT available end_POSTSUBSCRIPT</annotation></semantics></math> are then fed into a multi-layer perceptron (MLP) to produce the projected tokens <math alttext="\mathbf{T}^{\prime}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p5.9.m3.1"><semantics id="S3.SS2.p5.9.m3.1a"><msubsup id="S3.SS2.p5.9.m3.1.1" xref="S3.SS2.p5.9.m3.1.1.cmml"><mi id="S3.SS2.p5.9.m3.1.1.2.2" xref="S3.SS2.p5.9.m3.1.1.2.2.cmml">ğ“</mi><mi id="S3.SS2.p5.9.m3.1.1.3" xref="S3.SS2.p5.9.m3.1.1.3.cmml">i</mi><mo id="S3.SS2.p5.9.m3.1.1.2.3" xref="S3.SS2.p5.9.m3.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.9.m3.1b"><apply id="S3.SS2.p5.9.m3.1.1.cmml" xref="S3.SS2.p5.9.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.9.m3.1.1.1.cmml" xref="S3.SS2.p5.9.m3.1.1">subscript</csymbol><apply id="S3.SS2.p5.9.m3.1.1.2.cmml" xref="S3.SS2.p5.9.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.9.m3.1.1.2.1.cmml" xref="S3.SS2.p5.9.m3.1.1">superscript</csymbol><ci id="S3.SS2.p5.9.m3.1.1.2.2.cmml" xref="S3.SS2.p5.9.m3.1.1.2.2">ğ“</ci><ci id="S3.SS2.p5.9.m3.1.1.2.3.cmml" xref="S3.SS2.p5.9.m3.1.1.2.3">â€²</ci></apply><ci id="S3.SS2.p5.9.m3.1.1.3.cmml" xref="S3.SS2.p5.9.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.9.m3.1c">\mathbf{T}^{\prime}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.9.m3.1d">bold_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> for the missing modality <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p5.10.m4.1"><semantics id="S3.SS2.p5.10.m4.1a"><mi id="S3.SS2.p5.10.m4.1.1" xref="S3.SS2.p5.10.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.10.m4.1b"><ci id="S3.SS2.p5.10.m4.1.1.cmml" xref="S3.SS2.p5.10.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.10.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.10.m4.1d">italic_i</annotation></semantics></math> as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{T}^{\prime}_{i}=\text{MLP}(\mathbf{T}_{\text{available}})." class="ltx_Math" display="block" id="S3.E6.m1.1"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><msubsup id="S3.E6.m1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.1.1.3.2.2" xref="S3.E6.m1.1.1.1.1.3.2.2.cmml">ğ“</mi><mi id="S3.E6.m1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.3.3.cmml">i</mi><mo id="S3.E6.m1.1.1.1.1.3.2.3" xref="S3.E6.m1.1.1.1.1.3.2.3.cmml">â€²</mo></msubsup><mo id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml"><mtext id="S3.E6.m1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.3a.cmml">MLP</mtext><mo id="S3.E6.m1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E6.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E6.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E6.m1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.cmml">ğ“</mi><mtext id="S3.E6.m1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3a.cmml">available</mtext></msub><mo id="S3.E6.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E6.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E6.m1.1.1.1.2" lspace="0em" xref="S3.E6.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><eq id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2"></eq><apply id="S3.E6.m1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3">subscript</csymbol><apply id="S3.E6.m1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.1.1.3">superscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.2">ğ“</ci><ci id="S3.E6.m1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3">â€²</ci></apply><ci id="S3.E6.m1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"><times id="S3.E6.m1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.2"></times><ci id="S3.E6.m1.1.1.1.1.1.3a.cmml" xref="S3.E6.m1.1.1.1.1.1.3"><mtext id="S3.E6.m1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.3">MLP</mtext></ci><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2">ğ“</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3"><mtext id="S3.E6.m1.1.1.1.1.1.1.1.1.3.cmml" mathsize="70%" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3">available</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\mathbf{T}^{\prime}_{i}=\text{MLP}(\mathbf{T}_{\text{available}}).</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.1d">bold_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = MLP ( bold_T start_POSTSUBSCRIPT available end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">The missing modality tokens are replaced with their corresponding projected tokens <math alttext="\mathbf{T}^{\prime}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1.1"><semantics id="S3.SS2.p6.1.m1.1a"><msubsup id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml"><mi id="S3.SS2.p6.1.m1.1.1.2.2" xref="S3.SS2.p6.1.m1.1.1.2.2.cmml">ğ“</mi><mi id="S3.SS2.p6.1.m1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.3.cmml">i</mi><mo id="S3.SS2.p6.1.m1.1.1.2.3" xref="S3.SS2.p6.1.m1.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">subscript</csymbol><apply id="S3.SS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.2.1.cmml" xref="S3.SS2.p6.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.2.2">ğ“</ci><ci id="S3.SS2.p6.1.m1.1.1.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.2.3">â€²</ci></apply><ci id="S3.SS2.p6.1.m1.1.1.3.cmml" xref="S3.SS2.p6.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">\mathbf{T}^{\prime}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.1.m1.1d">bold_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and passed to their respective branch in the network.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Token and Dimension Variability</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.4">Our method can also be applied in cases where the number of tokens <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_N</annotation></semantics></math> or the embedding dimension <math alttext="d" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_d</annotation></semantics></math> differs across modalities. To address differences in embedding dimensions, we apply a linear layer at the beginning of the projection process to map tokens from each modality to a common embedding dimension. For cases where the number of tokens varies, we incorporate a linear layer within the MLP module to align the token count of the projected tokens <math alttext="\mathbf{T}^{\prime}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><msubsup id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2.2" xref="S3.SS3.p1.3.m3.1.1.2.2.cmml">ğ“</mi><mi id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">i</mi><mo id="S3.SS3.p1.3.m3.1.1.2.3" xref="S3.SS3.p1.3.m3.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><apply id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.2.1.cmml" xref="S3.SS3.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2.2">ğ“</ci><ci id="S3.SS3.p1.3.m3.1.1.2.3.cmml" xref="S3.SS3.p1.3.m3.1.1.2.3">â€²</ci></apply><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\mathbf{T}^{\prime}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">bold_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> with that of the missing modality <math alttext="i" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_i</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Alignment Loss Objective</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">To minimize the discrepancy between the projected and real tokens, we use an alignment loss objective to ensure that the projected tokens closely match their corresponding real tokens. If <math alttext="N_{\text{masked}}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">N</mi><mtext id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3a.cmml">masked</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">ğ‘</ci><ci id="S3.SS4.p1.1.m1.1.1.3a.cmml" xref="S3.SS4.p1.1.m1.1.1.3"><mtext id="S3.SS4.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS4.p1.1.m1.1.1.3">masked</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">N_{\text{masked}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">italic_N start_POSTSUBSCRIPT masked end_POSTSUBSCRIPT</annotation></semantics></math> denotes the number of masked modalities at an iteration, the alignment loss is computed as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{alignment}}=\frac{1}{N_{\text{masked}}}\sum_{i\in\text{%
masked}}\mathcal{L}_{\text{alignment}_{i}}(\mathbf{T}^{\prime}_{i},\mathbf{T}_%
{i})," class="ltx_Math" display="block" id="S3.E7.m1.1"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><mrow id="S3.E7.m1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><msub id="S3.E7.m1.1.1.1.1.4" xref="S3.E7.m1.1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.1.1.1.1.4.2" xref="S3.E7.m1.1.1.1.1.4.2.cmml">â„’</mi><mtext id="S3.E7.m1.1.1.1.1.4.3" xref="S3.E7.m1.1.1.1.1.4.3a.cmml">alignment</mtext></msub><mo id="S3.E7.m1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E7.m1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.2.cmml"><mfrac id="S3.E7.m1.1.1.1.1.2.4" xref="S3.E7.m1.1.1.1.1.2.4.cmml"><mn id="S3.E7.m1.1.1.1.1.2.4.2" xref="S3.E7.m1.1.1.1.1.2.4.2.cmml">1</mn><msub id="S3.E7.m1.1.1.1.1.2.4.3" xref="S3.E7.m1.1.1.1.1.2.4.3.cmml"><mi id="S3.E7.m1.1.1.1.1.2.4.3.2" xref="S3.E7.m1.1.1.1.1.2.4.3.2.cmml">N</mi><mtext id="S3.E7.m1.1.1.1.1.2.4.3.3" xref="S3.E7.m1.1.1.1.1.2.4.3.3a.cmml">masked</mtext></msub></mfrac><mo id="S3.E7.m1.1.1.1.1.2.3" xref="S3.E7.m1.1.1.1.1.2.3.cmml">â¢</mo><mrow id="S3.E7.m1.1.1.1.1.2.2" xref="S3.E7.m1.1.1.1.1.2.2.cmml"><munder id="S3.E7.m1.1.1.1.1.2.2.3" xref="S3.E7.m1.1.1.1.1.2.2.3.cmml"><mo id="S3.E7.m1.1.1.1.1.2.2.3.2" movablelimits="false" xref="S3.E7.m1.1.1.1.1.2.2.3.2.cmml">âˆ‘</mo><mrow id="S3.E7.m1.1.1.1.1.2.2.3.3" xref="S3.E7.m1.1.1.1.1.2.2.3.3.cmml"><mi id="S3.E7.m1.1.1.1.1.2.2.3.3.2" xref="S3.E7.m1.1.1.1.1.2.2.3.3.2.cmml">i</mi><mo id="S3.E7.m1.1.1.1.1.2.2.3.3.1" xref="S3.E7.m1.1.1.1.1.2.2.3.3.1.cmml">âˆˆ</mo><mtext id="S3.E7.m1.1.1.1.1.2.2.3.3.3" xref="S3.E7.m1.1.1.1.1.2.2.3.3.3a.cmml">masked</mtext></mrow></munder><mrow id="S3.E7.m1.1.1.1.1.2.2.2" xref="S3.E7.m1.1.1.1.1.2.2.2.cmml"><msub id="S3.E7.m1.1.1.1.1.2.2.2.4" xref="S3.E7.m1.1.1.1.1.2.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.1.1.1.1.2.2.2.4.2" xref="S3.E7.m1.1.1.1.1.2.2.2.4.2.cmml">â„’</mi><msub id="S3.E7.m1.1.1.1.1.2.2.2.4.3" xref="S3.E7.m1.1.1.1.1.2.2.2.4.3.cmml"><mtext id="S3.E7.m1.1.1.1.1.2.2.2.4.3.2" xref="S3.E7.m1.1.1.1.1.2.2.2.4.3.2a.cmml">alignment</mtext><mi id="S3.E7.m1.1.1.1.1.2.2.2.4.3.3" xref="S3.E7.m1.1.1.1.1.2.2.2.4.3.3.cmml">i</mi></msub></msub><mo id="S3.E7.m1.1.1.1.1.2.2.2.3" xref="S3.E7.m1.1.1.1.1.2.2.2.3.cmml">â¢</mo><mrow id="S3.E7.m1.1.1.1.1.2.2.2.2.2" xref="S3.E7.m1.1.1.1.1.2.2.2.2.3.cmml"><mo id="S3.E7.m1.1.1.1.1.2.2.2.2.2.3" stretchy="false" xref="S3.E7.m1.1.1.1.1.2.2.2.2.3.cmml">(</mo><msubsup id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğ“</mi><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi><mo id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">â€²</mo></msubsup><mo id="S3.E7.m1.1.1.1.1.2.2.2.2.2.4" xref="S3.E7.m1.1.1.1.1.2.2.2.2.3.cmml">,</mo><msub id="S3.E7.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E7.m1.1.1.1.1.2.2.2.2.2.2.cmml"><mi id="S3.E7.m1.1.1.1.1.2.2.2.2.2.2.2" xref="S3.E7.m1.1.1.1.1.2.2.2.2.2.2.2.cmml">ğ“</mi><mi id="S3.E7.m1.1.1.1.1.2.2.2.2.2.2.3" xref="S3.E7.m1.1.1.1.1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E7.m1.1.1.1.1.2.2.2.2.2.5" stretchy="false" xref="S3.E7.m1.1.1.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E7.m1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"><eq id="S3.E7.m1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.3"></eq><apply id="S3.E7.m1.1.1.1.1.4.cmml" xref="S3.E7.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.4.1.cmml" xref="S3.E7.m1.1.1.1.1.4">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.4.2.cmml" xref="S3.E7.m1.1.1.1.1.4.2">â„’</ci><ci id="S3.E7.m1.1.1.1.1.4.3a.cmml" xref="S3.E7.m1.1.1.1.1.4.3"><mtext id="S3.E7.m1.1.1.1.1.4.3.cmml" mathsize="70%" xref="S3.E7.m1.1.1.1.1.4.3">alignment</mtext></ci></apply><apply id="S3.E7.m1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.2"><times id="S3.E7.m1.1.1.1.1.2.3.cmml" xref="S3.E7.m1.1.1.1.1.2.3"></times><apply id="S3.E7.m1.1.1.1.1.2.4.cmml" xref="S3.E7.m1.1.1.1.1.2.4"><divide id="S3.E7.m1.1.1.1.1.2.4.1.cmml" xref="S3.E7.m1.1.1.1.1.2.4"></divide><cn id="S3.E7.m1.1.1.1.1.2.4.2.cmml" type="integer" xref="S3.E7.m1.1.1.1.1.2.4.2">1</cn><apply id="S3.E7.m1.1.1.1.1.2.4.3.cmml" xref="S3.E7.m1.1.1.1.1.2.4.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.2.4.3.1.cmml" xref="S3.E7.m1.1.1.1.1.2.4.3">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.2.4.3.2.cmml" xref="S3.E7.m1.1.1.1.1.2.4.3.2">ğ‘</ci><ci id="S3.E7.m1.1.1.1.1.2.4.3.3a.cmml" xref="S3.E7.m1.1.1.1.1.2.4.3.3"><mtext id="S3.E7.m1.1.1.1.1.2.4.3.3.cmml" mathsize="70%" xref="S3.E7.m1.1.1.1.1.2.4.3.3">masked</mtext></ci></apply></apply><apply id="S3.E7.m1.1.1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.1.1.2.2"><apply id="S3.E7.m1.1.1.1.1.2.2.3.cmml" xref="S3.E7.m1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E7.m1.1.1.1.1.2.2.3">subscript</csymbol><sum id="S3.E7.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E7.m1.1.1.1.1.2.2.3.2"></sum><apply id="S3.E7.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E7.m1.1.1.1.1.2.2.3.3"><in id="S3.E7.m1.1.1.1.1.2.2.3.3.1.cmml" xref="S3.E7.m1.1.1.1.1.2.2.3.3.1"></in><ci id="S3.E7.m1.1.1.1.1.2.2.3.3.2.cmml" xref="S3.E7.m1.1.1.1.1.2.2.3.3.2">ğ‘–</ci><ci id="S3.E7.m1.1.1.1.1.2.2.3.3.3a.cmml" xref="S3.E7.m1.1.1.1.1.2.2.3.3.3"><mtext id="S3.E7.m1.1.1.1.1.2.2.3.3.3.cmml" mathsize="70%" xref="S3.E7.m1.1.1.1.1.2.2.3.3.3">masked</mtext></ci></apply></apply><apply id="S3.E7.m1.1.1.1.1.2.2.2.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2"><times id="S3.E7.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.3"></times><apply id="S3.E7.m1.1.1.1.1.2.2.2.4.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.2.2.2.4.1.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.4">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.2.2.2.4.2.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.4.2">â„’</ci><apply id="S3.E7.m1.1.1.1.1.2.2.2.4.3.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.4.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.2.2.2.4.3.1.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.4.3">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.2.2.2.4.3.2a.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.4.3.2"><mtext id="S3.E7.m1.1.1.1.1.2.2.2.4.3.2.cmml" mathsize="70%" xref="S3.E7.m1.1.1.1.1.2.2.2.4.3.2">alignment</mtext></ci><ci id="S3.E7.m1.1.1.1.1.2.2.2.4.3.3.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.4.3.3">ğ‘–</ci></apply></apply><interval closure="open" id="S3.E7.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.2.2"><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.2">ğ“</ci><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.3">â€²</ci></apply><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E7.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.2.2.2.2">ğ“</ci><ci id="S3.E7.m1.1.1.1.1.2.2.2.2.2.2.3.cmml" xref="S3.E7.m1.1.1.1.1.2.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\mathcal{L}_{\text{alignment}}=\frac{1}{N_{\text{masked}}}\sum_{i\in\text{%
masked}}\mathcal{L}_{\text{alignment}_{i}}(\mathbf{T}^{\prime}_{i},\mathbf{T}_%
{i}),</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.1d">caligraphic_L start_POSTSUBSCRIPT alignment end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT masked end_POSTSUBSCRIPT end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i âˆˆ masked end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT alignment start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p1.5">where <math alttext="\mathcal{L}_{\text{alignment}_{i}}" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m1.1"><semantics id="S3.SS4.p1.2.m1.1a"><msub id="S3.SS4.p1.2.m1.1.1" xref="S3.SS4.p1.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.2.m1.1.1.2" xref="S3.SS4.p1.2.m1.1.1.2.cmml">â„’</mi><msub id="S3.SS4.p1.2.m1.1.1.3" xref="S3.SS4.p1.2.m1.1.1.3.cmml"><mtext id="S3.SS4.p1.2.m1.1.1.3.2" xref="S3.SS4.p1.2.m1.1.1.3.2a.cmml">alignment</mtext><mi id="S3.SS4.p1.2.m1.1.1.3.3" xref="S3.SS4.p1.2.m1.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m1.1b"><apply id="S3.SS4.p1.2.m1.1.1.cmml" xref="S3.SS4.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m1.1.1.1.cmml" xref="S3.SS4.p1.2.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m1.1.1.2.cmml" xref="S3.SS4.p1.2.m1.1.1.2">â„’</ci><apply id="S3.SS4.p1.2.m1.1.1.3.cmml" xref="S3.SS4.p1.2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m1.1.1.3.1.cmml" xref="S3.SS4.p1.2.m1.1.1.3">subscript</csymbol><ci id="S3.SS4.p1.2.m1.1.1.3.2a.cmml" xref="S3.SS4.p1.2.m1.1.1.3.2"><mtext id="S3.SS4.p1.2.m1.1.1.3.2.cmml" mathsize="70%" xref="S3.SS4.p1.2.m1.1.1.3.2">alignment</mtext></ci><ci id="S3.SS4.p1.2.m1.1.1.3.3.cmml" xref="S3.SS4.p1.2.m1.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m1.1c">\mathcal{L}_{\text{alignment}_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m1.1d">caligraphic_L start_POSTSUBSCRIPT alignment start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> represents the Smooth L1 loss between the real tokens <math alttext="\mathbf{T}_{i}" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m2.1"><semantics id="S3.SS4.p1.3.m2.1a"><msub id="S3.SS4.p1.3.m2.1.1" xref="S3.SS4.p1.3.m2.1.1.cmml"><mi id="S3.SS4.p1.3.m2.1.1.2" xref="S3.SS4.p1.3.m2.1.1.2.cmml">ğ“</mi><mi id="S3.SS4.p1.3.m2.1.1.3" xref="S3.SS4.p1.3.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m2.1b"><apply id="S3.SS4.p1.3.m2.1.1.cmml" xref="S3.SS4.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m2.1.1.1.cmml" xref="S3.SS4.p1.3.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m2.1.1.2.cmml" xref="S3.SS4.p1.3.m2.1.1.2">ğ“</ci><ci id="S3.SS4.p1.3.m2.1.1.3.cmml" xref="S3.SS4.p1.3.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m2.1c">\mathbf{T}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m2.1d">bold_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and the projected tokens <math alttext="\mathbf{T}^{\prime}_{i}" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m3.1"><semantics id="S3.SS4.p1.4.m3.1a"><msubsup id="S3.SS4.p1.4.m3.1.1" xref="S3.SS4.p1.4.m3.1.1.cmml"><mi id="S3.SS4.p1.4.m3.1.1.2.2" xref="S3.SS4.p1.4.m3.1.1.2.2.cmml">ğ“</mi><mi id="S3.SS4.p1.4.m3.1.1.3" xref="S3.SS4.p1.4.m3.1.1.3.cmml">i</mi><mo id="S3.SS4.p1.4.m3.1.1.2.3" xref="S3.SS4.p1.4.m3.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m3.1b"><apply id="S3.SS4.p1.4.m3.1.1.cmml" xref="S3.SS4.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m3.1.1.1.cmml" xref="S3.SS4.p1.4.m3.1.1">subscript</csymbol><apply id="S3.SS4.p1.4.m3.1.1.2.cmml" xref="S3.SS4.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m3.1.1.2.1.cmml" xref="S3.SS4.p1.4.m3.1.1">superscript</csymbol><ci id="S3.SS4.p1.4.m3.1.1.2.2.cmml" xref="S3.SS4.p1.4.m3.1.1.2.2">ğ“</ci><ci id="S3.SS4.p1.4.m3.1.1.2.3.cmml" xref="S3.SS4.p1.4.m3.1.1.2.3">â€²</ci></apply><ci id="S3.SS4.p1.4.m3.1.1.3.cmml" xref="S3.SS4.p1.4.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m3.1c">\mathbf{T}^{\prime}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m3.1d">bold_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. To incorporate the alignment loss into the overall optimization, we add <math alttext="\mathcal{L}_{\text{alignment}}" class="ltx_Math" display="inline" id="S3.SS4.p1.5.m4.1"><semantics id="S3.SS4.p1.5.m4.1a"><msub id="S3.SS4.p1.5.m4.1.1" xref="S3.SS4.p1.5.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.5.m4.1.1.2" xref="S3.SS4.p1.5.m4.1.1.2.cmml">â„’</mi><mtext id="S3.SS4.p1.5.m4.1.1.3" xref="S3.SS4.p1.5.m4.1.1.3a.cmml">alignment</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m4.1b"><apply id="S3.SS4.p1.5.m4.1.1.cmml" xref="S3.SS4.p1.5.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m4.1.1.1.cmml" xref="S3.SS4.p1.5.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.5.m4.1.1.2.cmml" xref="S3.SS4.p1.5.m4.1.1.2">â„’</ci><ci id="S3.SS4.p1.5.m4.1.1.3a.cmml" xref="S3.SS4.p1.5.m4.1.1.3"><mtext id="S3.SS4.p1.5.m4.1.1.3.cmml" mathsize="70%" xref="S3.SS4.p1.5.m4.1.1.3">alignment</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m4.1c">\mathcal{L}_{\text{alignment}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.5.m4.1d">caligraphic_L start_POSTSUBSCRIPT alignment end_POSTSUBSCRIPT</annotation></semantics></math> to the networkâ€™s primary loss function as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{total}}=\mathcal{L}_{\text{task}}+\mathcal{L}_{\text{%
alignment}}" class="ltx_Math" display="block" id="S3.E8.m1.1"><semantics id="S3.E8.m1.1a"><mrow id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml"><msub id="S3.E8.m1.1.1.2" xref="S3.E8.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E8.m1.1.1.2.2" xref="S3.E8.m1.1.1.2.2.cmml">â„’</mi><mtext id="S3.E8.m1.1.1.2.3" xref="S3.E8.m1.1.1.2.3a.cmml">total</mtext></msub><mo id="S3.E8.m1.1.1.1" xref="S3.E8.m1.1.1.1.cmml">=</mo><mrow id="S3.E8.m1.1.1.3" xref="S3.E8.m1.1.1.3.cmml"><msub id="S3.E8.m1.1.1.3.2" xref="S3.E8.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E8.m1.1.1.3.2.2" xref="S3.E8.m1.1.1.3.2.2.cmml">â„’</mi><mtext id="S3.E8.m1.1.1.3.2.3" xref="S3.E8.m1.1.1.3.2.3a.cmml">task</mtext></msub><mo id="S3.E8.m1.1.1.3.1" xref="S3.E8.m1.1.1.3.1.cmml">+</mo><msub id="S3.E8.m1.1.1.3.3" xref="S3.E8.m1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E8.m1.1.1.3.3.2" xref="S3.E8.m1.1.1.3.3.2.cmml">â„’</mi><mtext id="S3.E8.m1.1.1.3.3.3" xref="S3.E8.m1.1.1.3.3.3a.cmml">alignment</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1"><eq id="S3.E8.m1.1.1.1.cmml" xref="S3.E8.m1.1.1.1"></eq><apply id="S3.E8.m1.1.1.2.cmml" xref="S3.E8.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.2.1.cmml" xref="S3.E8.m1.1.1.2">subscript</csymbol><ci id="S3.E8.m1.1.1.2.2.cmml" xref="S3.E8.m1.1.1.2.2">â„’</ci><ci id="S3.E8.m1.1.1.2.3a.cmml" xref="S3.E8.m1.1.1.2.3"><mtext id="S3.E8.m1.1.1.2.3.cmml" mathsize="70%" xref="S3.E8.m1.1.1.2.3">total</mtext></ci></apply><apply id="S3.E8.m1.1.1.3.cmml" xref="S3.E8.m1.1.1.3"><plus id="S3.E8.m1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.3.1"></plus><apply id="S3.E8.m1.1.1.3.2.cmml" xref="S3.E8.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.3.2.1.cmml" xref="S3.E8.m1.1.1.3.2">subscript</csymbol><ci id="S3.E8.m1.1.1.3.2.2.cmml" xref="S3.E8.m1.1.1.3.2.2">â„’</ci><ci id="S3.E8.m1.1.1.3.2.3a.cmml" xref="S3.E8.m1.1.1.3.2.3"><mtext id="S3.E8.m1.1.1.3.2.3.cmml" mathsize="70%" xref="S3.E8.m1.1.1.3.2.3">task</mtext></ci></apply><apply id="S3.E8.m1.1.1.3.3.cmml" xref="S3.E8.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.3.3.1.cmml" xref="S3.E8.m1.1.1.3.3">subscript</csymbol><ci id="S3.E8.m1.1.1.3.3.2.cmml" xref="S3.E8.m1.1.1.3.3.2">â„’</ci><ci id="S3.E8.m1.1.1.3.3.3a.cmml" xref="S3.E8.m1.1.1.3.3.3"><mtext id="S3.E8.m1.1.1.3.3.3.cmml" mathsize="70%" xref="S3.E8.m1.1.1.3.3.3">alignment</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">\mathcal{L}_{\text{total}}=\mathcal{L}_{\text{task}}+\mathcal{L}_{\text{%
alignment}}</annotation><annotation encoding="application/x-llamapun" id="S3.E8.m1.1d">caligraphic_L start_POSTSUBSCRIPT total end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT task end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT alignment end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p1.6">where <math alttext="\mathcal{L}_{\text{task}}" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m1.1"><semantics id="S3.SS4.p1.6.m1.1a"><msub id="S3.SS4.p1.6.m1.1.1" xref="S3.SS4.p1.6.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.6.m1.1.1.2" xref="S3.SS4.p1.6.m1.1.1.2.cmml">â„’</mi><mtext id="S3.SS4.p1.6.m1.1.1.3" xref="S3.SS4.p1.6.m1.1.1.3a.cmml">task</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m1.1b"><apply id="S3.SS4.p1.6.m1.1.1.cmml" xref="S3.SS4.p1.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m1.1.1.1.cmml" xref="S3.SS4.p1.6.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.6.m1.1.1.2.cmml" xref="S3.SS4.p1.6.m1.1.1.2">â„’</ci><ci id="S3.SS4.p1.6.m1.1.1.3a.cmml" xref="S3.SS4.p1.6.m1.1.1.3"><mtext id="S3.SS4.p1.6.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS4.p1.6.m1.1.1.3">task</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m1.1c">\mathcal{L}_{\text{task}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.6.m1.1d">caligraphic_L start_POSTSUBSCRIPT task end_POSTSUBSCRIPT</annotation></semantics></math> represents the primary task-specific loss of the network.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1">Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.2">Input Modalities</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.3">Missing Modalities</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.4">Pretrained</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.5">Modality Dropout</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.6.1">MMP (ours)</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.1" rowspan="4"><span class="ltx_text" id="S3.T1.1.2.2.1.1">MCubeS</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.2">RGB-A-D-N</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.2.4.1">51.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.5">48.56</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.1.2.2.6">48.95</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.1">A-D-N</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.2">RGB</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.3">1.45</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.4">33.88</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.3.3.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.3.3.5.1">38.57</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.1">A-D</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.2">RGB-N</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.3">0.93</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.4">33.15</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.4.4.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.4.5.1">37.74</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.1">A</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.2">RGB-D-N</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.3">1.13</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.4">26.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.5.5.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.5.5.5.1">31.31</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.6.6.1" rowspan="3"><span class="ltx_text" id="S3.T1.1.6.6.1.1">NYUDv2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.6.6.2">RGB-Depth</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.6.6.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.6.6.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.6.6.4.1">56.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.6.6.5">51.12</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.1.6.6.6">53.81</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7">
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.1">RGB</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.2">Depth</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.3">51.05</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.4">48.80</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.7.7.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.7.5.1">52.04</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.8">
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.1">Depth</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.2">RGB</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.3">6.01</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.4">29.79</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.8.8.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.8.8.5.1">41.08</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.9.9.1" rowspan="3"><span class="ltx_text" id="S3.T1.1.9.9.1.1">FMB</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.9.9.2">RGB-Thermal</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.9.9.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.9.9.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.9.9.4.1">62.68</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.9.9.5">54.11</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.1.9.9.6">60.03</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.10">
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.1">RGB</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.2">Thermal</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.3">22.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.4">48.32</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.10.10.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.10.5.1">55.83</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.11.11.1">Thermal</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.11.11.2">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.11.11.3">23.35</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.11.11.4">39.66</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.1.11.11.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.11.11.5.1">51.73</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison (mIoU) of the pretrained model, modality dropout training, and MMP. Input and missing modalities columns indicate available and missing modalities during inference. A, D and N denote angle of linear polarization, degree of linear polarization, and near-infrared, respectively.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.1.1.1.1" style="padding-left:3.5pt;padding-right:3.5pt;">Methods</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.1.1.1.2" style="padding-left:3.5pt;padding-right:3.5pt;">Backbone</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.3" style="padding-left:3.5pt;padding-right:3.5pt;">RGB</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.4" style="padding-left:3.5pt;padding-right:3.5pt;">Depth</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.5" style="padding-left:3.5pt;padding-right:3.5pt;">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.2.1.1" style="padding-left:3.5pt;padding-right:3.5pt;">AsymFusion <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib39" title="">2020b</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.2.1.2" style="padding-left:3.5pt;padding-right:3.5pt;">ResNet-101</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.3" style="padding-left:3.5pt;padding-right:3.5pt;">46.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.4" style="padding-left:3.5pt;padding-right:3.5pt;">34.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.5" style="padding-left:3.5pt;padding-right:3.5pt;">40.40</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.3.2.1" style="padding-left:3.5pt;padding-right:3.5pt;">CEN <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib38" title="">2020a</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.3.2.2" style="padding-left:3.5pt;padding-right:3.5pt;">ResNet-101</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.3" style="padding-left:3.5pt;padding-right:3.5pt;">39.59</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.4" style="padding-left:3.5pt;padding-right:3.5pt;">19.32</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.5" style="padding-left:3.5pt;padding-right:3.5pt;">29.46</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.4.3.1" style="padding-left:3.5pt;padding-right:3.5pt;">TokenFusion <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib37" title="">2022</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.4.3.2" style="padding-left:3.5pt;padding-right:3.5pt;">MiT-B3</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.3" style="padding-left:3.5pt;padding-right:3.5pt;">49.32</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.4" style="padding-left:3.5pt;padding-right:3.5pt;">36.84</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.5" style="padding-left:3.5pt;padding-right:3.5pt;">43.08</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.5.4.1" style="padding-left:3.5pt;padding-right:3.5pt;">Reza et. al. <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib29" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.5.4.2" style="padding-left:3.5pt;padding-right:3.5pt;">MiT-B4</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.5.4.3.1">52.82</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.4" style="padding-left:3.5pt;padding-right:3.5pt;">36.72</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.1.5.4.5.1">44.77</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.6.5.1" style="padding-left:3.5pt;padding-right:3.5pt;">MMANet <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib40" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.6.5.2" style="padding-left:3.5pt;padding-right:3.5pt;">ResNet-50</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.3" style="padding-left:3.5pt;padding-right:3.5pt;">44.93</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.6.5.4.1">42.75</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.5" style="padding-left:3.5pt;padding-right:3.5pt;">43.84</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.7.6.1" style="padding-left:3.5pt;padding-right:3.5pt;">HeMIS <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib9" title="">2016</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.7.6.2" style="padding-left:3.5pt;padding-right:3.5pt;">ResNet-50</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.3" style="padding-left:3.5pt;padding-right:3.5pt;">33.23</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.4" style="padding-left:3.5pt;padding-right:3.5pt;">31.23</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.5" style="padding-left:3.5pt;padding-right:3.5pt;">32.23</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.8.7.1" style="padding-left:3.5pt;padding-right:3.5pt;">CMNeXt <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib50" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.8.7.2" style="padding-left:3.5pt;padding-right:3.5pt;">MiT-B4</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.3" style="padding-left:3.5pt;padding-right:3.5pt;">51.19</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.4" style="padding-left:3.5pt;padding-right:3.5pt;">5.26</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.5" style="padding-left:3.5pt;padding-right:3.5pt;">28.23</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.9.8.1" style="padding-left:3.5pt;padding-right:3.5pt;">RFNet <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib6" title="">2021</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.9.8.2" style="padding-left:3.5pt;padding-right:3.5pt;">ResNet-50</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.3" style="padding-left:3.5pt;padding-right:3.5pt;">42.89</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.4" style="padding-left:3.5pt;padding-right:3.5pt;">40.76</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.5" style="padding-left:3.5pt;padding-right:3.5pt;">41.82</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.1.10.9.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.10.9.1.1">MMP (Ours)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.1.10.9.2" style="padding-left:3.5pt;padding-right:3.5pt;">MiT-B4</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.10.9.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.1.10.9.3.1">52.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.10.9.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.1.10.9.4.1">41.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.10.9.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.10.9.5.1">46.56</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance (mIoU) comparison with existing methods on NYUDv2 dataset. RGB, Depth and Avg. columns report RGB only, Depth only and average performance respectively. Best and second-best results are shown as <span class="ltx_text ltx_font_bold" id="S3.T2.4.1">bold</span> and <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T2.5.2">underlined</span>, respectively.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="2" id="S3.T3.3.4.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Available Modality</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.3.4.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">ViLT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S3.T3.3.4.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">Missing Prompts <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib16" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.3.4.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">MMP</th>
</tr>
<tr class="ltx_tr" id="S3.T3.3.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S3.T3.3.5.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">Image</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S3.T3.3.5.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">Text</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S3.T3.3.5.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib15" title="">2021</a>)</cite></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.3.5.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">Attention</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.3.5.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">Input</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.3.5.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">(Ours)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="\text{92.71}^{\dagger}" class="ltx_Math" display="inline" id="S3.T3.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.m1.1a"><msup id="S3.T3.1.1.1.m1.1.1" xref="S3.T3.1.1.1.m1.1.1.cmml"><mtext id="S3.T3.1.1.1.m1.1.1.2" xref="S3.T3.1.1.1.m1.1.1.2a.cmml">92.71</mtext><mo id="S3.T3.1.1.1.m1.1.1.3" xref="S3.T3.1.1.1.m1.1.1.3.cmml">â€ </mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T3.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1">superscript</csymbol><ci id="S3.T3.1.1.1.m1.1.1.2a.cmml" xref="S3.T3.1.1.1.m1.1.1.2"><mtext id="S3.T3.1.1.1.m1.1.1.2.cmml" xref="S3.T3.1.1.1.m1.1.1.2">92.71</mtext></ci><ci id="S3.T3.1.1.1.m1.1.1.3.cmml" xref="S3.T3.1.1.1.m1.1.1.3">â€ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">\text{92.71}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.m1.1d">92.71 start_POSTSUPERSCRIPT â€  end_POSTSUPERSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">92.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">92.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.6.1">92.87</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.6.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.3.6.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.3.6.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">30%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.3.6.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">66.29</th>
<td class="ltx_td ltx_align_center" id="S3.T3.3.6.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">72.57</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.6.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.3.6.1.5.1">74.53</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.6.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">74.32</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">0%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="\text{23.70}^{\dagger}" class="ltx_Math" display="inline" id="S3.T3.2.2.1.m1.1"><semantics id="S3.T3.2.2.1.m1.1a"><msup id="S3.T3.2.2.1.m1.1.1" xref="S3.T3.2.2.1.m1.1.1.cmml"><mtext id="S3.T3.2.2.1.m1.1.1.2" xref="S3.T3.2.2.1.m1.1.1.2a.cmml">23.70</mtext><mo id="S3.T3.2.2.1.m1.1.1.3" xref="S3.T3.2.2.1.m1.1.1.3.cmml">â€ </mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.1.m1.1b"><apply id="S3.T3.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T3.2.2.1.m1.1.1.1.cmml" xref="S3.T3.2.2.1.m1.1.1">superscript</csymbol><ci id="S3.T3.2.2.1.m1.1.1.2a.cmml" xref="S3.T3.2.2.1.m1.1.1.2"><mtext id="S3.T3.2.2.1.m1.1.1.2.cmml" xref="S3.T3.2.2.1.m1.1.1.2">23.70</mtext></ci><ci id="S3.T3.2.2.1.m1.1.1.3.cmml" xref="S3.T3.2.2.1.m1.1.1.3">â€ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.1.m1.1c">\text{23.70}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.1.m1.1d">23.70 start_POSTSUPERSCRIPT â€  end_POSTSUPERSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">67.70</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.5.1">68.10</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">66.06</td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.7.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.3.7.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">65%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.3.7.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">65%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.3.7.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">69.25</th>
<td class="ltx_td ltx_align_center" id="S3.T3.3.7.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">78.09</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.7.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">79.08</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.7.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.3.7.2.6.1">80.28</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.8.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.3.8.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">30%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.3.8.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.3.8.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">76.66</th>
<td class="ltx_td ltx_align_center" id="S3.T3.3.8.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">86.05</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.8.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">86.18</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.8.3.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.3.8.3.6.1">87.71</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T3.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">0%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T3.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T3.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="\text{82.65}^{\dagger}" class="ltx_Math" display="inline" id="S3.T3.3.3.1.m1.1"><semantics id="S3.T3.3.3.1.m1.1a"><msup id="S3.T3.3.3.1.m1.1.1" xref="S3.T3.3.3.1.m1.1.1.cmml"><mtext id="S3.T3.3.3.1.m1.1.1.2" xref="S3.T3.3.3.1.m1.1.1.2a.cmml">82.65</mtext><mo id="S3.T3.3.3.1.m1.1.1.3" xref="S3.T3.3.3.1.m1.1.1.3.cmml">â€ </mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.1.m1.1b"><apply id="S3.T3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T3.3.3.1.m1.1.1.1.cmml" xref="S3.T3.3.3.1.m1.1.1">superscript</csymbol><ci id="S3.T3.3.3.1.m1.1.1.2a.cmml" xref="S3.T3.3.3.1.m1.1.1.2"><mtext id="S3.T3.3.3.1.m1.1.1.2.cmml" xref="S3.T3.3.3.1.m1.1.1.2">82.65</mtext></ci><ci id="S3.T3.3.3.1.m1.1.1.3.cmml" xref="S3.T3.3.3.1.m1.1.1.3">â€ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.1.m1.1c">\text{82.65}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.1.m1.1d">82.65 start_POSTSUPERSCRIPT â€  end_POSTSUPERSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">85.30</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">84.80</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.3.3.6.1">85.37</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance (accuracy) comparison for multimodal classification on UPMC Food-101 dataset. Available modality column shows the percentage of image and text modality available during inference. <math alttext="{\dagger}" class="ltx_Math" display="inline" id="S3.T3.5.m1.1"><semantics id="S3.T3.5.m1.1b"><mo id="S3.T3.5.m1.1.1" xref="S3.T3.5.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.m1.1c"><ci id="S3.T3.5.m1.1.1.cmml" xref="S3.T3.5.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.m1.1d">{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.m1.1e">â€ </annotation></semantics></math> indicates that the results were generated using the available code from the authors.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we provide a comprehensive evaluation of our proposed method through detailed experiments on multimodal segmentation and classification tasks across five datasets. We compare our approach with established baseline methods that address missing modalities to assess its effectiveness and generalizability.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">MCubeS dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib17" title="">2022</a>)</cite> contains 500 sets of images split into training, validation, and test sets containing 302, 96, and 102 sets of images respectively. It has 4 input modalities and per-pixel annotations for 20 material classes. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.2">NYUDv2 dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Silberman etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib32" title="">2012</a>)</cite> contains 1,449 aligned RGB-Depth image pairs, split into 795 for training and 654 for testing. Each images is 640 Ã— 480 pixels and has annotation for 40 classes.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.3">FMB dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib19" title="">2023</a>)</cite> has 1,500 calibrated RGB-Infrared image pairs, with 1,220 for training and 280 for testing. It includes diverse scenes across various lighting and weather conditions, and offers per-pixel ground truth annotations for 14 classes.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.4">UPMC Food-101 dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib36" title="">2015</a>)</cite> is a multimodal classification dataset containing image and text as input modalities. The dataset contains 90,704 image-text pairs divided into train, validation and test sets, and 101 food categories. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.5">CMU-MOSI dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Zadeh etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib49" title="">2016</a>)</cite> is widely used for multimodal sentiment analysis and includes audio, visual, and text modalities. It contains a total of 2,199 samples, which are divided into training, validation, and test sets with 1,284, 229, and 686 samples respectively.
<br class="ltx_break"/>Complete details about each dataset are added in the supplementary materials of this paper.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.3">We use CMNeXt <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib50" title="">2023</a>)</cite> as the base model for multimodal segmentation, ViLT <cite class="ltx_cite ltx_citemacro_citep">(Kim, Son, and Kim <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib15" title="">2021</a>)</cite> for multimodal classification, and multimodal transformer <cite class="ltx_cite ltx_citemacro_citep">(Tsai etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib34" title="">2019</a>)</cite> for multimodal sentiment analysis task. To evaluate missing modality performance, available modalities are provided while missing ones are set to zero for visual and audio data, and to empty strings for texts. For multimodal segmentation, we use a learning rate of <math alttext="6\times 10^{-5}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">6</mn><mo id="S4.SS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.1.m1.1.1.1.cmml">Ã—</mo><msup id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mn id="S4.SS2.p1.1.m1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.p1.1.m1.1.1.3.3" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml"><mo id="S4.SS2.p1.1.m1.1.1.3.3a" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="S4.SS2.p1.1.m1.1.1.3.3.2" xref="S4.SS2.p1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn id="S4.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.2">6</cn><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3">superscript</csymbol><cn id="S4.SS2.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.3.2">10</cn><apply id="S4.SS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3"><minus id="S4.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3"></minus><cn id="S4.SS2.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">6\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">6 Ã— 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> with a polynomial scheduler, and apply OHEM cross-entropy loss for the FMB and MCubeS datasets, and cross-entropy loss for NYUDv2. We utilize AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib21" title="">2019</a>)</cite>, with <math alttext="\epsilon=10^{-8}" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">Ïµ</mi><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">=</mo><msup id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml"><mn id="S4.SS2.p1.2.m2.1.1.3.2" xref="S4.SS2.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.p1.2.m2.1.1.3.3" xref="S4.SS2.p1.2.m2.1.1.3.3.cmml"><mo id="S4.SS2.p1.2.m2.1.1.3.3a" xref="S4.SS2.p1.2.m2.1.1.3.3.cmml">âˆ’</mo><mn id="S4.SS2.p1.2.m2.1.1.3.3.2" xref="S4.SS2.p1.2.m2.1.1.3.3.2.cmml">8</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><eq id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></eq><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">italic-Ïµ</ci><apply id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.3">superscript</csymbol><cn id="S4.SS2.p1.2.m2.1.1.3.2.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1.3.2">10</cn><apply id="S4.SS2.p1.2.m2.1.1.3.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3"><minus id="S4.SS2.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3"></minus><cn id="S4.SS2.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1.3.3.2">8</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\epsilon=10^{-8}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_Ïµ = 10 start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT</annotation></semantics></math> and a weight decay of 0.01. For this task, we utilize the CMNeXt model pre-trained with modality dropout augmentation. We set the batch size to 4, and train the model for 500 epochs on MCubeS and NYUDv2 dataset. For the FMB dataset, we use a batch size of 2 and train for 120 epochs. For multimodal classification, we set the learning rate to <math alttext="10^{-5}" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><msup id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">10</mn><mrow id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml"><mo id="S4.SS2.p1.3.m3.1.1.3a" xref="S4.SS2.p1.3.m3.1.1.3.cmml">âˆ’</mo><mn id="S4.SS2.p1.3.m3.1.1.3.2" xref="S4.SS2.p1.3.m3.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">superscript</csymbol><cn id="S4.SS2.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS2.p1.3.m3.1.1.2">10</cn><apply id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3"><minus id="S4.SS2.p1.3.m3.1.1.3.1.cmml" xref="S4.SS2.p1.3.m3.1.1.3"></minus><cn id="S4.SS2.p1.3.m3.1.1.3.2.cmml" type="integer" xref="S4.SS2.p1.3.m3.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, and utilize polynomial learning rate scheduler. We use cross-entropy loss, and AdamW optimizer with the same configuration as the multimodal segmentation task. Batch size is 16, and we train the model for 20 epochs. The remaining configurations of this task are the default settings from <cite class="ltx_cite ltx_citemacro_citep">(Lee etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib16" title="">2023</a>)</cite>. For multimodal sentiment analysis we used the default settings from <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib47" title="">2021</a>)</cite>. Further implementation details are added in the supplementary materials of this paper.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results on Multimodal Segmentation</h3>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1007" id="S4.F3.g1" src="extracted/5897133/figures/visual_predictions.png" width="1047"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visualization of predicted segmentation maps for the Pretrained (CMNeXt) model and our MMP approach. Title above each image indicates the method name (available modalities). Blue boxes mark the areas where the differences are more prominent. A and D denote angle and degree of linear polarization, respectively.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.1.1.1.2">Backbone</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T4.1.1.1.3">Text-Visual-Audio</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T4.1.1.1.4">Visual-Audio</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T4.1.1.1.5">Audio</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T4.1.1.1.6">Average</th>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.1.2.2.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="S4.T4.1.2.2.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.3">Acc</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.4">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.5">Acc</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.6">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.7">Acc</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.8">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.9">Acc</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.10">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.3.1.1">MulT <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib34" title="">2019</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.3.1.2">Transformer</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.3.1.3.1">79.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.3.1.4.1">79.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.5">48.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.6">41.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.7">48.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.8">40.98</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.9">58.93</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.1.3.1.10">54.20</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.4.2.1">TFN <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib48" title="">2017</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.4.2.2">LSTM</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.3">73.90</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.4">73.40</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.5">42.23</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.6">25.07</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.7">42.23</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.8">25.07</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.9">52.78</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.4.2.10">41.18</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.5.3.1">LMF <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib20" title="">2018</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.5.3.2">LSTM</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.3">76.40</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.4">75.70</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.5">43.29</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.6">27.61</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.7">42.23</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.8">25.07</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.9">53.97</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.5.3.10">42.79</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.6.4.1">Reza et. al. <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib29" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.6.4.2">Transformer</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.4.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.6.4.3.1">79.57</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.4.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.6.4.4.1">79.67</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.4.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.5.1">55.49</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.4.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.6.1">53.96</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.4.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.6.4.7.1">50.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.4.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.6.4.8.1">46.71</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.4.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.6.4.9.1">61.68</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.6.4.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.6.4.10.1">60.11</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.1.7.5.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.7.5.1.1">MMP (Ours)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.1.7.5.2">Transformer</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.7.5.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.7.5.3.1">80.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.7.5.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.7.5.4.1">80.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.7.5.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.7.5.5.1">54.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.7.5.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.7.5.6.1">52.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.7.5.7"><span class="ltx_text ltx_font_bold" id="S4.T4.1.7.5.7.1">55.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.7.5.8"><span class="ltx_text ltx_font_bold" id="S4.T4.1.7.5.8.1">53.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.7.5.9"><span class="ltx_text ltx_font_bold" id="S4.T4.1.7.5.9.1">63.26</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.1.7.5.10"><span class="ltx_text ltx_font_bold" id="S4.T4.1.7.5.10.1">62.08</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance (binary accuracy and F1 score)
comparison with existing methods for multimodal sentiment analysis on CMU-MOSI dataset. Column names indicate available modalities. Best and second-best results are shown as <span class="ltx_text ltx_font_bold" id="S4.T4.4.1">bold</span> and <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.5.2">underlined</span>, respectively.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T5.1.1.1.1" style="padding-left:2.2pt;padding-right:2.2pt;">Methods</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.2" style="padding-left:2.2pt;padding-right:2.2pt;">RGB-Depth</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.3" style="padding-left:2.2pt;padding-right:2.2pt;">RGB</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.4" style="padding-left:2.2pt;padding-right:2.2pt;">Depth</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.5" style="padding-left:2.2pt;padding-right:2.2pt;">Average</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.2.1.1" style="padding-left:2.2pt;padding-right:2.2pt;">Dropout</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.2" style="padding-left:2.2pt;padding-right:2.2pt;">51.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.3" style="padding-left:2.2pt;padding-right:2.2pt;">48.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.4" style="padding-left:2.2pt;padding-right:2.2pt;">29.79</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.1.2.1.5" style="padding-left:2.2pt;padding-right:2.2pt;">43.23</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.1.3.2.1" style="padding-left:2.2pt;padding-right:2.2pt;">Dropout + LP</th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.2" style="padding-left:2.2pt;padding-right:2.2pt;">51.31</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.3" style="padding-left:2.2pt;padding-right:2.2pt;">51.08</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.4" style="padding-left:2.2pt;padding-right:2.2pt;">35.48</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.3.2.5" style="padding-left:2.2pt;padding-right:2.2pt;">45.95</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.1.4.3.1" style="padding-left:2.2pt;padding-right:2.2pt;">Dropout + LP + Align</th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.2" style="padding-left:2.2pt;padding-right:2.2pt;">52.84</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.3" style="padding-left:2.2pt;padding-right:2.2pt;">50.73</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.4" style="padding-left:2.2pt;padding-right:2.2pt;">40.60</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.4.3.5" style="padding-left:2.2pt;padding-right:2.2pt;">48.05</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T5.1.5.4.1" style="padding-left:2.2pt;padding-right:2.2pt;">Dropout + CA + Align</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.5.4.2" style="padding-left:2.2pt;padding-right:2.2pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.1.5.4.2.1">53.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.5.4.3" style="padding-left:2.2pt;padding-right:2.2pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.1.5.4.3.1">52.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.5.4.4" style="padding-left:2.2pt;padding-right:2.2pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.1.5.4.4.1">41.08</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T5.1.5.4.5" style="padding-left:2.2pt;padding-right:2.2pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.1.5.4.5.1">48.97</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation studies on NYUDv2 dataset. Modality dropout shows significant performance drop when RGB is missing. Performance increases as we apply linear projection (LP). Adding alignment loss (Align) improves performance further. Finally, replacing linear projection with cross attention (CA) shows overall best performance.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We present a comparison of multimodal segmentation performance across the MCubeS, NYUDv2, and FMB datasets in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.T1" title="Table 1 â€£ 3.4 Alignment Loss Objective â€£ 3 Method â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">1</span></a>. All experiments were conducted using the CMNeXt <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib50" title="">2023</a>)</cite> model to ensure consistency and fairness. <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Pretrained</span> column indicates the performance when we train the model without dropout augmentation and test the performance on different missing modality scenarios. <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.2">Modality dropout</span> column indicates the performance when we use dropout augmentation while training the model. <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.3">MMP</span> column indicates the performance when we utilize our modality projection approach to generate tokens for the missing modalities.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Our findings indicate that the pretrained model experiences a notable performance drop when modalities get missing during test time. Although modality dropout improves performance in missing modality scenarios, it does not fully mitigate the performance drop. In contrast, our MMP approach utilizes the available modalities to generate tokens for missing ones, further enhancing robustness and performance across all datasets. Specifically, our MMP approach outperforms both the pretrained model and modality dropout in every missing modality scenario, demonstrating its effectiveness in compensating for missing modalities.
The slightly reduced performance of the MMP approach when all modalities are available is due to the base model being pretrained with modality dropout, which has lower performance when all modalities are present.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">We report experimental results for different baseline methods on NYUDv2 in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.T2" title="Table 2 â€£ 3.4 Alignment Loss Objective â€£ 3 Method â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">2</span></a>. Compared to other methods, our MMP approach achieves superior average performance. When Depth is missing, MMP ranks second to Reza et. al. <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib29" title="">2023</a>)</cite>, which learns adaptable layers for different modality combinations. The performance difference is minimal (-0.78%), and MMP outperforms this method in both average (+1.79%) and RGB-missing (+4.36%) scenarios. This demonstrates that MMP has matched or exceeded the performance of this method while eliminating the computational overhead of adapting the model separately for each modality combination. When RGB is missing, MMP is the second-best to MMANet <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib40" title="">2023</a>)</cite>. Notably, MMANet relies on a teacher model trained with all modalities, yet our MMP approach achieves better performance in other cases without the added complexity of training a separate teacher model.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Visualization of Predictions</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To better demonstrate the impact of our approach, we visualize the predicted segmentation maps from the pretrained CMNeXt model and our MMP approach in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.F3" title="Figure 3 â€£ 4.3 Results on Multimodal Segmentation â€£ 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">3</span></a>. For each dataset, we show the RGB image and predictions from the pretrained model and MMP approach with different available modalities (available modality names are shown in parentheses above each image). We highlight the impact of presence of the RGB modality due to its greater emphasis in the CMNeXt modelâ€™s design.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.F3" title="Figure 3 â€£ 4.3 Results on Multimodal Segmentation â€£ 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">3</span></a>a, we observe that the pretrained model struggles to detect the bikes and cars when the Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP), and Near-Infrared (NIR) modalities are missing. When RGB is missing along with NIR, and only AoLP and DoLP are available, the pretrained model fails to perform any segmentation. In contrast, our MMP approach successfully detects the bikes and cars even with missing modalities. On NYUDv2 dataset, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.F3" title="Figure 3 â€£ 4.3 Results on Multimodal Segmentation â€£ 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">3</span></a>b, MMP demonstrates superior accuracy in detecting kitchen cabinets, counter-tops, windows, and beds compared to the pretrained model with missing modalities. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.F3" title="Figure 3 â€£ 4.3 Results on Multimodal Segmentation â€£ 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">3</span></a>c, the pretrained model fails to detect the car, bicyclist, and humans when modalities are missing. However, our MMP approach successfully detects all these objects in every scenario. In all examples, our MMP predictions are either better or comparable to the pretrained model with all modalities available. When modalities are missing, MMP consistently outperforms the pretrained model and closely matches the performance achieved with full input.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Results on Multimodal Classification</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">We further evaluate the effectiveness of our approach by performing a comparison against the missing-aware prompts method <cite class="ltx_cite ltx_citemacro_citep">(Lee etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib16" title="">2023</a>)</cite> using the UPMC Food-101 <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib36" title="">2015</a>)</cite> dataset. Experiments on multimodal classification task were conducted on ViLT <cite class="ltx_cite ltx_citemacro_citep">(Kim, Son, and Kim <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib15" title="">2021</a>)</cite> as the base model. The results are detailed in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.T3" title="Table 3 â€£ 3.4 Alignment Loss Objective â€£ 3 Method â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">3</span></a>. Our MMP outperforms the prompting-based methods in most scenarios, achieving better overall results. Our performance shows a slight decrease in two cases, 0.21% lower when 70% of text is missing, and 2.04% lower when no text is available. This is because prompting-based methods learn one set of prompts for each missing modality scenario and thus outperform MMP in certain modality combinations. Notably, our approach maintains strong performance across various missing modality scenarios without requiring training for every possible modality combination.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Results on Multimodal Sentiment Analysis</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">We evaluated our approach on the CMU-MOSI dataset <cite class="ltx_cite ltx_citemacro_citep">(Zadeh etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib49" title="">2016</a>)</cite> for multimodal sentiment analysis, with results presented in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.T4" title="Table 4 â€£ 4.3 Results on Multimodal Segmentation â€£ 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">4</span></a>. We utilized the multimodal transformer (MulT) <cite class="ltx_cite ltx_citemacro_citep">(Tsai etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib34" title="">2019</a>)</cite> as the base model for this task. Our findings indicate that when the text modality is present, missing audio, video, or both has little impact on performance, as noted in <cite class="ltx_cite ltx_citemacro_citet">Hazarika etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib10" title="">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Reza, Prater-Bennette, and Asif (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib29" title="">2023</a>)</cite>. However, performance drops significantly without the text modality. Our MMP approach effectively compensates for this, providing a substantial improvement over the base MulT model. Specifically, we observe a 5.8% improvement in accuracy and a 10.29% increase in F1 score when text is missing. When only the audio modality is available, and both text and visual modalities are absent, MMP achieves larger improvements of 6.72% in accuracy and 13% in F1 score. MMP outperforms existing methods in both accuracy and F1 score across all scenarios, except when only text is missing, where it ranks second. The performance difference between MMP and the best-performing method (Reza et. al. <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib29" title="">2023</a>)</cite>) in this case is minimal. However, MMP surpasses this method by a large margin in other scenarios, resulting in higher average accuracy and F1 scores overall.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Ablation Studies</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">To further investigate the contributions of various components in our proposed MMP approach, we conducted an ablation study on the NYUDv2 dataset. TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S4.T5" title="Table 5 â€£ 4.3 Results on Multimodal Segmentation â€£ 4 Experiments and Results â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">5</span></a> summarizes the results. We began with evaluating the performance of modality dropout, which serves as a baseline for comparison. Modality dropout shows significant performance drop when RGB is missing. Then we add a single linear layer as the projection function (LP) with modality dropout, to predict the tokens of the dropped modality using a linear combination of the available ones. This approach led to performance improvements across all scenarios. Next, we utilized an alignment loss (Align) objective with the linear projection and modality dropout. We observed further performance improvement in all cases, particularly when RGB was missing. Finally, we replaced the single linear layer with MMPâ€™s cross-attention based (CA) projection approach, combined with modality dropout and alignment loss. This final configuration achieved the highest performance among all the tested setups. Based on these experimental results, we argue that each component in our MMP module plays a critical role in enhancing the overall model performance in different missing modality scenarios.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we introduced Masked Modality Projection (MMP), a novel approach designed to enhance missing modality robustness of multimodal models. Our approach eliminates the need for training or adapting models for specific missing modality scenarios. We demonstrate that a single model can effectively handle any missing modality scenario and outperform current baselines. Thus it reduces both time and computational overhead. Experimental results across several baseline models and datasets validate that MMP significantly improves performance and robustness compared to existing baseline methods. Future work will focus on further refining MMP and exploring its applicability to other multimodal tasks and datasets. We believe that MMP offers an efficient and effective solution to the challenge of missing modalities.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Acknowledgment:</span>
This work is supported in part by AFOSR award FA9550-21-1-0330 and an Amazon Gift award.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bachmann etÂ al. (2022)</span>
<span class="ltx_bibblock">
Bachmann, R.; Mizrahi, D.; Atanov, A.; and Zamir, A. 2022.

</span>
<span class="ltx_bibblock">MultiMAE: Multi-modal Multi-task Masked Autoencoders.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">European Conference on Computer Vision</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BaltruÅ¡aitis, Ahuja, and Morency (2018)</span>
<span class="ltx_bibblock">
BaltruÅ¡aitis, T.; Ahuja, C.; and Morency, L.-P. 2018.

</span>
<span class="ltx_bibblock">Multimodal machine learning: A survey and taxonomy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 41(2): 423â€“443.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bossard, Guillaumin, and VanÂ Gool (2014)</span>
<span class="ltx_bibblock">
Bossard, L.; Guillaumin, M.; and VanÂ Gool, L. 2014.

</span>
<span class="ltx_bibblock">Food-101â€“mining discriminative components with random forests.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Computer visionâ€“ECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part VI 13</em>, 446â€“461. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2020)</span>
<span class="ltx_bibblock">
Chen, X.; Lin, K.-Y.; Wang, J.; Wu, W.; Qian, C.; Li, H.; and Zeng, G. 2020.

</span>
<span class="ltx_bibblock">Bi-directional cross-modality feature propagation with separation-and-aggregation gate for RGB-D semantic segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">European conference on computer vision</em>, 561â€“577. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi and Lee (2019)</span>
<span class="ltx_bibblock">
Choi, J.-H.; and Lee, J.-S. 2019.

</span>
<span class="ltx_bibblock">EmbraceNet: A robust deep learning architecture for multimodal classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Information Fusion</em>, 51: 259â€“270.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding, Yu, and Yang (2021)</span>
<span class="ltx_bibblock">
Ding, Y.; Yu, X.; and Yang, Y. 2021.

</span>
<span class="ltx_bibblock">RFNet: Region-aware fusion network for incomplete multi-modal brain tumor segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 3975â€“3984.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dorent etÂ al. (2019)</span>
<span class="ltx_bibblock">
Dorent, R.; Joutard, S.; Modat, M.; Ourselin, S.; and Vercauteren, T. 2019.

</span>
<span class="ltx_bibblock">Hetero-modal variational encoder-decoder for joint modality completion and segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Medical Image Computing and Computer Assisted Interventionâ€“MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13â€“17, 2019, Proceedings, Part II 22</em>, 74â€“82. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan etÂ al. (2023)</span>
<span class="ltx_bibblock">
Fan, S.; Wang, Z.; Wang, Y.; and Liu, J. 2023.

</span>
<span class="ltx_bibblock">SpiderMesh: Spatial-aware Demand-guided Recursive Meshing for RGB-T Semantic Segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv:2303.08692</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Havaei etÂ al. (2016)</span>
<span class="ltx_bibblock">
Havaei, M.; Guizard, N.; Chapados, N.; and Bengio, Y. 2016.

</span>
<span class="ltx_bibblock">Hemis: Hetero-modal image segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Medical Image Computing and Computer-Assisted Interventionâ€“MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19</em>, 469â€“477. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hazarika etÂ al. (2022)</span>
<span class="ltx_bibblock">
Hazarika, D.; Li, Y.; Cheng, B.; Zhao, S.; Zimmermann, R.; and Poria, S. 2022.

</span>
<span class="ltx_bibblock">Analyzing Modality Robustness in Multimodal Sentiment Analysis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 685â€“696. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Huang, Y.; Du, C.; Xue, Z.; Chen, X.; Zhao, H.; and Huang, L. 2021.

</span>
<span class="ltx_bibblock">What makes multi-modal learning better than single (provably).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Advances in Neural Information Processing Systems</em>, 34: 10944â€“10956.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HussenÂ Abdelaziz etÂ al. (2020)</span>
<span class="ltx_bibblock">
HussenÂ Abdelaziz, A.; Theobald, B.-J.; Dixon, P.; Knothe, R.; Apostoloff, N.; and Kajareker, S. 2020.

</span>
<span class="ltx_bibblock">Modality dropout for improved performance-driven talking faces.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2020 International Conference on Multimodal Interaction</em>, 378â€“386.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang, Wang, and Kim (2024)</span>
<span class="ltx_bibblock">
Jang, J.; Wang, Y.; and Kim, C. 2024.

</span>
<span class="ltx_bibblock">Towards Robust Multimodal Prompting with Missing Modalities.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 8070â€“8074. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimijafarbigloo etÂ al. (2024)</span>
<span class="ltx_bibblock">
Karimijafarbigloo, S.; Azad, R.; Kazerouni, A.; Ebadollahi, S.; and Merhof, D. 2024.

</span>
<span class="ltx_bibblock">Mmcformer: Missing modality compensation transformer for brain tumor segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Medical Imaging with Deep Learning</em>, 1144â€“1162. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim, Son, and Kim (2021)</span>
<span class="ltx_bibblock">
Kim, W.; Son, B.; and Kim, I. 2021.

</span>
<span class="ltx_bibblock">Vilt: Vision-and-language transformer without convolution or region supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">International conference on machine learning</em>, 5583â€“5594. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al. (2023)</span>
<span class="ltx_bibblock">
Lee, Y.-L.; Tsai, Y.-H.; Chiu, W.-C.; and Lee, C.-Y. 2023.

</span>
<span class="ltx_bibblock">Multimodal prompting with missing modalities for visual recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 14943â€“14952.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Liang, Y.; Wakaki, R.; Nobuhara, S.; and Nishino, K. 2022.

</span>
<span class="ltx_bibblock">Multimodal Material Segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 19800â€“19808.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2023)</span>
<span class="ltx_bibblock">
Lin, B.; Lin, Z.; Guo, Y.; Zhang, Y.; Zou, J.; and Fan, S. 2023.

</span>
<span class="ltx_bibblock">Variational Probabilistic Fusion Network for RGB-T Semantic Segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2307.08536</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Liu, J.; Liu, Z.; Wu, G.; Ma, L.; Liu, R.; Zhong, W.; Luo, Z.; and Fan, X. 2023.

</span>
<span class="ltx_bibblock">Multi-interactive feature learning and a full-time multi-modality benchmark for image fusion and segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 8115â€“8124.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2018)</span>
<span class="ltx_bibblock">
Liu, Z.; Shen, Y.; Lakshminarasimhan, V.Â B.; Liang, P.Â P.; BagherÂ Zadeh, A.; and Morency, L.-P. 2018.

</span>
<span class="ltx_bibblock">Efficient Low-rank Multimodal Fusion With Modality-Specific Factors.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2247â€“2256. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Loshchilov, I.; and Hutter, F. 2019.

</span>
<span class="ltx_bibblock">Decoupled Weight Decay Regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu (2024)</span>
<span class="ltx_bibblock">
Lu, Z. 2024.

</span>
<span class="ltx_bibblock">A theory of multimodal learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2022)</span>
<span class="ltx_bibblock">
Ma, M.; Ren, J.; Zhao, L.; Testuggine, D.; and Peng, X. 2022.

</span>
<span class="ltx_bibblock">Are multimodal transformers robust to missing modality?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 18177â€“18186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2021)</span>
<span class="ltx_bibblock">
Ma, M.; Ren, J.; Zhao, L.; Tulyakov, S.; Wu, C.; and Peng, X. 2021.

</span>
<span class="ltx_bibblock">Smil: Multimodal learning with severely missing modality.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volumeÂ 35, 2302â€“2310.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maheshwari, Liu, and Kira (2024)</span>
<span class="ltx_bibblock">
Maheshwari, H.; Liu, Y.-C.; and Kira, Z. 2024.

</span>
<span class="ltx_bibblock">Missing modality robustness in semi-supervised multi-modal semantic segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 1009â€“1019. IEEE Computer Society.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo and Morgado (2024)</span>
<span class="ltx_bibblock">
Mo, S.; and Morgado, P. 2024.

</span>
<span class="ltx_bibblock">Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 27186â€“27196.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neverova etÂ al. (2015)</span>
<span class="ltx_bibblock">
Neverova, N.; Wolf, C.; Taylor, G.; and Nebout, F. 2015.

</span>
<span class="ltx_bibblock">ModDrop: Adaptive multi-modal gesture recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 38(8): 1692â€“1706.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Qu, J.; Yang, Y.; Dong, W.; and Yang, Y. 2024.

</span>
<span class="ltx_bibblock">LDS2AE: Local Diffusion Shared-Specific Autoencoder for Multimodal Remote Sensing Image Classification with Arbitrary Missing Modalities.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volumeÂ 38, 14731â€“14739.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reza, Prater-Bennette, and Asif (2023)</span>
<span class="ltx_bibblock">
Reza, M.Â K.; Prater-Bennette, A.; and Asif, M.Â S. 2023.

</span>
<span class="ltx_bibblock">Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2310.03986</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma and Hamarneh (2019)</span>
<span class="ltx_bibblock">
Sharma, A.; and Hamarneh, G. 2019.

</span>
<span class="ltx_bibblock">Missing MRI pulse sequence synthesis using multi-modal generative adversarial network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">IEEE Transactions on Medical Imaging</em>, 39(4): 1170â€“1183.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin, Lee, and Kweon (2023)</span>
<span class="ltx_bibblock">
Shin, U.; Lee, K.; and Kweon, I.Â S. 2023.

</span>
<span class="ltx_bibblock">Complementary Random Masking for RGB-Thermal Semantic Segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2303.17386</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silberman etÂ al. (2012)</span>
<span class="ltx_bibblock">
Silberman, N.; Hoiem, D.; Kohli, P.; and Fergus, R. 2012.

</span>
<span class="ltx_bibblock">Indoor Segmentation and Support Inference from RGBD Images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">European Conference on Computer Vision</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tarvainen and Valpola (2017)</span>
<span class="ltx_bibblock">
Tarvainen, A.; and Valpola, H. 2017.

</span>
<span class="ltx_bibblock">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsai etÂ al. (2019)</span>
<span class="ltx_bibblock">
Tsai, Y.-H.Â H.; Bai, S.; Liang, P.Â P.; Kolter, J.Â Z.; Morency, L.-P.; and Salakhutdinov, R. 2019.

</span>
<span class="ltx_bibblock">Multimodal transformer for unaligned multimodal language sequences.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the conference. Association for computational linguistics. Meeting</em>, volume 2019, 6558. NIH Public Access.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wang, H.; Chen, Y.; Ma, C.; Avery, J.; Hull, L.; and Carneiro, G. 2023.

</span>
<span class="ltx_bibblock">Multi-modal learning with missing modality via shared-specific feature modelling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 15878â€“15887.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2015)</span>
<span class="ltx_bibblock">
Wang, X.; Kumar, D.; Thome, N.; Cord, M.; and Precioso, F. 2015.

</span>
<span class="ltx_bibblock">Recipe recognition with large multimodal food dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">2015 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</em>, 1â€“6. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Wang, Y.; Chen, X.; Cao, L.; Huang, W.; Sun, F.; and Wang, Y. 2022.

</span>
<span class="ltx_bibblock">Multimodal token fusion for vision transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 12186â€“12195.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2020a)</span>
<span class="ltx_bibblock">
Wang, Y.; Huang, W.; Sun, F.; Xu, T.; Rong, Y.; and Huang, J. 2020a.

</span>
<span class="ltx_bibblock">Deep multimodal fusion by channel exchanging.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Advances in neural information processing systems</em>, 33: 4835â€“4845.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2020b)</span>
<span class="ltx_bibblock">
Wang, Y.; Sun, F.; Lu, M.; and Yao, A. 2020b.

</span>
<span class="ltx_bibblock">Learning deep multimodal feature representation with asymmetric multi-layer fusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 28th ACM International Conference on Multimedia</em>, 3902â€“3910.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei, Luo, and Luo (2023)</span>
<span class="ltx_bibblock">
Wei, S.; Luo, C.; and Luo, Y. 2023.

</span>
<span class="ltx_bibblock">MMANet: Margin-aware distillation and modality-aware regularization for incomplete multimodal learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 20039â€“20049.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Woo etÂ al. (2023)</span>
<span class="ltx_bibblock">
Woo, S.; Lee, S.; Park, Y.; Nugroho, M.Â A.; and Kim, C. 2023.

</span>
<span class="ltx_bibblock">Towards good practices for missing modality robust action recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volumeÂ 37, 2776â€“2784.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wortsman etÂ al. (2022)</span>
<span class="ltx_bibblock">
Wortsman, M.; Ilharco, G.; Kim, J.Â W.; Li, M.; Kornblith, S.; Roelofs, R.; Lopes, R.Â G.; Hajishirzi, H.; Farhadi, A.; and Namkoong, H. 2022.

</span>
<span class="ltx_bibblock">Robust fine-tuning of zero-shot models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 7959â€“7971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Wu, R.; Wang, H.; Dayoub, F.; and Chen, H.-T. 2024.

</span>
<span class="ltx_bibblock">Segment Beyond View: Handling Partially Missing Modality for Audio-Visual Semantic Segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volumeÂ 38, 6100â€“6108.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2021)</span>
<span class="ltx_bibblock">
Xie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J.Â M.; and Luo, P. 2021.

</span>
<span class="ltx_bibblock">SegFormer: Simple and efficient design for semantic segmentation with transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Advances in neural information processing systems</em>, 34: 12077â€“12090.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu, Zhu, and Clifton (2023)</span>
<span class="ltx_bibblock">
Xu, P.; Zhu, X.; and Clifton, D.Â A. 2023.

</span>
<span class="ltx_bibblock">Multimodal learning with transformers: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2018)</span>
<span class="ltx_bibblock">
Yu, B.; Zhou, L.; Wang, L.; Fripp, J.; and Bourgeat, P. 2018.

</span>
<span class="ltx_bibblock">3D cGAN based cross-modality MR image synthesis for brain tumor segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</em>, 626â€“630. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Yu, W.; Xu, H.; Yuan, Z.; and Wu, J. 2021.

</span>
<span class="ltx_bibblock">Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volumeÂ 35, 10790â€“10797.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zadeh etÂ al. (2017)</span>
<span class="ltx_bibblock">
Zadeh, A.; Chen, M.; Poria, S.; Cambria, E.; and Morency, L.-P. 2017.

</span>
<span class="ltx_bibblock">Tensor Fusion Network for Multimodal Sentiment Analysis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, 1103â€“1114. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zadeh etÂ al. (2016)</span>
<span class="ltx_bibblock">
Zadeh, A.; Zellers, R.; Pincus, E.; and Morency, L.-P. 2016.

</span>
<span class="ltx_bibblock">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">IEEE Intelligent Systems</em>, 31(6): 82â€“88.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhang, J.; Liu, R.; Shi, H.; Yang, K.; ReiÃŸ, S.; Peng, K.; Fu, H.; Wang, K.; and Stiefelhagen, R. 2023.

</span>
<span class="ltx_bibblock">Delivering arbitrary-modal semantic segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 1136â€“1147.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhang, Y.; Peng, C.; Wang, Q.; Song, D.; Li, K.; and Zhou, S.Â K. 2024.

</span>
<span class="ltx_bibblock">Unified multi-modal image synthesis for missing modality imputation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">IEEE Transactions on Medical Imaging</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Supplementary Material 
<br class="ltx_break"/>MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection</span></p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="598" id="S0.F1.g1" src="extracted/5897133/figures/food101-image-missing.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure S1: </span>Average cosine similarity between model predictions with real and projected tokens on UPMC Food-101 dataset when image is missing. We substitute the missing modality tokens with the projected tokens.</figcaption>
</figure>
<figure class="ltx_figure" id="S0.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="598" id="S0.F2.g1" src="extracted/5897133/figures/food101-text-missing.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure S2: </span>Average cosine similarity between model predictions with real and projected tokens on UPMC Food-101 dataset when text is missing. We substitute the missing modality tokens with the projected tokens.</figcaption>
</figure>
<section class="ltx_section" id="S1a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S1 </span>Datasets</h2>
<div class="ltx_para" id="S1a.p1">
<p class="ltx_p" id="S1a.p1.1">In this section, we provide an overview of the datasets used in our experiments. The motivation for selecting these datasets lies in their popularity and widespread usage in the field. These datasets cover a broad range of tasks, including multimodal segmentation, classification, and sentiment analysis. We utilize these datasets to include both homogeneous and heterogeneous data types, aiming to conduct a comprehensive evaluation of the effectiveness of our approach across different modality types.</p>
</div>
<div class="ltx_para" id="S1a.p2">
<p class="ltx_p" id="S1a.p2.1"><span class="ltx_text ltx_font_bold" id="S1a.p2.1.1">MCubeS dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib17" title="">2022</a>)</cite> is a multi-modal dataset featuring four distinct input modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP), and Near-Infrared (NIR). The dataset is organized into three subsets: 302 sets of images for training, 96 sets of images for validation, and 102 sets of images for testing. The dataset is annotated with per-pixel labels across 20 different material classes. This detailed annotation allows for comprehensive analysis and modeling of material properties under diverse imaging conditions.</p>
</div>
<div class="ltx_para" id="S1a.p3">
<p class="ltx_p" id="S1a.p3.1"><span class="ltx_text ltx_font_bold" id="S1a.p3.1.1">NYUDv2 dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Silberman etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib32" title="">2012</a>)</cite> comprises 1,449 aligned RGB-Depth image pairs, which are divided into 795 pairs for training and 654 pairs for testing. Each image pair has a resolution of 640 Ã— 480 pixels and includes detailed annotations for 40 distinct classes. For our experiments, we utilize HHA-encoded images instead of raw depth maps following recent studies <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib50" title="">2023</a>; Reza, Prater-Bennette, and Asif <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib29" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1a.p4">
<p class="ltx_p" id="S1a.p4.1"><span class="ltx_text ltx_font_bold" id="S1a.p4.1.1">FMB dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib19" title="">2023</a>)</cite> is a comprehensive dataset consisting of 1,500 pairs of calibrated RGB-Infrared images. The dataset is divided into 1,220 image pairs for training and 280 image pairs for testing. It covers a broad spectrum of scenes, including various lighting and weather conditions such as the Tyndall effect, rain, fog, and intense lighting. It has per-pixel ground truth annotation for 14 classes.</p>
</div>
<div class="ltx_para" id="S1a.p5">
<p class="ltx_p" id="S1a.p5.1"><span class="ltx_text ltx_font_bold" id="S1a.p5.1.1">UPMC Food-101 dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib36" title="">2015</a>)</cite> is widely used for multimodal classification. It contains 90,704 image-text pairs, which are split into training, validation, and test sets. The dataset features annotations for 101 distinct classes, consistent with those in the ETHZ Food-101 dataset <cite class="ltx_cite ltx_citemacro_citep">(Bossard, Guillaumin, and VanÂ Gool <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib3" title="">2014</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1a.p6">
<p class="ltx_p" id="S1a.p6.1"><span class="ltx_text ltx_font_bold" id="S1a.p6.1.1">CMU-MOSI dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Zadeh etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib49" title="">2016</a>)</cite> is a popular dataset for multimodal sentiment analysis. It consists of 2,199 samples, each including audio, visual, and text modalities. The dataset is split into training, validation, and test sets with 1,284, 229, and 686 samples respectively, and includes sentiment annotations for each sample.</p>
</div>
</section>
<section class="ltx_section" id="S2a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S2 </span>Implementation Details</h2>
<div class="ltx_para" id="S2a.p1">
<p class="ltx_p" id="S2a.p1.1">We use Python<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.python.org/</span></span></span> 3.8.18 and PyTorch<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://pytorch.org/</span></span></span> 2.1.1 for multimodal segmentation and classification, and Python 3.10.14 and Pytorch 2.4.0+cu121 for sentiment analysis task. Experiments are conducted using two NVIDIA RTX 2080 Ti GPUs with 16G memory. The code is configured with fixed seeds to ensure the results can be replicated.</p>
</div>
<div class="ltx_para" id="S2a.p2">
<p class="ltx_p" id="S2a.p2.1">To assess missing modality performance, we provide the available modalities to the model while setting the missing modalities to zero for visual and audio data, and to empty strings for texts.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S2.1 </span>Multimodal segmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.2">We use CMNeXt <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib50" title="">2023</a>)</cite> as the base model for multimodal segmentation. We use their publicly available code<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/jamycheung/DELIVER/</span></span></span> to train the base model with dropout augmentation. Then we use the resulting weights to initialize our models. The learning rate is set to <math alttext="6\times 10^{-5}" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">6</mn><mo id="S2.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p1.1.m1.1.1.1.cmml">Ã—</mo><msup id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml"><mn id="S2.SS1.p1.1.m1.1.1.3.2" xref="S2.SS1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.SS1.p1.1.m1.1.1.3.3" xref="S2.SS1.p1.1.m1.1.1.3.3.cmml"><mo id="S2.SS1.p1.1.m1.1.1.3.3a" xref="S2.SS1.p1.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="S2.SS1.p1.1.m1.1.1.3.3.2" xref="S2.SS1.p1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><times id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></times><cn id="S2.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1.2">6</cn><apply id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.3.1.cmml" xref="S2.SS1.p1.1.m1.1.1.3">superscript</csymbol><cn id="S2.SS1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1.3.2">10</cn><apply id="S2.SS1.p1.1.m1.1.1.3.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3"><minus id="S2.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3"></minus><cn id="S2.SS1.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">6\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">6 Ã— 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> and polynomial learning rate scheduler is applied
with power = 0.9. The first 10 epochs are warm-up epochs and the learning rate is set to 0.1 times the original rate. We use OHEM cross-entropy loss for FMB and MCubeS datasets, and cross-entropy loss for NYUDv2 dataset. For the optimizer, we utilize AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib21" title="">2019</a>)</cite> with <math alttext="\epsilon=10^{-8}" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">Ïµ</mi><mo id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.cmml">=</mo><msup id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml"><mn id="S2.SS1.p1.2.m2.1.1.3.2" xref="S2.SS1.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="S2.SS1.p1.2.m2.1.1.3.3" xref="S2.SS1.p1.2.m2.1.1.3.3.cmml"><mo id="S2.SS1.p1.2.m2.1.1.3.3a" xref="S2.SS1.p1.2.m2.1.1.3.3.cmml">âˆ’</mo><mn id="S2.SS1.p1.2.m2.1.1.3.3.2" xref="S2.SS1.p1.2.m2.1.1.3.3.2.cmml">8</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><eq id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1"></eq><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">italic-Ïµ</ci><apply id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3">superscript</csymbol><cn id="S2.SS1.p1.2.m2.1.1.3.2.cmml" type="integer" xref="S2.SS1.p1.2.m2.1.1.3.2">10</cn><apply id="S2.SS1.p1.2.m2.1.1.3.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3"><minus id="S2.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3"></minus><cn id="S2.SS1.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="S2.SS1.p1.2.m2.1.1.3.3.2">8</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\epsilon=10^{-8}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">italic_Ïµ = 10 start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT</annotation></semantics></math> and weight decay = 0.01. We train the model with a batch size of 4 for 500 epochs on the MCubeS and NYUDv2 datasets. For the FMB dataset, we use a batch size of 2 and train for 120 epochs.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="300" id="S2.F3.g1" src="extracted/5897133/figures/Mosi.jpeg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure S3: </span>Average Mean Squared Error (MSE) between model predictions with real and projected tokens on CMU-MOSI dataset.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.3"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.3.1">MCubeS dataset:</span> We adopt the data pre-processing and augmentation techniques outlined in <cite class="ltx_cite ltx_citemacro_citet">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib50" title="">2023</a>)</cite>. The MiT-B2 backbone from <cite class="ltx_cite ltx_citemacro_citet">Xie etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib44" title="">2021</a>)</cite> is utilized for this dataset. For training, the input images are resized to <math alttext="512\times 512" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mn id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">512</mn><mo id="S2.SS1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><times id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></times><cn id="S2.SS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S2.SS1.p2.1.m1.1.1.2">512</cn><cn id="S2.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.p2.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">512 Ã— 512</annotation></semantics></math>, while during testing, they are set to <math alttext="1024\times 1024" class="ltx_Math" display="inline" id="S2.SS1.p2.2.m2.1"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mn id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">1024</mn><mo id="S2.SS1.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><times id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1"></times><cn id="S2.SS1.p2.2.m2.1.1.2.cmml" type="integer" xref="S2.SS1.p2.2.m2.1.1.2">1024</cn><cn id="S2.SS1.p2.2.m2.1.1.3.cmml" type="integer" xref="S2.SS1.p2.2.m2.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">1024 Ã— 1024</annotation></semantics></math>. The results are reported based on single-scale performance using predicted segmentation maps at <math alttext="1024\times 1024" class="ltx_Math" display="inline" id="S2.SS1.p2.3.m3.1"><semantics id="S2.SS1.p2.3.m3.1a"><mrow id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><mn id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2.cmml">1024</mn><mo id="S2.SS1.p2.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p2.3.m3.1.1.1.cmml">Ã—</mo><mn id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><times id="S2.SS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1.1"></times><cn id="S2.SS1.p2.3.m3.1.1.2.cmml" type="integer" xref="S2.SS1.p2.3.m3.1.1.2">1024</cn><cn id="S2.SS1.p2.3.m3.1.1.3.cmml" type="integer" xref="S2.SS1.p2.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.3.m3.1d">1024 Ã— 1024</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">NYUDv2 dataset:</span> We utilize HHA-encoded images rather than raw depth maps following SA-Gate <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib4" title="">2020</a>)</cite> and CMNeXt <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib50" title="">2023</a>)</cite>. The preprocessed dataset can be downloaded from the SA-Gate repository<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/charlesCXK/ 
<br class="ltx_break"/>RGBD_Semantic_Segmentation_PyTorch/</span></span></span>. Both RGB and HHA images are resized to <math alttext="640\times 480" class="ltx_Math" display="inline" id="S2.SS1.p3.1.m1.1"><semantics id="S2.SS1.p3.1.m1.1a"><mrow id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml"><mn id="S2.SS1.p3.1.m1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.2.cmml">640</mn><mo id="S2.SS1.p3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p3.1.m1.1.1.1.cmml">Ã—</mo><mn id="S2.SS1.p3.1.m1.1.1.3" xref="S2.SS1.p3.1.m1.1.1.3.cmml">480</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"><times id="S2.SS1.p3.1.m1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1"></times><cn id="S2.SS1.p3.1.m1.1.1.2.cmml" type="integer" xref="S2.SS1.p3.1.m1.1.1.2">640</cn><cn id="S2.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.p3.1.m1.1.1.3">480</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">640\times 480</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.1.m1.1d">640 Ã— 480</annotation></semantics></math> pixels for both training and testing. Following the recommendation in the CMNeXt paper, we employ MiT-B4 backbone from <cite class="ltx_cite ltx_citemacro_citet">Xie etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib44" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p4.1.1">FMB dataset:</span> We follow the data pre-processing and augmentations used by <cite class="ltx_cite ltx_citemacro_citet">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib19" title="">2023</a>)</cite>, and MiT-B3 from <cite class="ltx_cite ltx_citemacro_citet">Xie etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib44" title="">2021</a>)</cite> is used as the backbone. We set the input image resolution to <math alttext="800\times 600" class="ltx_Math" display="inline" id="S2.SS1.p4.1.m1.1"><semantics id="S2.SS1.p4.1.m1.1a"><mrow id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml"><mn id="S2.SS1.p4.1.m1.1.1.2" xref="S2.SS1.p4.1.m1.1.1.2.cmml">800</mn><mo id="S2.SS1.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p4.1.m1.1.1.1.cmml">Ã—</mo><mn id="S2.SS1.p4.1.m1.1.1.3" xref="S2.SS1.p4.1.m1.1.1.3.cmml">600</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><apply id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1"><times id="S2.SS1.p4.1.m1.1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1.1"></times><cn id="S2.SS1.p4.1.m1.1.1.2.cmml" type="integer" xref="S2.SS1.p4.1.m1.1.1.2">800</cn><cn id="S2.SS1.p4.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.p4.1.m1.1.1.3">600</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">800\times 600</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.1.m1.1d">800 Ã— 600</annotation></semantics></math> during both training and testing.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S2.2 </span>Multimodal classification</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">UPMC Food-101 dataset:</span> For multimodal classification task, we use ViLT <cite class="ltx_cite ltx_citemacro_citep">(Kim, Son, and Kim <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib15" title="">2021</a>)</cite> as our base model. We set the learning rate to <math alttext="10^{-5}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><msup id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml"><mo id="S2.SS2.p1.1.m1.1.1.3a" xref="S2.SS2.p1.1.m1.1.1.3.cmml">âˆ’</mo><mn id="S2.SS2.p1.1.m1.1.1.3.2" xref="S2.SS2.p1.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">superscript</csymbol><cn id="S2.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.SS2.p1.1.m1.1.1.2">10</cn><apply id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3"><minus id="S2.SS2.p1.1.m1.1.1.3.1.cmml" xref="S2.SS2.p1.1.m1.1.1.3"></minus><cn id="S2.SS2.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S2.SS2.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, and polynomial learning rate scheduler is applied with power = 0.9, ratio = 0.1, and the first 2500 steps as the warm-up. We use cross-entropy loss, and AdamW optimizer with a weight decay of 0.02. Batch size is 16, and we train the model for 20 epochs. The remaining configurations for this task are the same as <cite class="ltx_cite ltx_citemacro_citet">Lee etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib16" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S2.3 </span>Multimodal sentiment analysis</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p1.1.1">CMU-MOSI dataset:</span> For multimodal sentiment analysis, we use multimodal transformer <cite class="ltx_cite ltx_citemacro_citep">(Tsai etÂ al. <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib34" title="">2019</a>)</cite> from this publicly available code<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/thuiar/MMSA/</span></span></span>, and use the default configurations from <cite class="ltx_cite ltx_citemacro_citet">Yu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#bib.bib47" title="">2021</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S2.4 </span>Reproducibility Statement</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">We made our source code and pretrained models available at this anonymous link<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://drive.google.com/drive/folders/155IsgLD88-Dt6Q9DJCKCENuDF7vjI9jI</span></span></span> to ensure the reproducibility of our results, and to facilitate the community in building upon our work. A comprehensive README.md file is added containing detailed instructions to set up environment, execute code, and reproduce the results. The main experimental results presented in this paper can be reproduced using the provided scripts, pretrained model weights, and instructions.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S3 </span>Projected and Real Tokens Alignment</h2>
<div class="ltx_para" id="S3a.p1">
<p class="ltx_p" id="S3a.p1.1">In this section, we examine the alignment between the real tokens of a modality and the projected tokens generated from the available modalities using our MMP approach. The alignment of these tokens is crucial. This ensures that the model can effectively predict and substitute the missing modality tokens from the available modalities. We employed a smooth L1 loss to align the projected tokens with the real tokens during training as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S3.SS4" title="3.4 Alignment Loss Objective â€£ 3 Method â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">3.4</span></a>. This alignment loss minimizes the discrepancy between the projected tokens and the original tokens. To evaluate the effectiveness of this alignment, we employ cosine similarity for multimodal classification on the UPMC Food-101 dataset and Mean Squared Error (MSE) for multimodal sentiment analysis on the CMU-MOSI dataset. These metrics help us visualize the degree to which the modelâ€™s predictions align when using real tokens versus projected tokens during testing.</p>
</div>
<section class="ltx_subsection" id="S3.SS1a">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S3.1 </span>UPMC Food-101 dataset</h3>
<div class="ltx_para" id="S3.SS1a.p1">
<p class="ltx_p" id="S3.SS1a.p1.1">The first analysis is conducted on the UPMC Food-101 dataset, as illustrated in Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S0.F1" title="Figure S1 â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">S1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S0.F2" title="Figure S2 â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">S2</span></a>. These figures show the cosine similarity between the modelâ€™s predictions using real tokens and projected tokens during testing. We show the cosine similarity of the first 50 out of 101 classes. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S0.F1" title="Figure S1 â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">S1</span></a> visualizes the scenario where the image modality is missing, and its tokens are replaced with projected tokens generated from the available text modality. The results show high cosine similarity, with all similarity scores exceeding 0.75 and most classes achieving scores above 0.8.</p>
</div>
<div class="ltx_para" id="S3.SS1a.p2">
<p class="ltx_p" id="S3.SS1a.p2.1">Similarly, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S0.F2" title="Figure S2 â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">S2</span></a> shows the cosine similarity when the text modality is missing. We see similar pattern here, with scores consistently above 0.7 and mostly above 0.8. This indicates that the MMP approach effectively projects the available modality tokens to estimate the missing ones, aligning these tokens in a manner that ensures stability in the modelâ€™s predictions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2a">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S3.2 </span>CMU-MOSI dataset</h3>
<div class="ltx_para" id="S3.SS2a.p1">
<p class="ltx_p" id="S3.SS2a.p1.1">We extend this analysis to the CMU-MOSI dataset. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03010v1#S2.F3" title="Figure S3 â€£ S2.1 Multimodal segmentation â€£ S2 Implementation Details â€£ MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection"><span class="ltx_text ltx_ref_tag">S3</span></a> illustrates the average Mean Squared Error (MSE) for each modality. The results show that the audio modality exhibits the lowest MSE, reflecting the highest alignment between real and projected tokens. This is followed by the visual modality, which also demonstrates low MSE. The text modality shows a higher MSE, though it remains below 3. This observation is consistent with related work, which has shown that the text modality often has the most significant impact when missing, making it more challenging to project accurately.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S4 </span>Evaluation Metrics</h2>
<div class="ltx_para" id="S4a.p1">
<p class="ltx_p" id="S4a.p1.1">In this section, we describe the evaluation metrics used to assess the performance of our MMP approach across three different tasks. We utilize mean Intersection over Union (mIoU) for segmentation, accuracy for classification, and binary accuracy and F1-score for sentiment analysis. These metrics are standard in their respective fields, and it is common practice to use them for benchmarking against related work.</p>
</div>
<section class="ltx_subsection" id="S4.SS1a">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S4.1 </span>Multimodal Segmentation</h3>
<div class="ltx_para" id="S4.SS1a.p1">
<p class="ltx_p" id="S4.SS1a.p1.1">For the multimodal segmentation task, we use the mean Intersection over Union (mIoU) as the evaluation metric. mIoU is a standard metric for evaluating semantic segmentation performance. It calculates the average overlap between the predicted segmentation and the ground truth across all classes. This provides a comprehensive evaluation by considering both false positives and false negatives, and capturing the spatial overlap between the predicted and true segmentations.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2a">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S4.2 </span>Multimodal Classification</h3>
<div class="ltx_para" id="S4.SS2a.p1">
<p class="ltx_p" id="S4.SS2a.p1.1">For the multimodal classification task, we employ accuracy as our evaluation metric. Accuracy measures the proportion of correct predictions made by the model out of all predictions. This is a straightforward and intuitive metric that provides an overall performance measure of the classification model.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3a">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S4.3 </span>Multimodal Sentiment Analysis</h3>
<div class="ltx_para" id="S4.SS3a.p1">
<p class="ltx_p" id="S4.SS3a.p1.1">For the multimodal sentiment analysis task, we utilize both accuracy and F1-score to evaluate the modelâ€™s performance. F1-score is the mean of precision and recall, providing a balance between the two. We use the binary metrics in this task, which only consider the Negative and Positive classes, following the common approach in related work.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct  3 20:34:17 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
