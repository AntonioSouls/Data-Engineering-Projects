<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.02431] MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection</title><meta property="og:description" content="We introduce Multi-Source 3D (MS3D), a new self-training pipeline for unsupervised domain adaptation in 3D object detection. Despite the remarkable accuracy of 3D detectors, they often overfit to specific domain biases…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.02431">

<!--Generated on Thu Feb 29 16:13:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Darren Tsai, Julie Stephany Berrio, Mao Shan, Eduardo Nebot and Stewart Worrall
</span><span class="ltx_author_notes">This work has been supported by the Australian Centre for Field Robotics (ACFR) and ARC LIEF grant LE200100049 Whopping Volta GPU Cluster - Transforming Artificial Intelligence Research. (Corresponding author: Darren Tsai.)The authors are with the Australian Centre for Field Robotics (ACFR) at the University of Sydney (NSW, Australia). E-mails: <span id="id1.1.id1" class="ltx_text" style="font-size:90%;">{d.tsai, j.berrio, m.shan, e.nebot, s.worrall}@acfr.usyd.edu.au</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">We introduce Multi-Source 3D (MS3D), a new self-training pipeline for unsupervised domain adaptation in 3D object detection. Despite the remarkable accuracy of 3D detectors, they often overfit to specific domain biases, leading to suboptimal performance in various sensor setups and environments. Existing methods typically focus on adapting a single detector to the target domain, overlooking the fact that different detectors possess distinct expertise on different unseen domains. MS3D leverages this by combining different pre-trained detectors from multiple source domains and incorporating temporal information to produce high-quality pseudo-labels for fine-tuning. Our proposed Kernel-Density Estimation (KDE) Box Fusion method fuses box proposals from multiple domains to obtain pseudo-labels that surpass the performance of the best source domain detectors. MS3D exhibits greater robustness to domain shift and produces accurate pseudo-labels over greater distances, making it well-suited for high-to-low beam domain adaptation and vice versa. Our method achieved state-of-the-art performance on all evaluated datasets, and we demonstrate that the pre-trained detector’s source dataset has minimal impact on the fine-tuned result, making MS3D suitable for real-world applications. Our code is available at <a target="_blank" href="https://github.com/darrenjkt/MS3D" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/darrenjkt/MS3D</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">3D object detection is a vital component of AVs, serving as a fundamental building block for critical downstream tasks such as path prediction. The field has benefited greatly from the availability of large-scale AV datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, which have fueled research in the development of 3D detectors. Unfortunately, directly applying these 3D detectors to new datasets results in suboptimal performance due to domain shift. Factors such as variations in lidars, geographic regions, scenarios, and sensor setups pose significant challenges for the effective testing and deployment of 3D object detectors. Traditionally, the approach to improving detectors for new datasets is to manually annotate the data for fine-tuning the existing detector. However, this approach is both costly and labour-intensive, therefore highlighting the need for effective approaches to adapt 3D detectors trained on labelled source domains to new, unlabelled target domains. This task is called unsupervised domain adaptation (UDA).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2304.02431/assets/images/plot_ms3d_st3d_sn.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="574" height="422" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.10.5.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.8.4" class="ltx_text" style="font-size:90%;">Performance of MS3D compared to SN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and ST3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> for Lyft <math id="S1.F1.5.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S1.F1.5.1.m1.1b"><mo stretchy="false" id="S1.F1.5.1.m1.1.1" xref="S1.F1.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.F1.5.1.m1.1c"><ci id="S1.F1.5.1.m1.1.1.cmml" xref="S1.F1.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.5.1.m1.1d">\rightarrow</annotation></semantics></math> Waymo, Waymo <math id="S1.F1.6.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S1.F1.6.2.m2.1b"><mo stretchy="false" id="S1.F1.6.2.m2.1.1" xref="S1.F1.6.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.F1.6.2.m2.1c"><ci id="S1.F1.6.2.m2.1.1.cmml" xref="S1.F1.6.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.2.m2.1d">\rightarrow</annotation></semantics></math> Lyft and Waymo <math id="S1.F1.7.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S1.F1.7.3.m3.1b"><mo stretchy="false" id="S1.F1.7.3.m3.1.1" xref="S1.F1.7.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.F1.7.3.m3.1c"><ci id="S1.F1.7.3.m3.1.1.cmml" xref="S1.F1.7.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.7.3.m3.1d">\rightarrow</annotation></semantics></math> nuScenes (from left to right). Graph shows how much these methods have reduced the domain gap (in <math id="S1.F1.8.4.m4.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S1.F1.8.4.m4.1b"><msub id="S1.F1.8.4.m4.1.1" xref="S1.F1.8.4.m4.1.1.cmml"><mtext id="S1.F1.8.4.m4.1.1.2" xref="S1.F1.8.4.m4.1.1.2a.cmml">AP</mtext><mtext id="S1.F1.8.4.m4.1.1.3" xref="S1.F1.8.4.m4.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S1.F1.8.4.m4.1c"><apply id="S1.F1.8.4.m4.1.1.cmml" xref="S1.F1.8.4.m4.1.1"><csymbol cd="ambiguous" id="S1.F1.8.4.m4.1.1.1.cmml" xref="S1.F1.8.4.m4.1.1">subscript</csymbol><ci id="S1.F1.8.4.m4.1.1.2a.cmml" xref="S1.F1.8.4.m4.1.1.2"><mtext id="S1.F1.8.4.m4.1.1.2.cmml" xref="S1.F1.8.4.m4.1.1.2">AP</mtext></ci><ci id="S1.F1.8.4.m4.1.1.3a.cmml" xref="S1.F1.8.4.m4.1.1.3"><mtext mathsize="70%" id="S1.F1.8.4.m4.1.1.3.cmml" xref="S1.F1.8.4.m4.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.8.4.m4.1d">\text{AP}_{\text{3D}}</annotation></semantics></math> at IoU=0.7) between source-only (no UDA, i.e., 0%), and fine-tuned with target domain labels (i.e., 100%).</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we introduce Multi-Source 3D (MS3D), a novel pipeline for generating high-quality pseudo-labels on an unlabelled dataset for self-training in the context of UDA for 3D Object Detection. Unlike existing methods, MS3D takes advantage of the collective knowledge of multiple detectors to generate pseudo-labels on a single novel dataset. Moreover, by accumulating frames, MS3D improves the robustness of detection boxes on static objects.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">One of the key benefits of MS3D is its ability to adapt robustly to a wide range of domain shifts by leveraging the expertise of multiple detectors. When using self-training in the standard single source-target domain adaptation setting, the choice of source dataset and detector can significantly impact fine-tuning performance. In practice, it is challenging to evaluate and identify the optimal source dataset and detector on new target datasets due to lack of labelled data. However, with MS3D, we eliminate the need to select a single pre-trained detector for self-training. We demonstrate that the combined pseudo-labels of multiple detectors are of higher quality than those of the optimal individual source-trained detector. As a result, our fine-tuned performance outperforms state-of-the-art single-source UDA methods as shown in <a href="#S1.F1" title="In I Introduction ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2304.02431/assets/images/ms3d_pipeline_3x.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.11.5.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.8.4" class="ltx_text ltx_font_bold" style="font-size:90%;">The MS3D self-training pipeline.<span id="S1.F2.8.4.4" class="ltx_text ltx_font_medium"> Given a set of N pre-trained 3D detectors from multiple source domains <math id="S1.F2.5.1.1.m1.1" class="ltx_Math" alttext="\textbf{D}_{\text{S-i}}" display="inline"><semantics id="S1.F2.5.1.1.m1.1b"><msub id="S1.F2.5.1.1.m1.1.1" xref="S1.F2.5.1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S1.F2.5.1.1.m1.1.1.2" xref="S1.F2.5.1.1.m1.1.1.2a.cmml">D</mtext><mtext id="S1.F2.5.1.1.m1.1.1.3" xref="S1.F2.5.1.1.m1.1.1.3a.cmml">S-i</mtext></msub><annotation-xml encoding="MathML-Content" id="S1.F2.5.1.1.m1.1c"><apply id="S1.F2.5.1.1.m1.1.1.cmml" xref="S1.F2.5.1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.F2.5.1.1.m1.1.1.1.cmml" xref="S1.F2.5.1.1.m1.1.1">subscript</csymbol><ci id="S1.F2.5.1.1.m1.1.1.2a.cmml" xref="S1.F2.5.1.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S1.F2.5.1.1.m1.1.1.2.cmml" xref="S1.F2.5.1.1.m1.1.1.2">D</mtext></ci><ci id="S1.F2.5.1.1.m1.1.1.3a.cmml" xref="S1.F2.5.1.1.m1.1.1.3"><mtext mathsize="70%" id="S1.F2.5.1.1.m1.1.1.3.cmml" xref="S1.F2.5.1.1.m1.1.1.3">S-i</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.5.1.1.m1.1d">\textbf{D}_{\text{S-i}}</annotation></semantics></math> where <math id="S1.F2.6.2.2.m2.4" class="ltx_Math" alttext="\text{i}=1,2,...,\text{N}" display="inline"><semantics id="S1.F2.6.2.2.m2.4b"><mrow id="S1.F2.6.2.2.m2.4.5" xref="S1.F2.6.2.2.m2.4.5.cmml"><mtext id="S1.F2.6.2.2.m2.4.5.2" xref="S1.F2.6.2.2.m2.4.5.2a.cmml">i</mtext><mo id="S1.F2.6.2.2.m2.4.5.1" xref="S1.F2.6.2.2.m2.4.5.1.cmml">=</mo><mrow id="S1.F2.6.2.2.m2.4.5.3.2" xref="S1.F2.6.2.2.m2.4.5.3.1.cmml"><mn id="S1.F2.6.2.2.m2.1.1" xref="S1.F2.6.2.2.m2.1.1.cmml">1</mn><mo id="S1.F2.6.2.2.m2.4.5.3.2.1" xref="S1.F2.6.2.2.m2.4.5.3.1.cmml">,</mo><mn id="S1.F2.6.2.2.m2.2.2" xref="S1.F2.6.2.2.m2.2.2.cmml">2</mn><mo id="S1.F2.6.2.2.m2.4.5.3.2.2" xref="S1.F2.6.2.2.m2.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="S1.F2.6.2.2.m2.3.3" xref="S1.F2.6.2.2.m2.3.3.cmml">…</mi><mo id="S1.F2.6.2.2.m2.4.5.3.2.3" xref="S1.F2.6.2.2.m2.4.5.3.1.cmml">,</mo><mtext id="S1.F2.6.2.2.m2.4.4" xref="S1.F2.6.2.2.m2.4.4a.cmml">N</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.F2.6.2.2.m2.4c"><apply id="S1.F2.6.2.2.m2.4.5.cmml" xref="S1.F2.6.2.2.m2.4.5"><eq id="S1.F2.6.2.2.m2.4.5.1.cmml" xref="S1.F2.6.2.2.m2.4.5.1"></eq><ci id="S1.F2.6.2.2.m2.4.5.2a.cmml" xref="S1.F2.6.2.2.m2.4.5.2"><mtext id="S1.F2.6.2.2.m2.4.5.2.cmml" xref="S1.F2.6.2.2.m2.4.5.2">i</mtext></ci><list id="S1.F2.6.2.2.m2.4.5.3.1.cmml" xref="S1.F2.6.2.2.m2.4.5.3.2"><cn type="integer" id="S1.F2.6.2.2.m2.1.1.cmml" xref="S1.F2.6.2.2.m2.1.1">1</cn><cn type="integer" id="S1.F2.6.2.2.m2.2.2.cmml" xref="S1.F2.6.2.2.m2.2.2">2</cn><ci id="S1.F2.6.2.2.m2.3.3.cmml" xref="S1.F2.6.2.2.m2.3.3">…</ci><ci id="S1.F2.6.2.2.m2.4.4a.cmml" xref="S1.F2.6.2.2.m2.4.4"><mtext id="S1.F2.6.2.2.m2.4.4.cmml" xref="S1.F2.6.2.2.m2.4.4">N</mtext></ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.6.2.2.m2.4d">\text{i}=1,2,...,\text{N}</annotation></semantics></math>, we generate detections for both 1-frame and multi-frame accumulated point clouds, and combine them with our KBF. Thereafter, we feed the sequence frames into a 3D tracker for generating trajectories of both 1-frame and multi-frame boxes. Finally, we refine static object boxes to get our final pseudo-labels for fine-tuning any pre-trained detector <math id="S1.F2.7.3.3.m3.1" class="ltx_Math" alttext="\textbf{D}_{\text{S-i}}" display="inline"><semantics id="S1.F2.7.3.3.m3.1b"><msub id="S1.F2.7.3.3.m3.1.1" xref="S1.F2.7.3.3.m3.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S1.F2.7.3.3.m3.1.1.2" xref="S1.F2.7.3.3.m3.1.1.2a.cmml">D</mtext><mtext id="S1.F2.7.3.3.m3.1.1.3" xref="S1.F2.7.3.3.m3.1.1.3a.cmml">S-i</mtext></msub><annotation-xml encoding="MathML-Content" id="S1.F2.7.3.3.m3.1c"><apply id="S1.F2.7.3.3.m3.1.1.cmml" xref="S1.F2.7.3.3.m3.1.1"><csymbol cd="ambiguous" id="S1.F2.7.3.3.m3.1.1.1.cmml" xref="S1.F2.7.3.3.m3.1.1">subscript</csymbol><ci id="S1.F2.7.3.3.m3.1.1.2a.cmml" xref="S1.F2.7.3.3.m3.1.1.2"><mtext class="ltx_mathvariant_bold" id="S1.F2.7.3.3.m3.1.1.2.cmml" xref="S1.F2.7.3.3.m3.1.1.2">D</mtext></ci><ci id="S1.F2.7.3.3.m3.1.1.3a.cmml" xref="S1.F2.7.3.3.m3.1.1.3"><mtext mathsize="70%" id="S1.F2.7.3.3.m3.1.1.3.cmml" xref="S1.F2.7.3.3.m3.1.1.3">S-i</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.7.3.3.m3.1d">\textbf{D}_{\text{S-i}}</annotation></semantics></math>. Visualized in this figure is our MS3D pipeline for Waymo/Lyft <math id="S1.F2.8.4.4.m4.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S1.F2.8.4.4.m4.1b"><mo stretchy="false" id="S1.F2.8.4.4.m4.1.1" xref="S1.F2.8.4.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.F2.8.4.4.m4.1c"><ci id="S1.F2.8.4.4.m4.1.1.cmml" xref="S1.F2.8.4.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.8.4.4.m4.1d">\rightarrow</annotation></semantics></math> nuScenes.</span></span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our proposed approach is built upon two fundamental observations. Firstly, we recognize that different detectors trained on various datasets possess distinct expertise. For instance, a detector trained on the Waymo dataset may be better suited for a specific novel dataset compared to a detector trained on nuScenes, and vice versa. Secondly, we observe that by utilizing temporal point clouds (i.e., multi-frame accumulation), we can aggregate various perspectives to gain a more comprehensive understanding of static objects’ complete geometry, leading to more accurate bounding boxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In real-world scenarios, Multi-Source Domain Adaptation (MSDA) is a practical and effective approach for adaptation to a target domain. The availability of multiple source domains enables us to harness diverse knowledge and improve the overall performance of 3D object detectors on unlabelled datasets. Fortunately, there are many large-scale datasets that can be leveraged for MSDA in 3D object detection, including nuScenes, Waymo, and Lyft, among others. By incorporating the rich information from these sources, we can significantly reduce the domain shift and improve the generalization ability of 3D detectors.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">A naive approach to combining multiple source-trained detectors on a new domain is to select the detection box produced by the most confident detector. However, this method neglects potentially valuable information provided by the other detectors, and it is possible that the most confident detector is not necessarily the most accurate. Instead, we propose KDE Box Fusion (KBF), a more robust box-fusion strategy that utilizes KDE to consider each detector’s proposal and enhance pseudo-label quality. We note that KBF can also be applied for detector ensembling in a supervised setting.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Point cloud accumulation has gained popularity as a promising direction for improving single-frame detection through densification. Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> demonstrated that concatenating 4 point cloud frames had the optimal performance boost before it started to deteriorate with more frames for the Waymo dataset. However, they observe that for static and slow objects, accumulating more than 4 frames continued to improve detection performance. Qi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> showed that accumulating 100+ frames of a sequence can be effectively utilized by separating static and dynamic vehicle box refinement. To capitalize on these findings, we explore the use of multi-frame detection in the context of UDA to refine the box localization of static objects. Furthermore, by extending the static boxes of parked vehicles in both directions temporally, we obtain precise pseudo-labels for objects located beyond 50m from the ego-vehicle.</p>
</div>
<div id="S1.p8" class="ltx_para ltx_noindent">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text ltx_font_bold">Contributions.</span> Our main contribution is MS3D, a multi-source domain adaptation approach for 3D object detection that leverages multiple source domain experts and temporal information to generate high quality pseudo-labels for self-training. The benefits of our approach are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Robust</span>: By combining the expertise of different detectors trained on various source domains, MS3D is more robust against domain shifts.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Versatile and Scalable</span>: MS3D is easily combined with multiple and various types of pre-trained 3D detectors to further boost pseudo-label quality.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Improved Pseudo-labels</span>: MS3D self-trained models can obtain state-of-the-art UDA performance regardless of the source pre-trained model chosen for fine-tuning. This is because our fused pseudo-labels consistently outperform the best individual source domain detector.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Speed</span>: MS3D does not add any processing latency during inference, ensuring it can be used in real-time applications.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p"><span id="S1.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Detection Performance</span>: MS3D achieves state-of-the-art on all UDA settings presented.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">3D object detection.</span> A significant portion of the current research in 3D detection is concentrated on single-frame detection. Within this, many works propose to enhance the feature representation of point clouds, by proposing feature representations that can be classified as voxel-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, point-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, or a combination of both <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Recent works have delved into the use of temporal information, where some works have shown that a simple multi-frame concatenation can already outperform single-frame detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. However, this improvement is limited and the performance deteriorates for longer sequences. To address this challenge, a few studies have focused on developing spatio-temporal feature encoding strategies that fully leverage information from longer sequences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Another strategy is to separately encode sequences of dynamic and static object <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> for box refining.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Unsupervised Domain Adaptation.</span>
Research in UDA for 3D object detection has been predominantly focused on adapting a single detector, trained on a labelled domain (source dataset), to a new domain (target dataset). UDA approaches can be categorised into adversarial methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, domain-invariant feature representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, self-training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, mean teacher <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, amongst others. More specifically in 3D object detection, prior research showcased successful adaptation of detectors from larger-car datasets like Waymo and nuScenes, to smaller-car datasets like KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. A popular direction for this topic is to use self-training, where the focus is on generating high quality pseudo-labels for fine-tuning the source-trained detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. ST3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> employed random object scaling augmentation within a self-training framework for more precise pseudo-labels for adaptation to the KITTI dataset. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> improved ST3D for adapting from a 64 to 32-beam dataset via their generative method of emulating a pseudo 32-beam lidar.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Multi-Source Domain Adaptation.</span> MSDA is an emerging variant of UDA that builds on UDA techniques while incorporating new techniques that leverage multiple source domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> uses style transfer to reduce the discrepancy of multiple source image styles in a collaborative learning framework. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> proposes to align the feature space of multiple sources through an exponential moving average parameter combination for object detection. Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> extended this to 3D object detection with a focus on learning domain-agnostic features from multiple source datasets to improve detector generalization. To the best of our knowledge, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> is the only attempt at MSDA for 3D object detection. Our approach differs from theirs in that we focus on combining detectors rather than datasets, and we accomplish this without modifying detector architecture or using source domain data (i.e., source-free). Our fusion of proposals from multiple detectors shares similarities with popular ensembling box filtering strategies such as NMS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, Soft-NMS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and Weighted Box Fusion (WBF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. In particular, WBF uses confidence scores as weights to determine new box corners in image detection. However, we find that applying WBF to the 3D space is suboptimal compared to our proposed KBF.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Multi-Source 3D</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Problem Statement</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.9" class="ltx_p">In unsupervised MSDA, we have multiple <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">M</annotation></semantics></math> labelled source domains <math id="S3.SS1.p1.2.m2.4" class="ltx_Math" alttext="S_{1},S_{2},...,S_{M}" display="inline"><semantics id="S3.SS1.p1.2.m2.4a"><mrow id="S3.SS1.p1.2.m2.4.4.3" xref="S3.SS1.p1.2.m2.4.4.4.cmml"><msub id="S3.SS1.p1.2.m2.2.2.1.1" xref="S3.SS1.p1.2.m2.2.2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.2.2.1.1.2" xref="S3.SS1.p1.2.m2.2.2.1.1.2.cmml">S</mi><mn id="S3.SS1.p1.2.m2.2.2.1.1.3" xref="S3.SS1.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.2.m2.4.4.3.4" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.2.m2.3.3.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.cmml"><mi id="S3.SS1.p1.2.m2.3.3.2.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.2.cmml">S</mi><mn id="S3.SS1.p1.2.m2.3.3.2.2.3" xref="S3.SS1.p1.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.2.m2.4.4.3.5" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">…</mi><mo id="S3.SS1.p1.2.m2.4.4.3.6" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.2.m2.4.4.3.3" xref="S3.SS1.p1.2.m2.4.4.3.3.cmml"><mi id="S3.SS1.p1.2.m2.4.4.3.3.2" xref="S3.SS1.p1.2.m2.4.4.3.3.2.cmml">S</mi><mi id="S3.SS1.p1.2.m2.4.4.3.3.3" xref="S3.SS1.p1.2.m2.4.4.3.3.3.cmml">M</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.4b"><list id="S3.SS1.p1.2.m2.4.4.4.cmml" xref="S3.SS1.p1.2.m2.4.4.3"><apply id="S3.SS1.p1.2.m2.2.2.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1.2">𝑆</ci><cn type="integer" id="S3.SS1.p1.2.m2.2.2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.p1.2.m2.3.3.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.2.2.1.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2.2">𝑆</ci><cn type="integer" id="S3.SS1.p1.2.m2.3.3.2.2.3.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">…</ci><apply id="S3.SS1.p1.2.m2.4.4.3.3.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.4.4.3.3.1.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.4.4.3.3.2.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3.2">𝑆</ci><ci id="S3.SS1.p1.2.m2.4.4.3.3.3.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3.3">𝑀</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.4c">S_{1},S_{2},...,S_{M}</annotation></semantics></math> and a single unlabelled target domain <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">T</annotation></semantics></math>. In our context, each <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">i</annotation></semantics></math>-th source <math id="S3.SS1.p1.5.m5.5" class="ltx_Math" alttext="S_{i}=\{(p_{S,i},L_{S,i})\}^{N}_{i}" display="inline"><semantics id="S3.SS1.p1.5.m5.5a"><mrow id="S3.SS1.p1.5.m5.5.5" xref="S3.SS1.p1.5.m5.5.5.cmml"><msub id="S3.SS1.p1.5.m5.5.5.3" xref="S3.SS1.p1.5.m5.5.5.3.cmml"><mi id="S3.SS1.p1.5.m5.5.5.3.2" xref="S3.SS1.p1.5.m5.5.5.3.2.cmml">S</mi><mi id="S3.SS1.p1.5.m5.5.5.3.3" xref="S3.SS1.p1.5.m5.5.5.3.3.cmml">i</mi></msub><mo id="S3.SS1.p1.5.m5.5.5.2" xref="S3.SS1.p1.5.m5.5.5.2.cmml">=</mo><msubsup id="S3.SS1.p1.5.m5.5.5.1" xref="S3.SS1.p1.5.m5.5.5.1.cmml"><mrow id="S3.SS1.p1.5.m5.5.5.1.1.1.1" xref="S3.SS1.p1.5.m5.5.5.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m5.5.5.1.1.1.1.2" xref="S3.SS1.p1.5.m5.5.5.1.1.1.2.cmml">{</mo><mrow id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.3" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.3.cmml">(</mo><msub id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.1.1" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.1.1.2.cmml">p</mi><mrow id="S3.SS1.p1.5.m5.2.2.2.4" xref="S3.SS1.p1.5.m5.2.2.2.3.cmml"><mi id="S3.SS1.p1.5.m5.1.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.1.cmml">S</mi><mo id="S3.SS1.p1.5.m5.2.2.2.4.1" xref="S3.SS1.p1.5.m5.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p1.5.m5.2.2.2.2" xref="S3.SS1.p1.5.m5.2.2.2.2.cmml">i</mi></mrow></msub><mo id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.4" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.3.cmml">,</mo><msub id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.2" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.2.cmml"><mi id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.2.2" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.2.2.cmml">L</mi><mrow id="S3.SS1.p1.5.m5.4.4.2.4" xref="S3.SS1.p1.5.m5.4.4.2.3.cmml"><mi id="S3.SS1.p1.5.m5.3.3.1.1" xref="S3.SS1.p1.5.m5.3.3.1.1.cmml">S</mi><mo id="S3.SS1.p1.5.m5.4.4.2.4.1" xref="S3.SS1.p1.5.m5.4.4.2.3.cmml">,</mo><mi id="S3.SS1.p1.5.m5.4.4.2.2" xref="S3.SS1.p1.5.m5.4.4.2.2.cmml">i</mi></mrow></msub><mo stretchy="false" id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.5" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.3.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS1.p1.5.m5.5.5.1.1.1.1.3" xref="S3.SS1.p1.5.m5.5.5.1.1.1.2.cmml">}</mo></mrow><mi id="S3.SS1.p1.5.m5.5.5.1.3" xref="S3.SS1.p1.5.m5.5.5.1.3.cmml">i</mi><mi id="S3.SS1.p1.5.m5.5.5.1.1.3" xref="S3.SS1.p1.5.m5.5.5.1.1.3.cmml">N</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.5b"><apply id="S3.SS1.p1.5.m5.5.5.cmml" xref="S3.SS1.p1.5.m5.5.5"><eq id="S3.SS1.p1.5.m5.5.5.2.cmml" xref="S3.SS1.p1.5.m5.5.5.2"></eq><apply id="S3.SS1.p1.5.m5.5.5.3.cmml" xref="S3.SS1.p1.5.m5.5.5.3"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.5.5.3.1.cmml" xref="S3.SS1.p1.5.m5.5.5.3">subscript</csymbol><ci id="S3.SS1.p1.5.m5.5.5.3.2.cmml" xref="S3.SS1.p1.5.m5.5.5.3.2">𝑆</ci><ci id="S3.SS1.p1.5.m5.5.5.3.3.cmml" xref="S3.SS1.p1.5.m5.5.5.3.3">𝑖</ci></apply><apply id="S3.SS1.p1.5.m5.5.5.1.cmml" xref="S3.SS1.p1.5.m5.5.5.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.5.5.1.2.cmml" xref="S3.SS1.p1.5.m5.5.5.1">subscript</csymbol><apply id="S3.SS1.p1.5.m5.5.5.1.1.cmml" xref="S3.SS1.p1.5.m5.5.5.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.5.5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.5.5.1">superscript</csymbol><set id="S3.SS1.p1.5.m5.5.5.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1"><interval closure="open" id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2"><apply id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.1.1.2">𝑝</ci><list id="S3.SS1.p1.5.m5.2.2.2.3.cmml" xref="S3.SS1.p1.5.m5.2.2.2.4"><ci id="S3.SS1.p1.5.m5.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1">𝑆</ci><ci id="S3.SS1.p1.5.m5.2.2.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2.2.2">𝑖</ci></list></apply><apply id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.2.1.cmml" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.2.2.cmml" xref="S3.SS1.p1.5.m5.5.5.1.1.1.1.1.2.2.2">𝐿</ci><list id="S3.SS1.p1.5.m5.4.4.2.3.cmml" xref="S3.SS1.p1.5.m5.4.4.2.4"><ci id="S3.SS1.p1.5.m5.3.3.1.1.cmml" xref="S3.SS1.p1.5.m5.3.3.1.1">𝑆</ci><ci id="S3.SS1.p1.5.m5.4.4.2.2.cmml" xref="S3.SS1.p1.5.m5.4.4.2.2">𝑖</ci></list></apply></interval></set><ci id="S3.SS1.p1.5.m5.5.5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.5.5.1.1.3">𝑁</ci></apply><ci id="S3.SS1.p1.5.m5.5.5.1.3.cmml" xref="S3.SS1.p1.5.m5.5.5.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.5c">S_{i}=\{(p_{S,i},L_{S,i})\}^{N}_{i}</annotation></semantics></math> is a detector that has been trained on a point cloud dataset <math id="S3.SS1.p1.6.m6.2" class="ltx_Math" alttext="p_{S,i}" display="inline"><semantics id="S3.SS1.p1.6.m6.2a"><msub id="S3.SS1.p1.6.m6.2.3" xref="S3.SS1.p1.6.m6.2.3.cmml"><mi id="S3.SS1.p1.6.m6.2.3.2" xref="S3.SS1.p1.6.m6.2.3.2.cmml">p</mi><mrow id="S3.SS1.p1.6.m6.2.2.2.4" xref="S3.SS1.p1.6.m6.2.2.2.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.cmml">S</mi><mo id="S3.SS1.p1.6.m6.2.2.2.4.1" xref="S3.SS1.p1.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p1.6.m6.2.2.2.2" xref="S3.SS1.p1.6.m6.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.2b"><apply id="S3.SS1.p1.6.m6.2.3.cmml" xref="S3.SS1.p1.6.m6.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.2.3.1.cmml" xref="S3.SS1.p1.6.m6.2.3">subscript</csymbol><ci id="S3.SS1.p1.6.m6.2.3.2.cmml" xref="S3.SS1.p1.6.m6.2.3.2">𝑝</ci><list id="S3.SS1.p1.6.m6.2.2.2.3.cmml" xref="S3.SS1.p1.6.m6.2.2.2.4"><ci id="S3.SS1.p1.6.m6.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1">𝑆</ci><ci id="S3.SS1.p1.6.m6.2.2.2.2.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.2c">p_{S,i}</annotation></semantics></math> with annotations <math id="S3.SS1.p1.7.m7.2" class="ltx_Math" alttext="L_{S,i}" display="inline"><semantics id="S3.SS1.p1.7.m7.2a"><msub id="S3.SS1.p1.7.m7.2.3" xref="S3.SS1.p1.7.m7.2.3.cmml"><mi id="S3.SS1.p1.7.m7.2.3.2" xref="S3.SS1.p1.7.m7.2.3.2.cmml">L</mi><mrow id="S3.SS1.p1.7.m7.2.2.2.4" xref="S3.SS1.p1.7.m7.2.2.2.3.cmml"><mi id="S3.SS1.p1.7.m7.1.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.1.cmml">S</mi><mo id="S3.SS1.p1.7.m7.2.2.2.4.1" xref="S3.SS1.p1.7.m7.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p1.7.m7.2.2.2.2" xref="S3.SS1.p1.7.m7.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.2b"><apply id="S3.SS1.p1.7.m7.2.3.cmml" xref="S3.SS1.p1.7.m7.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.2.3.1.cmml" xref="S3.SS1.p1.7.m7.2.3">subscript</csymbol><ci id="S3.SS1.p1.7.m7.2.3.2.cmml" xref="S3.SS1.p1.7.m7.2.3.2">𝐿</ci><list id="S3.SS1.p1.7.m7.2.2.2.3.cmml" xref="S3.SS1.p1.7.m7.2.2.2.4"><ci id="S3.SS1.p1.7.m7.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1">𝑆</ci><ci id="S3.SS1.p1.7.m7.2.2.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2.2.2">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.2c">L_{S,i}</annotation></semantics></math>. This source-trained detector’s box predictions on the <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">k</annotation></semantics></math>-th frame of the unlabelled target dataset is denoted as <math id="S3.SS1.p1.9.m9.3" class="ltx_Math" alttext="\{B_{T,i}^{k}\}^{N}_{k}" display="inline"><semantics id="S3.SS1.p1.9.m9.3a"><msubsup id="S3.SS1.p1.9.m9.3.3" xref="S3.SS1.p1.9.m9.3.3.cmml"><mrow id="S3.SS1.p1.9.m9.3.3.1.1.1" xref="S3.SS1.p1.9.m9.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.9.m9.3.3.1.1.1.2" xref="S3.SS1.p1.9.m9.3.3.1.1.2.cmml">{</mo><msubsup id="S3.SS1.p1.9.m9.3.3.1.1.1.1" xref="S3.SS1.p1.9.m9.3.3.1.1.1.1.cmml"><mi id="S3.SS1.p1.9.m9.3.3.1.1.1.1.2.2" xref="S3.SS1.p1.9.m9.3.3.1.1.1.1.2.2.cmml">B</mi><mrow id="S3.SS1.p1.9.m9.2.2.2.4" xref="S3.SS1.p1.9.m9.2.2.2.3.cmml"><mi id="S3.SS1.p1.9.m9.1.1.1.1" xref="S3.SS1.p1.9.m9.1.1.1.1.cmml">T</mi><mo id="S3.SS1.p1.9.m9.2.2.2.4.1" xref="S3.SS1.p1.9.m9.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p1.9.m9.2.2.2.2" xref="S3.SS1.p1.9.m9.2.2.2.2.cmml">i</mi></mrow><mi id="S3.SS1.p1.9.m9.3.3.1.1.1.1.3" xref="S3.SS1.p1.9.m9.3.3.1.1.1.1.3.cmml">k</mi></msubsup><mo stretchy="false" id="S3.SS1.p1.9.m9.3.3.1.1.1.3" xref="S3.SS1.p1.9.m9.3.3.1.1.2.cmml">}</mo></mrow><mi id="S3.SS1.p1.9.m9.3.3.3" xref="S3.SS1.p1.9.m9.3.3.3.cmml">k</mi><mi id="S3.SS1.p1.9.m9.3.3.1.3" xref="S3.SS1.p1.9.m9.3.3.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.3b"><apply id="S3.SS1.p1.9.m9.3.3.cmml" xref="S3.SS1.p1.9.m9.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.3.3.2.cmml" xref="S3.SS1.p1.9.m9.3.3">subscript</csymbol><apply id="S3.SS1.p1.9.m9.3.3.1.cmml" xref="S3.SS1.p1.9.m9.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.3.3.1.2.cmml" xref="S3.SS1.p1.9.m9.3.3">superscript</csymbol><set id="S3.SS1.p1.9.m9.3.3.1.1.2.cmml" xref="S3.SS1.p1.9.m9.3.3.1.1.1"><apply id="S3.SS1.p1.9.m9.3.3.1.1.1.1.cmml" xref="S3.SS1.p1.9.m9.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.3.3.1.1.1.1.1.cmml" xref="S3.SS1.p1.9.m9.3.3.1.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.9.m9.3.3.1.1.1.1.2.cmml" xref="S3.SS1.p1.9.m9.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.3.3.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.9.m9.3.3.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.9.m9.3.3.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.9.m9.3.3.1.1.1.1.2.2">𝐵</ci><list id="S3.SS1.p1.9.m9.2.2.2.3.cmml" xref="S3.SS1.p1.9.m9.2.2.2.4"><ci id="S3.SS1.p1.9.m9.1.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1.1.1">𝑇</ci><ci id="S3.SS1.p1.9.m9.2.2.2.2.cmml" xref="S3.SS1.p1.9.m9.2.2.2.2">𝑖</ci></list></apply><ci id="S3.SS1.p1.9.m9.3.3.1.1.1.1.3.cmml" xref="S3.SS1.p1.9.m9.3.3.1.1.1.1.3">𝑘</ci></apply></set><ci id="S3.SS1.p1.9.m9.3.3.1.3.cmml" xref="S3.SS1.p1.9.m9.3.3.1.3">𝑁</ci></apply><ci id="S3.SS1.p1.9.m9.3.3.3.cmml" xref="S3.SS1.p1.9.m9.3.3.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.3c">\{B_{T,i}^{k}\}^{N}_{k}</annotation></semantics></math>. Our goal is combine box predictions from each source-trained detector and utilize temporal information to robustly localize and classify objects in 3D for each frame. We assume that the vehicle ego-pose is known in world coordinates for ego-motion compensation. <a href="#S1.F2" title="In I Introduction ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> illustrates our proposed pipeline, which we will introduce in the following sections.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Fusing multiple sources</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Initial Pseudo-labels.</span> In order to enhance the resilience of our model against domain shifts in the unlabelled target domain, we use different types of detectors and source datasets for a greater range of knowledge. For example, with Waymo as our target domain, we employ both nuScenes and Lyft source domains, with two types of detectors, namely SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. We notice that Lyft’s lidar scan pattern is similar to that of Waymo, which could give Lyft-trained detectors an edge over nuScenes-trained detectors in certain scenarios. Additionally, different detectors possess varying areas of expertise. For instance, SECOND is adept at estimating object dimensions for axis-aligned objects due to its anchor boxes, whereas CenterPoint performs better with objects that are not axis-aligned <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.4" class="ltx_p">We take advantage of a detector’s full expertise by using two augmentations for Test Time Augmentation (TTA) - random world flip (RWF) across x and y axes, and random world rotation (RWR) in the range <math id="S3.SS2.p2.1.m1.2" class="ltx_Math" alttext="[-\pi,\pi]" display="inline"><semantics id="S3.SS2.p2.1.m1.2a"><mrow id="S3.SS2.p2.1.m1.2.2.1" xref="S3.SS2.p2.1.m1.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.2.2.1.2" xref="S3.SS2.p2.1.m1.2.2.2.cmml">[</mo><mrow id="S3.SS2.p2.1.m1.2.2.1.1" xref="S3.SS2.p2.1.m1.2.2.1.1.cmml"><mo id="S3.SS2.p2.1.m1.2.2.1.1a" xref="S3.SS2.p2.1.m1.2.2.1.1.cmml">−</mo><mi id="S3.SS2.p2.1.m1.2.2.1.1.2" xref="S3.SS2.p2.1.m1.2.2.1.1.2.cmml">π</mi></mrow><mo id="S3.SS2.p2.1.m1.2.2.1.3" xref="S3.SS2.p2.1.m1.2.2.2.cmml">,</mo><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">π</mi><mo stretchy="false" id="S3.SS2.p2.1.m1.2.2.1.4" xref="S3.SS2.p2.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.2b"><interval closure="closed" id="S3.SS2.p2.1.m1.2.2.2.cmml" xref="S3.SS2.p2.1.m1.2.2.1"><apply id="S3.SS2.p2.1.m1.2.2.1.1.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1"><minus id="S3.SS2.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1"></minus><ci id="S3.SS2.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1.2">𝜋</ci></apply><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝜋</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.2c">[-\pi,\pi]</annotation></semantics></math>. For each source detector we run four permutations: no TTA, RWF, RWR, RWF+RWR, which gives bounding boxes <math id="S3.SS2.p2.2.m2.3" class="ltx_Math" alttext="\{B_{T,i}^{k}\}^{4}_{i}" display="inline"><semantics id="S3.SS2.p2.2.m2.3a"><msubsup id="S3.SS2.p2.2.m2.3.3" xref="S3.SS2.p2.2.m2.3.3.cmml"><mrow id="S3.SS2.p2.2.m2.3.3.1.1.1" xref="S3.SS2.p2.2.m2.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.3.3.1.1.1.2" xref="S3.SS2.p2.2.m2.3.3.1.1.2.cmml">{</mo><msubsup id="S3.SS2.p2.2.m2.3.3.1.1.1.1" xref="S3.SS2.p2.2.m2.3.3.1.1.1.1.cmml"><mi id="S3.SS2.p2.2.m2.3.3.1.1.1.1.2.2" xref="S3.SS2.p2.2.m2.3.3.1.1.1.1.2.2.cmml">B</mi><mrow id="S3.SS2.p2.2.m2.2.2.2.4" xref="S3.SS2.p2.2.m2.2.2.2.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml">T</mi><mo id="S3.SS2.p2.2.m2.2.2.2.4.1" xref="S3.SS2.p2.2.m2.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p2.2.m2.2.2.2.2" xref="S3.SS2.p2.2.m2.2.2.2.2.cmml">i</mi></mrow><mi id="S3.SS2.p2.2.m2.3.3.1.1.1.1.3" xref="S3.SS2.p2.2.m2.3.3.1.1.1.1.3.cmml">k</mi></msubsup><mo stretchy="false" id="S3.SS2.p2.2.m2.3.3.1.1.1.3" xref="S3.SS2.p2.2.m2.3.3.1.1.2.cmml">}</mo></mrow><mi id="S3.SS2.p2.2.m2.3.3.3" xref="S3.SS2.p2.2.m2.3.3.3.cmml">i</mi><mn id="S3.SS2.p2.2.m2.3.3.1.3" xref="S3.SS2.p2.2.m2.3.3.1.3.cmml">4</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.3b"><apply id="S3.SS2.p2.2.m2.3.3.cmml" xref="S3.SS2.p2.2.m2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.3.3.2.cmml" xref="S3.SS2.p2.2.m2.3.3">subscript</csymbol><apply id="S3.SS2.p2.2.m2.3.3.1.cmml" xref="S3.SS2.p2.2.m2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.3.3.1.2.cmml" xref="S3.SS2.p2.2.m2.3.3">superscript</csymbol><set id="S3.SS2.p2.2.m2.3.3.1.1.2.cmml" xref="S3.SS2.p2.2.m2.3.3.1.1.1"><apply id="S3.SS2.p2.2.m2.3.3.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.3.3.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.3.3.1.1.1.1">superscript</csymbol><apply id="S3.SS2.p2.2.m2.3.3.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.3.3.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.3.3.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.3.3.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.3.3.1.1.1.1.2.2">𝐵</ci><list id="S3.SS2.p2.2.m2.2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.2.2.2.4"><ci id="S3.SS2.p2.2.m2.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1">𝑇</ci><ci id="S3.SS2.p2.2.m2.2.2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2">𝑖</ci></list></apply><ci id="S3.SS2.p2.2.m2.3.3.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.3.3.1.1.1.1.3">𝑘</ci></apply></set><cn type="integer" id="S3.SS2.p2.2.m2.3.3.1.3.cmml" xref="S3.SS2.p2.2.m2.3.3.1.3">4</cn></apply><ci id="S3.SS2.p2.2.m2.3.3.3.cmml" xref="S3.SS2.p2.2.m2.3.3.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.3c">\{B_{T,i}^{k}\}^{4}_{i}</annotation></semantics></math> for the <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">i</annotation></semantics></math>-th source detector on the <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">k</annotation></semantics></math>-th frame which we then fuse with KBF as follows.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.6" class="ltx_p"><span id="S3.SS2.p3.6.1" class="ltx_text ltx_font_bold">KDE Box Fusion.</span> The backbone of our KBF method, <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\kappa(\cdot)" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.2" xref="S3.SS2.p3.1.m1.1.2.cmml"><mi id="S3.SS2.p3.1.m1.1.2.2" xref="S3.SS2.p3.1.m1.1.2.2.cmml">κ</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.1.2.1" xref="S3.SS2.p3.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS2.p3.1.m1.1.2.3.2" xref="S3.SS2.p3.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.1.m1.1.2.3.2.1" xref="S3.SS2.p3.1.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS2.p3.1.m1.1.2.3.2.2" xref="S3.SS2.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.2"><times id="S3.SS2.p3.1.m1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.1.2.1"></times><ci id="S3.SS2.p3.1.m1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.1.2.2">𝜅</ci><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\kappa(\cdot)</annotation></semantics></math>, is KDE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, which is capable of estimating a probability density function (PDF) using a kernel function shown in <a href="#S3.E1" title="In III-B Fusing multiple sources ‣ III Multi-Source 3D ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">1</span></a>. KDE places a kernel <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">K</annotation></semantics></math> at each data point <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><msub id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">x</mi><mi id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">𝑥</ci><ci id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">x_{i}</annotation></semantics></math> which sums to give the final PDF estimate <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="\hat{f}(x)" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mrow id="S3.SS2.p3.4.m4.1.2" xref="S3.SS2.p3.4.m4.1.2.cmml"><mover accent="true" id="S3.SS2.p3.4.m4.1.2.2" xref="S3.SS2.p3.4.m4.1.2.2.cmml"><mi id="S3.SS2.p3.4.m4.1.2.2.2" xref="S3.SS2.p3.4.m4.1.2.2.2.cmml">f</mi><mo id="S3.SS2.p3.4.m4.1.2.2.1" xref="S3.SS2.p3.4.m4.1.2.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="S3.SS2.p3.4.m4.1.2.1" xref="S3.SS2.p3.4.m4.1.2.1.cmml">​</mo><mrow id="S3.SS2.p3.4.m4.1.2.3.2" xref="S3.SS2.p3.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.4.m4.1.2.3.2.1" xref="S3.SS2.p3.4.m4.1.2.cmml">(</mo><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.p3.4.m4.1.2.3.2.2" xref="S3.SS2.p3.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.2.cmml" xref="S3.SS2.p3.4.m4.1.2"><times id="S3.SS2.p3.4.m4.1.2.1.cmml" xref="S3.SS2.p3.4.m4.1.2.1"></times><apply id="S3.SS2.p3.4.m4.1.2.2.cmml" xref="S3.SS2.p3.4.m4.1.2.2"><ci id="S3.SS2.p3.4.m4.1.2.2.1.cmml" xref="S3.SS2.p3.4.m4.1.2.2.1">^</ci><ci id="S3.SS2.p3.4.m4.1.2.2.2.cmml" xref="S3.SS2.p3.4.m4.1.2.2.2">𝑓</ci></apply><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">\hat{f}(x)</annotation></semantics></math> as shown in <a href="#S3.F3.sf2" title="In Figure 3 ‣ III-B Fusing multiple sources ‣ III Multi-Source 3D ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3(b)</span></a>. A weight <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="\text{w}_{i}" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><msub id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mtext id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2a.cmml">w</mtext><mi id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.2a.cmml" xref="S3.SS2.p3.5.m5.1.1.2"><mtext id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2">w</mtext></ci><ci id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">\text{w}_{i}</annotation></semantics></math> for each data point and a kernel bandwidth <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mi id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">h</annotation></semantics></math> can be adjusted for fine-tuning the PDF estimate.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\hat{f}(x)=\frac{1}{h}\sum^{N}_{i=1}\text{w}_{i}K(\frac{x-x_{i}}{h})" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.3" xref="S3.E1.m1.2.3.cmml"><mrow id="S3.E1.m1.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><mover accent="true" id="S3.E1.m1.2.3.2.2" xref="S3.E1.m1.2.3.2.2.cmml"><mi id="S3.E1.m1.2.3.2.2.2" xref="S3.E1.m1.2.3.2.2.2.cmml">f</mi><mo id="S3.E1.m1.2.3.2.2.1" xref="S3.E1.m1.2.3.2.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.2.1" xref="S3.E1.m1.2.3.2.1.cmml">​</mo><mrow id="S3.E1.m1.2.3.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.3.2.3.2.1" xref="S3.E1.m1.2.3.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.E1.m1.2.3.2.3.2.2" xref="S3.E1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.3.1" xref="S3.E1.m1.2.3.1.cmml">=</mo><mrow id="S3.E1.m1.2.3.3" xref="S3.E1.m1.2.3.3.cmml"><mfrac id="S3.E1.m1.2.3.3.2" xref="S3.E1.m1.2.3.3.2.cmml"><mn id="S3.E1.m1.2.3.3.2.2" xref="S3.E1.m1.2.3.3.2.2.cmml">1</mn><mi id="S3.E1.m1.2.3.3.2.3" xref="S3.E1.m1.2.3.3.2.3.cmml">h</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.3.1" xref="S3.E1.m1.2.3.3.1.cmml">​</mo><mrow id="S3.E1.m1.2.3.3.3" xref="S3.E1.m1.2.3.3.3.cmml"><munderover id="S3.E1.m1.2.3.3.3.1" xref="S3.E1.m1.2.3.3.3.1.cmml"><mo movablelimits="false" id="S3.E1.m1.2.3.3.3.1.2.2" xref="S3.E1.m1.2.3.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.2.3.3.3.1.3" xref="S3.E1.m1.2.3.3.3.1.3.cmml"><mi id="S3.E1.m1.2.3.3.3.1.3.2" xref="S3.E1.m1.2.3.3.3.1.3.2.cmml">i</mi><mo id="S3.E1.m1.2.3.3.3.1.3.1" xref="S3.E1.m1.2.3.3.3.1.3.1.cmml">=</mo><mn id="S3.E1.m1.2.3.3.3.1.3.3" xref="S3.E1.m1.2.3.3.3.1.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.2.3.3.3.1.2.3" xref="S3.E1.m1.2.3.3.3.1.2.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.2.3.3.3.2" xref="S3.E1.m1.2.3.3.3.2.cmml"><msub id="S3.E1.m1.2.3.3.3.2.2" xref="S3.E1.m1.2.3.3.3.2.2.cmml"><mtext id="S3.E1.m1.2.3.3.3.2.2.2" xref="S3.E1.m1.2.3.3.3.2.2.2a.cmml">w</mtext><mi id="S3.E1.m1.2.3.3.3.2.2.3" xref="S3.E1.m1.2.3.3.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.3.3.2.1" xref="S3.E1.m1.2.3.3.3.2.1.cmml">​</mo><mi id="S3.E1.m1.2.3.3.3.2.3" xref="S3.E1.m1.2.3.3.3.2.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.3.3.2.1a" xref="S3.E1.m1.2.3.3.3.2.1.cmml">​</mo><mrow id="S3.E1.m1.2.3.3.3.2.4.2" xref="S3.E1.m1.2.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.3.3.3.2.4.2.1" xref="S3.E1.m1.2.2.cmml">(</mo><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">x</mi><mo id="S3.E1.m1.2.2.2.1" xref="S3.E1.m1.2.2.2.1.cmml">−</mo><msub id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml"><mi id="S3.E1.m1.2.2.2.3.2" xref="S3.E1.m1.2.2.2.3.2.cmml">x</mi><mi id="S3.E1.m1.2.2.2.3.3" xref="S3.E1.m1.2.2.2.3.3.cmml">i</mi></msub></mrow><mi id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">h</mi></mfrac><mo stretchy="false" id="S3.E1.m1.2.3.3.3.2.4.2.2" xref="S3.E1.m1.2.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.3.cmml" xref="S3.E1.m1.2.3"><eq id="S3.E1.m1.2.3.1.cmml" xref="S3.E1.m1.2.3.1"></eq><apply id="S3.E1.m1.2.3.2.cmml" xref="S3.E1.m1.2.3.2"><times id="S3.E1.m1.2.3.2.1.cmml" xref="S3.E1.m1.2.3.2.1"></times><apply id="S3.E1.m1.2.3.2.2.cmml" xref="S3.E1.m1.2.3.2.2"><ci id="S3.E1.m1.2.3.2.2.1.cmml" xref="S3.E1.m1.2.3.2.2.1">^</ci><ci id="S3.E1.m1.2.3.2.2.2.cmml" xref="S3.E1.m1.2.3.2.2.2">𝑓</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑥</ci></apply><apply id="S3.E1.m1.2.3.3.cmml" xref="S3.E1.m1.2.3.3"><times id="S3.E1.m1.2.3.3.1.cmml" xref="S3.E1.m1.2.3.3.1"></times><apply id="S3.E1.m1.2.3.3.2.cmml" xref="S3.E1.m1.2.3.3.2"><divide id="S3.E1.m1.2.3.3.2.1.cmml" xref="S3.E1.m1.2.3.3.2"></divide><cn type="integer" id="S3.E1.m1.2.3.3.2.2.cmml" xref="S3.E1.m1.2.3.3.2.2">1</cn><ci id="S3.E1.m1.2.3.3.2.3.cmml" xref="S3.E1.m1.2.3.3.2.3">ℎ</ci></apply><apply id="S3.E1.m1.2.3.3.3.cmml" xref="S3.E1.m1.2.3.3.3"><apply id="S3.E1.m1.2.3.3.3.1.cmml" xref="S3.E1.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.1.1.cmml" xref="S3.E1.m1.2.3.3.3.1">subscript</csymbol><apply id="S3.E1.m1.2.3.3.3.1.2.cmml" xref="S3.E1.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.1.2.1.cmml" xref="S3.E1.m1.2.3.3.3.1">superscript</csymbol><sum id="S3.E1.m1.2.3.3.3.1.2.2.cmml" xref="S3.E1.m1.2.3.3.3.1.2.2"></sum><ci id="S3.E1.m1.2.3.3.3.1.2.3.cmml" xref="S3.E1.m1.2.3.3.3.1.2.3">𝑁</ci></apply><apply id="S3.E1.m1.2.3.3.3.1.3.cmml" xref="S3.E1.m1.2.3.3.3.1.3"><eq id="S3.E1.m1.2.3.3.3.1.3.1.cmml" xref="S3.E1.m1.2.3.3.3.1.3.1"></eq><ci id="S3.E1.m1.2.3.3.3.1.3.2.cmml" xref="S3.E1.m1.2.3.3.3.1.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.2.3.3.3.1.3.3.cmml" xref="S3.E1.m1.2.3.3.3.1.3.3">1</cn></apply></apply><apply id="S3.E1.m1.2.3.3.3.2.cmml" xref="S3.E1.m1.2.3.3.3.2"><times id="S3.E1.m1.2.3.3.3.2.1.cmml" xref="S3.E1.m1.2.3.3.3.2.1"></times><apply id="S3.E1.m1.2.3.3.3.2.2.cmml" xref="S3.E1.m1.2.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.2.2.1.cmml" xref="S3.E1.m1.2.3.3.3.2.2">subscript</csymbol><ci id="S3.E1.m1.2.3.3.3.2.2.2a.cmml" xref="S3.E1.m1.2.3.3.3.2.2.2"><mtext id="S3.E1.m1.2.3.3.3.2.2.2.cmml" xref="S3.E1.m1.2.3.3.3.2.2.2">w</mtext></ci><ci id="S3.E1.m1.2.3.3.3.2.2.3.cmml" xref="S3.E1.m1.2.3.3.3.2.2.3">𝑖</ci></apply><ci id="S3.E1.m1.2.3.3.3.2.3.cmml" xref="S3.E1.m1.2.3.3.3.2.3">𝐾</ci><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.3.3.3.2.4.2"><divide id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.3.3.3.2.4.2"></divide><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><minus id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.1"></minus><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">𝑥</ci><apply id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.3.2.cmml" xref="S3.E1.m1.2.2.2.3.2">𝑥</ci><ci id="S3.E1.m1.2.2.2.3.3.cmml" xref="S3.E1.m1.2.2.2.3.3">𝑖</ci></apply></apply><ci id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3">ℎ</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\hat{f}(x)=\frac{1}{h}\sum^{N}_{i=1}\text{w}_{i}K(\frac{x-x_{i}}{h})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.02431/assets/images/kbf_example.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_square" width="592" height="627" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.02431/assets/images/kde_ry_ptnearmax.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_square" width="592" height="540" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">(a) Fused TTA predictions of 4 source detectors (multi-coloured boxes) with KDE Box Fusion (KBF); (b) Example of how we select the best rotation of multiple boxes with KDE.</span></figcaption>
</figure>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.4" class="ltx_p">In 3D object detection, a predicted box is parameterised by its centre <math id="S3.SS2.p5.1.m1.3" class="ltx_Math" alttext="(c_{x},c_{y},c_{z})" display="inline"><semantics id="S3.SS2.p5.1.m1.3a"><mrow id="S3.SS2.p5.1.m1.3.3.3" xref="S3.SS2.p5.1.m1.3.3.4.cmml"><mo stretchy="false" id="S3.SS2.p5.1.m1.3.3.3.4" xref="S3.SS2.p5.1.m1.3.3.4.cmml">(</mo><msub id="S3.SS2.p5.1.m1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.cmml">c</mi><mi id="S3.SS2.p5.1.m1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.1.3.cmml">x</mi></msub><mo id="S3.SS2.p5.1.m1.3.3.3.5" xref="S3.SS2.p5.1.m1.3.3.4.cmml">,</mo><msub id="S3.SS2.p5.1.m1.2.2.2.2" xref="S3.SS2.p5.1.m1.2.2.2.2.cmml"><mi id="S3.SS2.p5.1.m1.2.2.2.2.2" xref="S3.SS2.p5.1.m1.2.2.2.2.2.cmml">c</mi><mi id="S3.SS2.p5.1.m1.2.2.2.2.3" xref="S3.SS2.p5.1.m1.2.2.2.2.3.cmml">y</mi></msub><mo id="S3.SS2.p5.1.m1.3.3.3.6" xref="S3.SS2.p5.1.m1.3.3.4.cmml">,</mo><msub id="S3.SS2.p5.1.m1.3.3.3.3" xref="S3.SS2.p5.1.m1.3.3.3.3.cmml"><mi id="S3.SS2.p5.1.m1.3.3.3.3.2" xref="S3.SS2.p5.1.m1.3.3.3.3.2.cmml">c</mi><mi id="S3.SS2.p5.1.m1.3.3.3.3.3" xref="S3.SS2.p5.1.m1.3.3.3.3.3.cmml">z</mi></msub><mo stretchy="false" id="S3.SS2.p5.1.m1.3.3.3.7" xref="S3.SS2.p5.1.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.3b"><vector id="S3.SS2.p5.1.m1.3.3.4.cmml" xref="S3.SS2.p5.1.m1.3.3.3"><apply id="S3.SS2.p5.1.m1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2">𝑐</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3">𝑥</ci></apply><apply id="S3.SS2.p5.1.m1.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p5.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2.2">𝑐</ci><ci id="S3.SS2.p5.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2.3">𝑦</ci></apply><apply id="S3.SS2.p5.1.m1.3.3.3.3.cmml" xref="S3.SS2.p5.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.3.3.3.3.1.cmml" xref="S3.SS2.p5.1.m1.3.3.3.3">subscript</csymbol><ci id="S3.SS2.p5.1.m1.3.3.3.3.2.cmml" xref="S3.SS2.p5.1.m1.3.3.3.3.2">𝑐</ci><ci id="S3.SS2.p5.1.m1.3.3.3.3.3.cmml" xref="S3.SS2.p5.1.m1.3.3.3.3.3">𝑧</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.3c">(c_{x},c_{y},c_{z})</annotation></semantics></math>, dimensions <math id="S3.SS2.p5.2.m2.3" class="ltx_Math" alttext="(l,w,h)" display="inline"><semantics id="S3.SS2.p5.2.m2.3a"><mrow id="S3.SS2.p5.2.m2.3.4.2" xref="S3.SS2.p5.2.m2.3.4.1.cmml"><mo stretchy="false" id="S3.SS2.p5.2.m2.3.4.2.1" xref="S3.SS2.p5.2.m2.3.4.1.cmml">(</mo><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">l</mi><mo id="S3.SS2.p5.2.m2.3.4.2.2" xref="S3.SS2.p5.2.m2.3.4.1.cmml">,</mo><mi id="S3.SS2.p5.2.m2.2.2" xref="S3.SS2.p5.2.m2.2.2.cmml">w</mi><mo id="S3.SS2.p5.2.m2.3.4.2.3" xref="S3.SS2.p5.2.m2.3.4.1.cmml">,</mo><mi id="S3.SS2.p5.2.m2.3.3" xref="S3.SS2.p5.2.m2.3.3.cmml">h</mi><mo stretchy="false" id="S3.SS2.p5.2.m2.3.4.2.4" xref="S3.SS2.p5.2.m2.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.3b"><vector id="S3.SS2.p5.2.m2.3.4.1.cmml" xref="S3.SS2.p5.2.m2.3.4.2"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">𝑙</ci><ci id="S3.SS2.p5.2.m2.2.2.cmml" xref="S3.SS2.p5.2.m2.2.2">𝑤</ci><ci id="S3.SS2.p5.2.m2.3.3.cmml" xref="S3.SS2.p5.2.m2.3.3">ℎ</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.3c">(l,w,h)</annotation></semantics></math>, heading <math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><mi id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><ci id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">\theta</annotation></semantics></math> and a confidence score <math id="S3.SS2.p5.4.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS2.p5.4.m4.1a"><mi id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><ci id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">s</annotation></semantics></math>. Regardless of the localization accuracy, each box carries potentially valuable information. For example, we observe that a box with poor heading estimation may have accurate object dimensions. Therefore to combine the information of every box proposal, we opt to fuse the centre, length, width, height and heading separately.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.4" class="ltx_p">To merge boxes, we first create a KD-Tree with the box centroids. We identify matching boxes by querying for sets of boxes that are within a certain radius of each other. We compute KDE separately for the centroid, dimensions and score, then select the peak value of each of their PDF. We take the sine of the heading, <math id="S3.SS2.p6.1.m1.1" class="ltx_Math" alttext="\text{sin}(\theta)" display="inline"><semantics id="S3.SS2.p6.1.m1.1a"><mrow id="S3.SS2.p6.1.m1.1.2" xref="S3.SS2.p6.1.m1.1.2.cmml"><mtext id="S3.SS2.p6.1.m1.1.2.2" xref="S3.SS2.p6.1.m1.1.2.2a.cmml">sin</mtext><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.2.1" xref="S3.SS2.p6.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS2.p6.1.m1.1.2.3.2" xref="S3.SS2.p6.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p6.1.m1.1.2.3.2.1" xref="S3.SS2.p6.1.m1.1.2.cmml">(</mo><mi id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.SS2.p6.1.m1.1.2.3.2.2" xref="S3.SS2.p6.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.2"><times id="S3.SS2.p6.1.m1.1.2.1.cmml" xref="S3.SS2.p6.1.m1.1.2.1"></times><ci id="S3.SS2.p6.1.m1.1.2.2a.cmml" xref="S3.SS2.p6.1.m1.1.2.2"><mtext id="S3.SS2.p6.1.m1.1.2.2.cmml" xref="S3.SS2.p6.1.m1.1.2.2">sin</mtext></ci><ci id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">\text{sin}(\theta)</annotation></semantics></math>, to ensure rotational continuity before applying KDE. We found empirically that selecting rather than combining box headings produces the best result. Therefore, we choose the box heading with the highest likelihood based on the output PDF. For each box parameter we use a Gaussian kernel and use the predicted box confidence score as weights. Given the set of 1-frame detections from <math id="S3.SS2.p6.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p6.2.m2.1a"><mi id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><ci id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">i</annotation></semantics></math> source detectors, we fuse them with KBF to obtain boxes <math id="S3.SS2.p6.3.m3.3" class="ltx_Math" alttext="B_{\text{1f}}=\kappa(\{B_{i,\text{1f}}\})" display="inline"><semantics id="S3.SS2.p6.3.m3.3a"><mrow id="S3.SS2.p6.3.m3.3.3" xref="S3.SS2.p6.3.m3.3.3.cmml"><msub id="S3.SS2.p6.3.m3.3.3.3" xref="S3.SS2.p6.3.m3.3.3.3.cmml"><mi id="S3.SS2.p6.3.m3.3.3.3.2" xref="S3.SS2.p6.3.m3.3.3.3.2.cmml">B</mi><mtext id="S3.SS2.p6.3.m3.3.3.3.3" xref="S3.SS2.p6.3.m3.3.3.3.3a.cmml">1f</mtext></msub><mo id="S3.SS2.p6.3.m3.3.3.2" xref="S3.SS2.p6.3.m3.3.3.2.cmml">=</mo><mrow id="S3.SS2.p6.3.m3.3.3.1" xref="S3.SS2.p6.3.m3.3.3.1.cmml"><mi id="S3.SS2.p6.3.m3.3.3.1.3" xref="S3.SS2.p6.3.m3.3.3.1.3.cmml">κ</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.3.m3.3.3.1.2" xref="S3.SS2.p6.3.m3.3.3.1.2.cmml">​</mo><mrow id="S3.SS2.p6.3.m3.3.3.1.1.1" xref="S3.SS2.p6.3.m3.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p6.3.m3.3.3.1.1.1.2" xref="S3.SS2.p6.3.m3.3.3.1.cmml">(</mo><mrow id="S3.SS2.p6.3.m3.3.3.1.1.1.1.1" xref="S3.SS2.p6.3.m3.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.2" xref="S3.SS2.p6.3.m3.3.3.1.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.1" xref="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.1.2" xref="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.1.2.cmml">B</mi><mrow id="S3.SS2.p6.3.m3.2.2.2.4" xref="S3.SS2.p6.3.m3.2.2.2.3.cmml"><mi id="S3.SS2.p6.3.m3.1.1.1.1" xref="S3.SS2.p6.3.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p6.3.m3.2.2.2.4.1" xref="S3.SS2.p6.3.m3.2.2.2.3.cmml">,</mo><mtext id="S3.SS2.p6.3.m3.2.2.2.2" xref="S3.SS2.p6.3.m3.2.2.2.2a.cmml">1f</mtext></mrow></msub><mo stretchy="false" id="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.3" xref="S3.SS2.p6.3.m3.3.3.1.1.1.1.2.cmml">}</mo></mrow><mo stretchy="false" id="S3.SS2.p6.3.m3.3.3.1.1.1.3" xref="S3.SS2.p6.3.m3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.3b"><apply id="S3.SS2.p6.3.m3.3.3.cmml" xref="S3.SS2.p6.3.m3.3.3"><eq id="S3.SS2.p6.3.m3.3.3.2.cmml" xref="S3.SS2.p6.3.m3.3.3.2"></eq><apply id="S3.SS2.p6.3.m3.3.3.3.cmml" xref="S3.SS2.p6.3.m3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.3.3.3.1.cmml" xref="S3.SS2.p6.3.m3.3.3.3">subscript</csymbol><ci id="S3.SS2.p6.3.m3.3.3.3.2.cmml" xref="S3.SS2.p6.3.m3.3.3.3.2">𝐵</ci><ci id="S3.SS2.p6.3.m3.3.3.3.3a.cmml" xref="S3.SS2.p6.3.m3.3.3.3.3"><mtext mathsize="70%" id="S3.SS2.p6.3.m3.3.3.3.3.cmml" xref="S3.SS2.p6.3.m3.3.3.3.3">1f</mtext></ci></apply><apply id="S3.SS2.p6.3.m3.3.3.1.cmml" xref="S3.SS2.p6.3.m3.3.3.1"><times id="S3.SS2.p6.3.m3.3.3.1.2.cmml" xref="S3.SS2.p6.3.m3.3.3.1.2"></times><ci id="S3.SS2.p6.3.m3.3.3.1.3.cmml" xref="S3.SS2.p6.3.m3.3.3.1.3">𝜅</ci><set id="S3.SS2.p6.3.m3.3.3.1.1.1.1.2.cmml" xref="S3.SS2.p6.3.m3.3.3.1.1.1.1.1"><apply id="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p6.3.m3.3.3.1.1.1.1.1.1.2">𝐵</ci><list id="S3.SS2.p6.3.m3.2.2.2.3.cmml" xref="S3.SS2.p6.3.m3.2.2.2.4"><ci id="S3.SS2.p6.3.m3.1.1.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1.1.1">𝑖</ci><ci id="S3.SS2.p6.3.m3.2.2.2.2a.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2"><mtext mathsize="70%" id="S3.SS2.p6.3.m3.2.2.2.2.cmml" xref="S3.SS2.p6.3.m3.2.2.2.2">1f</mtext></ci></list></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.3c">B_{\text{1f}}=\kappa(\{B_{i,\text{1f}}\})</annotation></semantics></math> for each frame <math id="S3.SS2.p6.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p6.4.m4.1a"><mi id="S3.SS2.p6.4.m4.1.1" xref="S3.SS2.p6.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.4.m4.1b"><ci id="S3.SS2.p6.4.m4.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.4.m4.1c">k</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Utilizing Temporal Information for Static Vehicles</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.9" class="ltx_p">In urban city environments, parked vehicles are common and often are completely static for long periods. Car parks in particular are often present in datasets and provides an abundance of viewpoints for various types of vehicles. With this in mind, we use multi-frame detection to leverage the additional viewpoints provided by accumulated point clouds to better localize static vehicles. For multi-frame detection, the input point cloud <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="p^{k}_{t}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msubsup id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2.2" xref="S3.SS3.p1.1.m1.1.1.2.2.cmml">p</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">t</mi><mi id="S3.SS3.p1.1.m1.1.1.2.3" xref="S3.SS3.p1.1.m1.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><apply id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2.2">𝑝</ci><ci id="S3.SS3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.2.3">𝑘</ci></apply><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">p^{k}_{t}</annotation></semantics></math> for frame <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">k</annotation></semantics></math> is the stacking of <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">N</annotation></semantics></math> historical point clouds with the point cloud at time <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">t</annotation></semantics></math> where each historical point cloud has been transformed to the ego-frame at time <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mi id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">t</annotation></semantics></math>. With a 10Hz lidar, each point cloud is 0.1s apart, and the accumulated point cloud can be represented as <math id="S3.SS3.p1.6.m6.6" class="ltx_Math" alttext="p^{k}_{t}=\{p_{t},p_{t-0.1},p_{t-0.2},...,p_{t-N(0.1)}\}" display="inline"><semantics id="S3.SS3.p1.6.m6.6a"><mrow id="S3.SS3.p1.6.m6.6.6" xref="S3.SS3.p1.6.m6.6.6.cmml"><msubsup id="S3.SS3.p1.6.m6.6.6.6" xref="S3.SS3.p1.6.m6.6.6.6.cmml"><mi id="S3.SS3.p1.6.m6.6.6.6.2.2" xref="S3.SS3.p1.6.m6.6.6.6.2.2.cmml">p</mi><mi id="S3.SS3.p1.6.m6.6.6.6.3" xref="S3.SS3.p1.6.m6.6.6.6.3.cmml">t</mi><mi id="S3.SS3.p1.6.m6.6.6.6.2.3" xref="S3.SS3.p1.6.m6.6.6.6.2.3.cmml">k</mi></msubsup><mo id="S3.SS3.p1.6.m6.6.6.5" xref="S3.SS3.p1.6.m6.6.6.5.cmml">=</mo><mrow id="S3.SS3.p1.6.m6.6.6.4.4" xref="S3.SS3.p1.6.m6.6.6.4.5.cmml"><mo stretchy="false" id="S3.SS3.p1.6.m6.6.6.4.4.5" xref="S3.SS3.p1.6.m6.6.6.4.5.cmml">{</mo><msub id="S3.SS3.p1.6.m6.3.3.1.1.1" xref="S3.SS3.p1.6.m6.3.3.1.1.1.cmml"><mi id="S3.SS3.p1.6.m6.3.3.1.1.1.2" xref="S3.SS3.p1.6.m6.3.3.1.1.1.2.cmml">p</mi><mi id="S3.SS3.p1.6.m6.3.3.1.1.1.3" xref="S3.SS3.p1.6.m6.3.3.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS3.p1.6.m6.6.6.4.4.6" xref="S3.SS3.p1.6.m6.6.6.4.5.cmml">,</mo><msub id="S3.SS3.p1.6.m6.4.4.2.2.2" xref="S3.SS3.p1.6.m6.4.4.2.2.2.cmml"><mi id="S3.SS3.p1.6.m6.4.4.2.2.2.2" xref="S3.SS3.p1.6.m6.4.4.2.2.2.2.cmml">p</mi><mrow id="S3.SS3.p1.6.m6.4.4.2.2.2.3" xref="S3.SS3.p1.6.m6.4.4.2.2.2.3.cmml"><mi id="S3.SS3.p1.6.m6.4.4.2.2.2.3.2" xref="S3.SS3.p1.6.m6.4.4.2.2.2.3.2.cmml">t</mi><mo id="S3.SS3.p1.6.m6.4.4.2.2.2.3.1" xref="S3.SS3.p1.6.m6.4.4.2.2.2.3.1.cmml">−</mo><mn id="S3.SS3.p1.6.m6.4.4.2.2.2.3.3" xref="S3.SS3.p1.6.m6.4.4.2.2.2.3.3.cmml">0.1</mn></mrow></msub><mo id="S3.SS3.p1.6.m6.6.6.4.4.7" xref="S3.SS3.p1.6.m6.6.6.4.5.cmml">,</mo><msub id="S3.SS3.p1.6.m6.5.5.3.3.3" xref="S3.SS3.p1.6.m6.5.5.3.3.3.cmml"><mi id="S3.SS3.p1.6.m6.5.5.3.3.3.2" xref="S3.SS3.p1.6.m6.5.5.3.3.3.2.cmml">p</mi><mrow id="S3.SS3.p1.6.m6.5.5.3.3.3.3" xref="S3.SS3.p1.6.m6.5.5.3.3.3.3.cmml"><mi id="S3.SS3.p1.6.m6.5.5.3.3.3.3.2" xref="S3.SS3.p1.6.m6.5.5.3.3.3.3.2.cmml">t</mi><mo id="S3.SS3.p1.6.m6.5.5.3.3.3.3.1" xref="S3.SS3.p1.6.m6.5.5.3.3.3.3.1.cmml">−</mo><mn id="S3.SS3.p1.6.m6.5.5.3.3.3.3.3" xref="S3.SS3.p1.6.m6.5.5.3.3.3.3.3.cmml">0.2</mn></mrow></msub><mo id="S3.SS3.p1.6.m6.6.6.4.4.8" xref="S3.SS3.p1.6.m6.6.6.4.5.cmml">,</mo><mi mathvariant="normal" id="S3.SS3.p1.6.m6.2.2" xref="S3.SS3.p1.6.m6.2.2.cmml">…</mi><mo id="S3.SS3.p1.6.m6.6.6.4.4.9" xref="S3.SS3.p1.6.m6.6.6.4.5.cmml">,</mo><msub id="S3.SS3.p1.6.m6.6.6.4.4.4" xref="S3.SS3.p1.6.m6.6.6.4.4.4.cmml"><mi id="S3.SS3.p1.6.m6.6.6.4.4.4.2" xref="S3.SS3.p1.6.m6.6.6.4.4.4.2.cmml">p</mi><mrow id="S3.SS3.p1.6.m6.1.1.1" xref="S3.SS3.p1.6.m6.1.1.1.cmml"><mi id="S3.SS3.p1.6.m6.1.1.1.3" xref="S3.SS3.p1.6.m6.1.1.1.3.cmml">t</mi><mo id="S3.SS3.p1.6.m6.1.1.1.2" xref="S3.SS3.p1.6.m6.1.1.1.2.cmml">−</mo><mrow id="S3.SS3.p1.6.m6.1.1.1.4" xref="S3.SS3.p1.6.m6.1.1.1.4.cmml"><mi id="S3.SS3.p1.6.m6.1.1.1.4.2" xref="S3.SS3.p1.6.m6.1.1.1.4.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.6.m6.1.1.1.4.1" xref="S3.SS3.p1.6.m6.1.1.1.4.1.cmml">​</mo><mrow id="S3.SS3.p1.6.m6.1.1.1.4.3.2" xref="S3.SS3.p1.6.m6.1.1.1.4.cmml"><mo stretchy="false" id="S3.SS3.p1.6.m6.1.1.1.4.3.2.1" xref="S3.SS3.p1.6.m6.1.1.1.4.cmml">(</mo><mn id="S3.SS3.p1.6.m6.1.1.1.1" xref="S3.SS3.p1.6.m6.1.1.1.1.cmml">0.1</mn><mo stretchy="false" id="S3.SS3.p1.6.m6.1.1.1.4.3.2.2" xref="S3.SS3.p1.6.m6.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mo stretchy="false" id="S3.SS3.p1.6.m6.6.6.4.4.10" xref="S3.SS3.p1.6.m6.6.6.4.5.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.6b"><apply id="S3.SS3.p1.6.m6.6.6.cmml" xref="S3.SS3.p1.6.m6.6.6"><eq id="S3.SS3.p1.6.m6.6.6.5.cmml" xref="S3.SS3.p1.6.m6.6.6.5"></eq><apply id="S3.SS3.p1.6.m6.6.6.6.cmml" xref="S3.SS3.p1.6.m6.6.6.6"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.6.6.6.1.cmml" xref="S3.SS3.p1.6.m6.6.6.6">subscript</csymbol><apply id="S3.SS3.p1.6.m6.6.6.6.2.cmml" xref="S3.SS3.p1.6.m6.6.6.6"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.6.6.6.2.1.cmml" xref="S3.SS3.p1.6.m6.6.6.6">superscript</csymbol><ci id="S3.SS3.p1.6.m6.6.6.6.2.2.cmml" xref="S3.SS3.p1.6.m6.6.6.6.2.2">𝑝</ci><ci id="S3.SS3.p1.6.m6.6.6.6.2.3.cmml" xref="S3.SS3.p1.6.m6.6.6.6.2.3">𝑘</ci></apply><ci id="S3.SS3.p1.6.m6.6.6.6.3.cmml" xref="S3.SS3.p1.6.m6.6.6.6.3">𝑡</ci></apply><set id="S3.SS3.p1.6.m6.6.6.4.5.cmml" xref="S3.SS3.p1.6.m6.6.6.4.4"><apply id="S3.SS3.p1.6.m6.3.3.1.1.1.cmml" xref="S3.SS3.p1.6.m6.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.3.3.1.1.1.1.cmml" xref="S3.SS3.p1.6.m6.3.3.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.6.m6.3.3.1.1.1.2.cmml" xref="S3.SS3.p1.6.m6.3.3.1.1.1.2">𝑝</ci><ci id="S3.SS3.p1.6.m6.3.3.1.1.1.3.cmml" xref="S3.SS3.p1.6.m6.3.3.1.1.1.3">𝑡</ci></apply><apply id="S3.SS3.p1.6.m6.4.4.2.2.2.cmml" xref="S3.SS3.p1.6.m6.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.4.4.2.2.2.1.cmml" xref="S3.SS3.p1.6.m6.4.4.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.6.m6.4.4.2.2.2.2.cmml" xref="S3.SS3.p1.6.m6.4.4.2.2.2.2">𝑝</ci><apply id="S3.SS3.p1.6.m6.4.4.2.2.2.3.cmml" xref="S3.SS3.p1.6.m6.4.4.2.2.2.3"><minus id="S3.SS3.p1.6.m6.4.4.2.2.2.3.1.cmml" xref="S3.SS3.p1.6.m6.4.4.2.2.2.3.1"></minus><ci id="S3.SS3.p1.6.m6.4.4.2.2.2.3.2.cmml" xref="S3.SS3.p1.6.m6.4.4.2.2.2.3.2">𝑡</ci><cn type="float" id="S3.SS3.p1.6.m6.4.4.2.2.2.3.3.cmml" xref="S3.SS3.p1.6.m6.4.4.2.2.2.3.3">0.1</cn></apply></apply><apply id="S3.SS3.p1.6.m6.5.5.3.3.3.cmml" xref="S3.SS3.p1.6.m6.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.5.5.3.3.3.1.cmml" xref="S3.SS3.p1.6.m6.5.5.3.3.3">subscript</csymbol><ci id="S3.SS3.p1.6.m6.5.5.3.3.3.2.cmml" xref="S3.SS3.p1.6.m6.5.5.3.3.3.2">𝑝</ci><apply id="S3.SS3.p1.6.m6.5.5.3.3.3.3.cmml" xref="S3.SS3.p1.6.m6.5.5.3.3.3.3"><minus id="S3.SS3.p1.6.m6.5.5.3.3.3.3.1.cmml" xref="S3.SS3.p1.6.m6.5.5.3.3.3.3.1"></minus><ci id="S3.SS3.p1.6.m6.5.5.3.3.3.3.2.cmml" xref="S3.SS3.p1.6.m6.5.5.3.3.3.3.2">𝑡</ci><cn type="float" id="S3.SS3.p1.6.m6.5.5.3.3.3.3.3.cmml" xref="S3.SS3.p1.6.m6.5.5.3.3.3.3.3">0.2</cn></apply></apply><ci id="S3.SS3.p1.6.m6.2.2.cmml" xref="S3.SS3.p1.6.m6.2.2">…</ci><apply id="S3.SS3.p1.6.m6.6.6.4.4.4.cmml" xref="S3.SS3.p1.6.m6.6.6.4.4.4"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.6.6.4.4.4.1.cmml" xref="S3.SS3.p1.6.m6.6.6.4.4.4">subscript</csymbol><ci id="S3.SS3.p1.6.m6.6.6.4.4.4.2.cmml" xref="S3.SS3.p1.6.m6.6.6.4.4.4.2">𝑝</ci><apply id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1.1"><minus id="S3.SS3.p1.6.m6.1.1.1.2.cmml" xref="S3.SS3.p1.6.m6.1.1.1.2"></minus><ci id="S3.SS3.p1.6.m6.1.1.1.3.cmml" xref="S3.SS3.p1.6.m6.1.1.1.3">𝑡</ci><apply id="S3.SS3.p1.6.m6.1.1.1.4.cmml" xref="S3.SS3.p1.6.m6.1.1.1.4"><times id="S3.SS3.p1.6.m6.1.1.1.4.1.cmml" xref="S3.SS3.p1.6.m6.1.1.1.4.1"></times><ci id="S3.SS3.p1.6.m6.1.1.1.4.2.cmml" xref="S3.SS3.p1.6.m6.1.1.1.4.2">𝑁</ci><cn type="float" id="S3.SS3.p1.6.m6.1.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1.1.1">0.1</cn></apply></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.6c">p^{k}_{t}=\{p_{t},p_{t-0.1},p_{t-0.2},...,p_{t-N(0.1)}\}</annotation></semantics></math>. By feeding the source detector with <math id="S3.SS3.p1.7.m7.1" class="ltx_Math" alttext="p^{k}_{t}" display="inline"><semantics id="S3.SS3.p1.7.m7.1a"><msubsup id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml"><mi id="S3.SS3.p1.7.m7.1.1.2.2" xref="S3.SS3.p1.7.m7.1.1.2.2.cmml">p</mi><mi id="S3.SS3.p1.7.m7.1.1.3" xref="S3.SS3.p1.7.m7.1.1.3.cmml">t</mi><mi id="S3.SS3.p1.7.m7.1.1.2.3" xref="S3.SS3.p1.7.m7.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><apply id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">subscript</csymbol><apply id="S3.SS3.p1.7.m7.1.1.2.cmml" xref="S3.SS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.2.1.cmml" xref="S3.SS3.p1.7.m7.1.1">superscript</csymbol><ci id="S3.SS3.p1.7.m7.1.1.2.2.cmml" xref="S3.SS3.p1.7.m7.1.1.2.2">𝑝</ci><ci id="S3.SS3.p1.7.m7.1.1.2.3.cmml" xref="S3.SS3.p1.7.m7.1.1.2.3">𝑘</ci></apply><ci id="S3.SS3.p1.7.m7.1.1.3.cmml" xref="S3.SS3.p1.7.m7.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">p^{k}_{t}</annotation></semantics></math>, it can utilize the increased point density to improve the accuracy of box localization, particularly for static objects that may be occluded or located at a distance as shown in <a href="#S3.F4" title="In III-C Utilizing Temporal Information for Static Vehicles ‣ III Multi-Source 3D ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>. In particular, we follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to use <math id="S3.SS3.p1.8.m8.1" class="ltx_Math" alttext="N=16" display="inline"><semantics id="S3.SS3.p1.8.m8.1a"><mrow id="S3.SS3.p1.8.m8.1.1" xref="S3.SS3.p1.8.m8.1.1.cmml"><mi id="S3.SS3.p1.8.m8.1.1.2" xref="S3.SS3.p1.8.m8.1.1.2.cmml">N</mi><mo id="S3.SS3.p1.8.m8.1.1.1" xref="S3.SS3.p1.8.m8.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.8.m8.1.1.3" xref="S3.SS3.p1.8.m8.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.1b"><apply id="S3.SS3.p1.8.m8.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1"><eq id="S3.SS3.p1.8.m8.1.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1.1"></eq><ci id="S3.SS3.p1.8.m8.1.1.2.cmml" xref="S3.SS3.p1.8.m8.1.1.2">𝑁</ci><cn type="integer" id="S3.SS3.p1.8.m8.1.1.3.cmml" xref="S3.SS3.p1.8.m8.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.1c">N=16</annotation></semantics></math> frames for detection for a trade-off between TTA inference time and point cloud density. We use the same TTA permutations and combine box predictions with KBF, <math id="S3.SS3.p1.9.m9.3" class="ltx_Math" alttext="B_{\text{16f}}=\kappa(\{B_{i,\text{16f}}\})" display="inline"><semantics id="S3.SS3.p1.9.m9.3a"><mrow id="S3.SS3.p1.9.m9.3.3" xref="S3.SS3.p1.9.m9.3.3.cmml"><msub id="S3.SS3.p1.9.m9.3.3.3" xref="S3.SS3.p1.9.m9.3.3.3.cmml"><mi id="S3.SS3.p1.9.m9.3.3.3.2" xref="S3.SS3.p1.9.m9.3.3.3.2.cmml">B</mi><mtext id="S3.SS3.p1.9.m9.3.3.3.3" xref="S3.SS3.p1.9.m9.3.3.3.3a.cmml">16f</mtext></msub><mo id="S3.SS3.p1.9.m9.3.3.2" xref="S3.SS3.p1.9.m9.3.3.2.cmml">=</mo><mrow id="S3.SS3.p1.9.m9.3.3.1" xref="S3.SS3.p1.9.m9.3.3.1.cmml"><mi id="S3.SS3.p1.9.m9.3.3.1.3" xref="S3.SS3.p1.9.m9.3.3.1.3.cmml">κ</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.9.m9.3.3.1.2" xref="S3.SS3.p1.9.m9.3.3.1.2.cmml">​</mo><mrow id="S3.SS3.p1.9.m9.3.3.1.1.1" xref="S3.SS3.p1.9.m9.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.p1.9.m9.3.3.1.1.1.2" xref="S3.SS3.p1.9.m9.3.3.1.cmml">(</mo><mrow id="S3.SS3.p1.9.m9.3.3.1.1.1.1.1" xref="S3.SS3.p1.9.m9.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.2" xref="S3.SS3.p1.9.m9.3.3.1.1.1.1.2.cmml">{</mo><msub id="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.1" xref="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.1.2" xref="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.1.2.cmml">B</mi><mrow id="S3.SS3.p1.9.m9.2.2.2.4" xref="S3.SS3.p1.9.m9.2.2.2.3.cmml"><mi id="S3.SS3.p1.9.m9.1.1.1.1" xref="S3.SS3.p1.9.m9.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p1.9.m9.2.2.2.4.1" xref="S3.SS3.p1.9.m9.2.2.2.3.cmml">,</mo><mtext id="S3.SS3.p1.9.m9.2.2.2.2" xref="S3.SS3.p1.9.m9.2.2.2.2a.cmml">16f</mtext></mrow></msub><mo stretchy="false" id="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.3" xref="S3.SS3.p1.9.m9.3.3.1.1.1.1.2.cmml">}</mo></mrow><mo stretchy="false" id="S3.SS3.p1.9.m9.3.3.1.1.1.3" xref="S3.SS3.p1.9.m9.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m9.3b"><apply id="S3.SS3.p1.9.m9.3.3.cmml" xref="S3.SS3.p1.9.m9.3.3"><eq id="S3.SS3.p1.9.m9.3.3.2.cmml" xref="S3.SS3.p1.9.m9.3.3.2"></eq><apply id="S3.SS3.p1.9.m9.3.3.3.cmml" xref="S3.SS3.p1.9.m9.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.9.m9.3.3.3.1.cmml" xref="S3.SS3.p1.9.m9.3.3.3">subscript</csymbol><ci id="S3.SS3.p1.9.m9.3.3.3.2.cmml" xref="S3.SS3.p1.9.m9.3.3.3.2">𝐵</ci><ci id="S3.SS3.p1.9.m9.3.3.3.3a.cmml" xref="S3.SS3.p1.9.m9.3.3.3.3"><mtext mathsize="70%" id="S3.SS3.p1.9.m9.3.3.3.3.cmml" xref="S3.SS3.p1.9.m9.3.3.3.3">16f</mtext></ci></apply><apply id="S3.SS3.p1.9.m9.3.3.1.cmml" xref="S3.SS3.p1.9.m9.3.3.1"><times id="S3.SS3.p1.9.m9.3.3.1.2.cmml" xref="S3.SS3.p1.9.m9.3.3.1.2"></times><ci id="S3.SS3.p1.9.m9.3.3.1.3.cmml" xref="S3.SS3.p1.9.m9.3.3.1.3">𝜅</ci><set id="S3.SS3.p1.9.m9.3.3.1.1.1.1.2.cmml" xref="S3.SS3.p1.9.m9.3.3.1.1.1.1.1"><apply id="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.9.m9.3.3.1.1.1.1.1.1.2">𝐵</ci><list id="S3.SS3.p1.9.m9.2.2.2.3.cmml" xref="S3.SS3.p1.9.m9.2.2.2.4"><ci id="S3.SS3.p1.9.m9.1.1.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1.1.1">𝑖</ci><ci id="S3.SS3.p1.9.m9.2.2.2.2a.cmml" xref="S3.SS3.p1.9.m9.2.2.2.2"><mtext mathsize="70%" id="S3.SS3.p1.9.m9.2.2.2.2.cmml" xref="S3.SS3.p1.9.m9.2.2.2.2">16f</mtext></ci></list></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m9.3c">B_{\text{16f}}=\kappa(\{B_{i,\text{16f}}\})</annotation></semantics></math>, as in <a href="#S3.SS2" title="III-B Fusing multiple sources ‣ III Multi-Source 3D ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2304.02431/assets/images/1vs16f_n008-2018-07-26-12-13-50-0400__LIDAR_TOP__1532621756199796_newcol.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="586" height="564" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">1-frame KBF boxes compared to 16-frame KBF boxes on the nuScenes dataset with detectors trained on Waymo/Lyft as the source domain. Visualized is a 1-frame point cloud.</span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Motion State Classification</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.2" class="ltx_p">To assign temporally consistent boxes for static vehicles, we associate detection boxes across frames to classify the motion state of the object. To accomplish this, we transform all boxes to world coordinates with known sensor poses. Thereafter, we utilize a tracking-by-detection, Kalman Filter-based tracker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> which is an implementation variant of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> to generate box trajectories for both 1-frame, <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="T_{\text{1f}}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">T</mi><mtext id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3a.cmml">1f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝑇</ci><ci id="S3.SS4.p1.1.m1.1.1.3a.cmml" xref="S3.SS4.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">1f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">T_{\text{1f}}</annotation></semantics></math>, and 16-frame, <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="T_{\text{16f}}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><msub id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">T</mi><mtext id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3a.cmml">16f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">𝑇</ci><ci id="S3.SS4.p1.2.m2.1.1.3a.cmml" xref="S3.SS4.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">16f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">T_{\text{16f}}</annotation></semantics></math>, predictions. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, we classify the tracklets as either static or dynamic based on pre-defined thresholds of begin-to-end distance and box centre variance.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.3" class="ltx_p">Nonetheless, this is not sufficient in classifying <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="T_{\text{16f}}" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><msub id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">T</mi><mtext id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3a.cmml">16f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">𝑇</ci><ci id="S3.SS4.p2.1.m1.1.1.3a.cmml" xref="S3.SS4.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">16f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">T_{\text{16f}}</annotation></semantics></math> due to predictions on the motion trails of dynamic objects in 16-frame detection. We observe that when two dynamic vehicles are driving in the same lane at a similar speed, their motion tails often overlap, leading to a false ’static’ classification. A simple approach to replacing static boxes would be to rely solely on the motion classification of <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="T_{\text{1f}}" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><msub id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">T</mi><mtext id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3a.cmml">1f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">𝑇</ci><ci id="S3.SS4.p2.2.m2.1.1.3a.cmml" xref="S3.SS4.p2.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3">1f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">T_{\text{1f}}</annotation></semantics></math> and replace static objects with their corresponding <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="T_{\text{16f}}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><msub id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">T</mi><mtext id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3a.cmml">16f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">𝑇</ci><ci id="S3.SS4.p2.3.m3.1.1.3a.cmml" xref="S3.SS4.p2.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">16f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">T_{\text{16f}}</annotation></semantics></math> boxes. However, this approach is suboptimal since 1-frame detection often detects less static objects at a distance than 16-frame detection for certain target datasets as shown in <a href="#S3.F4" title="In III-C Utilizing Temporal Information for Static Vehicles ‣ III Multi-Source 3D ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, causing us to overlook valuable 16-frame boxes as a result.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.9" class="ltx_p">To improve the accuracy of <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="T_{\text{16f}}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">T</mi><mtext id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3a.cmml">16f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝑇</ci><ci id="S3.SS4.p3.1.m1.1.1.3a.cmml" xref="S3.SS4.p3.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">16f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">T_{\text{16f}}</annotation></semantics></math> motion classification, we make a key observation: in 3D space, dynamic objects cannot occupy the same space as an object that is static throughout the sequence. This means that the trajectory of a dynamic object will never overlap with that of a parked vehicle. To take advantage of this, we use <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="T_{\text{1f}}" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><msub id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">T</mi><mtext id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3a.cmml">1f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">𝑇</ci><ci id="S3.SS4.p3.2.m2.1.1.3a.cmml" xref="S3.SS4.p3.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.2.m2.1.1.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3">1f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">T_{\text{1f}}</annotation></semantics></math> to identify all dynamic objects in frame <math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">k</annotation></semantics></math>, and project their entire trajectory <math id="S3.SS4.p3.4.m4.1" class="ltx_Math" alttext="T_{\text{1f}}" display="inline"><semantics id="S3.SS4.p3.4.m4.1a"><msub id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml"><mi id="S3.SS4.p3.4.m4.1.1.2" xref="S3.SS4.p3.4.m4.1.1.2.cmml">T</mi><mtext id="S3.SS4.p3.4.m4.1.1.3" xref="S3.SS4.p3.4.m4.1.1.3a.cmml">1f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><apply id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.1.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p3.4.m4.1.1.2.cmml" xref="S3.SS4.p3.4.m4.1.1.2">𝑇</ci><ci id="S3.SS4.p3.4.m4.1.1.3a.cmml" xref="S3.SS4.p3.4.m4.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.4.m4.1.1.3.cmml" xref="S3.SS4.p3.4.m4.1.1.3">1f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">T_{\text{1f}}</annotation></semantics></math> into frame <math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><mi id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">k</annotation></semantics></math>. This effectively identifies drivable areas and enables us to filter out wrongly classified static boxes through comparing IOU-matched <math id="S3.SS4.p3.6.m6.1" class="ltx_Math" alttext="T_{\text{1f}}" display="inline"><semantics id="S3.SS4.p3.6.m6.1a"><msub id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml"><mi id="S3.SS4.p3.6.m6.1.1.2" xref="S3.SS4.p3.6.m6.1.1.2.cmml">T</mi><mtext id="S3.SS4.p3.6.m6.1.1.3" xref="S3.SS4.p3.6.m6.1.1.3a.cmml">1f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><apply id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.6.m6.1.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p3.6.m6.1.1.2.cmml" xref="S3.SS4.p3.6.m6.1.1.2">𝑇</ci><ci id="S3.SS4.p3.6.m6.1.1.3a.cmml" xref="S3.SS4.p3.6.m6.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.6.m6.1.1.3.cmml" xref="S3.SS4.p3.6.m6.1.1.3">1f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">T_{\text{1f}}</annotation></semantics></math> and <math id="S3.SS4.p3.7.m7.1" class="ltx_Math" alttext="T^{k}_{\text{16f}}" display="inline"><semantics id="S3.SS4.p3.7.m7.1a"><msubsup id="S3.SS4.p3.7.m7.1.1" xref="S3.SS4.p3.7.m7.1.1.cmml"><mi id="S3.SS4.p3.7.m7.1.1.2.2" xref="S3.SS4.p3.7.m7.1.1.2.2.cmml">T</mi><mtext id="S3.SS4.p3.7.m7.1.1.3" xref="S3.SS4.p3.7.m7.1.1.3a.cmml">16f</mtext><mi id="S3.SS4.p3.7.m7.1.1.2.3" xref="S3.SS4.p3.7.m7.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.7.m7.1b"><apply id="S3.SS4.p3.7.m7.1.1.cmml" xref="S3.SS4.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.7.m7.1.1.1.cmml" xref="S3.SS4.p3.7.m7.1.1">subscript</csymbol><apply id="S3.SS4.p3.7.m7.1.1.2.cmml" xref="S3.SS4.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.7.m7.1.1.2.1.cmml" xref="S3.SS4.p3.7.m7.1.1">superscript</csymbol><ci id="S3.SS4.p3.7.m7.1.1.2.2.cmml" xref="S3.SS4.p3.7.m7.1.1.2.2">𝑇</ci><ci id="S3.SS4.p3.7.m7.1.1.2.3.cmml" xref="S3.SS4.p3.7.m7.1.1.2.3">𝑘</ci></apply><ci id="S3.SS4.p3.7.m7.1.1.3a.cmml" xref="S3.SS4.p3.7.m7.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.7.m7.1.1.3.cmml" xref="S3.SS4.p3.7.m7.1.1.3">16f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.7.m7.1c">T^{k}_{\text{16f}}</annotation></semantics></math> boxes. If either <math id="S3.SS4.p3.8.m8.1" class="ltx_Math" alttext="T_{\text{1f}}" display="inline"><semantics id="S3.SS4.p3.8.m8.1a"><msub id="S3.SS4.p3.8.m8.1.1" xref="S3.SS4.p3.8.m8.1.1.cmml"><mi id="S3.SS4.p3.8.m8.1.1.2" xref="S3.SS4.p3.8.m8.1.1.2.cmml">T</mi><mtext id="S3.SS4.p3.8.m8.1.1.3" xref="S3.SS4.p3.8.m8.1.1.3a.cmml">1f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.8.m8.1b"><apply id="S3.SS4.p3.8.m8.1.1.cmml" xref="S3.SS4.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.8.m8.1.1.1.cmml" xref="S3.SS4.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS4.p3.8.m8.1.1.2.cmml" xref="S3.SS4.p3.8.m8.1.1.2">𝑇</ci><ci id="S3.SS4.p3.8.m8.1.1.3a.cmml" xref="S3.SS4.p3.8.m8.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.8.m8.1.1.3.cmml" xref="S3.SS4.p3.8.m8.1.1.3">1f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.8.m8.1c">T_{\text{1f}}</annotation></semantics></math> or <math id="S3.SS4.p3.9.m9.1" class="ltx_Math" alttext="T^{k}_{\text{16f}}" display="inline"><semantics id="S3.SS4.p3.9.m9.1a"><msubsup id="S3.SS4.p3.9.m9.1.1" xref="S3.SS4.p3.9.m9.1.1.cmml"><mi id="S3.SS4.p3.9.m9.1.1.2.2" xref="S3.SS4.p3.9.m9.1.1.2.2.cmml">T</mi><mtext id="S3.SS4.p3.9.m9.1.1.3" xref="S3.SS4.p3.9.m9.1.1.3a.cmml">16f</mtext><mi id="S3.SS4.p3.9.m9.1.1.2.3" xref="S3.SS4.p3.9.m9.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.9.m9.1b"><apply id="S3.SS4.p3.9.m9.1.1.cmml" xref="S3.SS4.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.9.m9.1.1.1.cmml" xref="S3.SS4.p3.9.m9.1.1">subscript</csymbol><apply id="S3.SS4.p3.9.m9.1.1.2.cmml" xref="S3.SS4.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.9.m9.1.1.2.1.cmml" xref="S3.SS4.p3.9.m9.1.1">superscript</csymbol><ci id="S3.SS4.p3.9.m9.1.1.2.2.cmml" xref="S3.SS4.p3.9.m9.1.1.2.2">𝑇</ci><ci id="S3.SS4.p3.9.m9.1.1.2.3.cmml" xref="S3.SS4.p3.9.m9.1.1.2.3">𝑘</ci></apply><ci id="S3.SS4.p3.9.m9.1.1.3a.cmml" xref="S3.SS4.p3.9.m9.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.9.m9.1.1.3.cmml" xref="S3.SS4.p3.9.m9.1.1.3">16f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.9.m9.1c">T^{k}_{\text{16f}}</annotation></semantics></math> is labelled as dynamic, we classify the object as dynamic. While this might lead to more objects falsely classified as dynamic, it prevents harmful extrapolation of vehicles that are only temporarily stationary.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Multi-frame Static Object Refinement</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">We anticipate that a parked car’s position in the real world should remain constant. However, we have observed that attempts to assign a single bounding box to a park car for an entire sequence often leads to the exclusion of some car points in certain frames. To investigate this, we accumulated points and ground truth boxes from all frames in a sequence and noticed many parked cars had poor registration across frames. Moreover, the bounding box positions were inconsistent. <a href="#S3.F5" title="In III-E Multi-frame Static Object Refinement ‣ III Multi-Source 3D ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows that annotators assign multiple boxes to a parked car over 197 frames due to poor registration, potentially caused by ego-pose drift. Selecting any ground truth box for the car may lead to the partial or complete exclusion of object points. This highlights the fact that relying on a single bounding box to label a parked vehicle throughout a sequence can be unsatisfactory.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.9" class="ltx_p">This poor registration of parked vehicles was observed in multiple datasets such as nuScenes, Waymo, Lyft and KITTI with some datasets worse than others. To address this, we propose to use KBF on <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mi id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">H</annotation></semantics></math> historical observation frames, <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="\kappa(\{T_{\text{16f}}\}^{k}_{k-H})" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mrow id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml"><mi id="S3.SS5.p2.2.m2.1.1.3" xref="S3.SS5.p2.2.m2.1.1.3.cmml">κ</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.2" xref="S3.SS5.p2.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS5.p2.2.m2.1.1.1.1" xref="S3.SS5.p2.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS5.p2.2.m2.1.1.1.1.2" xref="S3.SS5.p2.2.m2.1.1.1.1.1.cmml">(</mo><msubsup id="S3.SS5.p2.2.m2.1.1.1.1.1" xref="S3.SS5.p2.2.m2.1.1.1.1.1.cmml"><mrow id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.2" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.2" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.2.cmml">T</mi><mtext id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.3" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.3a.cmml">16f</mtext></msub><mo stretchy="false" id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.3" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS5.p2.2.m2.1.1.1.1.1.3" xref="S3.SS5.p2.2.m2.1.1.1.1.1.3.cmml"><mi id="S3.SS5.p2.2.m2.1.1.1.1.1.3.2" xref="S3.SS5.p2.2.m2.1.1.1.1.1.3.2.cmml">k</mi><mo id="S3.SS5.p2.2.m2.1.1.1.1.1.3.1" xref="S3.SS5.p2.2.m2.1.1.1.1.1.3.1.cmml">−</mo><mi id="S3.SS5.p2.2.m2.1.1.1.1.1.3.3" xref="S3.SS5.p2.2.m2.1.1.1.1.1.3.3.cmml">H</mi></mrow><mi id="S3.SS5.p2.2.m2.1.1.1.1.1.1.3" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.3.cmml">k</mi></msubsup><mo stretchy="false" id="S3.SS5.p2.2.m2.1.1.1.1.3" xref="S3.SS5.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><apply id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1"><times id="S3.SS5.p2.2.m2.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2"></times><ci id="S3.SS5.p2.2.m2.1.1.3.cmml" xref="S3.SS5.p2.2.m2.1.1.3">𝜅</ci><apply id="S3.SS5.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1">subscript</csymbol><apply id="S3.SS5.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1">superscript</csymbol><set id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1"><apply id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.2">𝑇</ci><ci id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.1.1.1.3">16f</mtext></ci></apply></set><ci id="S3.SS5.p2.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1.1.1.3">𝑘</ci></apply><apply id="S3.SS5.p2.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1.1.3"><minus id="S3.SS5.p2.2.m2.1.1.1.1.1.3.1.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1.1.3.1"></minus><ci id="S3.SS5.p2.2.m2.1.1.1.1.1.3.2.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1.1.3.2">𝑘</ci><ci id="S3.SS5.p2.2.m2.1.1.1.1.1.3.3.cmml" xref="S3.SS5.p2.2.m2.1.1.1.1.1.3.3">𝐻</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">\kappa(\{T_{\text{16f}}\}^{k}_{k-H})</annotation></semantics></math>, to obtain a static box for the <math id="S3.SS5.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS5.p2.3.m3.1a"><mi id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><ci id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">k</annotation></semantics></math>-th frame instead of using all frames. This gives us a static box <math id="S3.SS5.p2.4.m4.1" class="ltx_Math" alttext="\{B_{\text{static}}^{k}\}^{N}_{k}" display="inline"><semantics id="S3.SS5.p2.4.m4.1a"><msubsup id="S3.SS5.p2.4.m4.1.1" xref="S3.SS5.p2.4.m4.1.1.cmml"><mrow id="S3.SS5.p2.4.m4.1.1.1.1.1" xref="S3.SS5.p2.4.m4.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS5.p2.4.m4.1.1.1.1.1.2" xref="S3.SS5.p2.4.m4.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS5.p2.4.m4.1.1.1.1.1.1" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.2" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.2.cmml">B</mi><mtext id="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.3" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.3a.cmml">static</mtext><mi id="S3.SS5.p2.4.m4.1.1.1.1.1.1.3" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1.3.cmml">k</mi></msubsup><mo stretchy="false" id="S3.SS5.p2.4.m4.1.1.1.1.1.3" xref="S3.SS5.p2.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mi id="S3.SS5.p2.4.m4.1.1.3" xref="S3.SS5.p2.4.m4.1.1.3.cmml">k</mi><mi id="S3.SS5.p2.4.m4.1.1.1.3" xref="S3.SS5.p2.4.m4.1.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.4.m4.1b"><apply id="S3.SS5.p2.4.m4.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.4.m4.1.1.2.cmml" xref="S3.SS5.p2.4.m4.1.1">subscript</csymbol><apply id="S3.SS5.p2.4.m4.1.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.4.m4.1.1.1.2.cmml" xref="S3.SS5.p2.4.m4.1.1">superscript</csymbol><set id="S3.SS5.p2.4.m4.1.1.1.1.2.cmml" xref="S3.SS5.p2.4.m4.1.1.1.1.1"><apply id="S3.SS5.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.1.cmml" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.2">𝐵</ci><ci id="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.3a.cmml" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.3.cmml" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1.2.3">static</mtext></ci></apply><ci id="S3.SS5.p2.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS5.p2.4.m4.1.1.1.1.1.1.3">𝑘</ci></apply></set><ci id="S3.SS5.p2.4.m4.1.1.1.3.cmml" xref="S3.SS5.p2.4.m4.1.1.1.3">𝑁</ci></apply><ci id="S3.SS5.p2.4.m4.1.1.3.cmml" xref="S3.SS5.p2.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.4.m4.1c">\{B_{\text{static}}^{k}\}^{N}_{k}</annotation></semantics></math> for each <math id="S3.SS5.p2.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS5.p2.5.m5.1a"><mi id="S3.SS5.p2.5.m5.1.1" xref="S3.SS5.p2.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.5.m5.1b"><ci id="S3.SS5.p2.5.m5.1.1.cmml" xref="S3.SS5.p2.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.5.m5.1c">k</annotation></semantics></math>-th frame with a score determined by <math id="S3.SS5.p2.6.m6.1" class="ltx_math_unparsed" alttext="\text{max}(\alpha,s_{\text{static}}^{k}))" display="inline"><semantics id="S3.SS5.p2.6.m6.1a"><mrow id="S3.SS5.p2.6.m6.1b"><mtext id="S3.SS5.p2.6.m6.1.2">max</mtext><mrow id="S3.SS5.p2.6.m6.1.3"><mo stretchy="false" id="S3.SS5.p2.6.m6.1.3.1">(</mo><mi id="S3.SS5.p2.6.m6.1.1">α</mi><mo id="S3.SS5.p2.6.m6.1.3.2">,</mo><msubsup id="S3.SS5.p2.6.m6.1.3.3"><mi id="S3.SS5.p2.6.m6.1.3.3.2.2">s</mi><mtext id="S3.SS5.p2.6.m6.1.3.3.2.3">static</mtext><mi id="S3.SS5.p2.6.m6.1.3.3.3">k</mi></msubsup><mo stretchy="false" id="S3.SS5.p2.6.m6.1.3.4">)</mo></mrow><mo stretchy="false" id="S3.SS5.p2.6.m6.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS5.p2.6.m6.1c">\text{max}(\alpha,s_{\text{static}}^{k}))</annotation></semantics></math> where <math id="S3.SS5.p2.7.m7.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS5.p2.7.m7.1a"><mi id="S3.SS5.p2.7.m7.1.1" xref="S3.SS5.p2.7.m7.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.7.m7.1b"><ci id="S3.SS5.p2.7.m7.1.1.cmml" xref="S3.SS5.p2.7.m7.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.7.m7.1c">\alpha</annotation></semantics></math> is a minimum score threshold and <math id="S3.SS5.p2.8.m8.1" class="ltx_Math" alttext="s_{\text{static}}^{k}" display="inline"><semantics id="S3.SS5.p2.8.m8.1a"><msubsup id="S3.SS5.p2.8.m8.1.1" xref="S3.SS5.p2.8.m8.1.1.cmml"><mi id="S3.SS5.p2.8.m8.1.1.2.2" xref="S3.SS5.p2.8.m8.1.1.2.2.cmml">s</mi><mtext id="S3.SS5.p2.8.m8.1.1.2.3" xref="S3.SS5.p2.8.m8.1.1.2.3a.cmml">static</mtext><mi id="S3.SS5.p2.8.m8.1.1.3" xref="S3.SS5.p2.8.m8.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.8.m8.1b"><apply id="S3.SS5.p2.8.m8.1.1.cmml" xref="S3.SS5.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.8.m8.1.1.1.cmml" xref="S3.SS5.p2.8.m8.1.1">superscript</csymbol><apply id="S3.SS5.p2.8.m8.1.1.2.cmml" xref="S3.SS5.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.8.m8.1.1.2.1.cmml" xref="S3.SS5.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS5.p2.8.m8.1.1.2.2.cmml" xref="S3.SS5.p2.8.m8.1.1.2.2">𝑠</ci><ci id="S3.SS5.p2.8.m8.1.1.2.3a.cmml" xref="S3.SS5.p2.8.m8.1.1.2.3"><mtext mathsize="70%" id="S3.SS5.p2.8.m8.1.1.2.3.cmml" xref="S3.SS5.p2.8.m8.1.1.2.3">static</mtext></ci></apply><ci id="S3.SS5.p2.8.m8.1.1.3.cmml" xref="S3.SS5.p2.8.m8.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.8.m8.1c">s_{\text{static}}^{k}</annotation></semantics></math> is the fused score of <math id="S3.SS5.p2.9.m9.1" class="ltx_Math" alttext="\{B_{\text{static}}^{k}\}^{N}_{k}" display="inline"><semantics id="S3.SS5.p2.9.m9.1a"><msubsup id="S3.SS5.p2.9.m9.1.1" xref="S3.SS5.p2.9.m9.1.1.cmml"><mrow id="S3.SS5.p2.9.m9.1.1.1.1.1" xref="S3.SS5.p2.9.m9.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS5.p2.9.m9.1.1.1.1.1.2" xref="S3.SS5.p2.9.m9.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS5.p2.9.m9.1.1.1.1.1.1" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1.cmml"><mi id="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.2" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.2.cmml">B</mi><mtext id="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.3" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.3a.cmml">static</mtext><mi id="S3.SS5.p2.9.m9.1.1.1.1.1.1.3" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1.3.cmml">k</mi></msubsup><mo stretchy="false" id="S3.SS5.p2.9.m9.1.1.1.1.1.3" xref="S3.SS5.p2.9.m9.1.1.1.1.2.cmml">}</mo></mrow><mi id="S3.SS5.p2.9.m9.1.1.3" xref="S3.SS5.p2.9.m9.1.1.3.cmml">k</mi><mi id="S3.SS5.p2.9.m9.1.1.1.3" xref="S3.SS5.p2.9.m9.1.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.9.m9.1b"><apply id="S3.SS5.p2.9.m9.1.1.cmml" xref="S3.SS5.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.9.m9.1.1.2.cmml" xref="S3.SS5.p2.9.m9.1.1">subscript</csymbol><apply id="S3.SS5.p2.9.m9.1.1.1.cmml" xref="S3.SS5.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.9.m9.1.1.1.2.cmml" xref="S3.SS5.p2.9.m9.1.1">superscript</csymbol><set id="S3.SS5.p2.9.m9.1.1.1.1.2.cmml" xref="S3.SS5.p2.9.m9.1.1.1.1.1"><apply id="S3.SS5.p2.9.m9.1.1.1.1.1.1.cmml" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.9.m9.1.1.1.1.1.1.1.cmml" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.cmml" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.1.cmml" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.2.cmml" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.2">𝐵</ci><ci id="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.3a.cmml" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.3.cmml" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1.2.3">static</mtext></ci></apply><ci id="S3.SS5.p2.9.m9.1.1.1.1.1.1.3.cmml" xref="S3.SS5.p2.9.m9.1.1.1.1.1.1.3">𝑘</ci></apply></set><ci id="S3.SS5.p2.9.m9.1.1.1.3.cmml" xref="S3.SS5.p2.9.m9.1.1.1.3">𝑁</ci></apply><ci id="S3.SS5.p2.9.m9.1.1.3.cmml" xref="S3.SS5.p2.9.m9.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.9.m9.1c">\{B_{\text{static}}^{k}\}^{N}_{k}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">To extend the range of pseudo-labels, we assume that static vehicles with sufficiently long tracks will remain static throughout the sequence. Many source-trained detectors can only reliably identify vehicles within a closer range for a new target domain. Therefore, we propagate the first and last KBF boxes of static vehicles through historical and future frames, respectively, under the assumption that they remain static. To reduce the increase in false positives resulting from this propagation, we decay the confidence score of the propagated boxes by <math id="S3.SS5.p3.1.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS5.p3.1.m1.1a"><mi id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><ci id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">\beta</annotation></semantics></math> for each frame that it is propagated. For certain datasets, we found that updating the pseudo-labels in the self-training process to refine these propagated boxes can give a bump in performance.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2304.02431/assets/images/ego_pose_drift_singlecar_2x.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="574" height="296" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.7.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.8.2" class="ltx_text" style="font-size:90%;">Accumulated frames of a parked car and its assigned ground-truth boxes from all frames of a Waymo sequence (197 frames/boxes total). <span id="S3.F5.8.2.1" class="ltx_text ltx_font_bold" style="color:#80FF00;">Light green</span>, <span id="S3.F5.8.2.2" class="ltx_text ltx_font_bold" style="color:#009900;">dark green</span>, <span id="S3.F5.8.2.3" class="ltx_text ltx_font_bold" style="color:#FF85FF;">pink</span>, <span id="S3.F5.8.2.4" class="ltx_text ltx_font_bold" style="color:#FF0000;">red</span> and <span id="S3.F5.8.2.5" class="ltx_text ltx_font_bold" style="color:#FF6321;">orange</span> represent accumulated points in intervals of 40 frames in order where light green and orange represents frames 0-40 and 160-197 respectively. The bottom left annotation illustrates the registration misalignment, especially between light green (1) and red (4) outlines on the car’s rear, potentially caused by ego-pose drift. Best viewed in colour.</span></figcaption>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.5.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.6.2" class="ltx_text ltx_font_italic">Pseudo-Label Refinement</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.3" class="ltx_p">In the UDA setting, higher point density with 16-frame accumulation does not necessarily lead to improved detection at close ranges compared to 1-frame detections. This is partly because densified point clouds may accentuate differences in scan patterns, and also because static boxes can be susceptible to ego-pose drift, which negatively affects localization accuracy. To address this issue and combine all box proposals, we employ Non-Maximum Suppression (NMS) to allow confident single frame detections to replace 16-frame static boxes. We project the following into the scene and apply NMS for filtering: (1) 1-Frame KBF boxes; (2) <math id="S3.SS6.p1.1.m1.1" class="ltx_Math" alttext="T_{\text{1f}}" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><msub id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml"><mi id="S3.SS6.p1.1.m1.1.1.2" xref="S3.SS6.p1.1.m1.1.1.2.cmml">T</mi><mtext id="S3.SS6.p1.1.m1.1.1.3" xref="S3.SS6.p1.1.m1.1.1.3a.cmml">1f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><apply id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.1.m1.1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS6.p1.1.m1.1.1.2.cmml" xref="S3.SS6.p1.1.m1.1.1.2">𝑇</ci><ci id="S3.SS6.p1.1.m1.1.1.3a.cmml" xref="S3.SS6.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS6.p1.1.m1.1.1.3.cmml" xref="S3.SS6.p1.1.m1.1.1.3">1f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">T_{\text{1f}}</annotation></semantics></math> tracked boxes; and (3) Static object boxes, <math id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="\{B_{\text{static}}^{k}\}^{N}_{k}" display="inline"><semantics id="S3.SS6.p1.2.m2.1a"><msubsup id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml"><mrow id="S3.SS6.p1.2.m2.1.1.1.1.1" xref="S3.SS6.p1.2.m2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS6.p1.2.m2.1.1.1.1.1.2" xref="S3.SS6.p1.2.m2.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS6.p1.2.m2.1.1.1.1.1.1" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.2" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.2.cmml">B</mi><mtext id="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.3" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.3a.cmml">static</mtext><mi id="S3.SS6.p1.2.m2.1.1.1.1.1.1.3" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1.3.cmml">k</mi></msubsup><mo stretchy="false" id="S3.SS6.p1.2.m2.1.1.1.1.1.3" xref="S3.SS6.p1.2.m2.1.1.1.1.2.cmml">}</mo></mrow><mi id="S3.SS6.p1.2.m2.1.1.3" xref="S3.SS6.p1.2.m2.1.1.3.cmml">k</mi><mi id="S3.SS6.p1.2.m2.1.1.1.3" xref="S3.SS6.p1.2.m2.1.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><apply id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.2.m2.1.1.2.cmml" xref="S3.SS6.p1.2.m2.1.1">subscript</csymbol><apply id="S3.SS6.p1.2.m2.1.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.2.m2.1.1.1.2.cmml" xref="S3.SS6.p1.2.m2.1.1">superscript</csymbol><set id="S3.SS6.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS6.p1.2.m2.1.1.1.1.1"><apply id="S3.SS6.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.1.cmml" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.2.cmml" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.2">𝐵</ci><ci id="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.3a.cmml" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.3.cmml" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1.2.3">static</mtext></ci></apply><ci id="S3.SS6.p1.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS6.p1.2.m2.1.1.1.1.1.1.3">𝑘</ci></apply></set><ci id="S3.SS6.p1.2.m2.1.1.1.3.cmml" xref="S3.SS6.p1.2.m2.1.1.1.3">𝑁</ci></apply><ci id="S3.SS6.p1.2.m2.1.1.3.cmml" xref="S3.SS6.p1.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">\{B_{\text{static}}^{k}\}^{N}_{k}</annotation></semantics></math>. Finally, we remove any pseudo-labels with less than 1 point inside their box. By including <math id="S3.SS6.p1.3.m3.1" class="ltx_Math" alttext="T_{\text{1f}}" display="inline"><semantics id="S3.SS6.p1.3.m3.1a"><msub id="S3.SS6.p1.3.m3.1.1" xref="S3.SS6.p1.3.m3.1.1.cmml"><mi id="S3.SS6.p1.3.m3.1.1.2" xref="S3.SS6.p1.3.m3.1.1.2.cmml">T</mi><mtext id="S3.SS6.p1.3.m3.1.1.3" xref="S3.SS6.p1.3.m3.1.1.3a.cmml">1f</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.3.m3.1b"><apply id="S3.SS6.p1.3.m3.1.1.cmml" xref="S3.SS6.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.1.1.1.cmml" xref="S3.SS6.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS6.p1.3.m3.1.1.2.cmml" xref="S3.SS6.p1.3.m3.1.1.2">𝑇</ci><ci id="S3.SS6.p1.3.m3.1.1.3a.cmml" xref="S3.SS6.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS6.p1.3.m3.1.1.3.cmml" xref="S3.SS6.p1.3.m3.1.1.3">1f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.3.m3.1c">T_{\text{1f}}</annotation></semantics></math> tracked boxes, we can interpolate missing detections and extrapolate for farther ranges in a similar manner to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Setup</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Datasets.</span> Many works have addressed the object size domain gap and improved adaptation to the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. However, other factors such as scan pattern, weather and geographical differences have a significant impact on the domain gap. To focus on this, we select datasets with similar object sizes: Waymo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, Lyft<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and nuScenes<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, summarised in <a href="#S4.T1" title="In IV-A Setup ‣ IV Experiments ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a>, for our experiments.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.7.8.1" class="ltx_tr">
<th id="S4.T1.7.8.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Dataset</th>
<th id="S4.T1.7.8.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Lidar Beams</th>
<th id="S4.T1.7.8.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Size</th>
<th id="S4.T1.7.8.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Rain</th>
<th id="S4.T1.7.8.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Night</th>
<th id="S4.T1.7.8.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Country</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">nuScenes</th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1<math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><times id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\times</annotation></semantics></math>32</th>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">34,149</td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Yes</td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Yes</td>
<td id="S4.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">USA, Singapore</td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<th id="S4.T1.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">Waymo</th>
<th id="S4.T1.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">1<math id="S4.T1.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.2.2.1.m1.1a"><mo id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><times id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">\times</annotation></semantics></math>64 <math id="S4.T1.3.3.2.m2.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S4.T1.3.3.2.m2.1a"><mo id="S4.T1.3.3.2.m2.1.1" xref="S4.T1.3.3.2.m2.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.2.m2.1b"><plus id="S4.T1.3.3.2.m2.1.1.cmml" xref="S4.T1.3.3.2.m2.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.2.m2.1c">+</annotation></semantics></math> 4<math id="S4.T1.4.4.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.4.4.3.m3.1a"><mo id="S4.T1.4.4.3.m3.1.1" xref="S4.T1.4.4.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.3.m3.1b"><times id="S4.T1.4.4.3.m3.1.1.cmml" xref="S4.T1.4.4.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.3.m3.1c">\times</annotation></semantics></math>200</th>
<td id="S4.T1.4.4.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">192,484</td>
<td id="S4.T1.4.4.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Yes</td>
<td id="S4.T1.4.4.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Yes</td>
<td id="S4.T1.4.4.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">USA</td>
</tr>
<tr id="S4.T1.7.7" class="ltx_tr">
<th id="S4.T1.7.7.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding-left:2.0pt;padding-right:2.0pt;">Lyft</th>
<th id="S4.T1.7.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding-left:2.0pt;padding-right:2.0pt;">1<math id="S4.T1.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.5.5.1.m1.1a"><mo id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><times id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">\times</annotation></semantics></math>40 or 64 <math id="S4.T1.6.6.2.m2.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S4.T1.6.6.2.m2.1a"><mo id="S4.T1.6.6.2.m2.1.1" xref="S4.T1.6.6.2.m2.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.2.m2.1b"><plus id="S4.T1.6.6.2.m2.1.1.cmml" xref="S4.T1.6.6.2.m2.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.2.m2.1c">+</annotation></semantics></math> 2<math id="S4.T1.7.7.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.7.7.3.m3.1a"><mo id="S4.T1.7.7.3.m3.1.1" xref="S4.T1.7.7.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.3.m3.1b"><times id="S4.T1.7.7.3.m3.1.1.cmml" xref="S4.T1.7.7.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.3.m3.1c">\times</annotation></semantics></math>40</th>
<td id="S4.T1.7.7.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.0pt;padding-right:2.0pt;">18,634</td>
<td id="S4.T1.7.7.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.0pt;padding-right:2.0pt;">No</td>
<td id="S4.T1.7.7.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.0pt;padding-right:2.0pt;">No</td>
<td id="S4.T1.7.7.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.0pt;padding-right:2.0pt;">USA</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.9.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S4.T1.10.2" class="ltx_text" style="font-size:90%;">Overview of each dataset. Country refers to the country in which the dataset was collected.</span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Methods.</span> We compare MS3D with the following methods: (1) Source-only, which is the direct evaluation of a source-trained detector on the target dataset; (2) SN as a baseline to observe how much of the domain gap is caused by size differences; (3) ST3D, the state-of-the-art self-training approach for 3D object detection; (4) Lidar Distillation, an approach that builds on ST3D for the Waymo to nuScenes setting; (5) GT Fine-tune, the best performance we can achieve when we fine-tune the source-trained detector on the target ground-truth labels. We compare methods (1)-(4) with GT fine-tune as it is a more realistic scenario of what is achievable in the presence of perfect pseudo-labels, as opposed to careful selection of the voxel sizes and anchor box dimensions for an “Oracle” supervised benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We fine-tune all models with no frame accumulation. We only compare Lidar Distillation for the target-nuscenes setting as it is designed specifically for the high beam to low beam UDA setting. Though we use SECOND-IoU and CenterPoint for generating pseudo-labels, we only focus on the validation results of SECOND-IoU from different source domains to highlight domain discrepancies.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.2" class="ltx_p"><span id="S4.SS1.p3.2.1" class="ltx_text ltx_font_bold">Evaluation Metric.</span> We follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and adopt the KITTI evaluation metric of Average Precision (AP) over 40 recall positions, in Bird’s Eye View (BEV) and 3D, denoted as <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{BEV}}" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><msub id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mtext id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3a.cmml">BEV</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.2a.cmml" xref="S4.SS1.p3.1.m1.1.1.2"><mtext id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">AP</mtext></ci><ci id="S4.SS1.p3.1.m1.1.1.3a.cmml" xref="S4.SS1.p3.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">BEV</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\text{AP}_{\text{BEV}}</annotation></semantics></math>/<math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><msub id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mtext id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2a.cmml">AP</mtext><mtext id="S4.SS1.p3.2.m2.1.1.3" xref="S4.SS1.p3.2.m2.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p3.2.m2.1.1.2a.cmml" xref="S4.SS1.p3.2.m2.1.1.2"><mtext id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">AP</mtext></ci><ci id="S4.SS1.p3.2.m2.1.1.3a.cmml" xref="S4.SS1.p3.2.m2.1.1.3"><mtext mathsize="70%" id="S4.SS1.p3.2.m2.1.1.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\text{AP}_{\text{3D}}</annotation></semantics></math>. We assess all models on the “Vehicle” category. For Lyft and nuScenes, we map “truck” and “bus” to the “car” category. We report the AP at Intersection over Union (IoU) thresholds of 0.7 and 0.5 i.e. a vehicle is considered a true positive detection if the IoU of the predicted and ground truth box is over 0.7 or 0.5. We present results at IoU=0.5 as well to highlight that our method greatly improves the number of correctly detected vehicles.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.6.1.1" class="ltx_tr">
<th id="S4.T2.6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding:0.5pt 2.0pt;" colspan="5">Target: nuScenes</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.6.2.1" class="ltx_tr">
<th id="S4.T2.6.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Method</th>
<td id="S4.T2.6.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Source</td>
<td id="S4.T2.6.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Detector</td>
<td id="S4.T2.6.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">IoU=0.7</td>
<td id="S4.T2.6.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">IoU=0.5</td>
</tr>
<tr id="S4.T2.6.3.2" class="ltx_tr">
<th id="S4.T2.6.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;" rowspan="4"><span id="S4.T2.6.3.2.1.1" class="ltx_text">Source-Only</span></th>
<td id="S4.T2.6.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="S4.T2.6.3.2.2.1" class="ltx_text">Waymo</span></td>
<td id="S4.T2.6.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T2.6.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">32.91 / 17.24</td>
<td id="S4.T2.6.3.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">43.32 / 37.58</td>
</tr>
<tr id="S4.T2.6.4.3" class="ltx_tr">
<td id="S4.T2.6.4.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">CenterPoint</td>
<td id="S4.T2.6.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">32.10 / 17.77</td>
<td id="S4.T2.6.4.3.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">40.83 / 35.77</td>
</tr>
<tr id="S4.T2.6.5.4" class="ltx_tr">
<td id="S4.T2.6.5.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="S4.T2.6.5.4.1.1" class="ltx_text">Lyft</span></td>
<td id="S4.T2.6.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T2.6.5.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">24.15 / 13.47</td>
<td id="S4.T2.6.5.4.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">30.52 / 26.79</td>
</tr>
<tr id="S4.T2.6.6.5" class="ltx_tr">
<td id="S4.T2.6.6.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">CenterPoint</td>
<td id="S4.T2.6.6.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">23.71 / 11.31</td>
<td id="S4.T2.6.6.5.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">30.45 / 26.17</td>
</tr>
<tr id="S4.T2.6.7.6" class="ltx_tr">
<th id="S4.T2.6.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="S4.T2.6.7.6.1.1" class="ltx_text">SN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></span></th>
<td id="S4.T2.6.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Waymo</td>
<td id="S4.T2.6.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T2.6.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">33.23 / 18.57</td>
<td id="S4.T2.6.7.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">43.19 / 37.74</td>
</tr>
<tr id="S4.T2.6.8.7" class="ltx_tr">
<td id="S4.T2.6.8.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">Lyft</td>
<td id="S4.T2.6.8.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T2.6.8.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">27.51 / 17.00</td>
<td id="S4.T2.6.8.7.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">33.32 / 29.92</td>
</tr>
<tr id="S4.T2.6.9.8" class="ltx_tr">
<th id="S4.T2.6.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;" rowspan="2">
<span id="S4.T2.6.9.8.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S4.T2.6.9.8.1.2" class="ltx_text">ST3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span>
</th>
<td id="S4.T2.6.9.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">Waymo</td>
<td id="S4.T2.6.9.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T2.6.9.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">35.92 / 20.19</td>
<td id="S4.T2.6.9.8.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">43.03 / 38.99</td>
</tr>
<tr id="S4.T2.6.10.9" class="ltx_tr">
<td id="S4.T2.6.10.9.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">Lyft</td>
<td id="S4.T2.6.10.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T2.6.10.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">29.88 / 18.37</td>
<td id="S4.T2.6.10.9.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">33.18 / 30.67</td>
</tr>
<tr id="S4.T2.6.11.10" class="ltx_tr">
<th id="S4.T2.6.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">
<span id="S4.T2.6.11.10.1.1" class="ltx_ERROR undefined">\hdashline</span>Lidar Dist. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<td id="S4.T2.6.11.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">Waymo</td>
<td id="S4.T2.6.11.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T2.6.11.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">42.04 / 24.50</td>
<td id="S4.T2.6.11.10.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">- / -</td>
</tr>
<tr id="S4.T2.6.12.11" class="ltx_tr">
<th id="S4.T2.6.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;" rowspan="2">
<span id="S4.T2.6.12.11.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S4.T2.6.12.11.1.2" class="ltx_text">MS3D (Ours)</span>
</th>
<td id="S4.T2.6.12.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">Waymo</td>
<td id="S4.T2.6.12.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T2.6.12.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;"><span id="S4.T2.6.12.11.4.1" class="ltx_text ltx_font_bold">42.23 / 24.76</span></td>
<td id="S4.T2.6.12.11.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;"><span id="S4.T2.6.12.11.5.1" class="ltx_text ltx_font_bold">52.33 / 46.65</span></td>
</tr>
<tr id="S4.T2.6.13.12" class="ltx_tr">
<td id="S4.T2.6.13.12.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">Lyft</td>
<td id="S4.T2.6.13.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T2.6.13.12.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;"><span id="S4.T2.6.13.12.3.1" class="ltx_text ltx_font_bold">41.64 / 23.46</span></td>
<td id="S4.T2.6.13.12.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;"><span id="S4.T2.6.13.12.4.1" class="ltx_text ltx_font_bold">51.47 / 45.74</span></td>
</tr>
<tr id="S4.T2.6.14.13" class="ltx_tr">
<th id="S4.T2.6.14.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">GT Fine-Tune</th>
<td id="S4.T2.6.14.13.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Waymo</td>
<td id="S4.T2.6.14.13.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T2.6.14.13.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">44.39 / 29.46</td>
<td id="S4.T2.6.14.13.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding:0.5pt 2.0pt;">55.61 / 50.83</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.9.3.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S4.T2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">UDA from Waymo/Lyft to nuScenes.<span id="S4.T2.4.2.2" class="ltx_text ltx_font_medium"> We report <math id="S4.T2.3.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{BEV}}" display="inline"><semantics id="S4.T2.3.1.1.m1.1b"><msub id="S4.T2.3.1.1.m1.1.1" xref="S4.T2.3.1.1.m1.1.1.cmml"><mtext id="S4.T2.3.1.1.m1.1.1.2" xref="S4.T2.3.1.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T2.3.1.1.m1.1.1.3" xref="S4.T2.3.1.1.m1.1.1.3a.cmml">BEV</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.m1.1c"><apply id="S4.T2.3.1.1.m1.1.1.cmml" xref="S4.T2.3.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.1.1.m1.1.1.1.cmml" xref="S4.T2.3.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.3.1.1.m1.1.1.2a.cmml" xref="S4.T2.3.1.1.m1.1.1.2"><mtext id="S4.T2.3.1.1.m1.1.1.2.cmml" xref="S4.T2.3.1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.T2.3.1.1.m1.1.1.3a.cmml" xref="S4.T2.3.1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T2.3.1.1.m1.1.1.3.cmml" xref="S4.T2.3.1.1.m1.1.1.3">BEV</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.1.m1.1d">\text{AP}_{\text{BEV}}</annotation></semantics></math>/<math id="S4.T2.4.2.2.m2.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.T2.4.2.2.m2.1b"><msub id="S4.T2.4.2.2.m2.1.1" xref="S4.T2.4.2.2.m2.1.1.cmml"><mtext id="S4.T2.4.2.2.m2.1.1.2" xref="S4.T2.4.2.2.m2.1.1.2a.cmml">AP</mtext><mtext id="S4.T2.4.2.2.m2.1.1.3" xref="S4.T2.4.2.2.m2.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.2.m2.1c"><apply id="S4.T2.4.2.2.m2.1.1.cmml" xref="S4.T2.4.2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T2.4.2.2.m2.1.1.1.cmml" xref="S4.T2.4.2.2.m2.1.1">subscript</csymbol><ci id="S4.T2.4.2.2.m2.1.1.2a.cmml" xref="S4.T2.4.2.2.m2.1.1.2"><mtext id="S4.T2.4.2.2.m2.1.1.2.cmml" xref="S4.T2.4.2.2.m2.1.1.2">AP</mtext></ci><ci id="S4.T2.4.2.2.m2.1.1.3a.cmml" xref="S4.T2.4.2.2.m2.1.1.3"><mtext mathsize="70%" id="S4.T2.4.2.2.m2.1.1.3.cmml" xref="S4.T2.4.2.2.m2.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.2.m2.1d">\text{AP}_{\text{3D}}</annotation></semantics></math>. Best results for each source-target pair are highlighted in </span>bold<span id="S4.T2.4.2.3" class="ltx_text ltx_font_medium">. GT Fine-tune uses GT labels to fine-tune the pre-trained detectors with no frame accumulation.</span></span></figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.6" class="ltx_p"><span id="S4.SS1.p4.6.1" class="ltx_text ltx_font_bold">Implementation Details.</span> We train SECOND-IoU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> on all datasets and select the best pre-trained model based on evaluating with the source dataset labels. We re-use the same source pre-trained detectors for adapting to all settings. We argue that this is more realistic to real-world applications as we may not have sufficient labels to evaluate and tune the best model for adaptation to a new target dataset. All our models were trained with the OpenPCDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> codebase with tweaked settings for voxel size of <math id="S4.SS1.p4.1.m1.1" class="ltx_math_unparsed" alttext="(0.1" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1b"><mo stretchy="false" id="S4.SS1.p4.1.m1.1.1">(</mo><mn id="S4.SS1.p4.1.m1.1.2">0.1</mn></mrow><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">(0.1</annotation></semantics></math>m, <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mn id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><cn type="float" id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">0.1</annotation></semantics></math>m, <math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="0.15" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><mn id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml">0.15</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><cn type="float" id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1">0.15</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">0.15</annotation></semantics></math>m<math id="S4.SS1.p4.4.m4.1" class="ltx_Math" alttext=")" display="inline"><semantics id="S4.SS1.p4.4.m4.1a"><mo stretchy="false" id="S4.SS1.p4.4.m4.1.1" xref="S4.SS1.p4.4.m4.1.1.cmml">)</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m4.1b"><ci id="S4.SS1.p4.4.m4.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1">)</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m4.1c">)</annotation></semantics></math>, detection range of <math id="S4.SS1.p4.5.m5.2" class="ltx_Math" alttext="[-75,75]" display="inline"><semantics id="S4.SS1.p4.5.m5.2a"><mrow id="S4.SS1.p4.5.m5.2.2.1" xref="S4.SS1.p4.5.m5.2.2.2.cmml"><mo stretchy="false" id="S4.SS1.p4.5.m5.2.2.1.2" xref="S4.SS1.p4.5.m5.2.2.2.cmml">[</mo><mrow id="S4.SS1.p4.5.m5.2.2.1.1" xref="S4.SS1.p4.5.m5.2.2.1.1.cmml"><mo id="S4.SS1.p4.5.m5.2.2.1.1a" xref="S4.SS1.p4.5.m5.2.2.1.1.cmml">−</mo><mn id="S4.SS1.p4.5.m5.2.2.1.1.2" xref="S4.SS1.p4.5.m5.2.2.1.1.2.cmml">75</mn></mrow><mo id="S4.SS1.p4.5.m5.2.2.1.3" xref="S4.SS1.p4.5.m5.2.2.2.cmml">,</mo><mn id="S4.SS1.p4.5.m5.1.1" xref="S4.SS1.p4.5.m5.1.1.cmml">75</mn><mo stretchy="false" id="S4.SS1.p4.5.m5.2.2.1.4" xref="S4.SS1.p4.5.m5.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m5.2b"><interval closure="closed" id="S4.SS1.p4.5.m5.2.2.2.cmml" xref="S4.SS1.p4.5.m5.2.2.1"><apply id="S4.SS1.p4.5.m5.2.2.1.1.cmml" xref="S4.SS1.p4.5.m5.2.2.1.1"><minus id="S4.SS1.p4.5.m5.2.2.1.1.1.cmml" xref="S4.SS1.p4.5.m5.2.2.1.1"></minus><cn type="integer" id="S4.SS1.p4.5.m5.2.2.1.1.2.cmml" xref="S4.SS1.p4.5.m5.2.2.1.1.2">75</cn></apply><cn type="integer" id="S4.SS1.p4.5.m5.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1">75</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m5.2c">[-75,75]</annotation></semantics></math>m for x,y axes and <math id="S4.SS1.p4.6.m6.2" class="ltx_Math" alttext="[-2,4]" display="inline"><semantics id="S4.SS1.p4.6.m6.2a"><mrow id="S4.SS1.p4.6.m6.2.2.1" xref="S4.SS1.p4.6.m6.2.2.2.cmml"><mo stretchy="false" id="S4.SS1.p4.6.m6.2.2.1.2" xref="S4.SS1.p4.6.m6.2.2.2.cmml">[</mo><mrow id="S4.SS1.p4.6.m6.2.2.1.1" xref="S4.SS1.p4.6.m6.2.2.1.1.cmml"><mo id="S4.SS1.p4.6.m6.2.2.1.1a" xref="S4.SS1.p4.6.m6.2.2.1.1.cmml">−</mo><mn id="S4.SS1.p4.6.m6.2.2.1.1.2" xref="S4.SS1.p4.6.m6.2.2.1.1.2.cmml">2</mn></mrow><mo id="S4.SS1.p4.6.m6.2.2.1.3" xref="S4.SS1.p4.6.m6.2.2.2.cmml">,</mo><mn id="S4.SS1.p4.6.m6.1.1" xref="S4.SS1.p4.6.m6.1.1.cmml">4</mn><mo stretchy="false" id="S4.SS1.p4.6.m6.2.2.1.4" xref="S4.SS1.p4.6.m6.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.6.m6.2b"><interval closure="closed" id="S4.SS1.p4.6.m6.2.2.2.cmml" xref="S4.SS1.p4.6.m6.2.2.1"><apply id="S4.SS1.p4.6.m6.2.2.1.1.cmml" xref="S4.SS1.p4.6.m6.2.2.1.1"><minus id="S4.SS1.p4.6.m6.2.2.1.1.1.cmml" xref="S4.SS1.p4.6.m6.2.2.1.1"></minus><cn type="integer" id="S4.SS1.p4.6.m6.2.2.1.1.2.cmml" xref="S4.SS1.p4.6.m6.2.2.1.1.2">2</cn></apply><cn type="integer" id="S4.SS1.p4.6.m6.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1">4</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.6.m6.2c">[-2,4]</annotation></semantics></math> for z-axis, and we shift the point cloud frame to the ground plane. Lyft and nuScenes pre-trained models were trained with 5 and 10 frame accumulation respectively, following popular practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.5" class="ltx_p">For MS3D, we adopt the settings of ST3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and fine-tune all pre-trained detectors with no frame accumulation, for 30 epochs at a learning rate of <math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="1.5\times 10^{-3}" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><mrow id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml"><mn id="S4.SS1.p5.1.m1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.2.cmml">1.5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p5.1.m1.1.1.1" xref="S4.SS1.p5.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS1.p5.1.m1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.3.cmml"><mn id="S4.SS1.p5.1.m1.1.1.3.2" xref="S4.SS1.p5.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p5.1.m1.1.1.3.3" xref="S4.SS1.p5.1.m1.1.1.3.3.cmml"><mo id="S4.SS1.p5.1.m1.1.1.3.3a" xref="S4.SS1.p5.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS1.p5.1.m1.1.1.3.3.2" xref="S4.SS1.p5.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><apply id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1"><times id="S4.SS1.p5.1.m1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1.1"></times><cn type="float" id="S4.SS1.p5.1.m1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2">1.5</cn><apply id="S4.SS1.p5.1.m1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.1.1.3.1.cmml" xref="S4.SS1.p5.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p5.1.m1.1.1.3.2.cmml" xref="S4.SS1.p5.1.m1.1.1.3.2">10</cn><apply id="S4.SS1.p5.1.m1.1.1.3.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3.3"><minus id="S4.SS1.p5.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p5.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p5.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.p5.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">1.5\times 10^{-3}</annotation></semantics></math> with the Adam one-cycle scheduler. We use pseudo-labels with score threshold of <math id="S4.SS1.p5.2.m2.1" class="ltx_Math" alttext="s&gt;0.6" display="inline"><semantics id="S4.SS1.p5.2.m2.1a"><mrow id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml"><mi id="S4.SS1.p5.2.m2.1.1.2" xref="S4.SS1.p5.2.m2.1.1.2.cmml">s</mi><mo id="S4.SS1.p5.2.m2.1.1.1" xref="S4.SS1.p5.2.m2.1.1.1.cmml">&gt;</mo><mn id="S4.SS1.p5.2.m2.1.1.3" xref="S4.SS1.p5.2.m2.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b"><apply id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1"><gt id="S4.SS1.p5.2.m2.1.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1.1"></gt><ci id="S4.SS1.p5.2.m2.1.1.2.cmml" xref="S4.SS1.p5.2.m2.1.1.2">𝑠</ci><cn type="float" id="S4.SS1.p5.2.m2.1.1.3.cmml" xref="S4.SS1.p5.2.m2.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">s&gt;0.6</annotation></semantics></math> as the ground-truth labels for fine-tuning. For fusing proposals with KBF, we consider a vehicle as present if there are more than 4 box proposals with their centroids within 2m of each other. We use <math id="S4.SS1.p5.3.m3.1" class="ltx_Math" alttext="H=16" display="inline"><semantics id="S4.SS1.p5.3.m3.1a"><mrow id="S4.SS1.p5.3.m3.1.1" xref="S4.SS1.p5.3.m3.1.1.cmml"><mi id="S4.SS1.p5.3.m3.1.1.2" xref="S4.SS1.p5.3.m3.1.1.2.cmml">H</mi><mo id="S4.SS1.p5.3.m3.1.1.1" xref="S4.SS1.p5.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p5.3.m3.1.1.3" xref="S4.SS1.p5.3.m3.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.3.m3.1b"><apply id="S4.SS1.p5.3.m3.1.1.cmml" xref="S4.SS1.p5.3.m3.1.1"><eq id="S4.SS1.p5.3.m3.1.1.1.cmml" xref="S4.SS1.p5.3.m3.1.1.1"></eq><ci id="S4.SS1.p5.3.m3.1.1.2.cmml" xref="S4.SS1.p5.3.m3.1.1.2">𝐻</ci><cn type="integer" id="S4.SS1.p5.3.m3.1.1.3.cmml" xref="S4.SS1.p5.3.m3.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.3.m3.1c">H=16</annotation></semantics></math> historical frames for static object refinement and propagate them with a decay of <math id="S4.SS1.p5.4.m4.1" class="ltx_Math" alttext="\beta=0.95" display="inline"><semantics id="S4.SS1.p5.4.m4.1a"><mrow id="S4.SS1.p5.4.m4.1.1" xref="S4.SS1.p5.4.m4.1.1.cmml"><mi id="S4.SS1.p5.4.m4.1.1.2" xref="S4.SS1.p5.4.m4.1.1.2.cmml">β</mi><mo id="S4.SS1.p5.4.m4.1.1.1" xref="S4.SS1.p5.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS1.p5.4.m4.1.1.3" xref="S4.SS1.p5.4.m4.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.4.m4.1b"><apply id="S4.SS1.p5.4.m4.1.1.cmml" xref="S4.SS1.p5.4.m4.1.1"><eq id="S4.SS1.p5.4.m4.1.1.1.cmml" xref="S4.SS1.p5.4.m4.1.1.1"></eq><ci id="S4.SS1.p5.4.m4.1.1.2.cmml" xref="S4.SS1.p5.4.m4.1.1.2">𝛽</ci><cn type="float" id="S4.SS1.p5.4.m4.1.1.3.cmml" xref="S4.SS1.p5.4.m4.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.4.m4.1c">\beta=0.95</annotation></semantics></math> with a minimum score <math id="S4.SS1.p5.5.m5.1" class="ltx_Math" alttext="\alpha=0.7" display="inline"><semantics id="S4.SS1.p5.5.m5.1a"><mrow id="S4.SS1.p5.5.m5.1.1" xref="S4.SS1.p5.5.m5.1.1.cmml"><mi id="S4.SS1.p5.5.m5.1.1.2" xref="S4.SS1.p5.5.m5.1.1.2.cmml">α</mi><mo id="S4.SS1.p5.5.m5.1.1.1" xref="S4.SS1.p5.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS1.p5.5.m5.1.1.3" xref="S4.SS1.p5.5.m5.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.5.m5.1b"><apply id="S4.SS1.p5.5.m5.1.1.cmml" xref="S4.SS1.p5.5.m5.1.1"><eq id="S4.SS1.p5.5.m5.1.1.1.cmml" xref="S4.SS1.p5.5.m5.1.1.1"></eq><ci id="S4.SS1.p5.5.m5.1.1.2.cmml" xref="S4.SS1.p5.5.m5.1.1.2">𝛼</ci><cn type="float" id="S4.SS1.p5.5.m5.1.1.3.cmml" xref="S4.SS1.p5.5.m5.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.5.m5.1c">\alpha=0.7</annotation></semantics></math> if they have more than 7 tracked detections. To focus on static vehicles, we sort Waymo and nuScenes scenes by number of cars and select the top 150 and 190 scenes from nuScenes and Waymo respectively for training; scenes with many cars tend to be carparks. For an unlabelled target domain, this can similarly be done by counting the number of fused 1-frame detections per scene, or by intentionally collecting data from scenes with many parked vehicles. Additionally, this suggests that our model can be further improved with a second round of self-training with the remaining scenes which we leave for future work.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Results</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.7" class="ltx_p">As shown in <a href="#S4.T4" title="In IV-B Results ‣ IV Experiments ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tables</span> <span class="ltx_text ltx_ref_tag">IV</span></a>, <a href="#S4.T2" title="Table II ‣ IV-A Setup ‣ IV Experiments ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and <a href="#S4.T3" title="Table III ‣ IV-B Results ‣ IV Experiments ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, our approach outperforms all the other methods regardless of which source domain is used.
With nuscenes as target domain, <a href="#S4.T2" title="In IV-A Setup ‣ IV Experiments ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">II</span></a>, MS3D not only improves detections with accurate box localizations (IoU=0.7) and outperforms existing approaches but also greatly increases the number of correct object proposals (IoU=0.5) by a large margin. It can be seen that whilst SN and ST3D improved box localization with IoU=0.7, they did not greatly increase the number of true positive detections as IoU=0.5 remains relatively unchanged. In contrast, MS3D obtains both a high <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mtext id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2a.cmml" xref="S4.SS2.p1.1.m1.1.1.2"><mtext id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.SS2.p1.1.m1.1.1.3a.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\text{AP}_{\text{3D}}</annotation></semantics></math> in IoU=0.7 and IoU=0.5 criterions. In particular, we highlight that for IoU=0.5, MS3D is only <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\sim 4\text{AP}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml"></mi><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">∼</mo><mrow id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml"><mn id="S4.SS2.p1.2.m2.1.1.3.2" xref="S4.SS2.p1.2.m2.1.1.3.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p1.2.m2.1.1.3.1" xref="S4.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mtext id="S4.SS2.p1.2.m2.1.1.3.3" xref="S4.SS2.p1.2.m2.1.1.3.3a.cmml">AP</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">absent</csymbol><apply id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3"><times id="S4.SS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.3.1"></times><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.2.cmml" xref="S4.SS2.p1.2.m2.1.1.3.2">4</cn><ci id="S4.SS2.p1.2.m2.1.1.3.3a.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3"><mtext id="S4.SS2.p1.2.m2.1.1.3.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3">AP</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\sim 4\text{AP}</annotation></semantics></math> less than the GT Fine-Tune approach, indicating that we can nearly detect all vehicles in the scene. This is also apparent with Waymo as target, <a href="#S4.T3" title="In IV-B Results ‣ IV Experiments ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">III</span></a>, where MS3D obtains a high 42.88 <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><msub id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mtext id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2a.cmml">AP</mtext><mtext id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p1.3.m3.1.1.2a.cmml" xref="S4.SS2.p1.3.m3.1.1.2"><mtext id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">AP</mtext></ci><ci id="S4.SS2.p1.3.m3.1.1.3a.cmml" xref="S4.SS2.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">\text{AP}_{\text{3D}}</annotation></semantics></math> for IoU=0.7 and <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="72.34/69.30" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mn id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">72.34</mn><mo id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">/</mo><mn id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml">69.30</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><divide id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1"></divide><cn type="float" id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">72.34</cn><cn type="float" id="S4.SS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3">69.30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">72.34/69.30</annotation></semantics></math> <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="\text{AP}_{\text{BEV}}" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><msub id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mtext id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2a.cmml">AP</mtext><mtext id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3a.cmml">BEV</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.p1.5.m5.1.1.2a.cmml" xref="S4.SS2.p1.5.m5.1.1.2"><mtext id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">AP</mtext></ci><ci id="S4.SS2.p1.5.m5.1.1.3a.cmml" xref="S4.SS2.p1.5.m5.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3">BEV</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">\text{AP}_{\text{BEV}}</annotation></semantics></math>/<math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><msub id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mtext id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2a.cmml">AP</mtext><mtext id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.p1.6.m6.1.1.2a.cmml" xref="S4.SS2.p1.6.m6.1.1.2"><mtext id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">AP</mtext></ci><ci id="S4.SS2.p1.6.m6.1.1.3a.cmml" xref="S4.SS2.p1.6.m6.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.6.m6.1.1.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">\text{AP}_{\text{3D}}</annotation></semantics></math> for IoU=0.5 which is <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="\sim 3\text{AP}" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><mrow id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml"><mi id="S4.SS2.p1.7.m7.1.1.2" xref="S4.SS2.p1.7.m7.1.1.2.cmml"></mi><mo id="S4.SS2.p1.7.m7.1.1.1" xref="S4.SS2.p1.7.m7.1.1.1.cmml">∼</mo><mrow id="S4.SS2.p1.7.m7.1.1.3" xref="S4.SS2.p1.7.m7.1.1.3.cmml"><mn id="S4.SS2.p1.7.m7.1.1.3.2" xref="S4.SS2.p1.7.m7.1.1.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p1.7.m7.1.1.3.1" xref="S4.SS2.p1.7.m7.1.1.3.1.cmml">​</mo><mtext id="S4.SS2.p1.7.m7.1.1.3.3" xref="S4.SS2.p1.7.m7.1.1.3.3a.cmml">AP</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><apply id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1"><csymbol cd="latexml" id="S4.SS2.p1.7.m7.1.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS2.p1.7.m7.1.1.2.cmml" xref="S4.SS2.p1.7.m7.1.1.2">absent</csymbol><apply id="S4.SS2.p1.7.m7.1.1.3.cmml" xref="S4.SS2.p1.7.m7.1.1.3"><times id="S4.SS2.p1.7.m7.1.1.3.1.cmml" xref="S4.SS2.p1.7.m7.1.1.3.1"></times><cn type="integer" id="S4.SS2.p1.7.m7.1.1.3.2.cmml" xref="S4.SS2.p1.7.m7.1.1.3.2">3</cn><ci id="S4.SS2.p1.7.m7.1.1.3.3a.cmml" xref="S4.SS2.p1.7.m7.1.1.3.3"><mtext id="S4.SS2.p1.7.m7.1.1.3.3.cmml" xref="S4.SS2.p1.7.m7.1.1.3.3">AP</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">\sim 3\text{AP}</annotation></semantics></math> less than GT Fine-Tune. With Lyft as the target domain, <a href="#S4.T4" title="In IV-B Results ‣ IV Experiments ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">IV</span></a>, MS3D also outperforms other methods in both IoU criterions. From the above results, we have demonstrated that MS3D is able to adapt detectors from low (e.g. nuScenes) to high (e.g. Waymo) beam lidar datasets and vice-versa, unlike ST3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and Lidar Distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.6.1.1" class="ltx_tr">
<th id="S4.T3.6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding:0.5pt 2.0pt;" colspan="5">Target: Waymo</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.6.2.1" class="ltx_tr">
<th id="S4.T3.6.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Method</th>
<td id="S4.T3.6.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Source</td>
<td id="S4.T3.6.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Detector</td>
<td id="S4.T3.6.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">IoU=0.7</td>
<td id="S4.T3.6.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">IoU=0.5</td>
</tr>
<tr id="S4.T3.6.3.2" class="ltx_tr">
<th id="S4.T3.6.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;" rowspan="4"><span id="S4.T3.6.3.2.1.1" class="ltx_text">Source-Only</span></th>
<td id="S4.T3.6.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="S4.T3.6.3.2.2.1" class="ltx_text">Lyft</span></td>
<td id="S4.T3.6.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T3.6.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">52.14 / 34.40</td>
<td id="S4.T3.6.3.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">61.66 / 58.71</td>
</tr>
<tr id="S4.T3.6.4.3" class="ltx_tr">
<td id="S4.T3.6.4.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">CenterPoint</td>
<td id="S4.T3.6.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">50.56 / 31.02</td>
<td id="S4.T3.6.4.3.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">62.32 / 58.97</td>
</tr>
<tr id="S4.T3.6.5.4" class="ltx_tr">
<td id="S4.T3.6.5.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="S4.T3.6.5.4.1.1" class="ltx_text">nuScenes</span></td>
<td id="S4.T3.6.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T3.6.5.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">50.30 / 24.70</td>
<td id="S4.T3.6.5.4.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">61.40 / 53.73</td>
</tr>
<tr id="S4.T3.6.6.5" class="ltx_tr">
<td id="S4.T3.6.6.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">CenterPoint</td>
<td id="S4.T3.6.6.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">49.39 / 24.33</td>
<td id="S4.T3.6.6.5.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">59.52 / 52.64</td>
</tr>
<tr id="S4.T3.6.7.6" class="ltx_tr">
<th id="S4.T3.6.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="S4.T3.6.7.6.1.1" class="ltx_text">SN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></span></th>
<td id="S4.T3.6.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Lyft</td>
<td id="S4.T3.6.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T3.6.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">53.39 / 39.22</td>
<td id="S4.T3.6.7.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">62.42 / 59.55</td>
</tr>
<tr id="S4.T3.6.8.7" class="ltx_tr">
<td id="S4.T3.6.8.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">nuScenes</td>
<td id="S4.T3.6.8.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T3.6.8.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">50.69 / 28.86</td>
<td id="S4.T3.6.8.7.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">61.38 / 53.90</td>
</tr>
<tr id="S4.T3.6.9.8" class="ltx_tr">
<th id="S4.T3.6.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;" rowspan="2">
<span id="S4.T3.6.9.8.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S4.T3.6.9.8.1.2" class="ltx_text">ST3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span>
</th>
<td id="S4.T3.6.9.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">Lyft</td>
<td id="S4.T3.6.9.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T3.6.9.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">56.06 / 39.17</td>
<td id="S4.T3.6.9.8.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">65.16 / 62.39</td>
</tr>
<tr id="S4.T3.6.10.9" class="ltx_tr">
<td id="S4.T3.6.10.9.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">nuScenes</td>
<td id="S4.T3.6.10.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T3.6.10.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">55.67 / 28.83</td>
<td id="S4.T3.6.10.9.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">67.19 / 61.63</td>
</tr>
<tr id="S4.T3.6.11.10" class="ltx_tr">
<th id="S4.T3.6.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;" rowspan="2">
<span id="S4.T3.6.11.10.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S4.T3.6.11.10.1.2" class="ltx_text">MS3D (Ours)</span>
</th>
<td id="S4.T3.6.11.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">Lyft</td>
<td id="S4.T3.6.11.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T3.6.11.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;"><span id="S4.T3.6.11.10.4.1" class="ltx_text ltx_font_bold">61.25 / 42.88</span></td>
<td id="S4.T3.6.11.10.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;"><span id="S4.T3.6.11.10.5.1" class="ltx_text ltx_font_bold">72.34 / 69.30</span></td>
</tr>
<tr id="S4.T3.6.12.11" class="ltx_tr">
<td id="S4.T3.6.12.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">nuScenes</td>
<td id="S4.T3.6.12.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T3.6.12.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;"><span id="S4.T3.6.12.11.3.1" class="ltx_text ltx_font_bold">61.39 / 42.76</span></td>
<td id="S4.T3.6.12.11.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;"><span id="S4.T3.6.12.11.4.1" class="ltx_text ltx_font_bold">72.47 / 69.45</span></td>
</tr>
<tr id="S4.T3.6.13.12" class="ltx_tr">
<th id="S4.T3.6.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">GT Fine-Tune</th>
<td id="S4.T3.6.13.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">nuScenes</td>
<td id="S4.T3.6.13.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T3.6.13.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">66.76 / 52.50</td>
<td id="S4.T3.6.13.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding:0.5pt 2.0pt;">75.16 / 72.40</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.8.3.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S4.T3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">UDA from nuScenes/Lyft to Waymo.<span id="S4.T3.4.2.2" class="ltx_text ltx_font_medium"> We report <math id="S4.T3.3.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{BEV}}" display="inline"><semantics id="S4.T3.3.1.1.m1.1b"><msub id="S4.T3.3.1.1.m1.1.1" xref="S4.T3.3.1.1.m1.1.1.cmml"><mtext id="S4.T3.3.1.1.m1.1.1.2" xref="S4.T3.3.1.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T3.3.1.1.m1.1.1.3" xref="S4.T3.3.1.1.m1.1.1.3a.cmml">BEV</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.3.1.1.m1.1c"><apply id="S4.T3.3.1.1.m1.1.1.cmml" xref="S4.T3.3.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.3.1.1.m1.1.1.1.cmml" xref="S4.T3.3.1.1.m1.1.1">subscript</csymbol><ci id="S4.T3.3.1.1.m1.1.1.2a.cmml" xref="S4.T3.3.1.1.m1.1.1.2"><mtext id="S4.T3.3.1.1.m1.1.1.2.cmml" xref="S4.T3.3.1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.T3.3.1.1.m1.1.1.3a.cmml" xref="S4.T3.3.1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T3.3.1.1.m1.1.1.3.cmml" xref="S4.T3.3.1.1.m1.1.1.3">BEV</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.1.1.m1.1d">\text{AP}_{\text{BEV}}</annotation></semantics></math>/<math id="S4.T3.4.2.2.m2.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.T3.4.2.2.m2.1b"><msub id="S4.T3.4.2.2.m2.1.1" xref="S4.T3.4.2.2.m2.1.1.cmml"><mtext id="S4.T3.4.2.2.m2.1.1.2" xref="S4.T3.4.2.2.m2.1.1.2a.cmml">AP</mtext><mtext id="S4.T3.4.2.2.m2.1.1.3" xref="S4.T3.4.2.2.m2.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.4.2.2.m2.1c"><apply id="S4.T3.4.2.2.m2.1.1.cmml" xref="S4.T3.4.2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T3.4.2.2.m2.1.1.1.cmml" xref="S4.T3.4.2.2.m2.1.1">subscript</csymbol><ci id="S4.T3.4.2.2.m2.1.1.2a.cmml" xref="S4.T3.4.2.2.m2.1.1.2"><mtext id="S4.T3.4.2.2.m2.1.1.2.cmml" xref="S4.T3.4.2.2.m2.1.1.2">AP</mtext></ci><ci id="S4.T3.4.2.2.m2.1.1.3a.cmml" xref="S4.T3.4.2.2.m2.1.1.3"><mtext mathsize="70%" id="S4.T3.4.2.2.m2.1.1.3.cmml" xref="S4.T3.4.2.2.m2.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.2.2.m2.1d">\text{AP}_{\text{3D}}</annotation></semantics></math>.</span></span></figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.6.1.1" class="ltx_tr">
<td id="S4.T4.6.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;" colspan="5">Target: Lyft</td>
</tr>
<tr id="S4.T4.6.2.2" class="ltx_tr">
<td id="S4.T4.6.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Method</td>
<td id="S4.T4.6.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Source</td>
<td id="S4.T4.6.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Detector</td>
<td id="S4.T4.6.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">IoU=0.7</td>
<td id="S4.T4.6.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">IoU=0.5</td>
</tr>
<tr id="S4.T4.6.3.3" class="ltx_tr">
<td id="S4.T4.6.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;" rowspan="4"><span id="S4.T4.6.3.3.1.1" class="ltx_text">Source-Only</span></td>
<td id="S4.T4.6.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="S4.T4.6.3.3.2.1" class="ltx_text">Waymo</span></td>
<td id="S4.T4.6.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T4.6.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">49.68 / 38.94</td>
<td id="S4.T4.6.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">56.22 / 54.63</td>
</tr>
<tr id="S4.T4.6.4.4" class="ltx_tr">
<td id="S4.T4.6.4.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">CenterPoint</td>
<td id="S4.T4.6.4.4.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">43.83 / 32.98</td>
<td id="S4.T4.6.4.4.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">49.94 / 48.42</td>
</tr>
<tr id="S4.T4.6.5.5" class="ltx_tr">
<td id="S4.T4.6.5.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="S4.T4.6.5.5.1.1" class="ltx_text">nuScenes</span></td>
<td id="S4.T4.6.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T4.6.5.5.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">42.22 / 23.62</td>
<td id="S4.T4.6.5.5.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">48.45 / 45.51</td>
</tr>
<tr id="S4.T4.6.6.6" class="ltx_tr">
<td id="S4.T4.6.6.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">CenterPoint</td>
<td id="S4.T4.6.6.6.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">45.98 / 26.36</td>
<td id="S4.T4.6.6.6.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">52.41 / 49.43</td>
</tr>
<tr id="S4.T4.6.7.7" class="ltx_tr">
<td id="S4.T4.6.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="S4.T4.6.7.7.1.1" class="ltx_text">SN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></span></td>
<td id="S4.T4.6.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Waymo</td>
<td id="S4.T4.6.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T4.6.7.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">71.61 / 56.13</td>
<td id="S4.T4.6.7.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">80.65 / 78.52</td>
</tr>
<tr id="S4.T4.6.8.8" class="ltx_tr">
<td id="S4.T4.6.8.8.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">nuScenes</td>
<td id="S4.T4.6.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T4.6.8.8.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">63.11 / 39.60</td>
<td id="S4.T4.6.8.8.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">72.27 / 68.25</td>
</tr>
<tr id="S4.T4.6.9.9" class="ltx_tr">
<td id="S4.T4.6.9.9.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;" rowspan="2">
<span id="S4.T4.6.9.9.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S4.T4.6.9.9.1.2" class="ltx_text">ST3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span>
</td>
<td id="S4.T4.6.9.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">Waymo</td>
<td id="S4.T4.6.9.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T4.6.9.9.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">73.86 / 56.33</td>
<td id="S4.T4.6.9.9.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">82.90 / 80.81</td>
</tr>
<tr id="S4.T4.6.10.10" class="ltx_tr">
<td id="S4.T4.6.10.10.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">nuScenes</td>
<td id="S4.T4.6.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T4.6.10.10.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">67.33 / 41.82</td>
<td id="S4.T4.6.10.10.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">77.17 / 72.99</td>
</tr>
<tr id="S4.T4.6.11.11" class="ltx_tr">
<td id="S4.T4.6.11.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;" rowspan="2">
<span id="S4.T4.6.11.11.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S4.T4.6.11.11.1.2" class="ltx_text">MS3D (Ours)</span>
</td>
<td id="S4.T4.6.11.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">Waymo</td>
<td id="S4.T4.6.11.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T4.6.11.11.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;"><span id="S4.T4.6.11.11.4.1" class="ltx_text ltx_font_bold">76.48 / 61.23</span></td>
<td id="S4.T4.6.11.11.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;"><span id="S4.T4.6.11.11.5.1" class="ltx_text ltx_font_bold">83.36 / 81.49</span></td>
</tr>
<tr id="S4.T4.6.12.12" class="ltx_tr">
<td id="S4.T4.6.12.12.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">nuScenes</td>
<td id="S4.T4.6.12.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T4.6.12.12.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;"><span id="S4.T4.6.12.12.3.1" class="ltx_text ltx_font_bold">75.02 / 59.01</span></td>
<td id="S4.T4.6.12.12.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;"><span id="S4.T4.6.12.12.4.1" class="ltx_text ltx_font_bold">83.46 / 81.52</span></td>
</tr>
<tr id="S4.T4.6.13.13" class="ltx_tr">
<td id="S4.T4.6.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">GT Fine-Tune</td>
<td id="S4.T4.6.13.13.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">Waymo</td>
<td id="S4.T4.6.13.13.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">SECOND</td>
<td id="S4.T4.6.13.13.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding:0.5pt 2.0pt;">81.10 / 66.76</td>
<td id="S4.T4.6.13.13.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding:0.5pt 2.0pt;">91.12 / 88.69</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.8.3.1" class="ltx_text" style="font-size:90%;">TABLE IV</span>: </span><span id="S4.T4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">UDA from nuScenes/Waymo to Lyft.<span id="S4.T4.4.2.2" class="ltx_text ltx_font_medium"> We report <math id="S4.T4.3.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{BEV}}" display="inline"><semantics id="S4.T4.3.1.1.m1.1b"><msub id="S4.T4.3.1.1.m1.1.1" xref="S4.T4.3.1.1.m1.1.1.cmml"><mtext id="S4.T4.3.1.1.m1.1.1.2" xref="S4.T4.3.1.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T4.3.1.1.m1.1.1.3" xref="S4.T4.3.1.1.m1.1.1.3a.cmml">BEV</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T4.3.1.1.m1.1c"><apply id="S4.T4.3.1.1.m1.1.1.cmml" xref="S4.T4.3.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.3.1.1.m1.1.1.1.cmml" xref="S4.T4.3.1.1.m1.1.1">subscript</csymbol><ci id="S4.T4.3.1.1.m1.1.1.2a.cmml" xref="S4.T4.3.1.1.m1.1.1.2"><mtext id="S4.T4.3.1.1.m1.1.1.2.cmml" xref="S4.T4.3.1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.T4.3.1.1.m1.1.1.3a.cmml" xref="S4.T4.3.1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T4.3.1.1.m1.1.1.3.cmml" xref="S4.T4.3.1.1.m1.1.1.3">BEV</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.1.1.m1.1d">\text{AP}_{\text{BEV}}</annotation></semantics></math>/<math id="S4.T4.4.2.2.m2.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.T4.4.2.2.m2.1b"><msub id="S4.T4.4.2.2.m2.1.1" xref="S4.T4.4.2.2.m2.1.1.cmml"><mtext id="S4.T4.4.2.2.m2.1.1.2" xref="S4.T4.4.2.2.m2.1.1.2a.cmml">AP</mtext><mtext id="S4.T4.4.2.2.m2.1.1.3" xref="S4.T4.4.2.2.m2.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T4.4.2.2.m2.1c"><apply id="S4.T4.4.2.2.m2.1.1.cmml" xref="S4.T4.4.2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T4.4.2.2.m2.1.1.1.cmml" xref="S4.T4.4.2.2.m2.1.1">subscript</csymbol><ci id="S4.T4.4.2.2.m2.1.1.2a.cmml" xref="S4.T4.4.2.2.m2.1.1.2"><mtext id="S4.T4.4.2.2.m2.1.1.2.cmml" xref="S4.T4.4.2.2.m2.1.1.2">AP</mtext></ci><ci id="S4.T4.4.2.2.m2.1.1.3a.cmml" xref="S4.T4.4.2.2.m2.1.1.3"><mtext mathsize="70%" id="S4.T4.4.2.2.m2.1.1.3.cmml" xref="S4.T4.4.2.2.m2.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.2.2.m2.1d">\text{AP}_{\text{3D}}</annotation></semantics></math>. GT Fine-tune uses Lyft labels to fine-tune the pre-trained detector with no frame accumulation.</span></span></figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For each target domain, we show the detection performance from two different source domains. We demonstrate that with MS3D, regardless of the pre-trained detector’s source dataset, the detector can be fine-tuned to a similar state-of-the-art performance. In contrast, ST3D’s choice of pre-trained detector’s source dataset can have a significant impact on performance, with a 10.34 <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><msub id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mtext id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2a.cmml" xref="S4.SS2.p2.1.m1.1.1.2"><mtext id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">AP</mtext></ci><ci id="S4.SS2.p2.1.m1.1.1.3a.cmml" xref="S4.SS2.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\text{AP}_{\text{3D}}</annotation></semantics></math> difference at IoU=0.7 between selecting nuScenes or Lyft. MS3D’s robustness is especially valuable in real-world scenarios where acquiring sufficient labelled data for evaluation can be costly.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2304.02431/assets/images/plot_ps_quality.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="574" height="436" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Evaluation of MS3D’s pseudo-labels on different target domains. 1F MS3D refers to using only KBF on 1-frame detections from the 4 source domains. 1+16F MS3D refers to our final pseudo-labels after refining static vehicles.</span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Ablation</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.2" class="ltx_p"><span id="S4.SS3.p1.2.1" class="ltx_text ltx_font_bold">Pseudo-label Quality.</span> In <a href="#S4.F6" title="In IV-B Results ‣ IV Experiments ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, we show that our pseudo-labels consistently improve the detections of every source-trained detector on the target domain. When we combine all 4 sources with KBF, we can already surpass the best source-trained detector’s <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><msub id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mtext id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2a.cmml" xref="S4.SS3.p1.1.m1.1.1.2"><mtext id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.SS3.p1.1.m1.1.1.3a.cmml" xref="S4.SS3.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\text{AP}_{\text{3D}}</annotation></semantics></math>. By adding 16-frame detections for refining of static vehicles, we can further boost the detection by an impressive amount for all target domains. For example with nuScenes as the target domain, MS3D overcomes the detection range limitations of a sparse-beam lidar with our multi-frame static refinement, leading to a 8.84 <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><msub id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mtext id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2a.cmml">AP</mtext><mtext id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2a.cmml" xref="S4.SS3.p1.2.m2.1.1.2"><mtext id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">AP</mtext></ci><ci id="S4.SS3.p1.2.m2.1.1.3a.cmml" xref="S4.SS3.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">\text{AP}_{\text{3D}}</annotation></semantics></math> increase from 1F MS3D to 1F+16F MS3D.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.3" class="ltx_p"><span id="S4.SS3.p2.3.1" class="ltx_text ltx_font_bold">KDE Box Fusion.</span> We compare KBF with NMS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and two variants of Weighted Box Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> in <a href="#S4.T5" title="In IV-C Ablation ‣ IV Experiments ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">V</span></a>. WBF-C is the official 3D implementation using opposite 3D box corners, and WBF-P is weighted average of box params <math id="S4.SS3.p2.1.m1.6" class="ltx_Math" alttext="(c_{xyz},l,w,h,\theta,s)" display="inline"><semantics id="S4.SS3.p2.1.m1.6a"><mrow id="S4.SS3.p2.1.m1.6.6.1" xref="S4.SS3.p2.1.m1.6.6.2.cmml"><mo stretchy="false" id="S4.SS3.p2.1.m1.6.6.1.2" xref="S4.SS3.p2.1.m1.6.6.2.cmml">(</mo><msub id="S4.SS3.p2.1.m1.6.6.1.1" xref="S4.SS3.p2.1.m1.6.6.1.1.cmml"><mi id="S4.SS3.p2.1.m1.6.6.1.1.2" xref="S4.SS3.p2.1.m1.6.6.1.1.2.cmml">c</mi><mrow id="S4.SS3.p2.1.m1.6.6.1.1.3" xref="S4.SS3.p2.1.m1.6.6.1.1.3.cmml"><mi id="S4.SS3.p2.1.m1.6.6.1.1.3.2" xref="S4.SS3.p2.1.m1.6.6.1.1.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.6.6.1.1.3.1" xref="S4.SS3.p2.1.m1.6.6.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p2.1.m1.6.6.1.1.3.3" xref="S4.SS3.p2.1.m1.6.6.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.6.6.1.1.3.1a" xref="S4.SS3.p2.1.m1.6.6.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p2.1.m1.6.6.1.1.3.4" xref="S4.SS3.p2.1.m1.6.6.1.1.3.4.cmml">z</mi></mrow></msub><mo id="S4.SS3.p2.1.m1.6.6.1.3" xref="S4.SS3.p2.1.m1.6.6.2.cmml">,</mo><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">l</mi><mo id="S4.SS3.p2.1.m1.6.6.1.4" xref="S4.SS3.p2.1.m1.6.6.2.cmml">,</mo><mi id="S4.SS3.p2.1.m1.2.2" xref="S4.SS3.p2.1.m1.2.2.cmml">w</mi><mo id="S4.SS3.p2.1.m1.6.6.1.5" xref="S4.SS3.p2.1.m1.6.6.2.cmml">,</mo><mi id="S4.SS3.p2.1.m1.3.3" xref="S4.SS3.p2.1.m1.3.3.cmml">h</mi><mo id="S4.SS3.p2.1.m1.6.6.1.6" xref="S4.SS3.p2.1.m1.6.6.2.cmml">,</mo><mi id="S4.SS3.p2.1.m1.4.4" xref="S4.SS3.p2.1.m1.4.4.cmml">θ</mi><mo id="S4.SS3.p2.1.m1.6.6.1.7" xref="S4.SS3.p2.1.m1.6.6.2.cmml">,</mo><mi id="S4.SS3.p2.1.m1.5.5" xref="S4.SS3.p2.1.m1.5.5.cmml">s</mi><mo stretchy="false" id="S4.SS3.p2.1.m1.6.6.1.8" xref="S4.SS3.p2.1.m1.6.6.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.6b"><vector id="S4.SS3.p2.1.m1.6.6.2.cmml" xref="S4.SS3.p2.1.m1.6.6.1"><apply id="S4.SS3.p2.1.m1.6.6.1.1.cmml" xref="S4.SS3.p2.1.m1.6.6.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.6.6.1.1.1.cmml" xref="S4.SS3.p2.1.m1.6.6.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.6.6.1.1.2.cmml" xref="S4.SS3.p2.1.m1.6.6.1.1.2">𝑐</ci><apply id="S4.SS3.p2.1.m1.6.6.1.1.3.cmml" xref="S4.SS3.p2.1.m1.6.6.1.1.3"><times id="S4.SS3.p2.1.m1.6.6.1.1.3.1.cmml" xref="S4.SS3.p2.1.m1.6.6.1.1.3.1"></times><ci id="S4.SS3.p2.1.m1.6.6.1.1.3.2.cmml" xref="S4.SS3.p2.1.m1.6.6.1.1.3.2">𝑥</ci><ci id="S4.SS3.p2.1.m1.6.6.1.1.3.3.cmml" xref="S4.SS3.p2.1.m1.6.6.1.1.3.3">𝑦</ci><ci id="S4.SS3.p2.1.m1.6.6.1.1.3.4.cmml" xref="S4.SS3.p2.1.m1.6.6.1.1.3.4">𝑧</ci></apply></apply><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">𝑙</ci><ci id="S4.SS3.p2.1.m1.2.2.cmml" xref="S4.SS3.p2.1.m1.2.2">𝑤</ci><ci id="S4.SS3.p2.1.m1.3.3.cmml" xref="S4.SS3.p2.1.m1.3.3">ℎ</ci><ci id="S4.SS3.p2.1.m1.4.4.cmml" xref="S4.SS3.p2.1.m1.4.4">𝜃</ci><ci id="S4.SS3.p2.1.m1.5.5.cmml" xref="S4.SS3.p2.1.m1.5.5">𝑠</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.6c">(c_{xyz},l,w,h,\theta,s)</annotation></semantics></math>. We show that KBF outperforms other methods at all ranges, with the 50-80m range having the most improvement over existing methods. With KDE, setting a high bandwidth is similar to taking the average of the data points. Being able to tune the bandwidth for different parameters allows us to better capture the nuances of the box fusion process. For example, a common error in box prediction is the heading orientation being incorrect by an error of <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mi id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">π</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><ci id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">𝜋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\pi</annotation></semantics></math>. Setting a high bandwidth for rotation estimation would therefore be detrimental as we may select an in-between orientation instead of one that corrects the orientation by <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mi id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">π</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><ci id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">𝜋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">\pi</annotation></semantics></math>.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.5" class="ltx_p"><span id="S4.SS3.p3.5.1" class="ltx_text ltx_font_bold">Static Object Refinement.</span> In <a href="#S4.T6" title="In IV-C Ablation ‣ IV Experiments ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">VI</span></a>, we show that using 16-frame detections for static objects (<math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="H=0" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">H</mi><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><eq id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1"></eq><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">𝐻</ci><cn type="integer" id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">H=0</annotation></semantics></math>) gives a slight improvement over 1F MS3D (i.e., using KBF for 1-frame detections). Applying KBF on <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="H&gt;0" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mi id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml">H</mi><mo id="S4.SS3.p3.2.m2.1.1.1" xref="S4.SS3.p3.2.m2.1.1.1.cmml">&gt;</mo><mn id="S4.SS3.p3.2.m2.1.1.3" xref="S4.SS3.p3.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><gt id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1.1"></gt><ci id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2">𝐻</ci><cn type="integer" id="S4.SS3.p3.2.m2.1.1.3.cmml" xref="S4.SS3.p3.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">H&gt;0</annotation></semantics></math> historical boxes of a static object can further improve the localization of the box. As explained in <a href="#S3.SS5" title="III-E Multi-frame Static Object Refinement ‣ III Multi-Source 3D ‣ MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-E</span></span></a>, using KBF on all historical boxes is inadequate. This is shown in the lower performance of “Single Box” compared to using <math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="H=16" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mrow id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml"><mi id="S4.SS3.p3.3.m3.1.1.2" xref="S4.SS3.p3.3.m3.1.1.2.cmml">H</mi><mo id="S4.SS3.p3.3.m3.1.1.1" xref="S4.SS3.p3.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS3.p3.3.m3.1.1.3" xref="S4.SS3.p3.3.m3.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><apply id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1"><eq id="S4.SS3.p3.3.m3.1.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1.1"></eq><ci id="S4.SS3.p3.3.m3.1.1.2.cmml" xref="S4.SS3.p3.3.m3.1.1.2">𝐻</ci><cn type="integer" id="S4.SS3.p3.3.m3.1.1.3.cmml" xref="S4.SS3.p3.3.m3.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">H=16</annotation></semantics></math> boxes. Quantifying the extent of this difference is challenging because comparing <math id="S4.SS3.p3.4.m4.1" class="ltx_Math" alttext="H=16" display="inline"><semantics id="S4.SS3.p3.4.m4.1a"><mrow id="S4.SS3.p3.4.m4.1.1" xref="S4.SS3.p3.4.m4.1.1.cmml"><mi id="S4.SS3.p3.4.m4.1.1.2" xref="S4.SS3.p3.4.m4.1.1.2.cmml">H</mi><mo id="S4.SS3.p3.4.m4.1.1.1" xref="S4.SS3.p3.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS3.p3.4.m4.1.1.3" xref="S4.SS3.p3.4.m4.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.4.m4.1b"><apply id="S4.SS3.p3.4.m4.1.1.cmml" xref="S4.SS3.p3.4.m4.1.1"><eq id="S4.SS3.p3.4.m4.1.1.1.cmml" xref="S4.SS3.p3.4.m4.1.1.1"></eq><ci id="S4.SS3.p3.4.m4.1.1.2.cmml" xref="S4.SS3.p3.4.m4.1.1.2">𝐻</ci><cn type="integer" id="S4.SS3.p3.4.m4.1.1.3.cmml" xref="S4.SS3.p3.4.m4.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.4.m4.1c">H=16</annotation></semantics></math> and “Single box” to a GT label may only differ by a few cm, which can still meet the IoU=0.7 threshold. Errors in point cloud alignment over long sequences becomes problematic when there are sparse number of points on the car, as a few cm difference could result in the bounding box excluding the car’s points entirely. Therefore, based on qualitative and quantitative assessment, we chose to use <math id="S4.SS3.p3.5.m5.1" class="ltx_Math" alttext="H=16" display="inline"><semantics id="S4.SS3.p3.5.m5.1a"><mrow id="S4.SS3.p3.5.m5.1.1" xref="S4.SS3.p3.5.m5.1.1.cmml"><mi id="S4.SS3.p3.5.m5.1.1.2" xref="S4.SS3.p3.5.m5.1.1.2.cmml">H</mi><mo id="S4.SS3.p3.5.m5.1.1.1" xref="S4.SS3.p3.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS3.p3.5.m5.1.1.3" xref="S4.SS3.p3.5.m5.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.5.m5.1b"><apply id="S4.SS3.p3.5.m5.1.1.cmml" xref="S4.SS3.p3.5.m5.1.1"><eq id="S4.SS3.p3.5.m5.1.1.1.cmml" xref="S4.SS3.p3.5.m5.1.1.1"></eq><ci id="S4.SS3.p3.5.m5.1.1.2.cmml" xref="S4.SS3.p3.5.m5.1.1.2">𝐻</ci><cn type="integer" id="S4.SS3.p3.5.m5.1.1.3.cmml" xref="S4.SS3.p3.5.m5.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.5.m5.1c">H=16</annotation></semantics></math> boxes for estimating the bounding box of a parked vehicle.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.2.3.1" class="ltx_tr">
<th id="S4.T5.2.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;"></th>
<th id="S4.T5.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">IoU=0.7</th>
<td id="S4.T5.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;" colspan="3">RANGE (L2/APH)</td>
</tr>
<tr id="S4.T5.2.2" class="ltx_tr">
<th id="S4.T5.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">Method</th>
<th id="S4.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">
<math id="S4.T5.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{BEV}}" display="inline"><semantics id="S4.T5.1.1.1.m1.1a"><msub id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml"><mtext id="S4.T5.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T5.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.m1.1.1.3a.cmml">BEV</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T5.1.1.1.m1.1.1.2a.cmml" xref="S4.T5.1.1.1.m1.1.1.2"><mtext id="S4.T5.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.T5.1.1.1.m1.1.1.3a.cmml" xref="S4.T5.1.1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T5.1.1.1.m1.1.1.3.cmml" xref="S4.T5.1.1.1.m1.1.1.3">BEV</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\text{AP}_{\text{BEV}}</annotation></semantics></math>/<math id="S4.T5.2.2.2.m2.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.T5.2.2.2.m2.1a"><msub id="S4.T5.2.2.2.m2.1.1" xref="S4.T5.2.2.2.m2.1.1.cmml"><mtext id="S4.T5.2.2.2.m2.1.1.2" xref="S4.T5.2.2.2.m2.1.1.2a.cmml">AP</mtext><mtext id="S4.T5.2.2.2.m2.1.1.3" xref="S4.T5.2.2.2.m2.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.m2.1b"><apply id="S4.T5.2.2.2.m2.1.1.cmml" xref="S4.T5.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T5.2.2.2.m2.1.1.1.cmml" xref="S4.T5.2.2.2.m2.1.1">subscript</csymbol><ci id="S4.T5.2.2.2.m2.1.1.2a.cmml" xref="S4.T5.2.2.2.m2.1.1.2"><mtext id="S4.T5.2.2.2.m2.1.1.2.cmml" xref="S4.T5.2.2.2.m2.1.1.2">AP</mtext></ci><ci id="S4.T5.2.2.2.m2.1.1.3a.cmml" xref="S4.T5.2.2.2.m2.1.1.3"><mtext mathsize="70%" id="S4.T5.2.2.2.m2.1.1.3.cmml" xref="S4.T5.2.2.2.m2.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.m2.1c">\text{AP}_{\text{3D}}</annotation></semantics></math>
</th>
<td id="S4.T5.2.2.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">[0,30)</td>
<td id="S4.T5.2.2.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">[30,50)</td>
<td id="S4.T5.2.2.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">[50,+inf)</td>
</tr>
<tr id="S4.T5.2.4.2" class="ltx_tr">
<th id="S4.T5.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">nuScenes / CenterPoint</th>
<th id="S4.T5.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">49.49 / 23.33</th>
<td id="S4.T5.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">58.80</td>
<td id="S4.T5.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">17.56</td>
<td id="S4.T5.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">1.89</td>
</tr>
<tr id="S4.T5.2.5.3" class="ltx_tr">
<th id="S4.T5.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">nuScenes / SECOND</th>
<th id="S4.T5.2.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">50.74 / 23.64</th>
<td id="S4.T5.2.5.3.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">60.07</td>
<td id="S4.T5.2.5.3.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">15.56</td>
<td id="S4.T5.2.5.3.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">1.61</td>
</tr>
<tr id="S4.T5.2.6.4" class="ltx_tr">
<th id="S4.T5.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">Lyft / CenterPoint</th>
<th id="S4.T5.2.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">49.26 / 31.35</th>
<td id="S4.T5.2.6.4.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">61.53</td>
<td id="S4.T5.2.6.4.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">30.97</td>
<td id="S4.T5.2.6.4.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">9.88</td>
</tr>
<tr id="S4.T5.2.7.5" class="ltx_tr">
<th id="S4.T5.2.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">Lyft / SECOND</th>
<th id="S4.T5.2.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">50.83 / 35.09</th>
<td id="S4.T5.2.7.5.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">46.98</td>
<td id="S4.T5.2.7.5.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">19.11</td>
<td id="S4.T5.2.7.5.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">6.93</td>
</tr>
<tr id="S4.T5.2.8.6" class="ltx_tr">
<th id="S4.T5.2.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">NMS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<th id="S4.T5.2.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">
<span id="S4.T5.2.8.6.2.1" class="ltx_text ltx_font_bold">57.93</span> / 34.88</th>
<td id="S4.T5.2.8.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">57.13</td>
<td id="S4.T5.2.8.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">24.01</td>
<td id="S4.T5.2.8.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">7.59</td>
</tr>
<tr id="S4.T5.2.9.7" class="ltx_tr">
<th id="S4.T5.2.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">WBF-C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<th id="S4.T5.2.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">53.27 / 36.55</th>
<td id="S4.T5.2.9.7.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">67.17</td>
<td id="S4.T5.2.9.7.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">32.74</td>
<td id="S4.T5.2.9.7.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">8.33</td>
</tr>
<tr id="S4.T5.2.10.8" class="ltx_tr">
<th id="S4.T5.2.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">WBF-P <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<th id="S4.T5.2.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">54.44 / 36.75</th>
<td id="S4.T5.2.10.8.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">67.43</td>
<td id="S4.T5.2.10.8.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">33.32</td>
<td id="S4.T5.2.10.8.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">8.92</td>
</tr>
<tr id="S4.T5.2.11.9" class="ltx_tr">
<th id="S4.T5.2.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding:0.5pt 2.0pt;">KBF (Ours)</th>
<th id="S4.T5.2.11.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding:0.5pt 2.0pt;">57.22 / <span id="S4.T5.2.11.9.2.1" class="ltx_text ltx_font_bold">38.67</span>
</th>
<td id="S4.T5.2.11.9.3" class="ltx_td ltx_align_center ltx_border_b" style="padding:0.5pt 2.0pt;"><span id="S4.T5.2.11.9.3.1" class="ltx_text ltx_font_bold">68.89</span></td>
<td id="S4.T5.2.11.9.4" class="ltx_td ltx_align_center ltx_border_b" style="padding:0.5pt 2.0pt;"><span id="S4.T5.2.11.9.4.1" class="ltx_text ltx_font_bold">33.93</span></td>
<td id="S4.T5.2.11.9.5" class="ltx_td ltx_align_center ltx_border_b" style="padding:0.5pt 2.0pt;"><span id="S4.T5.2.11.9.5.1" class="ltx_text ltx_font_bold">10.40</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.4.1.1" class="ltx_text" style="font-size:90%;">TABLE V</span>: </span><span id="S4.T5.5.2" class="ltx_text" style="font-size:90%;">Ablation study of box fusion methods for multiple detector predictions on Waymo as target domain.</span></figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.5.6.1" class="ltx_tr">
<th id="S4.T6.5.6.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;"></th>
<th id="S4.T6.5.6.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">IoU=0.7</th>
<td id="S4.T6.5.6.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;" colspan="3">RANGE (L2/APH)</td>
</tr>
<tr id="S4.T6.2.2" class="ltx_tr">
<th id="S4.T6.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">Method</th>
<th id="S4.T6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">
<math id="S4.T6.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{BEV}}" display="inline"><semantics id="S4.T6.1.1.1.m1.1a"><msub id="S4.T6.1.1.1.m1.1.1" xref="S4.T6.1.1.1.m1.1.1.cmml"><mtext id="S4.T6.1.1.1.m1.1.1.2" xref="S4.T6.1.1.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T6.1.1.1.m1.1.1.3" xref="S4.T6.1.1.1.m1.1.1.3a.cmml">BEV</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.m1.1b"><apply id="S4.T6.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.1.1.1.m1.1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T6.1.1.1.m1.1.1.2a.cmml" xref="S4.T6.1.1.1.m1.1.1.2"><mtext id="S4.T6.1.1.1.m1.1.1.2.cmml" xref="S4.T6.1.1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.T6.1.1.1.m1.1.1.3a.cmml" xref="S4.T6.1.1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T6.1.1.1.m1.1.1.3.cmml" xref="S4.T6.1.1.1.m1.1.1.3">BEV</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.m1.1c">\text{AP}_{\text{BEV}}</annotation></semantics></math>/<math id="S4.T6.2.2.2.m2.1" class="ltx_Math" alttext="\text{AP}_{\text{3D}}" display="inline"><semantics id="S4.T6.2.2.2.m2.1a"><msub id="S4.T6.2.2.2.m2.1.1" xref="S4.T6.2.2.2.m2.1.1.cmml"><mtext id="S4.T6.2.2.2.m2.1.1.2" xref="S4.T6.2.2.2.m2.1.1.2a.cmml">AP</mtext><mtext id="S4.T6.2.2.2.m2.1.1.3" xref="S4.T6.2.2.2.m2.1.1.3a.cmml">3D</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.m2.1b"><apply id="S4.T6.2.2.2.m2.1.1.cmml" xref="S4.T6.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T6.2.2.2.m2.1.1.1.cmml" xref="S4.T6.2.2.2.m2.1.1">subscript</csymbol><ci id="S4.T6.2.2.2.m2.1.1.2a.cmml" xref="S4.T6.2.2.2.m2.1.1.2"><mtext id="S4.T6.2.2.2.m2.1.1.2.cmml" xref="S4.T6.2.2.2.m2.1.1.2">AP</mtext></ci><ci id="S4.T6.2.2.2.m2.1.1.3a.cmml" xref="S4.T6.2.2.2.m2.1.1.3"><mtext mathsize="70%" id="S4.T6.2.2.2.m2.1.1.3.cmml" xref="S4.T6.2.2.2.m2.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.m2.1c">\text{AP}_{\text{3D}}</annotation></semantics></math>
</th>
<td id="S4.T6.2.2.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">[0,30)</td>
<td id="S4.T6.2.2.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">[30,50)</td>
<td id="S4.T6.2.2.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">[50,+inf)</td>
</tr>
<tr id="S4.T6.5.7.2" class="ltx_tr">
<th id="S4.T6.5.7.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">nuScenes / CenterPoint</th>
<th id="S4.T6.5.7.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">49.49 / 23.33</th>
<td id="S4.T6.5.7.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">58.80</td>
<td id="S4.T6.5.7.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">17.56</td>
<td id="S4.T6.5.7.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">1.89</td>
</tr>
<tr id="S4.T6.5.8.3" class="ltx_tr">
<th id="S4.T6.5.8.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">nuScenes / SECOND</th>
<th id="S4.T6.5.8.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">50.74 / 23.64</th>
<td id="S4.T6.5.8.3.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">60.07</td>
<td id="S4.T6.5.8.3.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">15.56</td>
<td id="S4.T6.5.8.3.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">1.61</td>
</tr>
<tr id="S4.T6.5.9.4" class="ltx_tr">
<th id="S4.T6.5.9.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">Lyft / CenterPoint</th>
<th id="S4.T6.5.9.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">49.26 / 31.35</th>
<td id="S4.T6.5.9.4.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">61.53</td>
<td id="S4.T6.5.9.4.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">30.97</td>
<td id="S4.T6.5.9.4.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">9.88</td>
</tr>
<tr id="S4.T6.5.10.5" class="ltx_tr">
<th id="S4.T6.5.10.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">Lyft / SECOND</th>
<th id="S4.T6.5.10.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">50.83 / 35.09</th>
<td id="S4.T6.5.10.5.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">46.98</td>
<td id="S4.T6.5.10.5.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">19.11</td>
<td id="S4.T6.5.10.5.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">6.93</td>
</tr>
<tr id="S4.T6.5.11.6" class="ltx_tr">
<th id="S4.T6.5.11.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">1F MS3D</th>
<th id="S4.T6.5.11.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">57.22 / 38.67</th>
<td id="S4.T6.5.11.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">68.89</td>
<td id="S4.T6.5.11.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">33.93</td>
<td id="S4.T6.5.11.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">10.4</td>
</tr>
<tr id="S4.T6.3.3" class="ltx_tr">
<th id="S4.T6.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;"><math id="S4.T6.3.3.1.m1.1" class="ltx_Math" alttext="H=0" display="inline"><semantics id="S4.T6.3.3.1.m1.1a"><mrow id="S4.T6.3.3.1.m1.1.1" xref="S4.T6.3.3.1.m1.1.1.cmml"><mi id="S4.T6.3.3.1.m1.1.1.2" xref="S4.T6.3.3.1.m1.1.1.2.cmml">H</mi><mo id="S4.T6.3.3.1.m1.1.1.1" xref="S4.T6.3.3.1.m1.1.1.1.cmml">=</mo><mn id="S4.T6.3.3.1.m1.1.1.3" xref="S4.T6.3.3.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.1.m1.1b"><apply id="S4.T6.3.3.1.m1.1.1.cmml" xref="S4.T6.3.3.1.m1.1.1"><eq id="S4.T6.3.3.1.m1.1.1.1.cmml" xref="S4.T6.3.3.1.m1.1.1.1"></eq><ci id="S4.T6.3.3.1.m1.1.1.2.cmml" xref="S4.T6.3.3.1.m1.1.1.2">𝐻</ci><cn type="integer" id="S4.T6.3.3.1.m1.1.1.3.cmml" xref="S4.T6.3.3.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.1.m1.1c">H=0</annotation></semantics></math></th>
<th id="S4.T6.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">59.19 / 38.73</th>
<td id="S4.T6.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">69.49</td>
<td id="S4.T6.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">34.47</td>
<td id="S4.T6.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">10.63</td>
</tr>
<tr id="S4.T6.4.4" class="ltx_tr">
<th id="S4.T6.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;"><math id="S4.T6.4.4.1.m1.1" class="ltx_Math" alttext="H=4" display="inline"><semantics id="S4.T6.4.4.1.m1.1a"><mrow id="S4.T6.4.4.1.m1.1.1" xref="S4.T6.4.4.1.m1.1.1.cmml"><mi id="S4.T6.4.4.1.m1.1.1.2" xref="S4.T6.4.4.1.m1.1.1.2.cmml">H</mi><mo id="S4.T6.4.4.1.m1.1.1.1" xref="S4.T6.4.4.1.m1.1.1.1.cmml">=</mo><mn id="S4.T6.4.4.1.m1.1.1.3" xref="S4.T6.4.4.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.1.m1.1b"><apply id="S4.T6.4.4.1.m1.1.1.cmml" xref="S4.T6.4.4.1.m1.1.1"><eq id="S4.T6.4.4.1.m1.1.1.1.cmml" xref="S4.T6.4.4.1.m1.1.1.1"></eq><ci id="S4.T6.4.4.1.m1.1.1.2.cmml" xref="S4.T6.4.4.1.m1.1.1.2">𝐻</ci><cn type="integer" id="S4.T6.4.4.1.m1.1.1.3.cmml" xref="S4.T6.4.4.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.1.m1.1c">H=4</annotation></semantics></math></th>
<th id="S4.T6.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;">62.06 / 40.89</th>
<td id="S4.T6.4.4.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">71.74</td>
<td id="S4.T6.4.4.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">38.83</td>
<td id="S4.T6.4.4.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">14.11</td>
</tr>
<tr id="S4.T6.5.5" class="ltx_tr">
<th id="S4.T6.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;"><math id="S4.T6.5.5.1.m1.1" class="ltx_Math" alttext="H=16" display="inline"><semantics id="S4.T6.5.5.1.m1.1a"><mrow id="S4.T6.5.5.1.m1.1.1" xref="S4.T6.5.5.1.m1.1.1.cmml"><mi id="S4.T6.5.5.1.m1.1.1.2" xref="S4.T6.5.5.1.m1.1.1.2.cmml">H</mi><mo id="S4.T6.5.5.1.m1.1.1.1" xref="S4.T6.5.5.1.m1.1.1.1.cmml">=</mo><mn id="S4.T6.5.5.1.m1.1.1.3" xref="S4.T6.5.5.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.5.5.1.m1.1b"><apply id="S4.T6.5.5.1.m1.1.1.cmml" xref="S4.T6.5.5.1.m1.1.1"><eq id="S4.T6.5.5.1.m1.1.1.1.cmml" xref="S4.T6.5.5.1.m1.1.1.1"></eq><ci id="S4.T6.5.5.1.m1.1.1.2.cmml" xref="S4.T6.5.5.1.m1.1.1.2">𝐻</ci><cn type="integer" id="S4.T6.5.5.1.m1.1.1.3.cmml" xref="S4.T6.5.5.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.5.5.1.m1.1c">H=16</annotation></semantics></math></th>
<th id="S4.T6.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;"><span id="S4.T6.5.5.2.1" class="ltx_text ltx_font_bold">62.32 / 43.33</span></th>
<td id="S4.T6.5.5.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;"><span id="S4.T6.5.5.3.1" class="ltx_text ltx_font_bold">72.23</span></td>
<td id="S4.T6.5.5.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;"><span id="S4.T6.5.5.4.1" class="ltx_text ltx_font_bold">41.72</span></td>
<td id="S4.T6.5.5.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">17.63</td>
</tr>
<tr id="S4.T6.5.12.7" class="ltx_tr">
<th id="S4.T6.5.12.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding:0.5pt 2.0pt;">Single box</th>
<th id="S4.T6.5.12.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding:0.5pt 2.0pt;">62.32 / 43.12</th>
<td id="S4.T6.5.12.7.3" class="ltx_td ltx_align_center ltx_border_b" style="padding:0.5pt 2.0pt;">72.02</td>
<td id="S4.T6.5.12.7.4" class="ltx_td ltx_align_center ltx_border_b" style="padding:0.5pt 2.0pt;">41.30</td>
<td id="S4.T6.5.12.7.5" class="ltx_td ltx_align_center ltx_border_b" style="padding:0.5pt 2.0pt;"><span id="S4.T6.5.12.7.5.1" class="ltx_text ltx_font_bold">17.88</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.11.3.1" class="ltx_text" style="font-size:90%;">TABLE VI</span>: </span><span id="S4.T6.9.2" class="ltx_text" style="font-size:90%;">Ablation study of our static box labelling using a rolling window of <math id="S4.T6.8.1.m1.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S4.T6.8.1.m1.1b"><mi id="S4.T6.8.1.m1.1.1" xref="S4.T6.8.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S4.T6.8.1.m1.1c"><ci id="S4.T6.8.1.m1.1.1.cmml" xref="S4.T6.8.1.m1.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.8.1.m1.1d">H</annotation></semantics></math> frames for KBF on Waymo as target domain. For <math id="S4.T6.9.2.m2.1" class="ltx_Math" alttext="H=0" display="inline"><semantics id="S4.T6.9.2.m2.1b"><mrow id="S4.T6.9.2.m2.1.1" xref="S4.T6.9.2.m2.1.1.cmml"><mi id="S4.T6.9.2.m2.1.1.2" xref="S4.T6.9.2.m2.1.1.2.cmml">H</mi><mo id="S4.T6.9.2.m2.1.1.1" xref="S4.T6.9.2.m2.1.1.1.cmml">=</mo><mn id="S4.T6.9.2.m2.1.1.3" xref="S4.T6.9.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.9.2.m2.1c"><apply id="S4.T6.9.2.m2.1.1.cmml" xref="S4.T6.9.2.m2.1.1"><eq id="S4.T6.9.2.m2.1.1.1.cmml" xref="S4.T6.9.2.m2.1.1.1"></eq><ci id="S4.T6.9.2.m2.1.1.2.cmml" xref="S4.T6.9.2.m2.1.1.2">𝐻</ci><cn type="integer" id="S4.T6.9.2.m2.1.1.3.cmml" xref="S4.T6.9.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.9.2.m2.1d">H=0</annotation></semantics></math> we replace static boxes with their 16-frame detections. For “Single box”, we use KBF of all observed frames.</span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Discussion</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In our experimentation with various source to target domain pairs, we observed that while 16-frame accumulation increased point density, it did not necessarily help detectors localize well even when the entire car shape is visible. For example, when testing nuScenes/Waymo detectors on KITTI, we found that for a fully visible, 16-frame accumulated car at 5m away from the ego vehicle, detectors consistently predicted a box that was 0.5-1m longer. This was also observed when using a KITTI detector on nuScenes/Waymo where it would predict a smaller box that did not encapsulate the entire object. This is likely due to the scan pattern domain gap as 16-frame accumulated point clouds appear quite distinct depending on the lidar used. To address this gap when the target dataset statistics deviate significantly from the source dataset, we recommend incorporating Random Object Scaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> or SN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> in the self-training process.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we introduce MS3D, an innovative self-training pipeline that utilizes multiple source domains and temporal information to adapt to new target domains. Leveraging the availability of multiple labelled datasets in real-world applications, we demonstrate that combining detectors trained on these sources can effectively auto-label an unlabelled target dataset and fine-tune the source detector to boost performance. MS3D achieves state-of-the-art results on all tested UDA settings, with our proposed KBF method consistently outperforming the best individual source detector in terms of improving pseudo-label quality. While the optimal combination of detectors for an unlabelled target domain remains an open question, MS3D lays a strong foundation to integrate new and existing detector architectures and facilitates low-cost deployment in real-world scenarios.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? the
kitti vision benchmark suite,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2012 IEEE Conference on Computer
Vision and Pattern Recognition</em>.   IEEE,
2012, pp. 3354–3361.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo,
Y. Zhou, Y. Chai, B. Caine, <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Scalability in perception for
autonomous driving: Waymo open dataset,” in <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020, pp.
2446–2454.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for
autonomous driving,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, 2020, pp. 11 621–11 631.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nadhamuni, A. Ferreira, M. Yuan,
B. Low, A. Jain, P. Ondruska, S. Omari, S. Shah, A. Kulkarni, A. Kazakova,
C. Tao, L. Platinsky, W. Jiang, and V. Shet, “Woven planet perception
dataset 2020,”
<a target="_blank" href="https://www.woven-planet.global/en/data/perception-dataset" title="" class="ltx_ref ltx_url">https://www.woven-planet.global/en/data/perception-dataset</a>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y. Wang, X. Chen, Y. You, L. E. Li, B. Hariharan, M. Campbell, K. Q.
Weinberger, and W.-L. Chao, “Train in germany, test in the usa: Making 3d
object detectors generalize,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2020, pp.
11 713–11 723.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Yang, S. Shi, Z. Wang, H. Li, and X. Qi, “St3d: Self-training for
unsupervised domain adaptation on 3d object detection,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021,
pp. 10 368–10 378.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Z. Yang, Y. Zhou, Z. Chen, and J. Ngiam, “3d-man: 3d multi-frame attention
network for object detection,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 1863–1872.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
C. R. Qi, Y. Zhou, M. Najibi, P. Sun, K. Vo, B. Deng, and D. Anguelov,
“Offboard 3d object detection from point cloud sequences,” in
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2021, pp. 6134–6144.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
X. Chen, S. Shi, B. Zhu, K. C. Cheung, H. Xu, and H. Li, “Mppnet: Multi-frame
feature intertwining with proxy points for 3d temporal object detection,” in
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part VIII</em>.   Springer, 2022, pp. 680–697.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Yan, Y. Mao, and B. Li, “Second: Sparsely embedded convolutional
detection,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 18, no. 10, p. 3337, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,
“Pointpillars: Fast encoders for object detection from point clouds,” in
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2019, pp. 12 697–12 705.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Shi, X. Wang, and H. Li, “Pointrcnn: 3d object proposal generation and
detection from point cloud,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition</em>, 2019, pp. 770–779.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on point
sets for 3d classification and segmentation,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition</em>, 2017, pp.
652–660.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, “Pv-rcnn:
Point-voxel feature set abstraction for 3d object detection,” in
<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2020, pp. 10 529–10 538.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T. Yin, X. Zhou, and P. Krahenbuhl, “Center-based 3d object detection and
tracking,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition</em>, 2021, pp. 11 784–11 793.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Qin, L. Wang, Y. Zhang, and Y. Fu, “Generatively inferential co-training
for unsupervised domain adaptation,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision Workshops</em>, 2019, pp. 0–0.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K. Saito, Y. Ushiku, T. Harada, and K. Saenko, “Strong-weak distribution
alignment for adaptive object detection,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp.
6956–6965.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L. Yi, B. Gong, and T. Funkhouser, “Complete &amp; label: A domain adaptation
approach to semantic segmentation of lidar point clouds,” in
<em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2021, pp. 15 363–15 373.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Zou, Z. Yu, B. Kumar, and J. Wang, “Unsupervised domain adaptation for
semantic segmentation via class-balanced self-training,” in
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision (ECCV)</em>,
2018, pp. 289–305.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Tarvainen and H. Valpola, “Mean teachers are better role models:
Weight-averaged consistency targets improve semi-supervised deep learning
results,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 30,
2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Z. Luo, Z. Cai, C. Zhou, G. Zhang, H. Zhao, S. Yi, S. Lu, H. Li, S. Zhang, and
Z. Liu, “Unsupervised domain adaptive 3d detection with multi-level
consistency,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference
on Computer Vision</em>, 2021, pp. 8866–8875.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
D. Tsai, J. S. Berrio, M. Shan, S. Worrall, and E. Nebot, “See eye to eye: A
lidar-agnostic 3d detection framework for unsupervised multi-target domain
adaptation,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 7, no. 3, pp.
7904–7911, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
D. Tsai, J. S. Berrio, M. Shan, E. Nebot, and S. Worrall, “Viewer-centred
surface completion for unsupervised domain adaptation in 3d object
detection,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.06407</em>, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. You, C. A. Diaz-Ruiz, Y. Wang, W.-L. Chao, B. Hariharan, M. Campbell, and
K. Q. Weinbergert, “Exploiting playbacks in unsupervised domain adaptation
for 3d object detection in self-driving cars,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">2022 International
Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2022, pp. 5070–5077.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z. Li, Z. Chen, A. Li, L. Fang, Q. Jiang, X. Liu, and J. Jiang, “Unsupervised
domain adaptation for monocular 3d object detection via self-training,” in
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part IX</em>.   Springer, 2022, pp. 245–262.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y. Wei, Z. Wei, Y. Rao, J. Li, J. Zhou, and J. Lu, “Lidar distillation:
Bridging the beam-induced domain gap for 3d object detection,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2203.14956</em>, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. M. Ahmed, D. S. Raychaudhuri, S. Paul, S. Oymak, and A. K. Roy-Chowdhury,
“Unsupervised multi-source domain adaptation without access to source
data,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, 2021, pp. 10 103–10 112.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. Dong, Z. Fang, A. Liu, G. Sun, and T. Liu, “Confident anchor-induced
multi-source free domain adaptation,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, vol. 34, pp. 2848–2860, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. He, X. Jia, S. Chen, and J. Liu, “Multi-source domain adaptation with
collaborative learning for semantic segmentation,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021,
pp. 11 008–11 017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
X. Yao, S. Zhao, P. Xu, and J. Yang, “Multi-source domain adaptation for
object detection,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International
Conference on Computer Vision</em>, 2021, pp. 3273–3282.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
B. Zhang, J. Yuan, B. Shi, T. Chen, Y. Li, and Y. Qiao, “Uni3d: A unified
baseline for multi-dataset 3d object detection,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2303.06880</em>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Neubeck and L. Van Gool, “Efficient non-maximum suppression,” in
<em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">18th international conference on pattern recognition (ICPR’06)</em>,
vol. 3.   IEEE, 2006, pp. 850–855.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
N. Bodla, B. Singh, R. Chellappa, and L. S. Davis, “Soft-nms–improving object
detection with one line of code,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
international conference on computer vision</em>, 2017, pp. 5561–5569.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
R. Solovyev, W. Wang, and T. Gabruseva, “Weighted boxes fusion: Ensembling
boxes from different object detection models,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Image and Vision
Computing</em>, vol. 107, p. 104117, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
T. Yin, X. Zhou, and P. Krahenbuhl, “Center-based 3d object detection and
tracking,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition</em>, 2021, pp. 11 784–11 793.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
E. Parzen, “On estimation of a probability density function and mode,”
<em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">The annals of mathematical statistics</em>, vol. 33, no. 3, pp. 1065–1076,
1962.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Z. Pang, Z. Li, and N. Wang, “Simpletrack: Understanding and rethinking 3d
multi-object tracking,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.09621</em>, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
X. Weng, J. Wang, D. Held, and K. Kitani, “3D Multi-Object Tracking: A
Baseline and New Evaluation Metrics,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">IROS</em>, 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
O. D. Team, “Openpcdet: An open-source toolbox for 3d object detection from
point clouds,” <a target="_blank" href="https://github.com/open-mmlab/OpenPCDet" title="" class="ltx_ref ltx_url">https://github.com/open-mmlab/OpenPCDet</a>, 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.02429" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.02431" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.02431">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.02431" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.02432" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 16:13:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
