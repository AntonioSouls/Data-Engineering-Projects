<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.12938] Perspective-aware Convolution for Monocular 3D object detection</title><meta property="og:description" content="Monocular 3D object detection is a crucial and challenging task for autonomous driving vehicle, while it uses only a single camera image to infer 3D objects in the scene. To address the difficulty of predicting depth uâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Perspective-aware Convolution for Monocular 3D object detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Perspective-aware Convolution for Monocular 3D object detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.12938">

<!--Generated on Wed Feb 28 11:22:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Perspective-aware Convolution for Monocular 3D object detection</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Monocular 3D object detection is a crucial and challenging task for autonomous driving vehicle, while it uses only a single camera image to infer 3D objects in the scene. To address the difficulty of predicting depth using only pictorial clue, we propose a novel perspective-aware convolutional layer that captures long-range dependencies in images. By enforcing convolutional kernels to extract features along the depth axis of every image pixel, we incorporates perspective information into network architecture. We integrate our perspective-aware convolutional layer into a 3D object detector and demonstrate improved performance on the KITTI3D dataset, achieving a 23.9% average precision in the easy benchmark. These results underscore the importance of modeling scene clues for accurate depth inference and highlight the benefits of incorporating scene structure in network design. Our perspective-aware convolutional layer has the potential to enhance object detection accuracy by providing more precise and context-aware feature extraction.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">â€”â€‰</span></span>
Dilation Convolution, Monocular 3D Object Detection, Perspective-aware</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Estimating object depth and understanding the scene structure are critical tasks in image recognition, especially in the context of autonomous driving where safety is paramount. While inferring object depth from a single camera image is challenging, existing approaches predominantly rely on costly active sensors like LiDAR, Radar, or infrared cameras that directly provide depth information. However, cameras offer a more cost-effective and practical alternative, given their ease of installation on vehicles. The main hurdle with cameras is the absence of depth information in 2D images, posing a significant challenge for depth estimation algorithms.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite the absence of depth information in camera images, we posit that the human perception system is capable of inferring depth from limited visual cues by leveraging other scene information. For instance, as depicted in Fig.<a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, humans can infer the location of objects based on the relative positions of other nearby objects in the scene, exploiting their understanding of scene structure. By comparing the distances between adjacent objects, depth can be estimated even in cases where occlusion occurs. Hence, we believe understanding scene structure plays a pivotal role in accurate depth estimation. Our objective is to integrate this perspective information into our convolutional neural network.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2308.12938/assets/image/long_range_dependency.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="103" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text ltx_font_bold">Fig.Â 1</span>: </span>Illustration of long-range dependency that aids in depth inference. Directly predicting the depth of the green dot is challenging. However, if we can determine the depth-axis for every pixel and extract the surrounding front and back pixels along this axis, it can significantly enhance the accuracy of predicting the depth for the green dot. To achieve this, we utilize the camera intrinsic matrix to derive the depth-axis and introduce a skewed convolutional kernel designed to capture features along this axis.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we present a novel approach called perspective-aware convolution (PAC) to enhance the capability of convolutional neural networks in capturing perspective-related features. PAC extracts feature along the depth axis by adjusting the shape of the convolutional kernels. Additionally, we introduce a PAC module that integrates multiple dilation rates within parallel convolutional branches. By incorporating the PAC module into 3D object detection networks, we enable them to generate perspective-aware feature maps, enhancing their ability to analyze objects in specific perspective scene structure.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Finally, to demonstrate the effectiveness of the PAC module, we evaluate it in KITTI 3D object detection challenge, where objects are defined as cuboids in the camera coordinate system. We train and evaluate our network on the KITTI dataset and achieve 23.53% AP on easy metric.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The remainder of this paper is organized as follows: Section 2 introduces the related work on 3D object detection and dilated convolution modules. Section 3 explains our proposed perspective-aware convolution. Subsequently, we report our experimental results on the KITTI dataset in Section 4.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Convolutional Module</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Convolutional layers are essential components of deep learning network. It plays a crucial role in extracting features from image by convolving a learnable kernel with the image pixel in a sliding-window fashion. While convolutional layers excel at recognizing local patterns, they have limitations when it comes to capturing long-range dependencies within an image. To address this limitation, researchers have explored various techniques to increase networkâ€™s receptive field, which is, the size of the image region that the network considers when making predictions at a particular location.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">To enlarge the receptive field of a network, researchers often resort to building deeper networks or increasing number of downsampling to expand the receptive field size; however, these approaches can result in the loss of fine-grained details in feature maps. Alternatively, dilated convolutions, as introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, provide another solution. By skipping a certain number of image pixels during convolution, determined by the dilation rate, dilated convolutions enable the network to have larger receptive fields while no need of down-sampling. Expanding on the benefits of dilated convolutions, DeepLabv2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> introduced atrous spatial pyramid pooling (ASPP) to further enhance feature extraction. The ASPP module incorporates multiple parallel dilated convolutions with different dilation rates, allowing for the extraction of multi-scale features from the same feature map. Another approach to enhancing the feature extraction capability of convolutional layers is the receptive field block(RFB)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, which adjusts the kernel size based on the corresponding dilation rate of the convolutional module.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">However, all the aforementioned methods rely on fixed kernel shapes that are predefined before the training process. To address this limitation, Dai et al. proposed a novel convolutional layer called deformable convolutional networks (DCN)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and its improved version DCNv2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. These convolutional modules enable the network to dynamically adjust the shape of convolutional kernel during training, making the feature extractor more adaptive to the scene structure. While this method introduces more flexibility to the training process, it also incurs a non-negligible overhead.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Despite the extensive research on convolution layers, there are few methods that incorporate perspective information into the network. Therefore, drawing inspiration from the ASPP module and deformable convolution, we propose our perspective-aware convolution (PAC) module, which adjusts the kernel shape based on the depth axis in the image. This novel module allows the network to capture the underlying scene geometry and perspective, enhancing its ability to understand the 3D structure of the environment. In Section 3, we will provide a detailed explanation of our PAC module and its integration within our proposed method.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Monocular 3D Object Detection</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Monocular 3D object detection is a rapidly evolving research field that utilize single camera images as input to estimate the 3D appearance of objects within the scene. In this section, we provide an overview of the fundamental concepts of 3D object detection and discuss some relevant prior works in the field.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.4" class="ltx_p">In the field of 3D object detection, an object is represented by a cuboid which respect to camera coordinates. This cuboid is characterized by seven parameters: centroid coordinate <math id="S2.SS2.p2.1.m1.3" class="ltx_Math" alttext="(x,y,z)" display="inline"><semantics id="S2.SS2.p2.1.m1.3a"><mrow id="S2.SS2.p2.1.m1.3.4.2" xref="S2.SS2.p2.1.m1.3.4.1.cmml"><mo stretchy="false" id="S2.SS2.p2.1.m1.3.4.2.1" xref="S2.SS2.p2.1.m1.3.4.1.cmml">(</mo><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">x</mi><mo id="S2.SS2.p2.1.m1.3.4.2.2" xref="S2.SS2.p2.1.m1.3.4.1.cmml">,</mo><mi id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">y</mi><mo id="S2.SS2.p2.1.m1.3.4.2.3" xref="S2.SS2.p2.1.m1.3.4.1.cmml">,</mo><mi id="S2.SS2.p2.1.m1.3.3" xref="S2.SS2.p2.1.m1.3.3.cmml">z</mi><mo stretchy="false" id="S2.SS2.p2.1.m1.3.4.2.4" xref="S2.SS2.p2.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.3b"><vector id="S2.SS2.p2.1.m1.3.4.1.cmml" xref="S2.SS2.p2.1.m1.3.4.2"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">ğ‘¥</ci><ci id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">ğ‘¦</ci><ci id="S2.SS2.p2.1.m1.3.3.cmml" xref="S2.SS2.p2.1.m1.3.3">ğ‘§</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.3c">(x,y,z)</annotation></semantics></math>, which specifies the location of the cuboid center relative to the camera center, and dimensions <math id="S2.SS2.p2.2.m2.3" class="ltx_Math" alttext="(w,h,l)" display="inline"><semantics id="S2.SS2.p2.2.m2.3a"><mrow id="S2.SS2.p2.2.m2.3.4.2" xref="S2.SS2.p2.2.m2.3.4.1.cmml"><mo stretchy="false" id="S2.SS2.p2.2.m2.3.4.2.1" xref="S2.SS2.p2.2.m2.3.4.1.cmml">(</mo><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">w</mi><mo id="S2.SS2.p2.2.m2.3.4.2.2" xref="S2.SS2.p2.2.m2.3.4.1.cmml">,</mo><mi id="S2.SS2.p2.2.m2.2.2" xref="S2.SS2.p2.2.m2.2.2.cmml">h</mi><mo id="S2.SS2.p2.2.m2.3.4.2.3" xref="S2.SS2.p2.2.m2.3.4.1.cmml">,</mo><mi id="S2.SS2.p2.2.m2.3.3" xref="S2.SS2.p2.2.m2.3.3.cmml">l</mi><mo stretchy="false" id="S2.SS2.p2.2.m2.3.4.2.4" xref="S2.SS2.p2.2.m2.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.3b"><vector id="S2.SS2.p2.2.m2.3.4.1.cmml" xref="S2.SS2.p2.2.m2.3.4.2"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">ğ‘¤</ci><ci id="S2.SS2.p2.2.m2.2.2.cmml" xref="S2.SS2.p2.2.m2.2.2">â„</ci><ci id="S2.SS2.p2.2.m2.3.3.cmml" xref="S2.SS2.p2.2.m2.3.3">ğ‘™</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.3c">(w,h,l)</annotation></semantics></math>, which correspond to the width, height, and length of the cuboid, respectively. Additionally, the yaw angle <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mi id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><ci id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">\theta</annotation></semantics></math> denotes the orientation of the cuboid. Our objective in 3D object detection is to identify objects within images and accurately localize them in camera coordinates by predicting these seven variables <math id="S2.SS2.p2.4.m4.7" class="ltx_Math" alttext="(x,y,z,w,h,l,\theta)" display="inline"><semantics id="S2.SS2.p2.4.m4.7a"><mrow id="S2.SS2.p2.4.m4.7.8.2" xref="S2.SS2.p2.4.m4.7.8.1.cmml"><mo stretchy="false" id="S2.SS2.p2.4.m4.7.8.2.1" xref="S2.SS2.p2.4.m4.7.8.1.cmml">(</mo><mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">x</mi><mo id="S2.SS2.p2.4.m4.7.8.2.2" xref="S2.SS2.p2.4.m4.7.8.1.cmml">,</mo><mi id="S2.SS2.p2.4.m4.2.2" xref="S2.SS2.p2.4.m4.2.2.cmml">y</mi><mo id="S2.SS2.p2.4.m4.7.8.2.3" xref="S2.SS2.p2.4.m4.7.8.1.cmml">,</mo><mi id="S2.SS2.p2.4.m4.3.3" xref="S2.SS2.p2.4.m4.3.3.cmml">z</mi><mo id="S2.SS2.p2.4.m4.7.8.2.4" xref="S2.SS2.p2.4.m4.7.8.1.cmml">,</mo><mi id="S2.SS2.p2.4.m4.4.4" xref="S2.SS2.p2.4.m4.4.4.cmml">w</mi><mo id="S2.SS2.p2.4.m4.7.8.2.5" xref="S2.SS2.p2.4.m4.7.8.1.cmml">,</mo><mi id="S2.SS2.p2.4.m4.5.5" xref="S2.SS2.p2.4.m4.5.5.cmml">h</mi><mo id="S2.SS2.p2.4.m4.7.8.2.6" xref="S2.SS2.p2.4.m4.7.8.1.cmml">,</mo><mi id="S2.SS2.p2.4.m4.6.6" xref="S2.SS2.p2.4.m4.6.6.cmml">l</mi><mo id="S2.SS2.p2.4.m4.7.8.2.7" xref="S2.SS2.p2.4.m4.7.8.1.cmml">,</mo><mi id="S2.SS2.p2.4.m4.7.7" xref="S2.SS2.p2.4.m4.7.7.cmml">Î¸</mi><mo stretchy="false" id="S2.SS2.p2.4.m4.7.8.2.8" xref="S2.SS2.p2.4.m4.7.8.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.7b"><vector id="S2.SS2.p2.4.m4.7.8.1.cmml" xref="S2.SS2.p2.4.m4.7.8.2"><ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">ğ‘¥</ci><ci id="S2.SS2.p2.4.m4.2.2.cmml" xref="S2.SS2.p2.4.m4.2.2">ğ‘¦</ci><ci id="S2.SS2.p2.4.m4.3.3.cmml" xref="S2.SS2.p2.4.m4.3.3">ğ‘§</ci><ci id="S2.SS2.p2.4.m4.4.4.cmml" xref="S2.SS2.p2.4.m4.4.4">ğ‘¤</ci><ci id="S2.SS2.p2.4.m4.5.5.cmml" xref="S2.SS2.p2.4.m4.5.5">â„</ci><ci id="S2.SS2.p2.4.m4.6.6.cmml" xref="S2.SS2.p2.4.m4.6.6">ğ‘™</ci><ci id="S2.SS2.p2.4.m4.7.7.cmml" xref="S2.SS2.p2.4.m4.7.7">ğœƒ</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.7c">(x,y,z,w,h,l,\theta)</annotation></semantics></math> for each object. This concept is illustrated in Fig<a href="#S2.F2" title="Figure 2 â€£ 2.2 Monocular 3D Object Detection â€£ 2 Related Works â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2308.12938/assets/image/3d_od.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="102" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.9.1.1" class="ltx_text ltx_font_bold">Fig.Â 2</span>: </span>3D object detection. The cuboids in defined in camera coordinate and we need to find the location <math id="S2.F2.5.m1.3" class="ltx_Math" alttext="(x,y,z)" display="inline"><semantics id="S2.F2.5.m1.3b"><mrow id="S2.F2.5.m1.3.4.2" xref="S2.F2.5.m1.3.4.1.cmml"><mo stretchy="false" id="S2.F2.5.m1.3.4.2.1" xref="S2.F2.5.m1.3.4.1.cmml">(</mo><mi id="S2.F2.5.m1.1.1" xref="S2.F2.5.m1.1.1.cmml">x</mi><mo id="S2.F2.5.m1.3.4.2.2" xref="S2.F2.5.m1.3.4.1.cmml">,</mo><mi id="S2.F2.5.m1.2.2" xref="S2.F2.5.m1.2.2.cmml">y</mi><mo id="S2.F2.5.m1.3.4.2.3" xref="S2.F2.5.m1.3.4.1.cmml">,</mo><mi id="S2.F2.5.m1.3.3" xref="S2.F2.5.m1.3.3.cmml">z</mi><mo stretchy="false" id="S2.F2.5.m1.3.4.2.4" xref="S2.F2.5.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.5.m1.3c"><vector id="S2.F2.5.m1.3.4.1.cmml" xref="S2.F2.5.m1.3.4.2"><ci id="S2.F2.5.m1.1.1.cmml" xref="S2.F2.5.m1.1.1">ğ‘¥</ci><ci id="S2.F2.5.m1.2.2.cmml" xref="S2.F2.5.m1.2.2">ğ‘¦</ci><ci id="S2.F2.5.m1.3.3.cmml" xref="S2.F2.5.m1.3.3">ğ‘§</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.5.m1.3d">(x,y,z)</annotation></semantics></math>, dimension <math id="S2.F2.6.m2.3" class="ltx_Math" alttext="(w,h,l)" display="inline"><semantics id="S2.F2.6.m2.3b"><mrow id="S2.F2.6.m2.3.4.2" xref="S2.F2.6.m2.3.4.1.cmml"><mo stretchy="false" id="S2.F2.6.m2.3.4.2.1" xref="S2.F2.6.m2.3.4.1.cmml">(</mo><mi id="S2.F2.6.m2.1.1" xref="S2.F2.6.m2.1.1.cmml">w</mi><mo id="S2.F2.6.m2.3.4.2.2" xref="S2.F2.6.m2.3.4.1.cmml">,</mo><mi id="S2.F2.6.m2.2.2" xref="S2.F2.6.m2.2.2.cmml">h</mi><mo id="S2.F2.6.m2.3.4.2.3" xref="S2.F2.6.m2.3.4.1.cmml">,</mo><mi id="S2.F2.6.m2.3.3" xref="S2.F2.6.m2.3.3.cmml">l</mi><mo stretchy="false" id="S2.F2.6.m2.3.4.2.4" xref="S2.F2.6.m2.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.6.m2.3c"><vector id="S2.F2.6.m2.3.4.1.cmml" xref="S2.F2.6.m2.3.4.2"><ci id="S2.F2.6.m2.1.1.cmml" xref="S2.F2.6.m2.1.1">ğ‘¤</ci><ci id="S2.F2.6.m2.2.2.cmml" xref="S2.F2.6.m2.2.2">â„</ci><ci id="S2.F2.6.m2.3.3.cmml" xref="S2.F2.6.m2.3.3">ğ‘™</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.m2.3d">(w,h,l)</annotation></semantics></math>, and orientation <math id="S2.F2.7.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.F2.7.m3.1b"><mi id="S2.F2.7.m3.1.1" xref="S2.F2.7.m3.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.F2.7.m3.1c"><ci id="S2.F2.7.m3.1.1.cmml" xref="S2.F2.7.m3.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.7.m3.1d">\theta</annotation></semantics></math> of each cuboid. </figcaption>
</figure>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">The related work in 3D object detection can be divided into two main branches, as we shown in Fig<a href="#S2.F3" title="Figure 3 â€£ 2.2 Monocular 3D Object Detection â€£ 2 Related Works â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>: two-stage detectors and one-stage detectors. Two-stage detectors, such as Deep3DBox<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and FQNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, rely on a predicted 2D bounding box as prior information. These methods assume that all projected corners of a 3D bounding box must lie within its corresponding 2D bounding box. While these approaches reduce the complexity of the problem, they are sensitive to inaccuracies in the predicted 2D bounding box. On the other hand, other two-stage detector, including MonoDIS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, ROI10D<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, MonoGRNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and MonoPSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, utilize the predicted 2D bounding box as a region proposal to extract features and predict 3D box geometry from it. In specific, MonoDIS uses RoIAlign to extract fixed-length features from the regions of interest (ROIs) and proposes a disentangle loss function to avoid interference between each loss term and aid in faster convergence. MonoGRNet employs both early and deep features in the backbone network and uses deep features only for tasks that require a higher receptive field, such as depth estimation, to prevent excessive downsampling of the feature map. ROI10D and MonoPSR both utilize a pre-trained depth estimation model to improve their accuracy in estimating object depth.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2308.12938/assets/image/3dod_one_two_stage.png" id="S2.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="119" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.3.1.1" class="ltx_text ltx_font_bold">Fig.Â 3</span>: </span>
Related work in 3D object detection can be categorized into two-stage and one-stage detectors. Two-stage detectors utilize a predicted 2D bounding box as a prior and extract features within the 2D box, whereas one-stage detectors treat the prediction of 2D and 3D boxes as a unified task, predicting both boxes in parallel branches.</figcaption>
</figure>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">One-stage detectors take a different approach compared to two-stage detectors as they do not rely on 2D box priors. Instead, they treat 3D objects as extensions of 2D objects and utilize a unified network to predict both 2D and 3D bounding boxes in parallel branches without the need for region-specific feature extraction. One example is M3D-RPN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, which reformulates the 2D detector network to capture 3D proposals using a shared network for both tasks. M3D-RPN also incorporates statistical data from the training set to determine the 3D anchor box prior. Additionally, M3D-RPN proposes a depth-aware convolution, which employs separate kernels to extract features from different image rows, enabling the network to handle depth features separately. Another method, Ground-aware<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, focuses on locating and extracting the ground-contact point of each object to enhance its depth perception capability.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Another example of a one-stage detector is the keypoint-based network, as proposed by CenterNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. By detecting keypoints such as the center point or corner points of the 3D objects, these methods can efficiently predict 3D bounding boxes. RTM3D<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> takes a similar approach by identifying eight corners and the object center of the cuboid. It also introduces a feature pyramid network to capture multi-scale keypoints. SMOKE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> simplifies the network by eliminating the 2D box regression branch and incorporates MonoDISâ€™s disentangle loss to improve convergence. MonoPair<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> focuses on leveraging the relationship between adjacent objects by predicting keypoints at the midpoint of each adjacent object pair. Additionally, MonoPair introduces uncertainty in the regression branch and utilizes it for post-processing optimization of the detection results. MonoFlex<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> addresses truncated objects whose centers lie outside the image by proposing an edge fusion module to separate feature learning from truncated object prediction. MonoFlex employs an ensemble approach for depth prediction, considering depth uncertainty and the predicted 3D bounding boxâ€™s multiple pixel heights to enhance depth estimation accuracy.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">Overall, two-stage detectors have been pioneering in the field of 3D object detection, using 2D bounding boxes as priors. However, they tend to have performance degradation when the 2D bounding box predictions are inaccurate. On the other hand, one-stage detectors offer a more unified network architecture with improved performance. Therefore, in our work, we choose to adopt an anchor-based one-stage detector for our experiments, leveraging its superior performance in 3D object detection tasks.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Method</h2>

<figure id="S3.F4" class="ltx_figure"><img src="/html/2308.12938/assets/image/pac_example_2.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text ltx_font_bold">Fig.Â 4</span>: </span>Perspective-aware convolution. The red square in the image represent the kernel shape. The top image illustrates the kernel shape of dilation convolution, while the bottom image demonstrates the kernel shape of perspective-aware convolution. Our perspective-aware kernel dynamically adjusts its shape based on the depth axis at each pixel.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Perspective-aware Convolution</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We propose a novel perspective-aware convolutional layer that extracts features along the perspective lines at each pixel location. Our motivation arises from the recognition that the depth-axis-adjacent objects contain essential information for object depth estimation. Conventional convolutional layers often struggle to capture long-range dependencies in images. To overcome this limitation, we attempt to explicitly inject the perspective information into the network by guiding the convolutional kernels to capture features along the depth axis of each pixel. These axis are straight lines parallel to the cameraâ€™s depth axis in the camera coordinate system. By projecting these lines onto the image plane using the camera pinhole model, we obtain an indication of how each pixel would move on the image if its depth value were to change. Additionally, we can derive the angle between the depth axis and the u-axis, which we refer to as the perspective angle, and use it to represent the perspective line for simplicity. In the following part of this section, we will elaborate on how we derive the perspective angle based on the camera pinhole model.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">To get the perspective angle, we begin with the formulation of the pinhole camera model:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.3" class="ltx_Math" alttext="\begin{bmatrix}u\\
v\\
1\end{bmatrix}=\frac{1}{Z_{w}}\begin{bmatrix}f_{x}&amp;0&amp;C_{x}\\
0&amp;f_{y}&amp;C_{y}\\
0&amp;0&amp;1\\
\end{bmatrix}\begin{bmatrix}X_{w}\\
Y_{w}\\
Z_{w}\end{bmatrix}" display="block"><semantics id="S4.E1.m1.3a"><mrow id="S4.E1.m1.3.4" xref="S4.E1.m1.3.4.cmml"><mrow id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.2.cmml"><mo id="S4.E1.m1.1.1.3.1" xref="S4.E1.m1.1.1.2.1.cmml">[</mo><mtable displaystyle="true" rowspacing="0pt" id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mtr id="S4.E1.m1.1.1.1.1a" xref="S4.E1.m1.1.1.1.1.cmml"><mtd id="S4.E1.m1.1.1.1.1b" xref="S4.E1.m1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml">u</mi></mtd></mtr><mtr id="S4.E1.m1.1.1.1.1c" xref="S4.E1.m1.1.1.1.1.cmml"><mtd id="S4.E1.m1.1.1.1.1d" xref="S4.E1.m1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.2.1.1" xref="S4.E1.m1.1.1.1.1.2.1.1.cmml">v</mi></mtd></mtr><mtr id="S4.E1.m1.1.1.1.1e" xref="S4.E1.m1.1.1.1.1.cmml"><mtd id="S4.E1.m1.1.1.1.1f" xref="S4.E1.m1.1.1.1.1.cmml"><mn id="S4.E1.m1.1.1.1.1.3.1.1" xref="S4.E1.m1.1.1.1.1.3.1.1.cmml">1</mn></mtd></mtr></mtable><mo id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.2.1.cmml">]</mo></mrow><mo id="S4.E1.m1.3.4.1" xref="S4.E1.m1.3.4.1.cmml">=</mo><mrow id="S4.E1.m1.3.4.2" xref="S4.E1.m1.3.4.2.cmml"><mfrac id="S4.E1.m1.3.4.2.2" xref="S4.E1.m1.3.4.2.2.cmml"><mn id="S4.E1.m1.3.4.2.2.2" xref="S4.E1.m1.3.4.2.2.2.cmml">1</mn><msub id="S4.E1.m1.3.4.2.2.3" xref="S4.E1.m1.3.4.2.2.3.cmml"><mi id="S4.E1.m1.3.4.2.2.3.2" xref="S4.E1.m1.3.4.2.2.3.2.cmml">Z</mi><mi id="S4.E1.m1.3.4.2.2.3.3" xref="S4.E1.m1.3.4.2.2.3.3.cmml">w</mi></msub></mfrac><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.4.2.1" xref="S4.E1.m1.3.4.2.1.cmml">â€‹</mo><mrow id="S4.E1.m1.2.2.3" xref="S4.E1.m1.2.2.2.cmml"><mo id="S4.E1.m1.2.2.3.1" xref="S4.E1.m1.2.2.2.1.cmml">[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.cmml"><mtr id="S4.E1.m1.2.2.1.1a" xref="S4.E1.m1.2.2.1.1.cmml"><mtd id="S4.E1.m1.2.2.1.1b" xref="S4.E1.m1.2.2.1.1.cmml"><msub id="S4.E1.m1.2.2.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.2.cmml">f</mi><mi id="S4.E1.m1.2.2.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.3.cmml">x</mi></msub></mtd><mtd id="S4.E1.m1.2.2.1.1c" xref="S4.E1.m1.2.2.1.1.cmml"><mn id="S4.E1.m1.2.2.1.1.1.2.1" xref="S4.E1.m1.2.2.1.1.1.2.1.cmml">0</mn></mtd><mtd id="S4.E1.m1.2.2.1.1d" xref="S4.E1.m1.2.2.1.1.cmml"><msub id="S4.E1.m1.2.2.1.1.1.3.1" xref="S4.E1.m1.2.2.1.1.1.3.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.3.1.2" xref="S4.E1.m1.2.2.1.1.1.3.1.2.cmml">C</mi><mi id="S4.E1.m1.2.2.1.1.1.3.1.3" xref="S4.E1.m1.2.2.1.1.1.3.1.3.cmml">x</mi></msub></mtd></mtr><mtr id="S4.E1.m1.2.2.1.1e" xref="S4.E1.m1.2.2.1.1.cmml"><mtd id="S4.E1.m1.2.2.1.1f" xref="S4.E1.m1.2.2.1.1.cmml"><mn id="S4.E1.m1.2.2.1.1.2.1.1" xref="S4.E1.m1.2.2.1.1.2.1.1.cmml">0</mn></mtd><mtd id="S4.E1.m1.2.2.1.1g" xref="S4.E1.m1.2.2.1.1.cmml"><msub id="S4.E1.m1.2.2.1.1.2.2.1" xref="S4.E1.m1.2.2.1.1.2.2.1.cmml"><mi id="S4.E1.m1.2.2.1.1.2.2.1.2" xref="S4.E1.m1.2.2.1.1.2.2.1.2.cmml">f</mi><mi id="S4.E1.m1.2.2.1.1.2.2.1.3" xref="S4.E1.m1.2.2.1.1.2.2.1.3.cmml">y</mi></msub></mtd><mtd id="S4.E1.m1.2.2.1.1h" xref="S4.E1.m1.2.2.1.1.cmml"><msub id="S4.E1.m1.2.2.1.1.2.3.1" xref="S4.E1.m1.2.2.1.1.2.3.1.cmml"><mi id="S4.E1.m1.2.2.1.1.2.3.1.2" xref="S4.E1.m1.2.2.1.1.2.3.1.2.cmml">C</mi><mi id="S4.E1.m1.2.2.1.1.2.3.1.3" xref="S4.E1.m1.2.2.1.1.2.3.1.3.cmml">y</mi></msub></mtd></mtr><mtr id="S4.E1.m1.2.2.1.1i" xref="S4.E1.m1.2.2.1.1.cmml"><mtd id="S4.E1.m1.2.2.1.1j" xref="S4.E1.m1.2.2.1.1.cmml"><mn id="S4.E1.m1.2.2.1.1.3.1.1" xref="S4.E1.m1.2.2.1.1.3.1.1.cmml">0</mn></mtd><mtd id="S4.E1.m1.2.2.1.1k" xref="S4.E1.m1.2.2.1.1.cmml"><mn id="S4.E1.m1.2.2.1.1.3.2.1" xref="S4.E1.m1.2.2.1.1.3.2.1.cmml">0</mn></mtd><mtd id="S4.E1.m1.2.2.1.1l" xref="S4.E1.m1.2.2.1.1.cmml"><mn id="S4.E1.m1.2.2.1.1.3.3.1" xref="S4.E1.m1.2.2.1.1.3.3.1.cmml">1</mn></mtd></mtr></mtable><mo id="S4.E1.m1.2.2.3.2" xref="S4.E1.m1.2.2.2.1.cmml">]</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.4.2.1a" xref="S4.E1.m1.3.4.2.1.cmml">â€‹</mo><mrow id="S4.E1.m1.3.3.3" xref="S4.E1.m1.3.3.2.cmml"><mo id="S4.E1.m1.3.3.3.1" xref="S4.E1.m1.3.3.2.1.cmml">[</mo><mtable displaystyle="true" rowspacing="0pt" id="S4.E1.m1.3.3.1.1" xref="S4.E1.m1.3.3.1.1.cmml"><mtr id="S4.E1.m1.3.3.1.1a" xref="S4.E1.m1.3.3.1.1.cmml"><mtd id="S4.E1.m1.3.3.1.1b" xref="S4.E1.m1.3.3.1.1.cmml"><msub id="S4.E1.m1.3.3.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.1.2.cmml">X</mi><mi id="S4.E1.m1.3.3.1.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.1.3.cmml">w</mi></msub></mtd></mtr><mtr id="S4.E1.m1.3.3.1.1c" xref="S4.E1.m1.3.3.1.1.cmml"><mtd id="S4.E1.m1.3.3.1.1d" xref="S4.E1.m1.3.3.1.1.cmml"><msub id="S4.E1.m1.3.3.1.1.2.1.1" xref="S4.E1.m1.3.3.1.1.2.1.1.cmml"><mi id="S4.E1.m1.3.3.1.1.2.1.1.2" xref="S4.E1.m1.3.3.1.1.2.1.1.2.cmml">Y</mi><mi id="S4.E1.m1.3.3.1.1.2.1.1.3" xref="S4.E1.m1.3.3.1.1.2.1.1.3.cmml">w</mi></msub></mtd></mtr><mtr id="S4.E1.m1.3.3.1.1e" xref="S4.E1.m1.3.3.1.1.cmml"><mtd id="S4.E1.m1.3.3.1.1f" xref="S4.E1.m1.3.3.1.1.cmml"><msub id="S4.E1.m1.3.3.1.1.3.1.1" xref="S4.E1.m1.3.3.1.1.3.1.1.cmml"><mi id="S4.E1.m1.3.3.1.1.3.1.1.2" xref="S4.E1.m1.3.3.1.1.3.1.1.2.cmml">Z</mi><mi id="S4.E1.m1.3.3.1.1.3.1.1.3" xref="S4.E1.m1.3.3.1.1.3.1.1.3.cmml">w</mi></msub></mtd></mtr></mtable><mo id="S4.E1.m1.3.3.3.2" xref="S4.E1.m1.3.3.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.3b"><apply id="S4.E1.m1.3.4.cmml" xref="S4.E1.m1.3.4"><eq id="S4.E1.m1.3.4.1.cmml" xref="S4.E1.m1.3.4.1"></eq><apply id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.3"><csymbol cd="latexml" id="S4.E1.m1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.3.1">matrix</csymbol><matrix id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1"><matrixrow id="S4.E1.m1.1.1.1.1a.cmml" xref="S4.E1.m1.1.1.1.1"><ci id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1">ğ‘¢</ci></matrixrow><matrixrow id="S4.E1.m1.1.1.1.1b.cmml" xref="S4.E1.m1.1.1.1.1"><ci id="S4.E1.m1.1.1.1.1.2.1.1.cmml" xref="S4.E1.m1.1.1.1.1.2.1.1">ğ‘£</ci></matrixrow><matrixrow id="S4.E1.m1.1.1.1.1c.cmml" xref="S4.E1.m1.1.1.1.1"><cn type="integer" id="S4.E1.m1.1.1.1.1.3.1.1.cmml" xref="S4.E1.m1.1.1.1.1.3.1.1">1</cn></matrixrow></matrix></apply><apply id="S4.E1.m1.3.4.2.cmml" xref="S4.E1.m1.3.4.2"><times id="S4.E1.m1.3.4.2.1.cmml" xref="S4.E1.m1.3.4.2.1"></times><apply id="S4.E1.m1.3.4.2.2.cmml" xref="S4.E1.m1.3.4.2.2"><divide id="S4.E1.m1.3.4.2.2.1.cmml" xref="S4.E1.m1.3.4.2.2"></divide><cn type="integer" id="S4.E1.m1.3.4.2.2.2.cmml" xref="S4.E1.m1.3.4.2.2.2">1</cn><apply id="S4.E1.m1.3.4.2.2.3.cmml" xref="S4.E1.m1.3.4.2.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.4.2.2.3.1.cmml" xref="S4.E1.m1.3.4.2.2.3">subscript</csymbol><ci id="S4.E1.m1.3.4.2.2.3.2.cmml" xref="S4.E1.m1.3.4.2.2.3.2">ğ‘</ci><ci id="S4.E1.m1.3.4.2.2.3.3.cmml" xref="S4.E1.m1.3.4.2.2.3.3">ğ‘¤</ci></apply></apply><apply id="S4.E1.m1.2.2.2.cmml" xref="S4.E1.m1.2.2.3"><csymbol cd="latexml" id="S4.E1.m1.2.2.2.1.cmml" xref="S4.E1.m1.2.2.3.1">matrix</csymbol><matrix id="S4.E1.m1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1.1"><matrixrow id="S4.E1.m1.2.2.1.1a.cmml" xref="S4.E1.m1.2.2.1.1"><apply id="S4.E1.m1.2.2.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2">ğ‘“</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3">ğ‘¥</ci></apply><cn type="integer" id="S4.E1.m1.2.2.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.2.1">0</cn><apply id="S4.E1.m1.2.2.1.1.1.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.3.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.3.1">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.3.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.3.1.2">ğ¶</ci><ci id="S4.E1.m1.2.2.1.1.1.3.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.3.1.3">ğ‘¥</ci></apply></matrixrow><matrixrow id="S4.E1.m1.2.2.1.1b.cmml" xref="S4.E1.m1.2.2.1.1"><cn type="integer" id="S4.E1.m1.2.2.1.1.2.1.1.cmml" xref="S4.E1.m1.2.2.1.1.2.1.1">0</cn><apply id="S4.E1.m1.2.2.1.1.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.2.2.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1.1.2.2.1">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.2.2.1.2.cmml" xref="S4.E1.m1.2.2.1.1.2.2.1.2">ğ‘“</ci><ci id="S4.E1.m1.2.2.1.1.2.2.1.3.cmml" xref="S4.E1.m1.2.2.1.1.2.2.1.3">ğ‘¦</ci></apply><apply id="S4.E1.m1.2.2.1.1.2.3.1.cmml" xref="S4.E1.m1.2.2.1.1.2.3.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.2.3.1.1.cmml" xref="S4.E1.m1.2.2.1.1.2.3.1">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.2.3.1.2.cmml" xref="S4.E1.m1.2.2.1.1.2.3.1.2">ğ¶</ci><ci id="S4.E1.m1.2.2.1.1.2.3.1.3.cmml" xref="S4.E1.m1.2.2.1.1.2.3.1.3">ğ‘¦</ci></apply></matrixrow><matrixrow id="S4.E1.m1.2.2.1.1c.cmml" xref="S4.E1.m1.2.2.1.1"><cn type="integer" id="S4.E1.m1.2.2.1.1.3.1.1.cmml" xref="S4.E1.m1.2.2.1.1.3.1.1">0</cn><cn type="integer" id="S4.E1.m1.2.2.1.1.3.2.1.cmml" xref="S4.E1.m1.2.2.1.1.3.2.1">0</cn><cn type="integer" id="S4.E1.m1.2.2.1.1.3.3.1.cmml" xref="S4.E1.m1.2.2.1.1.3.3.1">1</cn></matrixrow></matrix></apply><apply id="S4.E1.m1.3.3.2.cmml" xref="S4.E1.m1.3.3.3"><csymbol cd="latexml" id="S4.E1.m1.3.3.2.1.cmml" xref="S4.E1.m1.3.3.3.1">matrix</csymbol><matrix id="S4.E1.m1.3.3.1.1.cmml" xref="S4.E1.m1.3.3.1.1"><matrixrow id="S4.E1.m1.3.3.1.1a.cmml" xref="S4.E1.m1.3.3.1.1"><apply id="S4.E1.m1.3.3.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.2">ğ‘‹</ci><ci id="S4.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.3">ğ‘¤</ci></apply></matrixrow><matrixrow id="S4.E1.m1.3.3.1.1b.cmml" xref="S4.E1.m1.3.3.1.1"><apply id="S4.E1.m1.3.3.1.1.2.1.1.cmml" xref="S4.E1.m1.3.3.1.1.2.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.2.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.2.1.1">subscript</csymbol><ci id="S4.E1.m1.3.3.1.1.2.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.2.1.1.2">ğ‘Œ</ci><ci id="S4.E1.m1.3.3.1.1.2.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.2.1.1.3">ğ‘¤</ci></apply></matrixrow><matrixrow id="S4.E1.m1.3.3.1.1c.cmml" xref="S4.E1.m1.3.3.1.1"><apply id="S4.E1.m1.3.3.1.1.3.1.1.cmml" xref="S4.E1.m1.3.3.1.1.3.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.3.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.3.1.1">subscript</csymbol><ci id="S4.E1.m1.3.3.1.1.3.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.3.1.1.2">ğ‘</ci><ci id="S4.E1.m1.3.3.1.1.3.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.3.1.1.3">ğ‘¤</ci></apply></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.3c">\begin{bmatrix}u\\
v\\
1\end{bmatrix}=\frac{1}{Z_{w}}\begin{bmatrix}f_{x}&amp;0&amp;C_{x}\\
0&amp;f_{y}&amp;C_{y}\\
0&amp;0&amp;1\\
\end{bmatrix}\begin{bmatrix}X_{w}\\
Y_{w}\\
Z_{w}\end{bmatrix}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.6" class="ltx_p">In Equation <a href="#S4.E1" title="In 4 Perspective-aware Convolution â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <math id="S4.p3.1.m1.3" class="ltx_Math" alttext="(X_{w},Y_{w},Z_{w})" display="inline"><semantics id="S4.p3.1.m1.3a"><mrow id="S4.p3.1.m1.3.3.3" xref="S4.p3.1.m1.3.3.4.cmml"><mo stretchy="false" id="S4.p3.1.m1.3.3.3.4" xref="S4.p3.1.m1.3.3.4.cmml">(</mo><msub id="S4.p3.1.m1.1.1.1.1" xref="S4.p3.1.m1.1.1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.1.1.2" xref="S4.p3.1.m1.1.1.1.1.2.cmml">X</mi><mi id="S4.p3.1.m1.1.1.1.1.3" xref="S4.p3.1.m1.1.1.1.1.3.cmml">w</mi></msub><mo id="S4.p3.1.m1.3.3.3.5" xref="S4.p3.1.m1.3.3.4.cmml">,</mo><msub id="S4.p3.1.m1.2.2.2.2" xref="S4.p3.1.m1.2.2.2.2.cmml"><mi id="S4.p3.1.m1.2.2.2.2.2" xref="S4.p3.1.m1.2.2.2.2.2.cmml">Y</mi><mi id="S4.p3.1.m1.2.2.2.2.3" xref="S4.p3.1.m1.2.2.2.2.3.cmml">w</mi></msub><mo id="S4.p3.1.m1.3.3.3.6" xref="S4.p3.1.m1.3.3.4.cmml">,</mo><msub id="S4.p3.1.m1.3.3.3.3" xref="S4.p3.1.m1.3.3.3.3.cmml"><mi id="S4.p3.1.m1.3.3.3.3.2" xref="S4.p3.1.m1.3.3.3.3.2.cmml">Z</mi><mi id="S4.p3.1.m1.3.3.3.3.3" xref="S4.p3.1.m1.3.3.3.3.3.cmml">w</mi></msub><mo stretchy="false" id="S4.p3.1.m1.3.3.3.7" xref="S4.p3.1.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.3b"><vector id="S4.p3.1.m1.3.3.4.cmml" xref="S4.p3.1.m1.3.3.3"><apply id="S4.p3.1.m1.1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.p3.1.m1.1.1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.1.1.2">ğ‘‹</ci><ci id="S4.p3.1.m1.1.1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.1.1.3">ğ‘¤</ci></apply><apply id="S4.p3.1.m1.2.2.2.2.cmml" xref="S4.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.p3.1.m1.2.2.2.2.1.cmml" xref="S4.p3.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.p3.1.m1.2.2.2.2.2.cmml" xref="S4.p3.1.m1.2.2.2.2.2">ğ‘Œ</ci><ci id="S4.p3.1.m1.2.2.2.2.3.cmml" xref="S4.p3.1.m1.2.2.2.2.3">ğ‘¤</ci></apply><apply id="S4.p3.1.m1.3.3.3.3.cmml" xref="S4.p3.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S4.p3.1.m1.3.3.3.3.1.cmml" xref="S4.p3.1.m1.3.3.3.3">subscript</csymbol><ci id="S4.p3.1.m1.3.3.3.3.2.cmml" xref="S4.p3.1.m1.3.3.3.3.2">ğ‘</ci><ci id="S4.p3.1.m1.3.3.3.3.3.cmml" xref="S4.p3.1.m1.3.3.3.3.3">ğ‘¤</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.3c">(X_{w},Y_{w},Z_{w})</annotation></semantics></math> represents the coordinates of a point in the camera coordinate system, while <math id="S4.p3.2.m2.2" class="ltx_Math" alttext="(u,v)" display="inline"><semantics id="S4.p3.2.m2.2a"><mrow id="S4.p3.2.m2.2.3.2" xref="S4.p3.2.m2.2.3.1.cmml"><mo stretchy="false" id="S4.p3.2.m2.2.3.2.1" xref="S4.p3.2.m2.2.3.1.cmml">(</mo><mi id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">u</mi><mo id="S4.p3.2.m2.2.3.2.2" xref="S4.p3.2.m2.2.3.1.cmml">,</mo><mi id="S4.p3.2.m2.2.2" xref="S4.p3.2.m2.2.2.cmml">v</mi><mo stretchy="false" id="S4.p3.2.m2.2.3.2.3" xref="S4.p3.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.2b"><interval closure="open" id="S4.p3.2.m2.2.3.1.cmml" xref="S4.p3.2.m2.2.3.2"><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">ğ‘¢</ci><ci id="S4.p3.2.m2.2.2.cmml" xref="S4.p3.2.m2.2.2">ğ‘£</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.2c">(u,v)</annotation></semantics></math> represents the projected pixel coordinates on the image plane. The parameters <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="f_{x}" display="inline"><semantics id="S4.p3.3.m3.1a"><msub id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml"><mi id="S4.p3.3.m3.1.1.2" xref="S4.p3.3.m3.1.1.2.cmml">f</mi><mi id="S4.p3.3.m3.1.1.3" xref="S4.p3.3.m3.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p3.3.m3.1.1.1.cmml" xref="S4.p3.3.m3.1.1">subscript</csymbol><ci id="S4.p3.3.m3.1.1.2.cmml" xref="S4.p3.3.m3.1.1.2">ğ‘“</ci><ci id="S4.p3.3.m3.1.1.3.cmml" xref="S4.p3.3.m3.1.1.3">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">f_{x}</annotation></semantics></math> and <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="f_{y}" display="inline"><semantics id="S4.p3.4.m4.1a"><msub id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml"><mi id="S4.p3.4.m4.1.1.2" xref="S4.p3.4.m4.1.1.2.cmml">f</mi><mi id="S4.p3.4.m4.1.1.3" xref="S4.p3.4.m4.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><apply id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p3.4.m4.1.1.1.cmml" xref="S4.p3.4.m4.1.1">subscript</csymbol><ci id="S4.p3.4.m4.1.1.2.cmml" xref="S4.p3.4.m4.1.1.2">ğ‘“</ci><ci id="S4.p3.4.m4.1.1.3.cmml" xref="S4.p3.4.m4.1.1.3">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">f_{y}</annotation></semantics></math> denote the focal lengths expressed in pixel units for the u and v axes. The values <math id="S4.p3.5.m5.1" class="ltx_Math" alttext="C_{x}" display="inline"><semantics id="S4.p3.5.m5.1a"><msub id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml"><mi id="S4.p3.5.m5.1.1.2" xref="S4.p3.5.m5.1.1.2.cmml">C</mi><mi id="S4.p3.5.m5.1.1.3" xref="S4.p3.5.m5.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.1b"><apply id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p3.5.m5.1.1.1.cmml" xref="S4.p3.5.m5.1.1">subscript</csymbol><ci id="S4.p3.5.m5.1.1.2.cmml" xref="S4.p3.5.m5.1.1.2">ğ¶</ci><ci id="S4.p3.5.m5.1.1.3.cmml" xref="S4.p3.5.m5.1.1.3">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.1c">C_{x}</annotation></semantics></math> and <math id="S4.p3.6.m6.1" class="ltx_Math" alttext="C_{y}" display="inline"><semantics id="S4.p3.6.m6.1a"><msub id="S4.p3.6.m6.1.1" xref="S4.p3.6.m6.1.1.cmml"><mi id="S4.p3.6.m6.1.1.2" xref="S4.p3.6.m6.1.1.2.cmml">C</mi><mi id="S4.p3.6.m6.1.1.3" xref="S4.p3.6.m6.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.6.m6.1b"><apply id="S4.p3.6.m6.1.1.cmml" xref="S4.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p3.6.m6.1.1.1.cmml" xref="S4.p3.6.m6.1.1">subscript</csymbol><ci id="S4.p3.6.m6.1.1.2.cmml" xref="S4.p3.6.m6.1.1.2">ğ¶</ci><ci id="S4.p3.6.m6.1.1.3.cmml" xref="S4.p3.6.m6.1.1.3">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.6.m6.1c">C_{y}</annotation></semantics></math> correspond to the principal point, which is the intersection point of the optical axis with the image plane.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">To determine the amount of pixel displacement caused by a change in depth, we differentiate Equation <a href="#S4.E1" title="In 4 Perspective-aware Convolution â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> with respect to <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="Z_{w}" display="inline"><semantics id="S4.p4.1.m1.1a"><msub id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mi id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml">Z</mi><mi id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1">subscript</csymbol><ci id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">ğ‘</ci><ci id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3">ğ‘¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">Z_{w}</annotation></semantics></math> and obtain:</p>
</div>
<div id="S4.p5" class="ltx_para">
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.2" class="ltx_Math" alttext="\begin{cases}\vspace{0.5em}\displaystyle\frac{du}{dZ_{w}}=-\frac{X_{w}f_{x}}{Z_{w}^{2}}\\
\displaystyle\frac{dv}{dZ_{w}}=-\frac{Y_{w}f_{y}}{Z_{w}^{2}}\\
\end{cases}" display="block"><semantics id="S4.E2.m1.2a"><mrow id="S4.E2.m1.2.2" xref="S4.E2.m1.2.3.1.cmml"><mo id="S4.E2.m1.2.2.3" xref="S4.E2.m1.2.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.E2.m1.2.2.2" xref="S4.E2.m1.2.3.1.cmml"><mtr id="S4.E2.m1.2.2.2a" xref="S4.E2.m1.2.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E2.m1.2.2.2b" xref="S4.E2.m1.2.3.1.cmml"><mrow id="S4.E2.m1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.cmml"><mfrac id="S4.E2.m1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.2.cmml"><mrow id="S4.E2.m1.1.1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.2.2.2" xref="S4.E2.m1.1.1.1.1.1.1.2.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.1.1.2.2.1" xref="S4.E2.m1.1.1.1.1.1.1.2.2.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.1.1.1.1.2.2.3" xref="S4.E2.m1.1.1.1.1.1.1.2.2.3.cmml">u</mi></mrow><mrow id="S4.E2.m1.1.1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.2.3.2" xref="S4.E2.m1.1.1.1.1.1.1.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.1.1.2.3.1" xref="S4.E2.m1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><msub id="S4.E2.m1.1.1.1.1.1.1.2.3.3" xref="S4.E2.m1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.2.3.3.2" xref="S4.E2.m1.1.1.1.1.1.1.2.3.3.2.cmml">Z</mi><mi id="S4.E2.m1.1.1.1.1.1.1.2.3.3.3" xref="S4.E2.m1.1.1.1.1.1.1.2.3.3.3.cmml">w</mi></msub></mrow></mfrac><mo id="S4.E2.m1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.3.cmml"><mo id="S4.E2.m1.1.1.1.1.1.1.3a" xref="S4.E2.m1.1.1.1.1.1.1.3.cmml">âˆ’</mo><mfrac id="S4.E2.m1.1.1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.1.1.3.2.cmml"><mrow id="S4.E2.m1.1.1.1.1.1.1.3.2.2" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.cmml"><msub id="S4.E2.m1.1.1.1.1.1.1.3.2.2.2" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.2.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.3.2.2.2.2" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.2.2.cmml">X</mi><mi id="S4.E2.m1.1.1.1.1.1.1.3.2.2.2.3" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.2.3.cmml">w</mi></msub><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.1.1.3.2.2.1" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.1.cmml">â€‹</mo><msub id="S4.E2.m1.1.1.1.1.1.1.3.2.2.3" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.3.2.2.3.2" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.3.2.cmml">f</mi><mi id="S4.E2.m1.1.1.1.1.1.1.3.2.2.3.3" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.3.3.cmml">x</mi></msub></mrow><msubsup id="S4.E2.m1.1.1.1.1.1.1.3.2.3" xref="S4.E2.m1.1.1.1.1.1.1.3.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.3.2.3.2.2" xref="S4.E2.m1.1.1.1.1.1.1.3.2.3.2.2.cmml">Z</mi><mi id="S4.E2.m1.1.1.1.1.1.1.3.2.3.2.3" xref="S4.E2.m1.1.1.1.1.1.1.3.2.3.2.3.cmml">w</mi><mn id="S4.E2.m1.1.1.1.1.1.1.3.2.3.3" xref="S4.E2.m1.1.1.1.1.1.1.3.2.3.3.cmml">2</mn></msubsup></mfrac></mrow></mrow></mtd><mtd id="S4.E2.m1.2.2.2c" xref="S4.E2.m1.2.3.1.1.cmml"></mtd></mtr><mtr id="S4.E2.m1.2.2.2d" xref="S4.E2.m1.2.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E2.m1.2.2.2e" xref="S4.E2.m1.2.3.1.cmml"><mrow id="S4.E2.m1.2.2.2.2.1.1" xref="S4.E2.m1.2.2.2.2.1.1.cmml"><mfrac id="S4.E2.m1.2.2.2.2.1.1.2" xref="S4.E2.m1.2.2.2.2.1.1.2.cmml"><mrow id="S4.E2.m1.2.2.2.2.1.1.2.2" xref="S4.E2.m1.2.2.2.2.1.1.2.2.cmml"><mi id="S4.E2.m1.2.2.2.2.1.1.2.2.2" xref="S4.E2.m1.2.2.2.2.1.1.2.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.2.2.2.2.1.1.2.2.1" xref="S4.E2.m1.2.2.2.2.1.1.2.2.1.cmml">â€‹</mo><mi id="S4.E2.m1.2.2.2.2.1.1.2.2.3" xref="S4.E2.m1.2.2.2.2.1.1.2.2.3.cmml">v</mi></mrow><mrow id="S4.E2.m1.2.2.2.2.1.1.2.3" xref="S4.E2.m1.2.2.2.2.1.1.2.3.cmml"><mi id="S4.E2.m1.2.2.2.2.1.1.2.3.2" xref="S4.E2.m1.2.2.2.2.1.1.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.2.2.2.2.1.1.2.3.1" xref="S4.E2.m1.2.2.2.2.1.1.2.3.1.cmml">â€‹</mo><msub id="S4.E2.m1.2.2.2.2.1.1.2.3.3" xref="S4.E2.m1.2.2.2.2.1.1.2.3.3.cmml"><mi id="S4.E2.m1.2.2.2.2.1.1.2.3.3.2" xref="S4.E2.m1.2.2.2.2.1.1.2.3.3.2.cmml">Z</mi><mi id="S4.E2.m1.2.2.2.2.1.1.2.3.3.3" xref="S4.E2.m1.2.2.2.2.1.1.2.3.3.3.cmml">w</mi></msub></mrow></mfrac><mo id="S4.E2.m1.2.2.2.2.1.1.1" xref="S4.E2.m1.2.2.2.2.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.2.2.2.2.1.1.3" xref="S4.E2.m1.2.2.2.2.1.1.3.cmml"><mo id="S4.E2.m1.2.2.2.2.1.1.3a" xref="S4.E2.m1.2.2.2.2.1.1.3.cmml">âˆ’</mo><mfrac id="S4.E2.m1.2.2.2.2.1.1.3.2" xref="S4.E2.m1.2.2.2.2.1.1.3.2.cmml"><mrow id="S4.E2.m1.2.2.2.2.1.1.3.2.2" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.cmml"><msub id="S4.E2.m1.2.2.2.2.1.1.3.2.2.2" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.2.cmml"><mi id="S4.E2.m1.2.2.2.2.1.1.3.2.2.2.2" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.2.2.cmml">Y</mi><mi id="S4.E2.m1.2.2.2.2.1.1.3.2.2.2.3" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.2.3.cmml">w</mi></msub><mo lspace="0em" rspace="0em" id="S4.E2.m1.2.2.2.2.1.1.3.2.2.1" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.1.cmml">â€‹</mo><msub id="S4.E2.m1.2.2.2.2.1.1.3.2.2.3" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.3.cmml"><mi id="S4.E2.m1.2.2.2.2.1.1.3.2.2.3.2" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.3.2.cmml">f</mi><mi id="S4.E2.m1.2.2.2.2.1.1.3.2.2.3.3" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.3.3.cmml">y</mi></msub></mrow><msubsup id="S4.E2.m1.2.2.2.2.1.1.3.2.3" xref="S4.E2.m1.2.2.2.2.1.1.3.2.3.cmml"><mi id="S4.E2.m1.2.2.2.2.1.1.3.2.3.2.2" xref="S4.E2.m1.2.2.2.2.1.1.3.2.3.2.2.cmml">Z</mi><mi id="S4.E2.m1.2.2.2.2.1.1.3.2.3.2.3" xref="S4.E2.m1.2.2.2.2.1.1.3.2.3.2.3.cmml">w</mi><mn id="S4.E2.m1.2.2.2.2.1.1.3.2.3.3" xref="S4.E2.m1.2.2.2.2.1.1.3.2.3.3.cmml">2</mn></msubsup></mfrac></mrow></mrow></mtd><mtd id="S4.E2.m1.2.2.2f" xref="S4.E2.m1.2.3.1.1.cmml"></mtd></mtr></mtable></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.2b"><apply id="S4.E2.m1.2.3.1.cmml" xref="S4.E2.m1.2.2"><csymbol cd="latexml" id="S4.E2.m1.2.3.1.1.cmml" xref="S4.E2.m1.2.2.3">cases</csymbol><apply id="S4.E2.m1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1"></eq><apply id="S4.E2.m1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2"><divide id="S4.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2"></divide><apply id="S4.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.2"><times id="S4.E2.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.2.1"></times><ci id="S4.E2.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.2.2">ğ‘‘</ci><ci id="S4.E2.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.2.3">ğ‘¢</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.3"><times id="S4.E2.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S4.E2.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.3.2">ğ‘‘</ci><apply id="S4.E2.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.3.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.3.3.2">ğ‘</ci><ci id="S4.E2.m1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.3.3.3">ğ‘¤</ci></apply></apply></apply><apply id="S4.E2.m1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3"><minus id="S4.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3"></minus><apply id="S4.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2"><divide id="S4.E2.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2"></divide><apply id="S4.E2.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2"><times id="S4.E2.m1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.1"></times><apply id="S4.E2.m1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.3.2.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.3.2.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.2.2">ğ‘‹</ci><ci id="S4.E2.m1.1.1.1.1.1.1.3.2.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.2.3">ğ‘¤</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.1.3.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.3.2.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.3.2.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.3.2">ğ‘“</ci><ci id="S4.E2.m1.1.1.1.1.1.1.3.2.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.2.3.3">ğ‘¥</ci></apply></apply><apply id="S4.E2.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.3">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.3.2.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.3.2.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.3.2.2">ğ‘</ci><ci id="S4.E2.m1.1.1.1.1.1.1.3.2.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.3.2.3">ğ‘¤</ci></apply><cn type="integer" id="S4.E2.m1.1.1.1.1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2.3.3">2</cn></apply></apply></apply></apply><ci id="S4.E2.m1.2.3.1.3a.cmml" xref="S4.E2.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S4.E2.m1.2.3.1.3.cmml" xref="S4.E2.m1.2.2.3">otherwise</mtext></ci><apply id="S4.E2.m1.2.2.2.2.1.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1"><eq id="S4.E2.m1.2.2.2.2.1.1.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.1"></eq><apply id="S4.E2.m1.2.2.2.2.1.1.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2"><divide id="S4.E2.m1.2.2.2.2.1.1.2.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2"></divide><apply id="S4.E2.m1.2.2.2.2.1.1.2.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2.2"><times id="S4.E2.m1.2.2.2.2.1.1.2.2.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2.2.1"></times><ci id="S4.E2.m1.2.2.2.2.1.1.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2.2.2">ğ‘‘</ci><ci id="S4.E2.m1.2.2.2.2.1.1.2.2.3.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2.2.3">ğ‘£</ci></apply><apply id="S4.E2.m1.2.2.2.2.1.1.2.3.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2.3"><times id="S4.E2.m1.2.2.2.2.1.1.2.3.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2.3.1"></times><ci id="S4.E2.m1.2.2.2.2.1.1.2.3.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2.3.2">ğ‘‘</ci><apply id="S4.E2.m1.2.2.2.2.1.1.2.3.3.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.2.2.1.1.2.3.3.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2.3.3">subscript</csymbol><ci id="S4.E2.m1.2.2.2.2.1.1.2.3.3.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2.3.3.2">ğ‘</ci><ci id="S4.E2.m1.2.2.2.2.1.1.2.3.3.3.cmml" xref="S4.E2.m1.2.2.2.2.1.1.2.3.3.3">ğ‘¤</ci></apply></apply></apply><apply id="S4.E2.m1.2.2.2.2.1.1.3.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3"><minus id="S4.E2.m1.2.2.2.2.1.1.3.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3"></minus><apply id="S4.E2.m1.2.2.2.2.1.1.3.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2"><divide id="S4.E2.m1.2.2.2.2.1.1.3.2.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2"></divide><apply id="S4.E2.m1.2.2.2.2.1.1.3.2.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2"><times id="S4.E2.m1.2.2.2.2.1.1.3.2.2.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.1"></times><apply id="S4.E2.m1.2.2.2.2.1.1.3.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.2.2.1.1.3.2.2.2.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.2">subscript</csymbol><ci id="S4.E2.m1.2.2.2.2.1.1.3.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.2.2">ğ‘Œ</ci><ci id="S4.E2.m1.2.2.2.2.1.1.3.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.2.3">ğ‘¤</ci></apply><apply id="S4.E2.m1.2.2.2.2.1.1.3.2.2.3.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.2.2.1.1.3.2.2.3.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.3">subscript</csymbol><ci id="S4.E2.m1.2.2.2.2.1.1.3.2.2.3.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.3.2">ğ‘“</ci><ci id="S4.E2.m1.2.2.2.2.1.1.3.2.2.3.3.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.2.3.3">ğ‘¦</ci></apply></apply><apply id="S4.E2.m1.2.2.2.2.1.1.3.2.3.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.2.2.1.1.3.2.3.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.3">superscript</csymbol><apply id="S4.E2.m1.2.2.2.2.1.1.3.2.3.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.2.2.1.1.3.2.3.2.1.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.3">subscript</csymbol><ci id="S4.E2.m1.2.2.2.2.1.1.3.2.3.2.2.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.3.2.2">ğ‘</ci><ci id="S4.E2.m1.2.2.2.2.1.1.3.2.3.2.3.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.3.2.3">ğ‘¤</ci></apply><cn type="integer" id="S4.E2.m1.2.2.2.2.1.1.3.2.3.3.cmml" xref="S4.E2.m1.2.2.2.2.1.1.3.2.3.3">2</cn></apply></apply></apply></apply><ci id="S4.E2.m1.2.3.1.5a.cmml" xref="S4.E2.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S4.E2.m1.2.3.1.5.cmml" xref="S4.E2.m1.2.2.3">otherwise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.2c">\begin{cases}\vspace{0.5em}\displaystyle\frac{du}{dZ_{w}}=-\frac{X_{w}f_{x}}{Z_{w}^{2}}\\
\displaystyle\frac{dv}{dZ_{w}}=-\frac{Y_{w}f_{y}}{Z_{w}^{2}}\\
\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.5" class="ltx_p">where <math id="S4.p6.1.m1.3" class="ltx_Math" alttext="(X_{w},Y_{w},Z_{w})" display="inline"><semantics id="S4.p6.1.m1.3a"><mrow id="S4.p6.1.m1.3.3.3" xref="S4.p6.1.m1.3.3.4.cmml"><mo stretchy="false" id="S4.p6.1.m1.3.3.3.4" xref="S4.p6.1.m1.3.3.4.cmml">(</mo><msub id="S4.p6.1.m1.1.1.1.1" xref="S4.p6.1.m1.1.1.1.1.cmml"><mi id="S4.p6.1.m1.1.1.1.1.2" xref="S4.p6.1.m1.1.1.1.1.2.cmml">X</mi><mi id="S4.p6.1.m1.1.1.1.1.3" xref="S4.p6.1.m1.1.1.1.1.3.cmml">w</mi></msub><mo id="S4.p6.1.m1.3.3.3.5" xref="S4.p6.1.m1.3.3.4.cmml">,</mo><msub id="S4.p6.1.m1.2.2.2.2" xref="S4.p6.1.m1.2.2.2.2.cmml"><mi id="S4.p6.1.m1.2.2.2.2.2" xref="S4.p6.1.m1.2.2.2.2.2.cmml">Y</mi><mi id="S4.p6.1.m1.2.2.2.2.3" xref="S4.p6.1.m1.2.2.2.2.3.cmml">w</mi></msub><mo id="S4.p6.1.m1.3.3.3.6" xref="S4.p6.1.m1.3.3.4.cmml">,</mo><msub id="S4.p6.1.m1.3.3.3.3" xref="S4.p6.1.m1.3.3.3.3.cmml"><mi id="S4.p6.1.m1.3.3.3.3.2" xref="S4.p6.1.m1.3.3.3.3.2.cmml">Z</mi><mi id="S4.p6.1.m1.3.3.3.3.3" xref="S4.p6.1.m1.3.3.3.3.3.cmml">w</mi></msub><mo stretchy="false" id="S4.p6.1.m1.3.3.3.7" xref="S4.p6.1.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.3b"><vector id="S4.p6.1.m1.3.3.4.cmml" xref="S4.p6.1.m1.3.3.3"><apply id="S4.p6.1.m1.1.1.1.1.cmml" xref="S4.p6.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p6.1.m1.1.1.1.1.1.cmml" xref="S4.p6.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.p6.1.m1.1.1.1.1.2.cmml" xref="S4.p6.1.m1.1.1.1.1.2">ğ‘‹</ci><ci id="S4.p6.1.m1.1.1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.1.1.3">ğ‘¤</ci></apply><apply id="S4.p6.1.m1.2.2.2.2.cmml" xref="S4.p6.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.p6.1.m1.2.2.2.2.1.cmml" xref="S4.p6.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.p6.1.m1.2.2.2.2.2.cmml" xref="S4.p6.1.m1.2.2.2.2.2">ğ‘Œ</ci><ci id="S4.p6.1.m1.2.2.2.2.3.cmml" xref="S4.p6.1.m1.2.2.2.2.3">ğ‘¤</ci></apply><apply id="S4.p6.1.m1.3.3.3.3.cmml" xref="S4.p6.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S4.p6.1.m1.3.3.3.3.1.cmml" xref="S4.p6.1.m1.3.3.3.3">subscript</csymbol><ci id="S4.p6.1.m1.3.3.3.3.2.cmml" xref="S4.p6.1.m1.3.3.3.3.2">ğ‘</ci><ci id="S4.p6.1.m1.3.3.3.3.3.cmml" xref="S4.p6.1.m1.3.3.3.3.3">ğ‘¤</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.3c">(X_{w},Y_{w},Z_{w})</annotation></semantics></math> represents a 3D point in camera coordinates, which is inversely projected from a pixel <math id="S4.p6.2.m2.2" class="ltx_Math" alttext="(u,v)" display="inline"><semantics id="S4.p6.2.m2.2a"><mrow id="S4.p6.2.m2.2.3.2" xref="S4.p6.2.m2.2.3.1.cmml"><mo stretchy="false" id="S4.p6.2.m2.2.3.2.1" xref="S4.p6.2.m2.2.3.1.cmml">(</mo><mi id="S4.p6.2.m2.1.1" xref="S4.p6.2.m2.1.1.cmml">u</mi><mo id="S4.p6.2.m2.2.3.2.2" xref="S4.p6.2.m2.2.3.1.cmml">,</mo><mi id="S4.p6.2.m2.2.2" xref="S4.p6.2.m2.2.2.cmml">v</mi><mo stretchy="false" id="S4.p6.2.m2.2.3.2.3" xref="S4.p6.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.2.m2.2b"><interval closure="open" id="S4.p6.2.m2.2.3.1.cmml" xref="S4.p6.2.m2.2.3.2"><ci id="S4.p6.2.m2.1.1.cmml" xref="S4.p6.2.m2.1.1">ğ‘¢</ci><ci id="S4.p6.2.m2.2.2.cmml" xref="S4.p6.2.m2.2.2">ğ‘£</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.2.m2.2c">(u,v)</annotation></semantics></math>. Since the depth of each pixel is unknown, we assume that all image pixels lie on the ground plane and set <math id="S4.p6.3.m3.1" class="ltx_Math" alttext="Y_{w}" display="inline"><semantics id="S4.p6.3.m3.1a"><msub id="S4.p6.3.m3.1.1" xref="S4.p6.3.m3.1.1.cmml"><mi id="S4.p6.3.m3.1.1.2" xref="S4.p6.3.m3.1.1.2.cmml">Y</mi><mi id="S4.p6.3.m3.1.1.3" xref="S4.p6.3.m3.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p6.3.m3.1b"><apply id="S4.p6.3.m3.1.1.cmml" xref="S4.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p6.3.m3.1.1.1.cmml" xref="S4.p6.3.m3.1.1">subscript</csymbol><ci id="S4.p6.3.m3.1.1.2.cmml" xref="S4.p6.3.m3.1.1.2">ğ‘Œ</ci><ci id="S4.p6.3.m3.1.1.3.cmml" xref="S4.p6.3.m3.1.1.3">ğ‘¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.3.m3.1c">Y_{w}</annotation></semantics></math> equal to the height of the ground plane, denoted as <math id="S4.p6.4.m4.1" class="ltx_Math" alttext="Y_{0}" display="inline"><semantics id="S4.p6.4.m4.1a"><msub id="S4.p6.4.m4.1.1" xref="S4.p6.4.m4.1.1.cmml"><mi id="S4.p6.4.m4.1.1.2" xref="S4.p6.4.m4.1.1.2.cmml">Y</mi><mn id="S4.p6.4.m4.1.1.3" xref="S4.p6.4.m4.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p6.4.m4.1b"><apply id="S4.p6.4.m4.1.1.cmml" xref="S4.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p6.4.m4.1.1.1.cmml" xref="S4.p6.4.m4.1.1">subscript</csymbol><ci id="S4.p6.4.m4.1.1.2.cmml" xref="S4.p6.4.m4.1.1.2">ğ‘Œ</ci><cn type="integer" id="S4.p6.4.m4.1.1.3.cmml" xref="S4.p6.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.4.m4.1c">Y_{0}</annotation></semantics></math>. By applying Equation <a href="#S4.E1" title="In 4 Perspective-aware Convolution â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can inversely project <math id="S4.p6.5.m5.2" class="ltx_Math" alttext="(u,v)" display="inline"><semantics id="S4.p6.5.m5.2a"><mrow id="S4.p6.5.m5.2.3.2" xref="S4.p6.5.m5.2.3.1.cmml"><mo stretchy="false" id="S4.p6.5.m5.2.3.2.1" xref="S4.p6.5.m5.2.3.1.cmml">(</mo><mi id="S4.p6.5.m5.1.1" xref="S4.p6.5.m5.1.1.cmml">u</mi><mo id="S4.p6.5.m5.2.3.2.2" xref="S4.p6.5.m5.2.3.1.cmml">,</mo><mi id="S4.p6.5.m5.2.2" xref="S4.p6.5.m5.2.2.cmml">v</mi><mo stretchy="false" id="S4.p6.5.m5.2.3.2.3" xref="S4.p6.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.5.m5.2b"><interval closure="open" id="S4.p6.5.m5.2.3.1.cmml" xref="S4.p6.5.m5.2.3.2"><ci id="S4.p6.5.m5.1.1.cmml" xref="S4.p6.5.m5.1.1">ğ‘¢</ci><ci id="S4.p6.5.m5.2.2.cmml" xref="S4.p6.5.m5.2.2">ğ‘£</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.5.m5.2c">(u,v)</annotation></semantics></math> back to the camera coordinate system and the inversely projection equation is as following:</p>
</div>
<div id="S4.p7" class="ltx_para">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.3" class="ltx_Math" alttext="\begin{cases}\vspace{0.7em}\displaystyle X_{0}=\frac{(u_{0}-C_{x})Y_{0}f_{y}}{(v_{0}-C_{y})f_{x}}\\
\displaystyle Y_{0}=Y_{0}\\
\displaystyle Z_{0}=\frac{Y_{0}f_{y}}{v_{0}-C_{y}}\\
\end{cases}" display="block"><semantics id="S4.E3.m1.3a"><mrow id="S4.E3.m1.3.3" xref="S4.E3.m1.3.4.1.cmml"><mo id="S4.E3.m1.3.3.4" xref="S4.E3.m1.3.4.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.E3.m1.3.3.3" xref="S4.E3.m1.3.4.1.cmml"><mtr id="S4.E3.m1.3.3.3a" xref="S4.E3.m1.3.4.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E3.m1.3.3.3b" xref="S4.E3.m1.3.4.1.cmml"><mrow id="S4.E3.m1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.cmml"><msub id="S4.E3.m1.1.1.1.1.1.1.4" xref="S4.E3.m1.1.1.1.1.1.1.4.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.4.2" xref="S4.E3.m1.1.1.1.1.1.1.4.2.cmml">X</mi><mn id="S4.E3.m1.1.1.1.1.1.1.4.3" xref="S4.E3.m1.1.1.1.1.1.1.4.3.cmml">0</mn></msub><mo id="S4.E3.m1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.3.cmml">=</mo><mfrac id="S4.E3.m1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.2.cmml"><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">u</mi><mn id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">C</mi><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">x</mi></msub></mrow><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><msub id="S4.E3.m1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml">Y</mi><mn id="S4.E3.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml">0</mn></msub><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.1.1.1.1.1.1.2a" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><msub id="S4.E3.m1.1.1.1.1.1.1.1.1.4" xref="S4.E3.m1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.4.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.4.2.cmml">f</mi><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.4.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.4.3.cmml">y</mi></msub></mrow><mrow id="S4.E3.m1.1.1.1.1.1.1.2.2" xref="S4.E3.m1.1.1.1.1.1.1.2.2.cmml"><mrow id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.cmml"><msub id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2.2" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2.2.cmml">v</mi><mn id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2.3" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3.2" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3.2.cmml">C</mi><mi id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3.3" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3.3.cmml">y</mi></msub></mrow><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.1.1.1.1.2.2.2" xref="S4.E3.m1.1.1.1.1.1.1.2.2.2.cmml">â€‹</mo><msub id="S4.E3.m1.1.1.1.1.1.1.2.2.3" xref="S4.E3.m1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.2.2.3.2" xref="S4.E3.m1.1.1.1.1.1.1.2.2.3.2.cmml">f</mi><mi id="S4.E3.m1.1.1.1.1.1.1.2.2.3.3" xref="S4.E3.m1.1.1.1.1.1.1.2.2.3.3.cmml">x</mi></msub></mrow></mfrac></mrow></mtd><mtd id="S4.E3.m1.3.3.3c" xref="S4.E3.m1.3.4.1.1.cmml"></mtd></mtr><mtr id="S4.E3.m1.3.3.3d" xref="S4.E3.m1.3.4.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E3.m1.3.3.3e" xref="S4.E3.m1.3.4.1.cmml"><mrow id="S4.E3.m1.2.2.2.2.1.1" xref="S4.E3.m1.2.2.2.2.1.1.cmml"><msub id="S4.E3.m1.2.2.2.2.1.1.2" xref="S4.E3.m1.2.2.2.2.1.1.2.cmml"><mi id="S4.E3.m1.2.2.2.2.1.1.2.2" xref="S4.E3.m1.2.2.2.2.1.1.2.2.cmml">Y</mi><mn id="S4.E3.m1.2.2.2.2.1.1.2.3" xref="S4.E3.m1.2.2.2.2.1.1.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.2.2.2.2.1.1.1" xref="S4.E3.m1.2.2.2.2.1.1.1.cmml">=</mo><msub id="S4.E3.m1.2.2.2.2.1.1.3" xref="S4.E3.m1.2.2.2.2.1.1.3.cmml"><mi id="S4.E3.m1.2.2.2.2.1.1.3.2" xref="S4.E3.m1.2.2.2.2.1.1.3.2.cmml">Y</mi><mn id="S4.E3.m1.2.2.2.2.1.1.3.3" xref="S4.E3.m1.2.2.2.2.1.1.3.3.cmml">0</mn></msub></mrow></mtd><mtd id="S4.E3.m1.3.3.3f" xref="S4.E3.m1.3.4.1.1.cmml"></mtd></mtr><mtr id="S4.E3.m1.3.3.3g" xref="S4.E3.m1.3.4.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E3.m1.3.3.3h" xref="S4.E3.m1.3.4.1.cmml"><mrow id="S4.E3.m1.3.3.3.3.1.1" xref="S4.E3.m1.3.3.3.3.1.1.cmml"><msub id="S4.E3.m1.3.3.3.3.1.1.2" xref="S4.E3.m1.3.3.3.3.1.1.2.cmml"><mi id="S4.E3.m1.3.3.3.3.1.1.2.2" xref="S4.E3.m1.3.3.3.3.1.1.2.2.cmml">Z</mi><mn id="S4.E3.m1.3.3.3.3.1.1.2.3" xref="S4.E3.m1.3.3.3.3.1.1.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.3.3.3.3.1.1.1" xref="S4.E3.m1.3.3.3.3.1.1.1.cmml">=</mo><mfrac id="S4.E3.m1.3.3.3.3.1.1.3" xref="S4.E3.m1.3.3.3.3.1.1.3.cmml"><mrow id="S4.E3.m1.3.3.3.3.1.1.3.2" xref="S4.E3.m1.3.3.3.3.1.1.3.2.cmml"><msub id="S4.E3.m1.3.3.3.3.1.1.3.2.2" xref="S4.E3.m1.3.3.3.3.1.1.3.2.2.cmml"><mi id="S4.E3.m1.3.3.3.3.1.1.3.2.2.2" xref="S4.E3.m1.3.3.3.3.1.1.3.2.2.2.cmml">Y</mi><mn id="S4.E3.m1.3.3.3.3.1.1.3.2.2.3" xref="S4.E3.m1.3.3.3.3.1.1.3.2.2.3.cmml">0</mn></msub><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.3.3.1.1.3.2.1" xref="S4.E3.m1.3.3.3.3.1.1.3.2.1.cmml">â€‹</mo><msub id="S4.E3.m1.3.3.3.3.1.1.3.2.3" xref="S4.E3.m1.3.3.3.3.1.1.3.2.3.cmml"><mi id="S4.E3.m1.3.3.3.3.1.1.3.2.3.2" xref="S4.E3.m1.3.3.3.3.1.1.3.2.3.2.cmml">f</mi><mi id="S4.E3.m1.3.3.3.3.1.1.3.2.3.3" xref="S4.E3.m1.3.3.3.3.1.1.3.2.3.3.cmml">y</mi></msub></mrow><mrow id="S4.E3.m1.3.3.3.3.1.1.3.3" xref="S4.E3.m1.3.3.3.3.1.1.3.3.cmml"><msub id="S4.E3.m1.3.3.3.3.1.1.3.3.2" xref="S4.E3.m1.3.3.3.3.1.1.3.3.2.cmml"><mi id="S4.E3.m1.3.3.3.3.1.1.3.3.2.2" xref="S4.E3.m1.3.3.3.3.1.1.3.3.2.2.cmml">v</mi><mn id="S4.E3.m1.3.3.3.3.1.1.3.3.2.3" xref="S4.E3.m1.3.3.3.3.1.1.3.3.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.3.3.3.3.1.1.3.3.1" xref="S4.E3.m1.3.3.3.3.1.1.3.3.1.cmml">âˆ’</mo><msub id="S4.E3.m1.3.3.3.3.1.1.3.3.3" xref="S4.E3.m1.3.3.3.3.1.1.3.3.3.cmml"><mi id="S4.E3.m1.3.3.3.3.1.1.3.3.3.2" xref="S4.E3.m1.3.3.3.3.1.1.3.3.3.2.cmml">C</mi><mi id="S4.E3.m1.3.3.3.3.1.1.3.3.3.3" xref="S4.E3.m1.3.3.3.3.1.1.3.3.3.3.cmml">y</mi></msub></mrow></mfrac></mrow></mtd><mtd id="S4.E3.m1.3.3.3i" xref="S4.E3.m1.3.4.1.1.cmml"></mtd></mtr></mtable></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.3b"><apply id="S4.E3.m1.3.4.1.cmml" xref="S4.E3.m1.3.3"><csymbol cd="latexml" id="S4.E3.m1.3.4.1.1.cmml" xref="S4.E3.m1.3.3.4">cases</csymbol><apply id="S4.E3.m1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1"><eq id="S4.E3.m1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3"></eq><apply id="S4.E3.m1.1.1.1.1.1.1.4.cmml" xref="S4.E3.m1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.4.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.4.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.4.2">ğ‘‹</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.1.1.4.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.4.3">0</cn></apply><apply id="S4.E3.m1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2"><divide id="S4.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2"></divide><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1"><times id="S4.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2"></times><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1"><minus id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ‘¢</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.3">0</cn></apply><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ¶</ci><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘¥</ci></apply></apply><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.2">ğ‘Œ</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.3">0</cn></apply><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.4.2">ğ‘“</ci><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.4.3">ğ‘¦</ci></apply></apply><apply id="S4.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2"><times id="S4.E3.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.2"></times><apply id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1"><minus id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.1"></minus><apply id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2.2">ğ‘£</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.2.3">0</cn></apply><apply id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3.2">ğ¶</ci><ci id="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.1.1.1.3.3">ğ‘¦</ci></apply></apply><apply id="S4.E3.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.3">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.3.2">ğ‘“</ci><ci id="S4.E3.m1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2.3.3">ğ‘¥</ci></apply></apply></apply></apply><ci id="S4.E3.m1.3.4.1.3a.cmml" xref="S4.E3.m1.3.3"><mtext class="ltx_mathvariant_italic" id="S4.E3.m1.3.4.1.3.cmml" xref="S4.E3.m1.3.3.4">otherwise</mtext></ci><apply id="S4.E3.m1.2.2.2.2.1.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1"><eq id="S4.E3.m1.2.2.2.2.1.1.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.1"></eq><apply id="S4.E3.m1.2.2.2.2.1.1.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.1.1.2.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S4.E3.m1.2.2.2.2.1.1.2.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.2.2">ğ‘Œ</ci><cn type="integer" id="S4.E3.m1.2.2.2.2.1.1.2.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.2.3">0</cn></apply><apply id="S4.E3.m1.2.2.2.2.1.1.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.1.1.3.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S4.E3.m1.2.2.2.2.1.1.3.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.2">ğ‘Œ</ci><cn type="integer" id="S4.E3.m1.2.2.2.2.1.1.3.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3">0</cn></apply></apply><ci id="S4.E3.m1.3.4.1.5a.cmml" xref="S4.E3.m1.3.3"><mtext class="ltx_mathvariant_italic" id="S4.E3.m1.3.4.1.5.cmml" xref="S4.E3.m1.3.3.4">otherwise</mtext></ci><apply id="S4.E3.m1.3.3.3.3.1.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1"><eq id="S4.E3.m1.3.3.3.3.1.1.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1"></eq><apply id="S4.E3.m1.3.3.3.3.1.1.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.3.3.1.1.2.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.2">subscript</csymbol><ci id="S4.E3.m1.3.3.3.3.1.1.2.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.2.2">ğ‘</ci><cn type="integer" id="S4.E3.m1.3.3.3.3.1.1.2.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.2.3">0</cn></apply><apply id="S4.E3.m1.3.3.3.3.1.1.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3"><divide id="S4.E3.m1.3.3.3.3.1.1.3.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3"></divide><apply id="S4.E3.m1.3.3.3.3.1.1.3.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.2"><times id="S4.E3.m1.3.3.3.3.1.1.3.2.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.2.1"></times><apply id="S4.E3.m1.3.3.3.3.1.1.3.2.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.3.3.1.1.3.2.2.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.2.2">subscript</csymbol><ci id="S4.E3.m1.3.3.3.3.1.1.3.2.2.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.2.2.2">ğ‘Œ</ci><cn type="integer" id="S4.E3.m1.3.3.3.3.1.1.3.2.2.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.2.2.3">0</cn></apply><apply id="S4.E3.m1.3.3.3.3.1.1.3.2.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.3.3.1.1.3.2.3.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.2.3">subscript</csymbol><ci id="S4.E3.m1.3.3.3.3.1.1.3.2.3.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.2.3.2">ğ‘“</ci><ci id="S4.E3.m1.3.3.3.3.1.1.3.2.3.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.2.3.3">ğ‘¦</ci></apply></apply><apply id="S4.E3.m1.3.3.3.3.1.1.3.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.3"><minus id="S4.E3.m1.3.3.3.3.1.1.3.3.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.3.1"></minus><apply id="S4.E3.m1.3.3.3.3.1.1.3.3.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.3.3.1.1.3.3.2.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.3.2">subscript</csymbol><ci id="S4.E3.m1.3.3.3.3.1.1.3.3.2.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.3.2.2">ğ‘£</ci><cn type="integer" id="S4.E3.m1.3.3.3.3.1.1.3.3.2.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.3.2.3">0</cn></apply><apply id="S4.E3.m1.3.3.3.3.1.1.3.3.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.3.3.1.1.3.3.3.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.3.3">subscript</csymbol><ci id="S4.E3.m1.3.3.3.3.1.1.3.3.3.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.3.3.2">ğ¶</ci><ci id="S4.E3.m1.3.3.3.3.1.1.3.3.3.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.3.3.3.3">ğ‘¦</ci></apply></apply></apply></apply><ci id="S4.E3.m1.3.4.1.7a.cmml" xref="S4.E3.m1.3.3"><mtext class="ltx_mathvariant_italic" id="S4.E3.m1.3.4.1.7.cmml" xref="S4.E3.m1.3.3.4">otherwise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.3c">\begin{cases}\vspace{0.7em}\displaystyle X_{0}=\frac{(u_{0}-C_{x})Y_{0}f_{y}}{(v_{0}-C_{y})f_{x}}\\
\displaystyle Y_{0}=Y_{0}\\
\displaystyle Z_{0}=\frac{Y_{0}f_{y}}{v_{0}-C_{y}}\\
\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.5" class="ltx_p">Here, <math id="S4.p8.1.m1.2" class="ltx_Math" alttext="(u_{0},v_{0})" display="inline"><semantics id="S4.p8.1.m1.2a"><mrow id="S4.p8.1.m1.2.2.2" xref="S4.p8.1.m1.2.2.3.cmml"><mo stretchy="false" id="S4.p8.1.m1.2.2.2.3" xref="S4.p8.1.m1.2.2.3.cmml">(</mo><msub id="S4.p8.1.m1.1.1.1.1" xref="S4.p8.1.m1.1.1.1.1.cmml"><mi id="S4.p8.1.m1.1.1.1.1.2" xref="S4.p8.1.m1.1.1.1.1.2.cmml">u</mi><mn id="S4.p8.1.m1.1.1.1.1.3" xref="S4.p8.1.m1.1.1.1.1.3.cmml">0</mn></msub><mo id="S4.p8.1.m1.2.2.2.4" xref="S4.p8.1.m1.2.2.3.cmml">,</mo><msub id="S4.p8.1.m1.2.2.2.2" xref="S4.p8.1.m1.2.2.2.2.cmml"><mi id="S4.p8.1.m1.2.2.2.2.2" xref="S4.p8.1.m1.2.2.2.2.2.cmml">v</mi><mn id="S4.p8.1.m1.2.2.2.2.3" xref="S4.p8.1.m1.2.2.2.2.3.cmml">0</mn></msub><mo stretchy="false" id="S4.p8.1.m1.2.2.2.5" xref="S4.p8.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.1.m1.2b"><interval closure="open" id="S4.p8.1.m1.2.2.3.cmml" xref="S4.p8.1.m1.2.2.2"><apply id="S4.p8.1.m1.1.1.1.1.cmml" xref="S4.p8.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p8.1.m1.1.1.1.1.1.cmml" xref="S4.p8.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.p8.1.m1.1.1.1.1.2.cmml" xref="S4.p8.1.m1.1.1.1.1.2">ğ‘¢</ci><cn type="integer" id="S4.p8.1.m1.1.1.1.1.3.cmml" xref="S4.p8.1.m1.1.1.1.1.3">0</cn></apply><apply id="S4.p8.1.m1.2.2.2.2.cmml" xref="S4.p8.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.p8.1.m1.2.2.2.2.1.cmml" xref="S4.p8.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.p8.1.m1.2.2.2.2.2.cmml" xref="S4.p8.1.m1.2.2.2.2.2">ğ‘£</ci><cn type="integer" id="S4.p8.1.m1.2.2.2.2.3.cmml" xref="S4.p8.1.m1.2.2.2.2.3">0</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.1.m1.2c">(u_{0},v_{0})</annotation></semantics></math> represents a specific pixel on the image, and <math id="S4.p8.2.m2.3" class="ltx_Math" alttext="(X_{0},Y_{0},Z_{0})" display="inline"><semantics id="S4.p8.2.m2.3a"><mrow id="S4.p8.2.m2.3.3.3" xref="S4.p8.2.m2.3.3.4.cmml"><mo stretchy="false" id="S4.p8.2.m2.3.3.3.4" xref="S4.p8.2.m2.3.3.4.cmml">(</mo><msub id="S4.p8.2.m2.1.1.1.1" xref="S4.p8.2.m2.1.1.1.1.cmml"><mi id="S4.p8.2.m2.1.1.1.1.2" xref="S4.p8.2.m2.1.1.1.1.2.cmml">X</mi><mn id="S4.p8.2.m2.1.1.1.1.3" xref="S4.p8.2.m2.1.1.1.1.3.cmml">0</mn></msub><mo id="S4.p8.2.m2.3.3.3.5" xref="S4.p8.2.m2.3.3.4.cmml">,</mo><msub id="S4.p8.2.m2.2.2.2.2" xref="S4.p8.2.m2.2.2.2.2.cmml"><mi id="S4.p8.2.m2.2.2.2.2.2" xref="S4.p8.2.m2.2.2.2.2.2.cmml">Y</mi><mn id="S4.p8.2.m2.2.2.2.2.3" xref="S4.p8.2.m2.2.2.2.2.3.cmml">0</mn></msub><mo id="S4.p8.2.m2.3.3.3.6" xref="S4.p8.2.m2.3.3.4.cmml">,</mo><msub id="S4.p8.2.m2.3.3.3.3" xref="S4.p8.2.m2.3.3.3.3.cmml"><mi id="S4.p8.2.m2.3.3.3.3.2" xref="S4.p8.2.m2.3.3.3.3.2.cmml">Z</mi><mn id="S4.p8.2.m2.3.3.3.3.3" xref="S4.p8.2.m2.3.3.3.3.3.cmml">0</mn></msub><mo stretchy="false" id="S4.p8.2.m2.3.3.3.7" xref="S4.p8.2.m2.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.2.m2.3b"><vector id="S4.p8.2.m2.3.3.4.cmml" xref="S4.p8.2.m2.3.3.3"><apply id="S4.p8.2.m2.1.1.1.1.cmml" xref="S4.p8.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.p8.2.m2.1.1.1.1.1.cmml" xref="S4.p8.2.m2.1.1.1.1">subscript</csymbol><ci id="S4.p8.2.m2.1.1.1.1.2.cmml" xref="S4.p8.2.m2.1.1.1.1.2">ğ‘‹</ci><cn type="integer" id="S4.p8.2.m2.1.1.1.1.3.cmml" xref="S4.p8.2.m2.1.1.1.1.3">0</cn></apply><apply id="S4.p8.2.m2.2.2.2.2.cmml" xref="S4.p8.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S4.p8.2.m2.2.2.2.2.1.cmml" xref="S4.p8.2.m2.2.2.2.2">subscript</csymbol><ci id="S4.p8.2.m2.2.2.2.2.2.cmml" xref="S4.p8.2.m2.2.2.2.2.2">ğ‘Œ</ci><cn type="integer" id="S4.p8.2.m2.2.2.2.2.3.cmml" xref="S4.p8.2.m2.2.2.2.2.3">0</cn></apply><apply id="S4.p8.2.m2.3.3.3.3.cmml" xref="S4.p8.2.m2.3.3.3.3"><csymbol cd="ambiguous" id="S4.p8.2.m2.3.3.3.3.1.cmml" xref="S4.p8.2.m2.3.3.3.3">subscript</csymbol><ci id="S4.p8.2.m2.3.3.3.3.2.cmml" xref="S4.p8.2.m2.3.3.3.3.2">ğ‘</ci><cn type="integer" id="S4.p8.2.m2.3.3.3.3.3.cmml" xref="S4.p8.2.m2.3.3.3.3.3">0</cn></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.2.m2.3c">(X_{0},Y_{0},Z_{0})</annotation></semantics></math> represents the inversely projected 3D point in camera coordinates. By substituting <math id="S4.p8.3.m3.3" class="ltx_Math" alttext="(X_{0},Y_{0},Z_{0})" display="inline"><semantics id="S4.p8.3.m3.3a"><mrow id="S4.p8.3.m3.3.3.3" xref="S4.p8.3.m3.3.3.4.cmml"><mo stretchy="false" id="S4.p8.3.m3.3.3.3.4" xref="S4.p8.3.m3.3.3.4.cmml">(</mo><msub id="S4.p8.3.m3.1.1.1.1" xref="S4.p8.3.m3.1.1.1.1.cmml"><mi id="S4.p8.3.m3.1.1.1.1.2" xref="S4.p8.3.m3.1.1.1.1.2.cmml">X</mi><mn id="S4.p8.3.m3.1.1.1.1.3" xref="S4.p8.3.m3.1.1.1.1.3.cmml">0</mn></msub><mo id="S4.p8.3.m3.3.3.3.5" xref="S4.p8.3.m3.3.3.4.cmml">,</mo><msub id="S4.p8.3.m3.2.2.2.2" xref="S4.p8.3.m3.2.2.2.2.cmml"><mi id="S4.p8.3.m3.2.2.2.2.2" xref="S4.p8.3.m3.2.2.2.2.2.cmml">Y</mi><mn id="S4.p8.3.m3.2.2.2.2.3" xref="S4.p8.3.m3.2.2.2.2.3.cmml">0</mn></msub><mo id="S4.p8.3.m3.3.3.3.6" xref="S4.p8.3.m3.3.3.4.cmml">,</mo><msub id="S4.p8.3.m3.3.3.3.3" xref="S4.p8.3.m3.3.3.3.3.cmml"><mi id="S4.p8.3.m3.3.3.3.3.2" xref="S4.p8.3.m3.3.3.3.3.2.cmml">Z</mi><mn id="S4.p8.3.m3.3.3.3.3.3" xref="S4.p8.3.m3.3.3.3.3.3.cmml">0</mn></msub><mo stretchy="false" id="S4.p8.3.m3.3.3.3.7" xref="S4.p8.3.m3.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.3.m3.3b"><vector id="S4.p8.3.m3.3.3.4.cmml" xref="S4.p8.3.m3.3.3.3"><apply id="S4.p8.3.m3.1.1.1.1.cmml" xref="S4.p8.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.p8.3.m3.1.1.1.1.1.cmml" xref="S4.p8.3.m3.1.1.1.1">subscript</csymbol><ci id="S4.p8.3.m3.1.1.1.1.2.cmml" xref="S4.p8.3.m3.1.1.1.1.2">ğ‘‹</ci><cn type="integer" id="S4.p8.3.m3.1.1.1.1.3.cmml" xref="S4.p8.3.m3.1.1.1.1.3">0</cn></apply><apply id="S4.p8.3.m3.2.2.2.2.cmml" xref="S4.p8.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S4.p8.3.m3.2.2.2.2.1.cmml" xref="S4.p8.3.m3.2.2.2.2">subscript</csymbol><ci id="S4.p8.3.m3.2.2.2.2.2.cmml" xref="S4.p8.3.m3.2.2.2.2.2">ğ‘Œ</ci><cn type="integer" id="S4.p8.3.m3.2.2.2.2.3.cmml" xref="S4.p8.3.m3.2.2.2.2.3">0</cn></apply><apply id="S4.p8.3.m3.3.3.3.3.cmml" xref="S4.p8.3.m3.3.3.3.3"><csymbol cd="ambiguous" id="S4.p8.3.m3.3.3.3.3.1.cmml" xref="S4.p8.3.m3.3.3.3.3">subscript</csymbol><ci id="S4.p8.3.m3.3.3.3.3.2.cmml" xref="S4.p8.3.m3.3.3.3.3.2">ğ‘</ci><cn type="integer" id="S4.p8.3.m3.3.3.3.3.3.cmml" xref="S4.p8.3.m3.3.3.3.3.3">0</cn></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.3.m3.3c">(X_{0},Y_{0},Z_{0})</annotation></semantics></math> into <math id="S4.p8.4.m4.3" class="ltx_Math" alttext="(X_{w},Y_{w},Z_{w})" display="inline"><semantics id="S4.p8.4.m4.3a"><mrow id="S4.p8.4.m4.3.3.3" xref="S4.p8.4.m4.3.3.4.cmml"><mo stretchy="false" id="S4.p8.4.m4.3.3.3.4" xref="S4.p8.4.m4.3.3.4.cmml">(</mo><msub id="S4.p8.4.m4.1.1.1.1" xref="S4.p8.4.m4.1.1.1.1.cmml"><mi id="S4.p8.4.m4.1.1.1.1.2" xref="S4.p8.4.m4.1.1.1.1.2.cmml">X</mi><mi id="S4.p8.4.m4.1.1.1.1.3" xref="S4.p8.4.m4.1.1.1.1.3.cmml">w</mi></msub><mo id="S4.p8.4.m4.3.3.3.5" xref="S4.p8.4.m4.3.3.4.cmml">,</mo><msub id="S4.p8.4.m4.2.2.2.2" xref="S4.p8.4.m4.2.2.2.2.cmml"><mi id="S4.p8.4.m4.2.2.2.2.2" xref="S4.p8.4.m4.2.2.2.2.2.cmml">Y</mi><mi id="S4.p8.4.m4.2.2.2.2.3" xref="S4.p8.4.m4.2.2.2.2.3.cmml">w</mi></msub><mo id="S4.p8.4.m4.3.3.3.6" xref="S4.p8.4.m4.3.3.4.cmml">,</mo><msub id="S4.p8.4.m4.3.3.3.3" xref="S4.p8.4.m4.3.3.3.3.cmml"><mi id="S4.p8.4.m4.3.3.3.3.2" xref="S4.p8.4.m4.3.3.3.3.2.cmml">Z</mi><mi id="S4.p8.4.m4.3.3.3.3.3" xref="S4.p8.4.m4.3.3.3.3.3.cmml">w</mi></msub><mo stretchy="false" id="S4.p8.4.m4.3.3.3.7" xref="S4.p8.4.m4.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.4.m4.3b"><vector id="S4.p8.4.m4.3.3.4.cmml" xref="S4.p8.4.m4.3.3.3"><apply id="S4.p8.4.m4.1.1.1.1.cmml" xref="S4.p8.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S4.p8.4.m4.1.1.1.1.1.cmml" xref="S4.p8.4.m4.1.1.1.1">subscript</csymbol><ci id="S4.p8.4.m4.1.1.1.1.2.cmml" xref="S4.p8.4.m4.1.1.1.1.2">ğ‘‹</ci><ci id="S4.p8.4.m4.1.1.1.1.3.cmml" xref="S4.p8.4.m4.1.1.1.1.3">ğ‘¤</ci></apply><apply id="S4.p8.4.m4.2.2.2.2.cmml" xref="S4.p8.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S4.p8.4.m4.2.2.2.2.1.cmml" xref="S4.p8.4.m4.2.2.2.2">subscript</csymbol><ci id="S4.p8.4.m4.2.2.2.2.2.cmml" xref="S4.p8.4.m4.2.2.2.2.2">ğ‘Œ</ci><ci id="S4.p8.4.m4.2.2.2.2.3.cmml" xref="S4.p8.4.m4.2.2.2.2.3">ğ‘¤</ci></apply><apply id="S4.p8.4.m4.3.3.3.3.cmml" xref="S4.p8.4.m4.3.3.3.3"><csymbol cd="ambiguous" id="S4.p8.4.m4.3.3.3.3.1.cmml" xref="S4.p8.4.m4.3.3.3.3">subscript</csymbol><ci id="S4.p8.4.m4.3.3.3.3.2.cmml" xref="S4.p8.4.m4.3.3.3.3.2">ğ‘</ci><ci id="S4.p8.4.m4.3.3.3.3.3.cmml" xref="S4.p8.4.m4.3.3.3.3.3">ğ‘¤</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.4.m4.3c">(X_{w},Y_{w},Z_{w})</annotation></semantics></math> in Equation <a href="#S4.E2" title="In 4 Perspective-aware Convolution â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we can calculate the derivatives and the perspective angle <math id="S4.p8.5.m5.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S4.p8.5.m5.1a"><mi id="S4.p8.5.m5.1.1" xref="S4.p8.5.m5.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S4.p8.5.m5.1b"><ci id="S4.p8.5.m5.1.1.cmml" xref="S4.p8.5.m5.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.5.m5.1c">\phi</annotation></semantics></math> is determined by the following equation:</p>
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.3" class="ltx_Math" alttext="\phi=\operatorname{atan2}\left(\frac{dv}{dZ_{w}},\frac{du}{dZ_{w}}\right)" display="block"><semantics id="S4.E4.m1.3a"><mrow id="S4.E4.m1.3.4" xref="S4.E4.m1.3.4.cmml"><mi id="S4.E4.m1.3.4.2" xref="S4.E4.m1.3.4.2.cmml">Ï•</mi><mo id="S4.E4.m1.3.4.1" xref="S4.E4.m1.3.4.1.cmml">=</mo><mrow id="S4.E4.m1.3.4.3.2" xref="S4.E4.m1.3.4.3.1.cmml"><mi id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml">atan2</mi><mo id="S4.E4.m1.3.4.3.2a" xref="S4.E4.m1.3.4.3.1.cmml">â¡</mo><mrow id="S4.E4.m1.3.4.3.2.1" xref="S4.E4.m1.3.4.3.1.cmml"><mo id="S4.E4.m1.3.4.3.2.1.1" xref="S4.E4.m1.3.4.3.1.cmml">(</mo><mfrac id="S4.E4.m1.2.2" xref="S4.E4.m1.2.2.cmml"><mrow id="S4.E4.m1.2.2.2" xref="S4.E4.m1.2.2.2.cmml"><mi id="S4.E4.m1.2.2.2.2" xref="S4.E4.m1.2.2.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.2.1" xref="S4.E4.m1.2.2.2.1.cmml">â€‹</mo><mi id="S4.E4.m1.2.2.2.3" xref="S4.E4.m1.2.2.2.3.cmml">v</mi></mrow><mrow id="S4.E4.m1.2.2.3" xref="S4.E4.m1.2.2.3.cmml"><mi id="S4.E4.m1.2.2.3.2" xref="S4.E4.m1.2.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.3.1" xref="S4.E4.m1.2.2.3.1.cmml">â€‹</mo><msub id="S4.E4.m1.2.2.3.3" xref="S4.E4.m1.2.2.3.3.cmml"><mi id="S4.E4.m1.2.2.3.3.2" xref="S4.E4.m1.2.2.3.3.2.cmml">Z</mi><mi id="S4.E4.m1.2.2.3.3.3" xref="S4.E4.m1.2.2.3.3.3.cmml">w</mi></msub></mrow></mfrac><mo id="S4.E4.m1.3.4.3.2.1.2" xref="S4.E4.m1.3.4.3.1.cmml">,</mo><mfrac id="S4.E4.m1.3.3" xref="S4.E4.m1.3.3.cmml"><mrow id="S4.E4.m1.3.3.2" xref="S4.E4.m1.3.3.2.cmml"><mi id="S4.E4.m1.3.3.2.2" xref="S4.E4.m1.3.3.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.2.1" xref="S4.E4.m1.3.3.2.1.cmml">â€‹</mo><mi id="S4.E4.m1.3.3.2.3" xref="S4.E4.m1.3.3.2.3.cmml">u</mi></mrow><mrow id="S4.E4.m1.3.3.3" xref="S4.E4.m1.3.3.3.cmml"><mi id="S4.E4.m1.3.3.3.2" xref="S4.E4.m1.3.3.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.3.1" xref="S4.E4.m1.3.3.3.1.cmml">â€‹</mo><msub id="S4.E4.m1.3.3.3.3" xref="S4.E4.m1.3.3.3.3.cmml"><mi id="S4.E4.m1.3.3.3.3.2" xref="S4.E4.m1.3.3.3.3.2.cmml">Z</mi><mi id="S4.E4.m1.3.3.3.3.3" xref="S4.E4.m1.3.3.3.3.3.cmml">w</mi></msub></mrow></mfrac><mo id="S4.E4.m1.3.4.3.2.1.3" xref="S4.E4.m1.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.3b"><apply id="S4.E4.m1.3.4.cmml" xref="S4.E4.m1.3.4"><eq id="S4.E4.m1.3.4.1.cmml" xref="S4.E4.m1.3.4.1"></eq><ci id="S4.E4.m1.3.4.2.cmml" xref="S4.E4.m1.3.4.2">italic-Ï•</ci><apply id="S4.E4.m1.3.4.3.1.cmml" xref="S4.E4.m1.3.4.3.2"><ci id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">atan2</ci><apply id="S4.E4.m1.2.2.cmml" xref="S4.E4.m1.2.2"><divide id="S4.E4.m1.2.2.1.cmml" xref="S4.E4.m1.2.2"></divide><apply id="S4.E4.m1.2.2.2.cmml" xref="S4.E4.m1.2.2.2"><times id="S4.E4.m1.2.2.2.1.cmml" xref="S4.E4.m1.2.2.2.1"></times><ci id="S4.E4.m1.2.2.2.2.cmml" xref="S4.E4.m1.2.2.2.2">ğ‘‘</ci><ci id="S4.E4.m1.2.2.2.3.cmml" xref="S4.E4.m1.2.2.2.3">ğ‘£</ci></apply><apply id="S4.E4.m1.2.2.3.cmml" xref="S4.E4.m1.2.2.3"><times id="S4.E4.m1.2.2.3.1.cmml" xref="S4.E4.m1.2.2.3.1"></times><ci id="S4.E4.m1.2.2.3.2.cmml" xref="S4.E4.m1.2.2.3.2">ğ‘‘</ci><apply id="S4.E4.m1.2.2.3.3.cmml" xref="S4.E4.m1.2.2.3.3"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.3.3.1.cmml" xref="S4.E4.m1.2.2.3.3">subscript</csymbol><ci id="S4.E4.m1.2.2.3.3.2.cmml" xref="S4.E4.m1.2.2.3.3.2">ğ‘</ci><ci id="S4.E4.m1.2.2.3.3.3.cmml" xref="S4.E4.m1.2.2.3.3.3">ğ‘¤</ci></apply></apply></apply><apply id="S4.E4.m1.3.3.cmml" xref="S4.E4.m1.3.3"><divide id="S4.E4.m1.3.3.1.cmml" xref="S4.E4.m1.3.3"></divide><apply id="S4.E4.m1.3.3.2.cmml" xref="S4.E4.m1.3.3.2"><times id="S4.E4.m1.3.3.2.1.cmml" xref="S4.E4.m1.3.3.2.1"></times><ci id="S4.E4.m1.3.3.2.2.cmml" xref="S4.E4.m1.3.3.2.2">ğ‘‘</ci><ci id="S4.E4.m1.3.3.2.3.cmml" xref="S4.E4.m1.3.3.2.3">ğ‘¢</ci></apply><apply id="S4.E4.m1.3.3.3.cmml" xref="S4.E4.m1.3.3.3"><times id="S4.E4.m1.3.3.3.1.cmml" xref="S4.E4.m1.3.3.3.1"></times><ci id="S4.E4.m1.3.3.3.2.cmml" xref="S4.E4.m1.3.3.3.2">ğ‘‘</ci><apply id="S4.E4.m1.3.3.3.3.cmml" xref="S4.E4.m1.3.3.3.3"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.3.3.1.cmml" xref="S4.E4.m1.3.3.3.3">subscript</csymbol><ci id="S4.E4.m1.3.3.3.3.2.cmml" xref="S4.E4.m1.3.3.3.3.2">ğ‘</ci><ci id="S4.E4.m1.3.3.3.3.3.cmml" xref="S4.E4.m1.3.3.3.3.3">ğ‘¤</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.3c">\phi=\operatorname{atan2}\left(\frac{dv}{dZ_{w}},\frac{du}{dZ_{w}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">Using Equations <a href="#S4.E2" title="In 4 Perspective-aware Convolution â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S4.E4" title="In 4 Perspective-aware Convolution â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> , we can calculate the perspective angle for each image pixel <math id="S4.p9.1.m1.2" class="ltx_Math" alttext="(u,v)" display="inline"><semantics id="S4.p9.1.m1.2a"><mrow id="S4.p9.1.m1.2.3.2" xref="S4.p9.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.p9.1.m1.2.3.2.1" xref="S4.p9.1.m1.2.3.1.cmml">(</mo><mi id="S4.p9.1.m1.1.1" xref="S4.p9.1.m1.1.1.cmml">u</mi><mo id="S4.p9.1.m1.2.3.2.2" xref="S4.p9.1.m1.2.3.1.cmml">,</mo><mi id="S4.p9.1.m1.2.2" xref="S4.p9.1.m1.2.2.cmml">v</mi><mo stretchy="false" id="S4.p9.1.m1.2.3.2.3" xref="S4.p9.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p9.1.m1.2b"><interval closure="open" id="S4.p9.1.m1.2.3.1.cmml" xref="S4.p9.1.m1.2.3.2"><ci id="S4.p9.1.m1.1.1.cmml" xref="S4.p9.1.m1.1.1">ğ‘¢</ci><ci id="S4.p9.1.m1.2.2.cmml" xref="S4.p9.1.m1.2.2">ğ‘£</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.1.m1.2c">(u,v)</annotation></semantics></math>. This perspective angle allows us to guide kernel to change their shape according to its pixel coordinate and perspective angle, as illustrated in Fig. <a href="#S3.F4" title="Figure 4 â€£ 3 Proposed Method â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2308.12938/assets/image/architecture.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="707" height="161" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text ltx_font_bold">Fig.Â 5</span>: </span>Our proposed architecture for 3D object detection. We adopt the network proposed by Ground-aware<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, which is a one-stage anchor-based architecture. We add our PAC module after the feature extractor, which is ResNet-101 in our implementation, to obtain the perspective-aware feature map. The top branch focuses on classifying positive and negative anchor predictions, while the lower branch is responsible for regressing the geometry of both 2D and 3D bounding boxes. This architecture enables accurate object detection by leveraging the benefits of the PAC module in capturing scene structure and improving 3D box regression.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Perspective-aware Convolutional Module</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In addition to incorporating a single PAC convolution layer, we employ a multiple-branch design inspired by ASPP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, utilizing different dilation rates for each branch. Our objective is to capture multi-scale features along each perspective line with the PAC module. Furthermore, to ensure the preservation of regular features, we include a branch with a standard 3x3 kernel in our PAC module. This design choice guarantees that the regular feature map passes through our module without any alterations. A comparison between ASPP module and PAC module is depicted in Fig. <a href="#S4.F6" title="Figure 6 â€£ 4.1 Perspective-aware Convolutional Module â€£ 4 Perspective-aware Convolution â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2308.12938/assets/image/pac_and_ASPP.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="116" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text ltx_font_bold">Fig.Â 6</span>: </span>Comparison of PAC module and ASPP module. Both modules utilize parallel branches to capture multi-scale features. However, the key distinction lies in the kernel shape employed for feature extraction. While the ASPP module utilizes a regular kernel shape, the PAC module incorporates a tilted kernel shape to guide feature extraction along the perspective line.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we present the experimental results of our proposed PAC module in the task of 3D object detection. We integrate the PAC module into the baseline network of Ground-aware<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and aim to enhance its performance. The training and evaluation of the detector are conducted on the KITTI3D dataset, consisting of 3711 training images and a validation set of 3768 images. We adopt to the data split recommended by Chen et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. During training, we set the batch size to 8 and utilize the Adam optimizer with a learning rate of <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S5.p1.1.m1.1a"><msup id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mn id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml"><mo id="S5.p1.1.m1.1.1.3a" xref="S5.p1.1.m1.1.1.3.cmml">âˆ’</mo><mn id="S5.p1.1.m1.1.1.3.2" xref="S5.p1.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">10</cn><apply id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3"><minus id="S5.p1.1.m1.1.1.3.1.cmml" xref="S5.p1.1.m1.1.1.3"></minus><cn type="integer" id="S5.p1.1.m1.1.1.3.2.cmml" xref="S5.p1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">10^{-4}</annotation></semantics></math>. To speed up the network, we crop the top 100 pixels from the images and resize them to 288x1280. Additionally, we apply horizontal flipping and photometric distortion techniques to improve the diversity of the training data.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We present our experiment results in Table <a href="#S5.T1" title="Table 1 â€£ 5 Experiments â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> where we compare with other 3D object detection networks. Our proposed method surpasses all other detectors in terms of average precision, showing the effectiveness of our PAC module.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.2.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Experimental results of the 3D object detection algorithm on the KITTI3D validation dataset. The best performance in each column is indicated in bold font.</figcaption>
<table id="S5.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.3.1.1" class="ltx_tr">
<th id="S5.T1.3.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S5.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">Car AP 3D (IoU=0.7)</th>
<th id="S5.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">Car AP BEV (IoU=0.7)</th>
</tr>
<tr id="S5.T1.3.2.2" class="ltx_tr">
<th id="S5.T1.3.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r">Methods</th>
<th id="S5.T1.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Easy</th>
<th id="S5.T1.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Moderate</th>
<th id="S5.T1.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Hard</th>
<th id="S5.T1.3.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Easy</th>
<th id="S5.T1.3.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">Moderate</th>
<th id="S5.T1.3.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Hard</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.3.3.1" class="ltx_tr">
<th id="S5.T1.3.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">MonoGRNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<td id="S5.T1.3.3.1.2" class="ltx_td ltx_align_center ltx_border_t">12.28</td>
<td id="S5.T1.3.3.1.3" class="ltx_td ltx_align_center ltx_border_t">7.76</td>
<td id="S5.T1.3.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.91</td>
<td id="S5.T1.3.3.1.5" class="ltx_td ltx_align_center ltx_border_t">19.89</td>
<td id="S5.T1.3.3.1.6" class="ltx_td ltx_align_center ltx_border_t">12.94</td>
<td id="S5.T1.3.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.31</td>
</tr>
<tr id="S5.T1.3.4.2" class="ltx_tr">
<th id="S5.T1.3.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">SMOKE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<td id="S5.T1.3.4.2.2" class="ltx_td ltx_align_center">6.96</td>
<td id="S5.T1.3.4.2.3" class="ltx_td ltx_align_center">4.30</td>
<td id="S5.T1.3.4.2.4" class="ltx_td ltx_align_center ltx_border_r">3.98</td>
<td id="S5.T1.3.4.2.5" class="ltx_td ltx_align_center">12.73</td>
<td id="S5.T1.3.4.2.6" class="ltx_td ltx_align_center">7.93</td>
<td id="S5.T1.3.4.2.7" class="ltx_td ltx_align_center ltx_border_r">6.94</td>
</tr>
<tr id="S5.T1.3.5.3" class="ltx_tr">
<th id="S5.T1.3.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">DD3D<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<td id="S5.T1.3.5.3.2" class="ltx_td ltx_align_center">19.16</td>
<td id="S5.T1.3.5.3.3" class="ltx_td ltx_align_center">15.27</td>
<td id="S5.T1.3.5.3.4" class="ltx_td ltx_align_center ltx_border_r">13.37</td>
<td id="S5.T1.3.5.3.5" class="ltx_td ltx_align_center">25.72</td>
<td id="S5.T1.3.5.3.6" class="ltx_td ltx_align_center">20.78</td>
<td id="S5.T1.3.5.3.7" class="ltx_td ltx_align_center ltx_border_r">18.38</td>
</tr>
<tr id="S5.T1.3.6.4" class="ltx_tr">
<th id="S5.T1.3.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">MonoFlex<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</th>
<td id="S5.T1.3.6.4.2" class="ltx_td ltx_align_center">22.14</td>
<td id="S5.T1.3.6.4.3" class="ltx_td ltx_align_center">16.19</td>
<td id="S5.T1.3.6.4.4" class="ltx_td ltx_align_center ltx_border_r">14.18</td>
<td id="S5.T1.3.6.4.5" class="ltx_td ltx_align_center">29.30</td>
<td id="S5.T1.3.6.4.6" class="ltx_td ltx_align_center">21.91</td>
<td id="S5.T1.3.6.4.7" class="ltx_td ltx_align_center ltx_border_r">18.82</td>
</tr>
<tr id="S5.T1.3.7.5" class="ltx_tr">
<th id="S5.T1.3.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Ground-aware<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</th>
<td id="S5.T1.3.7.5.2" class="ltx_td ltx_align_center">21.90</td>
<td id="S5.T1.3.7.5.3" class="ltx_td ltx_align_center">16.06</td>
<td id="S5.T1.3.7.5.4" class="ltx_td ltx_align_center ltx_border_r">13.17</td>
<td id="S5.T1.3.7.5.5" class="ltx_td ltx_align_center">28.29</td>
<td id="S5.T1.3.7.5.6" class="ltx_td ltx_align_center">20.98</td>
<td id="S5.T1.3.7.5.7" class="ltx_td ltx_align_center ltx_border_r">17.59</td>
</tr>
<tr id="S5.T1.3.8.6" class="ltx_tr">
<th id="S5.T1.3.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S5.T1.3.8.6.1.1" class="ltx_text ltx_font_bold">Ours(Ground-aware+PAC Module)</span></th>
<td id="S5.T1.3.8.6.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.3.8.6.2.1" class="ltx_text ltx_font_bold">23.53</span></td>
<td id="S5.T1.3.8.6.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.3.8.6.3.1" class="ltx_text ltx_font_bold">17.23</span></td>
<td id="S5.T1.3.8.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T1.3.8.6.4.1" class="ltx_text ltx_font_bold">14.33</span></td>
<td id="S5.T1.3.8.6.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.3.8.6.5.1" class="ltx_text ltx_font_bold">30.57</span></td>
<td id="S5.T1.3.8.6.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.3.8.6.6.1" class="ltx_text ltx_font_bold">22.44</span></td>
<td id="S5.T1.3.8.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T1.3.8.6.7.1" class="ltx_text ltx_font_bold">19.07</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Additionally, we compare PAC module with other convolutional module mentioned in Section 2 and report the result in <a href="#S5.T2" title="Table 2 â€£ 5 Experiments â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We adopt Ground-aware network as the baseline and applied the dilation convolution in the last three convolutional layers of the backbone and set the dilation rate to (2,2,2) and (3,3,3) in our experiment. We also add DCN, RFB, ASPP, and PAC after the feature extractor, following their respective recommended settings in each paper. We conducted experiments with a single layer of PAC, where the dilation rate is set to two. As for the PAC module, we set the dilation rate to 2, 4, 6, and 8 in each parallel branch.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.2.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Experimental results of the 3D object detection algorithm on the KITTI3D validation dataset. The best performance in each column is indicated in bold font.</figcaption>
<table id="S5.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.3.1.1" class="ltx_tr">
<th id="S5.T2.3.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S5.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">Car AP 3D (IoU=0.7)</th>
<th id="S5.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">Car AP BEV (IoU=0.7)</th>
</tr>
<tr id="S5.T2.3.2.2" class="ltx_tr">
<th id="S5.T2.3.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r">Methods</th>
<th id="S5.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Easy</th>
<th id="S5.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Moderate</th>
<th id="S5.T2.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Hard</th>
<th id="S5.T2.3.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Easy</th>
<th id="S5.T2.3.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">Moderate</th>
<th id="S5.T2.3.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Hard</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.3.3.1" class="ltx_tr">
<th id="S5.T2.3.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Baseline</th>
<td id="S5.T2.3.3.1.2" class="ltx_td ltx_align_center ltx_border_t">22.08</td>
<td id="S5.T2.3.3.1.3" class="ltx_td ltx_align_center ltx_border_t">15.64</td>
<td id="S5.T2.3.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.00</td>
<td id="S5.T2.3.3.1.5" class="ltx_td ltx_align_center ltx_border_t">28.56</td>
<td id="S5.T2.3.3.1.6" class="ltx_td ltx_align_center ltx_border_t">21.08</td>
<td id="S5.T2.3.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.48</td>
</tr>
<tr id="S5.T2.3.4.2" class="ltx_tr">
<th id="S5.T2.3.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Dilation Convolution(2,2,2)</th>
<td id="S5.T2.3.4.2.2" class="ltx_td ltx_align_center">20.32</td>
<td id="S5.T2.3.4.2.3" class="ltx_td ltx_align_center">14.07</td>
<td id="S5.T2.3.4.2.4" class="ltx_td ltx_align_center ltx_border_r">12.06</td>
<td id="S5.T2.3.4.2.5" class="ltx_td ltx_align_center">27.23</td>
<td id="S5.T2.3.4.2.6" class="ltx_td ltx_align_center">19.70</td>
<td id="S5.T2.3.4.2.7" class="ltx_td ltx_align_center ltx_border_r">16.72</td>
</tr>
<tr id="S5.T2.3.5.3" class="ltx_tr">
<th id="S5.T2.3.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Dilation Convolution(3,3,3)</th>
<td id="S5.T2.3.5.3.2" class="ltx_td ltx_align_center">17.93</td>
<td id="S5.T2.3.5.3.3" class="ltx_td ltx_align_center">12.93</td>
<td id="S5.T2.3.5.3.4" class="ltx_td ltx_align_center ltx_border_r">10.71</td>
<td id="S5.T2.3.5.3.5" class="ltx_td ltx_align_center">24.93</td>
<td id="S5.T2.3.5.3.6" class="ltx_td ltx_align_center">17.93</td>
<td id="S5.T2.3.5.3.7" class="ltx_td ltx_align_center ltx_border_r">15.68</td>
</tr>
<tr id="S5.T2.3.6.4" class="ltx_tr">
<th id="S5.T2.3.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">DCNv2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>(n=1)</th>
<td id="S5.T2.3.6.4.2" class="ltx_td ltx_align_center">22.13</td>
<td id="S5.T2.3.6.4.3" class="ltx_td ltx_align_center">16.20</td>
<td id="S5.T2.3.6.4.4" class="ltx_td ltx_align_center ltx_border_r">13.44</td>
<td id="S5.T2.3.6.4.5" class="ltx_td ltx_align_center">29.19</td>
<td id="S5.T2.3.6.4.6" class="ltx_td ltx_align_center">21.58</td>
<td id="S5.T2.3.6.4.7" class="ltx_td ltx_align_center ltx_border_r">18.57</td>
</tr>
<tr id="S5.T2.3.7.5" class="ltx_tr">
<th id="S5.T2.3.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">DCNv2(n=2)</th>
<td id="S5.T2.3.7.5.2" class="ltx_td ltx_align_center">20.21</td>
<td id="S5.T2.3.7.5.3" class="ltx_td ltx_align_center">15.52</td>
<td id="S5.T2.3.7.5.4" class="ltx_td ltx_align_center ltx_border_r">13.23</td>
<td id="S5.T2.3.7.5.5" class="ltx_td ltx_align_center">29.03</td>
<td id="S5.T2.3.7.5.6" class="ltx_td ltx_align_center">21.78</td>
<td id="S5.T2.3.7.5.7" class="ltx_td ltx_align_center ltx_border_r">18.93</td>
</tr>
<tr id="S5.T2.3.8.6" class="ltx_tr">
<th id="S5.T2.3.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">DCNv2(n=3)</th>
<td id="S5.T2.3.8.6.2" class="ltx_td ltx_align_center">20.21</td>
<td id="S5.T2.3.8.6.3" class="ltx_td ltx_align_center">14.58</td>
<td id="S5.T2.3.8.6.4" class="ltx_td ltx_align_center ltx_border_r">12.24</td>
<td id="S5.T2.3.8.6.5" class="ltx_td ltx_align_center">28.12</td>
<td id="S5.T2.3.8.6.6" class="ltx_td ltx_align_center">19.79</td>
<td id="S5.T2.3.8.6.7" class="ltx_td ltx_align_center ltx_border_r">17.11</td>
</tr>
<tr id="S5.T2.3.9.7" class="ltx_tr">
<th id="S5.T2.3.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">RFB<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="S5.T2.3.9.7.2" class="ltx_td ltx_align_center">21.32</td>
<td id="S5.T2.3.9.7.3" class="ltx_td ltx_align_center">15.62</td>
<td id="S5.T2.3.9.7.4" class="ltx_td ltx_align_center ltx_border_r">12.94</td>
<td id="S5.T2.3.9.7.5" class="ltx_td ltx_align_center">28.53</td>
<td id="S5.T2.3.9.7.6" class="ltx_td ltx_align_center">21.26</td>
<td id="S5.T2.3.9.7.7" class="ltx_td ltx_align_center ltx_border_r">18.35</td>
</tr>
<tr id="S5.T2.3.10.8" class="ltx_tr">
<th id="S5.T2.3.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">ASPP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</th>
<td id="S5.T2.3.10.8.2" class="ltx_td ltx_align_center">22.44</td>
<td id="S5.T2.3.10.8.3" class="ltx_td ltx_align_center">16.96</td>
<td id="S5.T2.3.10.8.4" class="ltx_td ltx_align_center ltx_border_r">14.23</td>
<td id="S5.T2.3.10.8.5" class="ltx_td ltx_align_center">29.69</td>
<td id="S5.T2.3.10.8.6" class="ltx_td ltx_align_center">22.20</td>
<td id="S5.T2.3.10.8.7" class="ltx_td ltx_align_center ltx_border_r">19.03</td>
</tr>
<tr id="S5.T2.3.11.9" class="ltx_tr">
<th id="S5.T2.3.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T2.3.11.9.1.1" class="ltx_text ltx_font_bold">PAC(Ours)</span></th>
<td id="S5.T2.3.11.9.2" class="ltx_td ltx_align_center">22.71</td>
<td id="S5.T2.3.11.9.3" class="ltx_td ltx_align_center">15.73</td>
<td id="S5.T2.3.11.9.4" class="ltx_td ltx_align_center ltx_border_r">13.05</td>
<td id="S5.T2.3.11.9.5" class="ltx_td ltx_align_center">30.55</td>
<td id="S5.T2.3.11.9.6" class="ltx_td ltx_align_center">21.85</td>
<td id="S5.T2.3.11.9.7" class="ltx_td ltx_align_center ltx_border_r">18.56</td>
</tr>
<tr id="S5.T2.3.12.10" class="ltx_tr">
<th id="S5.T2.3.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S5.T2.3.12.10.1.1" class="ltx_text ltx_font_bold">PAC Module(Ours)</span></th>
<td id="S5.T2.3.12.10.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.3.12.10.2.1" class="ltx_text ltx_font_bold">23.53</span></td>
<td id="S5.T2.3.12.10.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.3.12.10.3.1" class="ltx_text ltx_font_bold">17.23</span></td>
<td id="S5.T2.3.12.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T2.3.12.10.4.1" class="ltx_text ltx_font_bold">14.33</span></td>
<td id="S5.T2.3.12.10.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.3.12.10.5.1" class="ltx_text ltx_font_bold">30.57</span></td>
<td id="S5.T2.3.12.10.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.3.12.10.6.1" class="ltx_text ltx_font_bold">22.44</span></td>
<td id="S5.T2.3.12.10.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T2.3.12.10.7.1" class="ltx_text ltx_font_bold">19.07</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">As shown in our experimental results in Table <a href="#S5.T2" title="Table 2 â€£ 5 Experiments â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our proposed PAC module outperforms all other methods and achieves an improvement of +1.59% AP compare to baseline in the 3D metric with moderate difficulty. In contrast, dilation convolution showed no improvement compared to the baseline. Deformable convolution had a slight improvement with a single layer, but its performance dropped after using more than one DCN layer. We suspect this is because DCN introduces too many parameters to train, making it easier to overfit the training data, especially since KITTIâ€™s training set is small. RFB showed roughly the same performance as the baseline, while ASPP showed a fair improvement, especially with hard-difficulty objects. This indicates that far or truncated objects require long-range information to predict their depth accurately.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Qualitative Result</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">To facilitate a fair comparison of different 3D object detectors, we present some inference outcome example in the KITTI3D validation set, as shown in Figures <a href="#S5.F7" title="Figure 7 â€£ 5.1 Qualitative Result â€£ 5 Experiments â€£ Perspective-aware Convolution for Monocular 3D object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. In the figure, the left column shows the predicted 3D bounding box projected onto the image plane. However, to accurately evaluate the 3D box location, we recommend that readers use the birdâ€™s-eye-view (BEV) provided in the right column. In the BEV figures, the yellow box represents the ground truth and the green box represents the predicted result.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Based on these results, we can observe some interesting traits of each detector. Firstly, despite the impressive accuracy of Pseudo-LiDAR in 3D box estimation, it can sometimes incorrectly predict the object orientation in pretty obvious cases. This is because Pseudo-LiDAR converts the image to a point cloud, sacrificing some advantages that are only available when the data is in image form. As for MonoFlex and Ground-aware, they perform roughly the same in this experiment, showing keypoint-based and anchor-based method both has potential in 3D object detection. DD3D, on the other hand, tends to generate too many false positives, although their confidence scores are low. This also highlights the advantage of anchor-based methods, where non-maximum suppression is applied to avoid similar issues.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2308.12938/assets/image/000053.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="550" height="833" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text ltx_font_bold">Fig.Â 7</span>: </span>Inference example of 3D object detectors. The left column shows the predicted 3D bounding boxes in the image plane, while the right column displays the bounding boxes projected onto the birdâ€™s-eye-view (BEV) plane. The yellow boxes on the BEV represent the ground truth, while the green boxes indicate the predictions. In this experiment, we observe that most methods exhibit inaccuracies in predicting object depth, whereas our proposed method demonstrates more accurate depth estimation.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we introduced a novel perspective-aware convolution layer to address the limitations of traditional convolutional kernels in capturing long-range dependencies in images. The PAC module enforces the convolutional kernel to extract features along the perspective lines, making it able to extract perspective-aware features. We integrated the PAC module into a 3D object detector and evaluated its performance on the KITTI3D dataset. The experimental results demonstrate that our approach achieved significant improvements, achieving a 23.9% AP in the easy difficulty of the dataset and surpassing other 3D object detectors. Our findings highlight the importance of modeling scene clues for accurate depth inference in camera images and the benefits of incorporating perspective information into the neuron network. We believe that our proposed methods have the potential to application in autonomous driving, to enhance 3D object detection accuracy and driving safety.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Fisher Yu and Vladlen Koltun,

</span>
<span class="ltx_bibblock">â€œMulti-scale context aggregation by dilated convolutions,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.07122</span>, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and AlanÂ L
Yuille,

</span>
<span class="ltx_bibblock">â€œDeeplab: Semantic image segmentation with deep convolutional nets,
atrous convolution, and fully connected crfs,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>,
vol. 40, no. 4, pp. 834â€“848, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Songtao Liu, DiÂ Huang, etÂ al.,

</span>
<span class="ltx_bibblock">â€œReceptive field block net for accurate and fast object detection,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the European conference on computer vision
(ECCV)</span>, 2018, pp. 385â€“400.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Jifeng Dai, Haozhi Qi, Yuwen Xiong, YiÂ Li, Guodong Zhang, Han Hu, and Yichen
Wei,

</span>
<span class="ltx_bibblock">â€œDeformable convolutional networks,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, 2017, pp. 764â€“773.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai,

</span>
<span class="ltx_bibblock">â€œDeformable convnets v2: More deformable, better results,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, 2019, pp. 9308â€“9316.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Arsalan Mousavian, Dragomir Anguelov, John Flynn, and Jana Kosecka,

</span>
<span class="ltx_bibblock">â€œ3d bounding box estimation using deep learning and geometry,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition</span>, 2017, pp. 7074â€“7082.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Lijie Liu, Jiwen Lu, Chunjing Xu, QiÂ Tian, and Jie Zhou,

</span>
<span class="ltx_bibblock">â€œDeep fitting degree scoring network for monocular 3d object
detection,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, 2019, pp. 1057â€“1066.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Andrea Simonelli, SamuelÂ Rota Bulo, Lorenzo Porzi, Manuel LÃ³pez-Antequera,
and Peter Kontschieder,

</span>
<span class="ltx_bibblock">â€œDisentangling monocular 3d object detection,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, 2019, pp. 1991â€“1999.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Fabian Manhardt, Wadim Kehl, and Adrien Gaidon,

</span>
<span class="ltx_bibblock">â€œRoi-10d: Monocular lifting of 2d detection to 6d pose and metric
shape,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2019, pp. 2069â€“2078.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Zengyi Qin, Jinglu Wang, and Yan Lu,

</span>
<span class="ltx_bibblock">â€œMonogrnet: A geometric reasoning network for monocular 3d object
localization,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, 2019, vol.Â 33, pp. 8851â€“8858.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Jason Ku, AlexÂ D Pon, and StevenÂ L Waslander,

</span>
<span class="ltx_bibblock">â€œMonocular 3d object detection leveraging accurate proposals and
shape reconstruction,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, 2019, pp. 11867â€“11876.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Garrick Brazil and Xiaoming Liu,

</span>
<span class="ltx_bibblock">â€œM3d-rpn: Monocular 3d region proposal network for object
detection,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, 2019, pp. 9287â€“9296.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yuxuan Liu, Yuan Yixuan, and Ming Liu,

</span>
<span class="ltx_bibblock">â€œGround-aware monocular 3d object detection for autonomous
driving,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE Robotics and Automation Letters</span>, vol. 6, no. 2, pp.
919â€“926, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Xingyi Zhou, Dequan Wang, and Philipp KrÃ¤henbÃ¼hl,

</span>
<span class="ltx_bibblock">â€œObjects as points,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1904.07850</span>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Peixuan Li, Huaici Zhao, Pengfei Liu, and Feidao Cao,

</span>
<span class="ltx_bibblock">â€œRtm3d: Real-time monocular 3d detection from object keypoints for
autonomous driving,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Computer Visionâ€“ECCV 2020: 16th European Conference,
Glasgow, UK, August 23â€“28, 2020, Proceedings, Part III 16</span>. Springer, 2020,
pp. 644â€“660.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Zechen Liu, Zizhang Wu, and Roland TÃ³th,

</span>
<span class="ltx_bibblock">â€œSmoke: Single-stage monocular 3d object detection via keypoint
estimation,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span>, 2020, pp. 996â€“997.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yongjian Chen, Lei Tai, Kai Sun, and Mingyang Li,

</span>
<span class="ltx_bibblock">â€œMonopair: Monocular 3d object detection using pairwise spatial
relationships,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2020, pp. 12093â€“12102.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Yunpeng Zhang, Jiwen Lu, and Jie Zhou,

</span>
<span class="ltx_bibblock">â€œObjects are different: Flexible monocular 3d object detection,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2021, pp. 3289â€“3298.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, AndrewÂ G Berneshawi, Huimin Ma, Sanja
Fidler, and Raquel Urtasun,

</span>
<span class="ltx_bibblock">â€œ3d object proposals for accurate object class detection,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 28,
2015.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, and Adrien Gaidon,

</span>
<span class="ltx_bibblock">â€œIs pseudo-lidar needed for monocular 3d object detection?,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, 2021, pp. 3142â€“3152.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and AlanÂ L
Yuille,

</span>
<span class="ltx_bibblock">â€œDeeplab: Semantic image segmentation with deep convolutional nets,
atrous convolution, and fully connected crfs,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>,
vol. 40, no. 4, pp. 834â€“848, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.12937" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.12938" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.12938">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.12938" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.12939" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 11:22:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
