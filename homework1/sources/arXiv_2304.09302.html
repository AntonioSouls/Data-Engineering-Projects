<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.09302] CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation</title><meta property="og:description" content="We address the important problem of generalizing robotic rearrangement to clutter without any explicit object models. We first generate over 650K cluttered scenesâ€”orders of magnitude more than prior workâ€”in diverse eveâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.09302">

<!--Generated on Thu Feb 29 13:57:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\UseRawInputEncoding</span>
</div>
<h1 class="ltx_title ltx_font_bold ltx_title_document">CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Adithyavairavan Murali 
<br class="ltx_break">NVIDIA 
<br class="ltx_break"><span id="id5.1.id1" class="ltx_text ltx_font_typewriter">admurali@nvidia.com</span> 
<br class="ltx_break">&amp;Arsalan Mousavian 
<br class="ltx_break">NVIDIA 
<br class="ltx_break"><span id="id6.2.id2" class="ltx_text ltx_font_typewriter">amousavian@nvidia.com</span> 
<br class="ltx_break">&amp;Clemens Eppner 
<br class="ltx_break">NVIDIA 
<br class="ltx_break"><span id="id7.3.id3" class="ltx_text ltx_font_typewriter">ceppner@nvidia.com</span> 
<br class="ltx_break">Adam Fishman 
<br class="ltx_break">NVIDIA 
<br class="ltx_break"><span id="id8.4.id4" class="ltx_text ltx_font_typewriter">afishman@nvidia.com</span> 
<br class="ltx_break">&amp;Dieter Fox<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">footnotemark: </span></span></span></span>
<br class="ltx_break">NVIDIA 
<br class="ltx_break"><span id="id9.5.id5" class="ltx_text ltx_font_typewriter">dieterf@nvidia.com</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Author is also affiliated with the University of Washington</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.4" class="ltx_p">We address the important problem of generalizing robotic rearrangement to clutter without any explicit object models. We first generate over 650K cluttered scenesâ€”orders of magnitude more than prior workâ€”in diverse everyday environments, such as cabinets and shelves. We render synthetic partial point clouds from this data and use it to train our CabiNet model architecture. CabiNet is a collision model that accepts object and scene point clouds, captured from a single-view depth observation, and predicts collisions for SE<math id="id1.1.m1.1" class="ltx_Math" alttext="(3)" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.2.2"><mo stretchy="false" id="id1.1.m1.1.2.2.1">(</mo><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">3</mn><mo stretchy="false" id="id1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><cn type="integer" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">(3)</annotation></semantics></math> object poses in the scene. Our representation has a fast inference speed of 7<math id="id2.2.m2.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="id2.2.m2.1a"><mi id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">Î¼</mi><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">ğœ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\mu</annotation></semantics></math>s/query with nearly 20<math id="id3.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="id3.3.m3.1a"><mo id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><csymbol cd="latexml" id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\%</annotation></semantics></math> higher performance than baseline approaches in challenging environments. We use this collision model in conjunction with a Model Predictive Path Integral (MPPI) planner to generate collision-free trajectories for picking and placing in clutter. CabiNet also predicts waypoints, computed from the sceneâ€™s signed distance field (SDF), that allows the robot to navigate tight spaces during rearrangement. This improves rearrangement performance by nearly 35<math id="id4.4.m4.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="id4.4.m4.1a"><mo id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><csymbol cd="latexml" id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">\%</annotation></semantics></math> compared to baselines. We systematically evaluate our approach, procedurally generate simulated experiments, and demonstrate that our approach directly transfers to the real world, despite training exclusively in simulation. Videos of robot experiments in completely unknown scenes are available at: <a target="_blank" href="https://cabinet-object-rearrangement.github.io" title="" class="ltx_ref ltx_href">cabinet-object-rearrangement.github.io</a>.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2304.09302/assets/figures/cover_fig_dish_c.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.5.2" class="ltx_text" style="font-size:90%;">CabiNet is able to <span id="S0.F1.5.2.1" class="ltx_text ltx_font_italic">(right)</span> perform complex rearrangement tasks in novel, cluttered scenes on the real robot from just partial point cloud observations without object or environment models. The model is trained with over 650K procedurally generated synthetic scenes <span id="S0.F1.5.2.2" class="ltx_text ltx_font_italic">(left)</span>.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Object rearrangement is an important challenge in robotic manipulation and decision making <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. It requires the skills of picking, placing and generating complex collision-free motions in a cluttered environment. The Embodied AI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and Task-And-Motion-Planning (TAMP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> literature have reported impressive results of rearrangement in complex human environments like kitchens. But, they are largely limited to simulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, do not enable unconstrained 6-DOF manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> or make the strong assumption of state estimation of the environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> and objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> in the real world. Recent neural rearrangement methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> generalize from sensed observations, without requiring state information, and have been demonstrated in the real world. Yet, they are limited to tabletop scenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> or require expensive data collection on the real robot: 1.5 years on 13 robots in the case of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and six houses in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Overall, there are no rearrangement systems that generalizes to novel challenging environments and works out-of-the-box with minimal engineering effort for each new scene typeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The cost of system integration, a major task of which is environment modelling, comes up to 3X of the price of the robot itself <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. In this work, we aim to learn a single representation, trained over 650K scenes in simulation, that enables rearrangement in diverse unknown environments.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">One of the primary challenges in achieving generalization in object rearrangement is the limited availability of rearrangement datasets. Large data has been the driving force behind the success of visual learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and large language modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. There have been some recent attempts at large-scale learning in simulation, such as the HabitatÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and ManipulaTHORÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> efforts. However, these actions are abstract and are not realistic. For example, objects simply stick to the gripper based on proximity, thereby sidestepping the complex dynamics of pick-and-place that are explicitly addressed in prior work in the robotic grasping literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. Additionally, the policies learned in these simulated environments are unproven in the real world. To facilitate performance in a physical system, our approach is conditioned on 3D point clouds instead of the RGBD representation commonly used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. Prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> has demonstrated that point clouds are an effective representation to transfer from simulation-based training to real world observations.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In robotic rearrangement, a fundamental component for planning is collision detection with an unknown environment. Classical TAMP methods typically rely on the complete geometric model of the scene for planning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> in the form of a triangular mesh or Signed Distance Field (SDF).
Visual reconstruction systems such as SLAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, KinectFusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and more recently NERFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> are needed to generate a geometric model of the scene.
Each system has its drawbacks, which may include a long start-up timeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, multi-view requirements Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, costly updates in dynamic scenesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, or poor generalizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. Instead of explicitly reconstructing the scene for a traditional collision checker, recent neural methods speed up collision detection with learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and generalize to partial real-world observations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> proposed the CollisionNet model to predict collisions from scene point clouds for 6-DOF grasping. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> extended this to an architecture that allowed fast collision checking from point clouds, enabling fast sampling-based placing in cluttered tabletop scenes. In this work, we extend this 3D implicit representation to scale to multiple cluttered environments, which we call CabiNet and learn a SDF-based waypoint sampler from this 3D representation. We then apply a Model Predictive Path Integral (MPPI) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> algorithm on the GPU to use our CabiNet model to generate pick-and-place motion trajectories in clutter. In summary, our contributions are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Scaling up neural collision checking by 30X compared to prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, training over nearly 60 billion collision queries. We also learn from over 650K cluttered scenes generated procedurally, which is six orders of magnitude more scene data than prior work on learning rearrangement in simulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. We train a implicit 3D scene encoder CabiNet from this dataset with over 2.5 million synthetically rendered point clouds.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.2" class="ltx_p">We demonstrate that CabiNet achieves fast collision detection inference of around 7<math id="S1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S1.I1.i2.p1.1.m1.1a"><mi id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml">Î¼</mi><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><ci id="S1.I1.i2.p1.1.m1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1">ğœ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">\mu</annotation></semantics></math>s/query and is 19.7<math id="S1.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S1.I1.i2.p1.2.m2.1a"><mo id="S1.I1.i2.p1.2.m2.1.1" xref="S1.I1.i2.p1.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.2.m2.1b"><csymbol cd="latexml" id="S1.I1.i2.p1.2.m2.1.1.cmml" xref="S1.I1.i2.p1.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.2.m2.1c">\%</annotation></semantics></math> mAP higher than baselines when tested on 2.5 million queries in five diverse sets of environments. Using the same CabiNet encoding, we learn a scene SDF-based waypoint sampler and show that it is crucial for transitioning between pick and place actions.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We demonstrate zero-shot <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">sim2real</span> transfer for our model on completely unknown scenes and objects in the real world, including out-of-distribution kitchen environments.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Collision Detection from Point Clouds:</span> There are a variety of options to check for collision with known object meshes or fully visible point clouds. One can use computational geometry librariesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> if the object mesh is known. Alternatively, one can voxelize or spherizeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> the point clouds and formulate the collision checking problem as evaluating whether any of the elements is in collision with robot links. However, voxel-based approaches suffer from occlusion which is mitigated through use of multiple views. This unfortunately constraints the robot workspace or requires mapping of the the environments which needs to be dynamically updated as objects are moving around. SceneCollisionNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> frames the collision checking problem as a hybrid of classical voxel based method and data driven methods. It encodes the scene to coarse voxels where each voxel is represented by a deep embeddding vector. Each collision query is defined as a pair of query object and scene point clouds. Collision checking is done with a binary classifier which takes as input the scene voxel embedding, object embedding, and the relative SE<math id="S2.p1.1.m1.1" class="ltx_Math" alttext="(3)" display="inline"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.2.2"><mo stretchy="false" id="S2.p1.1.m1.1.2.2.1">(</mo><mn id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">3</mn><mo stretchy="false" id="S2.p1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><cn type="integer" id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">(3)</annotation></semantics></math> transformation to that voxel. SceneCollisionNet was trained on only the table top scenes. In this paper, we build on top of SceneCollisionNet. By scaling up the training data to go beyond table top settings, we observe a boost in performance and generalization across variety of different scenes.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Neural Rearrangement Planning:</span> Traditionally, solutions to object rearrangement have been dominated by model-based methods, such as TAMPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> which use an explicit 3D world representation estimated from sensor observations.
More recently, there has been an increase in learning-based vision-centric rearrangement approachesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, although the majority assumes simplified action spaces that abstract away the actual grasping and placing motion and often focus on navigation.
InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> a learned visual state estimator is combined with a Monte-Carlo tree search planner, to efficiently solve planar tabletop rearrangements.
When goals are provided as target images, transporter networksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, their equivariant versionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> or goal-conditioned transporter networksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> can be used for pick-and-place.
The problem of matching object instances between goal and initial image can also be simplified with vision-language modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
Closest to our approach for rearrangement are NeRPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and IFORÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Large-scale Procedural Scene Generation:</span> Probabilistic models for indoor scene generation have been originally developed in computer graphicsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>.
Apart from appealing visually, simulating scenes physically requires more care when arranging objects.
As a result most data available for learning robot manipulation in simulation is limited to a fixed number of artist designed scenes:Â iGibsonÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, Meta-WorldÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, RLBenchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, SapienÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, HabitatÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, or AI2 ManipulaTHORÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
Those scenes mostly consist of assets and arrangements from datasets such as PartNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, ReplicaCADÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, and 3D-FrontÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
More recently, ProcTHORÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> has shown to procedurally generate large amounts of entire apartment layouts with room-specific object arrangements. Nevertheless, our use case focuses on smaller-scale clutter for which ProcTHORâ€™s scenes are not dense enough. We will present our procedural scene generation pipeline next.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Procedural Data Generation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.10" class="ltx_p"><span id="S3.p1.10.1" class="ltx_text ltx_font_bold">Synthetic Scene Generation:</span> We procedurally generate synthetic data in simulation. To generate our cluttered scenes, we first assemble a set of environment assets <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{E}" display="inline"><semantics id="S3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">â„°</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">â„°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathcal{E}</annotation></semantics></math>, object assets <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{O}" display="inline"><semantics id="S3.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">ğ’ª</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ğ’ª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\mathcal{O}</annotation></semantics></math> and fixed robot manipulator <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics id="S3.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">â„›</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">â„›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\mathcal{R}</annotation></semantics></math> (Franka Panda in our case). We have a probabilistic grammar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">P</annotation></semantics></math> which dictates how the assets can be organized into random scene graphs <math id="S3.p1.5.m5.3" class="ltx_Math" alttext="\mathcal{S}\sim P(\mathcal{E},\mathcal{O},\mathcal{R})" display="inline"><semantics id="S3.p1.5.m5.3a"><mrow id="S3.p1.5.m5.3.4" xref="S3.p1.5.m5.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.5.m5.3.4.2" xref="S3.p1.5.m5.3.4.2.cmml">ğ’®</mi><mo id="S3.p1.5.m5.3.4.1" xref="S3.p1.5.m5.3.4.1.cmml">âˆ¼</mo><mrow id="S3.p1.5.m5.3.4.3" xref="S3.p1.5.m5.3.4.3.cmml"><mi id="S3.p1.5.m5.3.4.3.2" xref="S3.p1.5.m5.3.4.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.p1.5.m5.3.4.3.1" xref="S3.p1.5.m5.3.4.3.1.cmml">â€‹</mo><mrow id="S3.p1.5.m5.3.4.3.3.2" xref="S3.p1.5.m5.3.4.3.3.1.cmml"><mo stretchy="false" id="S3.p1.5.m5.3.4.3.3.2.1" xref="S3.p1.5.m5.3.4.3.3.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">â„°</mi><mo id="S3.p1.5.m5.3.4.3.3.2.2" xref="S3.p1.5.m5.3.4.3.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.p1.5.m5.2.2" xref="S3.p1.5.m5.2.2.cmml">ğ’ª</mi><mo id="S3.p1.5.m5.3.4.3.3.2.3" xref="S3.p1.5.m5.3.4.3.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.p1.5.m5.3.3" xref="S3.p1.5.m5.3.3.cmml">â„›</mi><mo stretchy="false" id="S3.p1.5.m5.3.4.3.3.2.4" xref="S3.p1.5.m5.3.4.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.3b"><apply id="S3.p1.5.m5.3.4.cmml" xref="S3.p1.5.m5.3.4"><csymbol cd="latexml" id="S3.p1.5.m5.3.4.1.cmml" xref="S3.p1.5.m5.3.4.1">similar-to</csymbol><ci id="S3.p1.5.m5.3.4.2.cmml" xref="S3.p1.5.m5.3.4.2">ğ’®</ci><apply id="S3.p1.5.m5.3.4.3.cmml" xref="S3.p1.5.m5.3.4.3"><times id="S3.p1.5.m5.3.4.3.1.cmml" xref="S3.p1.5.m5.3.4.3.1"></times><ci id="S3.p1.5.m5.3.4.3.2.cmml" xref="S3.p1.5.m5.3.4.3.2">ğ‘ƒ</ci><vector id="S3.p1.5.m5.3.4.3.3.1.cmml" xref="S3.p1.5.m5.3.4.3.3.2"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">â„°</ci><ci id="S3.p1.5.m5.2.2.cmml" xref="S3.p1.5.m5.2.2">ğ’ª</ci><ci id="S3.p1.5.m5.3.3.cmml" xref="S3.p1.5.m5.3.3">â„›</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.3c">\mathcal{S}\sim P(\mathcal{E},\mathcal{O},\mathcal{R})</annotation></semantics></math>. This grammar is composed of the following key components: 1) sampling potential supports surfaces <math id="S3.p1.6.m6.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.p1.6.m6.1a"><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">\gamma</annotation></semantics></math> in an environment asset to place objects 2) rejection sampling to sequentially place objects on these surfaces without colliding with the scene and 3) fixing the robot base in a region where there is sufficient intersection over union (IoU <math id="S3.p1.7.m7.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.p1.7.m7.1a"><mo id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><gt id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">&gt;</annotation></semantics></math> 0.8) between the workspace of the robot (approximated by a cuboid volume) and <math id="S3.p1.8.m8.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.p1.8.m8.1a"><mi id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><ci id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">\gamma</annotation></semantics></math>. Once the scene <math id="S3.p1.9.m9.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.p1.9.m9.1a"><mi id="S3.p1.9.m9.1.1" xref="S3.p1.9.m9.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.1b"><ci id="S3.p1.9.m9.1.1.cmml" xref="S3.p1.9.m9.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.1c">S</annotation></semantics></math> is generated, collision queries are sampled with free-floating object meshes (computed in a straight-line trajectory) and the scene. The synthetic point clouds <math id="S3.p1.10.m10.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.p1.10.m10.1a"><mi id="S3.p1.10.m10.1.1" xref="S3.p1.10.m10.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p1.10.m10.1b"><ci id="S3.p1.10.m10.1.1.cmml" xref="S3.p1.10.m10.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m10.1c">X</annotation></semantics></math> are rendered online during training.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2304.09302/assets/figures/figure_2_wide_c.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="285" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;">An example of a procedurally generated CabiNet rearrangement scene. The target object (here in purple) is chosen if it has a selection of valid collision-free grasp poses. The green region represents the placement shelf, which is chosen if a) it is different from the shelf the object originates from and b) has a valid placement pose for the target object.</span></figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Dataset:</span> Our dataset of object assets for training comes from ACRONYM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, that contains wide range of object geometries from 262 categories as well as high-quality <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="SE(3)" display="inline"><semantics id="S3.p2.1.m1.1a"><mrow id="S3.p2.1.m1.1.2" xref="S3.p2.1.m1.1.2.cmml"><mi id="S3.p2.1.m1.1.2.2" xref="S3.p2.1.m1.1.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.p2.1.m1.1.2.1" xref="S3.p2.1.m1.1.2.1.cmml">â€‹</mo><mi id="S3.p2.1.m1.1.2.3" xref="S3.p2.1.m1.1.2.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.p2.1.m1.1.2.1a" xref="S3.p2.1.m1.1.2.1.cmml">â€‹</mo><mrow id="S3.p2.1.m1.1.2.4.2" xref="S3.p2.1.m1.1.2.cmml"><mo stretchy="false" id="S3.p2.1.m1.1.2.4.2.1" xref="S3.p2.1.m1.1.2.cmml">(</mo><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">3</mn><mo stretchy="false" id="S3.p2.1.m1.1.2.4.2.2" xref="S3.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.2.cmml" xref="S3.p2.1.m1.1.2"><times id="S3.p2.1.m1.1.2.1.cmml" xref="S3.p2.1.m1.1.2.1"></times><ci id="S3.p2.1.m1.1.2.2.cmml" xref="S3.p2.1.m1.1.2.2">ğ‘†</ci><ci id="S3.p2.1.m1.1.2.3.cmml" xref="S3.p2.1.m1.1.2.3">ğ¸</ci><cn type="integer" id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">SE(3)</annotation></semantics></math> grasps which we use for picking objects. We split the dataset for training and testing. FigÂ <a href="#S0.F1" title="Figure 1 â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts examples of our environment assets, which we chose from common categories such as shelves, cubby, cabinet, drawers and table. All the assets are procedurally generated with the exception of shelves. For shelves, we aggregate the shelf categories from ShapeNetCore<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and filter assets which cannot be made watertight or if proper support surfaces cannot be extracted. Nonetheless, the object placements on all the environments, including the shelves, are procedurally generated. We include more examples of scenes in Appendix <a href="#A1" title="Appendix A Additional Examples of Synthetic Data and Scenes â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>. For testing, we only consider assets from the shelf environment dataset and the objects are from a novel dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> unseen during training. More exa</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Rearrangement Problem Generation:</span> A valid rearrangment problem needs a scene, a target object that the robot needs to grasp and the placement shelf that the robot needs to place the object. Given a scene, a target object is sampled if there exists a set of ground truth grasps associated with that object where they do not collide with the environment and have valid collision free inverse kinematic configuration. Once the target object is sampled, we check if a collision free placement pose for the target object exists within the placement shelf. To make sure that our rearrangement problems are challenging, the placement shelf is going to be different from the shelf that has the pick object. A problem is chosen if it passes both stages of sampling target object and also having a valid placement location. Overall, this process has a success rate of <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="20.8\%" display="inline"><semantics id="S3.p3.1.m1.1a"><mrow id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mn id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">20.8</mn><mo id="S3.p3.1.m1.1.1.1" xref="S3.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="latexml" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">20.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">20.8\%</annotation></semantics></math> in finding successful problems. An example of rearrangement problem is shown in FigÂ <a href="#S3.F2" title="Figure 2 â€£ 3 Procedural Data Generation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Neural Rearrangement Planning</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Our approach and model architecture is summarized in Fig <a href="#S4.F3" title="Figure 3 â€£ 4 Neural Rearrangement Planning â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We first learn a implicit 3D encoding of the scene point cloud. We use the encoded scene feature along with learned object features for fast point-cloud based collision detection with CabiNet. The same scene feature is then used for predicting waypoints for the rearrangement task with a simple feedforward network. Both these models are then used to generate robot trajectories for object rearrangement with a Model Predictive Path Integral (MPPI) policy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. More details of the trajectory generation process is given in Appendix <a href="#A2" title="Appendix B Trajectory Generation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.6" class="ltx_p"><span id="S4.p2.6.1" class="ltx_text ltx_font_bold">Collision Prediction:</span> This is a learned collision model that accepts as input the scene point cloud <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="X_{S}" display="inline"><semantics id="S4.p2.1.m1.1a"><msub id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">X</mi><mi id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">ğ‘‹</ci><ci id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">X_{S}</annotation></semantics></math> and the object point cloud <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="X_{O}" display="inline"><semantics id="S4.p2.2.m2.1a"><msub id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">X</mi><mi id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">O</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">ğ‘‹</ci><ci id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">ğ‘‚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">X_{O}</annotation></semantics></math>. The point cloud is encoded with voxelization and 3D convolution layers <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="\Psi_{S}=Enc(X_{S})" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><msub id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml"><mi mathvariant="normal" id="S4.p2.3.m3.1.1.3.2" xref="S4.p2.3.m3.1.1.3.2.cmml">Î¨</mi><mi id="S4.p2.3.m3.1.1.3.3" xref="S4.p2.3.m3.1.1.3.3.cmml">S</mi></msub><mo id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">=</mo><mrow id="S4.p2.3.m3.1.1.1" xref="S4.p2.3.m3.1.1.1.cmml"><mi id="S4.p2.3.m3.1.1.1.3" xref="S4.p2.3.m3.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.p2.3.m3.1.1.1.2" xref="S4.p2.3.m3.1.1.1.2.cmml">â€‹</mo><mi id="S4.p2.3.m3.1.1.1.4" xref="S4.p2.3.m3.1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p2.3.m3.1.1.1.2a" xref="S4.p2.3.m3.1.1.1.2.cmml">â€‹</mo><mi id="S4.p2.3.m3.1.1.1.5" xref="S4.p2.3.m3.1.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p2.3.m3.1.1.1.2b" xref="S4.p2.3.m3.1.1.1.2.cmml">â€‹</mo><mrow id="S4.p2.3.m3.1.1.1.1.1" xref="S4.p2.3.m3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p2.3.m3.1.1.1.1.1.2" xref="S4.p2.3.m3.1.1.1.1.1.1.cmml">(</mo><msub id="S4.p2.3.m3.1.1.1.1.1.1" xref="S4.p2.3.m3.1.1.1.1.1.1.cmml"><mi id="S4.p2.3.m3.1.1.1.1.1.1.2" xref="S4.p2.3.m3.1.1.1.1.1.1.2.cmml">X</mi><mi id="S4.p2.3.m3.1.1.1.1.1.1.3" xref="S4.p2.3.m3.1.1.1.1.1.1.3.cmml">S</mi></msub><mo stretchy="false" id="S4.p2.3.m3.1.1.1.1.1.3" xref="S4.p2.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><eq id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2"></eq><apply id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.3.1.cmml" xref="S4.p2.3.m3.1.1.3">subscript</csymbol><ci id="S4.p2.3.m3.1.1.3.2.cmml" xref="S4.p2.3.m3.1.1.3.2">Î¨</ci><ci id="S4.p2.3.m3.1.1.3.3.cmml" xref="S4.p2.3.m3.1.1.3.3">ğ‘†</ci></apply><apply id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1"><times id="S4.p2.3.m3.1.1.1.2.cmml" xref="S4.p2.3.m3.1.1.1.2"></times><ci id="S4.p2.3.m3.1.1.1.3.cmml" xref="S4.p2.3.m3.1.1.1.3">ğ¸</ci><ci id="S4.p2.3.m3.1.1.1.4.cmml" xref="S4.p2.3.m3.1.1.1.4">ğ‘›</ci><ci id="S4.p2.3.m3.1.1.1.5.cmml" xref="S4.p2.3.m3.1.1.1.5">ğ‘</ci><apply id="S4.p2.3.m3.1.1.1.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S4.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S4.p2.3.m3.1.1.1.1.1.1.2">ğ‘‹</ci><ci id="S4.p2.3.m3.1.1.1.1.1.1.3.cmml" xref="S4.p2.3.m3.1.1.1.1.1.1.3">ğ‘†</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">\Psi_{S}=Enc(X_{S})</annotation></semantics></math>. This is followed by a MLP binary classifier <math id="S4.p2.4.m4.3" class="ltx_Math" alttext="c=g_{\theta}(\Psi_{S},\Psi_{O},T_{O\rightarrow S})" display="inline"><semantics id="S4.p2.4.m4.3a"><mrow id="S4.p2.4.m4.3.3" xref="S4.p2.4.m4.3.3.cmml"><mi id="S4.p2.4.m4.3.3.5" xref="S4.p2.4.m4.3.3.5.cmml">c</mi><mo id="S4.p2.4.m4.3.3.4" xref="S4.p2.4.m4.3.3.4.cmml">=</mo><mrow id="S4.p2.4.m4.3.3.3" xref="S4.p2.4.m4.3.3.3.cmml"><msub id="S4.p2.4.m4.3.3.3.5" xref="S4.p2.4.m4.3.3.3.5.cmml"><mi id="S4.p2.4.m4.3.3.3.5.2" xref="S4.p2.4.m4.3.3.3.5.2.cmml">g</mi><mi id="S4.p2.4.m4.3.3.3.5.3" xref="S4.p2.4.m4.3.3.3.5.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S4.p2.4.m4.3.3.3.4" xref="S4.p2.4.m4.3.3.3.4.cmml">â€‹</mo><mrow id="S4.p2.4.m4.3.3.3.3.3" xref="S4.p2.4.m4.3.3.3.3.4.cmml"><mo stretchy="false" id="S4.p2.4.m4.3.3.3.3.3.4" xref="S4.p2.4.m4.3.3.3.3.4.cmml">(</mo><msub id="S4.p2.4.m4.1.1.1.1.1.1" xref="S4.p2.4.m4.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S4.p2.4.m4.1.1.1.1.1.1.2" xref="S4.p2.4.m4.1.1.1.1.1.1.2.cmml">Î¨</mi><mi id="S4.p2.4.m4.1.1.1.1.1.1.3" xref="S4.p2.4.m4.1.1.1.1.1.1.3.cmml">S</mi></msub><mo id="S4.p2.4.m4.3.3.3.3.3.5" xref="S4.p2.4.m4.3.3.3.3.4.cmml">,</mo><msub id="S4.p2.4.m4.2.2.2.2.2.2" xref="S4.p2.4.m4.2.2.2.2.2.2.cmml"><mi mathvariant="normal" id="S4.p2.4.m4.2.2.2.2.2.2.2" xref="S4.p2.4.m4.2.2.2.2.2.2.2.cmml">Î¨</mi><mi id="S4.p2.4.m4.2.2.2.2.2.2.3" xref="S4.p2.4.m4.2.2.2.2.2.2.3.cmml">O</mi></msub><mo id="S4.p2.4.m4.3.3.3.3.3.6" xref="S4.p2.4.m4.3.3.3.3.4.cmml">,</mo><msub id="S4.p2.4.m4.3.3.3.3.3.3" xref="S4.p2.4.m4.3.3.3.3.3.3.cmml"><mi id="S4.p2.4.m4.3.3.3.3.3.3.2" xref="S4.p2.4.m4.3.3.3.3.3.3.2.cmml">T</mi><mrow id="S4.p2.4.m4.3.3.3.3.3.3.3" xref="S4.p2.4.m4.3.3.3.3.3.3.3.cmml"><mi id="S4.p2.4.m4.3.3.3.3.3.3.3.2" xref="S4.p2.4.m4.3.3.3.3.3.3.3.2.cmml">O</mi><mo stretchy="false" id="S4.p2.4.m4.3.3.3.3.3.3.3.1" xref="S4.p2.4.m4.3.3.3.3.3.3.3.1.cmml">â†’</mo><mi id="S4.p2.4.m4.3.3.3.3.3.3.3.3" xref="S4.p2.4.m4.3.3.3.3.3.3.3.3.cmml">S</mi></mrow></msub><mo stretchy="false" id="S4.p2.4.m4.3.3.3.3.3.7" xref="S4.p2.4.m4.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.3b"><apply id="S4.p2.4.m4.3.3.cmml" xref="S4.p2.4.m4.3.3"><eq id="S4.p2.4.m4.3.3.4.cmml" xref="S4.p2.4.m4.3.3.4"></eq><ci id="S4.p2.4.m4.3.3.5.cmml" xref="S4.p2.4.m4.3.3.5">ğ‘</ci><apply id="S4.p2.4.m4.3.3.3.cmml" xref="S4.p2.4.m4.3.3.3"><times id="S4.p2.4.m4.3.3.3.4.cmml" xref="S4.p2.4.m4.3.3.3.4"></times><apply id="S4.p2.4.m4.3.3.3.5.cmml" xref="S4.p2.4.m4.3.3.3.5"><csymbol cd="ambiguous" id="S4.p2.4.m4.3.3.3.5.1.cmml" xref="S4.p2.4.m4.3.3.3.5">subscript</csymbol><ci id="S4.p2.4.m4.3.3.3.5.2.cmml" xref="S4.p2.4.m4.3.3.3.5.2">ğ‘”</ci><ci id="S4.p2.4.m4.3.3.3.5.3.cmml" xref="S4.p2.4.m4.3.3.3.5.3">ğœƒ</ci></apply><vector id="S4.p2.4.m4.3.3.3.3.4.cmml" xref="S4.p2.4.m4.3.3.3.3.3"><apply id="S4.p2.4.m4.1.1.1.1.1.1.cmml" xref="S4.p2.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S4.p2.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S4.p2.4.m4.1.1.1.1.1.1.2.cmml" xref="S4.p2.4.m4.1.1.1.1.1.1.2">Î¨</ci><ci id="S4.p2.4.m4.1.1.1.1.1.1.3.cmml" xref="S4.p2.4.m4.1.1.1.1.1.1.3">ğ‘†</ci></apply><apply id="S4.p2.4.m4.2.2.2.2.2.2.cmml" xref="S4.p2.4.m4.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.p2.4.m4.2.2.2.2.2.2.1.cmml" xref="S4.p2.4.m4.2.2.2.2.2.2">subscript</csymbol><ci id="S4.p2.4.m4.2.2.2.2.2.2.2.cmml" xref="S4.p2.4.m4.2.2.2.2.2.2.2">Î¨</ci><ci id="S4.p2.4.m4.2.2.2.2.2.2.3.cmml" xref="S4.p2.4.m4.2.2.2.2.2.2.3">ğ‘‚</ci></apply><apply id="S4.p2.4.m4.3.3.3.3.3.3.cmml" xref="S4.p2.4.m4.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S4.p2.4.m4.3.3.3.3.3.3.1.cmml" xref="S4.p2.4.m4.3.3.3.3.3.3">subscript</csymbol><ci id="S4.p2.4.m4.3.3.3.3.3.3.2.cmml" xref="S4.p2.4.m4.3.3.3.3.3.3.2">ğ‘‡</ci><apply id="S4.p2.4.m4.3.3.3.3.3.3.3.cmml" xref="S4.p2.4.m4.3.3.3.3.3.3.3"><ci id="S4.p2.4.m4.3.3.3.3.3.3.3.1.cmml" xref="S4.p2.4.m4.3.3.3.3.3.3.3.1">â†’</ci><ci id="S4.p2.4.m4.3.3.3.3.3.3.3.2.cmml" xref="S4.p2.4.m4.3.3.3.3.3.3.3.2">ğ‘‚</ci><ci id="S4.p2.4.m4.3.3.3.3.3.3.3.3.cmml" xref="S4.p2.4.m4.3.3.3.3.3.3.3.3">ğ‘†</ci></apply></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.3c">c=g_{\theta}(\Psi_{S},\Psi_{O},T_{O\rightarrow S})</annotation></semantics></math> that predicts if the object collide with the scene, where <math id="S4.p2.5.m5.1" class="ltx_Math" alttext="T_{O\rightarrow S}" display="inline"><semantics id="S4.p2.5.m5.1a"><msub id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml"><mi id="S4.p2.5.m5.1.1.2" xref="S4.p2.5.m5.1.1.2.cmml">T</mi><mrow id="S4.p2.5.m5.1.1.3" xref="S4.p2.5.m5.1.1.3.cmml"><mi id="S4.p2.5.m5.1.1.3.2" xref="S4.p2.5.m5.1.1.3.2.cmml">O</mi><mo stretchy="false" id="S4.p2.5.m5.1.1.3.1" xref="S4.p2.5.m5.1.1.3.1.cmml">â†’</mo><mi id="S4.p2.5.m5.1.1.3.3" xref="S4.p2.5.m5.1.1.3.3.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><apply id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p2.5.m5.1.1.1.cmml" xref="S4.p2.5.m5.1.1">subscript</csymbol><ci id="S4.p2.5.m5.1.1.2.cmml" xref="S4.p2.5.m5.1.1.2">ğ‘‡</ci><apply id="S4.p2.5.m5.1.1.3.cmml" xref="S4.p2.5.m5.1.1.3"><ci id="S4.p2.5.m5.1.1.3.1.cmml" xref="S4.p2.5.m5.1.1.3.1">â†’</ci><ci id="S4.p2.5.m5.1.1.3.2.cmml" xref="S4.p2.5.m5.1.1.3.2">ğ‘‚</ci><ci id="S4.p2.5.m5.1.1.3.3.cmml" xref="S4.p2.5.m5.1.1.3.3">ğ‘†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">T_{O\rightarrow S}</annotation></semantics></math> is the relative transformation between the object and the scene and <math id="S4.p2.6.m6.1" class="ltx_Math" alttext="\Psi_{O}" display="inline"><semantics id="S4.p2.6.m6.1a"><msub id="S4.p2.6.m6.1.1" xref="S4.p2.6.m6.1.1.cmml"><mi mathvariant="normal" id="S4.p2.6.m6.1.1.2" xref="S4.p2.6.m6.1.1.2.cmml">Î¨</mi><mi id="S4.p2.6.m6.1.1.3" xref="S4.p2.6.m6.1.1.3.cmml">O</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><apply id="S4.p2.6.m6.1.1.cmml" xref="S4.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p2.6.m6.1.1.1.cmml" xref="S4.p2.6.m6.1.1">subscript</csymbol><ci id="S4.p2.6.m6.1.1.2.cmml" xref="S4.p2.6.m6.1.1.2">Î¨</ci><ci id="S4.p2.6.m6.1.1.3.cmml" xref="S4.p2.6.m6.1.1.3">ğ‘‚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">\Psi_{O}</annotation></semantics></math> are the object features encoded with PointNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> layers akin to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. We adopt the model architecture of prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> but it was only trained on table top scenes which results in poor generalization to other type of scenes such as shelves. We showed that by scaling up the training data to more diverse set of environments the model generalizes to different type of scenes. CabiNet is trained with binary cross entropy loss and with SGD with constant learning rate. Overall, we train CabiNet for two weeks, considering over 650K scenes and for 60 billion query pairs. We also modified the architecture of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> by increasing the voxel sizes.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2304.09302/assets/figures/architecture_c.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="329" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.4.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.2.1" class="ltx_text" style="font-size:90%;">Our CabiNet architecture first encodes the scene point cloud with voxelization and 3D convolutions, shown in the top. The robot is only used for visualization and the robot point cloud is removed from the scene in practice. The scene features are then used with the object features to predict scene-object collision queries. We also predict waypoints (points colored in blue) for rearrangement, conditioned on latent vector <math id="S4.F3.2.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.F3.2.1.m1.1b"><mi id="S4.F3.2.1.m1.1.1" xref="S4.F3.2.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.F3.2.1.m1.1c"><ci id="S4.F3.2.1.m1.1.1.cmml" xref="S4.F3.2.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.2.1.m1.1d">z</annotation></semantics></math> and the current gripper position (shown in green).</span></figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.6" class="ltx_p"><span id="S4.p3.6.1" class="ltx_text ltx_font_bold">Waypoint Prediction:</span> Object rearrangement in more constrained environments such as shelves imposes new challenges. Some of the approaches that work quite reliably in table top settings fail in navigating the tight spaces between shelves. One such approach is the work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> that finds the collision free path by rolling out multiple trajectories in configuration space (<math id="S4.p3.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.p3.1.m1.1a"><mi id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">C</annotation></semantics></math>-space) between the current configuration of the robot and the closest goal <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">ğ’¢</mi><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">ğ’¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">\mathcal{G}</annotation></semantics></math> in the <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.p3.3.m3.1a"><mi id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><ci id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">C</annotation></semantics></math>-space. These rollouts are sampled around a straight line that connects the current robot configuration to <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.p3.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">ğ’¢</mi><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><ci id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">ğ’¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">\mathcal{G}</annotation></semantics></math>. Given the nominal line between the robot configuration and <math id="S4.p3.5.m5.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.p3.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml">ğ’¢</mi><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.1b"><ci id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1">ğ’¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.1c">\mathcal{G}</annotation></semantics></math>, different lines are sampled where the slope of the lines are gaussian distribution centered at the slope of nominal trajectory with a predefined variance. Each rollout is trimmed at the point of collision using SceneCollisionNet and ranked based on the distance of the rolloutâ€™s final point to <math id="S4.p3.6.m6.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.p3.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p3.6.m6.1.1" xref="S4.p3.6.m6.1.1.cmml">ğ’¢</mi><annotation-xml encoding="MathML-Content" id="S4.p3.6.m6.1b"><ci id="S4.p3.6.m6.1.1.cmml" xref="S4.p3.6.m6.1.1">ğ’¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.6.m6.1c">\mathcal{G}</annotation></semantics></math>. The success of this approach hinges on the quality of the sampled trajectory. The simple sampling explained above, works quite well for table top scenes where there is more free space. However, it fails to sample promising trajectories for shelves and more constrained environments, such as when the robot needs to move from one compartment of the shelf to another. The majority of the sampled trajectories around the nominal trajectory would go through the divider between shelves.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.7" class="ltx_p">To address this shortcoming, we propose to use CabiNet to sample waypoints with a larger signed distance value. Given the current end-effector position of the robot and the scene point cloud, it samples goals that gets the robot out of tight spaces. More formally, these waypoints <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="w\in\mathbb{R}^{3}" display="inline"><semantics id="S4.p4.1.m1.1a"><mrow id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mi id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml">w</mi><mo id="S4.p4.1.m1.1.1.1" xref="S4.p4.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml"><mi id="S4.p4.1.m1.1.1.3.2" xref="S4.p4.1.m1.1.1.3.2.cmml">â„</mi><mn id="S4.p4.1.m1.1.1.3.3" xref="S4.p4.1.m1.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><in id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1"></in><ci id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">ğ‘¤</ci><apply id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.1.3.1.cmml" xref="S4.p4.1.m1.1.1.3">superscript</csymbol><ci id="S4.p4.1.m1.1.1.3.2.cmml" xref="S4.p4.1.m1.1.1.3.2">â„</ci><cn type="integer" id="S4.p4.1.m1.1.1.3.3.cmml" xref="S4.p4.1.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">w\in\mathbb{R}^{3}</annotation></semantics></math> are defined as to be in the set <math id="S4.p4.2.m2.4" class="ltx_Math" alttext="\{w|\tau_{min}\leq SDF(S,w)\leq\tau_{max}\cap\lVert w-p_{gripper}\rVert\leq D\}" display="inline"><semantics id="S4.p4.2.m2.4a"><mrow id="S4.p4.2.m2.4.4.1" xref="S4.p4.2.m2.4.4.2.cmml"><mo stretchy="false" id="S4.p4.2.m2.4.4.1.2" xref="S4.p4.2.m2.4.4.2.1.cmml">{</mo><mi id="S4.p4.2.m2.3.3" xref="S4.p4.2.m2.3.3.cmml">w</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.3" xref="S4.p4.2.m2.4.4.2.1.cmml">|</mo><mrow id="S4.p4.2.m2.4.4.1.1" xref="S4.p4.2.m2.4.4.1.1.cmml"><msub id="S4.p4.2.m2.4.4.1.1.3" xref="S4.p4.2.m2.4.4.1.1.3.cmml"><mi id="S4.p4.2.m2.4.4.1.1.3.2" xref="S4.p4.2.m2.4.4.1.1.3.2.cmml">Ï„</mi><mrow id="S4.p4.2.m2.4.4.1.1.3.3" xref="S4.p4.2.m2.4.4.1.1.3.3.cmml"><mi id="S4.p4.2.m2.4.4.1.1.3.3.2" xref="S4.p4.2.m2.4.4.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.3.3.1" xref="S4.p4.2.m2.4.4.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.3.3.3" xref="S4.p4.2.m2.4.4.1.1.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.3.3.1a" xref="S4.p4.2.m2.4.4.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.3.3.4" xref="S4.p4.2.m2.4.4.1.1.3.3.4.cmml">n</mi></mrow></msub><mo id="S4.p4.2.m2.4.4.1.1.4" xref="S4.p4.2.m2.4.4.1.1.4.cmml">â‰¤</mo><mrow id="S4.p4.2.m2.4.4.1.1.5" xref="S4.p4.2.m2.4.4.1.1.5.cmml"><mi id="S4.p4.2.m2.4.4.1.1.5.2" xref="S4.p4.2.m2.4.4.1.1.5.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.5.1" xref="S4.p4.2.m2.4.4.1.1.5.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.5.3" xref="S4.p4.2.m2.4.4.1.1.5.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.5.1a" xref="S4.p4.2.m2.4.4.1.1.5.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.5.4" xref="S4.p4.2.m2.4.4.1.1.5.4.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.5.1b" xref="S4.p4.2.m2.4.4.1.1.5.1.cmml">â€‹</mo><mrow id="S4.p4.2.m2.4.4.1.1.5.5.2" xref="S4.p4.2.m2.4.4.1.1.5.5.1.cmml"><mo stretchy="false" id="S4.p4.2.m2.4.4.1.1.5.5.2.1" xref="S4.p4.2.m2.4.4.1.1.5.5.1.cmml">(</mo><mi id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">S</mi><mo id="S4.p4.2.m2.4.4.1.1.5.5.2.2" xref="S4.p4.2.m2.4.4.1.1.5.5.1.cmml">,</mo><mi id="S4.p4.2.m2.2.2" xref="S4.p4.2.m2.2.2.cmml">w</mi><mo stretchy="false" id="S4.p4.2.m2.4.4.1.1.5.5.2.3" xref="S4.p4.2.m2.4.4.1.1.5.5.1.cmml">)</mo></mrow></mrow><mo id="S4.p4.2.m2.4.4.1.1.6" xref="S4.p4.2.m2.4.4.1.1.6.cmml">â‰¤</mo><mrow id="S4.p4.2.m2.4.4.1.1.1" xref="S4.p4.2.m2.4.4.1.1.1.cmml"><msub id="S4.p4.2.m2.4.4.1.1.1.3" xref="S4.p4.2.m2.4.4.1.1.1.3.cmml"><mi id="S4.p4.2.m2.4.4.1.1.1.3.2" xref="S4.p4.2.m2.4.4.1.1.1.3.2.cmml">Ï„</mi><mrow id="S4.p4.2.m2.4.4.1.1.1.3.3" xref="S4.p4.2.m2.4.4.1.1.1.3.3.cmml"><mi id="S4.p4.2.m2.4.4.1.1.1.3.3.2" xref="S4.p4.2.m2.4.4.1.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.1.3.3.1" xref="S4.p4.2.m2.4.4.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.1.3.3.3" xref="S4.p4.2.m2.4.4.1.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.1.3.3.1a" xref="S4.p4.2.m2.4.4.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.1.3.3.4" xref="S4.p4.2.m2.4.4.1.1.1.3.3.4.cmml">x</mi></mrow></msub><mo id="S4.p4.2.m2.4.4.1.1.1.2" xref="S4.p4.2.m2.4.4.1.1.1.2.cmml">âˆ©</mo><mrow id="S4.p4.2.m2.4.4.1.1.1.1.1" xref="S4.p4.2.m2.4.4.1.1.1.1.2.cmml"><mo fence="true" lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.1.1.1.2" xref="S4.p4.2.m2.4.4.1.1.1.1.2.1.cmml">âˆ¥</mo><mrow id="S4.p4.2.m2.4.4.1.1.1.1.1.1" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.cmml"><mi id="S4.p4.2.m2.4.4.1.1.1.1.1.1.2" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.2.cmml">w</mi><mo id="S4.p4.2.m2.4.4.1.1.1.1.1.1.1" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.cmml"><mi id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.2" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.2.cmml">p</mi><mrow id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.cmml"><mi id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.2" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.3" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1a" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.4" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1b" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.5" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1c" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.6" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.6.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1d" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.7" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1e" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.8" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.8.cmml">r</mi></mrow></msub></mrow><mo fence="true" lspace="0em" rspace="0.1389em" id="S4.p4.2.m2.4.4.1.1.1.1.1.3" xref="S4.p4.2.m2.4.4.1.1.1.1.2.1.cmml">âˆ¥</mo></mrow></mrow><mo lspace="0.1389em" id="S4.p4.2.m2.4.4.1.1.7" xref="S4.p4.2.m2.4.4.1.1.7.cmml">â‰¤</mo><mi id="S4.p4.2.m2.4.4.1.1.8" xref="S4.p4.2.m2.4.4.1.1.8.cmml">D</mi></mrow><mo stretchy="false" id="S4.p4.2.m2.4.4.1.4" xref="S4.p4.2.m2.4.4.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.4b"><apply id="S4.p4.2.m2.4.4.2.cmml" xref="S4.p4.2.m2.4.4.1"><csymbol cd="latexml" id="S4.p4.2.m2.4.4.2.1.cmml" xref="S4.p4.2.m2.4.4.1.2">conditional-set</csymbol><ci id="S4.p4.2.m2.3.3.cmml" xref="S4.p4.2.m2.3.3">ğ‘¤</ci><apply id="S4.p4.2.m2.4.4.1.1.cmml" xref="S4.p4.2.m2.4.4.1.1"><and id="S4.p4.2.m2.4.4.1.1a.cmml" xref="S4.p4.2.m2.4.4.1.1"></and><apply id="S4.p4.2.m2.4.4.1.1b.cmml" xref="S4.p4.2.m2.4.4.1.1"><leq id="S4.p4.2.m2.4.4.1.1.4.cmml" xref="S4.p4.2.m2.4.4.1.1.4"></leq><apply id="S4.p4.2.m2.4.4.1.1.3.cmml" xref="S4.p4.2.m2.4.4.1.1.3"><csymbol cd="ambiguous" id="S4.p4.2.m2.4.4.1.1.3.1.cmml" xref="S4.p4.2.m2.4.4.1.1.3">subscript</csymbol><ci id="S4.p4.2.m2.4.4.1.1.3.2.cmml" xref="S4.p4.2.m2.4.4.1.1.3.2">ğœ</ci><apply id="S4.p4.2.m2.4.4.1.1.3.3.cmml" xref="S4.p4.2.m2.4.4.1.1.3.3"><times id="S4.p4.2.m2.4.4.1.1.3.3.1.cmml" xref="S4.p4.2.m2.4.4.1.1.3.3.1"></times><ci id="S4.p4.2.m2.4.4.1.1.3.3.2.cmml" xref="S4.p4.2.m2.4.4.1.1.3.3.2">ğ‘š</ci><ci id="S4.p4.2.m2.4.4.1.1.3.3.3.cmml" xref="S4.p4.2.m2.4.4.1.1.3.3.3">ğ‘–</ci><ci id="S4.p4.2.m2.4.4.1.1.3.3.4.cmml" xref="S4.p4.2.m2.4.4.1.1.3.3.4">ğ‘›</ci></apply></apply><apply id="S4.p4.2.m2.4.4.1.1.5.cmml" xref="S4.p4.2.m2.4.4.1.1.5"><times id="S4.p4.2.m2.4.4.1.1.5.1.cmml" xref="S4.p4.2.m2.4.4.1.1.5.1"></times><ci id="S4.p4.2.m2.4.4.1.1.5.2.cmml" xref="S4.p4.2.m2.4.4.1.1.5.2">ğ‘†</ci><ci id="S4.p4.2.m2.4.4.1.1.5.3.cmml" xref="S4.p4.2.m2.4.4.1.1.5.3">ğ·</ci><ci id="S4.p4.2.m2.4.4.1.1.5.4.cmml" xref="S4.p4.2.m2.4.4.1.1.5.4">ğ¹</ci><interval closure="open" id="S4.p4.2.m2.4.4.1.1.5.5.1.cmml" xref="S4.p4.2.m2.4.4.1.1.5.5.2"><ci id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1">ğ‘†</ci><ci id="S4.p4.2.m2.2.2.cmml" xref="S4.p4.2.m2.2.2">ğ‘¤</ci></interval></apply></apply><apply id="S4.p4.2.m2.4.4.1.1c.cmml" xref="S4.p4.2.m2.4.4.1.1"><leq id="S4.p4.2.m2.4.4.1.1.6.cmml" xref="S4.p4.2.m2.4.4.1.1.6"></leq><share href="#S4.p4.2.m2.4.4.1.1.5.cmml" id="S4.p4.2.m2.4.4.1.1d.cmml" xref="S4.p4.2.m2.4.4.1.1"></share><apply id="S4.p4.2.m2.4.4.1.1.1.cmml" xref="S4.p4.2.m2.4.4.1.1.1"><intersect id="S4.p4.2.m2.4.4.1.1.1.2.cmml" xref="S4.p4.2.m2.4.4.1.1.1.2"></intersect><apply id="S4.p4.2.m2.4.4.1.1.1.3.cmml" xref="S4.p4.2.m2.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S4.p4.2.m2.4.4.1.1.1.3.1.cmml" xref="S4.p4.2.m2.4.4.1.1.1.3">subscript</csymbol><ci id="S4.p4.2.m2.4.4.1.1.1.3.2.cmml" xref="S4.p4.2.m2.4.4.1.1.1.3.2">ğœ</ci><apply id="S4.p4.2.m2.4.4.1.1.1.3.3.cmml" xref="S4.p4.2.m2.4.4.1.1.1.3.3"><times id="S4.p4.2.m2.4.4.1.1.1.3.3.1.cmml" xref="S4.p4.2.m2.4.4.1.1.1.3.3.1"></times><ci id="S4.p4.2.m2.4.4.1.1.1.3.3.2.cmml" xref="S4.p4.2.m2.4.4.1.1.1.3.3.2">ğ‘š</ci><ci id="S4.p4.2.m2.4.4.1.1.1.3.3.3.cmml" xref="S4.p4.2.m2.4.4.1.1.1.3.3.3">ğ‘</ci><ci id="S4.p4.2.m2.4.4.1.1.1.3.3.4.cmml" xref="S4.p4.2.m2.4.4.1.1.1.3.3.4">ğ‘¥</ci></apply></apply><apply id="S4.p4.2.m2.4.4.1.1.1.1.2.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S4.p4.2.m2.4.4.1.1.1.1.2.1.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S4.p4.2.m2.4.4.1.1.1.1.1.1.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1"><minus id="S4.p4.2.m2.4.4.1.1.1.1.1.1.1.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.1"></minus><ci id="S4.p4.2.m2.4.4.1.1.1.1.1.1.2.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.2">ğ‘¤</ci><apply id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.1.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.2.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.2">ğ‘</ci><apply id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3"><times id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.1"></times><ci id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.2.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.2">ğ‘”</ci><ci id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.3.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.3">ğ‘Ÿ</ci><ci id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.4.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.4">ğ‘–</ci><ci id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.5.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.5">ğ‘</ci><ci id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.6.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.6">ğ‘</ci><ci id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.7.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.7">ğ‘’</ci><ci id="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.8.cmml" xref="S4.p4.2.m2.4.4.1.1.1.1.1.1.3.3.8">ğ‘Ÿ</ci></apply></apply></apply></apply></apply></apply><apply id="S4.p4.2.m2.4.4.1.1e.cmml" xref="S4.p4.2.m2.4.4.1.1"><leq id="S4.p4.2.m2.4.4.1.1.7.cmml" xref="S4.p4.2.m2.4.4.1.1.7"></leq><share href="#S4.p4.2.m2.4.4.1.1.1.cmml" id="S4.p4.2.m2.4.4.1.1f.cmml" xref="S4.p4.2.m2.4.4.1.1"></share><ci id="S4.p4.2.m2.4.4.1.1.8.cmml" xref="S4.p4.2.m2.4.4.1.1.8">ğ·</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.4c">\{w|\tau_{min}\leq SDF(S,w)\leq\tau_{max}\cap\lVert w-p_{gripper}\rVert\leq D\}</annotation></semantics></math>, where <math id="S4.p4.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.p4.3.m3.1a"><mi id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><ci id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">S</annotation></semantics></math> is the scene mesh, <math id="S4.p4.4.m4.1" class="ltx_Math" alttext="p_{gripper}" display="inline"><semantics id="S4.p4.4.m4.1a"><msub id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml"><mi id="S4.p4.4.m4.1.1.2" xref="S4.p4.4.m4.1.1.2.cmml">p</mi><mrow id="S4.p4.4.m4.1.1.3" xref="S4.p4.4.m4.1.1.3.cmml"><mi id="S4.p4.4.m4.1.1.3.2" xref="S4.p4.4.m4.1.1.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.p4.4.m4.1.1.3.1" xref="S4.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p4.4.m4.1.1.3.3" xref="S4.p4.4.m4.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p4.4.m4.1.1.3.1a" xref="S4.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p4.4.m4.1.1.3.4" xref="S4.p4.4.m4.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p4.4.m4.1.1.3.1b" xref="S4.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p4.4.m4.1.1.3.5" xref="S4.p4.4.m4.1.1.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p4.4.m4.1.1.3.1c" xref="S4.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p4.4.m4.1.1.3.6" xref="S4.p4.4.m4.1.1.3.6.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p4.4.m4.1.1.3.1d" xref="S4.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p4.4.m4.1.1.3.7" xref="S4.p4.4.m4.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p4.4.m4.1.1.3.1e" xref="S4.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p4.4.m4.1.1.3.8" xref="S4.p4.4.m4.1.1.3.8.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><apply id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p4.4.m4.1.1.1.cmml" xref="S4.p4.4.m4.1.1">subscript</csymbol><ci id="S4.p4.4.m4.1.1.2.cmml" xref="S4.p4.4.m4.1.1.2">ğ‘</ci><apply id="S4.p4.4.m4.1.1.3.cmml" xref="S4.p4.4.m4.1.1.3"><times id="S4.p4.4.m4.1.1.3.1.cmml" xref="S4.p4.4.m4.1.1.3.1"></times><ci id="S4.p4.4.m4.1.1.3.2.cmml" xref="S4.p4.4.m4.1.1.3.2">ğ‘”</ci><ci id="S4.p4.4.m4.1.1.3.3.cmml" xref="S4.p4.4.m4.1.1.3.3">ğ‘Ÿ</ci><ci id="S4.p4.4.m4.1.1.3.4.cmml" xref="S4.p4.4.m4.1.1.3.4">ğ‘–</ci><ci id="S4.p4.4.m4.1.1.3.5.cmml" xref="S4.p4.4.m4.1.1.3.5">ğ‘</ci><ci id="S4.p4.4.m4.1.1.3.6.cmml" xref="S4.p4.4.m4.1.1.3.6">ğ‘</ci><ci id="S4.p4.4.m4.1.1.3.7.cmml" xref="S4.p4.4.m4.1.1.3.7">ğ‘’</ci><ci id="S4.p4.4.m4.1.1.3.8.cmml" xref="S4.p4.4.m4.1.1.3.8">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">p_{gripper}</annotation></semantics></math> is the end-effector position. The CabiNet waypoint sampler is modelled as a conditional generator <math id="S4.p4.5.m5.3" class="ltx_Math" alttext="\hat{w}=f_{\theta}(\Psi_{S},p_{gripper},z)" display="inline"><semantics id="S4.p4.5.m5.3a"><mrow id="S4.p4.5.m5.3.3" xref="S4.p4.5.m5.3.3.cmml"><mover accent="true" id="S4.p4.5.m5.3.3.4" xref="S4.p4.5.m5.3.3.4.cmml"><mi id="S4.p4.5.m5.3.3.4.2" xref="S4.p4.5.m5.3.3.4.2.cmml">w</mi><mo id="S4.p4.5.m5.3.3.4.1" xref="S4.p4.5.m5.3.3.4.1.cmml">^</mo></mover><mo id="S4.p4.5.m5.3.3.3" xref="S4.p4.5.m5.3.3.3.cmml">=</mo><mrow id="S4.p4.5.m5.3.3.2" xref="S4.p4.5.m5.3.3.2.cmml"><msub id="S4.p4.5.m5.3.3.2.4" xref="S4.p4.5.m5.3.3.2.4.cmml"><mi id="S4.p4.5.m5.3.3.2.4.2" xref="S4.p4.5.m5.3.3.2.4.2.cmml">f</mi><mi id="S4.p4.5.m5.3.3.2.4.3" xref="S4.p4.5.m5.3.3.2.4.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S4.p4.5.m5.3.3.2.3" xref="S4.p4.5.m5.3.3.2.3.cmml">â€‹</mo><mrow id="S4.p4.5.m5.3.3.2.2.2" xref="S4.p4.5.m5.3.3.2.2.3.cmml"><mo stretchy="false" id="S4.p4.5.m5.3.3.2.2.2.3" xref="S4.p4.5.m5.3.3.2.2.3.cmml">(</mo><msub id="S4.p4.5.m5.2.2.1.1.1.1" xref="S4.p4.5.m5.2.2.1.1.1.1.cmml"><mi mathvariant="normal" id="S4.p4.5.m5.2.2.1.1.1.1.2" xref="S4.p4.5.m5.2.2.1.1.1.1.2.cmml">Î¨</mi><mi id="S4.p4.5.m5.2.2.1.1.1.1.3" xref="S4.p4.5.m5.2.2.1.1.1.1.3.cmml">S</mi></msub><mo id="S4.p4.5.m5.3.3.2.2.2.4" xref="S4.p4.5.m5.3.3.2.2.3.cmml">,</mo><msub id="S4.p4.5.m5.3.3.2.2.2.2" xref="S4.p4.5.m5.3.3.2.2.2.2.cmml"><mi id="S4.p4.5.m5.3.3.2.2.2.2.2" xref="S4.p4.5.m5.3.3.2.2.2.2.2.cmml">p</mi><mrow id="S4.p4.5.m5.3.3.2.2.2.2.3" xref="S4.p4.5.m5.3.3.2.2.2.2.3.cmml"><mi id="S4.p4.5.m5.3.3.2.2.2.2.3.2" xref="S4.p4.5.m5.3.3.2.2.2.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.p4.5.m5.3.3.2.2.2.2.3.1" xref="S4.p4.5.m5.3.3.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S4.p4.5.m5.3.3.2.2.2.2.3.3" xref="S4.p4.5.m5.3.3.2.2.2.2.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p4.5.m5.3.3.2.2.2.2.3.1a" xref="S4.p4.5.m5.3.3.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S4.p4.5.m5.3.3.2.2.2.2.3.4" xref="S4.p4.5.m5.3.3.2.2.2.2.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p4.5.m5.3.3.2.2.2.2.3.1b" xref="S4.p4.5.m5.3.3.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S4.p4.5.m5.3.3.2.2.2.2.3.5" xref="S4.p4.5.m5.3.3.2.2.2.2.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p4.5.m5.3.3.2.2.2.2.3.1c" xref="S4.p4.5.m5.3.3.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S4.p4.5.m5.3.3.2.2.2.2.3.6" xref="S4.p4.5.m5.3.3.2.2.2.2.3.6.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p4.5.m5.3.3.2.2.2.2.3.1d" xref="S4.p4.5.m5.3.3.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S4.p4.5.m5.3.3.2.2.2.2.3.7" xref="S4.p4.5.m5.3.3.2.2.2.2.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p4.5.m5.3.3.2.2.2.2.3.1e" xref="S4.p4.5.m5.3.3.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S4.p4.5.m5.3.3.2.2.2.2.3.8" xref="S4.p4.5.m5.3.3.2.2.2.2.3.8.cmml">r</mi></mrow></msub><mo id="S4.p4.5.m5.3.3.2.2.2.5" xref="S4.p4.5.m5.3.3.2.2.3.cmml">,</mo><mi id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml">z</mi><mo stretchy="false" id="S4.p4.5.m5.3.3.2.2.2.6" xref="S4.p4.5.m5.3.3.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.3b"><apply id="S4.p4.5.m5.3.3.cmml" xref="S4.p4.5.m5.3.3"><eq id="S4.p4.5.m5.3.3.3.cmml" xref="S4.p4.5.m5.3.3.3"></eq><apply id="S4.p4.5.m5.3.3.4.cmml" xref="S4.p4.5.m5.3.3.4"><ci id="S4.p4.5.m5.3.3.4.1.cmml" xref="S4.p4.5.m5.3.3.4.1">^</ci><ci id="S4.p4.5.m5.3.3.4.2.cmml" xref="S4.p4.5.m5.3.3.4.2">ğ‘¤</ci></apply><apply id="S4.p4.5.m5.3.3.2.cmml" xref="S4.p4.5.m5.3.3.2"><times id="S4.p4.5.m5.3.3.2.3.cmml" xref="S4.p4.5.m5.3.3.2.3"></times><apply id="S4.p4.5.m5.3.3.2.4.cmml" xref="S4.p4.5.m5.3.3.2.4"><csymbol cd="ambiguous" id="S4.p4.5.m5.3.3.2.4.1.cmml" xref="S4.p4.5.m5.3.3.2.4">subscript</csymbol><ci id="S4.p4.5.m5.3.3.2.4.2.cmml" xref="S4.p4.5.m5.3.3.2.4.2">ğ‘“</ci><ci id="S4.p4.5.m5.3.3.2.4.3.cmml" xref="S4.p4.5.m5.3.3.2.4.3">ğœƒ</ci></apply><vector id="S4.p4.5.m5.3.3.2.2.3.cmml" xref="S4.p4.5.m5.3.3.2.2.2"><apply id="S4.p4.5.m5.2.2.1.1.1.1.cmml" xref="S4.p4.5.m5.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.p4.5.m5.2.2.1.1.1.1.1.cmml" xref="S4.p4.5.m5.2.2.1.1.1.1">subscript</csymbol><ci id="S4.p4.5.m5.2.2.1.1.1.1.2.cmml" xref="S4.p4.5.m5.2.2.1.1.1.1.2">Î¨</ci><ci id="S4.p4.5.m5.2.2.1.1.1.1.3.cmml" xref="S4.p4.5.m5.2.2.1.1.1.1.3">ğ‘†</ci></apply><apply id="S4.p4.5.m5.3.3.2.2.2.2.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S4.p4.5.m5.3.3.2.2.2.2.1.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2">subscript</csymbol><ci id="S4.p4.5.m5.3.3.2.2.2.2.2.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2.2">ğ‘</ci><apply id="S4.p4.5.m5.3.3.2.2.2.2.3.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2.3"><times id="S4.p4.5.m5.3.3.2.2.2.2.3.1.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2.3.1"></times><ci id="S4.p4.5.m5.3.3.2.2.2.2.3.2.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2.3.2">ğ‘”</ci><ci id="S4.p4.5.m5.3.3.2.2.2.2.3.3.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2.3.3">ğ‘Ÿ</ci><ci id="S4.p4.5.m5.3.3.2.2.2.2.3.4.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2.3.4">ğ‘–</ci><ci id="S4.p4.5.m5.3.3.2.2.2.2.3.5.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2.3.5">ğ‘</ci><ci id="S4.p4.5.m5.3.3.2.2.2.2.3.6.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2.3.6">ğ‘</ci><ci id="S4.p4.5.m5.3.3.2.2.2.2.3.7.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2.3.7">ğ‘’</ci><ci id="S4.p4.5.m5.3.3.2.2.2.2.3.8.cmml" xref="S4.p4.5.m5.3.3.2.2.2.2.3.8">ğ‘Ÿ</ci></apply></apply><ci id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1">ğ‘§</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.3c">\hat{w}=f_{\theta}(\Psi_{S},p_{gripper},z)</annotation></semantics></math> as in Generative Adversarial Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
Instead of using adversarial training, we train the sampler with Implicit Maximum Likelihood Estimation (IMLE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> which attempts to make each generated sample similar to a ground truth sample. We empirically found that making the loss bidirectional improved the generated samples - enforcing that ground truth samples are also similar to the set of nearest predicted samples. Let <math id="S4.p4.6.m6.5" class="ltx_Math" alttext="z_{1},\ldots,z_{m}\sim\mathcal{N}(0,I)" display="inline"><semantics id="S4.p4.6.m6.5a"><mrow id="S4.p4.6.m6.5.5" xref="S4.p4.6.m6.5.5.cmml"><mrow id="S4.p4.6.m6.5.5.2.2" xref="S4.p4.6.m6.5.5.2.3.cmml"><msub id="S4.p4.6.m6.4.4.1.1.1" xref="S4.p4.6.m6.4.4.1.1.1.cmml"><mi id="S4.p4.6.m6.4.4.1.1.1.2" xref="S4.p4.6.m6.4.4.1.1.1.2.cmml">z</mi><mn id="S4.p4.6.m6.4.4.1.1.1.3" xref="S4.p4.6.m6.4.4.1.1.1.3.cmml">1</mn></msub><mo id="S4.p4.6.m6.5.5.2.2.3" xref="S4.p4.6.m6.5.5.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.p4.6.m6.3.3" xref="S4.p4.6.m6.3.3.cmml">â€¦</mi><mo id="S4.p4.6.m6.5.5.2.2.4" xref="S4.p4.6.m6.5.5.2.3.cmml">,</mo><msub id="S4.p4.6.m6.5.5.2.2.2" xref="S4.p4.6.m6.5.5.2.2.2.cmml"><mi id="S4.p4.6.m6.5.5.2.2.2.2" xref="S4.p4.6.m6.5.5.2.2.2.2.cmml">z</mi><mi id="S4.p4.6.m6.5.5.2.2.2.3" xref="S4.p4.6.m6.5.5.2.2.2.3.cmml">m</mi></msub></mrow><mo id="S4.p4.6.m6.5.5.3" xref="S4.p4.6.m6.5.5.3.cmml">âˆ¼</mo><mrow id="S4.p4.6.m6.5.5.4" xref="S4.p4.6.m6.5.5.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p4.6.m6.5.5.4.2" xref="S4.p4.6.m6.5.5.4.2.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="S4.p4.6.m6.5.5.4.1" xref="S4.p4.6.m6.5.5.4.1.cmml">â€‹</mo><mrow id="S4.p4.6.m6.5.5.4.3.2" xref="S4.p4.6.m6.5.5.4.3.1.cmml"><mo stretchy="false" id="S4.p4.6.m6.5.5.4.3.2.1" xref="S4.p4.6.m6.5.5.4.3.1.cmml">(</mo><mn id="S4.p4.6.m6.1.1" xref="S4.p4.6.m6.1.1.cmml">0</mn><mo id="S4.p4.6.m6.5.5.4.3.2.2" xref="S4.p4.6.m6.5.5.4.3.1.cmml">,</mo><mi id="S4.p4.6.m6.2.2" xref="S4.p4.6.m6.2.2.cmml">I</mi><mo stretchy="false" id="S4.p4.6.m6.5.5.4.3.2.3" xref="S4.p4.6.m6.5.5.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.6.m6.5b"><apply id="S4.p4.6.m6.5.5.cmml" xref="S4.p4.6.m6.5.5"><csymbol cd="latexml" id="S4.p4.6.m6.5.5.3.cmml" xref="S4.p4.6.m6.5.5.3">similar-to</csymbol><list id="S4.p4.6.m6.5.5.2.3.cmml" xref="S4.p4.6.m6.5.5.2.2"><apply id="S4.p4.6.m6.4.4.1.1.1.cmml" xref="S4.p4.6.m6.4.4.1.1.1"><csymbol cd="ambiguous" id="S4.p4.6.m6.4.4.1.1.1.1.cmml" xref="S4.p4.6.m6.4.4.1.1.1">subscript</csymbol><ci id="S4.p4.6.m6.4.4.1.1.1.2.cmml" xref="S4.p4.6.m6.4.4.1.1.1.2">ğ‘§</ci><cn type="integer" id="S4.p4.6.m6.4.4.1.1.1.3.cmml" xref="S4.p4.6.m6.4.4.1.1.1.3">1</cn></apply><ci id="S4.p4.6.m6.3.3.cmml" xref="S4.p4.6.m6.3.3">â€¦</ci><apply id="S4.p4.6.m6.5.5.2.2.2.cmml" xref="S4.p4.6.m6.5.5.2.2.2"><csymbol cd="ambiguous" id="S4.p4.6.m6.5.5.2.2.2.1.cmml" xref="S4.p4.6.m6.5.5.2.2.2">subscript</csymbol><ci id="S4.p4.6.m6.5.5.2.2.2.2.cmml" xref="S4.p4.6.m6.5.5.2.2.2.2">ğ‘§</ci><ci id="S4.p4.6.m6.5.5.2.2.2.3.cmml" xref="S4.p4.6.m6.5.5.2.2.2.3">ğ‘š</ci></apply></list><apply id="S4.p4.6.m6.5.5.4.cmml" xref="S4.p4.6.m6.5.5.4"><times id="S4.p4.6.m6.5.5.4.1.cmml" xref="S4.p4.6.m6.5.5.4.1"></times><ci id="S4.p4.6.m6.5.5.4.2.cmml" xref="S4.p4.6.m6.5.5.4.2">ğ’©</ci><interval closure="open" id="S4.p4.6.m6.5.5.4.3.1.cmml" xref="S4.p4.6.m6.5.5.4.3.2"><cn type="integer" id="S4.p4.6.m6.1.1.cmml" xref="S4.p4.6.m6.1.1">0</cn><ci id="S4.p4.6.m6.2.2.cmml" xref="S4.p4.6.m6.2.2">ğ¼</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.6.m6.5c">z_{1},\ldots,z_{m}\sim\mathcal{N}(0,I)</annotation></semantics></math> denote randomly sampled latent input noise vectors and <math id="S4.p4.7.m7.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S4.p4.7.m7.1a"><msub id="S4.p4.7.m7.1.1" xref="S4.p4.7.m7.1.1.cmml"><mi id="S4.p4.7.m7.1.1.2" xref="S4.p4.7.m7.1.1.2.cmml">w</mi><mi id="S4.p4.7.m7.1.1.3" xref="S4.p4.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p4.7.m7.1b"><apply id="S4.p4.7.m7.1.1.cmml" xref="S4.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S4.p4.7.m7.1.1.1.cmml" xref="S4.p4.7.m7.1.1">subscript</csymbol><ci id="S4.p4.7.m7.1.1.2.cmml" xref="S4.p4.7.m7.1.1.2">ğ‘¤</ci><ci id="S4.p4.7.m7.1.1.3.cmml" xref="S4.p4.7.m7.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.7.m7.1c">w_{i}</annotation></semantics></math> the ground truth waypoints. The IMLE loss is as follows:</p>
</div>
<div id="S4.p5" class="ltx_para">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.2" class="ltx_Math" alttext="\mathcal{L}_{IMLE}=\frac{1}{n}\sum_{i=1}^{n}\min_{j}|\hat{w_{j}}-w_{i}|+\frac{1}{m}\sum_{j=1}^{m}\min_{i}|\hat{w_{j}}-w_{i}|" display="block"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml"><msub id="S4.E1.m1.2.2.4" xref="S4.E1.m1.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.4.2" xref="S4.E1.m1.2.2.4.2.cmml">â„’</mi><mrow id="S4.E1.m1.2.2.4.3" xref="S4.E1.m1.2.2.4.3.cmml"><mi id="S4.E1.m1.2.2.4.3.2" xref="S4.E1.m1.2.2.4.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.4.3.1" xref="S4.E1.m1.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.2.2.4.3.3" xref="S4.E1.m1.2.2.4.3.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.4.3.1a" xref="S4.E1.m1.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.2.2.4.3.4" xref="S4.E1.m1.2.2.4.3.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.4.3.1b" xref="S4.E1.m1.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.2.2.4.3.5" xref="S4.E1.m1.2.2.4.3.5.cmml">E</mi></mrow></msub><mo id="S4.E1.m1.2.2.3" xref="S4.E1.m1.2.2.3.cmml">=</mo><mrow id="S4.E1.m1.2.2.2" xref="S4.E1.m1.2.2.2.cmml"><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mfrac id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml"><mn id="S4.E1.m1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.3.2.cmml">1</mn><mi id="S4.E1.m1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.cmml"><munderover id="S4.E1.m1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S4.E1.m1.1.1.1.1.1.2.2.2" xref="S4.E1.m1.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S4.E1.m1.1.1.1.1.1.2.2.3" xref="S4.E1.m1.1.1.1.1.1.2.2.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.2.2.3.2" xref="S4.E1.m1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E1.m1.1.1.1.1.1.2.2.3.1" xref="S4.E1.m1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E1.m1.1.1.1.1.1.2.2.3.3" xref="S4.E1.m1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.1.1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml"><munder id="S4.E1.m1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.1.1.2.2.cmml">min</mi><mi id="S4.E1.m1.1.1.1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.1.1.2.3.cmml">j</mi></munder><mo id="S4.E1.m1.1.1.1.1.1.1a" xref="S4.E1.m1.1.1.1.1.1.1.cmml">â¡</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">w</mi><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">j</mi></msub><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">w</mi><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><mo id="S4.E1.m1.2.2.2.3" xref="S4.E1.m1.2.2.2.3.cmml">+</mo><mrow id="S4.E1.m1.2.2.2.2" xref="S4.E1.m1.2.2.2.2.cmml"><mfrac id="S4.E1.m1.2.2.2.2.3" xref="S4.E1.m1.2.2.2.2.3.cmml"><mn id="S4.E1.m1.2.2.2.2.3.2" xref="S4.E1.m1.2.2.2.2.3.2.cmml">1</mn><mi id="S4.E1.m1.2.2.2.2.3.3" xref="S4.E1.m1.2.2.2.2.3.3.cmml">m</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2.2.2" xref="S4.E1.m1.2.2.2.2.2.cmml">â€‹</mo><mrow id="S4.E1.m1.2.2.2.2.1" xref="S4.E1.m1.2.2.2.2.1.cmml"><munderover id="S4.E1.m1.2.2.2.2.1.2" xref="S4.E1.m1.2.2.2.2.1.2.cmml"><mo movablelimits="false" id="S4.E1.m1.2.2.2.2.1.2.2.2" xref="S4.E1.m1.2.2.2.2.1.2.2.2.cmml">âˆ‘</mo><mrow id="S4.E1.m1.2.2.2.2.1.2.2.3" xref="S4.E1.m1.2.2.2.2.1.2.2.3.cmml"><mi id="S4.E1.m1.2.2.2.2.1.2.2.3.2" xref="S4.E1.m1.2.2.2.2.1.2.2.3.2.cmml">j</mi><mo id="S4.E1.m1.2.2.2.2.1.2.2.3.1" xref="S4.E1.m1.2.2.2.2.1.2.2.3.1.cmml">=</mo><mn id="S4.E1.m1.2.2.2.2.1.2.2.3.3" xref="S4.E1.m1.2.2.2.2.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.2.2.2.2.1.2.3" xref="S4.E1.m1.2.2.2.2.1.2.3.cmml">m</mi></munderover><mrow id="S4.E1.m1.2.2.2.2.1.1" xref="S4.E1.m1.2.2.2.2.1.1.cmml"><munder id="S4.E1.m1.2.2.2.2.1.1.2" xref="S4.E1.m1.2.2.2.2.1.1.2.cmml"><mi id="S4.E1.m1.2.2.2.2.1.1.2.2" xref="S4.E1.m1.2.2.2.2.1.1.2.2.cmml">min</mi><mi id="S4.E1.m1.2.2.2.2.1.1.2.3" xref="S4.E1.m1.2.2.2.2.1.1.2.3.cmml">i</mi></munder><mo id="S4.E1.m1.2.2.2.2.1.1a" xref="S4.E1.m1.2.2.2.2.1.1.cmml">â¡</mo><mrow id="S4.E1.m1.2.2.2.2.1.1.1.1" xref="S4.E1.m1.2.2.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.2.2.1.1.1.1.2" xref="S4.E1.m1.2.2.2.2.1.1.1.2.1.cmml">|</mo><mrow id="S4.E1.m1.2.2.2.2.1.1.1.1.1" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.cmml"><mover accent="true" id="S4.E1.m1.2.2.2.2.1.1.1.1.1.2" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.cmml"><msub id="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2.cmml"><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2.2" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2.2.cmml">w</mi><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2.3" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2.3.cmml">j</mi></msub><mo id="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.1" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S4.E1.m1.2.2.2.2.1.1.1.1.1.1" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E1.m1.2.2.2.2.1.1.1.1.1.3" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.1.3.2" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.3.2.cmml">w</mi><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.1.3.3" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.E1.m1.2.2.2.2.1.1.1.1.3" xref="S4.E1.m1.2.2.2.2.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2"><eq id="S4.E1.m1.2.2.3.cmml" xref="S4.E1.m1.2.2.3"></eq><apply id="S4.E1.m1.2.2.4.cmml" xref="S4.E1.m1.2.2.4"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.4.1.cmml" xref="S4.E1.m1.2.2.4">subscript</csymbol><ci id="S4.E1.m1.2.2.4.2.cmml" xref="S4.E1.m1.2.2.4.2">â„’</ci><apply id="S4.E1.m1.2.2.4.3.cmml" xref="S4.E1.m1.2.2.4.3"><times id="S4.E1.m1.2.2.4.3.1.cmml" xref="S4.E1.m1.2.2.4.3.1"></times><ci id="S4.E1.m1.2.2.4.3.2.cmml" xref="S4.E1.m1.2.2.4.3.2">ğ¼</ci><ci id="S4.E1.m1.2.2.4.3.3.cmml" xref="S4.E1.m1.2.2.4.3.3">ğ‘€</ci><ci id="S4.E1.m1.2.2.4.3.4.cmml" xref="S4.E1.m1.2.2.4.3.4">ğ¿</ci><ci id="S4.E1.m1.2.2.4.3.5.cmml" xref="S4.E1.m1.2.2.4.3.5">ğ¸</ci></apply></apply><apply id="S4.E1.m1.2.2.2.cmml" xref="S4.E1.m1.2.2.2"><plus id="S4.E1.m1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.3"></plus><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1"><times id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"></times><apply id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3"><divide id="S4.E1.m1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3"></divide><cn type="integer" id="S4.E1.m1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2">1</cn><ci id="S4.E1.m1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3">ğ‘›</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1"><apply id="S4.E1.m1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.2">subscript</csymbol><sum id="S4.E1.m1.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.2.2.2"></sum><apply id="S4.E1.m1.1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.2.2.3"><eq id="S4.E1.m1.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E1.m1.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S4.E1.m1.1.1.1.1.1.2.2.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.2.3">ğ‘›</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1"><apply id="S4.E1.m1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2">subscript</csymbol><min id="S4.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.2"></min><ci id="S4.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.3">ğ‘—</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1"><abs id="S4.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2"></abs><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1"><minus id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2"><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.1">^</ci><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2">ğ‘¤</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.2.3">ğ‘—</ci></apply></apply><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.2">ğ‘¤</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply><apply id="S4.E1.m1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2"><times id="S4.E1.m1.2.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.2"></times><apply id="S4.E1.m1.2.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.2.3"><divide id="S4.E1.m1.2.2.2.2.3.1.cmml" xref="S4.E1.m1.2.2.2.2.3"></divide><cn type="integer" id="S4.E1.m1.2.2.2.2.3.2.cmml" xref="S4.E1.m1.2.2.2.2.3.2">1</cn><ci id="S4.E1.m1.2.2.2.2.3.3.cmml" xref="S4.E1.m1.2.2.2.2.3.3">ğ‘š</ci></apply><apply id="S4.E1.m1.2.2.2.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1"><apply id="S4.E1.m1.2.2.2.2.1.2.cmml" xref="S4.E1.m1.2.2.2.2.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.1.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1.2">superscript</csymbol><apply id="S4.E1.m1.2.2.2.2.1.2.2.cmml" xref="S4.E1.m1.2.2.2.2.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.1.2.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1.2">subscript</csymbol><sum id="S4.E1.m1.2.2.2.2.1.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.1.2.2.2"></sum><apply id="S4.E1.m1.2.2.2.2.1.2.2.3.cmml" xref="S4.E1.m1.2.2.2.2.1.2.2.3"><eq id="S4.E1.m1.2.2.2.2.1.2.2.3.1.cmml" xref="S4.E1.m1.2.2.2.2.1.2.2.3.1"></eq><ci id="S4.E1.m1.2.2.2.2.1.2.2.3.2.cmml" xref="S4.E1.m1.2.2.2.2.1.2.2.3.2">ğ‘—</ci><cn type="integer" id="S4.E1.m1.2.2.2.2.1.2.2.3.3.cmml" xref="S4.E1.m1.2.2.2.2.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.2.2.2.2.1.2.3.cmml" xref="S4.E1.m1.2.2.2.2.1.2.3">ğ‘š</ci></apply><apply id="S4.E1.m1.2.2.2.2.1.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1"><apply id="S4.E1.m1.2.2.2.2.1.1.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.1.1.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.2">subscript</csymbol><min id="S4.E1.m1.2.2.2.2.1.1.2.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.2.2"></min><ci id="S4.E1.m1.2.2.2.2.1.1.2.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.2.3">ğ‘–</ci></apply><apply id="S4.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1"><abs id="S4.E1.m1.2.2.2.2.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.2"></abs><apply id="S4.E1.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1"><minus id="S4.E1.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.1"></minus><apply id="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.2"><ci id="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.1">^</ci><apply id="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2.2">ğ‘¤</ci><ci id="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.2.2.3">ğ‘—</ci></apply></apply><apply id="S4.E1.m1.2.2.2.2.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.3.2">ğ‘¤</ci><ci id="S4.E1.m1.2.2.2.2.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\mathcal{L}_{IMLE}=\frac{1}{n}\sum_{i=1}^{n}\min_{j}|\hat{w_{j}}-w_{i}|+\frac{1}{m}\sum_{j=1}^{m}\min_{i}|\hat{w_{j}}-w_{i}|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.7" class="ltx_p">In our experiment, we let <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="\tau_{min}=0.40" display="inline"><semantics id="S4.p6.1.m1.1a"><mrow id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml"><msub id="S4.p6.1.m1.1.1.2" xref="S4.p6.1.m1.1.1.2.cmml"><mi id="S4.p6.1.m1.1.1.2.2" xref="S4.p6.1.m1.1.1.2.2.cmml">Ï„</mi><mrow id="S4.p6.1.m1.1.1.2.3" xref="S4.p6.1.m1.1.1.2.3.cmml"><mi id="S4.p6.1.m1.1.1.2.3.2" xref="S4.p6.1.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.2.3.1" xref="S4.p6.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.p6.1.m1.1.1.2.3.3" xref="S4.p6.1.m1.1.1.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.2.3.1a" xref="S4.p6.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.p6.1.m1.1.1.2.3.4" xref="S4.p6.1.m1.1.1.2.3.4.cmml">n</mi></mrow></msub><mo id="S4.p6.1.m1.1.1.1" xref="S4.p6.1.m1.1.1.1.cmml">=</mo><mn id="S4.p6.1.m1.1.1.3" xref="S4.p6.1.m1.1.1.3.cmml">0.40</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><apply id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"><eq id="S4.p6.1.m1.1.1.1.cmml" xref="S4.p6.1.m1.1.1.1"></eq><apply id="S4.p6.1.m1.1.1.2.cmml" xref="S4.p6.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.p6.1.m1.1.1.2.1.cmml" xref="S4.p6.1.m1.1.1.2">subscript</csymbol><ci id="S4.p6.1.m1.1.1.2.2.cmml" xref="S4.p6.1.m1.1.1.2.2">ğœ</ci><apply id="S4.p6.1.m1.1.1.2.3.cmml" xref="S4.p6.1.m1.1.1.2.3"><times id="S4.p6.1.m1.1.1.2.3.1.cmml" xref="S4.p6.1.m1.1.1.2.3.1"></times><ci id="S4.p6.1.m1.1.1.2.3.2.cmml" xref="S4.p6.1.m1.1.1.2.3.2">ğ‘š</ci><ci id="S4.p6.1.m1.1.1.2.3.3.cmml" xref="S4.p6.1.m1.1.1.2.3.3">ğ‘–</ci><ci id="S4.p6.1.m1.1.1.2.3.4.cmml" xref="S4.p6.1.m1.1.1.2.3.4">ğ‘›</ci></apply></apply><cn type="float" id="S4.p6.1.m1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.3">0.40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">\tau_{min}=0.40</annotation></semantics></math>, <math id="S4.p6.2.m2.1" class="ltx_Math" alttext="\tau_{max}=0.45" display="inline"><semantics id="S4.p6.2.m2.1a"><mrow id="S4.p6.2.m2.1.1" xref="S4.p6.2.m2.1.1.cmml"><msub id="S4.p6.2.m2.1.1.2" xref="S4.p6.2.m2.1.1.2.cmml"><mi id="S4.p6.2.m2.1.1.2.2" xref="S4.p6.2.m2.1.1.2.2.cmml">Ï„</mi><mrow id="S4.p6.2.m2.1.1.2.3" xref="S4.p6.2.m2.1.1.2.3.cmml"><mi id="S4.p6.2.m2.1.1.2.3.2" xref="S4.p6.2.m2.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.p6.2.m2.1.1.2.3.1" xref="S4.p6.2.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.p6.2.m2.1.1.2.3.3" xref="S4.p6.2.m2.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p6.2.m2.1.1.2.3.1a" xref="S4.p6.2.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.p6.2.m2.1.1.2.3.4" xref="S4.p6.2.m2.1.1.2.3.4.cmml">x</mi></mrow></msub><mo id="S4.p6.2.m2.1.1.1" xref="S4.p6.2.m2.1.1.1.cmml">=</mo><mn id="S4.p6.2.m2.1.1.3" xref="S4.p6.2.m2.1.1.3.cmml">0.45</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.2.m2.1b"><apply id="S4.p6.2.m2.1.1.cmml" xref="S4.p6.2.m2.1.1"><eq id="S4.p6.2.m2.1.1.1.cmml" xref="S4.p6.2.m2.1.1.1"></eq><apply id="S4.p6.2.m2.1.1.2.cmml" xref="S4.p6.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.p6.2.m2.1.1.2.1.cmml" xref="S4.p6.2.m2.1.1.2">subscript</csymbol><ci id="S4.p6.2.m2.1.1.2.2.cmml" xref="S4.p6.2.m2.1.1.2.2">ğœ</ci><apply id="S4.p6.2.m2.1.1.2.3.cmml" xref="S4.p6.2.m2.1.1.2.3"><times id="S4.p6.2.m2.1.1.2.3.1.cmml" xref="S4.p6.2.m2.1.1.2.3.1"></times><ci id="S4.p6.2.m2.1.1.2.3.2.cmml" xref="S4.p6.2.m2.1.1.2.3.2">ğ‘š</ci><ci id="S4.p6.2.m2.1.1.2.3.3.cmml" xref="S4.p6.2.m2.1.1.2.3.3">ğ‘</ci><ci id="S4.p6.2.m2.1.1.2.3.4.cmml" xref="S4.p6.2.m2.1.1.2.3.4">ğ‘¥</ci></apply></apply><cn type="float" id="S4.p6.2.m2.1.1.3.cmml" xref="S4.p6.2.m2.1.1.3">0.45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.2.m2.1c">\tau_{max}=0.45</annotation></semantics></math> , <math id="S4.p6.3.m3.1" class="ltx_Math" alttext="D=0.40" display="inline"><semantics id="S4.p6.3.m3.1a"><mrow id="S4.p6.3.m3.1.1" xref="S4.p6.3.m3.1.1.cmml"><mi id="S4.p6.3.m3.1.1.2" xref="S4.p6.3.m3.1.1.2.cmml">D</mi><mo id="S4.p6.3.m3.1.1.1" xref="S4.p6.3.m3.1.1.1.cmml">=</mo><mn id="S4.p6.3.m3.1.1.3" xref="S4.p6.3.m3.1.1.3.cmml">0.40</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.3.m3.1b"><apply id="S4.p6.3.m3.1.1.cmml" xref="S4.p6.3.m3.1.1"><eq id="S4.p6.3.m3.1.1.1.cmml" xref="S4.p6.3.m3.1.1.1"></eq><ci id="S4.p6.3.m3.1.1.2.cmml" xref="S4.p6.3.m3.1.1.2">ğ·</ci><cn type="float" id="S4.p6.3.m3.1.1.3.cmml" xref="S4.p6.3.m3.1.1.3">0.40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.3.m3.1c">D=0.40</annotation></semantics></math> and both <math id="S4.p6.4.m4.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S4.p6.4.m4.1a"><mi id="S4.p6.4.m4.1.1" xref="S4.p6.4.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.p6.4.m4.1b"><ci id="S4.p6.4.m4.1.1.cmml" xref="S4.p6.4.m4.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.4.m4.1c">m</annotation></semantics></math> and <math id="S4.p6.5.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p6.5.m5.1a"><mi id="S4.p6.5.m5.1.1" xref="S4.p6.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p6.5.m5.1b"><ci id="S4.p6.5.m5.1.1.cmml" xref="S4.p6.5.m5.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.5.m5.1c">n</annotation></semantics></math> are 70. We let <math id="S4.p6.6.m6.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.p6.6.m6.1a"><mi id="S4.p6.6.m6.1.1" xref="S4.p6.6.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.p6.6.m6.1b"><ci id="S4.p6.6.m6.1.1.cmml" xref="S4.p6.6.m6.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.6.m6.1c">z</annotation></semantics></math> have two dimensions and train with SGD with a constant learning rate. At both training and inference, the latent vectors are sampled from <math id="S4.p6.7.m7.2" class="ltx_Math" alttext="z\sim\mathcal{N}(0,I)" display="inline"><semantics id="S4.p6.7.m7.2a"><mrow id="S4.p6.7.m7.2.3" xref="S4.p6.7.m7.2.3.cmml"><mi id="S4.p6.7.m7.2.3.2" xref="S4.p6.7.m7.2.3.2.cmml">z</mi><mo id="S4.p6.7.m7.2.3.1" xref="S4.p6.7.m7.2.3.1.cmml">âˆ¼</mo><mrow id="S4.p6.7.m7.2.3.3" xref="S4.p6.7.m7.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p6.7.m7.2.3.3.2" xref="S4.p6.7.m7.2.3.3.2.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="S4.p6.7.m7.2.3.3.1" xref="S4.p6.7.m7.2.3.3.1.cmml">â€‹</mo><mrow id="S4.p6.7.m7.2.3.3.3.2" xref="S4.p6.7.m7.2.3.3.3.1.cmml"><mo stretchy="false" id="S4.p6.7.m7.2.3.3.3.2.1" xref="S4.p6.7.m7.2.3.3.3.1.cmml">(</mo><mn id="S4.p6.7.m7.1.1" xref="S4.p6.7.m7.1.1.cmml">0</mn><mo id="S4.p6.7.m7.2.3.3.3.2.2" xref="S4.p6.7.m7.2.3.3.3.1.cmml">,</mo><mi id="S4.p6.7.m7.2.2" xref="S4.p6.7.m7.2.2.cmml">I</mi><mo stretchy="false" id="S4.p6.7.m7.2.3.3.3.2.3" xref="S4.p6.7.m7.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.7.m7.2b"><apply id="S4.p6.7.m7.2.3.cmml" xref="S4.p6.7.m7.2.3"><csymbol cd="latexml" id="S4.p6.7.m7.2.3.1.cmml" xref="S4.p6.7.m7.2.3.1">similar-to</csymbol><ci id="S4.p6.7.m7.2.3.2.cmml" xref="S4.p6.7.m7.2.3.2">ğ‘§</ci><apply id="S4.p6.7.m7.2.3.3.cmml" xref="S4.p6.7.m7.2.3.3"><times id="S4.p6.7.m7.2.3.3.1.cmml" xref="S4.p6.7.m7.2.3.3.1"></times><ci id="S4.p6.7.m7.2.3.3.2.cmml" xref="S4.p6.7.m7.2.3.3.2">ğ’©</ci><interval closure="open" id="S4.p6.7.m7.2.3.3.3.1.cmml" xref="S4.p6.7.m7.2.3.3.3.2"><cn type="integer" id="S4.p6.7.m7.1.1.cmml" xref="S4.p6.7.m7.1.1">0</cn><ci id="S4.p6.7.m7.2.2.cmml" xref="S4.p6.7.m7.2.2">ğ¼</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.7.m7.2c">z\sim\mathcal{N}(0,I)</annotation></semantics></math>.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p"><span id="S4.p7.1.1" class="ltx_text ltx_font_bold">Object Rearrangement: </span> The rearrangement process can typically be broken into a sequence of the following three states: <span id="S4.p7.1.2" class="ltx_text ltx_font_italic">pick</span>, <span id="S4.p7.1.3" class="ltx_text ltx_font_italic">transition</span> and <span id="S4.p7.1.4" class="ltx_text ltx_font_italic">place</span>. For the picking action, we use Contact-GraspnetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> to sample a batch of 6-DOF grasps for the target object. For placing objects, we first sample potential object positions based on the scene point cloud and the support surface. The placement orientation is set to the current object pose while it is being grasped. The poses are filtered by CabiNet for collisions with the environment and followed by whether a kinematic solution can be found for the manipulator. These 6-DOF poses are then used to construct motion planning problems used to generate robot trajectories with MPPI, as described in Section <a href="#A2" title="Appendix B Trajectory Generation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Evaluation</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation on Collision Benchmark</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We evaluate CabiNet on a collision benchmark against four baseline point cloud-based collision detection algorithms. We want to emphasize that our setting only requires a point cloud observation from a single view. We sample synthetic scene/object point cloud pair where the objects move in 64 linear trajectories in a scene. The collision ground truth information is computed with FCL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> in simulation. For each experiment, we have a balanced set of 256K collision and collision-free queries for a total of 512K queries/experiment. We evaluated on five environments (1000 scenes each) from Fig <a href="#S0.F1" title="Figure 1 â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and the results are averaged across them in Table <a href="#S5.T1" title="Table 1 â€£ 5.1 Evaluation on Collision Benchmark â€£ 5 Experimental Evaluation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Quantitative Metrics:</span> We report the following 1) mean Average Precision (mAP) score for the classifier, averaged across the five environment test sets 2) collision prediction accuracy and 3) time/query in <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">Î¼</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">ğœ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\mu</annotation></semantics></math>s. Our baselines are as follows:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">SceneCollisionNet</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>: We directly evaluated the pretrained model from prior work. This approach was just trained on a single environment (Tabletop) with a fixed robot-to-tabletop transformation, and directly infers collision from point clouds without any preprocessing.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Occupancy Mapping</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>: This is one of the most commonly used geometric collision checking heuristic representation used in the robotics community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. We use the open source implementation from Open3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> to convert the sensed point cloud to a occupancy map representation. Voxels are specified to be of 1<math id="S5.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="cm" display="inline"><semantics id="S5.I1.i2.p1.1.m1.1a"><mrow id="S5.I1.i2.p1.1.m1.1.1" xref="S5.I1.i2.p1.1.m1.1.1.cmml"><mi id="S5.I1.i2.p1.1.m1.1.1.2" xref="S5.I1.i2.p1.1.m1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.I1.i2.p1.1.m1.1.1.1" xref="S5.I1.i2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S5.I1.i2.p1.1.m1.1.1.3" xref="S5.I1.i2.p1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.1.m1.1b"><apply id="S5.I1.i2.p1.1.m1.1.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1"><times id="S5.I1.i2.p1.1.m1.1.1.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1.1"></times><ci id="S5.I1.i2.p1.1.m1.1.1.2.cmml" xref="S5.I1.i2.p1.1.m1.1.1.2">ğ‘</ci><ci id="S5.I1.i2.p1.1.m1.1.1.3.cmml" xref="S5.I1.i2.p1.1.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.1.m1.1c">cm</annotation></semantics></math> in size and are labelled to be either collision free or occupied.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Marching Cubes + FCL</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>: We first convert the scene and object point clouds to a mesh with the marching cubes algorithm and use FCL to compute the collision between the scene and object meshes. We parallelize this baseline across 10 processes for a fairer comparison.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p"><span id="S5.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Marching Cubes + SDF</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>: The scene point cloud is first converted to a mesh and we fit a SDF to it using the GPU implementation from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Each point in the object point cloud is computed for its SDF value from the scene. If any of the points have a negative distance (due to a penetration), the entire scene/object point cloud is considered to be in collision.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.4" class="ltx_p"><span id="S5.SS1.p3.4.1" class="ltx_text ltx_font_bold">Baseline Comparisons:</span> Overall CabiNet outperforms the baselines methods in terms of both accuracy and inference speed. It has the highest mAP across the five environment test sets. It is nearly 24<math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mo id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\%</annotation></semantics></math> and 15<math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mo id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><csymbol cd="latexml" id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">\%</annotation></semantics></math> higher mAP and accuracy respectively compared to OccupancyMap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> which is a popular method in the community, while being nearly 25x faster with a 7<math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><mi id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml">Î¼</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><ci id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">ğœ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">\mu</annotation></semantics></math>s inference time. OccupancyMap performance has direct correlation with the coverage of point cloud over occluded part of the scenes. The more occluded areas in the scene, the less accurate it becomes. CabiNet, on the other hand, does not suffer from the occlusion issue since it is trained with single camera and has been learned to extrapolate to occluded parts in order to solve collision queries. CabiNet also generalizes to more diverse environments and point data compared to the pretrained SceneCollisionNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> which shows the importance of training on diverse set of scenes and objects. Our approach is also about 4X faster than the parallelized FCL baseline with a 20.4<math id="S5.SS1.p3.4.m4.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS1.p3.4.m4.1a"><mo id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><csymbol cd="latexml" id="S5.SS1.p3.4.m4.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">\%</annotation></semantics></math> higher mAP score.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Ablation on Environments:</span> We show in Table <a href="#S5.T2" title="Table 2 â€£ 5.1 Evaluation on Collision Benchmark â€£ 5 Experimental Evaluation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> that CabiNet generalizes to diverse environments by training with more in-distribution data. We notice that the model generalizes to similar environments even without any training, such as cabinets and shelves. The model trained on all the environments performed the best on all the test sets.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.10.1.1" class="ltx_text" style="font-size:113%;">Table 1</span>: </span><span id="S5.T1.11.2" class="ltx_text" style="font-size:113%;">Results on Collision Benchmark</span></figcaption>
<table id="S5.T1.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Collision Model</span></th>
<th id="S5.T1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">mAP</span></th>
<th id="S5.T1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S5.T1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Accuracy</span><span id="S5.T1.1.1.4.2" class="ltx_text" style="font-size:80%;"> (%)</span>
</th>
<th id="S5.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Time/Query (<math id="S5.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S5.T1.1.1.1.1.m1.1a"><mi id="S5.T1.1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.1.m1.1.1.cmml">Î¼</mi><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.m1.1.1">ğœ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.m1.1c">\mu</annotation></semantics></math>s)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.2.2" class="ltx_tr">
<th id="S5.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T1.2.2.2.1" class="ltx_text" style="font-size:80%;">CabiNet (Ours)</span></th>
<td id="S5.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.971</span></td>
<td id="S5.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">89.0</span></td>
<td id="S5.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">6.41 <math id="S5.T1.2.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.2.2.1.1.m1.1a"><mo id="S5.T1.2.2.1.1.m1.1.1" xref="S5.T1.2.2.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.2.2.1.1.m1.1.1.cmml" xref="S5.T1.2.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.1.1.m1.1c">\pm</annotation></semantics></math> 3.58</span></td>
</tr>
<tr id="S5.T1.3.3" class="ltx_tr">
<th id="S5.T1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S5.T1.3.3.2.1" class="ltx_text" style="font-size:80%;">SceneCollisionNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T1.3.3.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S5.T1.3.3.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T1.3.3.3" class="ltx_td ltx_align_center"><span id="S5.T1.3.3.3.1" class="ltx_text" style="font-size:80%;">0.706</span></td>
<td id="S5.T1.3.3.4" class="ltx_td ltx_align_center"><span id="S5.T1.3.3.4.1" class="ltx_text" style="font-size:80%;">69.9</span></td>
<td id="S5.T1.3.3.1" class="ltx_td ltx_align_center">
<span id="S5.T1.3.3.1.1" class="ltx_text" style="font-size:80%;">7.03 </span><math id="S5.T1.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.3.3.1.m1.1a"><mo mathsize="80%" id="S5.T1.3.3.1.m1.1.1" xref="S5.T1.3.3.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.1.m1.1b"><csymbol cd="latexml" id="S5.T1.3.3.1.m1.1.1.cmml" xref="S5.T1.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T1.3.3.1.2" class="ltx_text" style="font-size:80%;"> 3.89</span>
</td>
</tr>
<tr id="S5.T1.4.4" class="ltx_tr">
<th id="S5.T1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S5.T1.4.4.2.1" class="ltx_text" style="font-size:80%;">OccupancyMap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T1.4.4.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S5.T1.4.4.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T1.4.4.3" class="ltx_td ltx_align_center"><span id="S5.T1.4.4.3.1" class="ltx_text" style="font-size:80%;">0.732</span></td>
<td id="S5.T1.4.4.4" class="ltx_td ltx_align_center"><span id="S5.T1.4.4.4.1" class="ltx_text" style="font-size:80%;">74.1</span></td>
<td id="S5.T1.4.4.1" class="ltx_td ltx_align_center">
<span id="S5.T1.4.4.1.1" class="ltx_text" style="font-size:80%;">174.5 </span><math id="S5.T1.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.4.4.1.m1.1a"><mo mathsize="80%" id="S5.T1.4.4.1.m1.1.1" xref="S5.T1.4.4.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.1.m1.1b"><csymbol cd="latexml" id="S5.T1.4.4.1.m1.1.1.cmml" xref="S5.T1.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T1.4.4.1.2" class="ltx_text" style="font-size:80%;"> 61.9</span>
</td>
</tr>
<tr id="S5.T1.5.5" class="ltx_tr">
<th id="S5.T1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S5.T1.5.5.2.1" class="ltx_text" style="font-size:80%;">MC + FCL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T1.5.5.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib49" title="" class="ltx_ref">49</a><span id="S5.T1.5.5.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T1.5.5.3" class="ltx_td ltx_align_center"><span id="S5.T1.5.5.3.1" class="ltx_text" style="font-size:80%;">0.767</span></td>
<td id="S5.T1.5.5.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.5.4.1" class="ltx_text" style="font-size:80%;">78.8</span></td>
<td id="S5.T1.5.5.1" class="ltx_td ltx_align_center">
<span id="S5.T1.5.5.1.1" class="ltx_text" style="font-size:80%;">27.5 </span><math id="S5.T1.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.5.5.1.m1.1a"><mo mathsize="80%" id="S5.T1.5.5.1.m1.1.1" xref="S5.T1.5.5.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.1.m1.1b"><csymbol cd="latexml" id="S5.T1.5.5.1.m1.1.1.cmml" xref="S5.T1.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T1.5.5.1.2" class="ltx_text" style="font-size:80%;"> 6.82</span>
</td>
</tr>
<tr id="S5.T1.6.6" class="ltx_tr">
<th id="S5.T1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">
<span id="S5.T1.6.6.2.1" class="ltx_text" style="font-size:80%;">MC + SDF </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T1.6.6.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S5.T1.6.6.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T1.6.6.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.6.6.3.1" class="ltx_text" style="font-size:80%;">0.773</span></td>
<td id="S5.T1.6.6.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.6.6.4.1" class="ltx_text" style="font-size:80%;">80.0</span></td>
<td id="S5.T1.6.6.1" class="ltx_td ltx_align_center ltx_border_b">
<span id="S5.T1.6.6.1.1" class="ltx_text" style="font-size:80%;">168.6 </span><math id="S5.T1.6.6.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.6.6.1.m1.1a"><mo mathsize="80%" id="S5.T1.6.6.1.m1.1.1" xref="S5.T1.6.6.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.1.m1.1b"><csymbol cd="latexml" id="S5.T1.6.6.1.m1.1.1.cmml" xref="S5.T1.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T1.6.6.1.2" class="ltx_text" style="font-size:80%;"> 46.4</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.4.1.1" class="ltx_text" style="font-size:113%;">Table 2</span>: </span><span id="S5.T2.5.2" class="ltx_text" style="font-size:113%;">CabiNet Generalization to Environments</span></figcaption>
<table id="S5.T2.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.6.1.1" class="ltx_tr">
<th id="S5.T2.6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T2.6.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Train Set</span></th>
<th id="S5.T2.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="6"><span id="S5.T2.6.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Test Set (AP)</span></th>
</tr>
<tr id="S5.T2.6.2.2" class="ltx_tr">
<th id="S5.T2.6.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.6.2.2.1.1" class="ltx_text" style="font-size:80%;">Tabletop</span></th>
<th id="S5.T2.6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.6.2.2.2.1" class="ltx_text" style="font-size:80%;">Shelf</span></th>
<th id="S5.T2.6.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.6.2.2.3.1" class="ltx_text" style="font-size:80%;">Cubby</span></th>
<th id="S5.T2.6.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.6.2.2.4.1" class="ltx_text" style="font-size:80%;">Drawers</span></th>
<th id="S5.T2.6.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.6.2.2.5.1" class="ltx_text" style="font-size:80%;">Cabinet</span></th>
<th id="S5.T2.6.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T2.6.2.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">mAP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.6.3.1" class="ltx_tr">
<th id="S5.T2.6.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T2.6.3.1.1.1" class="ltx_text" style="font-size:80%;">Tabletop</span></th>
<td id="S5.T2.6.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.989</span></td>
<td id="S5.T2.6.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.1.3.1" class="ltx_text" style="font-size:80%;">0.910</span></td>
<td id="S5.T2.6.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.1.4.1" class="ltx_text" style="font-size:80%;">0.855</span></td>
<td id="S5.T2.6.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.1.5.1" class="ltx_text" style="font-size:80%;">0.861</span></td>
<td id="S5.T2.6.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.1.6.1" class="ltx_text" style="font-size:80%;">0.855</span></td>
<td id="S5.T2.6.3.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.6.3.1.7.1" class="ltx_text" style="font-size:80%;">0.894</span></td>
</tr>
<tr id="S5.T2.6.4.2" class="ltx_tr">
<th id="S5.T2.6.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T2.6.4.2.1.1" class="ltx_text" style="font-size:80%;">Shelf</span></th>
<td id="S5.T2.6.4.2.2" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.2.2.1" class="ltx_text" style="font-size:80%;">0.924</span></td>
<td id="S5.T2.6.4.2.3" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.985</span></td>
<td id="S5.T2.6.4.2.4" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.978</span></td>
<td id="S5.T2.6.4.2.5" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.2.5.1" class="ltx_text" style="font-size:80%;">0.956</span></td>
<td id="S5.T2.6.4.2.6" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.2.6.1" class="ltx_text" style="font-size:80%;">0.972</span></td>
<td id="S5.T2.6.4.2.7" class="ltx_td ltx_align_center"><span id="S5.T2.6.4.2.7.1" class="ltx_text" style="font-size:80%;">0.963</span></td>
</tr>
<tr id="S5.T2.6.5.3" class="ltx_tr">
<th id="S5.T2.6.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T2.6.5.3.1.1" class="ltx_text" style="font-size:80%;">Cubby</span></th>
<td id="S5.T2.6.5.3.2" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.3.2.1" class="ltx_text" style="font-size:80%;">0.930</span></td>
<td id="S5.T2.6.5.3.3" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.3.3.1" class="ltx_text" style="font-size:80%;">0.974</span></td>
<td id="S5.T2.6.5.3.4" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.3.4.1" class="ltx_text" style="font-size:80%;">0.977</span></td>
<td id="S5.T2.6.5.3.5" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.3.5.1" class="ltx_text" style="font-size:80%;">0.961</span></td>
<td id="S5.T2.6.5.3.6" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.3.6.1" class="ltx_text" style="font-size:80%;">0.990</span></td>
<td id="S5.T2.6.5.3.7" class="ltx_td ltx_align_center"><span id="S5.T2.6.5.3.7.1" class="ltx_text" style="font-size:80%;">0.966</span></td>
</tr>
<tr id="S5.T2.6.6.4" class="ltx_tr">
<th id="S5.T2.6.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T2.6.6.4.1.1" class="ltx_text" style="font-size:80%;">Drawers</span></th>
<td id="S5.T2.6.6.4.2" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.4.2.1" class="ltx_text" style="font-size:80%;">0.923</span></td>
<td id="S5.T2.6.6.4.3" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.4.3.1" class="ltx_text" style="font-size:80%;">0.856</span></td>
<td id="S5.T2.6.6.4.4" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.4.4.1" class="ltx_text" style="font-size:80%;">0.848</span></td>
<td id="S5.T2.6.6.4.5" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.4.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.972</span></td>
<td id="S5.T2.6.6.4.6" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.4.6.1" class="ltx_text" style="font-size:80%;">0.914</span></td>
<td id="S5.T2.6.6.4.7" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.4.7.1" class="ltx_text" style="font-size:80%;">0.903</span></td>
</tr>
<tr id="S5.T2.6.7.5" class="ltx_tr">
<th id="S5.T2.6.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T2.6.7.5.1.1" class="ltx_text" style="font-size:80%;">Cabinet</span></th>
<td id="S5.T2.6.7.5.2" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.5.2.1" class="ltx_text" style="font-size:80%;">0.924</span></td>
<td id="S5.T2.6.7.5.3" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.5.3.1" class="ltx_text" style="font-size:80%;">0.971</span></td>
<td id="S5.T2.6.7.5.4" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.5.4.1" class="ltx_text" style="font-size:80%;">0.961</span></td>
<td id="S5.T2.6.7.5.5" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.5.5.1" class="ltx_text" style="font-size:80%;">0.961</span></td>
<td id="S5.T2.6.7.5.6" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.5.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.990</span></td>
<td id="S5.T2.6.7.5.7" class="ltx_td ltx_align_center"><span id="S5.T2.6.7.5.7.1" class="ltx_text" style="font-size:80%;">0.961</span></td>
</tr>
<tr id="S5.T2.6.8.6" class="ltx_tr">
<th id="S5.T2.6.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T2.6.8.6.1.1" class="ltx_text" style="font-size:80%;">All Envs</span></th>
<td id="S5.T2.6.8.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.8.6.2.1" class="ltx_text" style="font-size:80%;">0.971</span></td>
<td id="S5.T2.6.8.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.8.6.3.1" class="ltx_text" style="font-size:80%;">0.969</span></td>
<td id="S5.T2.6.8.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.8.6.4.1" class="ltx_text" style="font-size:80%;">0.965</span></td>
<td id="S5.T2.6.8.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.8.6.5.1" class="ltx_text" style="font-size:80%;">0.971</span></td>
<td id="S5.T2.6.8.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.8.6.6.1" class="ltx_text" style="font-size:80%;">0.979</span></td>
<td id="S5.T2.6.8.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.6.8.6.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.971</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.4.1.1" class="ltx_text" style="font-size:113%;">Table 3</span>: </span><span id="S5.T3.5.2" class="ltx_text" style="font-size:113%;">Simulated Rearrangement Experiments</span></figcaption>
<table id="S5.T3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.6.1.1" class="ltx_tr">
<th id="S5.T3.6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" rowspan="2"><span id="S5.T3.6.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Planner</span></th>
<th id="S5.T3.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T3.6.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Collision Model</span></th>
<th id="S5.T3.6.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3"><span id="S5.T3.6.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Waypoint Type</span></th>
</tr>
<tr id="S5.T3.6.2.2" class="ltx_tr">
<th id="S5.T3.6.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.6.2.2.1.1" class="ltx_text" style="font-size:80%;">CabiNet (Ours)</span></th>
<th id="S5.T3.6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.6.2.2.2.1" class="ltx_text" style="font-size:80%;">Reverse Approach</span></th>
<th id="S5.T3.6.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.6.2.2.3.1" class="ltx_text" style="font-size:80%;">No Waypoints</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.6.3.1" class="ltx_tr">
<th id="S5.T3.6.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S5.T3.6.3.1.1.1" class="ltx_text" style="font-size:80%;">MPPI</span></th>
<th id="S5.T3.6.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T3.6.3.1.2.1" class="ltx_text" style="font-size:80%;">CabiNet (Ours)</span></th>
<td id="S5.T3.6.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.6.3.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">36.6%/159s</span></td>
<td id="S5.T3.6.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.6.3.1.4.1" class="ltx_text" style="font-size:80%;">16.7%/158s</span></td>
<td id="S5.T3.6.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.6.3.1.5.1" class="ltx_text" style="font-size:80%;">4.3%/201s</span></td>
</tr>
<tr id="S5.T3.6.4.2" class="ltx_tr">
<th id="S5.T3.6.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T3.6.4.2.1.1" class="ltx_text" style="font-size:80%;">MPPI</span></th>
<th id="S5.T3.6.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S5.T3.6.4.2.2.1" class="ltx_text" style="font-size:80%;">OccupancyMap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.6.4.2.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S5.T3.6.4.2.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T3.6.4.2.3" class="ltx_td ltx_align_center"><span id="S5.T3.6.4.2.3.1" class="ltx_text" style="font-size:80%;">15.0%/289s</span></td>
<td id="S5.T3.6.4.2.4" class="ltx_td ltx_align_center"><span id="S5.T3.6.4.2.4.1" class="ltx_text" style="font-size:80%;">11.0%/234s</span></td>
<td id="S5.T3.6.4.2.5" class="ltx_td ltx_align_center"><span id="S5.T3.6.4.2.5.1" class="ltx_text" style="font-size:80%;">7.5%/307s</span></td>
</tr>
<tr id="S5.T3.6.5.3" class="ltx_tr">
<th id="S5.T3.6.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S5.T3.6.5.3.1.1" class="ltx_text" style="font-size:80%;">AIT* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.6.5.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S5.T3.6.5.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S5.T3.6.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S5.T3.6.5.3.2.1" class="ltx_text" style="font-size:80%;">OccupancyMap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.6.5.3.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S5.T3.6.5.3.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T3.6.5.3.3" class="ltx_td ltx_align_center"><span id="S5.T3.6.5.3.3.1" class="ltx_text" style="font-size:80%;">21.0%/808s</span></td>
<td id="S5.T3.6.5.3.4" class="ltx_td ltx_align_center"><span id="S5.T3.6.5.3.4.1" class="ltx_text" style="font-size:80%;">18.5%/380s</span></td>
<td id="S5.T3.6.5.3.5" class="ltx_td ltx_align_center"><span id="S5.T3.6.5.3.5.1" class="ltx_text" style="font-size:80%;">5.0%/451s</span></td>
</tr>
<tr id="S5.T3.6.6.4" class="ltx_tr">
<th id="S5.T3.6.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">
<span id="S5.T3.6.6.4.1.1" class="ltx_text" style="font-size:80%;">RRTConnect </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.6.6.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S5.T3.6.6.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S5.T3.6.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">
<span id="S5.T3.6.6.4.2.1" class="ltx_text" style="font-size:80%;">OccupancyMap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.6.6.4.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S5.T3.6.6.4.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T3.6.6.4.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.6.6.4.3.1" class="ltx_text" style="font-size:80%;">26.4%/357s</span></td>
<td id="S5.T3.6.6.4.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.6.6.4.4.1" class="ltx_text" style="font-size:80%;">18.1%/380s</span></td>
<td id="S5.T3.6.6.4.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.6.6.4.5.1" class="ltx_text" style="font-size:80%;">5.8%/389s</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.4.1.1" class="ltx_text" style="font-size:113%;">Table 4</span>: </span><span id="S5.T4.5.2" class="ltx_text" style="font-size:113%;">Success Rate by States</span></figcaption>
<table id="S5.T4.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.6.1.1" class="ltx_tr">
<th id="S5.T4.6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T4.6.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Waypoint Strategy</span></th>
<th id="S5.T4.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4"><span id="S5.T4.6.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">States</span></th>
</tr>
<tr id="S5.T4.6.2.2" class="ltx_tr">
<th id="S5.T4.6.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T4.6.2.2.1.1" class="ltx_text" style="font-size:80%;">Pick</span></th>
<th id="S5.T4.6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T4.6.2.2.2.1" class="ltx_text" style="font-size:80%;">Transition</span></th>
<th id="S5.T4.6.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T4.6.2.2.3.1" class="ltx_text" style="font-size:80%;">Place</span></th>
<th id="S5.T4.6.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T4.6.2.2.4.1" class="ltx_text" style="font-size:80%;">Overall</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.6.3.1" class="ltx_tr">
<th id="S5.T4.6.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T4.6.3.1.1.1" class="ltx_text" style="font-size:80%;">CabiNet (Ours)</span></th>
<td id="S5.T4.6.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.6.3.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">61.0%</span></td>
<td id="S5.T4.6.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.6.3.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">80.0%</span></td>
<td id="S5.T4.6.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.6.3.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">75.0%</span></td>
<td id="S5.T4.6.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.6.3.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">36.6%</span></td>
</tr>
<tr id="S5.T4.6.4.2" class="ltx_tr">
<th id="S5.T4.6.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.6.4.2.1.1" class="ltx_text" style="font-size:80%;">Reverse Approach</span></th>
<td id="S5.T4.6.4.2.2" class="ltx_td ltx_align_center"><span id="S5.T4.6.4.2.2.1" class="ltx_text" style="font-size:80%;">54.8%</span></td>
<td id="S5.T4.6.4.2.3" class="ltx_td ltx_align_center"><span id="S5.T4.6.4.2.3.1" class="ltx_text" style="font-size:80%;">43.5%</span></td>
<td id="S5.T4.6.4.2.4" class="ltx_td ltx_align_center"><span id="S5.T4.6.4.2.4.1" class="ltx_text" style="font-size:80%;">70.0%</span></td>
<td id="S5.T4.6.4.2.5" class="ltx_td ltx_align_center"><span id="S5.T4.6.4.2.5.1" class="ltx_text" style="font-size:80%;">16.7%</span></td>
</tr>
<tr id="S5.T4.6.5.3" class="ltx_tr">
<th id="S5.T4.6.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S5.T4.6.5.3.1.1" class="ltx_text" style="font-size:80%;">No Waypoints</span></th>
<td id="S5.T4.6.5.3.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T4.6.5.3.2.1" class="ltx_text" style="font-size:80%;">60.9%</span></td>
<td id="S5.T4.6.5.3.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T4.6.5.3.3.1" class="ltx_text" style="font-size:80%;">21.4%</span></td>
<td id="S5.T4.6.5.3.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T4.6.5.3.4.1" class="ltx_text" style="font-size:80%;">33.3%</span></td>
<td id="S5.T4.6.5.3.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T4.6.5.3.5.1" class="ltx_text" style="font-size:80%;">4.3%</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Object Rearrangement Evaluation in Simulation</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We evaluate our collision model in simulated rearrangement trials in IssacGym <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> as shown in Table <a href="#S5.T3" title="Table 3 â€£ 5.1 Evaluation on Collision Benchmark â€£ 5 Experimental Evaluation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. To focus more on the rearrangement aspect of the problem, we used the ground truth grasp poses from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. We adopt a standard state-machine for rearrangement tasks used in prior works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
The objects are chosen from bowl, box, and cylinder categories of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and are held out from training data.
For the environment assets, we use the seven shelves (from ShapeNet) in our test set to construct 30 scenes. We only use one fixed scene camera for all experiments and each scene gets two rearrangement trials, for a total of 60 experiments for each method.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Quantitative Metrics:</span> We report three metrics on this task: 1) <em id="S5.SS2.p2.1.2" class="ltx_emph ltx_font_italic">overall success rate</em> is the success rate for the whole pick and place operation where the robot picks the target object and place it in the designated shelf without any failures. 2) <em id="S5.SS2.p2.1.3" class="ltx_emph ltx_font_italic">individual success rate</em> is the success rate of each state given the number of times the policy state machine reaches to each particular state and 3) <em id="S5.SS2.p2.1.4" class="ltx_emph ltx_font_italic">total time</em> taken for each experiment. There are three stages in rearrangement: pick, transitioning to a placing pose and place. A pick is considered a success if the object is grasped after lifting the object from the support surface. The same is true for transition. The placement is considered a success if the object is detected to be resting on the place support surface, regardless of its final orientation. We emphasize that rearrangement is a long-horizon task and conditional success rates for each state are based on the performance of the previous state. As a result, even if the performance of individual state success rates are high, errors accumulate over time leading to a lower overall success rate.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Collision Representation for Planning:</span> We compared to Occupancy Mapping since it a popular heuristic collision representation used in the community<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The performance of all planners using this collision model significantly deteriorated compared to our CabiNet model. This shows the benefit of data driven approaches where they reason beyond the part of the point cloud that is visible and implicitly reason about occlusions as well. Occupancy maps is also significantly slower, increasing the rearrangement time by about 80% for the MPPI planner using the CabiNet waypoint sampler.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.3" class="ltx_p"><span id="S5.SS2.p4.3.1" class="ltx_text ltx_font_bold">Comparison to Global Planning pipeline:</span> We compare to the standard off-the-shelf motion planning pipeline used in the community with a Occupancy Mapping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> representation. Specifically, we compare to the state-of-the-art configuration space planner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, which is an almost-surely asymptotically optimal planner. We also compare to RRTConnect<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, which commonly is used to find feasible, though not necessarily optimal, paths. We give a timeout of 60<math id="S5.SS2.p4.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S5.SS2.p4.1.m1.1a"><mi id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><ci id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">s</annotation></semantics></math> to find a solution for both planners. After planning, we apply spline-based, collision-aware trajectory smoothingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> to the solutions. If the planner fails to find a valid solution, we simply execute the greedy solution to the goal to continue with the rearrangement process. Overall, the MPPI planner with our CabiNet model outperforms RRTConnect and AIT* by about 10<math id="S5.SS2.p4.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS2.p4.2.m2.1a"><mo id="S5.SS2.p4.2.m2.1.1" xref="S5.SS2.p4.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.2.m2.1b"><csymbol cd="latexml" id="S5.SS2.p4.2.m2.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.2.m2.1c">\%</annotation></semantics></math> and 15<math id="S5.SS2.p4.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS2.p4.3.m3.1a"><mo id="S5.SS2.p4.3.m3.1.1" xref="S5.SS2.p4.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.3.m3.1b"><csymbol cd="latexml" id="S5.SS2.p4.3.m3.1.1.cmml" xref="S5.SS2.p4.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.3.m3.1c">\%</annotation></semantics></math> respectively and is also significantly faster.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2304.09302/assets/figures/real_robot_c.png" id="S5.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="186" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.4.2" class="ltx_text" style="font-size:90%;">CabiNet is used for pick-and-place tasks in the real world. The scenes in the middle and the right are out-of-distribution environments in a real IKEA kitchen.</span></figcaption>
</figure>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.2" class="ltx_p"><span id="S5.SS2.p5.2.1" class="ltx_text ltx_font_bold">Importance of Waypoints:</span> We demonstrate that learning waypoints are crucial for rearrangement to navigate out of tight spaces. We compare to a common heuristic used in the motion generation literature to move robots out of tight spaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, which is to move the gripper in the reverse of the approach direction. We noticed that this approach, while proficient for primitive shapes (e.g. cylinders) it does not scale to more complex shapes like bowls, which have more complicated 6-DOF grasps. Hence, retracting in the reverse of the approach direction with an object in hand, the grasped object could collide with neighbouring support surfaces while transitioning from the pick to the place poses. As shown in Table <a href="#S5.T4" title="Table 4 â€£ 5.1 Evaluation on Collision Benchmark â€£ 5 Experimental Evaluation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, our CabiNet waypoint sampler improves the transition success rate by nearly 60<math id="S5.SS2.p5.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS2.p5.1.m1.1a"><mo id="S5.SS2.p5.1.m1.1.1" xref="S5.SS2.p5.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.1.m1.1b"><csymbol cd="latexml" id="S5.SS2.p5.1.m1.1.1.cmml" xref="S5.SS2.p5.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.1.m1.1c">\%</annotation></semantics></math> compared to when having no waypoints and about 35<math id="S5.SS2.p5.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS2.p5.2.m2.1a"><mo id="S5.SS2.p5.2.m2.1.1" xref="S5.SS2.p5.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.2.m2.1b"><csymbol cd="latexml" id="S5.SS2.p5.2.m2.1.1.cmml" xref="S5.SS2.p5.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.2.m2.1c">\%</annotation></semantics></math> when compared to the strategy of retracting in the opposite of the approach direction.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Real Robot Experiments</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We run experiments to show that our model transfers to a real robot despite only being trained in simulation.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_bold">Hardware Setup:</span> Experiments are done on a 7-DOF Franka Panda Robot with a parallel-jaw gripper. The system is equipped with two cameras: 1) Wrist mounted camera, which is an Intel Realsense D415 RGB-D camera, that is used for grasping 2) External camera, which is an Intel L515 RGB-D camera, that is used for generating the point cloud for CabiNet. Grasps are generated using Contact-GraspNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> and placement shelf is manually annotated for each problem by labeling the region that belongs to the desired placement shelf.
We use the model from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> to run instance segmentation on both cameras. The user selects the target object by clicking on the external camera image. Upon user selection of target object, the robot takes a closer look at the object and find the object in the wrist camera through relative camera pose between wrist camera and external camera. This step is crucial for grasp success since the wrist camera provides denser points on the object and it mitigates the effect of calibration imperfections. CabiNet has access to only the external camera point cloud and the inference is run on NVIDA Titan RTX gpu.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p"><span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">Experiment Setup:</span> We test our approach in novel environments, with unseen shelf and objects assets in unknown poses. We experiment with three objects from different object categories and two tasks. As shown in Fig <a href="#S5.F4" title="Figure 4 â€£ 5.2 Object Rearrangement Evaluation in Simulation â€£ 5 Experimental Evaluation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> in the vertical transport task, the robot has to pick an object from the top shelf and place it in the bottom one, or vice versa. Similarly for horizontal transport task the shelf compartments are horizontally next to each other. For each task-object pair, we attempt four trials, two of which go from one support compartment to the next and two in the opposite direction. In total we have 24 experiments and each task-object pair has a unique environment, where the object has to be picked and placed amidst clutter.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2304.09302/assets/figures/failure_modev2_c.png" id="S5.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="533" height="217" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.4.2" class="ltx_text" style="font-size:90%;">Examples of failure cases, left: roof partially occluded leading to collision with wrist camera during grasping, middle: grasped object collided with barrier, right: pick failure.</span></figcaption>
</figure>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.2" class="ltx_p"><span id="S5.SS3.p4.2.1" class="ltx_text ltx_font_bold">Discussion:</span> Results are reported in Table <a href="#S5.T5" title="Table 5 â€£ 5.3 Real Robot Experiments â€£ 5 Experimental Evaluation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and the vertical and horizontal transfer tasks have 75<math id="S5.SS3.p4.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS3.p4.1.m1.1a"><mo id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><csymbol cd="latexml" id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">\%</annotation></semantics></math> and 58<math id="S5.SS3.p4.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS3.p4.2.m2.1a"><mo id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><csymbol cd="latexml" id="S5.SS3.p4.2.m2.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">\%</annotation></semantics></math> success rates respectively. There were 2/24 pick failures due to incorrect grasps. One attempt failed during placing and 5/24 attempts failed when transitioning from pick to place states. The horizontal transfer task was relatively more challenging due to two reasons - the transitioning was over a longer distance leading to a greater chance of collision and the leftmost shelf compartment was not entirely visible from our static scene camera. Three failures were specifically due to occlusion and the CabiNet model not seeing enough of the leftmost cubby geometry from our camera setup, as shown in Fig <a href="#S5.F5" title="Figure 5 â€£ 5.3 Real Robot Experiments â€£ 5 Experimental Evaluation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The real robot executions are included in the <a target="_blank" href="https://youtu.be/Bs5QYkJVcjM" title="" class="ltx_ref ltx_href">supplementary video</a>.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p"><span id="S5.SS3.p5.1.1" class="ltx_text ltx_font_bold">Out-Of-Distribution Environments:</span> We were able to deploy the CabiNet model for pick-and-place tasks in out-of-distribution environments in a real IKEA kitchen (shown in Fig <a href="#S5.F4" title="Figure 4 â€£ 5.2 Object Rearrangement Evaluation in Simulation â€£ 5 Experimental Evaluation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), such as grabbing a plate from a dishwasher and grasping a box from a cabinet. The scene camera was extrinsically re-calibrated for each environment. We hypothesize that a combination of large-scale training in simulation and inductive biases in our architecture (including the use of camera calibration and point cloud representation) allowed CabiNet to easily generalize to these disparate environments.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.4.1.1" class="ltx_text" style="font-size:113%;">Table 5</span>: </span><span id="S5.T5.5.2" class="ltx_text" style="font-size:113%;">Real Robot Experiments</span></figcaption>
<table id="S5.T5.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.6.1.1" class="ltx_tr">
<th id="S5.T5.6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T5.6.1.1.1.1" class="ltx_text" style="font-size:80%;"><span id="S5.T5.6.1.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></span></th>
<th id="S5.T5.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3"><span id="S5.T5.6.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Object Category</span></th>
<th id="S5.T5.6.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
</tr>
<tr id="S5.T5.6.2.2" class="ltx_tr">
<th id="S5.T5.6.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T5.6.2.2.1.1" class="ltx_text" style="font-size:80%;">Bowl</span></th>
<th id="S5.T5.6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T5.6.2.2.2.1" class="ltx_text" style="font-size:80%;">Box</span></th>
<th id="S5.T5.6.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T5.6.2.2.3.1" class="ltx_text" style="font-size:80%;">Bottle</span></th>
<th id="S5.T5.6.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T5.6.2.2.4.1" class="ltx_text" style="font-size:80%;">Overall</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.6.3.1" class="ltx_tr">
<th id="S5.T5.6.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T5.6.3.1.1.1" class="ltx_text" style="font-size:80%;">Vertical Transport</span></th>
<td id="S5.T5.6.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.6.3.1.2.1" class="ltx_text" style="font-size:80%;">75.0%</span></td>
<td id="S5.T5.6.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.6.3.1.3.1" class="ltx_text" style="font-size:80%;">50.0%</span></td>
<td id="S5.T5.6.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.6.3.1.4.1" class="ltx_text" style="font-size:80%;">100.0%</span></td>
<td id="S5.T5.6.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.6.3.1.5.1" class="ltx_text" style="font-size:80%;">75.0%</span></td>
</tr>
<tr id="S5.T5.6.4.2" class="ltx_tr">
<th id="S5.T5.6.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S5.T5.6.4.2.1.1" class="ltx_text" style="font-size:80%;">Horizontal Transport</span></th>
<td id="S5.T5.6.4.2.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.6.4.2.2.1" class="ltx_text" style="font-size:80%;">50.0%</span></td>
<td id="S5.T5.6.4.2.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.6.4.2.3.1" class="ltx_text" style="font-size:80%;">25.0%</span></td>
<td id="S5.T5.6.4.2.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.6.4.2.4.1" class="ltx_text" style="font-size:80%;">100.0%</span></td>
<td id="S5.T5.6.4.2.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.6.4.2.5.1" class="ltx_text" style="font-size:80%;">58.0%</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We present an effort in scaling up neural rearrangement in clutter. We train our CabiNet model to predict collisions and motion waypoints from point cloud observations. It outperforms baseline approaches in terms of collision predicted, simulated experiments and also transfers well to real world clutter despite being only trained in simulation. While we perform our simulated and real experiments on a franka robot, we want to emphasize that the CabiNet model is conceptually robot-agnostic and can potentially work with other robots without re-training. A limitation of our architecture is that the 3D voxelization enforces queries to be within the model workspace which can sometimes be out of bounds during manipulation. Potential extensions could also explore more complicated scenes or learning to generate synthetic scenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. One could also explore hybrid architectures leveraging recent learned motion policies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> that are faster than traditional planners, along with CabiNet. Videos of robot experiments are available at: <a target="_blank" href="https://cabinet-object-rearrangement.github.io" title="" class="ltx_ref ltx_href">https://cabinet-object-rearrangement.github.io</a> and the supplementary material are in the appendix.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We would like to thank Ankur Handa and Jan Czarnowski for discussions and inspiring the project name; Wei Yang and Yu-Wei Chao for helping with Omniverse rendering and camera calibration; Balakumar Sundaralingam, Lucas Manuelli, Chris Paxton and â€ªTowaki Takikawa for discussions on the project.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M.Â Bennewitz C.Â Stachniss A.Â Hornung, K. M.Â Wurm and W.Â Burgard.

</span>
<span class="ltx_bibblock">Octomap: An efficient probabilistic 3d mapping framework based on
octrees.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Autonomous Robots</span>, 2013.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Dhruv Batra, AngelÂ X Chang, Sonia Chernova, AndrewÂ J Davison, Jia Deng, Vladlen
Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi,
Manolis Savva, and Hao Su.

</span>
<span class="ltx_bibblock">Rearrangement: A challenge for embodied ai.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Mathieu Belanger-Barrette.

</span>
<span class="ltx_bibblock">What is an average price for a collaborative robot?

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis,
Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine
Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth,
Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal,
Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor
Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl
Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag
Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton
Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted
Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich.

</span>
<span class="ltx_bibblock">Rt-1: Robotics transformer for real-world control at scale.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2212.06817</span>, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Constantinos Chamzas, Carlos Quintero-Pena, Zachary Kingston, Andreas Orthey,
Daniel Rakita, Michael Gleicher, Marc Toussaint, and LydiaÂ E. Kavraki.

</span>
<span class="ltx_bibblock">Motionbenchmaker: A tool to generate and benchmark motion planning
datasets.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE Robotics and Automation Letters</span>. IEEE, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A.X. Chang, T.Â Funkhouser, L.Â Guibas, Pat Hanrahan, Q.Â Huang, ZÂ Li,
S.Â Savarese, M.Â Savva, S.Â Song, H.Â Su, J.Â Xiao, L.Â Yi, and F.Â Yu.

</span>
<span class="ltx_bibblock">Shapenet: An information-rich 3d model repository.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Technical report, Stanford University â€” Princeton University
â€” Toyota Technological Institute at Chicago</span>, 2015.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
HaoÂ Su Charles RÂ Qi, LiÂ Yi and LeonidasÂ J Guibas.

</span>
<span class="ltx_bibblock">Pointnet++: Deep hierarchical feature learning on point sets in a
metric space.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Neural Information Processing Systems (NeurIPS)</span>, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Sachin Chitta, Ioan Sucan, and Steve Cousins.

</span>
<span class="ltx_bibblock">Moveit![ros topics].

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Robotics &amp; Automation Magazine</span>, 19(1):18â€“19, 2012.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Michael Danielczuk, Matthew Matl, Saurabh Gupta, Andrew Li, Andrew Lee, Jeff
Mahler, and Ken Goldberg.

</span>
<span class="ltx_bibblock">Segmenting unknown 3d objects from real depth images using mask r-cnn
trained on synthetic data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Robotics and Automation
(ICRA)</span>. IEEE, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Michael Danielczuk, Arsalan Mousavian, Clemens Eppner, and Dieter Fox.

</span>
<span class="ltx_bibblock">Object rearrangement using learned implicit collision functions.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">2021 IEEE International Conference on Robotics and Automation
(ICRA)</span>, pages 6010â€“6017, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Nikhil Das and Michael Yip.

</span>
<span class="ltx_bibblock">Learning-based proxy collision detection for robot motion planning
applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Transactions on Robotics</span>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana
Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, etÂ al.

</span>
<span class="ltx_bibblock">Procthor: Large-scale embodied ai using procedural generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2206.06994</span>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, and JakobÂ Uszkoreit andNeil Houlsby.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">In International Conference on Learning Representations</span>,
2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric
Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi.

</span>
<span class="ltx_bibblock">Manipulathor: A framework for visual object manipulation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Clemens Eppner, Arsalan Mousavian, and Dieter Fox.

</span>
<span class="ltx_bibblock">ACRONYM: A large-scale grasp dataset based on simulation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">2021 IEEE Int. Conf. on Robotics and Automation, ICRA</span>,
2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat
Hanrahan.

</span>
<span class="ltx_bibblock">Example-based synthesis of 3d object arrangements.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Graphics (TOG)</span>, 31(6):1â€“11, 2012.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Adam Fishman, Adithyavairavan Murali, Clemens Eppner, Bryan Peele, Byron Boots,
and Dieter Fox.

</span>
<span class="ltx_bibblock">Motion policy networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Conference on Robot Learning (CoRL)</span>, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng,
Chengyue Sun, Rongfei Jia, Binqiang Zhao, etÂ al.

</span>
<span class="ltx_bibblock">3d-front: 3d furnished rooms with layouts and semantics.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 10933â€“10942, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Clement FujiÂ Tsang, Maria Shugrina, JeanÂ Francois Lafleche, Towaki Takikawa,
Jiehan Wang, Charles Loop, Wenzheng Chen, KrishnaÂ Murthy Jatavallabhula,
Edward Smith, Artem Rozantsev, OrÂ Perel, Tianchang Shen, Jun Gao, Sanja
Fidler, Gavriel State, Jason Gorski, Tommy Xiang, Jianing Li, Michael Li, and
Rev Lebaredian.

</span>
<span class="ltx_bibblock">Kaolin: A pytorch library for accelerating 3d deep learning research.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/NVIDIAGameWorks/kaolin" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVIDIAGameWorks/kaolin</a>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A.Â Aldrich G.Â Williams and E.Â A. Theodorou.

</span>
<span class="ltx_bibblock">Model predictive path integral control: From theory to parallel
computation.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Journal of Guidance, Control, and Dynamics</span>, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
JonathanÂ D. Gammell, SiddharthaÂ S. Srinivasa, and TimÂ D. Barfoot.

</span>
<span class="ltx_bibblock">Batch informed trees (bit*): Sampling-based optimal planning via the
heuristically guided search of implicit random geometric graphs.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2015 IEEE International Conference on Robotics and Automation
(ICRA)</span>, pages 3067â€“3074, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
CaelanÂ Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Tom Silver,
LeslieÂ Pack Kaelbling, and TomÃ¡s Lozano-PÃ©rez.

</span>
<span class="ltx_bibblock">Integrated task and motion planning.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Annual Review of Control, Robotics, and Autonomous Systems</span>,
2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
CaelanÂ Reed Garrett, Chris Paxton, TomÃ¡s Lozano-PÃ©rez, LeslieÂ Pack Kaelbling,
and Dieter Fox.

</span>
<span class="ltx_bibblock">Online replanning in belief space for partially observable task and
motion problems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Robotics and Automation
(ICRA)</span>. IEEE, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial nets.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Neural Information Processing Systems</span>, 2014.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Walter Goodwin, Sagar Vaze, Ioannis Havoutis, and Ingmar Posner.

</span>
<span class="ltx_bibblock">Semantically grounded object matching for robust robotic scene
rearrangement.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">2022 International Conference on Robotics and Automation
(ICRA)</span>, pages 11138â€“11144. IEEE, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Ankit Goyal, Arsalan Mousavian, Chris Paxton, Yu-Wei Chao, Brian Okorn, Jia
Deng, and Dieter Fox.

</span>
<span class="ltx_bibblock">Ifor: Iterative flow minimization for robotic object rearrangement.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Abhinav Gupta, Adithyavairavan Murali, Dhiraj Gandhi, and Lerrel Pinto.

</span>
<span class="ltx_bibblock">Robot learning in homes: Improving generalization and reducing
dataset bias.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Neural Information Processing Systems (NeurIPS)</span>, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
KrisÂ K. Hauser and Victor Ng-Thow-Hing.

</span>
<span class="ltx_bibblock">Fast smoothing of manipulator trajectories using optimal
bounded-acceleration shortcuts.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">2010 IEEE International Conference on Robotics and Automation</span>,
pages 2493â€“2498, 2010.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
John Horst, Jeremy Marvel, and Elena Messina.

</span>
<span class="ltx_bibblock">Best practices for the integration of collaborative robots into
workcells within small and medium-sized manufacturing operations.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">NIST Advanced Manufacturing Series</span>, 100(41), 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Haojie Huang, Dian Wang, Robin Walter, and Robert Platt.

</span>
<span class="ltx_bibblock">Equivariant transporter network.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2202.09400</span>, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
PhilipÂ M Hubbard.

</span>
<span class="ltx_bibblock">Approximating polyhedra with spheres for time-critical collision
detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Graphics (TOG)</span>, 15(3):179â€“210, 1996.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Stephen James, Zicong Ma, David RovickÂ Arrojo, and AndrewÂ J. Davison.

</span>
<span class="ltx_bibblock">Rlbench: The robot learning benchmark &amp; learning environment.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">IEEE Robotics and Automation Letters</span>, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt
Rusiniak, David Acuna, Antonio Torralba, and Sanja Fidler.

</span>
<span class="ltx_bibblock">Meta-sim: Learning to generate synthetic datasets.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision (ICCV)</span>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J.Â Chase Kew, Brian Ichter, Maryam Bandari, Tsang-WeiÂ Edward Lee, and
Aleksandra Faust.

</span>
<span class="ltx_bibblock">Neural collision clearance estimator for batched motion planning.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Matthew Klingensmith, Siddartha Sirinivasa, and Michael Kaess.

</span>
<span class="ltx_bibblock">Articulated robot motion for simultaneous localization and mapping
(arm-slam).

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Robotics and Automation Letters</span>. IEEE, 2016.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
James Kuffner and StevenÂ M. LaValle.

</span>
<span class="ltx_bibblock">Rrt-connect: An efficient approach to single-query path planning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">International Conference on Robotics and Automation (ICRA)</span>.
IEEE, 2000.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Yann LabbÃ©, Sergey Zagoruyko, Igor Kalevatykh, Ivan Laptev, Justin
Carpentier, Mathieu Aubry, and Josef Sivic.

</span>
<span class="ltx_bibblock">Monte-carlo tree search for efficient visually guided rearrangement
planning.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">IEEE Robotics and Automation Letters</span>, 5(2):3715â€“3722, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Chengshu Li, Fei Xia, Roberto MartÃ­n-MartÃ­n, Michael Lingelbach,
Sanjana Srivastava, Bokui Shen, KentÂ Elliott Vainio, Cem Gokmen, Gokul
Dharan, Tanish Jain, Andrey Kurenkov, Karen Liu, Hyowon Gweon, Jiajun Wu,
LiÂ Fei-Fei, and Silvio Savarese.

</span>
<span class="ltx_bibblock">igibson 2.0: Object-centric simulation for robot learning of everyday
household tasks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Conference on Robot Learning</span>, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
KeÂ Li and Jitendra Malik.

</span>
<span class="ltx_bibblock">Implicit maximum likelihood estimation.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1809.09087</span>, 2018.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Jeffrey Mahler and Ken Goldberg.

</span>
<span class="ltx_bibblock">Learning deep policies for robot bin picking by simulating robust
grasping sequences.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Conference on Robot Learning</span>, 2017.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu
Liu, JuanÂ Aparicio Ojea, and Ken Goldberg.

</span>
<span class="ltx_bibblock">Dex-net 2.0: Deep learning to plan robust grasps with synthetic point
clouds and analytic grasp metrics.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">RSS</span>, 2017.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Lucas Majerowicz, Ariel Shamir, Alla Sheffer, and HolgerÂ H Hoos.

</span>
<span class="ltx_bibblock">Filling your shelves: Synthesizing diverse style-preserving artifact
arrangements.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">IEEE transactions on visualization and computer graphics</span>,
20(11):1507â€“1518, 2013.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Ben Mildenhall, PratulÂ P. Srinivasan, Matthew Tancik, JonathanÂ T. Barron, Ravi
Ramamoorthi, and Ren Ng.

</span>
<span class="ltx_bibblock">Nerf: Representing scenes as neural radiance fields for view
synthesis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision (ECCV)</span>, 2020.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Kaichun Mo, Shilin Zhu, AngelÂ X. Chang, LiÂ Yi, Subarna Tripathi, LeonidasÂ J.
Guibas, and Hao Su.

</span>
<span class="ltx_bibblock">PartNet: A large-scale benchmark for fine-grained and hierarchical
part-level 3D object understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, June 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Arsalan Mousavian, Clemens Eppner, and Dieter Fox.

</span>
<span class="ltx_bibblock">6-DOF GraspNet: Variational grasp generation for object
manipulation.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2019.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Arsalan Mousavian, Lucas Manuelli, Brian Okorn, YuÂ Xiang, Clemens Eppner,
Adithyavairavan Murali, and Dieter Fox.

</span>
<span class="ltx_bibblock">Objectseeker: A unified framework for one-shot object detection,
tracking, and instance segmentation of everyday objects.

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv</span>, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Adithyavairavan Murali, Arsalan Mousavian, Clemens Eppner, Chris Paxton, and
Dieter Fox.

</span>
<span class="ltx_bibblock">6-dof grasping for target-driven object manipulation in clutter.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Robotics and Automation
(ICRA)</span>, 2020.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Richard Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim,
AndrewÂ J. Davison, Pushmeet Kohli, Jamie Shotton, Steve Hodges, and Andrew
Fitzgibbon.

</span>
<span class="ltx_bibblock">Kinectfusion: Real-time dense surface mapping and tracking.

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">2011 10th IEEE International Symposium on Mixed and Augmented
Reality (ISMAR)</span>, 2011.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Jia Pan, Sachin Chitta, and Dinesh Manocha.

</span>
<span class="ltx_bibblock">Fcl: A general purpose library for collision and proximity queries.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">2012 IEEE International Conference on Robotics and Automation
(ICRA)</span>, 2012.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
AhmedÂ H Qureshi, Arsalan Mousavian, Chris Paxton, Michael Yip, and Dieter Fox.

</span>
<span class="ltx_bibblock">Nerp: Neural rearrangement planning for unknown objects.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">RSS</span>, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">arXiv:2103.00020</span>, 2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Siddharth Srivastava, Eugene Fang, Lorenzo Riano, Rohan Chitnis, Stuart
Russell, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Combined task and motion planning through an extensible
planner-independent interface layer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Robotics and Automation
(ICRA)</span>, 2014.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green,
JakobÂ J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson,
Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou,
Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler,
Luis Pesqueira, Manolis Savva, Dhruv Batra, HaukeÂ M. Strasdat, RenzoÂ De
Nardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe.

</span>
<span class="ltx_bibblock">The Replica dataset: A digital replica of indoor spaces.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.05797</span>, 2019.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Martin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, and Dieter Fox.

</span>
<span class="ltx_bibblock">Contact-graspnet: Efficient 6-dof grasp generation in cluttered
scenes.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John
Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets,
Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech
Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis
Savva, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Habitat 2.0: Training home assistants to rearrange their habitat.

</span>
<span class="ltx_bibblock">In <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</span>,
2021.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Karl VanÂ Wyk, Mandy Xie, Anqi Li, MuhammadÂ Asif Rana, Buck Babich, Bryan Peele,
Qian Wan, Iretiayo Akinola, Balakumar Sundaralingam, Dieter Fox, Byron Boots,
and NathanÂ D. Ratliff.

</span>
<span class="ltx_bibblock">Geometric fabrics: Generalizing classical mechanics to capture the
physics of behavior.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">IEEE Robotics and Automation Letters</span>, 7(2):3202â€“3209, 2022.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Yunrong Guo Michelle Lu Kier Storey Miles Macklin David Hoeller Nikita Rudin
Arthur Allshire Ankur Handa GavrielÂ State ViktorÂ Makoviychuk,
LukaszÂ Wawrzyniak.

</span>
<span class="ltx_bibblock">Isaac gym: High performance gpu-based physics simulation for robot
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">arXiv:2108.10470</span>, 2021.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Kentaro Wada, Stephen James, and AndrewÂ J. Davison.

</span>
<span class="ltx_bibblock">ReorientBot: Learning object reorientation for specific-posed
placement.

</span>
<span class="ltx_bibblock">In <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Robotics and Automation
(ICRA)</span>, 2022.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Hongtao Wu, Jikai Ye, Xin Meng, Chris Paxton, and Gregory Chirikjian.

</span>
<span class="ltx_bibblock">Transporters with visual foresight for solving unseen rearrangement
tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2202.10765</span>, 2022.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua
Liu, Hanxiao Jiang, Yifu Yuan, HeÂ Wang, LiÂ Yi, AngelÂ X. Chang, LeonidasÂ J.
Guibas, and Hao Su.

</span>
<span class="ltx_bibblock">SAPIEN: A simulated part-based interactive environment.

</span>
<span class="ltx_bibblock">In <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, June 2020.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Lap-Fai Yu, Sai-Kit Yeung, and Demetri Terzopoulos.

</span>
<span class="ltx_bibblock">The clutterpalette: An interactive tool for detailing indoor scenes.

</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">IEEE transactions on visualization and computer graphics</span>,
22(2):1138â€“1148, 2015.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea
Finn, and Sergey Levine.

</span>
<span class="ltx_bibblock">Meta-world: A benchmark and evaluation for multi-task and meta
reinforcement learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Conference on Robot Learning (CoRL)</span>, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien,
Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani,
etÂ al.

</span>
<span class="ltx_bibblock">Transporter networks: Rearranging the visual world for robotic
manipulation.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">Conference on Robot Learning</span>, 2020.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Open3D: A modern library for 3D data processing.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">arXiv:1801.09847</span>, 2018.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>

<div id="Ax1.p1" class="ltx_para">
<p id="Ax1.p1.1" class="ltx_p">We provide more examples of the synthetic data we used for training in Section <a href="#A1" title="Appendix A Additional Examples of Synthetic Data and Scenes â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>. We also describe how the CabiNet model was used in generating robot trajectories for object rearrangement in <a href="#A2" title="Appendix B Trajectory Generation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Examples of Synthetic Data and Scenes</h2>

<figure id="A1.F6" class="ltx_figure"><img src="/html/2304.09302/assets/figures/arxiv_more_dataset_c.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="631" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="A1.F6.3.2" class="ltx_text" style="font-size:90%;">We procedurally sampled scenes from five distribution of environments (from left to right): shelf (from ShapeNet), tabletop, cabinet, drawer and cubby. The scenes are exported to the USD file format and rendered in <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/" title="" class="ltx_ref ltx_href">NVIDIA Omniverse</a>. See Section <a href="#S3" title="3 Procedural Data Generation â€£ CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for more details.</span></figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Trajectory Generation</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.8" class="ltx_p">We refer the authors to the SceneCollisionNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> paper for a more detailed treatment of the trajectory generation process. We use a Model Predictive Control (MPC) framework to generate trajectories for object rearrangement. We leverage the GPU-parallelism inherent in the CabiNet architecture with batch collision queries. The task is specified entirely in configuration (joint) space and constraints in the form of joint limits, self-collisions and robot-scene collisions are enforced. With the exception of inverse kinematics (for which we use a CPU implementation with multi-processing), the entire trajectory generation stack consisting of trajectory sampling, cost computation, forward kinematics and collision checking is tensorized to run on a GPU for fast inference and closed-loop execution. We use a simple heuristic for sampling trajectories which we then filter using rejecting sampling. We construct a straight line trajectory <math id="A2.p1.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="A2.p1.1.m1.1a"><mi id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><ci id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">d</annotation></semantics></math> connecting the start and goal joint configuration and sample <math id="A2.p1.2.m2.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="A2.p1.2.m2.1a"><mi id="A2.p1.2.m2.1.1" xref="A2.p1.2.m2.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="A2.p1.2.m2.1b"><ci id="A2.p1.2.m2.1.1.cmml" xref="A2.p1.2.m2.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.2.m2.1c">\tau</annotation></semantics></math> trajectories drawn from a normal distribution centered on <math id="A2.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="A2.p1.3.m3.1a"><mi id="A2.p1.3.m3.1.1" xref="A2.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="A2.p1.3.m3.1b"><ci id="A2.p1.3.m3.1.1.cmml" xref="A2.p1.3.m3.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.3.m3.1c">d</annotation></semantics></math>: <math id="A2.p1.4.m4.3" class="ltx_Math" alttext="\hat{d}=N(\mathcal{N}(d,\Sigma))" display="inline"><semantics id="A2.p1.4.m4.3a"><mrow id="A2.p1.4.m4.3.3" xref="A2.p1.4.m4.3.3.cmml"><mover accent="true" id="A2.p1.4.m4.3.3.3" xref="A2.p1.4.m4.3.3.3.cmml"><mi id="A2.p1.4.m4.3.3.3.2" xref="A2.p1.4.m4.3.3.3.2.cmml">d</mi><mo id="A2.p1.4.m4.3.3.3.1" xref="A2.p1.4.m4.3.3.3.1.cmml">^</mo></mover><mo id="A2.p1.4.m4.3.3.2" xref="A2.p1.4.m4.3.3.2.cmml">=</mo><mrow id="A2.p1.4.m4.3.3.1" xref="A2.p1.4.m4.3.3.1.cmml"><mi id="A2.p1.4.m4.3.3.1.3" xref="A2.p1.4.m4.3.3.1.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="A2.p1.4.m4.3.3.1.2" xref="A2.p1.4.m4.3.3.1.2.cmml">â€‹</mo><mrow id="A2.p1.4.m4.3.3.1.1.1" xref="A2.p1.4.m4.3.3.1.1.1.1.cmml"><mo stretchy="false" id="A2.p1.4.m4.3.3.1.1.1.2" xref="A2.p1.4.m4.3.3.1.1.1.1.cmml">(</mo><mrow id="A2.p1.4.m4.3.3.1.1.1.1" xref="A2.p1.4.m4.3.3.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.p1.4.m4.3.3.1.1.1.1.2" xref="A2.p1.4.m4.3.3.1.1.1.1.2.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="A2.p1.4.m4.3.3.1.1.1.1.1" xref="A2.p1.4.m4.3.3.1.1.1.1.1.cmml">â€‹</mo><mrow id="A2.p1.4.m4.3.3.1.1.1.1.3.2" xref="A2.p1.4.m4.3.3.1.1.1.1.3.1.cmml"><mo stretchy="false" id="A2.p1.4.m4.3.3.1.1.1.1.3.2.1" xref="A2.p1.4.m4.3.3.1.1.1.1.3.1.cmml">(</mo><mi id="A2.p1.4.m4.1.1" xref="A2.p1.4.m4.1.1.cmml">d</mi><mo id="A2.p1.4.m4.3.3.1.1.1.1.3.2.2" xref="A2.p1.4.m4.3.3.1.1.1.1.3.1.cmml">,</mo><mi mathvariant="normal" id="A2.p1.4.m4.2.2" xref="A2.p1.4.m4.2.2.cmml">Î£</mi><mo stretchy="false" id="A2.p1.4.m4.3.3.1.1.1.1.3.2.3" xref="A2.p1.4.m4.3.3.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="A2.p1.4.m4.3.3.1.1.1.3" xref="A2.p1.4.m4.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.4.m4.3b"><apply id="A2.p1.4.m4.3.3.cmml" xref="A2.p1.4.m4.3.3"><eq id="A2.p1.4.m4.3.3.2.cmml" xref="A2.p1.4.m4.3.3.2"></eq><apply id="A2.p1.4.m4.3.3.3.cmml" xref="A2.p1.4.m4.3.3.3"><ci id="A2.p1.4.m4.3.3.3.1.cmml" xref="A2.p1.4.m4.3.3.3.1">^</ci><ci id="A2.p1.4.m4.3.3.3.2.cmml" xref="A2.p1.4.m4.3.3.3.2">ğ‘‘</ci></apply><apply id="A2.p1.4.m4.3.3.1.cmml" xref="A2.p1.4.m4.3.3.1"><times id="A2.p1.4.m4.3.3.1.2.cmml" xref="A2.p1.4.m4.3.3.1.2"></times><ci id="A2.p1.4.m4.3.3.1.3.cmml" xref="A2.p1.4.m4.3.3.1.3">ğ‘</ci><apply id="A2.p1.4.m4.3.3.1.1.1.1.cmml" xref="A2.p1.4.m4.3.3.1.1.1"><times id="A2.p1.4.m4.3.3.1.1.1.1.1.cmml" xref="A2.p1.4.m4.3.3.1.1.1.1.1"></times><ci id="A2.p1.4.m4.3.3.1.1.1.1.2.cmml" xref="A2.p1.4.m4.3.3.1.1.1.1.2">ğ’©</ci><interval closure="open" id="A2.p1.4.m4.3.3.1.1.1.1.3.1.cmml" xref="A2.p1.4.m4.3.3.1.1.1.1.3.2"><ci id="A2.p1.4.m4.1.1.cmml" xref="A2.p1.4.m4.1.1">ğ‘‘</ci><ci id="A2.p1.4.m4.2.2.cmml" xref="A2.p1.4.m4.2.2">Î£</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.4.m4.3c">\hat{d}=N(\mathcal{N}(d,\Sigma))</annotation></semantics></math>. We renormalize the direction vector <math id="A2.p1.5.m5.1" class="ltx_Math" alttext="\hat{d}" display="inline"><semantics id="A2.p1.5.m5.1a"><mover accent="true" id="A2.p1.5.m5.1.1" xref="A2.p1.5.m5.1.1.cmml"><mi id="A2.p1.5.m5.1.1.2" xref="A2.p1.5.m5.1.1.2.cmml">d</mi><mo id="A2.p1.5.m5.1.1.1" xref="A2.p1.5.m5.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A2.p1.5.m5.1b"><apply id="A2.p1.5.m5.1.1.cmml" xref="A2.p1.5.m5.1.1"><ci id="A2.p1.5.m5.1.1.1.cmml" xref="A2.p1.5.m5.1.1.1">^</ci><ci id="A2.p1.5.m5.1.1.2.cmml" xref="A2.p1.5.m5.1.1.2">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.5.m5.1c">\hat{d}</annotation></semantics></math> and construct trajectories <math id="A2.p1.6.m6.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="A2.p1.6.m6.1a"><mi id="A2.p1.6.m6.1.1" xref="A2.p1.6.m6.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="A2.p1.6.m6.1b"><ci id="A2.p1.6.m6.1.1.cmml" xref="A2.p1.6.m6.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.6.m6.1c">\tau</annotation></semantics></math> with <math id="A2.p1.7.m7.1" class="ltx_Math" alttext="H" display="inline"><semantics id="A2.p1.7.m7.1a"><mi id="A2.p1.7.m7.1.1" xref="A2.p1.7.m7.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="A2.p1.7.m7.1b"><ci id="A2.p1.7.m7.1.1.cmml" xref="A2.p1.7.m7.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.7.m7.1c">H</annotation></semantics></math> steps along <math id="A2.p1.8.m8.1" class="ltx_Math" alttext="\hat{d}" display="inline"><semantics id="A2.p1.8.m8.1a"><mover accent="true" id="A2.p1.8.m8.1.1" xref="A2.p1.8.m8.1.1.cmml"><mi id="A2.p1.8.m8.1.1.2" xref="A2.p1.8.m8.1.1.2.cmml">d</mi><mo id="A2.p1.8.m8.1.1.1" xref="A2.p1.8.m8.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A2.p1.8.m8.1b"><apply id="A2.p1.8.m8.1.1.cmml" xref="A2.p1.8.m8.1.1"><ci id="A2.p1.8.m8.1.1.1.cmml" xref="A2.p1.8.m8.1.1.1">^</ci><ci id="A2.p1.8.m8.1.1.2.cmml" xref="A2.p1.8.m8.1.1.2">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.8.m8.1c">\hat{d}</annotation></semantics></math>. The cost of each trajectory is based on distance in configuration space and lowest cost trajectory is then executed on the robot with a position controller.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.2" class="ltx_p">The sampled robot trajectories are filtered with rejecting sampling. We first enforce joint limit constraints by clipping the trajectories and check for self-collisions at each step of the trajectory using the model trained in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. For the robot-scene collisions, we use the CabiNet model. We sample point clouds from the surface of the collision mesh, for each link of the robot manipulator and treat it as a object point cloud when constructing the collision queries. For each trajectory, we make <math id="A2.p2.1.m1.1" class="ltx_Math" alttext="H\times D" display="inline"><semantics id="A2.p2.1.m1.1a"><mrow id="A2.p2.1.m1.1.1" xref="A2.p2.1.m1.1.1.cmml"><mi id="A2.p2.1.m1.1.1.2" xref="A2.p2.1.m1.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="A2.p2.1.m1.1.1.1" xref="A2.p2.1.m1.1.1.1.cmml">Ã—</mo><mi id="A2.p2.1.m1.1.1.3" xref="A2.p2.1.m1.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p2.1.m1.1b"><apply id="A2.p2.1.m1.1.1.cmml" xref="A2.p2.1.m1.1.1"><times id="A2.p2.1.m1.1.1.1.cmml" xref="A2.p2.1.m1.1.1.1"></times><ci id="A2.p2.1.m1.1.1.2.cmml" xref="A2.p2.1.m1.1.1.2">ğ»</ci><ci id="A2.p2.1.m1.1.1.3.cmml" xref="A2.p2.1.m1.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.m1.1c">H\times D</annotation></semantics></math> collision queries as a single forward pass in CabiNet, where <math id="A2.p2.2.m2.1" class="ltx_Math" alttext="D=7" display="inline"><semantics id="A2.p2.2.m2.1a"><mrow id="A2.p2.2.m2.1.1" xref="A2.p2.2.m2.1.1.cmml"><mi id="A2.p2.2.m2.1.1.2" xref="A2.p2.2.m2.1.1.2.cmml">D</mi><mo id="A2.p2.2.m2.1.1.1" xref="A2.p2.2.m2.1.1.1.cmml">=</mo><mn id="A2.p2.2.m2.1.1.3" xref="A2.p2.2.m2.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p2.2.m2.1b"><apply id="A2.p2.2.m2.1.1.cmml" xref="A2.p2.2.m2.1.1"><eq id="A2.p2.2.m2.1.1.1.cmml" xref="A2.p2.2.m2.1.1.1"></eq><ci id="A2.p2.2.m2.1.1.2.cmml" xref="A2.p2.2.m2.1.1.2">ğ·</ci><cn type="integer" id="A2.p2.2.m2.1.1.3.cmml" xref="A2.p2.2.m2.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.2.m2.1c">D=7</annotation></semantics></math> is the number of links for the franka robot. We empirically found that the CabiNet model could be used to sample robot-scene collision queries for the franka robot model even though it was not included in the training dataset. This demonstrates the generalization capability of the architecture since it trained with large scale synthetic data.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.09301" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.09302" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.09302">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.09302" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.09303" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 13:57:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
