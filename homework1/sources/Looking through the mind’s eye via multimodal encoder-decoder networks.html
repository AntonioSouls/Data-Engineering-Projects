<!DOCTYPE html><html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Looking through the mind’s eye via multimodal encoder-decoder networks</title>
<!--Generated on Fri Sep 27 19:52:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">




<base href="https://arxiv.org/html/2410.00047v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2410.00047v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2410.00047v1/#myForm">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2410.00047v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2410.00047v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S1" title="In Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S2" title="In Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S3" title="In Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S4" title="In Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S4.SS0.SSS0.Px1" title="In 4 Evaluation ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title">Implementation detail</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S4.SS0.SSS0.Px2" title="In 4 Evaluation ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title">Distribution matching</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S4.SS0.SSS0.Px3" title="In 4 Evaluation ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title">Reconstruction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S5" title="In Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S5.SS0.SSS0.Px1" title="In 5 Discussion ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title">Privacy and Ethical Implications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S5.SS0.SSS0.Px2" title="In 5 Discussion ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title">Security, Accuracy and Interpretation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S5.SS0.SSS0.Px3" title="In 5 Discussion ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title">Psychological Impact and Regulatory Framework</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S6" title="In Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Acknowledgments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div class="package-alerts ltx_document" role="status" aria-label="Conversion errors have been found">
      <button aria-label="Dismiss alert">
          <span aria-hidden="true"><svg role="presentation" width="20" height="20" viewBox="0 0 44 44" aria-hidden="true" focusable="false">
          <path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
          <path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
          </svg></span>
      </button>
      <p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
          <ul arial-label="Unsupported packages used in this paper">
              <li>failed: manyfoot</li>
          </ul>
      <p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
    </div><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY 4.0</a><div id="watermark-tr">arXiv:2410.00047v1 [eess.IV] 27 Sep 2024</div></div>
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Looking through the mind’s eye via multimodal encoder-decoder networks</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arman Afrasiyabi<sup class="ltx_sup" id="id14.14.id1"><span class="ltx_text ltx_font_italic" id="id14.14.id1.1">⋆⋄∙</span></sup>,
Erica Busch<sup class="ltx_sup" id="id15.15.id2"><span class="ltx_text ltx_font_italic" id="id15.15.id2.1">†∗∙</span></sup>, 
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id5.5.3">Rahul Singh<sup class="ltx_sup" id="id5.5.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id5.5.3.1.1">⋆∗∙</span></sup>, Dhananjay Bhaskar<sup class="ltx_sup" id="id5.5.3.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id5.5.3.2.1">⋆⋄∙</span></sup>, Laurent Caplette<sup class="ltx_sup" id="id5.5.3.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id5.5.3.3.1">†∗∙</span></sup></span>,

<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id7.7.5">Nicholas Turk-Browne<sup class="ltx_sup" id="id7.7.5.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.5.1.1">†∗∙</span></sup>, Smita Krishnaswamy<sup class="ltx_sup" id="id7.7.5.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.5.2.1">⋆⋄∘∗∙</span></sup></span>
<br class="ltx_break"><sup class="ltx_sup" id="id16.16.id3"><span class="ltx_text ltx_font_italic" id="id16.16.id3.1">⋆</span></sup> Department of Computer Science,
<sup class="ltx_sup" id="id17.17.id4"><span class="ltx_text ltx_font_italic" id="id17.17.id4.1">∘</span></sup> Applied Mathematics Program 
<br class="ltx_break"><sup class="ltx_sup" id="id18.18.id5"><span class="ltx_text ltx_font_italic" id="id18.18.id5.1">†</span></sup> Department of Psychology,
<sup class="ltx_sup" id="id19.19.id6"><span class="ltx_text ltx_font_italic" id="id19.19.id6.1">⋄</span></sup> Department of Genetics, 
<br class="ltx_break"><sup class="ltx_sup" id="id20.20.id7"><span class="ltx_text ltx_font_italic" id="id20.20.id7.1">∗</span></sup> Wu Tsai Institute,
<sup class="ltx_sup" id="id21.21.id8"><span class="ltx_text ltx_font_italic" id="id21.21.id8.1">∙</span></sup> Yale University
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><span class="ltx_text" id="footnotex1.1" style="color:#0000FF;"> This work is accepted and presented at the Center for Collaborative Arts and Media, Yale University.</span></span></span></span>
<p class="ltx_p" id="id22.id1"><span class="ltx_text" id="id22.id1.1">In this work, we explore the decoding of mental imagery from subjects using their fMRI measurements. In order to achieve this decoding, we first created a mapping between a subject’s fMRI signals elicited by the videos the subjects watched. This mapping associates the high dimensional fMRI activation states with visual imagery. Next, we prompted the subjects textually, primarily with emotion labels which had no direct reference to visual objects. Then to decode visual imagery that may have been in a person’s mind’s eye, we align a latent representation of these fMRI measurements with a corresponding video-fMRI based on textual labels given to the videos themselves. This alignment has the effect of overlapping the video fMRI embedding with the text-prompted fMRI embedding, thus allowing us to use our fMRI-to-video mapping to decode. Additionally, we enhance an existing fMRI dataset, initially consisting of data from five subjects, by including recordings from three more subjects gathered by our team. We demonstrate the efficacy of our model on this augmented dataset both in accurately creating a mapping, as well as in plausibly decoding mental imagery.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Humans have always endeavored to bring their thoughts and imaginations into tangible reality. In the 20th and 21th centuries, movements like Impressionism and Surrealism revolutionized the art world by depicting the stream of consciousness and dream-like states, pushing the boundaries of visual representation. In recent years, the field of AI-generated art has made significant strides, with algorithms like DALL<math alttext="\cdot" class="ltx_Math" display="inline" id="S1.p1.1.m1.1"><semantics id="S1.p1.1.m1.1a"><mo id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><ci id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="S1.p1.1.m1.1d">⋅</annotation></semantics></math>E&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib17" title="">17</a>]</cite>, Midjourney&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib14" title="">14</a>]</cite>, and Stable Diffusion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib9" title="">9</a>]</cite>, creating intricate images based on textual prompts provided by humans, further bridging the gap between thought and visual expression. Now, advances in neuroimaging and artificial intelligence are poised to unlock unprecedented capabilities, allowing for the direct decoding of images from human thought, thereby offering a new frontier in the visualization of the mind’s eye.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The trajectory of functional magnetic resonance imaging (fMRI)-based visual information reconstruction has rapidly evolved, initially relying on traditional decoding techniques&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib7" title="">7</a>]</cite> and advancing towards hyperrealistic reconstructions with the integration of deep learning and GANs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib12" title="">12</a>]</cite>. The introduction of diffusion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib9" title="">9</a>]</cite> and stable diffusion models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib18" title="">18</a>]</cite> has further refined fMRI-to-image reconstructions, achieving unparalleled quality and demonstrating the potential of neural imaging technology in decoding and visualizing human cognition&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib6" title="">6</a>]</cite>. This evolution underscores the merging of generative artificial intelligence with neuroimaging data, leading to breakthroughs in the direct translation of neural activity into visual representations and offering profound implications for aiding individuals with disabilities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib6" title="">6</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The existing methods are primarily trained on neuroimaging datasets obtained during image viewing to recreate those images, however, they fall short of fully capturing the nuanced spectrum of human thought and imagery. This is evident when considering the visualization of images that participants spontaneously generate in their minds. The core challenge lies in narrowing the gap between the tangible data used for training such models and the abstract, multifaceted nature of human thoughts and imagination. Our methodology aims to mitigate this limitation by aligning the representations of neuroimaging data with those of videos and imagined scenes. This approach transcends the mere replication of observed visuals; it endeavors to decode the thoughts evoked by a piece of text, identifying the image representations that best resonate with the neuroimaging activity elicited by our thoughts. This effort is about more than capturing snapshots of brain activity; it’s about deciphering the underlying neural mechanisms and converting them into a visual language understandable by AI. In doing this, we can reproduce not only the images a subject directly observes but also create visual representations of abstract thoughts and concepts, offering a fuller understanding of the cognitive and perceptual processes involved.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we introduce a novel method that captures and visually represents individuals’ thoughts stimulated textually, advancing beyond the conventional technique of reconstructing images from direct brain recordings triggered by visual stimuli. Our approach employs an innovative encoder-decoder-based algorithm, meticulously designed to link the visual representations encountered in scenes—such as those found in videos and images—with the brain recordings obtained during these visual experiences. This link is established during our model’s training phase. Subsequently, in the inference phase, our model innovatively generates and reconstructs visual appearances based on brain activity elicited by text prompts. Essentially, our algorithm translates the abstract thoughts captured in these recordings into vivid visual imagery, effectively aiming to ”visualize” the dreams or thought processes of individuals based on their brain activity.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This paper presents the following contributions. First, we introduce a novel multi-encoder-decoder architecture designed for effective training through both point-to-point and distribution-to-distribution matching between brain recordings and video samples. Secondly, we present the brain decoding inference method with a pre-trained diffusion model aimed at enhancing the quality of the decoded brain states. Thirdly, we expand upon an existing fMRI dataset, originally comprising data from five subjects as detailed in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib10" title="">10</a>]</cite>. We enrich this dataset by incorporating recordings from three additional subjects, collected by our research group, thereby broadening the scope and applicability of the dataset. Finally, we showcase the effectiveness of our proposed model through its application to this enhanced dataset. The results demonstrate that our model can reconstruct brain visualization of text in a manner that is both conceptually meaningful and consistent over time, underscoring the potential of our approach in decoding and visualizing human brain states.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The objective of this work is to establish a connection between cutting-edge generative models (images/videos, audio, and text) and human cognition or imagination represented via the corresponding brain recordings. In particular, we aim to comprehend how video clips can influence the model responsible for temporal imaginative inference, as illustrated in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S2.F1.sf1" title="In Figure 1 ‣ 2 Method ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_tag">1(a)</span></a>. To achieve this, we leverage an encoder-decoder model framework to introduce a method for text-based stimulation retrieval. This is facilitated by our newly developed guided generative models, which are adept at distribution matching. This approach significantly improves the accuracy of representation extraction from brain recordings, providing a novel pathway to understanding and enhancing the interface between artificial intelligence and human cognitive functions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S2.F1.sf2" title="In Figure 1 ‣ 2 Method ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_tag">1(b)</span></a>, we employ three encoder-decoder models(to infer the representations of video, text, and fMRI) aimed at reconstructing visual representations from brain recordings triggered by textual stimuli. Our approach incorporates three generative models to facilitate the creation of videos or images, specifically focusing on video reconstruction, brain stimulation, and a textual-stimulated encoder-decoder framework. Notably, our video-simulated brain decoder is designed to reconstruct brain recordings (such as those obtained from fMRI scans) by first mapping these recordings onto the video embeddings derived from the video encoder. Furthermore, we introduce an additional mapping function, visually represented by the orange model in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S2.F1.sf2" title="In Figure 1 ‣ 2 Method ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_tag">1(b)</span></a>, which facilitates the interaction between the video and the video-stimulated encoder-decoders. This enables our model to learn associative alignments through point-to-point matching techniques.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="646" id="S2.F1.sf1.g1" src="https://arxiv.org/html/2410.00047v1/x1.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>idea</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="461" id="S2.F1.sf2.g1" src="https://arxiv.org/html/2410.00047v1/x2.png" width="829">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>FluxBrain</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F1.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="968" id="S2.F1.sf3.g1" src="https://arxiv.org/html/2410.00047v1/x3.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>inference</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>In (a), our model provides a schematic overview, detailing its initial focus on learning the connections between video clips and imagery through machine learning techniques. Subsequently, it generates artistic visualizations based solely on brain recordings. (b) illustrates the architecture of our FluxBrain, comprising three distinct encoder-decoder models. These models are responsible for reconstructing video (left), brain activity during video stimulation (middle), and recovering text prompts (right). An orange network facilitates one-to-one matching between the embeddings of video and stimulus-brain recordings, while a green network aims at aligning the distributions of text- and video-stimulus prompts. (c) presents the inference pathway leading to the reconstruction of brain activity stimulated by text, utilizing our proposed architectural framework.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.2">Mathematically, the initial encoder, denoted as <math alttext="g_{\text{v}}(\cdot|\phi_{\text{v}})" class="ltx_math_unparsed" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><mrow id="S2.p3.1.m1.1b"><msub id="S2.p3.1.m1.1.1"><mi id="S2.p3.1.m1.1.1.2">g</mi><mtext id="S2.p3.1.m1.1.1.3">v</mtext></msub><mrow id="S2.p3.1.m1.1.2"><mo id="S2.p3.1.m1.1.2.1" stretchy="false">(</mo><mo id="S2.p3.1.m1.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo fence="false" id="S2.p3.1.m1.1.2.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.p3.1.m1.1.2.4"><mi id="S2.p3.1.m1.1.2.4.2">ϕ</mi><mtext id="S2.p3.1.m1.1.2.4.3">v</mtext></msub><mo id="S2.p3.1.m1.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">g_{\text{v}}(\cdot|\phi_{\text{v}})</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">italic_g start_POSTSUBSCRIPT v end_POSTSUBSCRIPT ( ⋅ | italic_ϕ start_POSTSUBSCRIPT v end_POSTSUBSCRIPT )</annotation></semantics></math>, and decoder, denoted as <math alttext="f_{\text{v}}(\cdot|\theta_{\text{v}})" class="ltx_math_unparsed" display="inline" id="S2.p3.2.m2.1"><semantics id="S2.p3.2.m2.1a"><mrow id="S2.p3.2.m2.1b"><msub id="S2.p3.2.m2.1.1"><mi id="S2.p3.2.m2.1.1.2">f</mi><mtext id="S2.p3.2.m2.1.1.3">v</mtext></msub><mrow id="S2.p3.2.m2.1.2"><mo id="S2.p3.2.m2.1.2.1" stretchy="false">(</mo><mo id="S2.p3.2.m2.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo fence="false" id="S2.p3.2.m2.1.2.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.p3.2.m2.1.2.4"><mi id="S2.p3.2.m2.1.2.4.2">θ</mi><mtext id="S2.p3.2.m2.1.2.4.3">v</mtext></msub><mo id="S2.p3.2.m2.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">f_{\text{v}}(\cdot|\theta_{\text{v}})</annotation><annotation encoding="application/x-llamapun" id="S2.p3.2.m2.1d">italic_f start_POSTSUBSCRIPT v end_POSTSUBSCRIPT ( ⋅ | italic_θ start_POSTSUBSCRIPT v end_POSTSUBSCRIPT )</annotation></semantics></math>, form a video-specific model architecture designed to learn representations by reconstructing video frames, thereby facilitating the learning of embeddings.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S6.EGx1">
<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{v}}=\frac{1}{|S_{\text{v}}|}\sum_{\mathbf{x}_{%
\text{v},i}\in S_{\text{v}}}||\mathbf{x}_{\text{v},i}-f_{\text{v}}(g_{\text{v}%
}(\mathbf{x}_{\text{v},i}|\phi_{\text{v}})|\theta_{\text{v}})||^{2}," class="ltx_math_unparsed" display="inline" id="S2.Ex1.m1.7"><semantics id="S2.Ex1.m1.7a"><mrow id="S2.Ex1.m1.7b"><msub id="S2.Ex1.m1.7.8"><mi class="ltx_font_mathcaligraphic" id="S2.Ex1.m1.7.8.2">ℒ</mi><mtext id="S2.Ex1.m1.7.8.3">v</mtext></msub><mo id="S2.Ex1.m1.7.9">=</mo><mstyle displaystyle="true" id="S2.Ex1.m1.1.1"><mfrac id="S2.Ex1.m1.1.1a"><mn id="S2.Ex1.m1.1.1.3">1</mn><mrow id="S2.Ex1.m1.1.1.1.1"><mo id="S2.Ex1.m1.1.1.1.1.2" stretchy="false">|</mo><msub id="S2.Ex1.m1.1.1.1.1.1"><mi id="S2.Ex1.m1.1.1.1.1.1.2">S</mi><mtext id="S2.Ex1.m1.1.1.1.1.1.3">v</mtext></msub><mo id="S2.Ex1.m1.1.1.1.1.3" stretchy="false">|</mo></mrow></mfrac></mstyle><mstyle displaystyle="true" id="S2.Ex1.m1.7.10"><munder id="S2.Ex1.m1.7.10a"><mo id="S2.Ex1.m1.7.10.2" movablelimits="false">∑</mo><mrow id="S2.Ex1.m1.3.3.2"><msub id="S2.Ex1.m1.3.3.2.4"><mi id="S2.Ex1.m1.3.3.2.4.2">𝐱</mi><mrow id="S2.Ex1.m1.3.3.2.2.2.4"><mtext id="S2.Ex1.m1.2.2.1.1.1.1">v</mtext><mo id="S2.Ex1.m1.3.3.2.2.2.4.1">,</mo><mi id="S2.Ex1.m1.3.3.2.2.2.2">i</mi></mrow></msub><mo id="S2.Ex1.m1.3.3.2.3">∈</mo><msub id="S2.Ex1.m1.3.3.2.5"><mi id="S2.Ex1.m1.3.3.2.5.2">S</mi><mtext id="S2.Ex1.m1.3.3.2.5.3">v</mtext></msub></mrow></munder></mstyle><mo fence="false" id="S2.Ex1.m1.7.11" rspace="0.167em" stretchy="false">|</mo><mo fence="false" id="S2.Ex1.m1.7.12" rspace="0.167em" stretchy="false">|</mo><msub id="S2.Ex1.m1.7.13"><mi id="S2.Ex1.m1.7.13.2">𝐱</mi><mrow id="S2.Ex1.m1.5.5.2.4"><mtext id="S2.Ex1.m1.4.4.1.1">v</mtext><mo id="S2.Ex1.m1.5.5.2.4.1">,</mo><mi id="S2.Ex1.m1.5.5.2.2">i</mi></mrow></msub><mo id="S2.Ex1.m1.7.14">−</mo><msub id="S2.Ex1.m1.7.15"><mi id="S2.Ex1.m1.7.15.2">f</mi><mtext id="S2.Ex1.m1.7.15.3">v</mtext></msub><mrow id="S2.Ex1.m1.7.16"><mo id="S2.Ex1.m1.7.16.1" stretchy="false">(</mo><msub id="S2.Ex1.m1.7.16.2"><mi id="S2.Ex1.m1.7.16.2.2">g</mi><mtext id="S2.Ex1.m1.7.16.2.3">v</mtext></msub><mrow id="S2.Ex1.m1.7.16.3"><mo id="S2.Ex1.m1.7.16.3.1" stretchy="false">(</mo><msub id="S2.Ex1.m1.7.16.3.2"><mi id="S2.Ex1.m1.7.16.3.2.2">𝐱</mi><mrow id="S2.Ex1.m1.7.7.2.4"><mtext id="S2.Ex1.m1.6.6.1.1">v</mtext><mo id="S2.Ex1.m1.7.7.2.4.1">,</mo><mi id="S2.Ex1.m1.7.7.2.2">i</mi></mrow></msub><mo fence="false" id="S2.Ex1.m1.7.16.3.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.Ex1.m1.7.16.3.4"><mi id="S2.Ex1.m1.7.16.3.4.2">ϕ</mi><mtext id="S2.Ex1.m1.7.16.3.4.3">v</mtext></msub><mo id="S2.Ex1.m1.7.16.3.5" stretchy="false">)</mo></mrow><mo fence="false" id="S2.Ex1.m1.7.16.4" rspace="0.167em" stretchy="false">|</mo><msub id="S2.Ex1.m1.7.16.5"><mi id="S2.Ex1.m1.7.16.5.2">θ</mi><mtext id="S2.Ex1.m1.7.16.5.3">v</mtext></msub><mo id="S2.Ex1.m1.7.16.6" stretchy="false">)</mo></mrow><mo fence="false" id="S2.Ex1.m1.7.17" rspace="0.167em" stretchy="false">|</mo><msup id="S2.Ex1.m1.7.18"><mo fence="false" id="S2.Ex1.m1.7.18.2" rspace="0.167em" stretchy="false">|</mo><mn id="S2.Ex1.m1.7.18.3">2</mn></msup><mo id="S2.Ex1.m1.7.19">,</mo></mrow><annotation encoding="application/x-tex" id="S2.Ex1.m1.7c">\displaystyle\mathcal{L}_{\text{v}}=\frac{1}{|S_{\text{v}}|}\sum_{\mathbf{x}_{%
\text{v},i}\in S_{\text{v}}}||\mathbf{x}_{\text{v},i}-f_{\text{v}}(g_{\text{v}%
}(\mathbf{x}_{\text{v},i}|\phi_{\text{v}})|\theta_{\text{v}})||^{2},</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.7d">caligraphic_L start_POSTSUBSCRIPT v end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG | italic_S start_POSTSUBSCRIPT v end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT v , italic_i end_POSTSUBSCRIPT ∈ italic_S start_POSTSUBSCRIPT v end_POSTSUBSCRIPT end_POSTSUBSCRIPT | | bold_x start_POSTSUBSCRIPT v , italic_i end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT v end_POSTSUBSCRIPT ( italic_g start_POSTSUBSCRIPT v end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT v , italic_i end_POSTSUBSCRIPT | italic_ϕ start_POSTSUBSCRIPT v end_POSTSUBSCRIPT ) | italic_θ start_POSTSUBSCRIPT v end_POSTSUBSCRIPT ) | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p3.8">where <math alttext="\mathbf{x}_{\text{v},i}" class="ltx_Math" display="inline" id="S2.p3.3.m1.2"><semantics id="S2.p3.3.m1.2a"><msub id="S2.p3.3.m1.2.3" xref="S2.p3.3.m1.2.3.cmml"><mi id="S2.p3.3.m1.2.3.2" xref="S2.p3.3.m1.2.3.2.cmml">𝐱</mi><mrow id="S2.p3.3.m1.2.2.2.4" xref="S2.p3.3.m1.2.2.2.3.cmml"><mtext id="S2.p3.3.m1.1.1.1.1" xref="S2.p3.3.m1.1.1.1.1a.cmml">v</mtext><mo id="S2.p3.3.m1.2.2.2.4.1" xref="S2.p3.3.m1.2.2.2.3.cmml">,</mo><mi id="S2.p3.3.m1.2.2.2.2" xref="S2.p3.3.m1.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p3.3.m1.2b"><apply id="S2.p3.3.m1.2.3.cmml" xref="S2.p3.3.m1.2.3"><csymbol cd="ambiguous" id="S2.p3.3.m1.2.3.1.cmml" xref="S2.p3.3.m1.2.3">subscript</csymbol><ci id="S2.p3.3.m1.2.3.2.cmml" xref="S2.p3.3.m1.2.3.2">𝐱</ci><list id="S2.p3.3.m1.2.2.2.3.cmml" xref="S2.p3.3.m1.2.2.2.4"><ci id="S2.p3.3.m1.1.1.1.1a.cmml" xref="S2.p3.3.m1.1.1.1.1"><mtext id="S2.p3.3.m1.1.1.1.1.cmml" mathsize="70%" xref="S2.p3.3.m1.1.1.1.1">v</mtext></ci><ci id="S2.p3.3.m1.2.2.2.2.cmml" xref="S2.p3.3.m1.2.2.2.2">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m1.2c">\mathbf{x}_{\text{v},i}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.3.m1.2d">bold_x start_POSTSUBSCRIPT v , italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is a frame of a video <math alttext="S_{\text{v}}" class="ltx_Math" display="inline" id="S2.p3.4.m2.1"><semantics id="S2.p3.4.m2.1a"><msub id="S2.p3.4.m2.1.1" xref="S2.p3.4.m2.1.1.cmml"><mi id="S2.p3.4.m2.1.1.2" xref="S2.p3.4.m2.1.1.2.cmml">S</mi><mtext id="S2.p3.4.m2.1.1.3" xref="S2.p3.4.m2.1.1.3a.cmml">v</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p3.4.m2.1b"><apply id="S2.p3.4.m2.1.1.cmml" xref="S2.p3.4.m2.1.1"><csymbol cd="ambiguous" id="S2.p3.4.m2.1.1.1.cmml" xref="S2.p3.4.m2.1.1">subscript</csymbol><ci id="S2.p3.4.m2.1.1.2.cmml" xref="S2.p3.4.m2.1.1.2">𝑆</ci><ci id="S2.p3.4.m2.1.1.3a.cmml" xref="S2.p3.4.m2.1.1.3"><mtext id="S2.p3.4.m2.1.1.3.cmml" mathsize="70%" xref="S2.p3.4.m2.1.1.3">v</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m2.1c">S_{\text{v}}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.4.m2.1d">italic_S start_POSTSUBSCRIPT v end_POSTSUBSCRIPT</annotation></semantics></math>. Beside, <math alttext="\theta_{\text{v}}" class="ltx_Math" display="inline" id="S2.p3.5.m3.1"><semantics id="S2.p3.5.m3.1a"><msub id="S2.p3.5.m3.1.1" xref="S2.p3.5.m3.1.1.cmml"><mi id="S2.p3.5.m3.1.1.2" xref="S2.p3.5.m3.1.1.2.cmml">θ</mi><mtext id="S2.p3.5.m3.1.1.3" xref="S2.p3.5.m3.1.1.3a.cmml">v</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p3.5.m3.1b"><apply id="S2.p3.5.m3.1.1.cmml" xref="S2.p3.5.m3.1.1"><csymbol cd="ambiguous" id="S2.p3.5.m3.1.1.1.cmml" xref="S2.p3.5.m3.1.1">subscript</csymbol><ci id="S2.p3.5.m3.1.1.2.cmml" xref="S2.p3.5.m3.1.1.2">𝜃</ci><ci id="S2.p3.5.m3.1.1.3a.cmml" xref="S2.p3.5.m3.1.1.3"><mtext id="S2.p3.5.m3.1.1.3.cmml" mathsize="70%" xref="S2.p3.5.m3.1.1.3">v</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m3.1c">\theta_{\text{v}}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.5.m3.1d">italic_θ start_POSTSUBSCRIPT v end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\phi_{\text{v}}" class="ltx_Math" display="inline" id="S2.p3.6.m4.1"><semantics id="S2.p3.6.m4.1a"><msub id="S2.p3.6.m4.1.1" xref="S2.p3.6.m4.1.1.cmml"><mi id="S2.p3.6.m4.1.1.2" xref="S2.p3.6.m4.1.1.2.cmml">ϕ</mi><mtext id="S2.p3.6.m4.1.1.3" xref="S2.p3.6.m4.1.1.3a.cmml">v</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p3.6.m4.1b"><apply id="S2.p3.6.m4.1.1.cmml" xref="S2.p3.6.m4.1.1"><csymbol cd="ambiguous" id="S2.p3.6.m4.1.1.1.cmml" xref="S2.p3.6.m4.1.1">subscript</csymbol><ci id="S2.p3.6.m4.1.1.2.cmml" xref="S2.p3.6.m4.1.1.2">italic-ϕ</ci><ci id="S2.p3.6.m4.1.1.3a.cmml" xref="S2.p3.6.m4.1.1.3"><mtext id="S2.p3.6.m4.1.1.3.cmml" mathsize="70%" xref="S2.p3.6.m4.1.1.3">v</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m4.1c">\phi_{\text{v}}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.6.m4.1d">italic_ϕ start_POSTSUBSCRIPT v end_POSTSUBSCRIPT</annotation></semantics></math> are the parameters of the <math alttext="f_{\text{v}}" class="ltx_Math" display="inline" id="S2.p3.7.m5.1"><semantics id="S2.p3.7.m5.1a"><msub id="S2.p3.7.m5.1.1" xref="S2.p3.7.m5.1.1.cmml"><mi id="S2.p3.7.m5.1.1.2" xref="S2.p3.7.m5.1.1.2.cmml">f</mi><mtext id="S2.p3.7.m5.1.1.3" xref="S2.p3.7.m5.1.1.3a.cmml">v</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p3.7.m5.1b"><apply id="S2.p3.7.m5.1.1.cmml" xref="S2.p3.7.m5.1.1"><csymbol cd="ambiguous" id="S2.p3.7.m5.1.1.1.cmml" xref="S2.p3.7.m5.1.1">subscript</csymbol><ci id="S2.p3.7.m5.1.1.2.cmml" xref="S2.p3.7.m5.1.1.2">𝑓</ci><ci id="S2.p3.7.m5.1.1.3a.cmml" xref="S2.p3.7.m5.1.1.3"><mtext id="S2.p3.7.m5.1.1.3.cmml" mathsize="70%" xref="S2.p3.7.m5.1.1.3">v</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m5.1c">f_{\text{v}}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.7.m5.1d">italic_f start_POSTSUBSCRIPT v end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="g_{\text{v}}" class="ltx_Math" display="inline" id="S2.p3.8.m6.1"><semantics id="S2.p3.8.m6.1a"><msub id="S2.p3.8.m6.1.1" xref="S2.p3.8.m6.1.1.cmml"><mi id="S2.p3.8.m6.1.1.2" xref="S2.p3.8.m6.1.1.2.cmml">g</mi><mtext id="S2.p3.8.m6.1.1.3" xref="S2.p3.8.m6.1.1.3a.cmml">v</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p3.8.m6.1b"><apply id="S2.p3.8.m6.1.1.cmml" xref="S2.p3.8.m6.1.1"><csymbol cd="ambiguous" id="S2.p3.8.m6.1.1.1.cmml" xref="S2.p3.8.m6.1.1">subscript</csymbol><ci id="S2.p3.8.m6.1.1.2.cmml" xref="S2.p3.8.m6.1.1.2">𝑔</ci><ci id="S2.p3.8.m6.1.1.3a.cmml" xref="S2.p3.8.m6.1.1.3"><mtext id="S2.p3.8.m6.1.1.3.cmml" mathsize="70%" xref="S2.p3.8.m6.1.1.3">v</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.8.m6.1c">g_{\text{v}}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.8.m6.1d">italic_g start_POSTSUBSCRIPT v end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.3">Equipped with the video reconstruction autoencoder, we proceed to construct an fMRI encoder, <math alttext="g_{\text{f}}(\cdot|\phi_{\text{f}})" class="ltx_math_unparsed" display="inline" id="S2.p4.1.m1.1"><semantics id="S2.p4.1.m1.1a"><mrow id="S2.p4.1.m1.1b"><msub id="S2.p4.1.m1.1.1"><mi id="S2.p4.1.m1.1.1.2">g</mi><mtext id="S2.p4.1.m1.1.1.3">f</mtext></msub><mrow id="S2.p4.1.m1.1.2"><mo id="S2.p4.1.m1.1.2.1" stretchy="false">(</mo><mo id="S2.p4.1.m1.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo fence="false" id="S2.p4.1.m1.1.2.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.p4.1.m1.1.2.4"><mi id="S2.p4.1.m1.1.2.4.2">ϕ</mi><mtext id="S2.p4.1.m1.1.2.4.3">f</mtext></msub><mo id="S2.p4.1.m1.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">g_{\text{f}}(\cdot|\phi_{\text{f}})</annotation><annotation encoding="application/x-llamapun" id="S2.p4.1.m1.1d">italic_g start_POSTSUBSCRIPT f end_POSTSUBSCRIPT ( ⋅ | italic_ϕ start_POSTSUBSCRIPT f end_POSTSUBSCRIPT )</annotation></semantics></math>, and decoder, <math alttext="f_{\text{f}}(\cdot|\theta_{\text{f}})" class="ltx_math_unparsed" display="inline" id="S2.p4.2.m2.1"><semantics id="S2.p4.2.m2.1a"><mrow id="S2.p4.2.m2.1b"><msub id="S2.p4.2.m2.1.1"><mi id="S2.p4.2.m2.1.1.2">f</mi><mtext id="S2.p4.2.m2.1.1.3">f</mtext></msub><mrow id="S2.p4.2.m2.1.2"><mo id="S2.p4.2.m2.1.2.1" stretchy="false">(</mo><mo id="S2.p4.2.m2.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo fence="false" id="S2.p4.2.m2.1.2.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.p4.2.m2.1.2.4"><mi id="S2.p4.2.m2.1.2.4.2">θ</mi><mtext id="S2.p4.2.m2.1.2.4.3">f</mtext></msub><mo id="S2.p4.2.m2.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.p4.2.m2.1c">f_{\text{f}}(\cdot|\theta_{\text{f}})</annotation><annotation encoding="application/x-llamapun" id="S2.p4.2.m2.1d">italic_f start_POSTSUBSCRIPT f end_POSTSUBSCRIPT ( ⋅ | italic_θ start_POSTSUBSCRIPT f end_POSTSUBSCRIPT )</annotation></semantics></math>, architecture specifically tailored to learn the representations of brain recordings stimulated by video. With these dual encoder-decoder architectures for video and fMRI, we now build a bridge to create a learning module that translates the fMRI encoder’s output into the video encoder’s input. To this end, we employ a lightweight mapping function, <math alttext="\text{MAP}_{\text{f}\rightarrow\text{v}}(\cdot)=q(\cdot|W_{\text{q}})" class="ltx_math_unparsed" display="inline" id="S2.p4.3.m3.1"><semantics id="S2.p4.3.m3.1a"><mrow id="S2.p4.3.m3.1b"><msub id="S2.p4.3.m3.1.2"><mtext id="S2.p4.3.m3.1.2.2">MAP</mtext><mrow id="S2.p4.3.m3.1.2.3"><mtext id="S2.p4.3.m3.1.2.3.2">f</mtext><mo id="S2.p4.3.m3.1.2.3.1" stretchy="false">→</mo><mtext id="S2.p4.3.m3.1.2.3.3">v</mtext></mrow></msub><mrow id="S2.p4.3.m3.1.3"><mo id="S2.p4.3.m3.1.3.1" stretchy="false">(</mo><mo id="S2.p4.3.m3.1.1" lspace="0em" rspace="0em">⋅</mo><mo id="S2.p4.3.m3.1.3.2" stretchy="false">)</mo></mrow><mo id="S2.p4.3.m3.1.4">=</mo><mi id="S2.p4.3.m3.1.5">q</mi><mrow id="S2.p4.3.m3.1.6"><mo id="S2.p4.3.m3.1.6.1" stretchy="false">(</mo><mo id="S2.p4.3.m3.1.6.2" lspace="0em" rspace="0em">⋅</mo><mo fence="false" id="S2.p4.3.m3.1.6.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.p4.3.m3.1.6.4"><mi id="S2.p4.3.m3.1.6.4.2">W</mi><mtext id="S2.p4.3.m3.1.6.4.3">q</mtext></msub><mo id="S2.p4.3.m3.1.6.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.p4.3.m3.1c">\text{MAP}_{\text{f}\rightarrow\text{v}}(\cdot)=q(\cdot|W_{\text{q}})</annotation><annotation encoding="application/x-llamapun" id="S2.p4.3.m3.1d">MAP start_POSTSUBSCRIPT f → v end_POSTSUBSCRIPT ( ⋅ ) = italic_q ( ⋅ | italic_W start_POSTSUBSCRIPT q end_POSTSUBSCRIPT )</annotation></semantics></math>, that aims at point-to-point matching fMRI embeddings with their corresponding video frames to which the brain has been exposed. Given these elements, the overall loss function for training the fMRI encoder-decoder and refining the encoder for both fMRI and video would be as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S6.EGx2">
<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{f}}=\frac{1}{|S_{\text{f}}|}\sum_{\mathbf{x}_{%
\text{f},i}\in S_{\text{f}}}||\mathbf{x}_{\text{f},i}-f_{\text{f}}(g_{\text{f}%
}(\mathbf{x}_{\text{f},i}|\phi_{\text{f}})|\theta_{\text{f}})||^{2}+\frac{1}{|%
S_{\text{f}}|}\sum_{\begin{subarray}{c}(\textbf{x}_{\text{v},i})\in S_{\text{v%
}}\\
(\textbf{x}_{\text{f},i})\in S_{\text{f}}\end{subarray}}\|g_{\text{v}}(\mathbf%
{x}_{\text{v},i}|\phi_{\text{v}})-\text{MAP}_{\text{f}\rightarrow\text{v}}(g_{%
\text{f}}(\mathbf{x}_{\text{f},i}|\phi_{\text{f}})\|^{2}," class="ltx_math_unparsed" display="inline" id="S2.Ex2.m1.13"><semantics id="S2.Ex2.m1.13a"><mrow id="S2.Ex2.m1.13b"><msub id="S2.Ex2.m1.13.14"><mi class="ltx_font_mathcaligraphic" id="S2.Ex2.m1.13.14.2">ℒ</mi><mtext id="S2.Ex2.m1.13.14.3">f</mtext></msub><mo id="S2.Ex2.m1.13.15">=</mo><mstyle displaystyle="true" id="S2.Ex2.m1.2.2"><mfrac id="S2.Ex2.m1.2.2a"><mn id="S2.Ex2.m1.2.2.3">1</mn><mrow id="S2.Ex2.m1.2.2.1.1"><mo id="S2.Ex2.m1.2.2.1.1.2" stretchy="false">|</mo><msub id="S2.Ex2.m1.2.2.1.1.1"><mi id="S2.Ex2.m1.2.2.1.1.1.2">S</mi><mtext id="S2.Ex2.m1.2.2.1.1.1.3">f</mtext></msub><mo id="S2.Ex2.m1.2.2.1.1.3" stretchy="false">|</mo></mrow></mfrac></mstyle><mstyle displaystyle="true" id="S2.Ex2.m1.13.16"><munder id="S2.Ex2.m1.13.16a"><mo id="S2.Ex2.m1.13.16.2" movablelimits="false">∑</mo><mrow id="S2.Ex2.m1.4.4.2"><msub id="S2.Ex2.m1.4.4.2.4"><mi id="S2.Ex2.m1.4.4.2.4.2">𝐱</mi><mrow id="S2.Ex2.m1.4.4.2.2.2.4"><mtext id="S2.Ex2.m1.3.3.1.1.1.1">f</mtext><mo id="S2.Ex2.m1.4.4.2.2.2.4.1">,</mo><mi id="S2.Ex2.m1.4.4.2.2.2.2">i</mi></mrow></msub><mo id="S2.Ex2.m1.4.4.2.3">∈</mo><msub id="S2.Ex2.m1.4.4.2.5"><mi id="S2.Ex2.m1.4.4.2.5.2">S</mi><mtext id="S2.Ex2.m1.4.4.2.5.3">f</mtext></msub></mrow></munder></mstyle><mo fence="false" id="S2.Ex2.m1.13.17" rspace="0.167em" stretchy="false">|</mo><mo fence="false" id="S2.Ex2.m1.13.18" rspace="0.167em" stretchy="false">|</mo><msub id="S2.Ex2.m1.13.19"><mi id="S2.Ex2.m1.13.19.2">𝐱</mi><mrow id="S2.Ex2.m1.6.6.2.4"><mtext id="S2.Ex2.m1.5.5.1.1">f</mtext><mo id="S2.Ex2.m1.6.6.2.4.1">,</mo><mi id="S2.Ex2.m1.6.6.2.2">i</mi></mrow></msub><mo id="S2.Ex2.m1.13.20">−</mo><msub id="S2.Ex2.m1.13.21"><mi id="S2.Ex2.m1.13.21.2">f</mi><mtext id="S2.Ex2.m1.13.21.3">f</mtext></msub><mrow id="S2.Ex2.m1.13.22"><mo id="S2.Ex2.m1.13.22.1" stretchy="false">(</mo><msub id="S2.Ex2.m1.13.22.2"><mi id="S2.Ex2.m1.13.22.2.2">g</mi><mtext id="S2.Ex2.m1.13.22.2.3">f</mtext></msub><mrow id="S2.Ex2.m1.13.22.3"><mo id="S2.Ex2.m1.13.22.3.1" stretchy="false">(</mo><msub id="S2.Ex2.m1.13.22.3.2"><mi id="S2.Ex2.m1.13.22.3.2.2">𝐱</mi><mrow id="S2.Ex2.m1.8.8.2.4"><mtext id="S2.Ex2.m1.7.7.1.1">f</mtext><mo id="S2.Ex2.m1.8.8.2.4.1">,</mo><mi id="S2.Ex2.m1.8.8.2.2">i</mi></mrow></msub><mo fence="false" id="S2.Ex2.m1.13.22.3.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.Ex2.m1.13.22.3.4"><mi id="S2.Ex2.m1.13.22.3.4.2">ϕ</mi><mtext id="S2.Ex2.m1.13.22.3.4.3">f</mtext></msub><mo id="S2.Ex2.m1.13.22.3.5" stretchy="false">)</mo></mrow><mo fence="false" id="S2.Ex2.m1.13.22.4" rspace="0.167em" stretchy="false">|</mo><msub id="S2.Ex2.m1.13.22.5"><mi id="S2.Ex2.m1.13.22.5.2">θ</mi><mtext id="S2.Ex2.m1.13.22.5.3">f</mtext></msub><mo id="S2.Ex2.m1.13.22.6" stretchy="false">)</mo></mrow><mo fence="false" id="S2.Ex2.m1.13.23" rspace="0.167em" stretchy="false">|</mo><msup id="S2.Ex2.m1.13.24"><mo fence="false" id="S2.Ex2.m1.13.24.2" stretchy="false">|</mo><mn id="S2.Ex2.m1.13.24.3">2</mn></msup><mo id="S2.Ex2.m1.13.25" lspace="0em">+</mo><mstyle displaystyle="true" id="S2.Ex2.m1.9.9"><mfrac id="S2.Ex2.m1.9.9a"><mn id="S2.Ex2.m1.9.9.3">1</mn><mrow id="S2.Ex2.m1.9.9.1.1"><mo id="S2.Ex2.m1.9.9.1.1.2" stretchy="false">|</mo><msub id="S2.Ex2.m1.9.9.1.1.1"><mi id="S2.Ex2.m1.9.9.1.1.1.2">S</mi><mtext id="S2.Ex2.m1.9.9.1.1.1.3">f</mtext></msub><mo id="S2.Ex2.m1.9.9.1.1.3" stretchy="false">|</mo></mrow></mfrac></mstyle><mstyle displaystyle="true" id="S2.Ex2.m1.13.26"><munder id="S2.Ex2.m1.13.26a"><mo id="S2.Ex2.m1.13.26.2" movablelimits="false">∑</mo><mtable id="S2.Ex2.m1.1.1.1.1.1.1" rowspacing="0pt"><mtr id="S2.Ex2.m1.1.1.1.1.1.1a"><mtd id="S2.Ex2.m1.1.1.1.1.1.1b"><mrow id="S2.Ex2.m1.1.1.1.1.1.1.3.3.3.3"><mrow id="S2.Ex2.m1.1.1.1.1.1.1.3.3.3.3.3.1"><mo id="S2.Ex2.m1.1.1.1.1.1.1.3.3.3.3.3.1.2" stretchy="false">(</mo><msub id="S2.Ex2.m1.1.1.1.1.1.1.3.3.3.3.3.1.1"><mtext class="ltx_mathvariant_bold" id="S2.Ex2.m1.1.1.1.1.1.1.3.3.3.3.3.1.1.2">x</mtext><mrow id="S2.Ex2.m1.1.1.1.1.1.1.2.2.2.2.2.2.4"><mtext id="S2.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1">v</mtext><mo id="S2.Ex2.m1.1.1.1.1.1.1.2.2.2.2.2.2.4.1">,</mo><mi id="S2.Ex2.m1.1.1.1.1.1.1.2.2.2.2.2.2.2">i</mi></mrow></msub><mo id="S2.Ex2.m1.1.1.1.1.1.1.3.3.3.3.3.1.3" stretchy="false">)</mo></mrow><mo id="S2.Ex2.m1.1.1.1.1.1.1.3.3.3.3.4">∈</mo><msub id="S2.Ex2.m1.1.1.1.1.1.1.3.3.3.3.5"><mi id="S2.Ex2.m1.1.1.1.1.1.1.3.3.3.3.5.2">S</mi><mtext id="S2.Ex2.m1.1.1.1.1.1.1.3.3.3.3.5.3">v</mtext></msub></mrow></mtd></mtr><mtr id="S2.Ex2.m1.1.1.1.1.1.1c"><mtd id="S2.Ex2.m1.1.1.1.1.1.1d"><mrow id="S2.Ex2.m1.1.1.1.1.1.1.6.6.3.3"><mrow id="S2.Ex2.m1.1.1.1.1.1.1.6.6.3.3.3.1"><mo id="S2.Ex2.m1.1.1.1.1.1.1.6.6.3.3.3.1.2" stretchy="false">(</mo><msub id="S2.Ex2.m1.1.1.1.1.1.1.6.6.3.3.3.1.1"><mtext class="ltx_mathvariant_bold" id="S2.Ex2.m1.1.1.1.1.1.1.6.6.3.3.3.1.1.2">x</mtext><mrow id="S2.Ex2.m1.1.1.1.1.1.1.5.5.2.2.2.2.4"><mtext id="S2.Ex2.m1.1.1.1.1.1.1.4.4.1.1.1.1.1">f</mtext><mo id="S2.Ex2.m1.1.1.1.1.1.1.5.5.2.2.2.2.4.1">,</mo><mi id="S2.Ex2.m1.1.1.1.1.1.1.5.5.2.2.2.2.2">i</mi></mrow></msub><mo id="S2.Ex2.m1.1.1.1.1.1.1.6.6.3.3.3.1.3" stretchy="false">)</mo></mrow><mo id="S2.Ex2.m1.1.1.1.1.1.1.6.6.3.3.4">∈</mo><msub id="S2.Ex2.m1.1.1.1.1.1.1.6.6.3.3.5"><mi id="S2.Ex2.m1.1.1.1.1.1.1.6.6.3.3.5.2">S</mi><mtext id="S2.Ex2.m1.1.1.1.1.1.1.6.6.3.3.5.3">f</mtext></msub></mrow></mtd></mtr></mtable></munder></mstyle><mo id="S2.Ex2.m1.13.27" lspace="0em" rspace="0.167em">∥</mo><msub id="S2.Ex2.m1.13.28"><mi id="S2.Ex2.m1.13.28.2">g</mi><mtext id="S2.Ex2.m1.13.28.3">v</mtext></msub><mrow id="S2.Ex2.m1.13.29"><mo id="S2.Ex2.m1.13.29.1" stretchy="false">(</mo><msub id="S2.Ex2.m1.13.29.2"><mi id="S2.Ex2.m1.13.29.2.2">𝐱</mi><mrow id="S2.Ex2.m1.11.11.2.4"><mtext id="S2.Ex2.m1.10.10.1.1">v</mtext><mo id="S2.Ex2.m1.11.11.2.4.1">,</mo><mi id="S2.Ex2.m1.11.11.2.2">i</mi></mrow></msub><mo fence="false" id="S2.Ex2.m1.13.29.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.Ex2.m1.13.29.4"><mi id="S2.Ex2.m1.13.29.4.2">ϕ</mi><mtext id="S2.Ex2.m1.13.29.4.3">v</mtext></msub><mo id="S2.Ex2.m1.13.29.5" stretchy="false">)</mo></mrow><mo id="S2.Ex2.m1.13.30">−</mo><msub id="S2.Ex2.m1.13.31"><mtext id="S2.Ex2.m1.13.31.2">MAP</mtext><mrow id="S2.Ex2.m1.13.31.3"><mtext id="S2.Ex2.m1.13.31.3.2">f</mtext><mo id="S2.Ex2.m1.13.31.3.1" stretchy="false">→</mo><mtext id="S2.Ex2.m1.13.31.3.3">v</mtext></mrow></msub><mrow id="S2.Ex2.m1.13.32"><mo id="S2.Ex2.m1.13.32.1" stretchy="false">(</mo><msub id="S2.Ex2.m1.13.32.2"><mi id="S2.Ex2.m1.13.32.2.2">g</mi><mtext id="S2.Ex2.m1.13.32.2.3">f</mtext></msub><mrow id="S2.Ex2.m1.13.32.3"><mo id="S2.Ex2.m1.13.32.3.1" stretchy="false">(</mo><msub id="S2.Ex2.m1.13.32.3.2"><mi id="S2.Ex2.m1.13.32.3.2.2">𝐱</mi><mrow id="S2.Ex2.m1.13.13.2.4"><mtext id="S2.Ex2.m1.12.12.1.1">f</mtext><mo id="S2.Ex2.m1.13.13.2.4.1">,</mo><mi id="S2.Ex2.m1.13.13.2.2">i</mi></mrow></msub><mo fence="false" id="S2.Ex2.m1.13.32.3.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.Ex2.m1.13.32.3.4"><mi id="S2.Ex2.m1.13.32.3.4.2">ϕ</mi><mtext id="S2.Ex2.m1.13.32.3.4.3">f</mtext></msub><mo id="S2.Ex2.m1.13.32.3.5" stretchy="false">)</mo></mrow><msup id="S2.Ex2.m1.13.32.4"><mo id="S2.Ex2.m1.13.32.4.2" lspace="0em" rspace="0.167em">∥</mo><mn id="S2.Ex2.m1.13.32.4.3">2</mn></msup><mo id="S2.Ex2.m1.13.32.5">,</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.Ex2.m1.13c">\displaystyle\mathcal{L}_{\text{f}}=\frac{1}{|S_{\text{f}}|}\sum_{\mathbf{x}_{%
\text{f},i}\in S_{\text{f}}}||\mathbf{x}_{\text{f},i}-f_{\text{f}}(g_{\text{f}%
}(\mathbf{x}_{\text{f},i}|\phi_{\text{f}})|\theta_{\text{f}})||^{2}+\frac{1}{|%
S_{\text{f}}|}\sum_{\begin{subarray}{c}(\textbf{x}_{\text{v},i})\in S_{\text{v%
}}\\
(\textbf{x}_{\text{f},i})\in S_{\text{f}}\end{subarray}}\|g_{\text{v}}(\mathbf%
{x}_{\text{v},i}|\phi_{\text{v}})-\text{MAP}_{\text{f}\rightarrow\text{v}}(g_{%
\text{f}}(\mathbf{x}_{\text{f},i}|\phi_{\text{f}})\|^{2},</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m1.13d">caligraphic_L start_POSTSUBSCRIPT f end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG | italic_S start_POSTSUBSCRIPT f end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT f , italic_i end_POSTSUBSCRIPT ∈ italic_S start_POSTSUBSCRIPT f end_POSTSUBSCRIPT end_POSTSUBSCRIPT | | bold_x start_POSTSUBSCRIPT f , italic_i end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT f end_POSTSUBSCRIPT ( italic_g start_POSTSUBSCRIPT f end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT f , italic_i end_POSTSUBSCRIPT | italic_ϕ start_POSTSUBSCRIPT f end_POSTSUBSCRIPT ) | italic_θ start_POSTSUBSCRIPT f end_POSTSUBSCRIPT ) | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG | italic_S start_POSTSUBSCRIPT f end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT start_ARG start_ROW start_CELL ( x start_POSTSUBSCRIPT v , italic_i end_POSTSUBSCRIPT ) ∈ italic_S start_POSTSUBSCRIPT v end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ( x start_POSTSUBSCRIPT f , italic_i end_POSTSUBSCRIPT ) ∈ italic_S start_POSTSUBSCRIPT f end_POSTSUBSCRIPT end_CELL end_ROW end_ARG end_POSTSUBSCRIPT ∥ italic_g start_POSTSUBSCRIPT v end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT v , italic_i end_POSTSUBSCRIPT | italic_ϕ start_POSTSUBSCRIPT v end_POSTSUBSCRIPT ) - MAP start_POSTSUBSCRIPT f → v end_POSTSUBSCRIPT ( italic_g start_POSTSUBSCRIPT f end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT f , italic_i end_POSTSUBSCRIPT | italic_ϕ start_POSTSUBSCRIPT f end_POSTSUBSCRIPT ) ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p4.10">where <math alttext="\mathbf{x}_{\text{f},i}" class="ltx_Math" display="inline" id="S2.p4.4.m1.2"><semantics id="S2.p4.4.m1.2a"><msub id="S2.p4.4.m1.2.3" xref="S2.p4.4.m1.2.3.cmml"><mi id="S2.p4.4.m1.2.3.2" xref="S2.p4.4.m1.2.3.2.cmml">𝐱</mi><mrow id="S2.p4.4.m1.2.2.2.4" xref="S2.p4.4.m1.2.2.2.3.cmml"><mtext id="S2.p4.4.m1.1.1.1.1" xref="S2.p4.4.m1.1.1.1.1a.cmml">f</mtext><mo id="S2.p4.4.m1.2.2.2.4.1" xref="S2.p4.4.m1.2.2.2.3.cmml">,</mo><mi id="S2.p4.4.m1.2.2.2.2" xref="S2.p4.4.m1.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p4.4.m1.2b"><apply id="S2.p4.4.m1.2.3.cmml" xref="S2.p4.4.m1.2.3"><csymbol cd="ambiguous" id="S2.p4.4.m1.2.3.1.cmml" xref="S2.p4.4.m1.2.3">subscript</csymbol><ci id="S2.p4.4.m1.2.3.2.cmml" xref="S2.p4.4.m1.2.3.2">𝐱</ci><list id="S2.p4.4.m1.2.2.2.3.cmml" xref="S2.p4.4.m1.2.2.2.4"><ci id="S2.p4.4.m1.1.1.1.1a.cmml" xref="S2.p4.4.m1.1.1.1.1"><mtext id="S2.p4.4.m1.1.1.1.1.cmml" mathsize="70%" xref="S2.p4.4.m1.1.1.1.1">f</mtext></ci><ci id="S2.p4.4.m1.2.2.2.2.cmml" xref="S2.p4.4.m1.2.2.2.2">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.4.m1.2c">\mathbf{x}_{\text{f},i}</annotation><annotation encoding="application/x-llamapun" id="S2.p4.4.m1.2d">bold_x start_POSTSUBSCRIPT f , italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is a frame of an fMRI recording while the brain has been stimulated by video <math alttext="S_{\text{f}}" class="ltx_Math" display="inline" id="S2.p4.5.m2.1"><semantics id="S2.p4.5.m2.1a"><msub id="S2.p4.5.m2.1.1" xref="S2.p4.5.m2.1.1.cmml"><mi id="S2.p4.5.m2.1.1.2" xref="S2.p4.5.m2.1.1.2.cmml">S</mi><mtext id="S2.p4.5.m2.1.1.3" xref="S2.p4.5.m2.1.1.3a.cmml">f</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.5.m2.1b"><apply id="S2.p4.5.m2.1.1.cmml" xref="S2.p4.5.m2.1.1"><csymbol cd="ambiguous" id="S2.p4.5.m2.1.1.1.cmml" xref="S2.p4.5.m2.1.1">subscript</csymbol><ci id="S2.p4.5.m2.1.1.2.cmml" xref="S2.p4.5.m2.1.1.2">𝑆</ci><ci id="S2.p4.5.m2.1.1.3a.cmml" xref="S2.p4.5.m2.1.1.3"><mtext id="S2.p4.5.m2.1.1.3.cmml" mathsize="70%" xref="S2.p4.5.m2.1.1.3">f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.5.m2.1c">S_{\text{f}}</annotation><annotation encoding="application/x-llamapun" id="S2.p4.5.m2.1d">italic_S start_POSTSUBSCRIPT f end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="|S_{\text{f}}|=|S_{\text{v}}|" class="ltx_Math" display="inline" id="S2.p4.6.m3.2"><semantics id="S2.p4.6.m3.2a"><mrow id="S2.p4.6.m3.2.2" xref="S2.p4.6.m3.2.2.cmml"><mrow id="S2.p4.6.m3.1.1.1.1" xref="S2.p4.6.m3.1.1.1.2.cmml"><mo id="S2.p4.6.m3.1.1.1.1.2" stretchy="false" xref="S2.p4.6.m3.1.1.1.2.1.cmml">|</mo><msub id="S2.p4.6.m3.1.1.1.1.1" xref="S2.p4.6.m3.1.1.1.1.1.cmml"><mi id="S2.p4.6.m3.1.1.1.1.1.2" xref="S2.p4.6.m3.1.1.1.1.1.2.cmml">S</mi><mtext id="S2.p4.6.m3.1.1.1.1.1.3" xref="S2.p4.6.m3.1.1.1.1.1.3a.cmml">f</mtext></msub><mo id="S2.p4.6.m3.1.1.1.1.3" stretchy="false" xref="S2.p4.6.m3.1.1.1.2.1.cmml">|</mo></mrow><mo id="S2.p4.6.m3.2.2.3" xref="S2.p4.6.m3.2.2.3.cmml">=</mo><mrow id="S2.p4.6.m3.2.2.2.1" xref="S2.p4.6.m3.2.2.2.2.cmml"><mo id="S2.p4.6.m3.2.2.2.1.2" stretchy="false" xref="S2.p4.6.m3.2.2.2.2.1.cmml">|</mo><msub id="S2.p4.6.m3.2.2.2.1.1" xref="S2.p4.6.m3.2.2.2.1.1.cmml"><mi id="S2.p4.6.m3.2.2.2.1.1.2" xref="S2.p4.6.m3.2.2.2.1.1.2.cmml">S</mi><mtext id="S2.p4.6.m3.2.2.2.1.1.3" xref="S2.p4.6.m3.2.2.2.1.1.3a.cmml">v</mtext></msub><mo id="S2.p4.6.m3.2.2.2.1.3" stretchy="false" xref="S2.p4.6.m3.2.2.2.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.6.m3.2b"><apply id="S2.p4.6.m3.2.2.cmml" xref="S2.p4.6.m3.2.2"><eq id="S2.p4.6.m3.2.2.3.cmml" xref="S2.p4.6.m3.2.2.3"></eq><apply id="S2.p4.6.m3.1.1.1.2.cmml" xref="S2.p4.6.m3.1.1.1.1"><abs id="S2.p4.6.m3.1.1.1.2.1.cmml" xref="S2.p4.6.m3.1.1.1.1.2"></abs><apply id="S2.p4.6.m3.1.1.1.1.1.cmml" xref="S2.p4.6.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p4.6.m3.1.1.1.1.1.1.cmml" xref="S2.p4.6.m3.1.1.1.1.1">subscript</csymbol><ci id="S2.p4.6.m3.1.1.1.1.1.2.cmml" xref="S2.p4.6.m3.1.1.1.1.1.2">𝑆</ci><ci id="S2.p4.6.m3.1.1.1.1.1.3a.cmml" xref="S2.p4.6.m3.1.1.1.1.1.3"><mtext id="S2.p4.6.m3.1.1.1.1.1.3.cmml" mathsize="70%" xref="S2.p4.6.m3.1.1.1.1.1.3">f</mtext></ci></apply></apply><apply id="S2.p4.6.m3.2.2.2.2.cmml" xref="S2.p4.6.m3.2.2.2.1"><abs id="S2.p4.6.m3.2.2.2.2.1.cmml" xref="S2.p4.6.m3.2.2.2.1.2"></abs><apply id="S2.p4.6.m3.2.2.2.1.1.cmml" xref="S2.p4.6.m3.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.p4.6.m3.2.2.2.1.1.1.cmml" xref="S2.p4.6.m3.2.2.2.1.1">subscript</csymbol><ci id="S2.p4.6.m3.2.2.2.1.1.2.cmml" xref="S2.p4.6.m3.2.2.2.1.1.2">𝑆</ci><ci id="S2.p4.6.m3.2.2.2.1.1.3a.cmml" xref="S2.p4.6.m3.2.2.2.1.1.3"><mtext id="S2.p4.6.m3.2.2.2.1.1.3.cmml" mathsize="70%" xref="S2.p4.6.m3.2.2.2.1.1.3">v</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.6.m3.2c">|S_{\text{f}}|=|S_{\text{v}}|</annotation><annotation encoding="application/x-llamapun" id="S2.p4.6.m3.2d">| italic_S start_POSTSUBSCRIPT f end_POSTSUBSCRIPT | = | italic_S start_POSTSUBSCRIPT v end_POSTSUBSCRIPT |</annotation></semantics></math>. Beside, <math alttext="\theta_{\text{f}}" class="ltx_Math" display="inline" id="S2.p4.7.m4.1"><semantics id="S2.p4.7.m4.1a"><msub id="S2.p4.7.m4.1.1" xref="S2.p4.7.m4.1.1.cmml"><mi id="S2.p4.7.m4.1.1.2" xref="S2.p4.7.m4.1.1.2.cmml">θ</mi><mtext id="S2.p4.7.m4.1.1.3" xref="S2.p4.7.m4.1.1.3a.cmml">f</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.7.m4.1b"><apply id="S2.p4.7.m4.1.1.cmml" xref="S2.p4.7.m4.1.1"><csymbol cd="ambiguous" id="S2.p4.7.m4.1.1.1.cmml" xref="S2.p4.7.m4.1.1">subscript</csymbol><ci id="S2.p4.7.m4.1.1.2.cmml" xref="S2.p4.7.m4.1.1.2">𝜃</ci><ci id="S2.p4.7.m4.1.1.3a.cmml" xref="S2.p4.7.m4.1.1.3"><mtext id="S2.p4.7.m4.1.1.3.cmml" mathsize="70%" xref="S2.p4.7.m4.1.1.3">f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.7.m4.1c">\theta_{\text{f}}</annotation><annotation encoding="application/x-llamapun" id="S2.p4.7.m4.1d">italic_θ start_POSTSUBSCRIPT f end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\phi_{\text{f}}" class="ltx_Math" display="inline" id="S2.p4.8.m5.1"><semantics id="S2.p4.8.m5.1a"><msub id="S2.p4.8.m5.1.1" xref="S2.p4.8.m5.1.1.cmml"><mi id="S2.p4.8.m5.1.1.2" xref="S2.p4.8.m5.1.1.2.cmml">ϕ</mi><mtext id="S2.p4.8.m5.1.1.3" xref="S2.p4.8.m5.1.1.3a.cmml">f</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.8.m5.1b"><apply id="S2.p4.8.m5.1.1.cmml" xref="S2.p4.8.m5.1.1"><csymbol cd="ambiguous" id="S2.p4.8.m5.1.1.1.cmml" xref="S2.p4.8.m5.1.1">subscript</csymbol><ci id="S2.p4.8.m5.1.1.2.cmml" xref="S2.p4.8.m5.1.1.2">italic-ϕ</ci><ci id="S2.p4.8.m5.1.1.3a.cmml" xref="S2.p4.8.m5.1.1.3"><mtext id="S2.p4.8.m5.1.1.3.cmml" mathsize="70%" xref="S2.p4.8.m5.1.1.3">f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.8.m5.1c">\phi_{\text{f}}</annotation><annotation encoding="application/x-llamapun" id="S2.p4.8.m5.1d">italic_ϕ start_POSTSUBSCRIPT f end_POSTSUBSCRIPT</annotation></semantics></math> are the parameters of the <math alttext="f_{\text{f}}" class="ltx_Math" display="inline" id="S2.p4.9.m6.1"><semantics id="S2.p4.9.m6.1a"><msub id="S2.p4.9.m6.1.1" xref="S2.p4.9.m6.1.1.cmml"><mi id="S2.p4.9.m6.1.1.2" xref="S2.p4.9.m6.1.1.2.cmml">f</mi><mtext id="S2.p4.9.m6.1.1.3" xref="S2.p4.9.m6.1.1.3a.cmml">f</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.9.m6.1b"><apply id="S2.p4.9.m6.1.1.cmml" xref="S2.p4.9.m6.1.1"><csymbol cd="ambiguous" id="S2.p4.9.m6.1.1.1.cmml" xref="S2.p4.9.m6.1.1">subscript</csymbol><ci id="S2.p4.9.m6.1.1.2.cmml" xref="S2.p4.9.m6.1.1.2">𝑓</ci><ci id="S2.p4.9.m6.1.1.3a.cmml" xref="S2.p4.9.m6.1.1.3"><mtext id="S2.p4.9.m6.1.1.3.cmml" mathsize="70%" xref="S2.p4.9.m6.1.1.3">f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.9.m6.1c">f_{\text{f}}</annotation><annotation encoding="application/x-llamapun" id="S2.p4.9.m6.1d">italic_f start_POSTSUBSCRIPT f end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="g_{\text{f}}" class="ltx_Math" display="inline" id="S2.p4.10.m7.1"><semantics id="S2.p4.10.m7.1a"><msub id="S2.p4.10.m7.1.1" xref="S2.p4.10.m7.1.1.cmml"><mi id="S2.p4.10.m7.1.1.2" xref="S2.p4.10.m7.1.1.2.cmml">g</mi><mtext id="S2.p4.10.m7.1.1.3" xref="S2.p4.10.m7.1.1.3a.cmml">f</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.10.m7.1b"><apply id="S2.p4.10.m7.1.1.cmml" xref="S2.p4.10.m7.1.1"><csymbol cd="ambiguous" id="S2.p4.10.m7.1.1.1.cmml" xref="S2.p4.10.m7.1.1">subscript</csymbol><ci id="S2.p4.10.m7.1.1.2.cmml" xref="S2.p4.10.m7.1.1.2">𝑔</ci><ci id="S2.p4.10.m7.1.1.3a.cmml" xref="S2.p4.10.m7.1.1.3"><mtext id="S2.p4.10.m7.1.1.3.cmml" mathsize="70%" xref="S2.p4.10.m7.1.1.3">f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.10.m7.1c">g_{\text{f}}</annotation><annotation encoding="application/x-llamapun" id="S2.p4.10.m7.1d">italic_g start_POSTSUBSCRIPT f end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.5">After configuring the video-stimulated brain encoder, we proceed by freezing the operations of the first and second networks while focusing on training the third network. This network is specifically designed to align with text-based stimuli, such as textual content as presented in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S2.F1.sf2" title="In Figure 1 ‣ 2 Method ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_tag">1(b)</span></a>. However, given the complexity of creating a direct point-to-point alignment in constructing a textually stimulated brain model, we shift our approach to analyzing the overall distribution patterns of textual-stimulated brain recordings. These patterns are compared against the embedding space generated by the video-stimulated architecture
To achieve this, our next step is to train a text-prompted fMRI encoder, <math alttext="g_{\text{e}}(\cdot|\theta_{\text{e}})" class="ltx_math_unparsed" display="inline" id="S2.p5.1.m1.1"><semantics id="S2.p5.1.m1.1a"><mrow id="S2.p5.1.m1.1b"><msub id="S2.p5.1.m1.1.1"><mi id="S2.p5.1.m1.1.1.2">g</mi><mtext id="S2.p5.1.m1.1.1.3">e</mtext></msub><mrow id="S2.p5.1.m1.1.2"><mo id="S2.p5.1.m1.1.2.1" stretchy="false">(</mo><mo id="S2.p5.1.m1.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo fence="false" id="S2.p5.1.m1.1.2.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.p5.1.m1.1.2.4"><mi id="S2.p5.1.m1.1.2.4.2">θ</mi><mtext id="S2.p5.1.m1.1.2.4.3">e</mtext></msub><mo id="S2.p5.1.m1.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.p5.1.m1.1c">g_{\text{e}}(\cdot|\theta_{\text{e}})</annotation><annotation encoding="application/x-llamapun" id="S2.p5.1.m1.1d">italic_g start_POSTSUBSCRIPT e end_POSTSUBSCRIPT ( ⋅ | italic_θ start_POSTSUBSCRIPT e end_POSTSUBSCRIPT )</annotation></semantics></math>, and decoder, <math alttext="f_{\text{e}}(\cdot|\phi_{\text{e}})" class="ltx_math_unparsed" display="inline" id="S2.p5.2.m2.1"><semantics id="S2.p5.2.m2.1a"><mrow id="S2.p5.2.m2.1b"><msub id="S2.p5.2.m2.1.1"><mi id="S2.p5.2.m2.1.1.2">f</mi><mtext id="S2.p5.2.m2.1.1.3">e</mtext></msub><mrow id="S2.p5.2.m2.1.2"><mo id="S2.p5.2.m2.1.2.1" stretchy="false">(</mo><mo id="S2.p5.2.m2.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo fence="false" id="S2.p5.2.m2.1.2.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.p5.2.m2.1.2.4"><mi id="S2.p5.2.m2.1.2.4.2">ϕ</mi><mtext id="S2.p5.2.m2.1.2.4.3">e</mtext></msub><mo id="S2.p5.2.m2.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.p5.2.m2.1c">f_{\text{e}}(\cdot|\phi_{\text{e}})</annotation><annotation encoding="application/x-llamapun" id="S2.p5.2.m2.1d">italic_f start_POSTSUBSCRIPT e end_POSTSUBSCRIPT ( ⋅ | italic_ϕ start_POSTSUBSCRIPT e end_POSTSUBSCRIPT )</annotation></semantics></math>.
Thus, we aim to align the text-based distribution with that of the distribution derived from visually and textually stimulated brain recordings. Despite science videos or visual content possessing textual labels, we undertake the task of text-based distribution-to-distribution matching. Our strategy involves guiding the embeddings of textually stimulated brain recordings closer to those from visually stimulated recordings. Motivated by prototypical networks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib23" title="">23</a>]</cite>, we begin by calculating the prototype for the <math alttext="k" class="ltx_Math" display="inline" id="S2.p5.3.m3.1"><semantics id="S2.p5.3.m3.1a"><mi id="S2.p5.3.m3.1.1" xref="S2.p5.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p5.3.m3.1b"><ci id="S2.p5.3.m3.1.1.cmml" xref="S2.p5.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.p5.3.m3.1d">italic_k</annotation></semantics></math>-th class <math alttext="C" class="ltx_Math" display="inline" id="S2.p5.4.m4.1"><semantics id="S2.p5.4.m4.1a"><mi id="S2.p5.4.m4.1.1" xref="S2.p5.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.p5.4.m4.1b"><ci id="S2.p5.4.m4.1.1.cmml" xref="S2.p5.4.m4.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.4.m4.1c">C</annotation><annotation encoding="application/x-llamapun" id="S2.p5.4.m4.1d">italic_C</annotation></semantics></math> in the visually stimulated fMRI context, denoted by <math alttext="\mathbf{p}_{\text{v}}^{k}" class="ltx_Math" display="inline" id="S2.p5.5.m5.1"><semantics id="S2.p5.5.m5.1a"><msubsup id="S2.p5.5.m5.1.1" xref="S2.p5.5.m5.1.1.cmml"><mi id="S2.p5.5.m5.1.1.2.2" xref="S2.p5.5.m5.1.1.2.2.cmml">𝐩</mi><mtext id="S2.p5.5.m5.1.1.2.3" xref="S2.p5.5.m5.1.1.2.3a.cmml">v</mtext><mi id="S2.p5.5.m5.1.1.3" xref="S2.p5.5.m5.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.p5.5.m5.1b"><apply id="S2.p5.5.m5.1.1.cmml" xref="S2.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p5.5.m5.1.1.1.cmml" xref="S2.p5.5.m5.1.1">superscript</csymbol><apply id="S2.p5.5.m5.1.1.2.cmml" xref="S2.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p5.5.m5.1.1.2.1.cmml" xref="S2.p5.5.m5.1.1">subscript</csymbol><ci id="S2.p5.5.m5.1.1.2.2.cmml" xref="S2.p5.5.m5.1.1.2.2">𝐩</ci><ci id="S2.p5.5.m5.1.1.2.3a.cmml" xref="S2.p5.5.m5.1.1.2.3"><mtext id="S2.p5.5.m5.1.1.2.3.cmml" mathsize="70%" xref="S2.p5.5.m5.1.1.2.3">v</mtext></ci></apply><ci id="S2.p5.5.m5.1.1.3.cmml" xref="S2.p5.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.5.m5.1c">\mathbf{p}_{\text{v}}^{k}</annotation><annotation encoding="application/x-llamapun" id="S2.p5.5.m5.1d">bold_p start_POSTSUBSCRIPT v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S6.EGx3">
<tbody id="S2.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbf{p}_{\text{f}}^{k}" class="ltx_Math" display="inline" id="S2.Ex3.m1.1"><semantics id="S2.Ex3.m1.1a"><msubsup id="S2.Ex3.m1.1.1" xref="S2.Ex3.m1.1.1.cmml"><mi id="S2.Ex3.m1.1.1.2.2" xref="S2.Ex3.m1.1.1.2.2.cmml">𝐩</mi><mtext id="S2.Ex3.m1.1.1.2.3" xref="S2.Ex3.m1.1.1.2.3a.cmml">f</mtext><mi id="S2.Ex3.m1.1.1.3" xref="S2.Ex3.m1.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.Ex3.m1.1b"><apply id="S2.Ex3.m1.1.1.cmml" xref="S2.Ex3.m1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.cmml" xref="S2.Ex3.m1.1.1">superscript</csymbol><apply id="S2.Ex3.m1.1.1.2.cmml" xref="S2.Ex3.m1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.2.1.cmml" xref="S2.Ex3.m1.1.1">subscript</csymbol><ci id="S2.Ex3.m1.1.1.2.2.cmml" xref="S2.Ex3.m1.1.1.2.2">𝐩</ci><ci id="S2.Ex3.m1.1.1.2.3a.cmml" xref="S2.Ex3.m1.1.1.2.3"><mtext id="S2.Ex3.m1.1.1.2.3.cmml" mathsize="70%" xref="S2.Ex3.m1.1.1.2.3">f</mtext></ci></apply><ci id="S2.Ex3.m1.1.1.3.cmml" xref="S2.Ex3.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m1.1c">\displaystyle\mathbf{p}_{\text{f}}^{k}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex3.m1.1d">bold_p start_POSTSUBSCRIPT f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{|S^{k}_{\text{f}}|}\sum_{(\mathbf{x}_{\text{f}}^{i},{y}%
^{i})\in S^{k}_{\text{f}}}g_{\text{f}}(\mathbf{x}_{\text{f}}^{i}|\phi_{\text{f%
}})," class="ltx_Math" display="inline" id="S2.Ex3.m2.4"><semantics id="S2.Ex3.m2.4a"><mrow id="S2.Ex3.m2.4.4.1" xref="S2.Ex3.m2.4.4.1.1.cmml"><mrow id="S2.Ex3.m2.4.4.1.1" xref="S2.Ex3.m2.4.4.1.1.cmml"><mi id="S2.Ex3.m2.4.4.1.1.3" xref="S2.Ex3.m2.4.4.1.1.3.cmml"></mi><mo id="S2.Ex3.m2.4.4.1.1.2" xref="S2.Ex3.m2.4.4.1.1.2.cmml">=</mo><mrow id="S2.Ex3.m2.4.4.1.1.1" xref="S2.Ex3.m2.4.4.1.1.1.cmml"><mstyle displaystyle="true" id="S2.Ex3.m2.1.1" xref="S2.Ex3.m2.1.1.cmml"><mfrac id="S2.Ex3.m2.1.1a" xref="S2.Ex3.m2.1.1.cmml"><mn id="S2.Ex3.m2.1.1.3" xref="S2.Ex3.m2.1.1.3.cmml">1</mn><mrow id="S2.Ex3.m2.1.1.1.1" xref="S2.Ex3.m2.1.1.1.2.cmml"><mo id="S2.Ex3.m2.1.1.1.1.2" stretchy="false" xref="S2.Ex3.m2.1.1.1.2.1.cmml">|</mo><msubsup id="S2.Ex3.m2.1.1.1.1.1" xref="S2.Ex3.m2.1.1.1.1.1.cmml"><mi id="S2.Ex3.m2.1.1.1.1.1.2.2" xref="S2.Ex3.m2.1.1.1.1.1.2.2.cmml">S</mi><mtext id="S2.Ex3.m2.1.1.1.1.1.3" xref="S2.Ex3.m2.1.1.1.1.1.3a.cmml">f</mtext><mi id="S2.Ex3.m2.1.1.1.1.1.2.3" xref="S2.Ex3.m2.1.1.1.1.1.2.3.cmml">k</mi></msubsup><mo id="S2.Ex3.m2.1.1.1.1.3" stretchy="false" xref="S2.Ex3.m2.1.1.1.2.1.cmml">|</mo></mrow></mfrac></mstyle><mo id="S2.Ex3.m2.4.4.1.1.1.2" xref="S2.Ex3.m2.4.4.1.1.1.2.cmml">⁢</mo><mrow id="S2.Ex3.m2.4.4.1.1.1.1" xref="S2.Ex3.m2.4.4.1.1.1.1.cmml"><mstyle displaystyle="true" id="S2.Ex3.m2.4.4.1.1.1.1.2" xref="S2.Ex3.m2.4.4.1.1.1.1.2.cmml"><munder id="S2.Ex3.m2.4.4.1.1.1.1.2a" xref="S2.Ex3.m2.4.4.1.1.1.1.2.cmml"><mo id="S2.Ex3.m2.4.4.1.1.1.1.2.2" movablelimits="false" xref="S2.Ex3.m2.4.4.1.1.1.1.2.2.cmml">∑</mo><mrow id="S2.Ex3.m2.3.3.2" xref="S2.Ex3.m2.3.3.2.cmml"><mrow id="S2.Ex3.m2.3.3.2.2.2" xref="S2.Ex3.m2.3.3.2.2.3.cmml"><mo id="S2.Ex3.m2.3.3.2.2.2.3" stretchy="false" xref="S2.Ex3.m2.3.3.2.2.3.cmml">(</mo><msubsup id="S2.Ex3.m2.2.2.1.1.1.1" xref="S2.Ex3.m2.2.2.1.1.1.1.cmml"><mi id="S2.Ex3.m2.2.2.1.1.1.1.2.2" xref="S2.Ex3.m2.2.2.1.1.1.1.2.2.cmml">𝐱</mi><mtext id="S2.Ex3.m2.2.2.1.1.1.1.2.3" xref="S2.Ex3.m2.2.2.1.1.1.1.2.3a.cmml">f</mtext><mi id="S2.Ex3.m2.2.2.1.1.1.1.3" xref="S2.Ex3.m2.2.2.1.1.1.1.3.cmml">i</mi></msubsup><mo id="S2.Ex3.m2.3.3.2.2.2.4" xref="S2.Ex3.m2.3.3.2.2.3.cmml">,</mo><msup id="S2.Ex3.m2.3.3.2.2.2.2" xref="S2.Ex3.m2.3.3.2.2.2.2.cmml"><mi id="S2.Ex3.m2.3.3.2.2.2.2.2" xref="S2.Ex3.m2.3.3.2.2.2.2.2.cmml">y</mi><mi id="S2.Ex3.m2.3.3.2.2.2.2.3" xref="S2.Ex3.m2.3.3.2.2.2.2.3.cmml">i</mi></msup><mo id="S2.Ex3.m2.3.3.2.2.2.5" stretchy="false" xref="S2.Ex3.m2.3.3.2.2.3.cmml">)</mo></mrow><mo id="S2.Ex3.m2.3.3.2.3" xref="S2.Ex3.m2.3.3.2.3.cmml">∈</mo><msubsup id="S2.Ex3.m2.3.3.2.4" xref="S2.Ex3.m2.3.3.2.4.cmml"><mi id="S2.Ex3.m2.3.3.2.4.2.2" xref="S2.Ex3.m2.3.3.2.4.2.2.cmml">S</mi><mtext id="S2.Ex3.m2.3.3.2.4.3" xref="S2.Ex3.m2.3.3.2.4.3a.cmml">f</mtext><mi id="S2.Ex3.m2.3.3.2.4.2.3" xref="S2.Ex3.m2.3.3.2.4.2.3.cmml">k</mi></msubsup></mrow></munder></mstyle><mrow id="S2.Ex3.m2.4.4.1.1.1.1.1" xref="S2.Ex3.m2.4.4.1.1.1.1.1.cmml"><msub id="S2.Ex3.m2.4.4.1.1.1.1.1.3" xref="S2.Ex3.m2.4.4.1.1.1.1.1.3.cmml"><mi id="S2.Ex3.m2.4.4.1.1.1.1.1.3.2" xref="S2.Ex3.m2.4.4.1.1.1.1.1.3.2.cmml">g</mi><mtext id="S2.Ex3.m2.4.4.1.1.1.1.1.3.3" xref="S2.Ex3.m2.4.4.1.1.1.1.1.3.3a.cmml">f</mtext></msub><mo id="S2.Ex3.m2.4.4.1.1.1.1.1.2" xref="S2.Ex3.m2.4.4.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.cmml"><mo id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.cmml"><msubsup id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.2" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.2.cmml">𝐱</mi><mtext id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.3" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.3a.cmml">f</mtext><mi id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.3" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msubsup><mo fence="false" id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.1" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.2" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.2.cmml">ϕ</mi><mtext id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.3" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.3a.cmml">f</mtext></msub></mrow><mo id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.Ex3.m2.4.4.1.2" xref="S2.Ex3.m2.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex3.m2.4b"><apply id="S2.Ex3.m2.4.4.1.1.cmml" xref="S2.Ex3.m2.4.4.1"><eq id="S2.Ex3.m2.4.4.1.1.2.cmml" xref="S2.Ex3.m2.4.4.1.1.2"></eq><csymbol cd="latexml" id="S2.Ex3.m2.4.4.1.1.3.cmml" xref="S2.Ex3.m2.4.4.1.1.3">absent</csymbol><apply id="S2.Ex3.m2.4.4.1.1.1.cmml" xref="S2.Ex3.m2.4.4.1.1.1"><times id="S2.Ex3.m2.4.4.1.1.1.2.cmml" xref="S2.Ex3.m2.4.4.1.1.1.2"></times><apply id="S2.Ex3.m2.1.1.cmml" xref="S2.Ex3.m2.1.1"><divide id="S2.Ex3.m2.1.1.2.cmml" xref="S2.Ex3.m2.1.1"></divide><cn id="S2.Ex3.m2.1.1.3.cmml" type="integer" xref="S2.Ex3.m2.1.1.3">1</cn><apply id="S2.Ex3.m2.1.1.1.2.cmml" xref="S2.Ex3.m2.1.1.1.1"><abs id="S2.Ex3.m2.1.1.1.2.1.cmml" xref="S2.Ex3.m2.1.1.1.1.2"></abs><apply id="S2.Ex3.m2.1.1.1.1.1.cmml" xref="S2.Ex3.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m2.1.1.1.1.1.1.cmml" xref="S2.Ex3.m2.1.1.1.1.1">subscript</csymbol><apply id="S2.Ex3.m2.1.1.1.1.1.2.cmml" xref="S2.Ex3.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m2.1.1.1.1.1.2.1.cmml" xref="S2.Ex3.m2.1.1.1.1.1">superscript</csymbol><ci id="S2.Ex3.m2.1.1.1.1.1.2.2.cmml" xref="S2.Ex3.m2.1.1.1.1.1.2.2">𝑆</ci><ci id="S2.Ex3.m2.1.1.1.1.1.2.3.cmml" xref="S2.Ex3.m2.1.1.1.1.1.2.3">𝑘</ci></apply><ci id="S2.Ex3.m2.1.1.1.1.1.3a.cmml" xref="S2.Ex3.m2.1.1.1.1.1.3"><mtext id="S2.Ex3.m2.1.1.1.1.1.3.cmml" mathsize="70%" xref="S2.Ex3.m2.1.1.1.1.1.3">f</mtext></ci></apply></apply></apply><apply id="S2.Ex3.m2.4.4.1.1.1.1.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1"><apply id="S2.Ex3.m2.4.4.1.1.1.1.2.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex3.m2.4.4.1.1.1.1.2.1.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.2">subscript</csymbol><sum id="S2.Ex3.m2.4.4.1.1.1.1.2.2.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.2.2"></sum><apply id="S2.Ex3.m2.3.3.2.cmml" xref="S2.Ex3.m2.3.3.2"><in id="S2.Ex3.m2.3.3.2.3.cmml" xref="S2.Ex3.m2.3.3.2.3"></in><interval closure="open" id="S2.Ex3.m2.3.3.2.2.3.cmml" xref="S2.Ex3.m2.3.3.2.2.2"><apply id="S2.Ex3.m2.2.2.1.1.1.1.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m2.2.2.1.1.1.1.1.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1">superscript</csymbol><apply id="S2.Ex3.m2.2.2.1.1.1.1.2.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m2.2.2.1.1.1.1.2.1.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1">subscript</csymbol><ci id="S2.Ex3.m2.2.2.1.1.1.1.2.2.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1.2.2">𝐱</ci><ci id="S2.Ex3.m2.2.2.1.1.1.1.2.3a.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1.2.3"><mtext id="S2.Ex3.m2.2.2.1.1.1.1.2.3.cmml" mathsize="50%" xref="S2.Ex3.m2.2.2.1.1.1.1.2.3">f</mtext></ci></apply><ci id="S2.Ex3.m2.2.2.1.1.1.1.3.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1.3">𝑖</ci></apply><apply id="S2.Ex3.m2.3.3.2.2.2.2.cmml" xref="S2.Ex3.m2.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S2.Ex3.m2.3.3.2.2.2.2.1.cmml" xref="S2.Ex3.m2.3.3.2.2.2.2">superscript</csymbol><ci id="S2.Ex3.m2.3.3.2.2.2.2.2.cmml" xref="S2.Ex3.m2.3.3.2.2.2.2.2">𝑦</ci><ci id="S2.Ex3.m2.3.3.2.2.2.2.3.cmml" xref="S2.Ex3.m2.3.3.2.2.2.2.3">𝑖</ci></apply></interval><apply id="S2.Ex3.m2.3.3.2.4.cmml" xref="S2.Ex3.m2.3.3.2.4"><csymbol cd="ambiguous" id="S2.Ex3.m2.3.3.2.4.1.cmml" xref="S2.Ex3.m2.3.3.2.4">subscript</csymbol><apply id="S2.Ex3.m2.3.3.2.4.2.cmml" xref="S2.Ex3.m2.3.3.2.4"><csymbol cd="ambiguous" id="S2.Ex3.m2.3.3.2.4.2.1.cmml" xref="S2.Ex3.m2.3.3.2.4">superscript</csymbol><ci id="S2.Ex3.m2.3.3.2.4.2.2.cmml" xref="S2.Ex3.m2.3.3.2.4.2.2">𝑆</ci><ci id="S2.Ex3.m2.3.3.2.4.2.3.cmml" xref="S2.Ex3.m2.3.3.2.4.2.3">𝑘</ci></apply><ci id="S2.Ex3.m2.3.3.2.4.3a.cmml" xref="S2.Ex3.m2.3.3.2.4.3"><mtext id="S2.Ex3.m2.3.3.2.4.3.cmml" mathsize="50%" xref="S2.Ex3.m2.3.3.2.4.3">f</mtext></ci></apply></apply></apply><apply id="S2.Ex3.m2.4.4.1.1.1.1.1.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1"><times id="S2.Ex3.m2.4.4.1.1.1.1.1.2.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.2"></times><apply id="S2.Ex3.m2.4.4.1.1.1.1.1.3.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex3.m2.4.4.1.1.1.1.1.3.1.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex3.m2.4.4.1.1.1.1.1.3.2.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.3.2">𝑔</ci><ci id="S2.Ex3.m2.4.4.1.1.1.1.1.3.3a.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.3.3"><mtext id="S2.Ex3.m2.4.4.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.Ex3.m2.4.4.1.1.1.1.1.3.3">f</mtext></ci></apply><apply id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.2">𝐱</ci><ci id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.3a.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.3"><mtext id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.3.cmml" mathsize="70%" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.2.3">f</mtext></ci></apply><ci id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.2">italic-ϕ</ci><ci id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.3"><mtext id="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.Ex3.m2.4.4.1.1.1.1.1.1.1.1.3.3">f</mtext></ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m2.4c">\displaystyle=\frac{1}{|S^{k}_{\text{f}}|}\sum_{(\mathbf{x}_{\text{f}}^{i},{y}%
^{i})\in S^{k}_{\text{f}}}g_{\text{f}}(\mathbf{x}_{\text{f}}^{i}|\phi_{\text{f%
}}),</annotation><annotation encoding="application/x-llamapun" id="S2.Ex3.m2.4d">= divide start_ARG 1 end_ARG start_ARG | italic_S start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT f end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) ∈ italic_S start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT f end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT f end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_ϕ start_POSTSUBSCRIPT f end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p5.9">where <math alttext="\mathbf{p}_{\text{f}}^{k}" class="ltx_Math" display="inline" id="S2.p5.6.m1.1"><semantics id="S2.p5.6.m1.1a"><msubsup id="S2.p5.6.m1.1.1" xref="S2.p5.6.m1.1.1.cmml"><mi id="S2.p5.6.m1.1.1.2.2" xref="S2.p5.6.m1.1.1.2.2.cmml">𝐩</mi><mtext id="S2.p5.6.m1.1.1.2.3" xref="S2.p5.6.m1.1.1.2.3a.cmml">f</mtext><mi id="S2.p5.6.m1.1.1.3" xref="S2.p5.6.m1.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.p5.6.m1.1b"><apply id="S2.p5.6.m1.1.1.cmml" xref="S2.p5.6.m1.1.1"><csymbol cd="ambiguous" id="S2.p5.6.m1.1.1.1.cmml" xref="S2.p5.6.m1.1.1">superscript</csymbol><apply id="S2.p5.6.m1.1.1.2.cmml" xref="S2.p5.6.m1.1.1"><csymbol cd="ambiguous" id="S2.p5.6.m1.1.1.2.1.cmml" xref="S2.p5.6.m1.1.1">subscript</csymbol><ci id="S2.p5.6.m1.1.1.2.2.cmml" xref="S2.p5.6.m1.1.1.2.2">𝐩</ci><ci id="S2.p5.6.m1.1.1.2.3a.cmml" xref="S2.p5.6.m1.1.1.2.3"><mtext id="S2.p5.6.m1.1.1.2.3.cmml" mathsize="70%" xref="S2.p5.6.m1.1.1.2.3">f</mtext></ci></apply><ci id="S2.p5.6.m1.1.1.3.cmml" xref="S2.p5.6.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.6.m1.1c">\mathbf{p}_{\text{f}}^{k}</annotation><annotation encoding="application/x-llamapun" id="S2.p5.6.m1.1d">bold_p start_POSTSUBSCRIPT f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> represents the centroid of the fMRI recordings for subjects watching a video from the <math alttext="k" class="ltx_Math" display="inline" id="S2.p5.7.m2.1"><semantics id="S2.p5.7.m2.1a"><mi id="S2.p5.7.m2.1.1" xref="S2.p5.7.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p5.7.m2.1b"><ci id="S2.p5.7.m2.1.1.cmml" xref="S2.p5.7.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.7.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.p5.7.m2.1d">italic_k</annotation></semantics></math>-th class. We subsequently apply an MLP-based mapping function, <math alttext="\text{MAP}_{\text{e}\rightarrow\text{f}}(\cdot)=h(\cdot|W_{\text{h}})" class="ltx_math_unparsed" display="inline" id="S2.p5.8.m3.1"><semantics id="S2.p5.8.m3.1a"><mrow id="S2.p5.8.m3.1b"><msub id="S2.p5.8.m3.1.2"><mtext id="S2.p5.8.m3.1.2.2">MAP</mtext><mrow id="S2.p5.8.m3.1.2.3"><mtext id="S2.p5.8.m3.1.2.3.2">e</mtext><mo id="S2.p5.8.m3.1.2.3.1" stretchy="false">→</mo><mtext id="S2.p5.8.m3.1.2.3.3">f</mtext></mrow></msub><mrow id="S2.p5.8.m3.1.3"><mo id="S2.p5.8.m3.1.3.1" stretchy="false">(</mo><mo id="S2.p5.8.m3.1.1" lspace="0em" rspace="0em">⋅</mo><mo id="S2.p5.8.m3.1.3.2" stretchy="false">)</mo></mrow><mo id="S2.p5.8.m3.1.4">=</mo><mi id="S2.p5.8.m3.1.5">h</mi><mrow id="S2.p5.8.m3.1.6"><mo id="S2.p5.8.m3.1.6.1" stretchy="false">(</mo><mo id="S2.p5.8.m3.1.6.2" lspace="0em" rspace="0em">⋅</mo><mo fence="false" id="S2.p5.8.m3.1.6.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.p5.8.m3.1.6.4"><mi id="S2.p5.8.m3.1.6.4.2">W</mi><mtext id="S2.p5.8.m3.1.6.4.3">h</mtext></msub><mo id="S2.p5.8.m3.1.6.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.p5.8.m3.1c">\text{MAP}_{\text{e}\rightarrow\text{f}}(\cdot)=h(\cdot|W_{\text{h}})</annotation><annotation encoding="application/x-llamapun" id="S2.p5.8.m3.1d">MAP start_POSTSUBSCRIPT e → f end_POSTSUBSCRIPT ( ⋅ ) = italic_h ( ⋅ | italic_W start_POSTSUBSCRIPT h end_POSTSUBSCRIPT )</annotation></semantics></math>, to align textually stimulated fMRI recordings with visually stimulated fMRI recordings that share the same class label. To achieve this, we use a distance function <math alttext="d(\cdot,\cdot)" class="ltx_Math" display="inline" id="S2.p5.9.m4.2"><semantics id="S2.p5.9.m4.2a"><mrow id="S2.p5.9.m4.2.3" xref="S2.p5.9.m4.2.3.cmml"><mi id="S2.p5.9.m4.2.3.2" xref="S2.p5.9.m4.2.3.2.cmml">d</mi><mo id="S2.p5.9.m4.2.3.1" xref="S2.p5.9.m4.2.3.1.cmml">⁢</mo><mrow id="S2.p5.9.m4.2.3.3.2" xref="S2.p5.9.m4.2.3.3.1.cmml"><mo id="S2.p5.9.m4.2.3.3.2.1" stretchy="false" xref="S2.p5.9.m4.2.3.3.1.cmml">(</mo><mo id="S2.p5.9.m4.1.1" lspace="0em" rspace="0em" xref="S2.p5.9.m4.1.1.cmml">⋅</mo><mo id="S2.p5.9.m4.2.3.3.2.2" rspace="0em" xref="S2.p5.9.m4.2.3.3.1.cmml">,</mo><mo id="S2.p5.9.m4.2.2" lspace="0em" rspace="0em" xref="S2.p5.9.m4.2.2.cmml">⋅</mo><mo id="S2.p5.9.m4.2.3.3.2.3" stretchy="false" xref="S2.p5.9.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p5.9.m4.2b"><apply id="S2.p5.9.m4.2.3.cmml" xref="S2.p5.9.m4.2.3"><times id="S2.p5.9.m4.2.3.1.cmml" xref="S2.p5.9.m4.2.3.1"></times><ci id="S2.p5.9.m4.2.3.2.cmml" xref="S2.p5.9.m4.2.3.2">𝑑</ci><interval closure="open" id="S2.p5.9.m4.2.3.3.1.cmml" xref="S2.p5.9.m4.2.3.3.2"><ci id="S2.p5.9.m4.1.1.cmml" xref="S2.p5.9.m4.1.1">⋅</ci><ci id="S2.p5.9.m4.2.2.cmml" xref="S2.p5.9.m4.2.2">⋅</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.9.m4.2c">d(\cdot,\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.p5.9.m4.2d">italic_d ( ⋅ , ⋅ )</annotation></semantics></math>. Specifically, we define a cross-entropy loss function to perform textual-based distribution matching based on the following softmax function:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S6.EGx4">
<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P_{\phi}(y=k|\mathbf{p}_{\text{f}}^{k})=\frac{\exp\big{(}-d\big{%
(}\text{MAP}_{\text{e}\rightarrow\text{f}}(g_{\text{e}}(\mathbf{x}^{i}_{\text{%
e}}|\phi_{\text{e}})),\mathbf{p}_{\text{f}}^{k}\big{)}\big{)}}{\sum_{k^{\prime%
}}\exp\big{(}-d\big{(}\text{MAP}_{\text{e}\rightarrow\text{f}}(g_{\text{e}}(%
\mathbf{x}^{i}_{\text{e}}|\phi_{\text{e}})),\mathbf{p}_{\text{f}}^{k^{\prime}}%
\big{)}\big{)}}," class="ltx_Math" display="inline" id="S2.E1.m1.5"><semantics id="S2.E1.m1.5a"><mrow id="S2.E1.m1.5.5.1" xref="S2.E1.m1.5.5.1.1.cmml"><mrow id="S2.E1.m1.5.5.1.1" xref="S2.E1.m1.5.5.1.1.cmml"><mrow id="S2.E1.m1.5.5.1.1.1" xref="S2.E1.m1.5.5.1.1.1.cmml"><msub id="S2.E1.m1.5.5.1.1.1.3" xref="S2.E1.m1.5.5.1.1.1.3.cmml"><mi id="S2.E1.m1.5.5.1.1.1.3.2" xref="S2.E1.m1.5.5.1.1.1.3.2.cmml">P</mi><mi id="S2.E1.m1.5.5.1.1.1.3.3" xref="S2.E1.m1.5.5.1.1.1.3.3.cmml">ϕ</mi></msub><mo id="S2.E1.m1.5.5.1.1.1.2" xref="S2.E1.m1.5.5.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.5.5.1.1.1.1.1" xref="S2.E1.m1.5.5.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.5.5.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.5.5.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.5.5.1.1.1.1.1.1" xref="S2.E1.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.5.5.1.1.1.1.1.1.2" xref="S2.E1.m1.5.5.1.1.1.1.1.1.2.cmml">y</mi><mo id="S2.E1.m1.5.5.1.1.1.1.1.1.1" xref="S2.E1.m1.5.5.1.1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.5.5.1.1.1.1.1.1.3" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.5.5.1.1.1.1.1.1.3.2" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.2.cmml">k</mi><mo fence="false" id="S2.E1.m1.5.5.1.1.1.1.1.1.3.1" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.1.cmml">|</mo><msubsup id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.2" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.2.cmml">𝐩</mi><mtext id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.3" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.3a.cmml">f</mtext><mi id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.3.cmml">k</mi></msubsup></mrow></mrow><mo id="S2.E1.m1.5.5.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.5.5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.5.5.1.1.2" xref="S2.E1.m1.5.5.1.1.2.cmml">=</mo><mstyle displaystyle="true" id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml"><mfrac id="S2.E1.m1.4.4a" xref="S2.E1.m1.4.4.cmml"><mrow id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">exp</mi><mo id="S2.E1.m1.2.2.2.2a" xref="S2.E1.m1.2.2.2.3.cmml">⁡</mo><mrow id="S2.E1.m1.2.2.2.2.1" xref="S2.E1.m1.2.2.2.3.cmml"><mo id="S2.E1.m1.2.2.2.2.1.2" maxsize="120%" minsize="120%" xref="S2.E1.m1.2.2.2.3.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.2.1.1" xref="S2.E1.m1.2.2.2.2.1.1.cmml"><mo id="S2.E1.m1.2.2.2.2.1.1a" xref="S2.E1.m1.2.2.2.2.1.1.cmml">−</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.2.4" xref="S2.E1.m1.2.2.2.2.1.1.2.4.cmml">d</mi><mo id="S2.E1.m1.2.2.2.2.1.1.2.3" xref="S2.E1.m1.2.2.2.2.1.1.2.3.cmml">⁢</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.2.2.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.3.cmml"><mo id="S2.E1.m1.2.2.2.2.1.1.2.2.2.3" maxsize="120%" minsize="120%" xref="S2.E1.m1.2.2.2.2.1.1.2.2.3.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.cmml"><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.2" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.2a.cmml">MAP</mtext><mrow id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.cmml"><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.2" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.2a.cmml">e</mtext><mo id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.1" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.1.cmml">→</mo><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.3a.cmml">f</mtext></mrow></msub><mo id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml">g</mi><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.3a.cmml">e</mtext></msub><mo id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝐱</mi><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml">e</mtext><mi id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msubsup><mo fence="false" id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">ϕ</mi><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3a.cmml">e</mtext></msub></mrow><mo id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.2.2.1.1.2.2.2.4" xref="S2.E1.m1.2.2.2.2.1.1.2.2.3.cmml">,</mo><msubsup id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.2.cmml">𝐩</mi><mtext id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.3a.cmml">f</mtext><mi id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.3.cmml">k</mi></msubsup><mo id="S2.E1.m1.2.2.2.2.1.1.2.2.2.5" maxsize="120%" minsize="120%" xref="S2.E1.m1.2.2.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.2.2.2.2.1.3" maxsize="120%" minsize="120%" xref="S2.E1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S2.E1.m1.4.4.4" xref="S2.E1.m1.4.4.4.cmml"><msub id="S2.E1.m1.4.4.4.3" xref="S2.E1.m1.4.4.4.3.cmml"><mo id="S2.E1.m1.4.4.4.3.2" xref="S2.E1.m1.4.4.4.3.2.cmml">∑</mo><msup id="S2.E1.m1.4.4.4.3.3" xref="S2.E1.m1.4.4.4.3.3.cmml"><mi id="S2.E1.m1.4.4.4.3.3.2" xref="S2.E1.m1.4.4.4.3.3.2.cmml">k</mi><mo id="S2.E1.m1.4.4.4.3.3.3" xref="S2.E1.m1.4.4.4.3.3.3.cmml">′</mo></msup></msub><mrow id="S2.E1.m1.4.4.4.2.1" xref="S2.E1.m1.4.4.4.2.2.cmml"><mi id="S2.E1.m1.3.3.3.1" xref="S2.E1.m1.3.3.3.1.cmml">exp</mi><mo id="S2.E1.m1.4.4.4.2.1a" xref="S2.E1.m1.4.4.4.2.2.cmml">⁡</mo><mrow id="S2.E1.m1.4.4.4.2.1.1" xref="S2.E1.m1.4.4.4.2.2.cmml"><mo id="S2.E1.m1.4.4.4.2.1.1.2" maxsize="120%" minsize="120%" xref="S2.E1.m1.4.4.4.2.2.cmml">(</mo><mrow id="S2.E1.m1.4.4.4.2.1.1.1" xref="S2.E1.m1.4.4.4.2.1.1.1.cmml"><mo id="S2.E1.m1.4.4.4.2.1.1.1a" xref="S2.E1.m1.4.4.4.2.1.1.1.cmml">−</mo><mrow id="S2.E1.m1.4.4.4.2.1.1.1.2" xref="S2.E1.m1.4.4.4.2.1.1.1.2.cmml"><mi id="S2.E1.m1.4.4.4.2.1.1.1.2.4" xref="S2.E1.m1.4.4.4.2.1.1.1.2.4.cmml">d</mi><mo id="S2.E1.m1.4.4.4.2.1.1.1.2.3" xref="S2.E1.m1.4.4.4.2.1.1.1.2.3.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.3.cmml"><mo id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.3" maxsize="120%" minsize="120%" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.3.cmml">(</mo><mrow id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.cmml"><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.2a.cmml">MAP</mtext><mrow id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.cmml"><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.2" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.2a.cmml">e</mtext><mo id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.1" stretchy="false" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.1.cmml">→</mo><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.3a.cmml">f</mtext></mrow></msub><mo id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml">g</mi><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.3a.cmml">e</mtext></msub><mo id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝐱</mi><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml">e</mtext><mi id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msubsup><mo fence="false" id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">ϕ</mi><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3a.cmml">e</mtext></msub></mrow><mo id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.4" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.3.cmml">,</mo><msubsup id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.cmml"><mi id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.2" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.2.cmml">𝐩</mi><mtext id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.3" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.3a.cmml">f</mtext><msup id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.cmml"><mi id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.2" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.2.cmml">k</mi><mo id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.3" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.3.cmml">′</mo></msup></msubsup><mo id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.5" maxsize="120%" minsize="120%" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.4.4.4.2.1.1.3" maxsize="120%" minsize="120%" xref="S2.E1.m1.4.4.4.2.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo id="S2.E1.m1.5.5.1.2" xref="S2.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.5b"><apply id="S2.E1.m1.5.5.1.1.cmml" xref="S2.E1.m1.5.5.1"><eq id="S2.E1.m1.5.5.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.2"></eq><apply id="S2.E1.m1.5.5.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1"><times id="S2.E1.m1.5.5.1.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.2"></times><apply id="S2.E1.m1.5.5.1.1.1.3.cmml" xref="S2.E1.m1.5.5.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.1.1.1.3.1.cmml" xref="S2.E1.m1.5.5.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.5.5.1.1.1.3.2.cmml" xref="S2.E1.m1.5.5.1.1.1.3.2">𝑃</ci><ci id="S2.E1.m1.5.5.1.1.1.3.3.cmml" xref="S2.E1.m1.5.5.1.1.1.3.3">italic-ϕ</ci></apply><apply id="S2.E1.m1.5.5.1.1.1.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1"><eq id="S2.E1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.1"></eq><ci id="S2.E1.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.2">𝑦</ci><apply id="S2.E1.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3"><csymbol cd="latexml" id="S2.E1.m1.5.5.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.1">conditional</csymbol><ci id="S2.E1.m1.5.5.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.2">𝑘</ci><apply id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3">superscript</csymbol><apply id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.2">𝐩</ci><ci id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.3a.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.3"><mtext id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.3.cmml" mathsize="70%" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.2.3">f</mtext></ci></apply><ci id="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.3.3">𝑘</ci></apply></apply></apply></apply><apply id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4"><divide id="S2.E1.m1.4.4.5.cmml" xref="S2.E1.m1.4.4"></divide><apply id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2"><exp id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1"></exp><apply id="S2.E1.m1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1"><minus id="S2.E1.m1.2.2.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1"></minus><apply id="S2.E1.m1.2.2.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2"><times id="S2.E1.m1.2.2.2.2.1.1.2.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.3"></times><ci id="S2.E1.m1.2.2.2.2.1.1.2.4.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.4">𝑑</ci><interval closure="open" id="S2.E1.m1.2.2.2.2.1.1.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2"><apply id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1"><times id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.2a.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.2"><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.2">MAP</mtext></ci><apply id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3"><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.1">→</ci><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.2a.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.2"><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.2.cmml" mathsize="70%" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.2">e</mtext></ci><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.3a.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.3"><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.3.cmml" mathsize="70%" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.3.3">f</mtext></ci></apply></apply><apply id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1"><times id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.2">𝑔</ci><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.3"><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.3.3">e</mtext></ci></apply><apply id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝐱</ci><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3">e</mtext></ci></apply><apply id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2">italic-ϕ</ci><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3"><mtext id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3">e</mtext></ci></apply></apply></apply></apply><apply id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2">superscript</csymbol><apply id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.2">𝐩</ci><ci id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.3a.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.3"><mtext id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.3.cmml" mathsize="70%" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.3">f</mtext></ci></apply><ci id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.3">𝑘</ci></apply></interval></apply></apply></apply><apply id="S2.E1.m1.4.4.4.cmml" xref="S2.E1.m1.4.4.4"><apply id="S2.E1.m1.4.4.4.3.cmml" xref="S2.E1.m1.4.4.4.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.3.1.cmml" xref="S2.E1.m1.4.4.4.3">subscript</csymbol><sum id="S2.E1.m1.4.4.4.3.2.cmml" xref="S2.E1.m1.4.4.4.3.2"></sum><apply id="S2.E1.m1.4.4.4.3.3.cmml" xref="S2.E1.m1.4.4.4.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.3.3.1.cmml" xref="S2.E1.m1.4.4.4.3.3">superscript</csymbol><ci id="S2.E1.m1.4.4.4.3.3.2.cmml" xref="S2.E1.m1.4.4.4.3.3.2">𝑘</ci><ci id="S2.E1.m1.4.4.4.3.3.3.cmml" xref="S2.E1.m1.4.4.4.3.3.3">′</ci></apply></apply><apply id="S2.E1.m1.4.4.4.2.2.cmml" xref="S2.E1.m1.4.4.4.2.1"><exp id="S2.E1.m1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3.1"></exp><apply id="S2.E1.m1.4.4.4.2.1.1.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1"><minus id="S2.E1.m1.4.4.4.2.1.1.1.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1"></minus><apply id="S2.E1.m1.4.4.4.2.1.1.1.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2"><times id="S2.E1.m1.4.4.4.2.1.1.1.2.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.3"></times><ci id="S2.E1.m1.4.4.4.2.1.1.1.2.4.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.4">𝑑</ci><interval closure="open" id="S2.E1.m1.4.4.4.2.1.1.1.2.2.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2"><apply id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1"><times id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.2a.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.2"><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.2">MAP</mtext></ci><apply id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3"><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.1">→</ci><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.2a.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.2"><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.2.cmml" mathsize="70%" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.2">e</mtext></ci><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.3a.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.3"><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.3.cmml" mathsize="70%" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.3">f</mtext></ci></apply></apply><apply id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1"><times id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.2">𝑔</ci><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.3"><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.3.3">e</mtext></ci></apply><apply id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝐱</ci><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">e</mtext></ci></apply><apply id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">italic-ϕ</ci><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3"><mtext id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">e</mtext></ci></apply></apply></apply></apply><apply id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2">superscript</csymbol><apply id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.2">𝐩</ci><ci id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.3a.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.3"><mtext id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.3.cmml" mathsize="70%" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.3">f</mtext></ci></apply><apply id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3">superscript</csymbol><ci id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.2">𝑘</ci><ci id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.3">′</ci></apply></apply></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.5c">\displaystyle P_{\phi}(y=k|\mathbf{p}_{\text{f}}^{k})=\frac{\exp\big{(}-d\big{%
(}\text{MAP}_{\text{e}\rightarrow\text{f}}(g_{\text{e}}(\mathbf{x}^{i}_{\text{%
e}}|\phi_{\text{e}})),\mathbf{p}_{\text{f}}^{k}\big{)}\big{)}}{\sum_{k^{\prime%
}}\exp\big{(}-d\big{(}\text{MAP}_{\text{e}\rightarrow\text{f}}(g_{\text{e}}(%
\mathbf{x}^{i}_{\text{e}}|\phi_{\text{e}})),\mathbf{p}_{\text{f}}^{k^{\prime}}%
\big{)}\big{)}},</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.5d">italic_P start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_y = italic_k | bold_p start_POSTSUBSCRIPT f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) = divide start_ARG roman_exp ( - italic_d ( MAP start_POSTSUBSCRIPT e → f end_POSTSUBSCRIPT ( italic_g start_POSTSUBSCRIPT e end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT e end_POSTSUBSCRIPT | italic_ϕ start_POSTSUBSCRIPT e end_POSTSUBSCRIPT ) ) , bold_p start_POSTSUBSCRIPT f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_exp ( - italic_d ( MAP start_POSTSUBSCRIPT e → f end_POSTSUBSCRIPT ( italic_g start_POSTSUBSCRIPT e end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT e end_POSTSUBSCRIPT | italic_ϕ start_POSTSUBSCRIPT e end_POSTSUBSCRIPT ) ) , bold_p start_POSTSUBSCRIPT f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ) ) end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p5.11">Moreover, we employed a mean-squared loss function for representation learning of the textually stimulated brain recordings by both the encoder and decoder. The learning process progresses by minimizing the MSE loss function and or negative log-probability for distribution matching <math alttext="\mathcal{L}_{\text{matching}}=-\log P_{\phi}(y=k|\mathbf{x})" class="ltx_Math" display="inline" id="S2.p5.10.m1.1"><semantics id="S2.p5.10.m1.1a"><mrow id="S2.p5.10.m1.1.1" xref="S2.p5.10.m1.1.1.cmml"><msub id="S2.p5.10.m1.1.1.3" xref="S2.p5.10.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p5.10.m1.1.1.3.2" xref="S2.p5.10.m1.1.1.3.2.cmml">ℒ</mi><mtext id="S2.p5.10.m1.1.1.3.3" xref="S2.p5.10.m1.1.1.3.3a.cmml">matching</mtext></msub><mo id="S2.p5.10.m1.1.1.2" xref="S2.p5.10.m1.1.1.2.cmml">=</mo><mrow id="S2.p5.10.m1.1.1.1" xref="S2.p5.10.m1.1.1.1.cmml"><mo id="S2.p5.10.m1.1.1.1a" rspace="0.167em" xref="S2.p5.10.m1.1.1.1.cmml">−</mo><mrow id="S2.p5.10.m1.1.1.1.1" xref="S2.p5.10.m1.1.1.1.1.cmml"><mrow id="S2.p5.10.m1.1.1.1.1.3" xref="S2.p5.10.m1.1.1.1.1.3.cmml"><mi id="S2.p5.10.m1.1.1.1.1.3.1" xref="S2.p5.10.m1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.p5.10.m1.1.1.1.1.3a" lspace="0.167em" xref="S2.p5.10.m1.1.1.1.1.3.cmml">⁡</mo><msub id="S2.p5.10.m1.1.1.1.1.3.2" xref="S2.p5.10.m1.1.1.1.1.3.2.cmml"><mi id="S2.p5.10.m1.1.1.1.1.3.2.2" xref="S2.p5.10.m1.1.1.1.1.3.2.2.cmml">P</mi><mi id="S2.p5.10.m1.1.1.1.1.3.2.3" xref="S2.p5.10.m1.1.1.1.1.3.2.3.cmml">ϕ</mi></msub></mrow><mo id="S2.p5.10.m1.1.1.1.1.2" xref="S2.p5.10.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.p5.10.m1.1.1.1.1.1.1" xref="S2.p5.10.m1.1.1.1.1.1.1.1.cmml"><mo id="S2.p5.10.m1.1.1.1.1.1.1.2" stretchy="false" xref="S2.p5.10.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.p5.10.m1.1.1.1.1.1.1.1" xref="S2.p5.10.m1.1.1.1.1.1.1.1.cmml"><mi id="S2.p5.10.m1.1.1.1.1.1.1.1.2" xref="S2.p5.10.m1.1.1.1.1.1.1.1.2.cmml">y</mi><mo id="S2.p5.10.m1.1.1.1.1.1.1.1.1" xref="S2.p5.10.m1.1.1.1.1.1.1.1.1.cmml">=</mo><mrow id="S2.p5.10.m1.1.1.1.1.1.1.1.3" xref="S2.p5.10.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.p5.10.m1.1.1.1.1.1.1.1.3.2" xref="S2.p5.10.m1.1.1.1.1.1.1.1.3.2.cmml">k</mi><mo fence="false" id="S2.p5.10.m1.1.1.1.1.1.1.1.3.1" xref="S2.p5.10.m1.1.1.1.1.1.1.1.3.1.cmml">|</mo><mi id="S2.p5.10.m1.1.1.1.1.1.1.1.3.3" xref="S2.p5.10.m1.1.1.1.1.1.1.1.3.3.cmml">𝐱</mi></mrow></mrow><mo id="S2.p5.10.m1.1.1.1.1.1.1.3" stretchy="false" xref="S2.p5.10.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p5.10.m1.1b"><apply id="S2.p5.10.m1.1.1.cmml" xref="S2.p5.10.m1.1.1"><eq id="S2.p5.10.m1.1.1.2.cmml" xref="S2.p5.10.m1.1.1.2"></eq><apply id="S2.p5.10.m1.1.1.3.cmml" xref="S2.p5.10.m1.1.1.3"><csymbol cd="ambiguous" id="S2.p5.10.m1.1.1.3.1.cmml" xref="S2.p5.10.m1.1.1.3">subscript</csymbol><ci id="S2.p5.10.m1.1.1.3.2.cmml" xref="S2.p5.10.m1.1.1.3.2">ℒ</ci><ci id="S2.p5.10.m1.1.1.3.3a.cmml" xref="S2.p5.10.m1.1.1.3.3"><mtext id="S2.p5.10.m1.1.1.3.3.cmml" mathsize="70%" xref="S2.p5.10.m1.1.1.3.3">matching</mtext></ci></apply><apply id="S2.p5.10.m1.1.1.1.cmml" xref="S2.p5.10.m1.1.1.1"><minus id="S2.p5.10.m1.1.1.1.2.cmml" xref="S2.p5.10.m1.1.1.1"></minus><apply id="S2.p5.10.m1.1.1.1.1.cmml" xref="S2.p5.10.m1.1.1.1.1"><times id="S2.p5.10.m1.1.1.1.1.2.cmml" xref="S2.p5.10.m1.1.1.1.1.2"></times><apply id="S2.p5.10.m1.1.1.1.1.3.cmml" xref="S2.p5.10.m1.1.1.1.1.3"><log id="S2.p5.10.m1.1.1.1.1.3.1.cmml" xref="S2.p5.10.m1.1.1.1.1.3.1"></log><apply id="S2.p5.10.m1.1.1.1.1.3.2.cmml" xref="S2.p5.10.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.p5.10.m1.1.1.1.1.3.2.1.cmml" xref="S2.p5.10.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.p5.10.m1.1.1.1.1.3.2.2.cmml" xref="S2.p5.10.m1.1.1.1.1.3.2.2">𝑃</ci><ci id="S2.p5.10.m1.1.1.1.1.3.2.3.cmml" xref="S2.p5.10.m1.1.1.1.1.3.2.3">italic-ϕ</ci></apply></apply><apply id="S2.p5.10.m1.1.1.1.1.1.1.1.cmml" xref="S2.p5.10.m1.1.1.1.1.1.1"><eq id="S2.p5.10.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.p5.10.m1.1.1.1.1.1.1.1.1"></eq><ci id="S2.p5.10.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.p5.10.m1.1.1.1.1.1.1.1.2">𝑦</ci><apply id="S2.p5.10.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.p5.10.m1.1.1.1.1.1.1.1.3"><csymbol cd="latexml" id="S2.p5.10.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.p5.10.m1.1.1.1.1.1.1.1.3.1">conditional</csymbol><ci id="S2.p5.10.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.p5.10.m1.1.1.1.1.1.1.1.3.2">𝑘</ci><ci id="S2.p5.10.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.p5.10.m1.1.1.1.1.1.1.1.3.3">𝐱</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.10.m1.1c">\mathcal{L}_{\text{matching}}=-\log P_{\phi}(y=k|\mathbf{x})</annotation><annotation encoding="application/x-llamapun" id="S2.p5.10.m1.1d">caligraphic_L start_POSTSUBSCRIPT matching end_POSTSUBSCRIPT = - roman_log italic_P start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_y = italic_k | bold_x )</annotation></semantics></math> of the correct class <math alttext="k" class="ltx_Math" display="inline" id="S2.p5.11.m2.1"><semantics id="S2.p5.11.m2.1a"><mi id="S2.p5.11.m2.1.1" xref="S2.p5.11.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p5.11.m2.1b"><ci id="S2.p5.11.m2.1.1.cmml" xref="S2.p5.11.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.11.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.p5.11.m2.1d">italic_k</annotation></semantics></math> through Stochastic Gradient Descent (SGD).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S6.EGx5">
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{e}}=\frac{1}{|S_{\text{e}}|}\sum_{\mathbf{x}_{%
\text{e},t}\in S_{\text{e}}}||\mathbf{x}_{\text{e},t}-f_{\text{e}}(g_{\text{e}%
}(\mathbf{x}_{\text{e},t}|\phi_{\text{e}})|\theta_{\text{e}})||^{2}+\mathcal{L%
}_{\text{matching}}," class="ltx_math_unparsed" display="inline" id="S2.E2.m1.7"><semantics id="S2.E2.m1.7a"><mrow id="S2.E2.m1.7b"><msub id="S2.E2.m1.7.8"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.7.8.2">ℒ</mi><mtext id="S2.E2.m1.7.8.3">e</mtext></msub><mo id="S2.E2.m1.7.9">=</mo><mstyle displaystyle="true" id="S2.E2.m1.1.1"><mfrac id="S2.E2.m1.1.1a"><mn id="S2.E2.m1.1.1.3">1</mn><mrow id="S2.E2.m1.1.1.1.1"><mo id="S2.E2.m1.1.1.1.1.2" stretchy="false">|</mo><msub id="S2.E2.m1.1.1.1.1.1"><mi id="S2.E2.m1.1.1.1.1.1.2">S</mi><mtext id="S2.E2.m1.1.1.1.1.1.3">e</mtext></msub><mo id="S2.E2.m1.1.1.1.1.3" stretchy="false">|</mo></mrow></mfrac></mstyle><mstyle displaystyle="true" id="S2.E2.m1.7.10"><munder id="S2.E2.m1.7.10a"><mo id="S2.E2.m1.7.10.2" movablelimits="false">∑</mo><mrow id="S2.E2.m1.3.3.2"><msub id="S2.E2.m1.3.3.2.4"><mi id="S2.E2.m1.3.3.2.4.2">𝐱</mi><mrow id="S2.E2.m1.3.3.2.2.2.4"><mtext id="S2.E2.m1.2.2.1.1.1.1">e</mtext><mo id="S2.E2.m1.3.3.2.2.2.4.1">,</mo><mi id="S2.E2.m1.3.3.2.2.2.2">t</mi></mrow></msub><mo id="S2.E2.m1.3.3.2.3">∈</mo><msub id="S2.E2.m1.3.3.2.5"><mi id="S2.E2.m1.3.3.2.5.2">S</mi><mtext id="S2.E2.m1.3.3.2.5.3">e</mtext></msub></mrow></munder></mstyle><mo fence="false" id="S2.E2.m1.7.11" rspace="0.167em" stretchy="false">|</mo><mo fence="false" id="S2.E2.m1.7.12" rspace="0.167em" stretchy="false">|</mo><msub id="S2.E2.m1.7.13"><mi id="S2.E2.m1.7.13.2">𝐱</mi><mrow id="S2.E2.m1.5.5.2.4"><mtext id="S2.E2.m1.4.4.1.1">e</mtext><mo id="S2.E2.m1.5.5.2.4.1">,</mo><mi id="S2.E2.m1.5.5.2.2">t</mi></mrow></msub><mo id="S2.E2.m1.7.14">−</mo><msub id="S2.E2.m1.7.15"><mi id="S2.E2.m1.7.15.2">f</mi><mtext id="S2.E2.m1.7.15.3">e</mtext></msub><mrow id="S2.E2.m1.7.16"><mo id="S2.E2.m1.7.16.1" stretchy="false">(</mo><msub id="S2.E2.m1.7.16.2"><mi id="S2.E2.m1.7.16.2.2">g</mi><mtext id="S2.E2.m1.7.16.2.3">e</mtext></msub><mrow id="S2.E2.m1.7.16.3"><mo id="S2.E2.m1.7.16.3.1" stretchy="false">(</mo><msub id="S2.E2.m1.7.16.3.2"><mi id="S2.E2.m1.7.16.3.2.2">𝐱</mi><mrow id="S2.E2.m1.7.7.2.4"><mtext id="S2.E2.m1.6.6.1.1">e</mtext><mo id="S2.E2.m1.7.7.2.4.1">,</mo><mi id="S2.E2.m1.7.7.2.2">t</mi></mrow></msub><mo fence="false" id="S2.E2.m1.7.16.3.3" rspace="0.167em" stretchy="false">|</mo><msub id="S2.E2.m1.7.16.3.4"><mi id="S2.E2.m1.7.16.3.4.2">ϕ</mi><mtext id="S2.E2.m1.7.16.3.4.3">e</mtext></msub><mo id="S2.E2.m1.7.16.3.5" stretchy="false">)</mo></mrow><mo fence="false" id="S2.E2.m1.7.16.4" rspace="0.167em" stretchy="false">|</mo><msub id="S2.E2.m1.7.16.5"><mi id="S2.E2.m1.7.16.5.2">θ</mi><mtext id="S2.E2.m1.7.16.5.3">e</mtext></msub><mo id="S2.E2.m1.7.16.6" stretchy="false">)</mo></mrow><mo fence="false" id="S2.E2.m1.7.17" rspace="0.167em" stretchy="false">|</mo><msup id="S2.E2.m1.7.18"><mo fence="false" id="S2.E2.m1.7.18.2" stretchy="false">|</mo><mn id="S2.E2.m1.7.18.3">2</mn></msup><mo id="S2.E2.m1.7.19" lspace="0em">+</mo><msub id="S2.E2.m1.7.20"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.7.20.2">ℒ</mi><mtext id="S2.E2.m1.7.20.3">matching</mtext></msub><mo id="S2.E2.m1.7.21">,</mo></mrow><annotation encoding="application/x-tex" id="S2.E2.m1.7c">\displaystyle\mathcal{L}_{\text{e}}=\frac{1}{|S_{\text{e}}|}\sum_{\mathbf{x}_{%
\text{e},t}\in S_{\text{e}}}||\mathbf{x}_{\text{e},t}-f_{\text{e}}(g_{\text{e}%
}(\mathbf{x}_{\text{e},t}|\phi_{\text{e}})|\theta_{\text{e}})||^{2}+\mathcal{L%
}_{\text{matching}},</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.7d">caligraphic_L start_POSTSUBSCRIPT e end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG | italic_S start_POSTSUBSCRIPT e end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT e , italic_t end_POSTSUBSCRIPT ∈ italic_S start_POSTSUBSCRIPT e end_POSTSUBSCRIPT end_POSTSUBSCRIPT | | bold_x start_POSTSUBSCRIPT e , italic_t end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT e end_POSTSUBSCRIPT ( italic_g start_POSTSUBSCRIPT e end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT e , italic_t end_POSTSUBSCRIPT | italic_ϕ start_POSTSUBSCRIPT e end_POSTSUBSCRIPT ) | italic_θ start_POSTSUBSCRIPT e end_POSTSUBSCRIPT ) | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + caligraphic_L start_POSTSUBSCRIPT matching end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p5.17">where <math alttext="\mathbf{x}_{\text{e},t}" class="ltx_Math" display="inline" id="S2.p5.12.m1.2"><semantics id="S2.p5.12.m1.2a"><msub id="S2.p5.12.m1.2.3" xref="S2.p5.12.m1.2.3.cmml"><mi id="S2.p5.12.m1.2.3.2" xref="S2.p5.12.m1.2.3.2.cmml">𝐱</mi><mrow id="S2.p5.12.m1.2.2.2.4" xref="S2.p5.12.m1.2.2.2.3.cmml"><mtext id="S2.p5.12.m1.1.1.1.1" xref="S2.p5.12.m1.1.1.1.1a.cmml">e</mtext><mo id="S2.p5.12.m1.2.2.2.4.1" xref="S2.p5.12.m1.2.2.2.3.cmml">,</mo><mi id="S2.p5.12.m1.2.2.2.2" xref="S2.p5.12.m1.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p5.12.m1.2b"><apply id="S2.p5.12.m1.2.3.cmml" xref="S2.p5.12.m1.2.3"><csymbol cd="ambiguous" id="S2.p5.12.m1.2.3.1.cmml" xref="S2.p5.12.m1.2.3">subscript</csymbol><ci id="S2.p5.12.m1.2.3.2.cmml" xref="S2.p5.12.m1.2.3.2">𝐱</ci><list id="S2.p5.12.m1.2.2.2.3.cmml" xref="S2.p5.12.m1.2.2.2.4"><ci id="S2.p5.12.m1.1.1.1.1a.cmml" xref="S2.p5.12.m1.1.1.1.1"><mtext id="S2.p5.12.m1.1.1.1.1.cmml" mathsize="70%" xref="S2.p5.12.m1.1.1.1.1">e</mtext></ci><ci id="S2.p5.12.m1.2.2.2.2.cmml" xref="S2.p5.12.m1.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.12.m1.2c">\mathbf{x}_{\text{e},t}</annotation><annotation encoding="application/x-llamapun" id="S2.p5.12.m1.2d">bold_x start_POSTSUBSCRIPT e , italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the brain recording captured at time <math alttext="t" class="ltx_Math" display="inline" id="S2.p5.13.m2.1"><semantics id="S2.p5.13.m2.1a"><mi id="S2.p5.13.m2.1.1" xref="S2.p5.13.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.p5.13.m2.1b"><ci id="S2.p5.13.m2.1.1.cmml" xref="S2.p5.13.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.13.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.p5.13.m2.1d">italic_t</annotation></semantics></math>. Beside, <math alttext="\theta_{\text{e}}" class="ltx_Math" display="inline" id="S2.p5.14.m3.1"><semantics id="S2.p5.14.m3.1a"><msub id="S2.p5.14.m3.1.1" xref="S2.p5.14.m3.1.1.cmml"><mi id="S2.p5.14.m3.1.1.2" xref="S2.p5.14.m3.1.1.2.cmml">θ</mi><mtext id="S2.p5.14.m3.1.1.3" xref="S2.p5.14.m3.1.1.3a.cmml">e</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p5.14.m3.1b"><apply id="S2.p5.14.m3.1.1.cmml" xref="S2.p5.14.m3.1.1"><csymbol cd="ambiguous" id="S2.p5.14.m3.1.1.1.cmml" xref="S2.p5.14.m3.1.1">subscript</csymbol><ci id="S2.p5.14.m3.1.1.2.cmml" xref="S2.p5.14.m3.1.1.2">𝜃</ci><ci id="S2.p5.14.m3.1.1.3a.cmml" xref="S2.p5.14.m3.1.1.3"><mtext id="S2.p5.14.m3.1.1.3.cmml" mathsize="70%" xref="S2.p5.14.m3.1.1.3">e</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.14.m3.1c">\theta_{\text{e}}</annotation><annotation encoding="application/x-llamapun" id="S2.p5.14.m3.1d">italic_θ start_POSTSUBSCRIPT e end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\phi_{\text{e}}" class="ltx_Math" display="inline" id="S2.p5.15.m4.1"><semantics id="S2.p5.15.m4.1a"><msub id="S2.p5.15.m4.1.1" xref="S2.p5.15.m4.1.1.cmml"><mi id="S2.p5.15.m4.1.1.2" xref="S2.p5.15.m4.1.1.2.cmml">ϕ</mi><mtext id="S2.p5.15.m4.1.1.3" xref="S2.p5.15.m4.1.1.3a.cmml">e</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p5.15.m4.1b"><apply id="S2.p5.15.m4.1.1.cmml" xref="S2.p5.15.m4.1.1"><csymbol cd="ambiguous" id="S2.p5.15.m4.1.1.1.cmml" xref="S2.p5.15.m4.1.1">subscript</csymbol><ci id="S2.p5.15.m4.1.1.2.cmml" xref="S2.p5.15.m4.1.1.2">italic-ϕ</ci><ci id="S2.p5.15.m4.1.1.3a.cmml" xref="S2.p5.15.m4.1.1.3"><mtext id="S2.p5.15.m4.1.1.3.cmml" mathsize="70%" xref="S2.p5.15.m4.1.1.3">e</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.15.m4.1c">\phi_{\text{e}}</annotation><annotation encoding="application/x-llamapun" id="S2.p5.15.m4.1d">italic_ϕ start_POSTSUBSCRIPT e end_POSTSUBSCRIPT</annotation></semantics></math> are the parameters of the <math alttext="f_{\text{e}}" class="ltx_Math" display="inline" id="S2.p5.16.m5.1"><semantics id="S2.p5.16.m5.1a"><msub id="S2.p5.16.m5.1.1" xref="S2.p5.16.m5.1.1.cmml"><mi id="S2.p5.16.m5.1.1.2" xref="S2.p5.16.m5.1.1.2.cmml">f</mi><mtext id="S2.p5.16.m5.1.1.3" xref="S2.p5.16.m5.1.1.3a.cmml">e</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p5.16.m5.1b"><apply id="S2.p5.16.m5.1.1.cmml" xref="S2.p5.16.m5.1.1"><csymbol cd="ambiguous" id="S2.p5.16.m5.1.1.1.cmml" xref="S2.p5.16.m5.1.1">subscript</csymbol><ci id="S2.p5.16.m5.1.1.2.cmml" xref="S2.p5.16.m5.1.1.2">𝑓</ci><ci id="S2.p5.16.m5.1.1.3a.cmml" xref="S2.p5.16.m5.1.1.3"><mtext id="S2.p5.16.m5.1.1.3.cmml" mathsize="70%" xref="S2.p5.16.m5.1.1.3">e</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.16.m5.1c">f_{\text{e}}</annotation><annotation encoding="application/x-llamapun" id="S2.p5.16.m5.1d">italic_f start_POSTSUBSCRIPT e end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="g_{\text{e}}" class="ltx_Math" display="inline" id="S2.p5.17.m6.1"><semantics id="S2.p5.17.m6.1a"><msub id="S2.p5.17.m6.1.1" xref="S2.p5.17.m6.1.1.cmml"><mi id="S2.p5.17.m6.1.1.2" xref="S2.p5.17.m6.1.1.2.cmml">g</mi><mtext id="S2.p5.17.m6.1.1.3" xref="S2.p5.17.m6.1.1.3a.cmml">e</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p5.17.m6.1b"><apply id="S2.p5.17.m6.1.1.cmml" xref="S2.p5.17.m6.1.1"><csymbol cd="ambiguous" id="S2.p5.17.m6.1.1.1.cmml" xref="S2.p5.17.m6.1.1">subscript</csymbol><ci id="S2.p5.17.m6.1.1.2.cmml" xref="S2.p5.17.m6.1.1.2">𝑔</ci><ci id="S2.p5.17.m6.1.1.3a.cmml" xref="S2.p5.17.m6.1.1.3"><mtext id="S2.p5.17.m6.1.1.3.cmml" mathsize="70%" xref="S2.p5.17.m6.1.1.3">e</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.17.m6.1c">g_{\text{e}}</annotation><annotation encoding="application/x-llamapun" id="S2.p5.17.m6.1d">italic_g start_POSTSUBSCRIPT e end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Furthermore, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S2.F1.sf3" title="In Figure 1 ‣ 2 Method ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_tag">1(c)</span></a>, we improve the quality of reconstructed images through the application of an advanced large diffusion generative model. This method involves a sophisticated process where the diffusion model iteratively refines the image, gradually enhancing its clarity, detail, and overall fidelity to the original text or cognitive stimulus. Specifically, this approach starts by generating a basic outline or low-resolution version of the target image based on the initial brain recordings. The large diffusion model then incrementally applies a series of transformations, each designed to enhance the image’s accuracy and detail, by closely aligning it with the associated textual or cognitive data. This process leverages the power of deep learning to simulate and amplify the intricate process of human perception and memory reconstruction and produce high-quality images that more accurately reflect the subtleties and complexities of the original brain recordings.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this project, we use an open-source dataset of short video clips, validated to be evocative of a wide range of emotions &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib2" title="">2</a>]</cite>. The dataset contains 2,185 videos (0.15 – 90s) and across two experiments, participants provided ratings of the videos’ emotional contents, uncovering 27 dimensions of emotion categories. In a separate study, researchers presented the same short videos to five participants (1 female, mean age = 25.8 years) undergoing fMRI scans. Participants were instructed to simply pay attention to the videos. Videos were edited to an 8s presentation time, with a 2s fixation cross between presentations. This resulted in approximately 8 hours of fMRI scanning per participant&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib10" title="">10</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In our experiments, we extended beyond the original study by Horikawa and colleagues&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib10" title="">10</a>]</cite>. First, we identified emotions of interest from the video dataset released by Cowen and colleagues&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib2" title="">2</a>]</cite>, and sorted the videos according to their primary emotion labels. Using the matched fMRI dataset, we identified stimuli that resulted in the most “emotionally charged” brain activation patterns, across participants. In other words, the fMRI responses to these stimuli were reliably predictive of the primary emotion label identified. As in the study by Horikawa et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib10" title="">10</a>]</cite>, we presented these stimuli to three participants (1 female, mean age = 29.3 years) in the fMRI scanner, looping the videos for 8s with a 2s rest period between. However, we interspersed the video trials with emotion-matched imagery trials. During these trials, participants were presented with a text prompt of a single emotion category from the emotions of interest we identified (i.e., “joy”, “surprise”), which stayed on the screen for the 8s duration. Participants were instructed to imagine “in their mind’s eye” a memory or an image that evokes that emotion; for instance, they could envision a birthday party from their childhood for “joy”. Categories were repeated each 10 times across the trials, and participants were instructed to conjure the same image each time.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Implementation detail</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">Our proposed methodology was developed using the PyTorch library, leveraging an NVIDIA A100 GPU with 80GB of VRAM for computational power. To train our network effectively in each iteration, we selected a random batch of sample sets categorized by emotion labels. These batches encompassed three distinct types of data: frames from video clips, fMRI recordings triggered by video stimuli, and fMRI recordings stimulated by emotional text. For the video processing component, we crafted a four-layer UNet architecture, which is essentially a 2D convolutional neural network (CNN) designed for encoding and decoding video information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="180" id="S4.F2.g1" src="https://arxiv.org/html/2410.00047v1/extracted/5884660/images/embeddings.png" width="180">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>PHATE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib13" title="">13</a>]</cite> visualization showcasing the distribution alignment between video prompts and their corresponding emotion expressed as text.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p2.1">Additionally, for the fMRI data, we designed an autoencoder based on a 1D CNN. The training process utilized the Adam optimization algorithm, with a set learning rate of 0.0005, to adjust the network’s parameters efficiently.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Distribution matching</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S4.F2" title="Figure 2 ‣ Implementation detail ‣ 4 Evaluation ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_tag">2</span></a> displays a PHATE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#bib.bib13" title="">13</a>]</cite> visualization, highlighting the embedding space after training our model and performing inference. In this visualization, emotions are differentiated by color coding, with fMRI recordings represented as stars and video embeddings depicted as circles. The figure demonstrates our model’s capability in achieving distribution matching between fMRI data and video frames within the embedding space, effectively bridging the gap between neural recordings and visual stimuli.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="440" id="S4.F3.g1" src="https://arxiv.org/html/2410.00047v1/x4.png" width="823">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Video-prompt fMRI reconstruction based on brain recordings stimulated by video prompts, with each row representing a unique video simulation scenario. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">Reconstruction</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S4.F3" title="Figure 3 ‣ Distribution matching ‣ 4 Evaluation ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_tag">3</span></a> showcases our model’s capability in reconstructing videos from fMRI recordings, using video prompts such as the Yale-Sterling Library, Motion Cherry, and Yale’s Kingman. These examples, excluded from the training set, demonstrate the model’s proficiency in accurately capturing both the location and motion dynamics. As depicted in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S4.F3" title="Figure 3 ‣ Distribution matching ‣ 4 Evaluation ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_tag">3</span></a>, the reconstructed visuals affirm the model’s effectiveness, offering clear and visually appealing representations of the original stimuli.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p2.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.00047v1#S4.F4" title="Figure 4 ‣ Reconstruction ‣ 4 Evaluation ‣ Looking through the mind’s eye via multimodal encoder-decoder networks"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrates our model’s ability to reconstruct the overall essence of emotions and text prompts into visual form. Specifically, it showcases the reconstruction of emotions associated with ”Carving” in the first row, ”Entrancement” in the second row, and ”Yale” in the third row. The figure illustrates that our model achieves conceptually coherent results. For instance, the last frame related to the Yale prompt reveals a scene with a green landscape and a building, suggesting a particular location reminiscent of what the subject envisioned. These results stem from test sets that were excluded from the training phase. According to subjects’ reports, the reconstructions conceptually align with their imagined scenarios during data collection.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="441" id="S4.F4.g1" src="https://arxiv.org/html/2410.00047v1/x5.png" width="821">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Text-prompt fMRI reconstruction from brain recordings triggered by text prompts, where each row displays a distinct emotion simulation scenario across six sequential frames.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This work is a step forward towards visualization of the mind’s eye, i.e., decoding of imagery from human thought. The proposed technology holds the promise to be employed in multiple fronts including aiding individuals with disabilities and diagnosis. For individuals with communication impairments, such as those resulting from strokes or traumatic brain injury, this technology can restore the ability to communicate through the reconstruction of their visual thoughts into images or text, thereby providing a new form of expression for those who are unable to speak or write. Moreover, the ability to decode images and video from brain activity could revolutionize the field of neurology and psychiatry by providing unprecedented insights into brain function. It could aid in the diagnosis of neurological disorders, such as Alzheimer’s, epilepsy, or brain injuries, by allowing doctors to observe the visual and sensory experiences of patients directly. Additionally, it could improve the understanding and treatment of mental health conditions like PTSD, schizophrenia, and depression. However, in addition to the promises of this technology of visualizing the mind’s eye, one must also be aware of the following implications.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Privacy and Ethical Implications</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">The most immediate concern is the invasion of privacy. With machine learning models being able to decode and reconstruct visual experiences from brain activity, they could potentially access the most private and intimate thoughts of an individual without their consent. Ensuring that such technology is used only with explicit, informed consent is crucial. Moreover, the purpose for which this technology is used must be ethically justified. While it could have beneficial applications, such as helping individuals who cannot communicate verbally to express themselves, it could also be used for nefarious purposes, such as surveillance or manipulating thoughts and behaviors.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Security, Accuracy and Interpretation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">The risk of unauthorized access or hacking into this sensitive data could lead to unprecedented levels of personal exposure and blackmail. Strong security measures would be essential to protect individuals from such vulnerabilities. The reliability of the technology is another concern. Misinterpretation of the decoded images or videos could lead to false conclusions about an individual’s thoughts or intentions, with potentially severe consequences for personal freedom and privacy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">Psychological Impact and Regulatory Framework</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">Knowing that one’s visual thoughts could be decoded and viewed by others could lead to psychological stress and a sense of lost autonomy over one’s inner life. The impact on mental health and the concept of personal identity would need careful consideration. Moreover, the development and use of this technology would necessitate robust legal frameworks to regulate its use, protect individuals’ rights, and ensure that ethical standards are maintained. These frameworks would need to balance the potential benefits of the technology with the risks to individual rights and freedoms.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We would like to express our thanks to E. Chandra Fincke for assistance with data collection. We also acknowledge the use of DALL<math alttext="\cdot" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><mo id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><ci id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">⋅</annotation></semantics></math>E for the generating of the graphic illustration presented in Figure 1(a), both were created based on our designed prompts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Chen, J.&nbsp;Qing, T.&nbsp;Xiang, W.&nbsp;L. Yue, and J.&nbsp;H. Zhou.

</span>
<span class="ltx_bibblock">Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 22710–22720, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cowen and Keltner [2017]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;S. Cowen and D.&nbsp;Keltner.

</span>
<span class="ltx_bibblock">Self-report captures 27 distinct categories of emotion bridged by continuous gradients.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the national academy of sciences</em>, 114(38):E7900–E7909, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cox and Savoy [2003]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;D. Cox and R.&nbsp;L. Savoy.

</span>
<span class="ltx_bibblock">Functional magnetic resonance imaging (fmri)“brain reading”: detecting and classifying distributed patterns of fmri activity in human visual cortex.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Neuroimage</em>, 19(2):261–270, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dado et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Dado, Y.&nbsp;Güçlütürk, L.&nbsp;Ambrogioni, G.&nbsp;Ras, S.&nbsp;Bosch, M.&nbsp;van Gerven, and U.&nbsp;Güçlü.

</span>
<span class="ltx_bibblock">Hyperrealistic neural decoding for reconstructing faces from fmri activations via the gan latent space.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Scientific reports</em>, 12(1):141, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et&nbsp;al. [2014]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Goodfellow, J.&nbsp;Pouget-Abadie, M.&nbsp;Mirza, B.&nbsp;Xu, D.&nbsp;Warde-Farley, S.&nbsp;Ozair, A.&nbsp;Courville, and Y.&nbsp;Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial nets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in neural information processing systems</em>, 27, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Gu, K.&nbsp;Jamison, A.&nbsp;Kuceyeski, and M.&nbsp;Sabuncu.

</span>
<span class="ltx_bibblock">Decoding natural image stimuli from fmri data with a surface-based convolutional network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2212.02409</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haxby et&nbsp;al. [2001]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;V. Haxby, M.&nbsp;I. Gobbini, M.&nbsp;L. Furey, A.&nbsp;Ishai, J.&nbsp;L. Schouten, and P.&nbsp;Pietrini.

</span>
<span class="ltx_bibblock">Distributed and overlapping representations of faces and objects in ventral temporal cortex.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Science</em>, 293(5539):2425–2430, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haynes and Rees [2005]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.-D. Haynes and G.&nbsp;Rees.

</span>
<span class="ltx_bibblock">Predicting the orientation of invisible stimuli from activity in human primary visual cortex.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Nature neuroscience</em>, 8(5):686–691, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Ho, A.&nbsp;Jain, and P.&nbsp;Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Advances in neural information processing systems</em>, 33:6840–6851, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horikawa et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Horikawa, A.&nbsp;S. Cowen, D.&nbsp;Keltner, and Y.&nbsp;Kamitani.

</span>
<span class="ltx_bibblock">The neural representation of visually evoked emotion is high-dimensional, categorical, and distributed across transmodal brain regions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Iscience</em>, 23(5):101060, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamitani and Tong [2005]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Kamitani and F.&nbsp;Tong.

</span>
<span class="ltx_bibblock">Decoding the visual and subjective contents of the human brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Nature neuroscience</em>, 8(5):679–685, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Lin, T.&nbsp;Sprague, and A.&nbsp;K. Singh.

</span>
<span class="ltx_bibblock">Mind reader: Reconstructing complex images from brain activities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Advances in Neural Information Processing Systems</em>, 35:29624–29636, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et&nbsp;al. [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;R. Moon, D.&nbsp;Van&nbsp;Dijk, Z.&nbsp;Wang, S.&nbsp;Gigante, D.&nbsp;B. Burkhardt, W.&nbsp;S. Chen, K.&nbsp;Yim, A.&nbsp;v.&nbsp;d. Elzen, M.&nbsp;J. Hirn, R.&nbsp;R. Coifman, et&nbsp;al.

</span>
<span class="ltx_bibblock">Visualizing structure and transitions in high-dimensional biological data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Nature biotechnology</em>, 37(12):1482–1492, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oppenlaender [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Oppenlaender.

</span>
<span class="ltx_bibblock">The creativity of text-to-image generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 25th International Academic Mindtrek Conference</em>, pages 192–202, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozcelik and VanRullen [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Ozcelik and R.&nbsp;VanRullen.

</span>
<span class="ltx_bibblock">Brain-diffuser: Natural scene reconstruction from fmri signals using generative latent diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2303.05334</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozcelik et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Ozcelik, B.&nbsp;Choksi, M.&nbsp;Mozafari, L.&nbsp;Reddy, and R.&nbsp;VanRullen.

</span>
<span class="ltx_bibblock">Reconstruction of perceived images from fmri patterns and semantic brain exploration using instance-conditioned gans.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">2022 International Joint Conference on Neural Networks (IJCNN)</em>, pages 1–8. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Ramesh, P.&nbsp;Dhariwal, A.&nbsp;Nichol, C.&nbsp;Chu, and M.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2204.06125</em>, 1(2):3, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Rombach, A.&nbsp;Blattmann, D.&nbsp;Lorenz, P.&nbsp;Esser, and B.&nbsp;Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 10684–10695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scotti et&nbsp;al. [2024a]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Scotti, A.&nbsp;Banerjee, J.&nbsp;Goode, S.&nbsp;Shabalin, A.&nbsp;Nguyen, A.&nbsp;Dempster, N.&nbsp;Verlinde, E.&nbsp;Yundler, D.&nbsp;Weisberg, K.&nbsp;Norman, et&nbsp;al.

</span>
<span class="ltx_bibblock">Reconstructing the mind’s eye: fmri-to-image with contrastive learning and diffusion priors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scotti et&nbsp;al. [2024b]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;S. Scotti, M.&nbsp;Tripathy, C.&nbsp;K.&nbsp;T. Villanueva, R.&nbsp;Kneeland, T.&nbsp;Chen, A.&nbsp;Narang, C.&nbsp;Santhirasegaran, J.&nbsp;Xu, T.&nbsp;Naselaris, K.&nbsp;A. Norman, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mindeye2: Shared-subject models enable fmri-to-image with 1 hour of data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2403.11207</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seeliger et&nbsp;al. [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Seeliger, U.&nbsp;Güçlü, L.&nbsp;Ambrogioni, Y.&nbsp;Güçlütürk, and M.&nbsp;A. van Gerven.

</span>
<span class="ltx_bibblock">Generative adversarial networks for reconstructing natural images from brain activity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">NeuroImage</em>, 181:775–785, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et&nbsp;al. [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Shen, T.&nbsp;Horikawa, K.&nbsp;Majima, and Y.&nbsp;Kamitani.

</span>
<span class="ltx_bibblock">Deep image reconstruction from human brain activity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">PLoS computational biology</em>, 15(1):e1006633, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snell et&nbsp;al. [2017]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Snell, K.&nbsp;Swersky, and R.&nbsp;Zemel.

</span>
<span class="ltx_bibblock">Prototypical networks for few-shot learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Takagi and Nishimoto [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Takagi and S.&nbsp;Nishimoto.

</span>
<span class="ltx_bibblock">High-resolution image reconstruction with latent diffusion models from human brain activity.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 14453–14463, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body></html>