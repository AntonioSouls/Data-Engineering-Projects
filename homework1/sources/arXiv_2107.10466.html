<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2107.10466] PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding</title><meta property="og:description" content="Current methods of multi-person pose estimation typically treat the localization and the association of body joints separately. It is convenient but inefficient, leading to additional computation and a waste of time.
Tâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2107.10466">

<!--Generated on Sat Mar  2 05:29:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Chenyu Tian<sup id="id9.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ran Yu<sup id="id10.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinyuan Zhao<sup id="id11.2.id1" class="ltx_sup">2</sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weihao Xia<sup id="id12.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haoqian Wang<sup id="id13.5.id1" class="ltx_sup">1</sup>&amp;Yujiu Yang<sup id="id14.6.id2" class="ltx_sup">1</sup>
<sup id="id15.7.id3" class="ltx_sup">1</sup>Tsinghua University
<br class="ltx_break"><sup id="id16.8.id4" class="ltx_sup">2</sup>Northwestern University
<br class="ltx_break">tcy19@mails.tsinghue.edu.cn,
yang.yujiu@sz.tsinghua.edu.cn
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">Current methods of multi-person pose estimation typically treat the <span id="id17.id1.1" class="ltx_text ltx_font_italic">localization</span> and the <span id="id17.id1.2" class="ltx_text ltx_font_italic">association</span> of body joints separately. It is convenient but inefficient, leading to additional computation and a waste of time.
This paper, however, presents a novel framework PoseDet (Estimating <span id="id17.id1.3" class="ltx_text ltx_font_bold">Pose</span> by <span id="id17.id1.4" class="ltx_text ltx_font_bold">Det</span>ection) to localize and associate body joints simultaneously at higher inference speed.
Moreover, we propose the <span id="id17.id1.5" class="ltx_text ltx_font_italic">keypoint-aware pose embedding</span> to represent an object in terms of the locations of its keypoints.
The proposed pose embedding contains semantic and geometric information, allowing us to access discriminative and informative features efficiently.
It is utilized for candidate classification and body joint localization in PoseDet, leading to robust predictions of various poses.
This simple framework achieves an unprecedented speed and a competitive accuracy on the COCO benchmark compared with state-of-the-art methods.
Extensive experiments on the CrowdPose benchmark show the robustness in the crowd scenes.
Source code is available.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Multi-person pose estimation is a fundamental yet challenging task in computer vision, which plays a vital role in human understanding. It has broad applications such as human-robot interaction, virtual reality, sports analysis, <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">etc</em>., in which real-time performance is essential. The challenge remains that estimating accurate poses of multi-person in the 2D image at a real-time speed.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2107.10466/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="166" height="77" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Keypoint-aware pose embedding</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As an extension of single-person pose estimation, multi-person pose estimation requires locating all joints and associating the joints with the same person. Current approaches can be categorized as either top-downÂ <cite class="ltx_cite ltx_citemacro_cite">Sun <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>); Xiao <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib38" title="" class="ltx_ref">2018</a>); Chen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>); Huang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib13" title="" class="ltx_ref">2017</a>); Papandreou <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib27" title="" class="ltx_ref">2017</a>); Fang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>); Guo <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>); Ruan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>); Jiang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> or bottom-upÂ <cite class="ltx_cite ltx_citemacro_cite">Newell <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>); Cao <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>, <a href="#bib.bib4" title="" class="ltx_ref">2021</a>); Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>); Pishchulin <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib28" title="" class="ltx_ref">2016</a>); Insafutdinov <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib14" title="" class="ltx_ref">2016</a>); Iqbal and Gall (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite>. Top-down approaches accomplish body joint association by a well-trained person detector. They usually detect person instances by using a person detector, then estimate the single-person pose within the detected box. Bottom-up approaches instead detect all body joints in an image and then associate them with individuals. Both of them carry out a two-stage strategy, which remains an obstacle to real-time inference.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we propose a single-stage framework, PoseDet. It combines joint localization and association using a simply designed network to eliminate redundant operations.
The central idea of PoseDet is to view multi-person pose estimation as an object detection problem, <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, estimating poses by detection.
Object detection aims to regress two corner points of the bounding box, while pose estimation can be regarded as the regression of a set of body joints.
By replacing corner points with key points of body joints, poses can be estimated using an object detection system directly.
However, only the replacement of regression target decreases the performance due to the complexity of regression space.
Techniques including displacement refinement and keypoint-based operations are adopted, making the detection model more accessible to pose estimation.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Another challenge is the diversity of poses. To accurately regress the keypoints of body joints, the representation of objects is supposed to be informative and fine-grained.
In modern detection systems, the object is usually represented as axis-aligned bounding boxÂ <cite class="ltx_cite ltx_citemacro_cite">Ren <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib30" title="" class="ltx_ref">2015</a>)</cite> or the center pointÂ <cite class="ltx_cite ltx_citemacro_cite">Tian <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite>.
However, these bounding boxes contained unexpected background areas that might lower the feature quality, and the highly overlapped boxes could be incorrectly suppressed by non-maximum suppression (NMS).
Representation with the center point faces the problem that center points of distinctive objects could be too close to distinguish.
Both representations provide rough information of the object and are not suitable for pose estimation.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">A more delicate representation is proposed by RepPointsÂ <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>. The object is represented using a set of sample points whose locations are learned in an unsupervised way. Point-set AnchorsÂ <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite> utilize it in pose estimation, where predefined anchors of poses are used as initial locations of sample points.
Inspired byÂ <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>); Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>, we propose the keypoint-aware pose embedding.
As shown in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we use the body joints as keypoints to represent the person instance.
In conjunction with the deformable convolutional network (DCN)Â <cite class="ltx_cite ltx_citemacro_cite">Dai <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite>, we generate the pose embedding by aggregating the extracted feature vectors at locations of keypoints.
Pose embedding is informative as it contains both semantic information of joint category and geometric information of the pose.
The pose embedding directly predicts the refined pose and the score of each instance.
Different from Point-set Anchors, the locations of keypoints are learnable and supervised by GT keypoints, making them closer to target keypoints during prediction, which allows PoseDet to be robust to various poses. Moreover, PoseDet is an anchor-free detector, leading to lower complexity of the network and a higher inference speed than those of Point-set Anchors.
As a general and flexible representation strategy, the proposed pose embedding can be easily applied to understand other kinds of objects.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Due to the efficient pipeline and the adequate representation, the proposed PoseDet achieves an unprecedented speed and a competitive accuracy for multi-person pose estimation.
We evaluate our method on COCOÂ <cite class="ltx_cite ltx_citemacro_cite">Lin <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib21" title="" class="ltx_ref">2014</a>)</cite> benchmark. The inference speed of PoseDet is 2.5 times higher (10 FPS) than Point-set Anchors with the same scale of the input image and backbone HRNet-W48Â <cite class="ltx_cite ltx_citemacro_cite">Sun <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite>.
By replacing the backbone with DLA-34Â <cite class="ltx_cite ltx_citemacro_cite">Yu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib40" title="" class="ltx_ref">2018</a>)</cite>, the model runs faster at 42 FPS with the accuracyâ€™s decline.
The effectiveness of keypoint-aware pose embedding is demonstrated on CrowdPoseÂ <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> benchmark. PoseDet achieves comparable accuracy with the state-of-the-art solutions and shows its robustness in the crowd scenes.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The contributions of our work are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We reformulate the multi-person pose estimation as an object detection problem. Our proposed PoseDet framework achieves an unprecedented speed and a competitive accuracy compared with state-of-the-art approaches;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose the keypoint-aware pose embedding as a general and flexible representation for person instances, making PoseDet being robust to real-world crowd scenes with occlusion and various poses. It can be easily applied to other instance-level recognition tasks like person re-identification;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The performance of PoseDet is further improved by displacement refinement and keypoint-based operations.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Due to the various applications in the real world, human pose estimation is an active research topic for decades. In the early works, joint locations are predicted based on hand-craft features, such as histogram of oriented gradientÂ <cite class="ltx_cite ltx_citemacro_cite">Sun and Savarese (<a href="#bib.bib32" title="" class="ltx_ref">2011</a>); Wang and Li (<a href="#bib.bib35" title="" class="ltx_ref">2013</a>)</cite>. Recently, state-of-the-art performance is achieved using the Convolutional Neural Networks (CNNs). In this work, based on CNNs, we focus on simplifying detection procedures and improving object representation for multi-person pose estimation.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Real-time multi-person pose estimation.</span> Multi-person pose estimation requires dense prediction over spatial locations as well as detection of the person instance. The complexity makes it hard to inference at real-time speed. The key is to optimize processing procedures and eliminate redundant structures.Â <cite class="ltx_cite ltx_citemacro_cite">Cao <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite> is the first real-time algorithm to tackle this problem. They generate the ground-truth (GT) confidence maps to localize joints and associate them via part affinity fields. Â <cite class="ltx_cite ltx_citemacro_cite">Kocabas <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite> predict the heatmap of joints for joint localization and associate them with a person detector and the pose residual network, where all subnets share the same backbone. Â <cite class="ltx_cite ltx_citemacro_cite">Nie <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> present SPM, a single-stage solution that predicts hierarchical offsets of joints based on detected root points. The single-stage strategy makes it faster than two-stage (<em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, bottom-up or top-down) strategies. We present a faster single-stage solution that combines joint localization and association in the same pipeline.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Object representation in multi-person pose estimation.</span> Object representation serves as a guide to extract features for purposes such as object classification and location refinement.Â <cite class="ltx_cite ltx_citemacro_cite">He <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite> use the bounding box to represent an object. Features are extracted by the bounding box to predict the heatmap of joints. Box-based representation faces the problem of including the non-target area, resulting in low-quality feature and computation overhead.Â <cite class="ltx_cite ltx_citemacro_cite">Zhou <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite> model an object as a single point and regress joint offsets at each point. Center point-based representation is fast but contains limited information for dense prediction of joints.Â <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite> obtains features at a set of predefined anchors to acquire informative features. For refinement of segmentation boundary,Â <cite class="ltx_cite ltx_citemacro_cite">Kirillov <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite> regards an object as a continuous entity, which is encoded in feature maps and can be accessed at any points by interpolation. We follow the point-set representation and the concept of continuous entity, presenting pose embedding that extracts keypoint-aware features.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2107.10466/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="386" height="135" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of PoseDet. We reformulate multi-person pose estimation as an object detection problem. Regression and classification branch conduct dense predictions over feature maps.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first elaborate keypoint-aware pose embedding, a key component of PoseDet in understanding objects effectively and efficiently. Next, we present the multi-level anchor-free detector and show how to utilize pose embedding in it to estimate 2D poses of persons in an image.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Keypoint-Aware Pose Embedding</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Object representation is an important medium to recognize objects in visual tasks (<em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, classification and regression). As discussed in SectionÂ <a href="#S1" title="1 Introduction â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, features extracted by the bounding box and center point are either redundant or limited for pose estimation. In our system, objects are represented as keypoints, and features at keypoints are extracted and aggregated for classification and regression.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Given the preceding feature maps, the person instance can be regarded as a continuous entity encoded in the feature maps. As shown in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the feature vectors at locations of body joints contain semantic and geometric information of the pose.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.6" class="ltx_p">In natural language processing, discrete text data are represented as continuous vectors via word embedding to capture semantic and syntactic featuresÂ <cite class="ltx_cite ltx_citemacro_cite">Paccanaro and
Hinton (<a href="#bib.bib26" title="" class="ltx_ref">2001</a>)</cite>. Analogously, we extract and aggregate the irregular feature vectors to represent the person instance. The representation is mapped from a set of vectors to one vector, named pose embedding. Formally, the process can be summarized as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="f:\{\psi_{i}\}_{i=1}^{K}\rightarrow\varepsilon," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">f</mi><mo lspace="0.278em" rspace="0.278em" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">:</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><msubsup id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">{</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">Ïˆ</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">K</mi></msubsup><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">â†’</mo><mi id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">Îµ</mi></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><ci id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2">:</ci><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">ğ‘“</ci><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><ci id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2">â†’</ci><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1">subscript</csymbol><set id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2">ğœ“</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><eq id="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1"></eq><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">ğ¾</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">ğœ€</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">f:\{\psi_{i}\}_{i=1}^{K}\rightarrow\varepsilon,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.5" class="ltx_p">where <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\psi_{i}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">Ïˆ</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">ğœ“</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\psi_{i}</annotation></semantics></math> donates feature vector at point <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">i</annotation></semantics></math> on the feature maps, <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">K</annotation></semantics></math> is the number of joints. <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">Îµ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">ğœ€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\varepsilon</annotation></semantics></math> is a vector, named pose embedding, containing the information of <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mi id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><ci id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">K</annotation></semantics></math> joints.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The pose embedding contains semantic and geometric information because both the shape and joint categories are confirmed. Itâ€™s an informative representation that can be used to estimate pose and classify positive or negative candidates, but itâ€™s hard to make full use of them by conventional CNNs due to the restriction of the local receptive field and various poses.
Considering dense hypotheses over spatial locations of feature maps in modern detection networks, which means dense pose embeddings are required. FollowingÂ <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>); Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>, we generate dense pose embeddings for these hypotheses via one DCN layer simultaneously. The DCN layer takes offsets as well as feature maps as input to generate pose embeddings. The offset is a 2K-dimensional vector for each hypothesis, which indicates the positions of K keypoints based on the location of the hypothesis on the feature map. DCN generates the pose embedding for each hypothesis according to different offsets, which allows us to access features at arbitrary positions without limitation of geometric structure efficiently.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">The learning of offsets is crucial to the performance of DCN. <cite class="ltx_cite ltx_citemacro_cite">Dai <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> adopt an unsupervised way to drive the learning of offsets implicitly. <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite> use weak supervision by minimizing the loss between the GT bounding boxes and pseudo boxes generated by offsets. <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite> use the most common poses as predefined anchors to generate fixed offsets. In this work, a bypass is applied to generate the keypoint-aware offsets, which is supervised by GT joints, driving the learning of offsets explicitly. This allows us to generate meaningful feature representation stably. Moreover, dynamic offsets provide more flexibility than<cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>, making our PoseDet robust to various poses.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">Pose embedding is applied in PoseDet to estimate pose. As a general representation strategy, we note that itâ€™s easy to expand to other objects by replacing the joint with other kinds of keypoint.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>PoseDet</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Recently, one-stage detectors show compatible accuracy to two-stage strategies with higher inference speed. For fast pose estimation, we start from RepPointsÂ <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>, a one-stage anchor-free detector. By utilize pose embedding in RepPoints, we propose PoseDet, a pose estimation machine that combines human detection and poses estimation into the same pipeline.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The pipeline of our approach is illustrated in FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3 Approach â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. PoseDet comprises a backbone for feature extraction, a bypass for offsets prediction, and two branches for joint localization and candidate scoring.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The backbone is a feature pyramid network (FPN), producing a multi-scale feature pyramid from a single resolution image. Objects are assigned to different levels according to the scale, which partially solves the problem of scale diversity of poses. PoseDet makes dense hypotheses over the spatial location of multi-level feature maps, also known as candidates. We estimate the pose of each candidate and score it.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.5" class="ltx_p">Let <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\mathbb{F}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">ğ”½</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">ğ”½</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\mathbb{F}</annotation></semantics></math> be the single-level feature maps of size <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="C\times H\times W" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mrow id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p4.2.m2.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p4.2.m2.1.1.1a" xref="S3.SS2.p4.2.m2.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p4.2.m2.1.1.4" xref="S3.SS2.p4.2.m2.1.1.4.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><times id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1"></times><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">ğ¶</ci><ci id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">ğ»</ci><ci id="S3.SS2.p4.2.m2.1.1.4.cmml" xref="S3.SS2.p4.2.m2.1.1.4">ğ‘Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">C\times H\times W</annotation></semantics></math>. There are <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="H\times W" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mrow id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p4.3.m3.1.1.1" xref="S3.SS2.p4.3.m3.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><times id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1.1"></times><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">ğ»</ci><ci id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">ğ‘Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">H\times W</annotation></semantics></math> candidates at that level, processed by the two branches to estimate <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="H\times W" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><mrow id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml"><mi id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p4.4.m4.1.1.1" xref="S3.SS2.p4.4.m4.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1"><times id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1.1"></times><ci id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2">ğ»</ci><ci id="S3.SS2.p4.4.m4.1.1.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3">ğ‘Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">H\times W</annotation></semantics></math> possible poses. The bypass produces <math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="H\times W" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><mrow id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml"><mi id="S3.SS2.p4.5.m5.1.1.2" xref="S3.SS2.p4.5.m5.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p4.5.m5.1.1.1" xref="S3.SS2.p4.5.m5.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p4.5.m5.1.1.3" xref="S3.SS2.p4.5.m5.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1"><times id="S3.SS2.p4.5.m5.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1.1"></times><ci id="S3.SS2.p4.5.m5.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.2">ğ»</ci><ci id="S3.SS2.p4.5.m5.1.1.3.cmml" xref="S3.SS2.p4.5.m5.1.1.3">ğ‘Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">H\times W</annotation></semantics></math> sets of offsets, where each set is a 2K-dimensional vector, indicating the offsets from the location of a candidate to the body joints of a person instance. Given the predicted offsets and feature maps extracted by FPN, the other two branches implement task-specific pose embedding for displacement refinement and candidate scoring, respectively. We utilize the above full convolutional networks on each level of FPN, producing dense multi-level predictions, then adopt NMS to eliminate redundant predictions.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">In the following sections, we describe the details of regressing body joints and scoring candidates and the intermediate supervision of FPN. Finally, we introduce training and inference of PoseDet, including assignment strategies and NMS.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Joint Localization</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Multi-person poses in an image can be represented as:</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\mathbb{P}=\{(x_{1}^{i},y_{1}^{i}),(x_{2}^{i},y_{2}^{i}),...,(x_{K}^{i},y_{K}^{i})\}_{i=1}^{N}," display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.5" xref="S3.E2.m1.2.2.1.1.5.cmml">â„™</mi><mo id="S3.E2.m1.2.2.1.1.4" xref="S3.E2.m1.2.2.1.1.4.cmml">=</mo><msubsup id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><mrow id="S3.E2.m1.2.2.1.1.3.3.3.3" xref="S3.E2.m1.2.2.1.1.3.3.3.4.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.3.3.3.3.4" xref="S3.E2.m1.2.2.1.1.3.3.3.4.cmml">{</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">(</mo><msubsup id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mn id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">1</mn><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msubsup><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.4" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">,</mo><msubsup id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.cmml">y</mi><mn id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.2.3.cmml">1</mn><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.5" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.E2.m1.2.2.1.1.3.3.3.3.5" xref="S3.E2.m1.2.2.1.1.3.3.3.4.cmml">,</mo><mrow id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.3.cmml">(</mo><msubsup id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.2.2.cmml">x</mi><mn id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.2.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.2.3.cmml">2</mn><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.3.cmml">i</mi></msubsup><mo id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.4" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.3.cmml">,</mo><msubsup id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.cmml">y</mi><mn id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.3.cmml">2</mn><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.5" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.E2.m1.2.2.1.1.3.3.3.3.6" xref="S3.E2.m1.2.2.1.1.3.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">â€¦</mi><mo id="S3.E2.m1.2.2.1.1.3.3.3.3.7" xref="S3.E2.m1.2.2.1.1.3.3.3.4.cmml">,</mo><mrow id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.3.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.3" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.3.cmml">(</mo><msubsup id="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.2.2" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.2.2.cmml">x</mi><mi id="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.2.3" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.2.3.cmml">K</mi><mi id="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.3" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.3.cmml">i</mi></msubsup><mo id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.4" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.3.cmml">,</mo><msubsup id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.2.2" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.2.2.cmml">y</mi><mi id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.2.3" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.2.3.cmml">K</mi><mi id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.3" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.5" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.3.cmml">)</mo></mrow><mo stretchy="false" id="S3.E2.m1.2.2.1.1.3.3.3.3.8" xref="S3.E2.m1.2.2.1.1.3.3.3.4.cmml">}</mo></mrow><mrow id="S3.E2.m1.2.2.1.1.3.3.5" xref="S3.E2.m1.2.2.1.1.3.3.5.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.5.2" xref="S3.E2.m1.2.2.1.1.3.3.5.2.cmml">i</mi><mo id="S3.E2.m1.2.2.1.1.3.3.5.1" xref="S3.E2.m1.2.2.1.1.3.3.5.1.cmml">=</mo><mn id="S3.E2.m1.2.2.1.1.3.3.5.3" xref="S3.E2.m1.2.2.1.1.3.3.5.3.cmml">1</mn></mrow><mi id="S3.E2.m1.2.2.1.1.3.5" xref="S3.E2.m1.2.2.1.1.3.5.cmml">N</mi></msubsup></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.4.cmml" xref="S3.E2.m1.2.2.1.1.4"></eq><ci id="S3.E2.m1.2.2.1.1.5.cmml" xref="S3.E2.m1.2.2.1.1.5">â„™</ci><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3">subscript</csymbol><set id="S3.E2.m1.2.2.1.1.3.3.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3"><interval closure="open" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2"><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2">ğ‘¥</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3">1</cn></apply><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.2.2">ğ‘¦</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.3">ğ‘–</ci></apply></interval><interval closure="open" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2"><apply id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.2.2">ğ‘¥</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.2.3">2</cn></apply><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.3">ğ‘–</ci></apply><apply id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.2">ğ‘¦</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.3">2</cn></apply><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.3">ğ‘–</ci></apply></interval><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">â€¦</ci><interval closure="open" id="S3.E2.m1.2.2.1.1.3.3.3.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2"><apply id="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.2.2">ğ‘¥</ci><ci id="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.2.3">ğ¾</ci></apply><ci id="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.1.1.3">ğ‘–</ci></apply><apply id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.2.2">ğ‘¦</ci><ci id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.2.3">ğ¾</ci></apply><ci id="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.2.2.3">ğ‘–</ci></apply></interval></set><apply id="S3.E2.m1.2.2.1.1.3.3.5.cmml" xref="S3.E2.m1.2.2.1.1.3.3.5"><eq id="S3.E2.m1.2.2.1.1.3.3.5.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.5.1"></eq><ci id="S3.E2.m1.2.2.1.1.3.3.5.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.5.2">ğ‘–</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.3.3.5.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.5.3">1</cn></apply></apply><ci id="S3.E2.m1.2.2.1.1.3.5.cmml" xref="S3.E2.m1.2.2.1.1.3.5">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\mathbb{P}=\{(x_{1}^{i},y_{1}^{i}),(x_{2}^{i},y_{2}^{i}),...,(x_{K}^{i},y_{K}^{i})\}_{i=1}^{N},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p2.3" class="ltx_p">where N is the number of persons in images, and <math id="S3.SS2.SSS1.p2.1.m1.2" class="ltx_Math" alttext="(x_{j}^{i},y_{j}^{i})" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.2a"><mrow id="S3.SS2.SSS1.p2.1.m1.2.2.2" xref="S3.SS2.SSS1.p2.1.m1.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p2.1.m1.2.2.2.3" xref="S3.SS2.SSS1.p2.1.m1.2.2.3.cmml">(</mo><msubsup id="S3.SS2.SSS1.p2.1.m1.1.1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p2.1.m1.1.1.1.1.2.2" xref="S3.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.SS2.SSS1.p2.1.m1.1.1.1.1.2.3" xref="S3.SS2.SSS1.p2.1.m1.1.1.1.1.2.3.cmml">j</mi><mi id="S3.SS2.SSS1.p2.1.m1.1.1.1.1.3" xref="S3.SS2.SSS1.p2.1.m1.1.1.1.1.3.cmml">i</mi></msubsup><mo id="S3.SS2.SSS1.p2.1.m1.2.2.2.4" xref="S3.SS2.SSS1.p2.1.m1.2.2.3.cmml">,</mo><msubsup id="S3.SS2.SSS1.p2.1.m1.2.2.2.2" xref="S3.SS2.SSS1.p2.1.m1.2.2.2.2.cmml"><mi id="S3.SS2.SSS1.p2.1.m1.2.2.2.2.2.2" xref="S3.SS2.SSS1.p2.1.m1.2.2.2.2.2.2.cmml">y</mi><mi id="S3.SS2.SSS1.p2.1.m1.2.2.2.2.2.3" xref="S3.SS2.SSS1.p2.1.m1.2.2.2.2.2.3.cmml">j</mi><mi id="S3.SS2.SSS1.p2.1.m1.2.2.2.2.3" xref="S3.SS2.SSS1.p2.1.m1.2.2.2.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.SS2.SSS1.p2.1.m1.2.2.2.5" xref="S3.SS2.SSS1.p2.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.2b"><interval closure="open" id="S3.SS2.SSS1.p2.1.m1.2.2.3.cmml" xref="S3.SS2.SSS1.p2.1.m1.2.2.2"><apply id="S3.SS2.SSS1.p2.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS1.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1.1.1.2.2">ğ‘¥</ci><ci id="S3.SS2.SSS1.p2.1.m1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1.1.1.2.3">ğ‘—</ci></apply><ci id="S3.SS2.SSS1.p2.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS2.SSS1.p2.1.m1.2.2.2.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.2.2.2.2">superscript</csymbol><apply id="S3.SS2.SSS1.p2.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.2.2.2.2.2.2">ğ‘¦</ci><ci id="S3.SS2.SSS1.p2.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p2.1.m1.2.2.2.2.2.3">ğ‘—</ci></apply><ci id="S3.SS2.SSS1.p2.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p2.1.m1.2.2.2.2.3">ğ‘–</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.2c">(x_{j}^{i},y_{j}^{i})</annotation></semantics></math> donates the coordinates of <math id="S3.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.SSS1.p2.2.m2.1a"><mi id="S3.SS2.SSS1.p2.2.m2.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.1b"><ci id="S3.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.1c">j</annotation></semantics></math> the joints of person <math id="S3.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS1.p2.3.m3.1a"><mi id="S3.SS2.SSS1.p2.3.m3.1.1" xref="S3.SS2.SSS1.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.3.m3.1b"><ci id="S3.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.3.m3.1c">i</annotation></semantics></math>. To estimate the pose of a person instance, we regress the displacements from the candidateâ€™s point to GT joints (each candidate is assigned with a GT pose). Itâ€™s hard to regress the displacements directly due to the variety of poses. To tackle the problem, followingÂ <cite class="ltx_cite ltx_citemacro_cite">Nie <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>, we implement a coarse-to-fine manner to approximate a more accurate location estimation. The process can be formulated as:</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.3" class="ltx_Math" alttext="(x_{j}^{i},y_{j}^{i})=(x,y)+(\delta_{x}^{c},\delta_{y}^{c})+(\delta_{x}^{f},\delta_{y}^{f})." display="block"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.2.2" xref="S3.E3.m1.3.3.1.1.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.2.2.3" xref="S3.E3.m1.3.3.1.1.2.3.cmml">(</mo><msubsup id="S3.E3.m1.3.3.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.E3.m1.3.3.1.1.1.1.1.2.3" xref="S3.E3.m1.3.3.1.1.1.1.1.2.3.cmml">j</mi><mi id="S3.E3.m1.3.3.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.3.cmml">i</mi></msubsup><mo id="S3.E3.m1.3.3.1.1.2.2.4" xref="S3.E3.m1.3.3.1.1.2.3.cmml">,</mo><msubsup id="S3.E3.m1.3.3.1.1.2.2.2" xref="S3.E3.m1.3.3.1.1.2.2.2.cmml"><mi id="S3.E3.m1.3.3.1.1.2.2.2.2.2" xref="S3.E3.m1.3.3.1.1.2.2.2.2.2.cmml">y</mi><mi id="S3.E3.m1.3.3.1.1.2.2.2.2.3" xref="S3.E3.m1.3.3.1.1.2.2.2.2.3.cmml">j</mi><mi id="S3.E3.m1.3.3.1.1.2.2.2.3" xref="S3.E3.m1.3.3.1.1.2.2.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E3.m1.3.3.1.1.2.2.5" xref="S3.E3.m1.3.3.1.1.2.3.cmml">)</mo></mrow><mo id="S3.E3.m1.3.3.1.1.7" xref="S3.E3.m1.3.3.1.1.7.cmml">=</mo><mrow id="S3.E3.m1.3.3.1.1.6" xref="S3.E3.m1.3.3.1.1.6.cmml"><mrow id="S3.E3.m1.3.3.1.1.6.6.2" xref="S3.E3.m1.3.3.1.1.6.6.1.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.6.6.2.1" xref="S3.E3.m1.3.3.1.1.6.6.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">x</mi><mo id="S3.E3.m1.3.3.1.1.6.6.2.2" xref="S3.E3.m1.3.3.1.1.6.6.1.cmml">,</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.E3.m1.3.3.1.1.6.6.2.3" xref="S3.E3.m1.3.3.1.1.6.6.1.cmml">)</mo></mrow><mo id="S3.E3.m1.3.3.1.1.6.5" xref="S3.E3.m1.3.3.1.1.6.5.cmml">+</mo><mrow id="S3.E3.m1.3.3.1.1.4.2.2" xref="S3.E3.m1.3.3.1.1.4.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.4.2.2.3" xref="S3.E3.m1.3.3.1.1.4.2.3.cmml">(</mo><msubsup id="S3.E3.m1.3.3.1.1.3.1.1.1" xref="S3.E3.m1.3.3.1.1.3.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.3.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.3.1.1.1.2.2.cmml">Î´</mi><mi id="S3.E3.m1.3.3.1.1.3.1.1.1.2.3" xref="S3.E3.m1.3.3.1.1.3.1.1.1.2.3.cmml">x</mi><mi id="S3.E3.m1.3.3.1.1.3.1.1.1.3" xref="S3.E3.m1.3.3.1.1.3.1.1.1.3.cmml">c</mi></msubsup><mo id="S3.E3.m1.3.3.1.1.4.2.2.4" xref="S3.E3.m1.3.3.1.1.4.2.3.cmml">,</mo><msubsup id="S3.E3.m1.3.3.1.1.4.2.2.2" xref="S3.E3.m1.3.3.1.1.4.2.2.2.cmml"><mi id="S3.E3.m1.3.3.1.1.4.2.2.2.2.2" xref="S3.E3.m1.3.3.1.1.4.2.2.2.2.2.cmml">Î´</mi><mi id="S3.E3.m1.3.3.1.1.4.2.2.2.2.3" xref="S3.E3.m1.3.3.1.1.4.2.2.2.2.3.cmml">y</mi><mi id="S3.E3.m1.3.3.1.1.4.2.2.2.3" xref="S3.E3.m1.3.3.1.1.4.2.2.2.3.cmml">c</mi></msubsup><mo stretchy="false" id="S3.E3.m1.3.3.1.1.4.2.2.5" xref="S3.E3.m1.3.3.1.1.4.2.3.cmml">)</mo></mrow><mo id="S3.E3.m1.3.3.1.1.6.5a" xref="S3.E3.m1.3.3.1.1.6.5.cmml">+</mo><mrow id="S3.E3.m1.3.3.1.1.6.4.2" xref="S3.E3.m1.3.3.1.1.6.4.3.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.6.4.2.3" xref="S3.E3.m1.3.3.1.1.6.4.3.cmml">(</mo><msubsup id="S3.E3.m1.3.3.1.1.5.3.1.1" xref="S3.E3.m1.3.3.1.1.5.3.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.5.3.1.1.2.2" xref="S3.E3.m1.3.3.1.1.5.3.1.1.2.2.cmml">Î´</mi><mi id="S3.E3.m1.3.3.1.1.5.3.1.1.2.3" xref="S3.E3.m1.3.3.1.1.5.3.1.1.2.3.cmml">x</mi><mi id="S3.E3.m1.3.3.1.1.5.3.1.1.3" xref="S3.E3.m1.3.3.1.1.5.3.1.1.3.cmml">f</mi></msubsup><mo id="S3.E3.m1.3.3.1.1.6.4.2.4" xref="S3.E3.m1.3.3.1.1.6.4.3.cmml">,</mo><msubsup id="S3.E3.m1.3.3.1.1.6.4.2.2" xref="S3.E3.m1.3.3.1.1.6.4.2.2.cmml"><mi id="S3.E3.m1.3.3.1.1.6.4.2.2.2.2" xref="S3.E3.m1.3.3.1.1.6.4.2.2.2.2.cmml">Î´</mi><mi id="S3.E3.m1.3.3.1.1.6.4.2.2.2.3" xref="S3.E3.m1.3.3.1.1.6.4.2.2.2.3.cmml">y</mi><mi id="S3.E3.m1.3.3.1.1.6.4.2.2.3" xref="S3.E3.m1.3.3.1.1.6.4.2.2.3.cmml">f</mi></msubsup><mo stretchy="false" id="S3.E3.m1.3.3.1.1.6.4.2.5" xref="S3.E3.m1.3.3.1.1.6.4.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E3.m1.3.3.1.2" xref="S3.E3.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1"><eq id="S3.E3.m1.3.3.1.1.7.cmml" xref="S3.E3.m1.3.3.1.1.7"></eq><interval closure="open" id="S3.E3.m1.3.3.1.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.2.2"><apply id="S3.E3.m1.3.3.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.2.2">ğ‘¥</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.2.3">ğ‘—</ci></apply><ci id="S3.E3.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E3.m1.3.3.1.1.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.2.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.2.2.2">superscript</csymbol><apply id="S3.E3.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.2.2.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.2.2.2.2.2">ğ‘¦</ci><ci id="S3.E3.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.2.2.2.2.3">ğ‘—</ci></apply><ci id="S3.E3.m1.3.3.1.1.2.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.2.2.2.3">ğ‘–</ci></apply></interval><apply id="S3.E3.m1.3.3.1.1.6.cmml" xref="S3.E3.m1.3.3.1.1.6"><plus id="S3.E3.m1.3.3.1.1.6.5.cmml" xref="S3.E3.m1.3.3.1.1.6.5"></plus><interval closure="open" id="S3.E3.m1.3.3.1.1.6.6.1.cmml" xref="S3.E3.m1.3.3.1.1.6.6.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğ‘¥</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">ğ‘¦</ci></interval><interval closure="open" id="S3.E3.m1.3.3.1.1.4.2.3.cmml" xref="S3.E3.m1.3.3.1.1.4.2.2"><apply id="S3.E3.m1.3.3.1.1.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.3.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.3.1.1.1">superscript</csymbol><apply id="S3.E3.m1.3.3.1.1.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.3.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.3.1.1.1">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.3.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.3.1.1.1.2.2">ğ›¿</ci><ci id="S3.E3.m1.3.3.1.1.3.1.1.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.3.1.1.1.2.3">ğ‘¥</ci></apply><ci id="S3.E3.m1.3.3.1.1.3.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3.1.1.1.3">ğ‘</ci></apply><apply id="S3.E3.m1.3.3.1.1.4.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.4.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.4.2.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.4.2.2.2">superscript</csymbol><apply id="S3.E3.m1.3.3.1.1.4.2.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.4.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.4.2.2.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.4.2.2.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.4.2.2.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.4.2.2.2.2.2">ğ›¿</ci><ci id="S3.E3.m1.3.3.1.1.4.2.2.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.4.2.2.2.2.3">ğ‘¦</ci></apply><ci id="S3.E3.m1.3.3.1.1.4.2.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.4.2.2.2.3">ğ‘</ci></apply></interval><interval closure="open" id="S3.E3.m1.3.3.1.1.6.4.3.cmml" xref="S3.E3.m1.3.3.1.1.6.4.2"><apply id="S3.E3.m1.3.3.1.1.5.3.1.1.cmml" xref="S3.E3.m1.3.3.1.1.5.3.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.5.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.5.3.1.1">superscript</csymbol><apply id="S3.E3.m1.3.3.1.1.5.3.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.5.3.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.5.3.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.5.3.1.1">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.5.3.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.5.3.1.1.2.2">ğ›¿</ci><ci id="S3.E3.m1.3.3.1.1.5.3.1.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.5.3.1.1.2.3">ğ‘¥</ci></apply><ci id="S3.E3.m1.3.3.1.1.5.3.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.5.3.1.1.3">ğ‘“</ci></apply><apply id="S3.E3.m1.3.3.1.1.6.4.2.2.cmml" xref="S3.E3.m1.3.3.1.1.6.4.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.6.4.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.6.4.2.2">superscript</csymbol><apply id="S3.E3.m1.3.3.1.1.6.4.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.6.4.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.6.4.2.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.6.4.2.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.6.4.2.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.6.4.2.2.2.2">ğ›¿</ci><ci id="S3.E3.m1.3.3.1.1.6.4.2.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.6.4.2.2.2.3">ğ‘¦</ci></apply><ci id="S3.E3.m1.3.3.1.1.6.4.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.6.4.2.2.3">ğ‘“</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">(x_{j}^{i},y_{j}^{i})=(x,y)+(\delta_{x}^{c},\delta_{y}^{c})+(\delta_{x}^{f},\delta_{y}^{f}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p3.2" class="ltx_p">Here, (x, y) is the coordinate of the candidate, <math id="S3.SS2.SSS1.p3.1.m1.2" class="ltx_Math" alttext="(\delta_{x}^{c},\delta_{y}^{c})" display="inline"><semantics id="S3.SS2.SSS1.p3.1.m1.2a"><mrow id="S3.SS2.SSS1.p3.1.m1.2.2.2" xref="S3.SS2.SSS1.p3.1.m1.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.1.m1.2.2.2.3" xref="S3.SS2.SSS1.p3.1.m1.2.2.3.cmml">(</mo><msubsup id="S3.SS2.SSS1.p3.1.m1.1.1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.2.2" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.2.2.cmml">Î´</mi><mi id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.2.3" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.2.3.cmml">x</mi><mi id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.3" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.3.cmml">c</mi></msubsup><mo id="S3.SS2.SSS1.p3.1.m1.2.2.2.4" xref="S3.SS2.SSS1.p3.1.m1.2.2.3.cmml">,</mo><msubsup id="S3.SS2.SSS1.p3.1.m1.2.2.2.2" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.2" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.2.cmml">Î´</mi><mi id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.cmml">y</mi><mi id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.3" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.3.cmml">c</mi></msubsup><mo stretchy="false" id="S3.SS2.SSS1.p3.1.m1.2.2.2.5" xref="S3.SS2.SSS1.p3.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.1.m1.2b"><interval closure="open" id="S3.SS2.SSS1.p3.1.m1.2.2.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2"><apply id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.2.2">ğ›¿</ci><ci id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.2.3">ğ‘¥</ci></apply><ci id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.3">ğ‘</ci></apply><apply id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2">superscript</csymbol><apply id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.2">ğ›¿</ci><ci id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3">ğ‘¦</ci></apply><ci id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.3">ğ‘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.1.m1.2c">(\delta_{x}^{c},\delta_{y}^{c})</annotation></semantics></math> is coarse displacement, and <math id="S3.SS2.SSS1.p3.2.m2.2" class="ltx_Math" alttext="(\delta_{x}^{f},\delta_{y}^{f})" display="inline"><semantics id="S3.SS2.SSS1.p3.2.m2.2a"><mrow id="S3.SS2.SSS1.p3.2.m2.2.2.2" xref="S3.SS2.SSS1.p3.2.m2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.2.m2.2.2.2.3" xref="S3.SS2.SSS1.p3.2.m2.2.2.3.cmml">(</mo><msubsup id="S3.SS2.SSS1.p3.2.m2.1.1.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.cmml">Î´</mi><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.3.cmml">x</mi><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.3.cmml">f</mi></msubsup><mo id="S3.SS2.SSS1.p3.2.m2.2.2.2.4" xref="S3.SS2.SSS1.p3.2.m2.2.2.3.cmml">,</mo><msubsup id="S3.SS2.SSS1.p3.2.m2.2.2.2.2" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.2" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.2.cmml">Î´</mi><mi id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.cmml">y</mi><mi id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.3" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.3.cmml">f</mi></msubsup><mo stretchy="false" id="S3.SS2.SSS1.p3.2.m2.2.2.2.5" xref="S3.SS2.SSS1.p3.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.2.m2.2b"><interval closure="open" id="S3.SS2.SSS1.p3.2.m2.2.2.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2"><apply id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2">ğ›¿</ci><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.3">ğ‘¥</ci></apply><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.3">ğ‘“</ci></apply><apply id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2">superscript</csymbol><apply id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.2">ğ›¿</ci><ci id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3">ğ‘¦</ci></apply><ci id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.3">ğ‘“</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.2.m2.2c">(\delta_{x}^{f},\delta_{y}^{f})</annotation></semantics></math> is refined displacement.</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.2" class="ltx_p">The offsets predicted by the bypass provide a coarse prediction of the pose, which not only guides DCN to generate pose embedding but also gives the coarse displacements. We estimate the refined displacements based on the coarse prediction. The regression branch is composed of two <math id="S3.SS2.SSS1.p4.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS2.SSS1.p4.1.m1.1a"><mrow id="S3.SS2.SSS1.p4.1.m1.1.1" xref="S3.SS2.SSS1.p4.1.m1.1.1.cmml"><mn id="S3.SS2.SSS1.p4.1.m1.1.1.2" xref="S3.SS2.SSS1.p4.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p4.1.m1.1.1.1" xref="S3.SS2.SSS1.p4.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS1.p4.1.m1.1.1.3" xref="S3.SS2.SSS1.p4.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.1.m1.1b"><apply id="S3.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1"><times id="S3.SS2.SSS1.p4.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS1.p4.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1.2">3</cn><cn type="integer" id="S3.SS2.SSS1.p4.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.1.m1.1c">3\times 3</annotation></semantics></math> conv layers, and a deformable convolution layer is applied to produce pose embedding. The refined displacements are regressed directly via <math id="S3.SS2.SSS1.p4.2.m2.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS2.SSS1.p4.2.m2.1a"><mrow id="S3.SS2.SSS1.p4.2.m2.1.1" xref="S3.SS2.SSS1.p4.2.m2.1.1.cmml"><mn id="S3.SS2.SSS1.p4.2.m2.1.1.2" xref="S3.SS2.SSS1.p4.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p4.2.m2.1.1.1" xref="S3.SS2.SSS1.p4.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS1.p4.2.m2.1.1.3" xref="S3.SS2.SSS1.p4.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.2.m2.1b"><apply id="S3.SS2.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1"><times id="S3.SS2.SSS1.p4.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.SSS1.p4.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1.2">1</cn><cn type="integer" id="S3.SS2.SSS1.p4.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.2.m2.1c">1\times 1</annotation></semantics></math> conv layer, which takes pose embedding as input. We get the final prediction by aggregating the coarse prediction and the refinement (Equation <a href="#S3.E3" title="In 3.2.1 Joint Localization â€£ 3.2 PoseDet â€£ 3 Approach â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div id="S3.SS2.SSS1.p5" class="ltx_para">
<p id="S3.SS2.SSS1.p5.1" class="ltx_p">Pose embedding encodes the bias of coarse prediction, leading to a more precise estimation, solving the problem of regressing offsets on a large scale, high-dimensional vector space.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Candidate Classification</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Based on the coarse prediction, we evaluate the candidate by classifying positive and negative predictions, where the predicted score indicates the probability of positive prediction. We use Object Keypoint Similarity (OKS) as a metric to evaluate the similarity between predicted pose and GT pose. Predictions with OKS higher than 0.6 indicate positive, lower than 0.5 indicate negative. OKS is a collection of distances between every two joints of the same category (<em id="S3.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, the similarity between two point sets). It provides a point-wise evaluation for the evaluation of poses.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.2" class="ltx_p">The classification branch after FPN is used to generate pose embedding for candidate scoring, which is composed of two <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mrow id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.SSS2.p2.1.m1.1.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p2.1.m1.1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS2.p2.1.m1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><apply id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1"><times id="S3.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.2">3</cn><cn type="integer" id="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">3\times 3</annotation></semantics></math> conv layers and a deformable convolution layer. A following <math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><mrow id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml"><mn id="S3.SS2.SSS2.p2.2.m2.1.1.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p2.2.m2.1.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS2.p2.2.m2.1.1.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><apply id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1"><times id="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.2">1</cn><cn type="integer" id="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">1\times 1</annotation></semantics></math> conv layer is applied to estimate the score. The predicted scores and poses are used in NMS to suppress redundant predictions.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Intermediate Supervision</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.3" class="ltx_p">The feature map of FPN is considered a mapping of the person instance, where we extract the object representation to generate keypoint-aware pose embeddings. To force the FPN to be sensitive to body joints, followingÂ <cite class="ltx_cite ltx_citemacro_cite">Newell <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2016</a>); Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib36" title="" class="ltx_ref">2016</a>)</cite>, we utilize intermediate supervision with GT joints. Specifically, we add a branch consisted of a <math id="S3.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mrow id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS3.p1.1.m1.1.1.2" xref="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS3.p1.1.m1.1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS3.p1.1.m1.1.1.3" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><apply id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1"><times id="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.2">3</cn><cn type="integer" id="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">3\times 3</annotation></semantics></math> conv layer and a <math id="S3.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS2.SSS3.p1.2.m2.1a"><mrow id="S3.SS2.SSS3.p1.2.m2.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.cmml"><mn id="S3.SS2.SSS3.p1.2.m2.1.1.2" xref="S3.SS2.SSS3.p1.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS3.p1.2.m2.1.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS3.p1.2.m2.1.1.3" xref="S3.SS2.SSS3.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.2.m2.1b"><apply id="S3.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1"><times id="S3.SS2.SSS3.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.SSS3.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1.2">1</cn><cn type="integer" id="S3.SS2.SSS3.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.2.m2.1c">1\times 1</annotation></semantics></math> conv layer, which takes the multi-level feature maps of FPN as input and generates multi-level heatmaps with the same resolutions. The heatmaps indicate the probabilities of joints over spatial locations, where the number of channels is equal to the number of joints. Naturally, for the heatmaps of each level, we generate 2D Gaussian maps (with a standard deviation of 2 pixels) centered on the GT joint location as targets. We minimize the <math id="S3.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S3.SS2.SSS3.p1.3.m3.1a"><msub id="S3.SS2.SSS3.p1.3.m3.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS3.p1.3.m3.1.1.2" xref="S3.SS2.SSS3.p1.3.m3.1.1.2.cmml">l</mi><mn id="S3.SS2.SSS3.p1.3.m3.1.1.3" xref="S3.SS2.SSS3.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.3.m3.1b"><apply id="S3.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.2">ğ‘™</ci><cn type="integer" id="S3.SS2.SSS3.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.3.m3.1c">l_{2}</annotation></semantics></math> loss between predicted heatmaps and targets during training.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">We note that the intermediate supervision is a cost-free improvement because this branch is not involved in testing. We demonstrate the benefit of it in ablation study (SectionÂ <a href="#S4.SS4" title="4.4 Ablation study â€£ 4 Experiments â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>).</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2107.10466/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="179" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Upper bounds of OKS-based NMS and IoU-based NMS on CrowdPose.</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Training and Inference</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.1" class="ltx_p">In the training phase, we use different losses and assigning strategies in the bypass, the classification branch, and the regression branch. The loss of network can be summarized as:</p>
</div>
<div id="S3.SS2.SSS4.p2" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.81" class="ltx_Math" alttext="\begin{split}Loss=L_{2}(GTPose,\delta^{c})+L_{2}(GTPose-\delta^{c},\delta^{f})\\
+FL(GT,scores)+L_{2}(GM,Heatmaps)\end{split}" display="block"><semantics id="S3.E4.m1.81a"><mtable displaystyle="true" rowspacing="0pt" id="S3.E4.m1.81.81.16" xref="S3.E4.m1.73.73.8.cmml"><mtr id="S3.E4.m1.81.81.16a" xref="S3.E4.m1.73.73.8.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E4.m1.81.81.16b" xref="S3.E4.m1.73.73.8.cmml"><mrow id="S3.E4.m1.77.77.12.69.39.39" xref="S3.E4.m1.73.73.8.cmml"><mrow id="S3.E4.m1.77.77.12.69.39.39.40" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.77.77.12.69.39.39.40.1" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.77.77.12.69.39.39.40.1a" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.3.3.3.3.3.3" xref="S3.E4.m1.3.3.3.3.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.77.77.12.69.39.39.40.1b" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.4.4.4.4.4.4" xref="S3.E4.m1.4.4.4.4.4.4.cmml">s</mi></mrow><mo id="S3.E4.m1.5.5.5.5.5.5" xref="S3.E4.m1.5.5.5.5.5.5.cmml">=</mo><mrow id="S3.E4.m1.77.77.12.69.39.39.39" xref="S3.E4.m1.73.73.8.cmml"><mrow id="S3.E4.m1.75.75.10.67.37.37.37.2" xref="S3.E4.m1.73.73.8.cmml"><msub id="S3.E4.m1.75.75.10.67.37.37.37.2.4" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.6.6.6.6.6.6" xref="S3.E4.m1.6.6.6.6.6.6.cmml">L</mi><mn id="S3.E4.m1.7.7.7.7.7.7.1" xref="S3.E4.m1.7.7.7.7.7.7.1.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.75.75.10.67.37.37.37.2.3" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mrow id="S3.E4.m1.75.75.10.67.37.37.37.2.2.2" xref="S3.E4.m1.73.73.8.cmml"><mo stretchy="false" id="S3.E4.m1.8.8.8.8.8.8" xref="S3.E4.m1.73.73.8.cmml">(</mo><mrow id="S3.E4.m1.74.74.9.66.36.36.36.1.1.1.1" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.9.9.9.9.9.9" xref="S3.E4.m1.9.9.9.9.9.9.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.74.74.9.66.36.36.36.1.1.1.1.1" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.10.10.10.10.10.10" xref="S3.E4.m1.10.10.10.10.10.10.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.74.74.9.66.36.36.36.1.1.1.1.1a" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.11.11.11.11.11.11" xref="S3.E4.m1.11.11.11.11.11.11.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.74.74.9.66.36.36.36.1.1.1.1.1b" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.12.12.12.12.12.12" xref="S3.E4.m1.12.12.12.12.12.12.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.74.74.9.66.36.36.36.1.1.1.1.1c" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.13.13.13.13.13.13" xref="S3.E4.m1.13.13.13.13.13.13.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.74.74.9.66.36.36.36.1.1.1.1.1d" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.14.14.14.14.14.14" xref="S3.E4.m1.14.14.14.14.14.14.cmml">e</mi></mrow><mo id="S3.E4.m1.15.15.15.15.15.15" xref="S3.E4.m1.73.73.8.cmml">,</mo><msup id="S3.E4.m1.75.75.10.67.37.37.37.2.2.2.2" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.16.16.16.16.16.16" xref="S3.E4.m1.16.16.16.16.16.16.cmml">Î´</mi><mi id="S3.E4.m1.17.17.17.17.17.17.1" xref="S3.E4.m1.17.17.17.17.17.17.1.cmml">c</mi></msup><mo stretchy="false" id="S3.E4.m1.18.18.18.18.18.18" xref="S3.E4.m1.73.73.8.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.19.19.19.19.19.19" xref="S3.E4.m1.19.19.19.19.19.19.cmml">+</mo><mrow id="S3.E4.m1.77.77.12.69.39.39.39.4" xref="S3.E4.m1.73.73.8.cmml"><msub id="S3.E4.m1.77.77.12.69.39.39.39.4.4" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.20.20.20.20.20.20" xref="S3.E4.m1.20.20.20.20.20.20.cmml">L</mi><mn id="S3.E4.m1.21.21.21.21.21.21.1" xref="S3.E4.m1.21.21.21.21.21.21.1.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.77.77.12.69.39.39.39.4.3" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mrow id="S3.E4.m1.77.77.12.69.39.39.39.4.2.2" xref="S3.E4.m1.73.73.8.cmml"><mo stretchy="false" id="S3.E4.m1.22.22.22.22.22.22" xref="S3.E4.m1.73.73.8.cmml">(</mo><mrow id="S3.E4.m1.76.76.11.68.38.38.38.3.1.1.1" xref="S3.E4.m1.73.73.8.cmml"><mrow id="S3.E4.m1.76.76.11.68.38.38.38.3.1.1.1.1" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.23.23.23.23.23.23" xref="S3.E4.m1.23.23.23.23.23.23.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.76.76.11.68.38.38.38.3.1.1.1.1.1" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.24.24.24.24.24.24" xref="S3.E4.m1.24.24.24.24.24.24.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.76.76.11.68.38.38.38.3.1.1.1.1.1a" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.25.25.25.25.25.25" xref="S3.E4.m1.25.25.25.25.25.25.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.76.76.11.68.38.38.38.3.1.1.1.1.1b" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.26.26.26.26.26.26" xref="S3.E4.m1.26.26.26.26.26.26.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.76.76.11.68.38.38.38.3.1.1.1.1.1c" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.27.27.27.27.27.27" xref="S3.E4.m1.27.27.27.27.27.27.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.76.76.11.68.38.38.38.3.1.1.1.1.1d" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.28.28.28.28.28.28" xref="S3.E4.m1.28.28.28.28.28.28.cmml">e</mi></mrow><mo id="S3.E4.m1.29.29.29.29.29.29" xref="S3.E4.m1.29.29.29.29.29.29.cmml">âˆ’</mo><msup id="S3.E4.m1.76.76.11.68.38.38.38.3.1.1.1.2" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.30.30.30.30.30.30" xref="S3.E4.m1.30.30.30.30.30.30.cmml">Î´</mi><mi id="S3.E4.m1.31.31.31.31.31.31.1" xref="S3.E4.m1.31.31.31.31.31.31.1.cmml">c</mi></msup></mrow><mo id="S3.E4.m1.32.32.32.32.32.32" xref="S3.E4.m1.73.73.8.cmml">,</mo><msup id="S3.E4.m1.77.77.12.69.39.39.39.4.2.2.2" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.33.33.33.33.33.33" xref="S3.E4.m1.33.33.33.33.33.33.cmml">Î´</mi><mi id="S3.E4.m1.34.34.34.34.34.34.1" xref="S3.E4.m1.34.34.34.34.34.34.1.cmml">f</mi></msup><mo stretchy="false" id="S3.E4.m1.35.35.35.35.35.35" xref="S3.E4.m1.73.73.8.cmml">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E4.m1.81.81.16c" xref="S3.E4.m1.73.73.8.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E4.m1.81.81.16d" xref="S3.E4.m1.73.73.8.cmml"><mrow id="S3.E4.m1.81.81.16.73.34.34" xref="S3.E4.m1.73.73.8.cmml"><mrow id="S3.E4.m1.79.79.14.71.32.32.32" xref="S3.E4.m1.73.73.8.cmml"><mo id="S3.E4.m1.79.79.14.71.32.32.32a" xref="S3.E4.m1.73.73.8.cmml">+</mo><mrow id="S3.E4.m1.79.79.14.71.32.32.32.2" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.37.37.37.2.2.2" xref="S3.E4.m1.37.37.37.2.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.79.79.14.71.32.32.32.2.3" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.38.38.38.3.3.3" xref="S3.E4.m1.38.38.38.3.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.79.79.14.71.32.32.32.2.3a" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mrow id="S3.E4.m1.79.79.14.71.32.32.32.2.2.2" xref="S3.E4.m1.73.73.8.cmml"><mo stretchy="false" id="S3.E4.m1.39.39.39.4.4.4" xref="S3.E4.m1.73.73.8.cmml">(</mo><mrow id="S3.E4.m1.78.78.13.70.31.31.31.1.1.1.1" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.40.40.40.5.5.5" xref="S3.E4.m1.40.40.40.5.5.5.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.78.78.13.70.31.31.31.1.1.1.1.1" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.41.41.41.6.6.6" xref="S3.E4.m1.41.41.41.6.6.6.cmml">T</mi></mrow><mo id="S3.E4.m1.42.42.42.7.7.7" xref="S3.E4.m1.73.73.8.cmml">,</mo><mrow id="S3.E4.m1.79.79.14.71.32.32.32.2.2.2.2" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.43.43.43.8.8.8" xref="S3.E4.m1.43.43.43.8.8.8.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.79.79.14.71.32.32.32.2.2.2.2.1" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.44.44.44.9.9.9" xref="S3.E4.m1.44.44.44.9.9.9.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.79.79.14.71.32.32.32.2.2.2.2.1a" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.45.45.45.10.10.10" xref="S3.E4.m1.45.45.45.10.10.10.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.79.79.14.71.32.32.32.2.2.2.2.1b" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.46.46.46.11.11.11" xref="S3.E4.m1.46.46.46.11.11.11.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.79.79.14.71.32.32.32.2.2.2.2.1c" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.47.47.47.12.12.12" xref="S3.E4.m1.47.47.47.12.12.12.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.79.79.14.71.32.32.32.2.2.2.2.1d" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.48.48.48.13.13.13" xref="S3.E4.m1.48.48.48.13.13.13.cmml">s</mi></mrow><mo stretchy="false" id="S3.E4.m1.49.49.49.14.14.14" xref="S3.E4.m1.73.73.8.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.50.50.50.15.15.15" xref="S3.E4.m1.73.73.8.cmml">+</mo><mrow id="S3.E4.m1.81.81.16.73.34.34.34" xref="S3.E4.m1.73.73.8.cmml"><msub id="S3.E4.m1.81.81.16.73.34.34.34.4" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.51.51.51.16.16.16" xref="S3.E4.m1.51.51.51.16.16.16.cmml">L</mi><mn id="S3.E4.m1.52.52.52.17.17.17.1" xref="S3.E4.m1.52.52.52.17.17.17.1.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.81.81.16.73.34.34.34.3" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mrow id="S3.E4.m1.81.81.16.73.34.34.34.2.2" xref="S3.E4.m1.73.73.8.cmml"><mo stretchy="false" id="S3.E4.m1.53.53.53.18.18.18" xref="S3.E4.m1.73.73.8.cmml">(</mo><mrow id="S3.E4.m1.80.80.15.72.33.33.33.1.1.1" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.54.54.54.19.19.19" xref="S3.E4.m1.54.54.54.19.19.19.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.80.80.15.72.33.33.33.1.1.1.1" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.55.55.55.20.20.20" xref="S3.E4.m1.55.55.55.20.20.20.cmml">M</mi></mrow><mo id="S3.E4.m1.56.56.56.21.21.21" xref="S3.E4.m1.73.73.8.cmml">,</mo><mrow id="S3.E4.m1.81.81.16.73.34.34.34.2.2.2" xref="S3.E4.m1.73.73.8.cmml"><mi id="S3.E4.m1.57.57.57.22.22.22" xref="S3.E4.m1.57.57.57.22.22.22.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.81.81.16.73.34.34.34.2.2.2.1" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.58.58.58.23.23.23" xref="S3.E4.m1.58.58.58.23.23.23.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.81.81.16.73.34.34.34.2.2.2.1a" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.59.59.59.24.24.24" xref="S3.E4.m1.59.59.59.24.24.24.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.81.81.16.73.34.34.34.2.2.2.1b" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.60.60.60.25.25.25" xref="S3.E4.m1.60.60.60.25.25.25.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.81.81.16.73.34.34.34.2.2.2.1c" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.61.61.61.26.26.26" xref="S3.E4.m1.61.61.61.26.26.26.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.81.81.16.73.34.34.34.2.2.2.1d" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.62.62.62.27.27.27" xref="S3.E4.m1.62.62.62.27.27.27.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.81.81.16.73.34.34.34.2.2.2.1e" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.63.63.63.28.28.28" xref="S3.E4.m1.63.63.63.28.28.28.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.81.81.16.73.34.34.34.2.2.2.1f" xref="S3.E4.m1.73.73.8.cmml">â€‹</mo><mi id="S3.E4.m1.64.64.64.29.29.29" xref="S3.E4.m1.64.64.64.29.29.29.cmml">s</mi></mrow><mo stretchy="false" id="S3.E4.m1.65.65.65.30.30.30" xref="S3.E4.m1.73.73.8.cmml">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E4.m1.81b"><apply id="S3.E4.m1.73.73.8.cmml" xref="S3.E4.m1.81.81.16"><eq id="S3.E4.m1.5.5.5.5.5.5.cmml" xref="S3.E4.m1.5.5.5.5.5.5"></eq><apply id="S3.E4.m1.73.73.8.10.cmml" xref="S3.E4.m1.81.81.16"><times id="S3.E4.m1.73.73.8.10.1.cmml" xref="S3.E4.m1.81.81.16"></times><ci id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1">ğ¿</ci><ci id="S3.E4.m1.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2">ğ‘œ</ci><ci id="S3.E4.m1.3.3.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3.3.3">ğ‘ </ci><ci id="S3.E4.m1.4.4.4.4.4.4.cmml" xref="S3.E4.m1.4.4.4.4.4.4">ğ‘ </ci></apply><apply id="S3.E4.m1.73.73.8.8.cmml" xref="S3.E4.m1.81.81.16"><plus id="S3.E4.m1.19.19.19.19.19.19.cmml" xref="S3.E4.m1.19.19.19.19.19.19"></plus><apply id="S3.E4.m1.67.67.2.2.2.cmml" xref="S3.E4.m1.81.81.16"><times id="S3.E4.m1.67.67.2.2.2.3.cmml" xref="S3.E4.m1.81.81.16"></times><apply id="S3.E4.m1.67.67.2.2.2.4.cmml" xref="S3.E4.m1.81.81.16"><csymbol cd="ambiguous" id="S3.E4.m1.67.67.2.2.2.4.1.cmml" xref="S3.E4.m1.81.81.16">subscript</csymbol><ci id="S3.E4.m1.6.6.6.6.6.6.cmml" xref="S3.E4.m1.6.6.6.6.6.6">ğ¿</ci><cn type="integer" id="S3.E4.m1.7.7.7.7.7.7.1.cmml" xref="S3.E4.m1.7.7.7.7.7.7.1">2</cn></apply><interval closure="open" id="S3.E4.m1.67.67.2.2.2.2.3.cmml" xref="S3.E4.m1.81.81.16"><apply id="S3.E4.m1.66.66.1.1.1.1.1.1.cmml" xref="S3.E4.m1.81.81.16"><times id="S3.E4.m1.66.66.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.81.81.16"></times><ci id="S3.E4.m1.9.9.9.9.9.9.cmml" xref="S3.E4.m1.9.9.9.9.9.9">ğº</ci><ci id="S3.E4.m1.10.10.10.10.10.10.cmml" xref="S3.E4.m1.10.10.10.10.10.10">ğ‘‡</ci><ci id="S3.E4.m1.11.11.11.11.11.11.cmml" xref="S3.E4.m1.11.11.11.11.11.11">ğ‘ƒ</ci><ci id="S3.E4.m1.12.12.12.12.12.12.cmml" xref="S3.E4.m1.12.12.12.12.12.12">ğ‘œ</ci><ci id="S3.E4.m1.13.13.13.13.13.13.cmml" xref="S3.E4.m1.13.13.13.13.13.13">ğ‘ </ci><ci id="S3.E4.m1.14.14.14.14.14.14.cmml" xref="S3.E4.m1.14.14.14.14.14.14">ğ‘’</ci></apply><apply id="S3.E4.m1.67.67.2.2.2.2.2.2.cmml" xref="S3.E4.m1.81.81.16"><csymbol cd="ambiguous" id="S3.E4.m1.67.67.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.81.81.16">superscript</csymbol><ci id="S3.E4.m1.16.16.16.16.16.16.cmml" xref="S3.E4.m1.16.16.16.16.16.16">ğ›¿</ci><ci id="S3.E4.m1.17.17.17.17.17.17.1.cmml" xref="S3.E4.m1.17.17.17.17.17.17.1">ğ‘</ci></apply></interval></apply><apply id="S3.E4.m1.69.69.4.4.4.cmml" xref="S3.E4.m1.81.81.16"><times id="S3.E4.m1.69.69.4.4.4.3.cmml" xref="S3.E4.m1.81.81.16"></times><apply id="S3.E4.m1.69.69.4.4.4.4.cmml" xref="S3.E4.m1.81.81.16"><csymbol cd="ambiguous" id="S3.E4.m1.69.69.4.4.4.4.1.cmml" xref="S3.E4.m1.81.81.16">subscript</csymbol><ci id="S3.E4.m1.20.20.20.20.20.20.cmml" xref="S3.E4.m1.20.20.20.20.20.20">ğ¿</ci><cn type="integer" id="S3.E4.m1.21.21.21.21.21.21.1.cmml" xref="S3.E4.m1.21.21.21.21.21.21.1">2</cn></apply><interval closure="open" id="S3.E4.m1.69.69.4.4.4.2.3.cmml" xref="S3.E4.m1.81.81.16"><apply id="S3.E4.m1.68.68.3.3.3.1.1.1.cmml" xref="S3.E4.m1.81.81.16"><minus id="S3.E4.m1.29.29.29.29.29.29.cmml" xref="S3.E4.m1.29.29.29.29.29.29"></minus><apply id="S3.E4.m1.68.68.3.3.3.1.1.1.2.cmml" xref="S3.E4.m1.81.81.16"><times id="S3.E4.m1.68.68.3.3.3.1.1.1.2.1.cmml" xref="S3.E4.m1.81.81.16"></times><ci id="S3.E4.m1.23.23.23.23.23.23.cmml" xref="S3.E4.m1.23.23.23.23.23.23">ğº</ci><ci id="S3.E4.m1.24.24.24.24.24.24.cmml" xref="S3.E4.m1.24.24.24.24.24.24">ğ‘‡</ci><ci id="S3.E4.m1.25.25.25.25.25.25.cmml" xref="S3.E4.m1.25.25.25.25.25.25">ğ‘ƒ</ci><ci id="S3.E4.m1.26.26.26.26.26.26.cmml" xref="S3.E4.m1.26.26.26.26.26.26">ğ‘œ</ci><ci id="S3.E4.m1.27.27.27.27.27.27.cmml" xref="S3.E4.m1.27.27.27.27.27.27">ğ‘ </ci><ci id="S3.E4.m1.28.28.28.28.28.28.cmml" xref="S3.E4.m1.28.28.28.28.28.28">ğ‘’</ci></apply><apply id="S3.E4.m1.68.68.3.3.3.1.1.1.3.cmml" xref="S3.E4.m1.81.81.16"><csymbol cd="ambiguous" id="S3.E4.m1.68.68.3.3.3.1.1.1.3.1.cmml" xref="S3.E4.m1.81.81.16">superscript</csymbol><ci id="S3.E4.m1.30.30.30.30.30.30.cmml" xref="S3.E4.m1.30.30.30.30.30.30">ğ›¿</ci><ci id="S3.E4.m1.31.31.31.31.31.31.1.cmml" xref="S3.E4.m1.31.31.31.31.31.31.1">ğ‘</ci></apply></apply><apply id="S3.E4.m1.69.69.4.4.4.2.2.2.cmml" xref="S3.E4.m1.81.81.16"><csymbol cd="ambiguous" id="S3.E4.m1.69.69.4.4.4.2.2.2.1.cmml" xref="S3.E4.m1.81.81.16">superscript</csymbol><ci id="S3.E4.m1.33.33.33.33.33.33.cmml" xref="S3.E4.m1.33.33.33.33.33.33">ğ›¿</ci><ci id="S3.E4.m1.34.34.34.34.34.34.1.cmml" xref="S3.E4.m1.34.34.34.34.34.34.1">ğ‘“</ci></apply></interval></apply><apply id="S3.E4.m1.71.71.6.6.6.cmml" xref="S3.E4.m1.81.81.16"><times id="S3.E4.m1.71.71.6.6.6.3.cmml" xref="S3.E4.m1.81.81.16"></times><ci id="S3.E4.m1.37.37.37.2.2.2.cmml" xref="S3.E4.m1.37.37.37.2.2.2">ğ¹</ci><ci id="S3.E4.m1.38.38.38.3.3.3.cmml" xref="S3.E4.m1.38.38.38.3.3.3">ğ¿</ci><interval closure="open" id="S3.E4.m1.71.71.6.6.6.2.3.cmml" xref="S3.E4.m1.81.81.16"><apply id="S3.E4.m1.70.70.5.5.5.1.1.1.cmml" xref="S3.E4.m1.81.81.16"><times id="S3.E4.m1.70.70.5.5.5.1.1.1.1.cmml" xref="S3.E4.m1.81.81.16"></times><ci id="S3.E4.m1.40.40.40.5.5.5.cmml" xref="S3.E4.m1.40.40.40.5.5.5">ğº</ci><ci id="S3.E4.m1.41.41.41.6.6.6.cmml" xref="S3.E4.m1.41.41.41.6.6.6">ğ‘‡</ci></apply><apply id="S3.E4.m1.71.71.6.6.6.2.2.2.cmml" xref="S3.E4.m1.81.81.16"><times id="S3.E4.m1.71.71.6.6.6.2.2.2.1.cmml" xref="S3.E4.m1.81.81.16"></times><ci id="S3.E4.m1.43.43.43.8.8.8.cmml" xref="S3.E4.m1.43.43.43.8.8.8">ğ‘ </ci><ci id="S3.E4.m1.44.44.44.9.9.9.cmml" xref="S3.E4.m1.44.44.44.9.9.9">ğ‘</ci><ci id="S3.E4.m1.45.45.45.10.10.10.cmml" xref="S3.E4.m1.45.45.45.10.10.10">ğ‘œ</ci><ci id="S3.E4.m1.46.46.46.11.11.11.cmml" xref="S3.E4.m1.46.46.46.11.11.11">ğ‘Ÿ</ci><ci id="S3.E4.m1.47.47.47.12.12.12.cmml" xref="S3.E4.m1.47.47.47.12.12.12">ğ‘’</ci><ci id="S3.E4.m1.48.48.48.13.13.13.cmml" xref="S3.E4.m1.48.48.48.13.13.13">ğ‘ </ci></apply></interval></apply><apply id="S3.E4.m1.73.73.8.8.8.cmml" xref="S3.E4.m1.81.81.16"><times id="S3.E4.m1.73.73.8.8.8.3.cmml" xref="S3.E4.m1.81.81.16"></times><apply id="S3.E4.m1.73.73.8.8.8.4.cmml" xref="S3.E4.m1.81.81.16"><csymbol cd="ambiguous" id="S3.E4.m1.73.73.8.8.8.4.1.cmml" xref="S3.E4.m1.81.81.16">subscript</csymbol><ci id="S3.E4.m1.51.51.51.16.16.16.cmml" xref="S3.E4.m1.51.51.51.16.16.16">ğ¿</ci><cn type="integer" id="S3.E4.m1.52.52.52.17.17.17.1.cmml" xref="S3.E4.m1.52.52.52.17.17.17.1">2</cn></apply><interval closure="open" id="S3.E4.m1.73.73.8.8.8.2.3.cmml" xref="S3.E4.m1.81.81.16"><apply id="S3.E4.m1.72.72.7.7.7.1.1.1.cmml" xref="S3.E4.m1.81.81.16"><times id="S3.E4.m1.72.72.7.7.7.1.1.1.1.cmml" xref="S3.E4.m1.81.81.16"></times><ci id="S3.E4.m1.54.54.54.19.19.19.cmml" xref="S3.E4.m1.54.54.54.19.19.19">ğº</ci><ci id="S3.E4.m1.55.55.55.20.20.20.cmml" xref="S3.E4.m1.55.55.55.20.20.20">ğ‘€</ci></apply><apply id="S3.E4.m1.73.73.8.8.8.2.2.2.cmml" xref="S3.E4.m1.81.81.16"><times id="S3.E4.m1.73.73.8.8.8.2.2.2.1.cmml" xref="S3.E4.m1.81.81.16"></times><ci id="S3.E4.m1.57.57.57.22.22.22.cmml" xref="S3.E4.m1.57.57.57.22.22.22">ğ»</ci><ci id="S3.E4.m1.58.58.58.23.23.23.cmml" xref="S3.E4.m1.58.58.58.23.23.23">ğ‘’</ci><ci id="S3.E4.m1.59.59.59.24.24.24.cmml" xref="S3.E4.m1.59.59.59.24.24.24">ğ‘</ci><ci id="S3.E4.m1.60.60.60.25.25.25.cmml" xref="S3.E4.m1.60.60.60.25.25.25">ğ‘¡</ci><ci id="S3.E4.m1.61.61.61.26.26.26.cmml" xref="S3.E4.m1.61.61.61.26.26.26">ğ‘š</ci><ci id="S3.E4.m1.62.62.62.27.27.27.cmml" xref="S3.E4.m1.62.62.62.27.27.27">ğ‘</ci><ci id="S3.E4.m1.63.63.63.28.28.28.cmml" xref="S3.E4.m1.63.63.63.28.28.28">ğ‘</ci><ci id="S3.E4.m1.64.64.64.29.29.29.cmml" xref="S3.E4.m1.64.64.64.29.29.29">ğ‘ </ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.81c">\begin{split}Loss=L_{2}(GTPose,\delta^{c})+L_{2}(GTPose-\delta^{c},\delta^{f})\\
+FL(GT,scores)+L_{2}(GM,Heatmaps)\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS4.p3" class="ltx_para">
<p id="S3.SS2.SSS4.p3.3" class="ltx_p">On the right side of the equation, the first item is loss of the bypass. The bypass is responsible for providing a coarse prediction of each candidate. We use GT joints as supervision. Each GT pose is allocated to a level in FPN according to its size and assigned to a candidate at the same level closest to the centroid of the GT pose. The centroid is calculated by mapping the spatial locations of the joints to feature maps and averaging the coordinates. We demonstrate the advantage of the centroid of joints compared with the centroid of the bounding box in SectionÂ <a href="#S4.SS4" title="4.4 Ablation study â€£ 4 Experiments â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>. Candidates assigned with targets are trained with <math id="S3.SS2.SSS4.p3.1.m1.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S3.SS2.SSS4.p3.1.m1.1a"><msub id="S3.SS2.SSS4.p3.1.m1.1.1" xref="S3.SS2.SSS4.p3.1.m1.1.1.cmml"><mi id="S3.SS2.SSS4.p3.1.m1.1.1.2" xref="S3.SS2.SSS4.p3.1.m1.1.1.2.cmml">l</mi><mn id="S3.SS2.SSS4.p3.1.m1.1.1.3" xref="S3.SS2.SSS4.p3.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p3.1.m1.1b"><apply id="S3.SS2.SSS4.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS4.p3.1.m1.1.1.2">ğ‘™</ci><cn type="integer" id="S3.SS2.SSS4.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS4.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p3.1.m1.1c">l_{2}</annotation></semantics></math> loss. The second item is the loss of the regression branch, which is responsible for predicting more delicate poses based on the coarse predictions. We compute OKS between the coarse prediction and every GT pose, then assign the larges OKS to the candidate. Candidates with OKS higher than 0.7 are trained with <math id="S3.SS2.SSS4.p3.2.m2.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S3.SS2.SSS4.p3.2.m2.1a"><msub id="S3.SS2.SSS4.p3.2.m2.1.1" xref="S3.SS2.SSS4.p3.2.m2.1.1.cmml"><mi id="S3.SS2.SSS4.p3.2.m2.1.1.2" xref="S3.SS2.SSS4.p3.2.m2.1.1.2.cmml">l</mi><mn id="S3.SS2.SSS4.p3.2.m2.1.1.3" xref="S3.SS2.SSS4.p3.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p3.2.m2.1b"><apply id="S3.SS2.SSS4.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS4.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS4.p3.2.m2.1.1.2">ğ‘™</ci><cn type="integer" id="S3.SS2.SSS4.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS4.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p3.2.m2.1c">l_{2}</annotation></semantics></math> loss. The third item is the loss of the classification branch. Both positive and negative candidates are trained with focal lossÂ <cite class="ltx_cite ltx_citemacro_cite">Lin <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite> (FL) to address the large class imbalance issue. The last item is loss of intermediate supervision, which is <math id="S3.SS2.SSS4.p3.3.m3.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S3.SS2.SSS4.p3.3.m3.1a"><msub id="S3.SS2.SSS4.p3.3.m3.1.1" xref="S3.SS2.SSS4.p3.3.m3.1.1.cmml"><mi id="S3.SS2.SSS4.p3.3.m3.1.1.2" xref="S3.SS2.SSS4.p3.3.m3.1.1.2.cmml">l</mi><mn id="S3.SS2.SSS4.p3.3.m3.1.1.3" xref="S3.SS2.SSS4.p3.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p3.3.m3.1b"><apply id="S3.SS2.SSS4.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p3.3.m3.1.1.1.cmml" xref="S3.SS2.SSS4.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p3.3.m3.1.1.2.cmml" xref="S3.SS2.SSS4.p3.3.m3.1.1.2">ğ‘™</ci><cn type="integer" id="S3.SS2.SSS4.p3.3.m3.1.1.3.cmml" xref="S3.SS2.SSS4.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p3.3.m3.1c">l_{2}</annotation></semantics></math> loss between Gaussian maps (GM) and predicted heatmaps.</p>
</div>
<div id="S3.SS2.SSS4.p4" class="ltx_para">
<p id="S3.SS2.SSS4.p4.1" class="ltx_p">For the inference, we predict each candidateâ€™s pose and confidence score, then implement OKS-based NMSÂ <cite class="ltx_cite ltx_citemacro_cite">Papandreou <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite> to remove redundant predictions.</p>
</div>
<div id="S3.SS2.SSS4.p5" class="ltx_para">
<p id="S3.SS2.SSS4.p5.1" class="ltx_p">Most of the modern detection systems use the IoU of bounding boxes as the measurement to evaluate the overlap of predictions. When two predictions have an overlap over the predefined threshold, the prediction with a lower confidence score is suppressed. One problem is that the IoU-based NMS might suppress positive samples that occlude heavily, which is not suitable for multi-person pose estimation, especially for the crowd scenes.</p>
</div>
<div id="S3.SS2.SSS4.p6" class="ltx_para">
<p id="S3.SS2.SSS4.p6.1" class="ltx_p">In our system, the overlap is measured in terms of OKS. OKS is dependent on distances between joints, leading to a more discriminate measurement of the similarity. To examine the upper bound on persons with a high crowd index, we use GT keypoints in the CrowdPose test set as input, performing OKS-based NMS and IoU-based NMS on different thresholds, respectively. AP Hard is adopted as the metric that contains the most crowd. As shown in FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2.3 Intermediate Supervision â€£ 3.2 PoseDet â€£ 3 Approach â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the IoU-based NMS wrongly suppress more positive samples than OKS-based NMS, showing that the OKS is a better measurement to distinguish occluded persons.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.6.6" class="ltx_tr">
<th id="S4.T1.6.6.7" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T1.6.6.8" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">Backbone</th>
<th id="S4.T1.6.6.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">Input size</th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt"><span id="S4.T1.1.1.1.1" class="ltx_text ltx_markedasmath">FPS</span></th>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="S4.T1.2.2.2.1" class="ltx_text ltx_markedasmath">AP</span></td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_right ltx_border_tt"><math id="S4.T1.3.3.3.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{50}}" display="inline"><semantics id="S4.T1.3.3.3.m1.1a"><msup id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml"><mtext id="S4.T1.3.3.3.m1.1.1.2" xref="S4.T1.3.3.3.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T1.3.3.3.m1.1.1.3" xref="S4.T1.3.3.3.m1.1.1.3a.cmml">50</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><apply id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.3.m1.1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">superscript</csymbol><ci id="S4.T1.3.3.3.m1.1.1.2a.cmml" xref="S4.T1.3.3.3.m1.1.1.2"><mtext id="S4.T1.3.3.3.m1.1.1.2.cmml" xref="S4.T1.3.3.3.m1.1.1.2">AP</mtext></ci><ci id="S4.T1.3.3.3.m1.1.1.3a.cmml" xref="S4.T1.3.3.3.m1.1.1.3"><mtext mathsize="70%" id="S4.T1.3.3.3.m1.1.1.3.cmml" xref="S4.T1.3.3.3.m1.1.1.3">50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\text{AP}^{\text{50}}</annotation></semantics></math></td>
<td id="S4.T1.4.4.4" class="ltx_td ltx_align_right ltx_border_tt"><math id="S4.T1.4.4.4.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{75}}" display="inline"><semantics id="S4.T1.4.4.4.m1.1a"><msup id="S4.T1.4.4.4.m1.1.1" xref="S4.T1.4.4.4.m1.1.1.cmml"><mtext id="S4.T1.4.4.4.m1.1.1.2" xref="S4.T1.4.4.4.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T1.4.4.4.m1.1.1.3" xref="S4.T1.4.4.4.m1.1.1.3a.cmml">75</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.m1.1b"><apply id="S4.T1.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.4.4.4.m1.1.1.1.cmml" xref="S4.T1.4.4.4.m1.1.1">superscript</csymbol><ci id="S4.T1.4.4.4.m1.1.1.2a.cmml" xref="S4.T1.4.4.4.m1.1.1.2"><mtext id="S4.T1.4.4.4.m1.1.1.2.cmml" xref="S4.T1.4.4.4.m1.1.1.2">AP</mtext></ci><ci id="S4.T1.4.4.4.m1.1.1.3a.cmml" xref="S4.T1.4.4.4.m1.1.1.3"><mtext mathsize="70%" id="S4.T1.4.4.4.m1.1.1.3.cmml" xref="S4.T1.4.4.4.m1.1.1.3">75</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.m1.1c">\text{AP}^{\text{75}}</annotation></semantics></math></td>
<td id="S4.T1.5.5.5" class="ltx_td ltx_align_right ltx_border_tt"><math id="S4.T1.5.5.5.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{M}}" display="inline"><semantics id="S4.T1.5.5.5.m1.1a"><msup id="S4.T1.5.5.5.m1.1.1" xref="S4.T1.5.5.5.m1.1.1.cmml"><mtext id="S4.T1.5.5.5.m1.1.1.2" xref="S4.T1.5.5.5.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T1.5.5.5.m1.1.1.3" xref="S4.T1.5.5.5.m1.1.1.3a.cmml">M</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.m1.1b"><apply id="S4.T1.5.5.5.m1.1.1.cmml" xref="S4.T1.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.5.5.5.m1.1.1.1.cmml" xref="S4.T1.5.5.5.m1.1.1">superscript</csymbol><ci id="S4.T1.5.5.5.m1.1.1.2a.cmml" xref="S4.T1.5.5.5.m1.1.1.2"><mtext id="S4.T1.5.5.5.m1.1.1.2.cmml" xref="S4.T1.5.5.5.m1.1.1.2">AP</mtext></ci><ci id="S4.T1.5.5.5.m1.1.1.3a.cmml" xref="S4.T1.5.5.5.m1.1.1.3"><mtext mathsize="70%" id="S4.T1.5.5.5.m1.1.1.3.cmml" xref="S4.T1.5.5.5.m1.1.1.3">M</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.m1.1c">\text{AP}^{\text{M}}</annotation></semantics></math></td>
<td id="S4.T1.6.6.6" class="ltx_td ltx_align_right ltx_border_tt"><math id="S4.T1.6.6.6.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{L}}" display="inline"><semantics id="S4.T1.6.6.6.m1.1a"><msup id="S4.T1.6.6.6.m1.1.1" xref="S4.T1.6.6.6.m1.1.1.cmml"><mtext id="S4.T1.6.6.6.m1.1.1.2" xref="S4.T1.6.6.6.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T1.6.6.6.m1.1.1.3" xref="S4.T1.6.6.6.m1.1.1.3a.cmml">L</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.m1.1b"><apply id="S4.T1.6.6.6.m1.1.1.cmml" xref="S4.T1.6.6.6.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.6.6.6.m1.1.1.1.cmml" xref="S4.T1.6.6.6.m1.1.1">superscript</csymbol><ci id="S4.T1.6.6.6.m1.1.1.2a.cmml" xref="S4.T1.6.6.6.m1.1.1.2"><mtext id="S4.T1.6.6.6.m1.1.1.2.cmml" xref="S4.T1.6.6.6.m1.1.1.2">AP</mtext></ci><ci id="S4.T1.6.6.6.m1.1.1.3a.cmml" xref="S4.T1.6.6.6.m1.1.1.3"><mtext mathsize="70%" id="S4.T1.6.6.6.m1.1.1.3.cmml" xref="S4.T1.6.6.6.m1.1.1.3">L</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.m1.1c">\text{AP}^{\text{L}}</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.6.7.1" class="ltx_tr">
<th id="S4.T1.6.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Ass.Embedding (2017)</th>
<th id="S4.T1.6.7.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Hourglass-4</th>
<th id="S4.T1.6.7.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">512</th>
<th id="S4.T1.6.7.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">4</th>
<td id="S4.T1.6.7.1.5" class="ltx_td ltx_align_right ltx_border_t">56.6</td>
<td id="S4.T1.6.7.1.6" class="ltx_td ltx_align_right ltx_border_t">81.8</td>
<td id="S4.T1.6.7.1.7" class="ltx_td ltx_align_right ltx_border_t">61.8</td>
<td id="S4.T1.6.7.1.8" class="ltx_td ltx_align_right ltx_border_t">49.8</td>
<td id="S4.T1.6.7.1.9" class="ltx_td ltx_align_right ltx_border_t">67.0</td>
</tr>
<tr id="S4.T1.6.8.2" class="ltx_tr">
<th id="S4.T1.6.8.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CMU-Pose (2017)</th>
<th id="S4.T1.6.8.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">3CM-3PAF(102)</th>
<th id="S4.T1.6.8.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">368</th>
<th id="S4.T1.6.8.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_row" style="background-color:#D9D9D9;"><span id="S4.T1.6.8.2.4.1" class="ltx_text" style="background-color:#D9D9D9;">10</span></th>
<td id="S4.T1.6.8.2.5" class="ltx_td ltx_align_right">61.8</td>
<td id="S4.T1.6.8.2.6" class="ltx_td ltx_align_right">84.9</td>
<td id="S4.T1.6.8.2.7" class="ltx_td ltx_align_right">67.5</td>
<td id="S4.T1.6.8.2.8" class="ltx_td ltx_align_right">57.1</td>
<td id="S4.T1.6.8.2.9" class="ltx_td ltx_align_right">68.2</td>
</tr>
<tr id="S4.T1.6.9.3" class="ltx_tr">
<th id="S4.T1.6.9.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PersonLab (2018)</th>
<th id="S4.T1.6.9.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">ResNet-152</th>
<th id="S4.T1.6.9.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">1401</th>
<th id="S4.T1.6.9.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">2</th>
<td id="S4.T1.6.9.3.5" class="ltx_td ltx_align_right" style="background-color:#B3B3B3;"><span id="S4.T1.6.9.3.5.1" class="ltx_text" style="background-color:#B3B3B3;">66.5</span></td>
<td id="S4.T1.6.9.3.6" class="ltx_td ltx_align_right">88.0</td>
<td id="S4.T1.6.9.3.7" class="ltx_td ltx_align_right">72.6</td>
<td id="S4.T1.6.9.3.8" class="ltx_td ltx_align_right">62.4</td>
<td id="S4.T1.6.9.3.9" class="ltx_td ltx_align_right">72.3</td>
</tr>
<tr id="S4.T1.6.10.4" class="ltx_tr">
<th id="S4.T1.6.10.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Higher HRNet (2020)</th>
<th id="S4.T1.6.10.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">HRNet-W48</th>
<th id="S4.T1.6.10.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">640</th>
<th id="S4.T1.6.10.4.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">4</th>
<td id="S4.T1.6.10.4.5" class="ltx_td ltx_align_right">65.7</td>
<td id="S4.T1.6.10.4.6" class="ltx_td ltx_align_right">86.3</td>
<td id="S4.T1.6.10.4.7" class="ltx_td ltx_align_right">72.3</td>
<td id="S4.T1.6.10.4.8" class="ltx_td ltx_align_right">60.8</td>
<td id="S4.T1.6.10.4.9" class="ltx_td ltx_align_right">72.6</td>
</tr>
<tr id="S4.T1.6.11.5" class="ltx_tr">
<th id="S4.T1.6.11.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Poi. Anchors (2020)</th>
<th id="S4.T1.6.11.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">HRNet-W48</th>
<th id="S4.T1.6.11.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">640</th>
<th id="S4.T1.6.11.5.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">4</th>
<td id="S4.T1.6.11.5.5" class="ltx_td ltx_align_right" style="background-color:#8C8C8C;"><span id="S4.T1.6.11.5.5.1" class="ltx_text" style="background-color:#8C8C8C;">67.4</span></td>
<td id="S4.T1.6.11.5.6" class="ltx_td ltx_align_right">88.3</td>
<td id="S4.T1.6.11.5.7" class="ltx_td ltx_align_right">74.6</td>
<td id="S4.T1.6.11.5.8" class="ltx_td ltx_align_right">64.7</td>
<td id="S4.T1.6.11.5.9" class="ltx_td ltx_align_right">73.0</td>
</tr>
<tr id="S4.T1.6.12.6" class="ltx_tr">
<th id="S4.T1.6.12.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PoseDet (Ours)</th>
<th id="S4.T1.6.12.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">HRNet-W48</th>
<th id="S4.T1.6.12.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">640</th>
<th id="S4.T1.6.12.6.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" style="background-color:#D9D9D9;"><span id="S4.T1.6.12.6.4.1" class="ltx_text" style="background-color:#D9D9D9;">10</span></th>
<td id="S4.T1.6.12.6.5" class="ltx_td ltx_align_right ltx_border_t" style="background-color:#D9D9D9;"><span id="S4.T1.6.12.6.5.1" class="ltx_text" style="background-color:#D9D9D9;">66.2</span></td>
<td id="S4.T1.6.12.6.6" class="ltx_td ltx_align_right ltx_border_t">87.5</td>
<td id="S4.T1.6.12.6.7" class="ltx_td ltx_align_right ltx_border_t">73.2</td>
<td id="S4.T1.6.12.6.8" class="ltx_td ltx_align_right ltx_border_t">62.3</td>
<td id="S4.T1.6.12.6.9" class="ltx_td ltx_align_right ltx_border_t">73.2</td>
</tr>
<tr id="S4.T1.6.13.7" class="ltx_tr">
<th id="S4.T1.6.13.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PoseDet (Ours)</th>
<th id="S4.T1.6.13.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">HRNet-W32</th>
<th id="S4.T1.6.13.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">512</th>
<th id="S4.T1.6.13.7.4" class="ltx_td ltx_align_right ltx_th ltx_th_row" style="background-color:#B3B3B3;"><span id="S4.T1.6.13.7.4.1" class="ltx_text" style="background-color:#B3B3B3;">18</span></th>
<td id="S4.T1.6.13.7.5" class="ltx_td ltx_align_right">64.4</td>
<td id="S4.T1.6.13.7.6" class="ltx_td ltx_align_right">86.7</td>
<td id="S4.T1.6.13.7.7" class="ltx_td ltx_align_right">71.0</td>
<td id="S4.T1.6.13.7.8" class="ltx_td ltx_align_right">58.8</td>
<td id="S4.T1.6.13.7.9" class="ltx_td ltx_align_right">73.3</td>
</tr>
<tr id="S4.T1.6.14.8" class="ltx_tr">
<th id="S4.T1.6.14.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">PoseDet (Ours)</th>
<th id="S4.T1.6.14.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">DLA-34</th>
<th id="S4.T1.6.14.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">512</th>
<th id="S4.T1.6.14.8.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" style="background-color:#8C8C8C;"><span id="S4.T1.6.14.8.4.1" class="ltx_text" style="background-color:#8C8C8C;">42</span></th>
<td id="S4.T1.6.14.8.5" class="ltx_td ltx_align_right ltx_border_bb">59.2</td>
<td id="S4.T1.6.14.8.6" class="ltx_td ltx_align_right ltx_border_bb">84.3</td>
<td id="S4.T1.6.14.8.7" class="ltx_td ltx_align_right ltx_border_bb">64.6</td>
<td id="S4.T1.6.14.8.8" class="ltx_td ltx_align_right ltx_border_bb">52.7</td>
<td id="S4.T1.6.14.8.9" class="ltx_td ltx_align_right ltx_border_bb">68.8</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison with state-of-the-art bottom-up approaches on COCO <span id="S4.T1.14.1" class="ltx_text ltx_font_italic">test-dev</span>, both keypoints <span id="S4.T1.15.2" class="ltx_text ltx_markedasmath">AP</span> and <span id="S4.T1.16.3" class="ltx_text ltx_markedasmath">FPS</span> are obtained without TTA. Input size indicates the sort side of an input image.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setting</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset.</span> We evaluate PoseDet for accuracy and speed of multi-person pose estimation on a widely adopt benchmark, COCO, and evaluate its robustness in the crowd scenes on the CrowdPose benchmark.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">COCO is a large dataset contains over 200K images, divided into <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">train</span>/<span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">val</span>/<span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_italic">test-dev</span> splits. Each person is labeled with 17 body joints. Models are trained using <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_italic">train</span> split (64k images). We report results on COCO <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_italic">test-dev</span> split (20K images) for comparisons to state-of-the-art methods. The official Average Precision (AP), calculated in terms of OKS between the prediction and GT pose, is adopted as a metric to evaluate the accuracy, and Frames per Second (<span id="S4.SS1.p2.1.6" class="ltx_text ltx_markedasmath">FPS</span>) is adopted for the speed.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.4" class="ltx_p">CrowdPose consists of 20,000 images, including about 80,000 persons. Each person is labeled with 14 body joints. CrowdPose is divided into train, validation, and test sets in the proportion of 5:1:4. The average intersection over union(IoU) of human bounding boxes is 0.27 (0.06 in COCO). We evaluate PoseDet on CrowdPose to validate the robustness in the crowd scenes. The test set of CrowdPose is divided into three crowding levels, indicated as easy, medium, and hard. Three metrics, <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{E}}" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><msup id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mtext id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3a.cmml">E</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.2a.cmml" xref="S4.SS1.p3.1.m1.1.1.2"><mtext id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">AP</mtext></ci><ci id="S4.SS1.p3.1.m1.1.1.3a.cmml" xref="S4.SS1.p3.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">E</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\text{AP}^{\text{E}}</annotation></semantics></math>, <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="\text{AP}^{\text{M}}" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><msup id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mtext id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2a.cmml">AP</mtext><mtext id="S4.SS1.p3.2.m2.1.1.3" xref="S4.SS1.p3.2.m2.1.1.3a.cmml">M</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">superscript</csymbol><ci id="S4.SS1.p3.2.m2.1.1.2a.cmml" xref="S4.SS1.p3.2.m2.1.1.2"><mtext id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">AP</mtext></ci><ci id="S4.SS1.p3.2.m2.1.1.3a.cmml" xref="S4.SS1.p3.2.m2.1.1.3"><mtext mathsize="70%" id="S4.SS1.p3.2.m2.1.1.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3">M</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\text{AP}^{\text{M}}</annotation></semantics></math>, and <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="\text{AP}^{\text{H}}" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><msup id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mtext id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2a.cmml">AP</mtext><mtext id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3a.cmml">H</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">superscript</csymbol><ci id="S4.SS1.p3.3.m3.1.1.2a.cmml" xref="S4.SS1.p3.3.m3.1.1.2"><mtext id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">AP</mtext></ci><ci id="S4.SS1.p3.3.m3.1.1.3a.cmml" xref="S4.SS1.p3.3.m3.1.1.3"><mtext mathsize="70%" id="S4.SS1.p3.3.m3.1.1.3.cmml" xref="S4.SS1.p3.3.m3.1.1.3">H</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">\text{AP}^{\text{H}}</annotation></semantics></math> are adopted respectively, where <math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="\text{AP}^{\text{H}}" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><msup id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml"><mtext id="S4.SS1.p3.4.m4.1.1.2" xref="S4.SS1.p3.4.m4.1.1.2a.cmml">AP</mtext><mtext id="S4.SS1.p3.4.m4.1.1.3" xref="S4.SS1.p3.4.m4.1.1.3a.cmml">H</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><apply id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">superscript</csymbol><ci id="S4.SS1.p3.4.m4.1.1.2a.cmml" xref="S4.SS1.p3.4.m4.1.1.2"><mtext id="S4.SS1.p3.4.m4.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1.2">AP</mtext></ci><ci id="S4.SS1.p3.4.m4.1.1.3a.cmml" xref="S4.SS1.p3.4.m4.1.1.3"><mtext mathsize="70%" id="S4.SS1.p3.4.m4.1.1.3.cmml" xref="S4.SS1.p3.4.m4.1.1.3">H</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">\text{AP}^{\text{H}}</annotation></semantics></math> contains the most crowd.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Implementation Details.</span> We follow the conventional data augmentation strategies and training schedules for multi-person pose estimation. The COCO and CrowdPose dataset share the same training/testing settings for convenience.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.2" class="ltx_p">During model training, images are augmented with horizontally flipping and resize factor in <math id="S4.SS1.p5.1.m1.2" class="ltx_Math" alttext="[0.5,1.5]" display="inline"><semantics id="S4.SS1.p5.1.m1.2a"><mrow id="S4.SS1.p5.1.m1.2.3.2" xref="S4.SS1.p5.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p5.1.m1.2.3.2.1" xref="S4.SS1.p5.1.m1.2.3.1.cmml">[</mo><mn id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">0.5</mn><mo id="S4.SS1.p5.1.m1.2.3.2.2" xref="S4.SS1.p5.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.p5.1.m1.2.2" xref="S4.SS1.p5.1.m1.2.2.cmml">1.5</mn><mo stretchy="false" id="S4.SS1.p5.1.m1.2.3.2.3" xref="S4.SS1.p5.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.2b"><interval closure="closed" id="S4.SS1.p5.1.m1.2.3.1.cmml" xref="S4.SS1.p5.1.m1.2.3.2"><cn type="float" id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">0.5</cn><cn type="float" id="S4.SS1.p5.1.m1.2.2.cmml" xref="S4.SS1.p5.1.m1.2.2">1.5</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.2c">[0.5,1.5]</annotation></semantics></math>, then cropped centered on a random person to <math id="S4.SS1.p5.2.m2.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S4.SS1.p5.2.m2.1a"><mrow id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml"><mn id="S4.SS1.p5.2.m2.1.1.2" xref="S4.SS1.p5.2.m2.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p5.2.m2.1.1.1" xref="S4.SS1.p5.2.m2.1.1.1.cmml">Ã—</mo><mn id="S4.SS1.p5.2.m2.1.1.3" xref="S4.SS1.p5.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b"><apply id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1"><times id="S4.SS1.p5.2.m2.1.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.p5.2.m2.1.1.2.cmml" xref="S4.SS1.p5.2.m2.1.1.2">512</cn><cn type="integer" id="S4.SS1.p5.2.m2.1.1.3.cmml" xref="S4.SS1.p5.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">512\times 512</annotation></semantics></math> patches. The cropped patches are fed into the network as training samples. All models are optimized with AdamÂ <cite class="ltx_cite ltx_citemacro_cite">Kingma and Ba (<a href="#bib.bib17" title="" class="ltx_ref">2015</a>)</cite> over 8 NVIDIA V100 GPUs with a mini-batch of 48 patches (6 patches per GPU). The initial learning rate is 0.0001, decreased at epoch 180 and 200 by a factor of 0.1. Training converges at epoch 210. We examine different backbones before FPN, which is initialized with the weights pre-trained on ImageNetÂ <cite class="ltx_cite ltx_citemacro_cite">Deng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib9" title="" class="ltx_ref">2009</a>)</cite>.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">For evaluation, we conduct single-scale and multi-scale testing. For single-scale testing, unless specified, the input images are resized to have a shorter side of 640 and a longer side less than 1000, with an unchanged aspect ratio. For multi-scale testing, the input image is resized to an image pyramid with a sorter side of <math id="S4.SS1.p6.1.m1.6" class="ltx_Math" alttext="\{300,400,500,600,700,800\}" display="inline"><semantics id="S4.SS1.p6.1.m1.6a"><mrow id="S4.SS1.p6.1.m1.6.7.2" xref="S4.SS1.p6.1.m1.6.7.1.cmml"><mo stretchy="false" id="S4.SS1.p6.1.m1.6.7.2.1" xref="S4.SS1.p6.1.m1.6.7.1.cmml">{</mo><mn id="S4.SS1.p6.1.m1.1.1" xref="S4.SS1.p6.1.m1.1.1.cmml">300</mn><mo id="S4.SS1.p6.1.m1.6.7.2.2" xref="S4.SS1.p6.1.m1.6.7.1.cmml">,</mo><mn id="S4.SS1.p6.1.m1.2.2" xref="S4.SS1.p6.1.m1.2.2.cmml">400</mn><mo id="S4.SS1.p6.1.m1.6.7.2.3" xref="S4.SS1.p6.1.m1.6.7.1.cmml">,</mo><mn id="S4.SS1.p6.1.m1.3.3" xref="S4.SS1.p6.1.m1.3.3.cmml">500</mn><mo id="S4.SS1.p6.1.m1.6.7.2.4" xref="S4.SS1.p6.1.m1.6.7.1.cmml">,</mo><mn id="S4.SS1.p6.1.m1.4.4" xref="S4.SS1.p6.1.m1.4.4.cmml">600</mn><mo id="S4.SS1.p6.1.m1.6.7.2.5" xref="S4.SS1.p6.1.m1.6.7.1.cmml">,</mo><mn id="S4.SS1.p6.1.m1.5.5" xref="S4.SS1.p6.1.m1.5.5.cmml">700</mn><mo id="S4.SS1.p6.1.m1.6.7.2.6" xref="S4.SS1.p6.1.m1.6.7.1.cmml">,</mo><mn id="S4.SS1.p6.1.m1.6.6" xref="S4.SS1.p6.1.m1.6.6.cmml">800</mn><mo stretchy="false" id="S4.SS1.p6.1.m1.6.7.2.7" xref="S4.SS1.p6.1.m1.6.7.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.1.m1.6b"><set id="S4.SS1.p6.1.m1.6.7.1.cmml" xref="S4.SS1.p6.1.m1.6.7.2"><cn type="integer" id="S4.SS1.p6.1.m1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1">300</cn><cn type="integer" id="S4.SS1.p6.1.m1.2.2.cmml" xref="S4.SS1.p6.1.m1.2.2">400</cn><cn type="integer" id="S4.SS1.p6.1.m1.3.3.cmml" xref="S4.SS1.p6.1.m1.3.3">500</cn><cn type="integer" id="S4.SS1.p6.1.m1.4.4.cmml" xref="S4.SS1.p6.1.m1.4.4">600</cn><cn type="integer" id="S4.SS1.p6.1.m1.5.5.cmml" xref="S4.SS1.p6.1.m1.5.5">700</cn><cn type="integer" id="S4.SS1.p6.1.m1.6.6.cmml" xref="S4.SS1.p6.1.m1.6.6">800</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.6c">\{300,400,500,600,700,800\}</annotation></semantics></math> and augmented by horizontal flip. NMS is conducted over the results of augmented images. The inference speed is evaluated on GPU GTX 1080Ti and CPU Intel i9-7980XE. All experiments are implemented based on mmdetectionÂ <cite class="ltx_cite ltx_citemacro_cite">Chen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Analysis of Accuracy and Speed</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Both accuracy and speed are important in real-world applications of multi-person pose estimation, while current pose estimation metrics are based on the accuracy of keypoints. To provide a more complete comparison ,we evaluate approaches on COCO <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">test-dev</span> and present keypoints <span id="S4.SS2.p1.1.2" class="ltx_text ltx_markedasmath">AP</span> and inference speed (FPS) in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4 Experiments â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We compare the performances of bottom-up approaches without test-time augmentation (TTA).</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">For multi-person pose estimation, models with the highest accuracy are dominated by top-down approachesÂ <cite class="ltx_cite ltx_citemacro_cite">Sun <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>); Cai <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>. The bounding boxes of persons are detected by a person detector and resized to a fixed size, which avoids the scale diversity, leading to higher accuracy and lower inference speed than bottom-up approaches. For example, HRNet<cite class="ltx_cite ltx_citemacro_cite">Sun <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite> achieves keypoints <span id="S4.SS2.p2.2.1" class="ltx_text ltx_markedasmath">AP</span> of 75.5 on COCO <span id="S4.SS2.p2.2.2" class="ltx_text ltx_font_italic">test-dev</span>. They perform single-person pose estimation on detected boxes (20 boxes per image roughly, each box is resized to <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mn id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><times id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">384</cn><cn type="integer" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">384\times 288</annotation></semantics></math>), with an inference time of about 90 ms per box. The performance of inference speed can not be analyzed due to the inference time of the person detector is not reported. We only consider bottom-up approaches in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4 Experiments â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.2" class="ltx_p">Moreover, TTA, including multi-scale testing and flip, is widely adopted in multi-person pose estimation.
With TTA, too big or too small targets can be resized to a more proper size, making them easier to be detected.
TTA can improve the accuracy by additional detection but increases inference time and computation overhead.
TTA with a six-scale image pyramid and flip increase the computation by about 12 times.
We argue that discarding TTA is more practical in application scenarios where real-time performance is required.
Hence, we provide a consistent comparison in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4 Experiments â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where both <span id="S4.SS2.p3.2.1" class="ltx_text ltx_markedasmath">AP</span> and <span id="S4.SS2.p3.2.2" class="ltx_text ltx_markedasmath">FPS</span> are obtained without TTA.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.2" class="ltx_p">Specifically, for CMU-PoseÂ <cite class="ltx_cite ltx_citemacro_cite">Cao <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite> and Associative EmbeddingÂ <cite class="ltx_cite ltx_citemacro_cite">Newell <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>, we report the results cited from their papers or GitHub repositories. For Point AnchorsÂ <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite> and Higher HRNetÂ <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, the results are based on our implementation, because either <span id="S4.SS2.p4.2.1" class="ltx_text ltx_markedasmath">AP</span> or <span id="S4.SS2.p4.2.2" class="ltx_text ltx_markedasmath">FPS</span> without TTA is not reported by authors.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.3" class="ltx_p">Given the results, we can see that PoseDet outperforms other methods in terms of the tradeoff between accuracy and speed. The inference speed of PoseDet with backbone HRNet-W48 is 2.5 times higher than Point Anchors and Higher HRNet with the same backbone and scale of the input image. PoseDet with a lighter backbone DLA-34 achieves a maximum speed (42 <span id="S4.SS2.p5.3.1" class="ltx_text ltx_markedasmath">FPS</span>) with lower accuracy (59.2 <span id="S4.SS2.p5.3.2" class="ltx_text ltx_markedasmath">AP</span>). The main reason why backbone HRNet achieves higher accuracy than DLA, especially refers to <math id="S4.SS2.p5.3.m3.1" class="ltx_Math" alttext="\text{AP}^{\text{75}}" display="inline"><semantics id="S4.SS2.p5.3.m3.1a"><msup id="S4.SS2.p5.3.m3.1.1" xref="S4.SS2.p5.3.m3.1.1.cmml"><mtext id="S4.SS2.p5.3.m3.1.1.2" xref="S4.SS2.p5.3.m3.1.1.2a.cmml">AP</mtext><mtext id="S4.SS2.p5.3.m3.1.1.3" xref="S4.SS2.p5.3.m3.1.1.3a.cmml">75</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.3.m3.1b"><apply id="S4.SS2.p5.3.m3.1.1.cmml" xref="S4.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.3.m3.1.1.1.cmml" xref="S4.SS2.p5.3.m3.1.1">superscript</csymbol><ci id="S4.SS2.p5.3.m3.1.1.2a.cmml" xref="S4.SS2.p5.3.m3.1.1.2"><mtext id="S4.SS2.p5.3.m3.1.1.2.cmml" xref="S4.SS2.p5.3.m3.1.1.2">AP</mtext></ci><ci id="S4.SS2.p5.3.m3.1.1.3a.cmml" xref="S4.SS2.p5.3.m3.1.1.3"><mtext mathsize="70%" id="S4.SS2.p5.3.m3.1.1.3.cmml" xref="S4.SS2.p5.3.m3.1.1.3">75</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.3.m3.1c">\text{AP}^{\text{75}}</annotation></semantics></math>, is that HRNet extracts high-resolution representation rich features, which are significant for the pixel-level prediction of joints. To best our knowledge, PoseDet is the fastest approach for multi-person pose estimation.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">Without consideration of the inference speed, we also report results with TTA in TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.2 Analysis of Accuracy and Speed â€£ 4 Experiments â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The comparison indicates the ability of PoseDet to estimate accurate poses. All reported numbers for comparison are cited from the papers.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:231.0pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.5pt,18.0pt) scale(0.75,0.75) ;">
<table id="S4.T2.5.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.5.5.5" class="ltx_tr">
<th id="S4.T2.5.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T2.5.5.5.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Backbone</th>
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_markedasmath">AP</span></th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{50}}" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><msup id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml"><mtext id="S4.T2.2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.2.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T2.2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.2.m1.1.1.3a.cmml">50</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">superscript</csymbol><ci id="S4.T2.2.2.2.2.m1.1.1.2a.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2"><mtext id="S4.T2.2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2">AP</mtext></ci><ci id="S4.T2.2.2.2.2.m1.1.1.3a.cmml" xref="S4.T2.2.2.2.2.m1.1.1.3"><mtext mathsize="70%" id="S4.T2.2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.2.2.2.2.m1.1.1.3">50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\text{AP}^{\text{50}}</annotation></semantics></math></th>
<th id="S4.T2.3.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{75}}" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><msup id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml"><mtext id="S4.T2.3.3.3.3.m1.1.1.2" xref="S4.T2.3.3.3.3.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T2.3.3.3.3.m1.1.1.3" xref="S4.T2.3.3.3.3.m1.1.1.3a.cmml">75</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><apply id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.3.3.3.m1.1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">superscript</csymbol><ci id="S4.T2.3.3.3.3.m1.1.1.2a.cmml" xref="S4.T2.3.3.3.3.m1.1.1.2"><mtext id="S4.T2.3.3.3.3.m1.1.1.2.cmml" xref="S4.T2.3.3.3.3.m1.1.1.2">AP</mtext></ci><ci id="S4.T2.3.3.3.3.m1.1.1.3a.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3"><mtext mathsize="70%" id="S4.T2.3.3.3.3.m1.1.1.3.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3">75</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">\text{AP}^{\text{75}}</annotation></semantics></math></th>
<th id="S4.T2.4.4.4.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.4.4.4.4.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{M}}" display="inline"><semantics id="S4.T2.4.4.4.4.m1.1a"><msup id="S4.T2.4.4.4.4.m1.1.1" xref="S4.T2.4.4.4.4.m1.1.1.cmml"><mtext id="S4.T2.4.4.4.4.m1.1.1.2" xref="S4.T2.4.4.4.4.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T2.4.4.4.4.m1.1.1.3" xref="S4.T2.4.4.4.4.m1.1.1.3a.cmml">M</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.m1.1b"><apply id="S4.T2.4.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.4.4.4.4.m1.1.1.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1">superscript</csymbol><ci id="S4.T2.4.4.4.4.m1.1.1.2a.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2"><mtext id="S4.T2.4.4.4.4.m1.1.1.2.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2">AP</mtext></ci><ci id="S4.T2.4.4.4.4.m1.1.1.3a.cmml" xref="S4.T2.4.4.4.4.m1.1.1.3"><mtext mathsize="70%" id="S4.T2.4.4.4.4.m1.1.1.3.cmml" xref="S4.T2.4.4.4.4.m1.1.1.3">M</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.m1.1c">\text{AP}^{\text{M}}</annotation></semantics></math></th>
<th id="S4.T2.5.5.5.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.5.5.5.5.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{L}}" display="inline"><semantics id="S4.T2.5.5.5.5.m1.1a"><msup id="S4.T2.5.5.5.5.m1.1.1" xref="S4.T2.5.5.5.5.m1.1.1.cmml"><mtext id="S4.T2.5.5.5.5.m1.1.1.2" xref="S4.T2.5.5.5.5.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T2.5.5.5.5.m1.1.1.3" xref="S4.T2.5.5.5.5.m1.1.1.3a.cmml">L</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.5.m1.1b"><apply id="S4.T2.5.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.5.5.5.5.m1.1.1.1.cmml" xref="S4.T2.5.5.5.5.m1.1.1">superscript</csymbol><ci id="S4.T2.5.5.5.5.m1.1.1.2a.cmml" xref="S4.T2.5.5.5.5.m1.1.1.2"><mtext id="S4.T2.5.5.5.5.m1.1.1.2.cmml" xref="S4.T2.5.5.5.5.m1.1.1.2">AP</mtext></ci><ci id="S4.T2.5.5.5.5.m1.1.1.3a.cmml" xref="S4.T2.5.5.5.5.m1.1.1.3"><mtext mathsize="70%" id="S4.T2.5.5.5.5.m1.1.1.3.cmml" xref="S4.T2.5.5.5.5.m1.1.1.3">L</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.5.m1.1c">\text{AP}^{\text{L}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.5.5.6.1" class="ltx_tr">
<th id="S4.T2.5.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PersonLab</th>
<th id="S4.T2.5.5.6.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet-152</th>
<td id="S4.T2.5.5.6.1.3" class="ltx_td ltx_align_right ltx_border_t">68.7</td>
<td id="S4.T2.5.5.6.1.4" class="ltx_td ltx_align_right ltx_border_t">89.0</td>
<td id="S4.T2.5.5.6.1.5" class="ltx_td ltx_align_right ltx_border_t">75.4</td>
<td id="S4.T2.5.5.6.1.6" class="ltx_td ltx_align_right ltx_border_t">64.1</td>
<td id="S4.T2.5.5.6.1.7" class="ltx_td ltx_align_right ltx_border_t">75.5</td>
</tr>
<tr id="S4.T2.5.5.7.2" class="ltx_tr">
<th id="S4.T2.5.5.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ass.Embedding</th>
<th id="S4.T2.5.5.7.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Hourglass-4</th>
<td id="S4.T2.5.5.7.2.3" class="ltx_td ltx_align_right">65.5</td>
<td id="S4.T2.5.5.7.2.4" class="ltx_td ltx_align_right">86.8</td>
<td id="S4.T2.5.5.7.2.5" class="ltx_td ltx_align_right">72.3</td>
<td id="S4.T2.5.5.7.2.6" class="ltx_td ltx_align_right">60.6</td>
<td id="S4.T2.5.5.7.2.7" class="ltx_td ltx_align_right">72.6</td>
</tr>
<tr id="S4.T2.5.5.8.3" class="ltx_tr">
<th id="S4.T2.5.5.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SPM</th>
<th id="S4.T2.5.5.8.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Hourglass-8</th>
<td id="S4.T2.5.5.8.3.3" class="ltx_td ltx_align_right">66.9</td>
<td id="S4.T2.5.5.8.3.4" class="ltx_td ltx_align_right">88.5</td>
<td id="S4.T2.5.5.8.3.5" class="ltx_td ltx_align_right">72.9</td>
<td id="S4.T2.5.5.8.3.6" class="ltx_td ltx_align_right">62.6</td>
<td id="S4.T2.5.5.8.3.7" class="ltx_td ltx_align_right">73.1</td>
</tr>
<tr id="S4.T2.5.5.9.4" class="ltx_tr">
<th id="S4.T2.5.5.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Higher HRNet</th>
<th id="S4.T2.5.5.9.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">HRNet-W48</th>
<td id="S4.T2.5.5.9.4.3" class="ltx_td ltx_align_right">70.5</td>
<td id="S4.T2.5.5.9.4.4" class="ltx_td ltx_align_right">89.3</td>
<td id="S4.T2.5.5.9.4.5" class="ltx_td ltx_align_right">77.2</td>
<td id="S4.T2.5.5.9.4.6" class="ltx_td ltx_align_right">66.6</td>
<td id="S4.T2.5.5.9.4.7" class="ltx_td ltx_align_right">75.8</td>
</tr>
<tr id="S4.T2.5.5.10.5" class="ltx_tr">
<th id="S4.T2.5.5.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Poi. Anchors</th>
<th id="S4.T2.5.5.10.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">HRNet-W48</th>
<td id="S4.T2.5.5.10.5.3" class="ltx_td ltx_align_right">68.7</td>
<td id="S4.T2.5.5.10.5.4" class="ltx_td ltx_align_right">89.9</td>
<td id="S4.T2.5.5.10.5.5" class="ltx_td ltx_align_right">76.3</td>
<td id="S4.T2.5.5.10.5.6" class="ltx_td ltx_align_right">64.8</td>
<td id="S4.T2.5.5.10.5.7" class="ltx_td ltx_align_right">75.3</td>
</tr>
<tr id="S4.T2.5.5.11.6" class="ltx_tr">
<th id="S4.T2.5.5.11.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PoseDet (Ours)</th>
<th id="S4.T2.5.5.11.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">HRNet-W48</th>
<td id="S4.T2.5.5.11.6.3" class="ltx_td ltx_align_right">67.8</td>
<td id="S4.T2.5.5.11.6.4" class="ltx_td ltx_align_right">89.3</td>
<td id="S4.T2.5.5.11.6.5" class="ltx_td ltx_align_right">75.0</td>
<td id="S4.T2.5.5.11.6.6" class="ltx_td ltx_align_right">63.3</td>
<td id="S4.T2.5.5.11.6.7" class="ltx_td ltx_align_right">75.5</td>
</tr>
<tr id="S4.T2.5.5.12.7" class="ltx_tr">
<th id="S4.T2.5.5.12.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">PoseDet (Ours)</th>
<th id="S4.T2.5.5.12.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">HRNet-W32</th>
<td id="S4.T2.5.5.12.7.3" class="ltx_td ltx_align_right ltx_border_bb">66.7</td>
<td id="S4.T2.5.5.12.7.4" class="ltx_td ltx_align_right ltx_border_bb">88.5</td>
<td id="S4.T2.5.5.12.7.5" class="ltx_td ltx_align_right ltx_border_bb">73.7</td>
<td id="S4.T2.5.5.12.7.6" class="ltx_td ltx_align_right ltx_border_bb">62.2</td>
<td id="S4.T2.5.5.12.7.7" class="ltx_td ltx_align_right ltx_border_bb">75.5</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance on COCO <span id="S4.T2.7.1" class="ltx_text ltx_font_italic">test-dev</span> with TTA.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Robustness in the Crowd Scenes</h3>

<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:232.2pt;height:107.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.5pt,9.4pt) scale(0.85,0.85) ;">
<table id="S4.T3.6.6" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.6.6.6" class="ltx_tr">
<th id="S4.T3.6.6.6.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Method</th>
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_markedasmath">AP</span></th>
<th id="S4.T3.2.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{50}}" display="inline"><semantics id="S4.T3.2.2.2.2.m1.1a"><msup id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml"><mtext id="S4.T3.2.2.2.2.m1.1.1.2" xref="S4.T3.2.2.2.2.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T3.2.2.2.2.m1.1.1.3" xref="S4.T3.2.2.2.2.m1.1.1.3a.cmml">50</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><apply id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.2.2.2.2.m1.1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">superscript</csymbol><ci id="S4.T3.2.2.2.2.m1.1.1.2a.cmml" xref="S4.T3.2.2.2.2.m1.1.1.2"><mtext id="S4.T3.2.2.2.2.m1.1.1.2.cmml" xref="S4.T3.2.2.2.2.m1.1.1.2">AP</mtext></ci><ci id="S4.T3.2.2.2.2.m1.1.1.3a.cmml" xref="S4.T3.2.2.2.2.m1.1.1.3"><mtext mathsize="70%" id="S4.T3.2.2.2.2.m1.1.1.3.cmml" xref="S4.T3.2.2.2.2.m1.1.1.3">50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">\text{AP}^{\text{50}}</annotation></semantics></math></th>
<th id="S4.T3.3.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{75}}" display="inline"><semantics id="S4.T3.3.3.3.3.m1.1a"><msup id="S4.T3.3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.3.m1.1.1.cmml"><mtext id="S4.T3.3.3.3.3.m1.1.1.2" xref="S4.T3.3.3.3.3.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T3.3.3.3.3.m1.1.1.3" xref="S4.T3.3.3.3.3.m1.1.1.3a.cmml">75</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><apply id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.3.3.3.3.m1.1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1">superscript</csymbol><ci id="S4.T3.3.3.3.3.m1.1.1.2a.cmml" xref="S4.T3.3.3.3.3.m1.1.1.2"><mtext id="S4.T3.3.3.3.3.m1.1.1.2.cmml" xref="S4.T3.3.3.3.3.m1.1.1.2">AP</mtext></ci><ci id="S4.T3.3.3.3.3.m1.1.1.3a.cmml" xref="S4.T3.3.3.3.3.m1.1.1.3"><mtext mathsize="70%" id="S4.T3.3.3.3.3.m1.1.1.3.cmml" xref="S4.T3.3.3.3.3.m1.1.1.3">75</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">\text{AP}^{\text{75}}</annotation></semantics></math></th>
<th id="S4.T3.4.4.4.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T3.4.4.4.4.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{E}}" display="inline"><semantics id="S4.T3.4.4.4.4.m1.1a"><msup id="S4.T3.4.4.4.4.m1.1.1" xref="S4.T3.4.4.4.4.m1.1.1.cmml"><mtext id="S4.T3.4.4.4.4.m1.1.1.2" xref="S4.T3.4.4.4.4.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T3.4.4.4.4.m1.1.1.3" xref="S4.T3.4.4.4.4.m1.1.1.3a.cmml">E</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.4.m1.1b"><apply id="S4.T3.4.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.4.4.4.4.m1.1.1.1.cmml" xref="S4.T3.4.4.4.4.m1.1.1">superscript</csymbol><ci id="S4.T3.4.4.4.4.m1.1.1.2a.cmml" xref="S4.T3.4.4.4.4.m1.1.1.2"><mtext id="S4.T3.4.4.4.4.m1.1.1.2.cmml" xref="S4.T3.4.4.4.4.m1.1.1.2">AP</mtext></ci><ci id="S4.T3.4.4.4.4.m1.1.1.3a.cmml" xref="S4.T3.4.4.4.4.m1.1.1.3"><mtext mathsize="70%" id="S4.T3.4.4.4.4.m1.1.1.3.cmml" xref="S4.T3.4.4.4.4.m1.1.1.3">E</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.4.m1.1c">\text{AP}^{\text{E}}</annotation></semantics></math></th>
<th id="S4.T3.5.5.5.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T3.5.5.5.5.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{M}}" display="inline"><semantics id="S4.T3.5.5.5.5.m1.1a"><msup id="S4.T3.5.5.5.5.m1.1.1" xref="S4.T3.5.5.5.5.m1.1.1.cmml"><mtext id="S4.T3.5.5.5.5.m1.1.1.2" xref="S4.T3.5.5.5.5.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T3.5.5.5.5.m1.1.1.3" xref="S4.T3.5.5.5.5.m1.1.1.3a.cmml">M</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.5.m1.1b"><apply id="S4.T3.5.5.5.5.m1.1.1.cmml" xref="S4.T3.5.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.5.5.5.5.m1.1.1.1.cmml" xref="S4.T3.5.5.5.5.m1.1.1">superscript</csymbol><ci id="S4.T3.5.5.5.5.m1.1.1.2a.cmml" xref="S4.T3.5.5.5.5.m1.1.1.2"><mtext id="S4.T3.5.5.5.5.m1.1.1.2.cmml" xref="S4.T3.5.5.5.5.m1.1.1.2">AP</mtext></ci><ci id="S4.T3.5.5.5.5.m1.1.1.3a.cmml" xref="S4.T3.5.5.5.5.m1.1.1.3"><mtext mathsize="70%" id="S4.T3.5.5.5.5.m1.1.1.3.cmml" xref="S4.T3.5.5.5.5.m1.1.1.3">M</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.5.m1.1c">\text{AP}^{\text{M}}</annotation></semantics></math></th>
<th id="S4.T3.6.6.6.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T3.6.6.6.6.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{H}}" display="inline"><semantics id="S4.T3.6.6.6.6.m1.1a"><msup id="S4.T3.6.6.6.6.m1.1.1" xref="S4.T3.6.6.6.6.m1.1.1.cmml"><mtext id="S4.T3.6.6.6.6.m1.1.1.2" xref="S4.T3.6.6.6.6.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T3.6.6.6.6.m1.1.1.3" xref="S4.T3.6.6.6.6.m1.1.1.3a.cmml">H</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.6.m1.1b"><apply id="S4.T3.6.6.6.6.m1.1.1.cmml" xref="S4.T3.6.6.6.6.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.6.6.6.6.m1.1.1.1.cmml" xref="S4.T3.6.6.6.6.m1.1.1">superscript</csymbol><ci id="S4.T3.6.6.6.6.m1.1.1.2a.cmml" xref="S4.T3.6.6.6.6.m1.1.1.2"><mtext id="S4.T3.6.6.6.6.m1.1.1.2.cmml" xref="S4.T3.6.6.6.6.m1.1.1.2">AP</mtext></ci><ci id="S4.T3.6.6.6.6.m1.1.1.3a.cmml" xref="S4.T3.6.6.6.6.m1.1.1.3"><mtext mathsize="70%" id="S4.T3.6.6.6.6.m1.1.1.3.cmml" xref="S4.T3.6.6.6.6.m1.1.1.3">H</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.6.m1.1c">\text{AP}^{\text{H}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.6.6.7.1" class="ltx_tr">
<th id="S4.T3.6.6.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Mask-RCNN</th>
<td id="S4.T3.6.6.7.1.2" class="ltx_td ltx_align_right ltx_border_t">57.2</td>
<td id="S4.T3.6.6.7.1.3" class="ltx_td ltx_align_right ltx_border_t">83.5</td>
<td id="S4.T3.6.6.7.1.4" class="ltx_td ltx_align_right ltx_border_t">60.3</td>
<td id="S4.T3.6.6.7.1.5" class="ltx_td ltx_align_right ltx_border_t">69.4</td>
<td id="S4.T3.6.6.7.1.6" class="ltx_td ltx_align_right ltx_border_t">57.9</td>
<td id="S4.T3.6.6.7.1.7" class="ltx_td ltx_align_right ltx_border_t">45.8</td>
</tr>
<tr id="S4.T3.6.6.8.2" class="ltx_tr">
<th id="S4.T3.6.6.8.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AlphaPose</th>
<td id="S4.T3.6.6.8.2.2" class="ltx_td ltx_align_right">61.0</td>
<td id="S4.T3.6.6.8.2.3" class="ltx_td ltx_align_right">81.3</td>
<td id="S4.T3.6.6.8.2.4" class="ltx_td ltx_align_right">66.0</td>
<td id="S4.T3.6.6.8.2.5" class="ltx_td ltx_align_right">71.2</td>
<td id="S4.T3.6.6.8.2.6" class="ltx_td ltx_align_right">61.4</td>
<td id="S4.T3.6.6.8.2.7" class="ltx_td ltx_align_right">51.1</td>
</tr>
<tr id="S4.T3.6.6.9.3" class="ltx_tr">
<th id="S4.T3.6.6.9.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Simple Baseline</th>
<td id="S4.T3.6.6.9.3.2" class="ltx_td ltx_align_right">60.8</td>
<td id="S4.T3.6.6.9.3.3" class="ltx_td ltx_align_right">81.4</td>
<td id="S4.T3.6.6.9.3.4" class="ltx_td ltx_align_right">65.7</td>
<td id="S4.T3.6.6.9.3.5" class="ltx_td ltx_align_right">71.4</td>
<td id="S4.T3.6.6.9.3.6" class="ltx_td ltx_align_right">61.2</td>
<td id="S4.T3.6.6.9.3.7" class="ltx_td ltx_align_right">51.2</td>
</tr>
<tr id="S4.T3.6.6.10.4" class="ltx_tr">
<th id="S4.T3.6.6.10.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SPPE</th>
<td id="S4.T3.6.6.10.4.2" class="ltx_td ltx_align_right">66.0</td>
<td id="S4.T3.6.6.10.4.3" class="ltx_td ltx_align_right">84.2</td>
<td id="S4.T3.6.6.10.4.4" class="ltx_td ltx_align_right">71.5</td>
<td id="S4.T3.6.6.10.4.5" class="ltx_td ltx_align_right">75.5</td>
<td id="S4.T3.6.6.10.4.6" class="ltx_td ltx_align_right">66.3</td>
<td id="S4.T3.6.6.10.4.7" class="ltx_td ltx_align_right">57.4</td>
</tr>
<tr id="S4.T3.6.6.11.5" class="ltx_tr">
<th id="S4.T3.6.6.11.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Higher HRNet</th>
<td id="S4.T3.6.6.11.5.2" class="ltx_td ltx_align_right"><span id="S4.T3.6.6.11.5.2.1" class="ltx_text ltx_font_bold">67.6</span></td>
<td id="S4.T3.6.6.11.5.3" class="ltx_td ltx_align_right">87.4</td>
<td id="S4.T3.6.6.11.5.4" class="ltx_td ltx_align_right">72.6</td>
<td id="S4.T3.6.6.11.5.5" class="ltx_td ltx_align_right"><span id="S4.T3.6.6.11.5.5.1" class="ltx_text ltx_font_bold">75.8</span></td>
<td id="S4.T3.6.6.11.5.6" class="ltx_td ltx_align_right"><span id="S4.T3.6.6.11.5.6.1" class="ltx_text ltx_font_bold">68.1</span></td>
<td id="S4.T3.6.6.11.5.7" class="ltx_td ltx_align_right"><span id="S4.T3.6.6.11.5.7.1" class="ltx_text ltx_font_bold">58.9</span></td>
</tr>
<tr id="S4.T3.6.6.12.6" class="ltx_tr">
<th id="S4.T3.6.6.12.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">PoseDet (Ours)</th>
<td id="S4.T3.6.6.12.6.2" class="ltx_td ltx_align_right ltx_border_bb">67.3</td>
<td id="S4.T3.6.6.12.6.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T3.6.6.12.6.3.1" class="ltx_text ltx_font_bold">88.9</span></td>
<td id="S4.T3.6.6.12.6.4" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T3.6.6.12.6.4.1" class="ltx_text ltx_font_bold">72.8</span></td>
<td id="S4.T3.6.6.12.6.5" class="ltx_td ltx_align_right ltx_border_bb">75.2</td>
<td id="S4.T3.6.6.12.6.6" class="ltx_td ltx_align_right ltx_border_bb">68.0</td>
<td id="S4.T3.6.6.12.6.7" class="ltx_td ltx_align_right ltx_border_bb">58.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison with state-of-the-art methods on CrowdPose with TTA.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Another important issue in real-world applications is the robustness to various poses, including occluded persons and various poses. Most of images in popular datasets (<em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, COCOÂ <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> and MPIIÂ <cite class="ltx_cite ltx_citemacro_cite">Andriluka <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib1" title="" class="ltx_ref">2014</a>)</cite>) have no overlapped persons. We evaluate the robustness of PoseDet on the CrowdPose benchmark, which has a larger crowd index and more persons in an image than COCO. The inference speed of PoseDet is not affected by the number of persons, which is the same on CrowdPose as COCO, so we focus on evaluating the accuracy on CrowdPose. PoseDet with HRNet-W48 backbone is trained using the train set and evaluated using the test set of CrowdPose with TTA. TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3 Robustness in the Crowd Scenes â€£ 4 Experiments â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes the results compared with the state-of-the-art approaches.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Higher HRNetÂ <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite> achieves the best accuracy on CrowdPose. With the same backbone, PoseDet achieves higher inference speed and competitive accuracy than Higher HRNet, showing the robustness of PoseDet. Further, The accuracy of PoseDet suppress top-down methods (Mask-RCNNÂ <cite class="ltx_cite ltx_citemacro_cite">He <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>, AlphaPoseÂ <cite class="ltx_cite ltx_citemacro_cite">Fang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite> and Simple BaselineÂ <cite class="ltx_cite ltx_citemacro_cite">Xiao <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib38" title="" class="ltx_ref">2018</a>)</cite>) by a large margin, especially in the heavily occluded cases (<math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{H}}" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><msup id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mtext id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3a.cmml">H</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2a.cmml" xref="S4.SS3.p2.1.m1.1.1.2"><mtext id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">AP</mtext></ci><ci id="S4.SS3.p2.1.m1.1.1.3a.cmml" xref="S4.SS3.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">H</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\text{AP}^{\text{H}}</annotation></semantics></math>). The main reason is that these top-down methods use a person detector to separate person instances with rectangular boxes. As discussed inÂ <a href="#S1" title="1 Introduction â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, bounding boxes perform poorly for occluded cases.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">The reason for robustness is that PoseDet adopts keypoint-aware pose embedding. The offsets of pose embedding are learnable and supervised by keypoints. As shown in FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.3 Robustness in the Crowd Scenes â€£ 4 Experiments â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, PoseDet achieves a more accurate prediction than Point AnchorsÂ <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite> in corner cases, where Point Anchors use fixed anchors as offsets.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:184.3pt;height:45.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.3pt,4.1pt) scale(0.85,0.85) ;">
<table id="S4.T4.6.6" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.6.6.6" class="ltx_tr">
<th id="S4.T4.6.6.6.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">JR</th>
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_markedasmath">AP</span></th>
<th id="S4.T4.2.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{50}}" display="inline"><semantics id="S4.T4.2.2.2.2.m1.1a"><msup id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml"><mtext id="S4.T4.2.2.2.2.m1.1.1.2" xref="S4.T4.2.2.2.2.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T4.2.2.2.2.m1.1.1.3" xref="S4.T4.2.2.2.2.m1.1.1.3a.cmml">50</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><apply id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.2.2.2.2.m1.1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">superscript</csymbol><ci id="S4.T4.2.2.2.2.m1.1.1.2a.cmml" xref="S4.T4.2.2.2.2.m1.1.1.2"><mtext id="S4.T4.2.2.2.2.m1.1.1.2.cmml" xref="S4.T4.2.2.2.2.m1.1.1.2">AP</mtext></ci><ci id="S4.T4.2.2.2.2.m1.1.1.3a.cmml" xref="S4.T4.2.2.2.2.m1.1.1.3"><mtext mathsize="70%" id="S4.T4.2.2.2.2.m1.1.1.3.cmml" xref="S4.T4.2.2.2.2.m1.1.1.3">50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\text{AP}^{\text{50}}</annotation></semantics></math></th>
<th id="S4.T4.3.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T4.3.3.3.3.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{75}}" display="inline"><semantics id="S4.T4.3.3.3.3.m1.1a"><msup id="S4.T4.3.3.3.3.m1.1.1" xref="S4.T4.3.3.3.3.m1.1.1.cmml"><mtext id="S4.T4.3.3.3.3.m1.1.1.2" xref="S4.T4.3.3.3.3.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T4.3.3.3.3.m1.1.1.3" xref="S4.T4.3.3.3.3.m1.1.1.3a.cmml">75</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.3.m1.1b"><apply id="S4.T4.3.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.3.3.3.3.m1.1.1.1.cmml" xref="S4.T4.3.3.3.3.m1.1.1">superscript</csymbol><ci id="S4.T4.3.3.3.3.m1.1.1.2a.cmml" xref="S4.T4.3.3.3.3.m1.1.1.2"><mtext id="S4.T4.3.3.3.3.m1.1.1.2.cmml" xref="S4.T4.3.3.3.3.m1.1.1.2">AP</mtext></ci><ci id="S4.T4.3.3.3.3.m1.1.1.3a.cmml" xref="S4.T4.3.3.3.3.m1.1.1.3"><mtext mathsize="70%" id="S4.T4.3.3.3.3.m1.1.1.3.cmml" xref="S4.T4.3.3.3.3.m1.1.1.3">75</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.3.m1.1c">\text{AP}^{\text{75}}</annotation></semantics></math></th>
<th id="S4.T4.4.4.4.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T4.4.4.4.4.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{E}}" display="inline"><semantics id="S4.T4.4.4.4.4.m1.1a"><msup id="S4.T4.4.4.4.4.m1.1.1" xref="S4.T4.4.4.4.4.m1.1.1.cmml"><mtext id="S4.T4.4.4.4.4.m1.1.1.2" xref="S4.T4.4.4.4.4.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T4.4.4.4.4.m1.1.1.3" xref="S4.T4.4.4.4.4.m1.1.1.3a.cmml">E</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.4.m1.1b"><apply id="S4.T4.4.4.4.4.m1.1.1.cmml" xref="S4.T4.4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.4.4.4.4.m1.1.1.1.cmml" xref="S4.T4.4.4.4.4.m1.1.1">superscript</csymbol><ci id="S4.T4.4.4.4.4.m1.1.1.2a.cmml" xref="S4.T4.4.4.4.4.m1.1.1.2"><mtext id="S4.T4.4.4.4.4.m1.1.1.2.cmml" xref="S4.T4.4.4.4.4.m1.1.1.2">AP</mtext></ci><ci id="S4.T4.4.4.4.4.m1.1.1.3a.cmml" xref="S4.T4.4.4.4.4.m1.1.1.3"><mtext mathsize="70%" id="S4.T4.4.4.4.4.m1.1.1.3.cmml" xref="S4.T4.4.4.4.4.m1.1.1.3">E</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.4.m1.1c">\text{AP}^{\text{E}}</annotation></semantics></math></th>
<th id="S4.T4.5.5.5.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T4.5.5.5.5.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{M}}" display="inline"><semantics id="S4.T4.5.5.5.5.m1.1a"><msup id="S4.T4.5.5.5.5.m1.1.1" xref="S4.T4.5.5.5.5.m1.1.1.cmml"><mtext id="S4.T4.5.5.5.5.m1.1.1.2" xref="S4.T4.5.5.5.5.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T4.5.5.5.5.m1.1.1.3" xref="S4.T4.5.5.5.5.m1.1.1.3a.cmml">M</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.5.m1.1b"><apply id="S4.T4.5.5.5.5.m1.1.1.cmml" xref="S4.T4.5.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.5.5.5.5.m1.1.1.1.cmml" xref="S4.T4.5.5.5.5.m1.1.1">superscript</csymbol><ci id="S4.T4.5.5.5.5.m1.1.1.2a.cmml" xref="S4.T4.5.5.5.5.m1.1.1.2"><mtext id="S4.T4.5.5.5.5.m1.1.1.2.cmml" xref="S4.T4.5.5.5.5.m1.1.1.2">AP</mtext></ci><ci id="S4.T4.5.5.5.5.m1.1.1.3a.cmml" xref="S4.T4.5.5.5.5.m1.1.1.3"><mtext mathsize="70%" id="S4.T4.5.5.5.5.m1.1.1.3.cmml" xref="S4.T4.5.5.5.5.m1.1.1.3">M</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.5.m1.1c">\text{AP}^{\text{M}}</annotation></semantics></math></th>
<th id="S4.T4.6.6.6.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T4.6.6.6.6.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{H}}" display="inline"><semantics id="S4.T4.6.6.6.6.m1.1a"><msup id="S4.T4.6.6.6.6.m1.1.1" xref="S4.T4.6.6.6.6.m1.1.1.cmml"><mtext id="S4.T4.6.6.6.6.m1.1.1.2" xref="S4.T4.6.6.6.6.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T4.6.6.6.6.m1.1.1.3" xref="S4.T4.6.6.6.6.m1.1.1.3a.cmml">H</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.6.m1.1b"><apply id="S4.T4.6.6.6.6.m1.1.1.cmml" xref="S4.T4.6.6.6.6.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.6.6.6.6.m1.1.1.1.cmml" xref="S4.T4.6.6.6.6.m1.1.1">superscript</csymbol><ci id="S4.T4.6.6.6.6.m1.1.1.2a.cmml" xref="S4.T4.6.6.6.6.m1.1.1.2"><mtext id="S4.T4.6.6.6.6.m1.1.1.2.cmml" xref="S4.T4.6.6.6.6.m1.1.1.2">AP</mtext></ci><ci id="S4.T4.6.6.6.6.m1.1.1.3a.cmml" xref="S4.T4.6.6.6.6.m1.1.1.3"><mtext mathsize="70%" id="S4.T4.6.6.6.6.m1.1.1.3.cmml" xref="S4.T4.6.6.6.6.m1.1.1.3">H</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.6.m1.1c">\text{AP}^{\text{H}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.6.6.7.1" class="ltx_tr">
<th id="S4.T4.6.6.7.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T4.6.6.7.1.2" class="ltx_td ltx_align_right ltx_border_t">51.7</td>
<td id="S4.T4.6.6.7.1.3" class="ltx_td ltx_align_right ltx_border_t">85.2</td>
<td id="S4.T4.6.6.7.1.4" class="ltx_td ltx_align_right ltx_border_t">54.9</td>
<td id="S4.T4.6.6.7.1.5" class="ltx_td ltx_align_right ltx_border_t">57.8</td>
<td id="S4.T4.6.6.7.1.6" class="ltx_td ltx_align_right ltx_border_t">52.1</td>
<td id="S4.T4.6.6.7.1.7" class="ltx_td ltx_align_right ltx_border_t">45.5</td>
</tr>
<tr id="S4.T4.6.6.8.2" class="ltx_tr">
<th id="S4.T4.6.6.8.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">âœ“</th>
<td id="S4.T4.6.6.8.2.2" class="ltx_td ltx_align_right ltx_border_bb">65.9</td>
<td id="S4.T4.6.6.8.2.3" class="ltx_td ltx_align_right ltx_border_bb">87.7</td>
<td id="S4.T4.6.6.8.2.4" class="ltx_td ltx_align_right ltx_border_bb">76.1</td>
<td id="S4.T4.6.6.8.2.5" class="ltx_td ltx_align_right ltx_border_bb">73.6</td>
<td id="S4.T4.6.6.8.2.6" class="ltx_td ltx_align_right ltx_border_bb">66.8</td>
<td id="S4.T4.6.6.8.2.7" class="ltx_td ltx_align_right ltx_border_bb">56.9</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Effect of joint refinement (JR) on CrowdPose with backbone HRNet-W48.</figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<div id="S4.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:212.4pt;height:137.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.7pt,12.1pt) scale(0.85,0.85) ;">
<table id="S4.T5.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.3.3.3" class="ltx_tr">
<th id="S4.T5.3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Centroid type</th>
<th id="S4.T5.3.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">NMS type</th>
<th id="S4.T5.3.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">IS</th>
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{E}}" display="inline"><semantics id="S4.T5.1.1.1.1.m1.1a"><msup id="S4.T5.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.cmml"><mtext id="S4.T5.1.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T5.1.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.1.m1.1.1.3a.cmml">E</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.1.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S4.T5.1.1.1.1.m1.1.1.2a.cmml" xref="S4.T5.1.1.1.1.m1.1.1.2"><mtext id="S4.T5.1.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.T5.1.1.1.1.m1.1.1.3a.cmml" xref="S4.T5.1.1.1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T5.1.1.1.1.m1.1.1.3.cmml" xref="S4.T5.1.1.1.1.m1.1.1.3">E</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.m1.1c">\text{AP}^{\text{E}}</annotation></semantics></math></th>
<th id="S4.T5.2.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T5.2.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{M}}" display="inline"><semantics id="S4.T5.2.2.2.2.m1.1a"><msup id="S4.T5.2.2.2.2.m1.1.1" xref="S4.T5.2.2.2.2.m1.1.1.cmml"><mtext id="S4.T5.2.2.2.2.m1.1.1.2" xref="S4.T5.2.2.2.2.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T5.2.2.2.2.m1.1.1.3" xref="S4.T5.2.2.2.2.m1.1.1.3a.cmml">M</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.m1.1b"><apply id="S4.T5.2.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.2.2.2.2.m1.1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1">superscript</csymbol><ci id="S4.T5.2.2.2.2.m1.1.1.2a.cmml" xref="S4.T5.2.2.2.2.m1.1.1.2"><mtext id="S4.T5.2.2.2.2.m1.1.1.2.cmml" xref="S4.T5.2.2.2.2.m1.1.1.2">AP</mtext></ci><ci id="S4.T5.2.2.2.2.m1.1.1.3a.cmml" xref="S4.T5.2.2.2.2.m1.1.1.3"><mtext mathsize="70%" id="S4.T5.2.2.2.2.m1.1.1.3.cmml" xref="S4.T5.2.2.2.2.m1.1.1.3">M</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.m1.1c">\text{AP}^{\text{M}}</annotation></semantics></math></th>
<th id="S4.T5.3.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S4.T5.3.3.3.3.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{H}}" display="inline"><semantics id="S4.T5.3.3.3.3.m1.1a"><msup id="S4.T5.3.3.3.3.m1.1.1" xref="S4.T5.3.3.3.3.m1.1.1.cmml"><mtext id="S4.T5.3.3.3.3.m1.1.1.2" xref="S4.T5.3.3.3.3.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.T5.3.3.3.3.m1.1.1.3" xref="S4.T5.3.3.3.3.m1.1.1.3a.cmml">H</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.3.m1.1b"><apply id="S4.T5.3.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.3.3.3.3.m1.1.1.1.cmml" xref="S4.T5.3.3.3.3.m1.1.1">superscript</csymbol><ci id="S4.T5.3.3.3.3.m1.1.1.2a.cmml" xref="S4.T5.3.3.3.3.m1.1.1.2"><mtext id="S4.T5.3.3.3.3.m1.1.1.2.cmml" xref="S4.T5.3.3.3.3.m1.1.1.2">AP</mtext></ci><ci id="S4.T5.3.3.3.3.m1.1.1.3a.cmml" xref="S4.T5.3.3.3.3.m1.1.1.3"><mtext mathsize="70%" id="S4.T5.3.3.3.3.m1.1.1.3.cmml" xref="S4.T5.3.3.3.3.m1.1.1.3">H</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.3.m1.1c">\text{AP}^{\text{H}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.3.3.4.1" class="ltx_tr">
<th id="S4.T5.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T5.3.3.4.1.1.1" class="ltx_text">Bounding box</span></th>
<th id="S4.T5.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.3.3.4.1.2.1" class="ltx_text">IoU</span></th>
<th id="S4.T5.3.3.4.1.3" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T5.3.3.4.1.4" class="ltx_td ltx_align_right ltx_border_t">72.4</td>
<td id="S4.T5.3.3.4.1.5" class="ltx_td ltx_align_right ltx_border_t">65.3</td>
<td id="S4.T5.3.3.4.1.6" class="ltx_td ltx_align_right ltx_border_t">54.4</td>
</tr>
<tr id="S4.T5.3.3.5.2" class="ltx_tr">
<th id="S4.T5.3.3.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">âœ“</th>
<td id="S4.T5.3.3.5.2.2" class="ltx_td ltx_align_right ltx_border_t">73.3</td>
<td id="S4.T5.3.3.5.2.3" class="ltx_td ltx_align_right ltx_border_t">65.9</td>
<td id="S4.T5.3.3.5.2.4" class="ltx_td ltx_align_right ltx_border_t">55.2</td>
</tr>
<tr id="S4.T5.3.3.6.3" class="ltx_tr">
<th id="S4.T5.3.3.6.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.3.3.6.3.1.1" class="ltx_text">OKS</span></th>
<th id="S4.T5.3.3.6.3.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T5.3.3.6.3.3" class="ltx_td ltx_align_right ltx_border_t">72.9</td>
<td id="S4.T5.3.3.6.3.4" class="ltx_td ltx_align_right ltx_border_t">66.1</td>
<td id="S4.T5.3.3.6.3.5" class="ltx_td ltx_align_right ltx_border_t">55.7</td>
</tr>
<tr id="S4.T5.3.3.7.4" class="ltx_tr">
<th id="S4.T5.3.3.7.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">âœ“</th>
<td id="S4.T5.3.3.7.4.2" class="ltx_td ltx_align_right ltx_border_t">73.6</td>
<td id="S4.T5.3.3.7.4.3" class="ltx_td ltx_align_right ltx_border_t">66.7</td>
<td id="S4.T5.3.3.7.4.4" class="ltx_td ltx_align_right ltx_border_t">56.6</td>
</tr>
<tr id="S4.T5.3.3.8.5" class="ltx_tr">
<th id="S4.T5.3.3.8.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T5.3.3.8.5.1.1" class="ltx_text">Keypoints</span></th>
<th id="S4.T5.3.3.8.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.3.3.8.5.2.1" class="ltx_text">IoU</span></th>
<th id="S4.T5.3.3.8.5.3" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T5.3.3.8.5.4" class="ltx_td ltx_align_right ltx_border_t">72.6</td>
<td id="S4.T5.3.3.8.5.5" class="ltx_td ltx_align_right ltx_border_t">65.5</td>
<td id="S4.T5.3.3.8.5.6" class="ltx_td ltx_align_right ltx_border_t">55.0</td>
</tr>
<tr id="S4.T5.3.3.9.6" class="ltx_tr">
<th id="S4.T5.3.3.9.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">âœ“</th>
<td id="S4.T5.3.3.9.6.2" class="ltx_td ltx_align_right ltx_border_t">73.3</td>
<td id="S4.T5.3.3.9.6.3" class="ltx_td ltx_align_right ltx_border_t">66.1</td>
<td id="S4.T5.3.3.9.6.4" class="ltx_td ltx_align_right ltx_border_t">55.7</td>
</tr>
<tr id="S4.T5.3.3.10.7" class="ltx_tr">
<th id="S4.T5.3.3.10.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.3.3.10.7.1.1" class="ltx_text">OKS</span></th>
<th id="S4.T5.3.3.10.7.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T5.3.3.10.7.3" class="ltx_td ltx_align_right ltx_border_t">72.9</td>
<td id="S4.T5.3.3.10.7.4" class="ltx_td ltx_align_right ltx_border_t">66.2</td>
<td id="S4.T5.3.3.10.7.5" class="ltx_td ltx_align_right ltx_border_t">56.6</td>
</tr>
<tr id="S4.T5.3.3.11.8" class="ltx_tr">
<th id="S4.T5.3.3.11.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">âœ“</th>
<td id="S4.T5.3.3.11.8.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S4.T5.3.3.11.8.2.1" class="ltx_text ltx_font_bold">73.6</span></td>
<td id="S4.T5.3.3.11.8.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S4.T5.3.3.11.8.3.1" class="ltx_text ltx_font_bold">66.8</span></td>
<td id="S4.T5.3.3.11.8.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S4.T5.3.3.11.8.4.1" class="ltx_text ltx_font_bold">56.9</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Effect of keypoint-based operations on CrowdPose with backbone HRNet-W48. (<span id="S4.T5.5.1" class="ltx_text ltx_font_italic">IS: Intermediate supervision</span>)</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2107.10466/assets/figs/picked_image.jpg" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="494" height="136" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of Point Anchors and PoseDet. The upper row is predictions of Point Anchors, and the lower row is results of PoseDet. PoseDet shows more robustness in corner cases (<em id="S4.F4.2.1" class="ltx_emph ltx_font_italic">i.e.</em>, larger persons and rare poses).</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2107.10466/assets/figs/picked_image_woDR.jpg" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="494" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Predictions of PoseDet, where the upper row is the coarse predictions and the lower row is the finer predictions.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">PoseDet is basically a detector. Displacement refinement and keypoint-based operations are adopted to make it friendly for pose estimation. We perform ablation experiments to analyze the effect of these strategies. All experiments are conducted using PoseDet with backbone HRNet-W48. Models are trained using the train set, tested on the test set of CrowdPose without TTA.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.4" class="ltx_p"><span id="S4.SS4.p2.4.1" class="ltx_text ltx_font_bold">Effect of displacement refinement.</span> TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.3 Robustness in the Crowd Scenes â€£ 4 Experiments â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the results of using displacement refinement (DR). PoseDet conducts DR via pose embedding based on the coarse prediction. The metrics, <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{50}}" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><msup id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mtext id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.SS4.p2.1.m1.1.1.3" xref="S4.SS4.p2.1.m1.1.1.3a.cmml">50</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS4.p2.1.m1.1.1.2a.cmml" xref="S4.SS4.p2.1.m1.1.1.2"><mtext id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">AP</mtext></ci><ci id="S4.SS4.p2.1.m1.1.1.3a.cmml" xref="S4.SS4.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS4.p2.1.m1.1.1.3.cmml" xref="S4.SS4.p2.1.m1.1.1.3">50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">\text{AP}^{\text{50}}</annotation></semantics></math> and <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="\text{AP}^{\text{75}}" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><msup id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mtext id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2a.cmml">AP</mtext><mtext id="S4.SS4.p2.2.m2.1.1.3" xref="S4.SS4.p2.2.m2.1.1.3a.cmml">75</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">superscript</csymbol><ci id="S4.SS4.p2.2.m2.1.1.2a.cmml" xref="S4.SS4.p2.2.m2.1.1.2"><mtext id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">AP</mtext></ci><ci id="S4.SS4.p2.2.m2.1.1.3a.cmml" xref="S4.SS4.p2.2.m2.1.1.3"><mtext mathsize="70%" id="S4.SS4.p2.2.m2.1.1.3.cmml" xref="S4.SS4.p2.2.m2.1.1.3">75</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">\text{AP}^{\text{75}}</annotation></semantics></math>, measure predictions with OKS higher than 50 and 75 respectively. We can find out that PoseDet without DR already achieves strong performance on the looser metric (85.2 <math id="S4.SS4.p2.3.m3.1" class="ltx_Math" alttext="\text{AP}^{\text{50}}" display="inline"><semantics id="S4.SS4.p2.3.m3.1a"><msup id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mtext id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2a.cmml">AP</mtext><mtext id="S4.SS4.p2.3.m3.1.1.3" xref="S4.SS4.p2.3.m3.1.1.3a.cmml">50</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1">superscript</csymbol><ci id="S4.SS4.p2.3.m3.1.1.2a.cmml" xref="S4.SS4.p2.3.m3.1.1.2"><mtext id="S4.SS4.p2.3.m3.1.1.2.cmml" xref="S4.SS4.p2.3.m3.1.1.2">AP</mtext></ci><ci id="S4.SS4.p2.3.m3.1.1.3a.cmml" xref="S4.SS4.p2.3.m3.1.1.3"><mtext mathsize="70%" id="S4.SS4.p2.3.m3.1.1.3.cmml" xref="S4.SS4.p2.3.m3.1.1.3">50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">\text{AP}^{\text{50}}</annotation></semantics></math>). DR improve the accurate prediction by a large margin (+21.2 <math id="S4.SS4.p2.4.m4.1" class="ltx_Math" alttext="\text{AP}^{\text{75}}" display="inline"><semantics id="S4.SS4.p2.4.m4.1a"><msup id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml"><mtext id="S4.SS4.p2.4.m4.1.1.2" xref="S4.SS4.p2.4.m4.1.1.2a.cmml">AP</mtext><mtext id="S4.SS4.p2.4.m4.1.1.3" xref="S4.SS4.p2.4.m4.1.1.3a.cmml">75</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><apply id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.4.m4.1.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1">superscript</csymbol><ci id="S4.SS4.p2.4.m4.1.1.2a.cmml" xref="S4.SS4.p2.4.m4.1.1.2"><mtext id="S4.SS4.p2.4.m4.1.1.2.cmml" xref="S4.SS4.p2.4.m4.1.1.2">AP</mtext></ci><ci id="S4.SS4.p2.4.m4.1.1.3a.cmml" xref="S4.SS4.p2.4.m4.1.1.3"><mtext mathsize="70%" id="S4.SS4.p2.4.m4.1.1.3.cmml" xref="S4.SS4.p2.4.m4.1.1.3">75</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">\text{AP}^{\text{75}}</annotation></semantics></math>). The comparison demonstrates the effectiveness of pose embedding on encoding the bias of the coarse prediction.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">The qualitative analysis in FigureÂ <a href="#S4.F5" title="Figure 5 â€£ 4.3 Robustness in the Crowd Scenes â€£ 4 Experiments â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> comes to consistent results. In the upper row, coarse predictions cover most keypoints. A high <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{50}}" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><msup id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mtext id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.SS4.p3.1.m1.1.1.3" xref="S4.SS4.p3.1.m1.1.1.3a.cmml">50</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">superscript</csymbol><ci id="S4.SS4.p3.1.m1.1.1.2a.cmml" xref="S4.SS4.p3.1.m1.1.1.2"><mtext id="S4.SS4.p3.1.m1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.2">AP</mtext></ci><ci id="S4.SS4.p3.1.m1.1.1.3a.cmml" xref="S4.SS4.p3.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS4.p3.1.m1.1.1.3.cmml" xref="S4.SS4.p3.1.m1.1.1.3">50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\text{AP}^{\text{50}}</annotation></semantics></math> is achieved with the coarse prediction. It also provides offsets for the DCN layer to generate keypoint-aware pose embedding, where the offsets are closely relative to keypoints. In the upper row, some keypoints that are not accurate enough are refined by predictions of pose embeddings. Estimation of poses is handled in a coarse-to-fine manner successfully.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p"><span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold">Effect of keypoint-based operations.</span> We compare PoseDet with different strategies of target assignment and NMS, as well as intermediate supervision in TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.3 Robustness in the Crowd Scenes â€£ 4 Experiments â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">The centroid of the bounding box is widely used as a basis to assign targets in object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">Qiu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib29" title="" class="ltx_ref">2020</a>); Tian <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite>. Pose estimation contains information on keypoints, which is more informative than the bounding box. We make use of it by replacing the centroid of the bounding box with the centroid of keypoints. We argue that the centroid of keypoints works better in occluded cases. We compare the performance that assigning a GT pose to the closest candidate according to the centroid of the bounding box or keypoints during training. In the results, we can find out that the centroid of keypoints improves the accuracy consistently.</p>
</div>
<div id="S4.SS4.p6" class="ltx_para">
<p id="S4.SS4.p6.1" class="ltx_p">As discussed in SectionÂ <a href="#S3.SS2.SSS4" title="3.2.4 Training and Inference â€£ 3.2 PoseDet â€£ 3 Approach â€£ PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.4</span></a>, OKS is a more discriminate measurement to distinguish persons in the crowd than IoU. We conduct experiments with IoU NMS and OKS NMS. OKS NMS improves the performance of PoseDet in the crowd scenes (+1.4 <math id="S4.SS4.p6.1.m1.1" class="ltx_Math" alttext="\text{AP}^{\text{H}}" display="inline"><semantics id="S4.SS4.p6.1.m1.1a"><msup id="S4.SS4.p6.1.m1.1.1" xref="S4.SS4.p6.1.m1.1.1.cmml"><mtext id="S4.SS4.p6.1.m1.1.1.2" xref="S4.SS4.p6.1.m1.1.1.2a.cmml">AP</mtext><mtext id="S4.SS4.p6.1.m1.1.1.3" xref="S4.SS4.p6.1.m1.1.1.3a.cmml">H</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p6.1.m1.1b"><apply id="S4.SS4.p6.1.m1.1.1.cmml" xref="S4.SS4.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p6.1.m1.1.1.1.cmml" xref="S4.SS4.p6.1.m1.1.1">superscript</csymbol><ci id="S4.SS4.p6.1.m1.1.1.2a.cmml" xref="S4.SS4.p6.1.m1.1.1.2"><mtext id="S4.SS4.p6.1.m1.1.1.2.cmml" xref="S4.SS4.p6.1.m1.1.1.2">AP</mtext></ci><ci id="S4.SS4.p6.1.m1.1.1.3a.cmml" xref="S4.SS4.p6.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS4.p6.1.m1.1.1.3.cmml" xref="S4.SS4.p6.1.m1.1.1.3">H</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p6.1.m1.1c">\text{AP}^{\text{H}}</annotation></semantics></math> on average).</p>
</div>
<div id="S4.SS4.p7" class="ltx_para">
<p id="S4.SS4.p7.1" class="ltx_p">The keypoints are further used in intermediate supervision. We use GT keypoints to generate Gaussian maps as supervision of FPN, forcing the FPN to perceive body joints. The accuracy is improved consistently.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Works</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we present keypoint-aware pose embedding, which can be seen as a general strategy to efficiently extract and aggregate informative feature representation.
We propose PoseDet, a simple yet efficient method for multi-person pose estimation, in conjunction with the pose embedding.
PoseDet simplifies the procedures of estimating, predicting accurate poses at an unprecedented speed.
We conduct displacement refinement and keypoint-based operations to improve the accuracy of PoseDet.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The future works include the applications to other instance-level recognition tasks, <em id="S5.p2.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, person re-identification, pose tracking, as well as the investigation on selecting the number of keypoints adaptively.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andriluka <span id="bib.bib1.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2014]</span>
<span class="ltx_bibblock">
Mykhaylo Andriluka, Leonid Pishchulin, PeterÂ V. Gehler, and Bernt Schiele.

</span>
<span class="ltx_bibblock">2d human pose estimation: New benchmark and state of the art
analysis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.3.1" class="ltx_text ltx_font_italic">2014 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2014</span>, pages 3686â€“3693. IEEE Computer Society, 2014.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai <span id="bib.bib2.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Yuanhao Cai, Zhicheng Wang, Zhengxiong Luo, Binyi Yin, Angang Du, Haoqian Wang,
Xiangyu Zhang, Xinyu Zhou, Erjin Zhou, and Jian Sun.

</span>
<span class="ltx_bibblock">Learning delicate local representations for multi-person pose
estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.3.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2020 - 16th European Conference,
Part III</span>, volume 12348 of <span id="bib.bib2.4.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pages
455â€“472. Springer, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao <span id="bib.bib3.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.

</span>
<span class="ltx_bibblock">Realtime multi-person 2d pose estimation using part affinity fields.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.3.1" class="ltx_text ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017</span>, pages 1302â€“1310. IEEE Computer Society, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao <span id="bib.bib4.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh.

</span>
<span class="ltx_bibblock">Openpose: Realtime multi-person 2d pose estimation using part
affinity fields.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</span>, 43(1):172â€“186,
2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span id="bib.bib5.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun.

</span>
<span class="ltx_bibblock">Cascaded pyramid network for multi-person pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.3.1" class="ltx_text ltx_font_italic">2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018</span>, pages 7103â€“7112. IEEE Computer Society, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span id="bib.bib6.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, YuÂ Xiong, Xiaoxiao Li,
Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng,
Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,
Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, ChenÂ Change Loy, and
Dahua Lin.

</span>
<span class="ltx_bibblock">Mmdetection: Open mmlab detection toolbox and benchmark.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1906.07155, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng <span id="bib.bib7.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, ThomasÂ S. Huang, and Lei
Zhang.

</span>
<span class="ltx_bibblock">Higherhrnet: Scale-aware representation learning for bottom-up human
pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.3.1" class="ltx_text ltx_font_italic">2020 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2020</span>, pages 5385â€“5394. IEEE, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai <span id="bib.bib8.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Jifeng Dai, Haozhi Qi, Yuwen Xiong, YiÂ Li, Guodong Zhang, Han Hu, and Yichen
Wei.

</span>
<span class="ltx_bibblock">Deformable convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.3.1" class="ltx_text ltx_font_italic">IEEE International Conference on Computer Vision, ICCV
2017</span>, pages 764â€“773. IEEE Computer Society, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng <span id="bib.bib9.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2009]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.3.1" class="ltx_text ltx_font_italic">2009 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition CVPR 2009</span>, pages 248â€“255. IEEE Computer
Society, 2009.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang <span id="bib.bib10.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Haoshu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.

</span>
<span class="ltx_bibblock">RMPE: regional multi-person pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.3.1" class="ltx_text ltx_font_italic">IEEE International Conference on Computer Vision, ICCV
2017</span>, pages 2353â€“2362. IEEE Computer Society, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo <span id="bib.bib11.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Yuyu Guo, Lianli Gao, Jingkuan Song, Peng Wang, Wuyuan Xie, and HengÂ Tao Shen.

</span>
<span class="ltx_bibblock">Adaptive multi-path aggregation for human densepose estimation in the
wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.3.1" class="ltx_text ltx_font_italic">Proceedings of the 27th ACM International Conference on
Multimedia, MM2019</span>, pages 356â€“364. ACM, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He <span id="bib.bib12.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and RossÂ B. Girshick.

</span>
<span class="ltx_bibblock">Mask R-CNN.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.3.1" class="ltx_text ltx_font_italic">IEEE International Conference on Computer Vision, ICCV
2017</span>, pages 2980â€“2988. IEEE Computer Society, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang <span id="bib.bib13.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Shaoli Huang, Mingming Gong, and Dacheng Tao.

</span>
<span class="ltx_bibblock">A coarse-fine network for keypoint localization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.3.1" class="ltx_text ltx_font_italic">IEEE International Conference on Computer Vision, ICCV
2017</span>, pages 3047â€“3056. IEEE Computer Society, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Insafutdinov <span id="bib.bib14.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2016]</span>
<span class="ltx_bibblock">
Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, and
Bernt Schiele.

</span>
<span class="ltx_bibblock">Deepercut: A deeper, stronger, and faster multi-person pose
estimation model.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.3.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2016 - 14th European Conference,
Part VI</span>, volume 9910 of <span id="bib.bib14.4.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pages
34â€“50. Springer, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iqbal and Gall [2016]</span>
<span class="ltx_bibblock">
Umar Iqbal and Juergen Gall.

</span>
<span class="ltx_bibblock">Multi-person pose estimation with local joint-to-person associations.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2016 Workshops, Part II</span>, volume
9914 of <span id="bib.bib15.2.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pages 627â€“642, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang <span id="bib.bib16.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Chenru Jiang, Kaizhu Huang, Shufei Zhang, Xinheng Wang, and Jimin Xiao.

</span>
<span class="ltx_bibblock">Pay attention selectively and comprehensively: Pyramid gating network
for human pose estimation without pre-training.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.3.1" class="ltx_text ltx_font_italic">MM â€™20: The 28th ACM International Conference on
Multimedia, 2020</span>, pages 2364â€“2371. ACM, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba [2015]</span>
<span class="ltx_bibblock">
DiederikÂ P. Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">2015.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov <span id="bib.bib18.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Yuxin Wu, Kaiming He, and RossÂ B. Girshick.

</span>
<span class="ltx_bibblock">Pointrend: Image segmentation as rendering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.3.1" class="ltx_text ltx_font_italic">2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2020</span>, pages 9796â€“9805. IEEE, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocabas <span id="bib.bib19.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Muhammed Kocabas, Salih Karagoz, and Emre Akbas.

</span>
<span class="ltx_bibblock">Multiposenet: Fast multi-person pose estimation using pose residual
network.

</span>
<span class="ltx_bibblock">In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair
Weiss, editors, <span id="bib.bib19.3.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2018 - 15th European
Conference, Part XI</span>, volume 11215 of <span id="bib.bib19.4.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer
Science</span>, pages 437â€“453. Springer, 2018.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib20.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Haoshu Fang, and Cewu Lu.

</span>
<span class="ltx_bibblock">Crowdpose: Efficient crowded scenes pose estimation and a new
benchmark.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.3.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2019</span>, pages 10863â€“10872. IEEE, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin <span id="bib.bib21.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2014]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, SergeÂ J. Belongie, James Hays, Pietro Perona,
Deva Ramanan, Piotr DollÃ¡r, and C.Â Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO: common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.3.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2014 - 13th European Conference,
Proceedings, Part V</span>, volume 8693 of <span id="bib.bib21.4.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer
Science</span>, pages 740â€“755. Springer, 2014.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin <span id="bib.bib22.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Priya Goyal, RossÂ B. Girshick, Kaiming He, and Piotr
DollÃ¡r.

</span>
<span class="ltx_bibblock">Focal loss for dense object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.3.1" class="ltx_text ltx_font_italic">IEEE International Conference on Computer Vision, ICCV
2017</span>, pages 2999â€“3007. IEEE Computer Society, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newell <span id="bib.bib23.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2016]</span>
<span class="ltx_bibblock">
Alejandro Newell, Kaiyu Yang, and Jia Deng.

</span>
<span class="ltx_bibblock">Stacked hourglass networks for human pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.3.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2016 - 14th European Conference,
Part VIII</span>, volume 9912 of <span id="bib.bib23.4.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pages
483â€“499. Springer, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newell <span id="bib.bib24.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Alejandro Newell, Zhiao Huang, and Jia Deng.

</span>
<span class="ltx_bibblock">Associative embedding: End-to-end learning for joint detection and
grouping.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.3.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017</span>, pages 2277â€“2287,
2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie <span id="bib.bib25.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Xuecheng Nie, Jiashi Feng, Jianfeng Zhang, and Shuicheng Yan.

</span>
<span class="ltx_bibblock">Single-stage multi-person pose machines.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.3.1" class="ltx_text ltx_font_italic">2019 IEEE/CVF International Conference on Computer Vision,
ICCV 2019</span>, pages 6950â€“6959. IEEE, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paccanaro and
Hinton [2001]</span>
<span class="ltx_bibblock">
Alberto Paccanaro and GeoffreyÂ E. Hinton.

</span>
<span class="ltx_bibblock">Learning distributed representations of concepts using linear
relational embedding.

</span>
<span class="ltx_bibblock">volumeÂ 13, pages 232â€“244, 2001.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papandreou <span id="bib.bib27.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan
Tompson, Chris Bregler, and Kevin Murphy.

</span>
<span class="ltx_bibblock">Towards accurate multi-person pose estimation in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.3.1" class="ltx_text ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017</span>, pages 3711â€“3719. IEEE Computer Society, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pishchulin <span id="bib.bib28.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2016]</span>
<span class="ltx_bibblock">
Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo
Andriluka, PeterÂ V. Gehler, and Bernt Schiele.

</span>
<span class="ltx_bibblock">Deepcut: Joint subset partition and labeling for multi person pose
estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.3.1" class="ltx_text ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016</span>, pages 4929â€“4937. IEEE Computer Society, 2016.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu <span id="bib.bib29.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Han Qiu, Yuchen Ma, Zeming Li, Songtao Liu, and Jian Sun.

</span>
<span class="ltx_bibblock">Borderdet: Border feature for dense object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.3.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2020 - 16th European Conference,
Part I</span>, volume 12346 of <span id="bib.bib29.4.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pages
549â€“564. Springer, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren <span id="bib.bib30.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2015]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, RossÂ B. Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster R-CNN: towards real-time object detection with region
proposal networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.3.1" class="ltx_text ltx_font_italic">Annual Conference on Neural Information Processing Systems
2015 NeurIPS</span>, pages 91â€“99, 2015.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruan <span id="bib.bib31.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Weijian Ruan, WuÂ Liu, Qian Bao, Jun Chen, Yuhao Cheng, and Tao Mei.

</span>
<span class="ltx_bibblock">Poinet: Pose-guided ovonic insight network for multi-person pose
tracking.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.3.1" class="ltx_text ltx_font_italic">Proceedings of the 27th ACM International Conference on
Multimedia, MM 2019</span>, pages 284â€“292. ACM, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun and Savarese [2011]</span>
<span class="ltx_bibblock">
Min Sun and Silvio Savarese.

</span>
<span class="ltx_bibblock">Articulated part-based model for joint object detection and pose
estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Computer Vision, ICCV
2011</span>, pages 723â€“730. IEEE Computer Society, 2011.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun <span id="bib.bib33.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
KeÂ Sun, Bin Xiao, Dong Liu, and Jingdong Wang.

</span>
<span class="ltx_bibblock">Deep high-resolution representation learning for human pose
estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.3.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2019</span>, pages 5693â€“5703. IEEE, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian <span id="bib.bib34.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Zhi Tian, Chunhua Shen, Hao Chen, and Tong He.

</span>
<span class="ltx_bibblock">FCOS: fully convolutional one-stage object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.3.1" class="ltx_text ltx_font_italic">2019 IEEE/CVF International Conference on Computer Vision,
ICCV</span>, pages 9626â€“9635. IEEE, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Li [2013]</span>
<span class="ltx_bibblock">
Fang Wang and YiÂ Li.

</span>
<span class="ltx_bibblock">Beyond physical connections: Tree models in human pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">2013 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR</span>, pages 596â€“603. IEEE Computer Society, 2013.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei <span id="bib.bib36.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2016]</span>
<span class="ltx_bibblock">
Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh.

</span>
<span class="ltx_bibblock">Convolutional pose machines.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.3.1" class="ltx_text ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016</span>, pages
4724â€“4732. IEEE Computer Society, 2016.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei <span id="bib.bib37.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Fangyun Wei, Xiao Sun, Hongyang Li, Jingdong Wang, and Stephen Lin.

</span>
<span class="ltx_bibblock">Point-set anchors for object detection, instance segmentation and
pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.3.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2020 - 16th European Conference,
Part X</span>, volume 12355 of <span id="bib.bib37.4.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pages
527â€“544. Springer, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao <span id="bib.bib38.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Bin Xiao, Haiping Wu, and Yichen Wei.

</span>
<span class="ltx_bibblock">Simple baselines for human pose estimation and tracking.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.3.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2018 - 15th European Conference,
Part VI</span>, volume 11210 of <span id="bib.bib38.4.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pages
472â€“487. Springer, 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang <span id="bib.bib39.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
ZeÂ Yang, Shaohui Liu, Han Hu, Liwei Wang, and Stephen Lin.

</span>
<span class="ltx_bibblock">Reppoints: Point set representation for object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.3.1" class="ltx_text ltx_font_italic">2019 IEEE International Conference on Computer Vision,
ICCV 2019</span>, pages 9656â€“9665. IEEE, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu <span id="bib.bib40.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Deep layer aggregation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.3.1" class="ltx_text ltx_font_italic">2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018</span>, pages 2403â€“2412. IEEE Computer Society, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou <span id="bib.bib41.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Xingyi Zhou, Dequan Wang, and Philipp KrÃ¤henbÃ¼hl.

</span>
<span class="ltx_bibblock">Objects as points.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1904.07850, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2107.10465" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2107.10466" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2107.10466">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2107.10466" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2107.10467" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 05:29:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
