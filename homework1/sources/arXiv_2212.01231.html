<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.01231] BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks</title><meta property="og:description" content="Bird’s-Eye-View (BEV) 3D Object Detection is a crucial multi-view technique for autonomous driving systems. Recently, plenty of works are proposed, following a similar paradigm consisting of three essential components,…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.01231">

<!--Generated on Fri Mar  1 08:15:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaowei Chi<sup id="id1.1.id1" class="ltx_sup">1,4</sup>,
Jiaming Liu<sup id="id2.2.id2" class="ltx_sup">1*</sup>,
Ming Lu<sup id="id3.3.id3" class="ltx_sup">1*</sup>,
Rongyu Zhang<sup id="id4.4.id4" class="ltx_sup">2</sup>,
<br class="ltx_break">Zhaoqing Wang<sup id="id5.5.id5" class="ltx_sup">3</sup>,
Yandong Guo<sup id="id6.6.id6" class="ltx_sup">5</sup>,
Shanghang Zhang<sup id="id7.7.id7" class="ltx_sup">1</sup>
<br class="ltx_break"><sup id="id8.8.id8" class="ltx_sup">1</sup>Peking University, <sup id="id9.9.id9" class="ltx_sup">2</sup>The Chinese University of Hong Kong, Shenzhen,
<sup id="id10.10.id10" class="ltx_sup">3</sup> The University of Sydney
<br class="ltx_break"><sup id="id11.11.id11" class="ltx_sup">4</sup>The Chinese University of Hong Kong,
<sup id="id12.12.id12" class="ltx_sup">5</sup>Beijing University of Posts and Telecommunications
<br class="ltx_break">
</span><span class="ltx_author_notes">Equal contributionCorresponding author: shzhang.pku@gmail.com</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.id1" class="ltx_p">Bird’s-Eye-View (BEV) 3D Object Detection is a crucial multi-view technique for autonomous driving systems. Recently, plenty of works are proposed, following a similar paradigm consisting of three essential components, i.e., camera feature extraction, BEV feature construction, and task heads. Among the three components, BEV feature construction is BEV-specific compared with 2D tasks. Existing methods aggregate the multi-view camera features to the flattened grid in order to construct the BEV feature. However, flattening the BEV space along the height dimension fails to emphasize the informative features of different heights. For example, the barrier is located at a low height while the truck is located at a high height. In this paper, we propose a novel method named BEV Slice Attention Network (BEV-SAN) for exploiting the intrinsic characteristics of different heights. Instead of flattening the BEV space, we first sample along the height dimension to build the global and local BEV slices. Then, the features of BEV slices are aggregated from the camera features and merged by the attention mechanism. Finally, we fuse the merged local and global BEV features by a transformer to generate the final feature map for task heads. The purpose of local BEV slices is to emphasize informative heights. In order to find them, we further propose a LiDAR-guided sampling strategy to leverage the statistical distribution of LiDAR to determine the heights of local slices. Compared with uniform sampling, LiDAR-guided sampling can determine more informative heights. We conduct detailed experiments to demonstrate the effectiveness of BEV-SAN. Code will be released.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Object detection is an essential computer vision task, which has wide applications in security, robotics, autonomous driving, etc. With the development of Deep Neural Networks (DNNs), a huge amount of methods are proposed for 2D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and 3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> object detection. As there are too many methods, we focus our introduction on the cutting-edge multi-view camera-based 3D object detection, which has gained increasing attention from the community. The Bird’s-Eye-View (BEV) is a unified representation of the surrounding scene and is suitable for autonomous driving tasks. Therefore, plenty of 3D object detection methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> are proposed for multi-view BEV perception recently.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2212.01231/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_square" width="178" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">The statistics of 3D bounding boxes along the height dimension.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Although the model architectures of those methods are different, they commonly follow a similar paradigm consisting of three essential components including camera feature extraction, BEV feature extraction, and task heads. Among the three components, BEV feature construction is BEV-specific compared with 2D tasks. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> presents a new framework that learns a unified BEV representation with spatio-temporal transformers. They first lift each query on the flattened BEV grid to a pillar-like query and then project the sampled 3D points to 2D views. The extracted features of hit views are weighted and summed as the output of spatial cross-attention. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> first predicts the depth for RGB input and projects the image features to frustum space. Then they sum up the frustum features that fall into the same flatted BEV grid. Both methods have pros and cons, while they all flatten the BEV space along the height dimension.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Motivated by the fact that different object classes locate at different heights. For instance, barrier is located at a low height while the truck is located at a high height. Flattening the BEV space along the height dimension fails to exploit the benefit of different heights. In this paper, we propose a novel method named BEV Slice Attention Network (BEV-SAN) to explore the intrinsic properties of different heights. We first sample along the height dimension to build the global and local BEV slices, which are represented as the upper and lower bounds of BEV slice height. The global slices are similar to former works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, which aim at covering the large height range of BEV space, while the local BEV slices aim at emphasizing informative heights. We aggregate the features from multi-view cameras to construct the features of global and local BEV slices. To merge the global and local slices, we first use the height attention mechanism to fuse the global and local slices separately. Then we adopt a transformer to fuse the merged global and local features. The final fused feature map is used for task-specific heads. In this paper, we mainly conduct the evaluation of BEV-SAN on 3D object detection. It is to be noted that our method can also be used in other BEV perception tasks such as map segmentation and planning.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In order to improve the performance, we further propose a LiDAR-guided sampling strategy to leverage the statistical distribution of LiDAR to determine the optimal heights of local slices. We project the LiDAR points to the BEV space and calculate the histogram along the height dimension. According to the histogram, we can sample the upper and lower height bounds of local slices. Compared with uniform sampling or random sampling, our strategy can choose informative ranges for BEV perception. We want to point out that we only use LiDAR data to build the local BEV slices. Our contributions can be concluded as follows:</p>
</div>
<div id="S1.p5" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a novel method named BEV Slice Attention Network (BEV-SAN) that exploits the features of different heights in BEV space, achieving an accurate performance of BEV 3D object detection.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present a LiDAR-guided sampling strategy to determine the optimal heights of local slices, resulting in informative ranges for BEV perception.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We conduct detailed experiments to demonstrate the effectiveness of our method. Our method can also be applied to other BEV perception tasks like map segmentation and planning.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Relate work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Monocular 3D object detection</span> Monocular 3D object detection is a useful but challenging technique in autonomous driving since it needs to predict the 3D bounding boxes from a single 2D image. Deep3DBox <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> firstly regresses relatively stable 3D bounding box properties using DNNs and combines them with geometric constraints to generate the final results. M3D-RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> designs depth-aware convolutional layers and 3D region proposal network, significantly improving the performance of monocular 3D object detection. SMOKE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> predicts a 3D bounding box for each detected 2D object by combining a single keypoint estimate with regressed 3D variables. FCOS3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> proposes a one-stage framework that predicts the decoupled 2D and 3D attributes for 3D targets. MonoDLE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> quantifies the impact introduced by each sub-task of monocular 3D object detection and proposes three strategies to reduce the localization error. PGD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> constructs geometric relation graphs across predicted objects and uses the graph to improve the depth estimation for monocular 3D object detection. MonoPair <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> improves monocular 3D object detection by considering the relationship of paired samples. RTM3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> predicts the nine perspective key points in 3D space and recovers the dimension, location, and orientation from the nine key points. MonoFlex <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> proposes a flexible framework that explicitly decouples the truncated objects and adaptively combines multiple approaches for object depth estimation. GUP-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> proposes to tackle the error amplification problem introduced by the projection process. MonoDETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> introduces a novel framework using a depth-guided transformer and achieves state-of-the-art performance on benchmarks.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2212.01231/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="385" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">The pipeline of the proposed SAN method. Our method constructs the BEV feature based on the global and local slices. We use a two-stage fusion strategy to merge the features of global and local slices for task heads.</span></figcaption>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Multi-View BEV 3D object detection</span> As a unified representation of the surrounding scene, BEV 3D object detection is becoming prevailing in the multi-view camera systems. Recently, plenty of methods are proposed for multi-view BEV 3D object detection. DETR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> uses a sparse set of 3D object queries to index the extracted 2D features from multi-view camera images. They make the bounding box prediction per query using the set-to-set loss. BEVDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> first predicts the depth for each camera image and then projects the extracted image features to BEV space by the LSS operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Finally, the task-specific head is constructed upon the BEV feature. BEVDet4D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> fuses the feature from the previous frame with the current frame to lift the BEVDet paradigm from 3D space to spatial-temporal 4D space. BEVFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> exploits both the spatial and temporal information by interacting with spatial and temporal space through pre-defined grid-shaped BEV queries. PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> encodes the position information of 3D coordinates into image features and performs end-to-end object detection based on 3D position-aware features. BEVDepth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> reveals that the quality of intermediate depth is the key to improving multi-view 3D object detection. They get explicit depth supervision utilizing encoded intrinsic and extrinsic parameters. PolarDETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> uses the Polar Parametrization for 3D detection by reformulating position parametrization, velocity decomposition, perception range, label assignment, and loss function in the polar coordinate system. BEVStereo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> introduces an effective temporal stereo method to dynamically select the scale of matching candidates for multi-view stereo. They further design an iterative algorithm to update more valuable candidates, making it adaptive to moving candidates. STS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> proposes a surround-view temporal stereo technique to leverage the geometry correspondence between frames across time to improve the quality of depth.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.5" class="ltx_p">Our method follows the pipeline of existing methods such as BEVDepth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, which consist of three components: camera feature extraction, BEV feature construction, and task heads. To be more specific, Given an input multi-view image <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="{I_{k}}\in{R^{3\times H\times W}}" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><msub id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml"><mi id="S3.p1.1.m1.1.1.2.2" xref="S3.p1.1.m1.1.1.2.2.cmml">I</mi><mi id="S3.p1.1.m1.1.1.2.3" xref="S3.p1.1.m1.1.1.2.3.cmml">k</mi></msub><mo id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml"><mi id="S3.p1.1.m1.1.1.3.2" xref="S3.p1.1.m1.1.1.3.2.cmml">R</mi><mrow id="S3.p1.1.m1.1.1.3.3" xref="S3.p1.1.m1.1.1.3.3.cmml"><mn id="S3.p1.1.m1.1.1.3.3.2" xref="S3.p1.1.m1.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p1.1.m1.1.1.3.3.1" xref="S3.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.p1.1.m1.1.1.3.3.3" xref="S3.p1.1.m1.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p1.1.m1.1.1.3.3.1a" xref="S3.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.p1.1.m1.1.1.3.3.4" xref="S3.p1.1.m1.1.1.3.3.4.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><in id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></in><apply id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.2.1.cmml" xref="S3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.p1.1.m1.1.1.2.2.cmml" xref="S3.p1.1.m1.1.1.2.2">𝐼</ci><ci id="S3.p1.1.m1.1.1.2.3.cmml" xref="S3.p1.1.m1.1.1.2.3">𝑘</ci></apply><apply id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.1.cmml" xref="S3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.p1.1.m1.1.1.3.2.cmml" xref="S3.p1.1.m1.1.1.3.2">𝑅</ci><apply id="S3.p1.1.m1.1.1.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3"><times id="S3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.p1.1.m1.1.1.3.3.1"></times><cn type="integer" id="S3.p1.1.m1.1.1.3.3.2.cmml" xref="S3.p1.1.m1.1.1.3.3.2">3</cn><ci id="S3.p1.1.m1.1.1.3.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3.3">𝐻</ci><ci id="S3.p1.1.m1.1.1.3.3.4.cmml" xref="S3.p1.1.m1.1.1.3.3.4">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">{I_{k}}\in{R^{3\times H\times W}}</annotation></semantics></math>, we adopt a shared backbone model to extract the feature <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="{F_{k}}\in{R^{C\times{H_{f}}\times{W_{f}}}}" display="inline"><semantics id="S3.p1.2.m2.1a"><mrow id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><msub id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml"><mi id="S3.p1.2.m2.1.1.2.2" xref="S3.p1.2.m2.1.1.2.2.cmml">F</mi><mi id="S3.p1.2.m2.1.1.2.3" xref="S3.p1.2.m2.1.1.2.3.cmml">k</mi></msub><mo id="S3.p1.2.m2.1.1.1" xref="S3.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml"><mi id="S3.p1.2.m2.1.1.3.2" xref="S3.p1.2.m2.1.1.3.2.cmml">R</mi><mrow id="S3.p1.2.m2.1.1.3.3" xref="S3.p1.2.m2.1.1.3.3.cmml"><mi id="S3.p1.2.m2.1.1.3.3.2" xref="S3.p1.2.m2.1.1.3.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p1.2.m2.1.1.3.3.1" xref="S3.p1.2.m2.1.1.3.3.1.cmml">×</mo><msub id="S3.p1.2.m2.1.1.3.3.3" xref="S3.p1.2.m2.1.1.3.3.3.cmml"><mi id="S3.p1.2.m2.1.1.3.3.3.2" xref="S3.p1.2.m2.1.1.3.3.3.2.cmml">H</mi><mi id="S3.p1.2.m2.1.1.3.3.3.3" xref="S3.p1.2.m2.1.1.3.3.3.3.cmml">f</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.p1.2.m2.1.1.3.3.1a" xref="S3.p1.2.m2.1.1.3.3.1.cmml">×</mo><msub id="S3.p1.2.m2.1.1.3.3.4" xref="S3.p1.2.m2.1.1.3.3.4.cmml"><mi id="S3.p1.2.m2.1.1.3.3.4.2" xref="S3.p1.2.m2.1.1.3.3.4.2.cmml">W</mi><mi id="S3.p1.2.m2.1.1.3.3.4.3" xref="S3.p1.2.m2.1.1.3.3.4.3.cmml">f</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><in id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1"></in><apply id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.2.1.cmml" xref="S3.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.2.cmml" xref="S3.p1.2.m2.1.1.2.2">𝐹</ci><ci id="S3.p1.2.m2.1.1.2.3.cmml" xref="S3.p1.2.m2.1.1.2.3">𝑘</ci></apply><apply id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.3.1.cmml" xref="S3.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.p1.2.m2.1.1.3.2.cmml" xref="S3.p1.2.m2.1.1.3.2">𝑅</ci><apply id="S3.p1.2.m2.1.1.3.3.cmml" xref="S3.p1.2.m2.1.1.3.3"><times id="S3.p1.2.m2.1.1.3.3.1.cmml" xref="S3.p1.2.m2.1.1.3.3.1"></times><ci id="S3.p1.2.m2.1.1.3.3.2.cmml" xref="S3.p1.2.m2.1.1.3.3.2">𝐶</ci><apply id="S3.p1.2.m2.1.1.3.3.3.cmml" xref="S3.p1.2.m2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.3.3.3.1.cmml" xref="S3.p1.2.m2.1.1.3.3.3">subscript</csymbol><ci id="S3.p1.2.m2.1.1.3.3.3.2.cmml" xref="S3.p1.2.m2.1.1.3.3.3.2">𝐻</ci><ci id="S3.p1.2.m2.1.1.3.3.3.3.cmml" xref="S3.p1.2.m2.1.1.3.3.3.3">𝑓</ci></apply><apply id="S3.p1.2.m2.1.1.3.3.4.cmml" xref="S3.p1.2.m2.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.3.3.4.1.cmml" xref="S3.p1.2.m2.1.1.3.3.4">subscript</csymbol><ci id="S3.p1.2.m2.1.1.3.3.4.2.cmml" xref="S3.p1.2.m2.1.1.3.3.4.2">𝑊</ci><ci id="S3.p1.2.m2.1.1.3.3.4.3.cmml" xref="S3.p1.2.m2.1.1.3.3.4.3">𝑓</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">{F_{k}}\in{R^{C\times{H_{f}}\times{W_{f}}}}</annotation></semantics></math>, where k is the index of the camera. we also predict the depth distribution map for each input image <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="{D_{k}}\in{R^{D\times{H_{f}}\times{W_{f}}}}" display="inline"><semantics id="S3.p1.3.m3.1a"><mrow id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><msub id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml"><mi id="S3.p1.3.m3.1.1.2.2" xref="S3.p1.3.m3.1.1.2.2.cmml">D</mi><mi id="S3.p1.3.m3.1.1.2.3" xref="S3.p1.3.m3.1.1.2.3.cmml">k</mi></msub><mo id="S3.p1.3.m3.1.1.1" xref="S3.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml"><mi id="S3.p1.3.m3.1.1.3.2" xref="S3.p1.3.m3.1.1.3.2.cmml">R</mi><mrow id="S3.p1.3.m3.1.1.3.3" xref="S3.p1.3.m3.1.1.3.3.cmml"><mi id="S3.p1.3.m3.1.1.3.3.2" xref="S3.p1.3.m3.1.1.3.3.2.cmml">D</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p1.3.m3.1.1.3.3.1" xref="S3.p1.3.m3.1.1.3.3.1.cmml">×</mo><msub id="S3.p1.3.m3.1.1.3.3.3" xref="S3.p1.3.m3.1.1.3.3.3.cmml"><mi id="S3.p1.3.m3.1.1.3.3.3.2" xref="S3.p1.3.m3.1.1.3.3.3.2.cmml">H</mi><mi id="S3.p1.3.m3.1.1.3.3.3.3" xref="S3.p1.3.m3.1.1.3.3.3.3.cmml">f</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.p1.3.m3.1.1.3.3.1a" xref="S3.p1.3.m3.1.1.3.3.1.cmml">×</mo><msub id="S3.p1.3.m3.1.1.3.3.4" xref="S3.p1.3.m3.1.1.3.3.4.cmml"><mi id="S3.p1.3.m3.1.1.3.3.4.2" xref="S3.p1.3.m3.1.1.3.3.4.2.cmml">W</mi><mi id="S3.p1.3.m3.1.1.3.3.4.3" xref="S3.p1.3.m3.1.1.3.3.4.3.cmml">f</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><in id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1.1"></in><apply id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.2.1.cmml" xref="S3.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.p1.3.m3.1.1.2.2.cmml" xref="S3.p1.3.m3.1.1.2.2">𝐷</ci><ci id="S3.p1.3.m3.1.1.2.3.cmml" xref="S3.p1.3.m3.1.1.2.3">𝑘</ci></apply><apply id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.3.1.cmml" xref="S3.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.p1.3.m3.1.1.3.2.cmml" xref="S3.p1.3.m3.1.1.3.2">𝑅</ci><apply id="S3.p1.3.m3.1.1.3.3.cmml" xref="S3.p1.3.m3.1.1.3.3"><times id="S3.p1.3.m3.1.1.3.3.1.cmml" xref="S3.p1.3.m3.1.1.3.3.1"></times><ci id="S3.p1.3.m3.1.1.3.3.2.cmml" xref="S3.p1.3.m3.1.1.3.3.2">𝐷</ci><apply id="S3.p1.3.m3.1.1.3.3.3.cmml" xref="S3.p1.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.3.3.3.1.cmml" xref="S3.p1.3.m3.1.1.3.3.3">subscript</csymbol><ci id="S3.p1.3.m3.1.1.3.3.3.2.cmml" xref="S3.p1.3.m3.1.1.3.3.3.2">𝐻</ci><ci id="S3.p1.3.m3.1.1.3.3.3.3.cmml" xref="S3.p1.3.m3.1.1.3.3.3.3">𝑓</ci></apply><apply id="S3.p1.3.m3.1.1.3.3.4.cmml" xref="S3.p1.3.m3.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.3.3.4.1.cmml" xref="S3.p1.3.m3.1.1.3.3.4">subscript</csymbol><ci id="S3.p1.3.m3.1.1.3.3.4.2.cmml" xref="S3.p1.3.m3.1.1.3.3.4.2">𝑊</ci><ci id="S3.p1.3.m3.1.1.3.3.4.3.cmml" xref="S3.p1.3.m3.1.1.3.3.4.3">𝑓</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">{D_{k}}\in{R^{D\times{H_{f}}\times{W_{f}}}}</annotation></semantics></math>. Then we project the camera features to viewing frustum <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="{V_{k}}\in{R^{C\times D\times{H_{f}}\times{W_{f}}}}" display="inline"><semantics id="S3.p1.4.m4.1a"><mrow id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><msub id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml"><mi id="S3.p1.4.m4.1.1.2.2" xref="S3.p1.4.m4.1.1.2.2.cmml">V</mi><mi id="S3.p1.4.m4.1.1.2.3" xref="S3.p1.4.m4.1.1.2.3.cmml">k</mi></msub><mo id="S3.p1.4.m4.1.1.1" xref="S3.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml"><mi id="S3.p1.4.m4.1.1.3.2" xref="S3.p1.4.m4.1.1.3.2.cmml">R</mi><mrow id="S3.p1.4.m4.1.1.3.3" xref="S3.p1.4.m4.1.1.3.3.cmml"><mi id="S3.p1.4.m4.1.1.3.3.2" xref="S3.p1.4.m4.1.1.3.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p1.4.m4.1.1.3.3.1" xref="S3.p1.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S3.p1.4.m4.1.1.3.3.3" xref="S3.p1.4.m4.1.1.3.3.3.cmml">D</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p1.4.m4.1.1.3.3.1a" xref="S3.p1.4.m4.1.1.3.3.1.cmml">×</mo><msub id="S3.p1.4.m4.1.1.3.3.4" xref="S3.p1.4.m4.1.1.3.3.4.cmml"><mi id="S3.p1.4.m4.1.1.3.3.4.2" xref="S3.p1.4.m4.1.1.3.3.4.2.cmml">H</mi><mi id="S3.p1.4.m4.1.1.3.3.4.3" xref="S3.p1.4.m4.1.1.3.3.4.3.cmml">f</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.p1.4.m4.1.1.3.3.1b" xref="S3.p1.4.m4.1.1.3.3.1.cmml">×</mo><msub id="S3.p1.4.m4.1.1.3.3.5" xref="S3.p1.4.m4.1.1.3.3.5.cmml"><mi id="S3.p1.4.m4.1.1.3.3.5.2" xref="S3.p1.4.m4.1.1.3.3.5.2.cmml">W</mi><mi id="S3.p1.4.m4.1.1.3.3.5.3" xref="S3.p1.4.m4.1.1.3.3.5.3.cmml">f</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><in id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1"></in><apply id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.2.1.cmml" xref="S3.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.p1.4.m4.1.1.2.2.cmml" xref="S3.p1.4.m4.1.1.2.2">𝑉</ci><ci id="S3.p1.4.m4.1.1.2.3.cmml" xref="S3.p1.4.m4.1.1.2.3">𝑘</ci></apply><apply id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.3.1.cmml" xref="S3.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.p1.4.m4.1.1.3.2.cmml" xref="S3.p1.4.m4.1.1.3.2">𝑅</ci><apply id="S3.p1.4.m4.1.1.3.3.cmml" xref="S3.p1.4.m4.1.1.3.3"><times id="S3.p1.4.m4.1.1.3.3.1.cmml" xref="S3.p1.4.m4.1.1.3.3.1"></times><ci id="S3.p1.4.m4.1.1.3.3.2.cmml" xref="S3.p1.4.m4.1.1.3.3.2">𝐶</ci><ci id="S3.p1.4.m4.1.1.3.3.3.cmml" xref="S3.p1.4.m4.1.1.3.3.3">𝐷</ci><apply id="S3.p1.4.m4.1.1.3.3.4.cmml" xref="S3.p1.4.m4.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.3.3.4.1.cmml" xref="S3.p1.4.m4.1.1.3.3.4">subscript</csymbol><ci id="S3.p1.4.m4.1.1.3.3.4.2.cmml" xref="S3.p1.4.m4.1.1.3.3.4.2">𝐻</ci><ci id="S3.p1.4.m4.1.1.3.3.4.3.cmml" xref="S3.p1.4.m4.1.1.3.3.4.3">𝑓</ci></apply><apply id="S3.p1.4.m4.1.1.3.3.5.cmml" xref="S3.p1.4.m4.1.1.3.3.5"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.3.3.5.1.cmml" xref="S3.p1.4.m4.1.1.3.3.5">subscript</csymbol><ci id="S3.p1.4.m4.1.1.3.3.5.2.cmml" xref="S3.p1.4.m4.1.1.3.3.5.2">𝑊</ci><ci id="S3.p1.4.m4.1.1.3.3.5.3.cmml" xref="S3.p1.4.m4.1.1.3.3.5.3">𝑓</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">{V_{k}}\in{R^{C\times D\times{H_{f}}\times{W_{f}}}}</annotation></semantics></math> and construct the flattened BEV feature <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="B\in{R^{C\times{H_{e}}\times{W_{e}}}}" display="inline"><semantics id="S3.p1.5.m5.1a"><mrow id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml"><mi id="S3.p1.5.m5.1.1.2" xref="S3.p1.5.m5.1.1.2.cmml">B</mi><mo id="S3.p1.5.m5.1.1.1" xref="S3.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S3.p1.5.m5.1.1.3" xref="S3.p1.5.m5.1.1.3.cmml"><mi id="S3.p1.5.m5.1.1.3.2" xref="S3.p1.5.m5.1.1.3.2.cmml">R</mi><mrow id="S3.p1.5.m5.1.1.3.3" xref="S3.p1.5.m5.1.1.3.3.cmml"><mi id="S3.p1.5.m5.1.1.3.3.2" xref="S3.p1.5.m5.1.1.3.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p1.5.m5.1.1.3.3.1" xref="S3.p1.5.m5.1.1.3.3.1.cmml">×</mo><msub id="S3.p1.5.m5.1.1.3.3.3" xref="S3.p1.5.m5.1.1.3.3.3.cmml"><mi id="S3.p1.5.m5.1.1.3.3.3.2" xref="S3.p1.5.m5.1.1.3.3.3.2.cmml">H</mi><mi id="S3.p1.5.m5.1.1.3.3.3.3" xref="S3.p1.5.m5.1.1.3.3.3.3.cmml">e</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.p1.5.m5.1.1.3.3.1a" xref="S3.p1.5.m5.1.1.3.3.1.cmml">×</mo><msub id="S3.p1.5.m5.1.1.3.3.4" xref="S3.p1.5.m5.1.1.3.3.4.cmml"><mi id="S3.p1.5.m5.1.1.3.3.4.2" xref="S3.p1.5.m5.1.1.3.3.4.2.cmml">W</mi><mi id="S3.p1.5.m5.1.1.3.3.4.3" xref="S3.p1.5.m5.1.1.3.3.4.3.cmml">e</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><apply id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1"><in id="S3.p1.5.m5.1.1.1.cmml" xref="S3.p1.5.m5.1.1.1"></in><ci id="S3.p1.5.m5.1.1.2.cmml" xref="S3.p1.5.m5.1.1.2">𝐵</ci><apply id="S3.p1.5.m5.1.1.3.cmml" xref="S3.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.3.1.cmml" xref="S3.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.p1.5.m5.1.1.3.2.cmml" xref="S3.p1.5.m5.1.1.3.2">𝑅</ci><apply id="S3.p1.5.m5.1.1.3.3.cmml" xref="S3.p1.5.m5.1.1.3.3"><times id="S3.p1.5.m5.1.1.3.3.1.cmml" xref="S3.p1.5.m5.1.1.3.3.1"></times><ci id="S3.p1.5.m5.1.1.3.3.2.cmml" xref="S3.p1.5.m5.1.1.3.3.2">𝐶</ci><apply id="S3.p1.5.m5.1.1.3.3.3.cmml" xref="S3.p1.5.m5.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.3.3.3.1.cmml" xref="S3.p1.5.m5.1.1.3.3.3">subscript</csymbol><ci id="S3.p1.5.m5.1.1.3.3.3.2.cmml" xref="S3.p1.5.m5.1.1.3.3.3.2">𝐻</ci><ci id="S3.p1.5.m5.1.1.3.3.3.3.cmml" xref="S3.p1.5.m5.1.1.3.3.3.3">𝑒</ci></apply><apply id="S3.p1.5.m5.1.1.3.3.4.cmml" xref="S3.p1.5.m5.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.3.3.4.1.cmml" xref="S3.p1.5.m5.1.1.3.3.4">subscript</csymbol><ci id="S3.p1.5.m5.1.1.3.3.4.2.cmml" xref="S3.p1.5.m5.1.1.3.3.4.2">𝑊</ci><ci id="S3.p1.5.m5.1.1.3.3.4.3.cmml" xref="S3.p1.5.m5.1.1.3.3.4.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">B\in{R^{C\times{H_{e}}\times{W_{e}}}}</annotation></semantics></math> with the proposed <span id="S3.p1.5.1" class="ltx_text ltx_font_bold">Slice Attention Module</span>. Finally, the task-specific heads are applied to the BEV feature. We will first introduce the motivation in Sec. <a href="#S3.SS1" title="3.1 Motivation ‣ 3 Methods ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> and then present the proposed Slice Attention Module in Sec. <a href="#S3.SS2" title="3.2 Slice Attention Module ‣ 3 Methods ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. The whole framework of our method is illustrated in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Relate work ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Motivation</h3>

<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.7.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.8.2" class="ltx_text" style="font-size:90%;">The mAP results of Traffic Cone, Person and Bus with BEV slices of different height ranges.</span></figcaption>
<table id="S3.T1.5.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.5.5.6.1" class="ltx_tr">
<th id="S3.T1.5.5.6.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">Height</th>
<th id="S3.T1.5.5.6.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">Traffic Cone</th>
<th id="S3.T1.5.5.6.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">Person</th>
<th id="S3.T1.5.5.6.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">Bus</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;"><math id="S3.T1.1.1.1.1.m1.2" class="ltx_Math" alttext="[-2,0]" display="inline"><semantics id="S3.T1.1.1.1.1.m1.2a"><mrow id="S3.T1.1.1.1.1.m1.2.2.1" xref="S3.T1.1.1.1.1.m1.2.2.2.cmml"><mo stretchy="false" id="S3.T1.1.1.1.1.m1.2.2.1.2" xref="S3.T1.1.1.1.1.m1.2.2.2.cmml">[</mo><mrow id="S3.T1.1.1.1.1.m1.2.2.1.1" xref="S3.T1.1.1.1.1.m1.2.2.1.1.cmml"><mo id="S3.T1.1.1.1.1.m1.2.2.1.1a" xref="S3.T1.1.1.1.1.m1.2.2.1.1.cmml">−</mo><mn id="S3.T1.1.1.1.1.m1.2.2.1.1.2" xref="S3.T1.1.1.1.1.m1.2.2.1.1.2.cmml">2</mn></mrow><mo id="S3.T1.1.1.1.1.m1.2.2.1.3" xref="S3.T1.1.1.1.1.m1.2.2.2.cmml">,</mo><mn id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml">0</mn><mo stretchy="false" id="S3.T1.1.1.1.1.m1.2.2.1.4" xref="S3.T1.1.1.1.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.2b"><interval closure="closed" id="S3.T1.1.1.1.1.m1.2.2.2.cmml" xref="S3.T1.1.1.1.1.m1.2.2.1"><apply id="S3.T1.1.1.1.1.m1.2.2.1.1.cmml" xref="S3.T1.1.1.1.1.m1.2.2.1.1"><minus id="S3.T1.1.1.1.1.m1.2.2.1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.2.2.1.1"></minus><cn type="integer" id="S3.T1.1.1.1.1.m1.2.2.1.1.2.cmml" xref="S3.T1.1.1.1.1.m1.2.2.1.1.2">2</cn></apply><cn type="integer" id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">0</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.2c">[-2,0]</annotation></semantics></math></th>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">0.087</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">0.0</td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">0.001</td>
</tr>
<tr id="S3.T1.2.2.2" class="ltx_tr">
<th id="S3.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;"><math id="S3.T1.2.2.2.1.m1.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S3.T1.2.2.2.1.m1.2a"><mrow id="S3.T1.2.2.2.1.m1.2.3.2" xref="S3.T1.2.2.2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.2.2.2.1.m1.2.3.2.1" xref="S3.T1.2.2.2.1.m1.2.3.1.cmml">[</mo><mn id="S3.T1.2.2.2.1.m1.1.1" xref="S3.T1.2.2.2.1.m1.1.1.cmml">0</mn><mo id="S3.T1.2.2.2.1.m1.2.3.2.2" xref="S3.T1.2.2.2.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.2.2.2.1.m1.2.2" xref="S3.T1.2.2.2.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S3.T1.2.2.2.1.m1.2.3.2.3" xref="S3.T1.2.2.2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.1.m1.2b"><interval closure="closed" id="S3.T1.2.2.2.1.m1.2.3.1.cmml" xref="S3.T1.2.2.2.1.m1.2.3.2"><cn type="integer" id="S3.T1.2.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.2.1.m1.1.1">0</cn><cn type="integer" id="S3.T1.2.2.2.1.m1.2.2.cmml" xref="S3.T1.2.2.2.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.1.m1.2c">[0,1]</annotation></semantics></math></th>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S3.T1.2.2.2.2.1" class="ltx_text ltx_font_bold">0.436</span></td>
<td id="S3.T1.2.2.2.3" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.217</td>
<td id="S3.T1.2.2.2.4" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.273</td>
</tr>
<tr id="S3.T1.3.3.3" class="ltx_tr">
<th id="S3.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;"><math id="S3.T1.3.3.3.1.m1.2" class="ltx_Math" alttext="[1,2]" display="inline"><semantics id="S3.T1.3.3.3.1.m1.2a"><mrow id="S3.T1.3.3.3.1.m1.2.3.2" xref="S3.T1.3.3.3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.3.3.3.1.m1.2.3.2.1" xref="S3.T1.3.3.3.1.m1.2.3.1.cmml">[</mo><mn id="S3.T1.3.3.3.1.m1.1.1" xref="S3.T1.3.3.3.1.m1.1.1.cmml">1</mn><mo id="S3.T1.3.3.3.1.m1.2.3.2.2" xref="S3.T1.3.3.3.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.3.3.3.1.m1.2.2" xref="S3.T1.3.3.3.1.m1.2.2.cmml">2</mn><mo stretchy="false" id="S3.T1.3.3.3.1.m1.2.3.2.3" xref="S3.T1.3.3.3.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.1.m1.2b"><interval closure="closed" id="S3.T1.3.3.3.1.m1.2.3.1.cmml" xref="S3.T1.3.3.3.1.m1.2.3.2"><cn type="integer" id="S3.T1.3.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.3.1.m1.1.1">1</cn><cn type="integer" id="S3.T1.3.3.3.1.m1.2.2.cmml" xref="S3.T1.3.3.3.1.m1.2.2">2</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.1.m1.2c">[1,2]</annotation></semantics></math></th>
<td id="S3.T1.3.3.3.2" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.367</td>
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.245</td>
<td id="S3.T1.3.3.3.4" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0. 307</td>
</tr>
<tr id="S3.T1.4.4.4" class="ltx_tr">
<th id="S3.T1.4.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;"><math id="S3.T1.4.4.4.1.m1.2" class="ltx_Math" alttext="[2,3]" display="inline"><semantics id="S3.T1.4.4.4.1.m1.2a"><mrow id="S3.T1.4.4.4.1.m1.2.3.2" xref="S3.T1.4.4.4.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.4.4.4.1.m1.2.3.2.1" xref="S3.T1.4.4.4.1.m1.2.3.1.cmml">[</mo><mn id="S3.T1.4.4.4.1.m1.1.1" xref="S3.T1.4.4.4.1.m1.1.1.cmml">2</mn><mo id="S3.T1.4.4.4.1.m1.2.3.2.2" xref="S3.T1.4.4.4.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.4.4.4.1.m1.2.2" xref="S3.T1.4.4.4.1.m1.2.2.cmml">3</mn><mo stretchy="false" id="S3.T1.4.4.4.1.m1.2.3.2.3" xref="S3.T1.4.4.4.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.m1.2b"><interval closure="closed" id="S3.T1.4.4.4.1.m1.2.3.1.cmml" xref="S3.T1.4.4.4.1.m1.2.3.2"><cn type="integer" id="S3.T1.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1">2</cn><cn type="integer" id="S3.T1.4.4.4.1.m1.2.2.cmml" xref="S3.T1.4.4.4.1.m1.2.2">3</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.m1.2c">[2,3]</annotation></semantics></math></th>
<td id="S3.T1.4.4.4.2" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.446</td>
<td id="S3.T1.4.4.4.3" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S3.T1.4.4.4.3.1" class="ltx_text ltx_font_bold">0.265</span></td>
<td id="S3.T1.4.4.4.4" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.340</td>
</tr>
<tr id="S3.T1.5.5.5" class="ltx_tr">
<th id="S3.T1.5.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;"><math id="S3.T1.5.5.5.1.m1.2" class="ltx_Math" alttext="[3,4]" display="inline"><semantics id="S3.T1.5.5.5.1.m1.2a"><mrow id="S3.T1.5.5.5.1.m1.2.3.2" xref="S3.T1.5.5.5.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.5.5.5.1.m1.2.3.2.1" xref="S3.T1.5.5.5.1.m1.2.3.1.cmml">[</mo><mn id="S3.T1.5.5.5.1.m1.1.1" xref="S3.T1.5.5.5.1.m1.1.1.cmml">3</mn><mo id="S3.T1.5.5.5.1.m1.2.3.2.2" xref="S3.T1.5.5.5.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.5.5.5.1.m1.2.2" xref="S3.T1.5.5.5.1.m1.2.2.cmml">4</mn><mo stretchy="false" id="S3.T1.5.5.5.1.m1.2.3.2.3" xref="S3.T1.5.5.5.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.1.m1.2b"><interval closure="closed" id="S3.T1.5.5.5.1.m1.2.3.1.cmml" xref="S3.T1.5.5.5.1.m1.2.3.2"><cn type="integer" id="S3.T1.5.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.5.1.m1.1.1">3</cn><cn type="integer" id="S3.T1.5.5.5.1.m1.2.2.cmml" xref="S3.T1.5.5.5.1.m1.2.2">4</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.1.m1.2c">[3,4]</annotation></semantics></math></th>
<td id="S3.T1.5.5.5.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:11.4pt;padding-right:11.4pt;">0.368</td>
<td id="S3.T1.5.5.5.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:11.4pt;padding-right:11.4pt;">0.257</td>
<td id="S3.T1.5.5.5.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S3.T1.5.5.5.4.1" class="ltx_text ltx_font_bold">0.348</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In the practical applications of autonomous driving, the detection targets vary in shape and size, causing severe bias in visual-based learning. For example, barrier is located at a low height while the truck is located at a high height. However, existing methods like BEVDepth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> sum up the frustum features that fall into the same flattened BEV grid. Therefore, they fail to exploit the benefit of different heights for BEV perception. In this section, we intend to demonstrate the motivation for slicing the BEV space based on different heights. We first visualize the heights of annotated 3D bounding boxes according to their object classes. As shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, different object classes actually have different height distributions. This is consistent with our motivation.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To further study this motivation, we adjust the height range of BEVDepth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and evaluate the 3D object detection performance of different classes as shown in Tab. <a href="#S3.T1" title="Table 1 ‣ 3.1 Motivation ‣ 3 Methods ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As can be seen, the traffic cone, which is lower compared with person and bus, shows obviously different performances at different height ranges (0.466 in [-2,0] and 0.368 in [2,4] separately). This indicates that the height range will greatly affect the detection performance of different object classes. This observation inspires us to take better advantage of different heights to improve detection performance. We will introduce the proposed Slice Attention Module in the next section.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2212.01231/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_img_landscape" width="179" height="141" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">The statistics of LiDAR points along the height dimension. We use this LiDAR histogram to guide the sampling of local slices, which emphasize the informative heights.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Slice Attention Module</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this section, we introduce the proposed Slice Attention Module. We define the slice using the height range in BEV space. We will first explain how to sample the BEV space to generate the global and local slices. The global slices are sampled to cover the large height ranges of BEV space. The local slices are sampled to emphasize the informative heights. Then we present our method to fuse the sampled global and local slices with an attention mechanism. Finally, we fuse the global feature and local feature for the task heads.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Global and Local Slices</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.4" class="ltx_p">For the multi-view images, we can extract the features by a shared backbone model <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="{F_{k}}\in{R^{C\times{H_{f}}\times{W_{f}}}}" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><msub id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.2.cmml">F</mi><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.cmml">k</mi></msub><mo id="S3.SS2.SSS1.p1.1.m1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">R</mi><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.2.cmml">H</mi><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.3.cmml">f</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1a" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.2.cmml">W</mi><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.3.cmml">f</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><in id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1"></in><apply id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.2">𝐹</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3">𝑘</ci></apply><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2">𝑅</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3"><times id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2">𝐶</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.2">𝐻</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.3">𝑓</ci></apply><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.2">𝑊</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.3">𝑓</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">{F_{k}}\in{R^{C\times{H_{f}}\times{W_{f}}}}</annotation></semantics></math>, where <math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mi id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><ci id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">k</annotation></semantics></math> is the index of the camera. We can aggregate the image features to construct the BEV feature <math id="S3.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="{B_{s}}\in{R^{C\times{H_{e}}\times{W_{e}}}}" display="inline"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mrow id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml"><msub id="S3.SS2.SSS1.p1.3.m3.1.1.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.2.cmml">B</mi><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.3.cmml">s</mi></msub><mo id="S3.SS2.SSS1.p1.3.m3.1.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.cmml">R</mi><mrow id="S3.SS2.SSS1.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.cmml">H</mi><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.cmml">e</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.1a" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4.2.cmml">W</mi><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4.3.cmml">e</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><in id="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1"></in><apply id="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.2">𝐵</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.3">𝑠</ci></apply><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2">𝑅</ci><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3"><times id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.1"></times><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.2">𝐶</ci><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2">𝐻</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3">𝑒</ci></apply><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4">subscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4.2">𝑊</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.4.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">{B_{s}}\in{R^{C\times{H_{e}}\times{W_{e}}}}</annotation></semantics></math> given the height range <math id="S3.SS2.SSS1.p1.4.m4.2" class="ltx_Math" alttext="s=[l,u]" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m4.2a"><mrow id="S3.SS2.SSS1.p1.4.m4.2.3" xref="S3.SS2.SSS1.p1.4.m4.2.3.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.2.3.2" xref="S3.SS2.SSS1.p1.4.m4.2.3.2.cmml">s</mi><mo id="S3.SS2.SSS1.p1.4.m4.2.3.1" xref="S3.SS2.SSS1.p1.4.m4.2.3.1.cmml">=</mo><mrow id="S3.SS2.SSS1.p1.4.m4.2.3.3.2" xref="S3.SS2.SSS1.p1.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p1.4.m4.2.3.3.2.1" xref="S3.SS2.SSS1.p1.4.m4.2.3.3.1.cmml">[</mo><mi id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml">l</mi><mo id="S3.SS2.SSS1.p1.4.m4.2.3.3.2.2" xref="S3.SS2.SSS1.p1.4.m4.2.3.3.1.cmml">,</mo><mi id="S3.SS2.SSS1.p1.4.m4.2.2" xref="S3.SS2.SSS1.p1.4.m4.2.2.cmml">u</mi><mo stretchy="false" id="S3.SS2.SSS1.p1.4.m4.2.3.3.2.3" xref="S3.SS2.SSS1.p1.4.m4.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.2b"><apply id="S3.SS2.SSS1.p1.4.m4.2.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.2.3"><eq id="S3.SS2.SSS1.p1.4.m4.2.3.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.2.3.1"></eq><ci id="S3.SS2.SSS1.p1.4.m4.2.3.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.2.3.2">𝑠</ci><interval closure="closed" id="S3.SS2.SSS1.p1.4.m4.2.3.3.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.2.3.3.2"><ci id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">𝑙</ci><ci id="S3.SS2.SSS1.p1.4.m4.2.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.2.2">𝑢</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.2c">s=[l,u]</annotation></semantics></math> in BEV space. We define a height range as a BEV slice.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.5" class="ltx_p"><span id="S3.SS2.SSS1.p2.5.1" class="ltx_text ltx_font_bold">Global Slices</span> We empirically determine the global slices as <math id="S3.SS2.SSS1.p2.1.m1.7" class="ltx_Math" alttext="\{{s_{g}}\}=\left[{\left[{-6,4}\right],\left[{-5,3}\right],\left[{-4,2}\right]}\right]" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.7a"><mrow id="S3.SS2.SSS1.p2.1.m1.7.7" xref="S3.SS2.SSS1.p2.1.m1.7.7.cmml"><mrow id="S3.SS2.SSS1.p2.1.m1.4.4.1.1" xref="S3.SS2.SSS1.p2.1.m1.4.4.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p2.1.m1.4.4.1.1.2" xref="S3.SS2.SSS1.p2.1.m1.4.4.1.2.cmml">{</mo><msub id="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1" xref="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1.cmml"><mi id="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1.2" xref="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1.2.cmml">s</mi><mi id="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1.3" xref="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1.3.cmml">g</mi></msub><mo stretchy="false" id="S3.SS2.SSS1.p2.1.m1.4.4.1.1.3" xref="S3.SS2.SSS1.p2.1.m1.4.4.1.2.cmml">}</mo></mrow><mo id="S3.SS2.SSS1.p2.1.m1.7.7.5" xref="S3.SS2.SSS1.p2.1.m1.7.7.5.cmml">=</mo><mrow id="S3.SS2.SSS1.p2.1.m1.7.7.4.3" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.4.cmml"><mo id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.4" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.4.cmml">[</mo><mrow id="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1" xref="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.2.cmml"><mo id="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.2" xref="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.2.cmml">[</mo><mrow id="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1" xref="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1.cmml"><mo id="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1a" xref="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1.2" xref="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1.2.cmml">6</mn></mrow><mo id="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.3" xref="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.2.cmml">,</mo><mn id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">4</mn><mo id="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.4" xref="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.2.cmml">]</mo></mrow><mo id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.5" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.4.cmml">,</mo><mrow id="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1" xref="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.2.cmml"><mo id="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.2" xref="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.2.cmml">[</mo><mrow id="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1" xref="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1.cmml"><mo id="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1a" xref="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1.2" xref="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1.2.cmml">5</mn></mrow><mo id="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.3" xref="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.2.cmml">,</mo><mn id="S3.SS2.SSS1.p2.1.m1.2.2" xref="S3.SS2.SSS1.p2.1.m1.2.2.cmml">3</mn><mo id="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.4" xref="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.2.cmml">]</mo></mrow><mo id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.6" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.4.cmml">,</mo><mrow id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.2.cmml"><mo id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.2" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.2.cmml">[</mo><mrow id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1.cmml"><mo id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1a" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1.2" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1.2.cmml">4</mn></mrow><mo id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.3" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.2.cmml">,</mo><mn id="S3.SS2.SSS1.p2.1.m1.3.3" xref="S3.SS2.SSS1.p2.1.m1.3.3.cmml">2</mn><mo id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.4" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.2.cmml">]</mo></mrow><mo id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.7" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.7b"><apply id="S3.SS2.SSS1.p2.1.m1.7.7.cmml" xref="S3.SS2.SSS1.p2.1.m1.7.7"><eq id="S3.SS2.SSS1.p2.1.m1.7.7.5.cmml" xref="S3.SS2.SSS1.p2.1.m1.7.7.5"></eq><set id="S3.SS2.SSS1.p2.1.m1.4.4.1.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.4.4.1.1"><apply id="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1.2">𝑠</ci><ci id="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1.3.cmml" xref="S3.SS2.SSS1.p2.1.m1.4.4.1.1.1.3">𝑔</ci></apply></set><list id="S3.SS2.SSS1.p2.1.m1.7.7.4.4.cmml" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3"><interval closure="closed" id="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1"><apply id="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1"><minus id="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.5.5.2.1.1.1.1.2">6</cn></apply><cn type="integer" id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">4</cn></interval><interval closure="closed" id="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1"><apply id="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1"><minus id="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.6.6.3.2.2.1.1.2">5</cn></apply><cn type="integer" id="S3.SS2.SSS1.p2.1.m1.2.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.2.2">3</cn></interval><interval closure="closed" id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1"><apply id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1"><minus id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.7.7.4.3.3.1.1.2">4</cn></apply><cn type="integer" id="S3.SS2.SSS1.p2.1.m1.3.3.cmml" xref="S3.SS2.SSS1.p2.1.m1.3.3">2</cn></interval></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.7c">\{{s_{g}}\}=\left[{\left[{-6,4}\right],\left[{-5,3}\right],\left[{-4,2}\right]}\right]</annotation></semantics></math>. Although the largest range <math id="S3.SS2.SSS1.p2.2.m2.2" class="ltx_Math" alttext="[-6,4]" display="inline"><semantics id="S3.SS2.SSS1.p2.2.m2.2a"><mrow id="S3.SS2.SSS1.p2.2.m2.2.2.1" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p2.2.m2.2.2.1.2" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.cmml">[</mo><mrow id="S3.SS2.SSS1.p2.2.m2.2.2.1.1" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1.cmml"><mo id="S3.SS2.SSS1.p2.2.m2.2.2.1.1a" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p2.2.m2.2.2.1.1.2" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1.2.cmml">6</mn></mrow><mo id="S3.SS2.SSS1.p2.2.m2.2.2.1.3" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.cmml">,</mo><mn id="S3.SS2.SSS1.p2.2.m2.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.cmml">4</mn><mo stretchy="false" id="S3.SS2.SSS1.p2.2.m2.2.2.1.4" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.2b"><interval closure="closed" id="S3.SS2.SSS1.p2.2.m2.2.2.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.1"><apply id="S3.SS2.SSS1.p2.2.m2.2.2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1"><minus id="S3.SS2.SSS1.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p2.2.m2.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1.2">6</cn></apply><cn type="integer" id="S3.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1">4</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.2c">[-6,4]</annotation></semantics></math> contains the overall information of the whole space, the corresponding BEV feature representation is significantly different from <math id="S3.SS2.SSS1.p2.3.m3.2" class="ltx_Math" alttext="[-5,3]" display="inline"><semantics id="S3.SS2.SSS1.p2.3.m3.2a"><mrow id="S3.SS2.SSS1.p2.3.m3.2.2.1" xref="S3.SS2.SSS1.p2.3.m3.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p2.3.m3.2.2.1.2" xref="S3.SS2.SSS1.p2.3.m3.2.2.2.cmml">[</mo><mrow id="S3.SS2.SSS1.p2.3.m3.2.2.1.1" xref="S3.SS2.SSS1.p2.3.m3.2.2.1.1.cmml"><mo id="S3.SS2.SSS1.p2.3.m3.2.2.1.1a" xref="S3.SS2.SSS1.p2.3.m3.2.2.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p2.3.m3.2.2.1.1.2" xref="S3.SS2.SSS1.p2.3.m3.2.2.1.1.2.cmml">5</mn></mrow><mo id="S3.SS2.SSS1.p2.3.m3.2.2.1.3" xref="S3.SS2.SSS1.p2.3.m3.2.2.2.cmml">,</mo><mn id="S3.SS2.SSS1.p2.3.m3.1.1" xref="S3.SS2.SSS1.p2.3.m3.1.1.cmml">3</mn><mo stretchy="false" id="S3.SS2.SSS1.p2.3.m3.2.2.1.4" xref="S3.SS2.SSS1.p2.3.m3.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.3.m3.2b"><interval closure="closed" id="S3.SS2.SSS1.p2.3.m3.2.2.2.cmml" xref="S3.SS2.SSS1.p2.3.m3.2.2.1"><apply id="S3.SS2.SSS1.p2.3.m3.2.2.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.2.2.1.1"><minus id="S3.SS2.SSS1.p2.3.m3.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.2.2.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p2.3.m3.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p2.3.m3.2.2.1.1.2">5</cn></apply><cn type="integer" id="S3.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1">3</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.3.m3.2c">[-5,3]</annotation></semantics></math> or <math id="S3.SS2.SSS1.p2.4.m4.2" class="ltx_Math" alttext="[-4,2]" display="inline"><semantics id="S3.SS2.SSS1.p2.4.m4.2a"><mrow id="S3.SS2.SSS1.p2.4.m4.2.2.1" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p2.4.m4.2.2.1.2" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.cmml">[</mo><mrow id="S3.SS2.SSS1.p2.4.m4.2.2.1.1" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1.cmml"><mo id="S3.SS2.SSS1.p2.4.m4.2.2.1.1a" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p2.4.m4.2.2.1.1.2" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1.2.cmml">4</mn></mrow><mo id="S3.SS2.SSS1.p2.4.m4.2.2.1.3" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.cmml">,</mo><mn id="S3.SS2.SSS1.p2.4.m4.1.1" xref="S3.SS2.SSS1.p2.4.m4.1.1.cmml">2</mn><mo stretchy="false" id="S3.SS2.SSS1.p2.4.m4.2.2.1.4" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.4.m4.2b"><interval closure="closed" id="S3.SS2.SSS1.p2.4.m4.2.2.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.1"><apply id="S3.SS2.SSS1.p2.4.m4.2.2.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1"><minus id="S3.SS2.SSS1.p2.4.m4.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p2.4.m4.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1.2">4</cn></apply><cn type="integer" id="S3.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1">2</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.4.m4.2c">[-4,2]</annotation></semantics></math>. Since the height information is viewed as channel dimension, we adopt a channel-wise attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to adaptively aggregate the multiple global-level slices. The attention mechanism between three global slices provides a learnable way to fully explore different semantic knowledge and thus improve the global contextual representation in BEV latent space. The attention between three global slices will be necessary to help improve the performance at the global level. We denote the constructed features of global slices as <math id="S3.SS2.SSS1.p2.5.m5.1" class="ltx_Math" alttext="\{B_{{}_{{s_{g}}}}^{i}\}" display="inline"><semantics id="S3.SS2.SSS1.p2.5.m5.1a"><mrow id="S3.SS2.SSS1.p2.5.m5.1.1.1" xref="S3.SS2.SSS1.p2.5.m5.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p2.5.m5.1.1.1.2" xref="S3.SS2.SSS1.p2.5.m5.1.1.2.cmml">{</mo><msubsup id="S3.SS2.SSS1.p2.5.m5.1.1.1.1" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.2" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.2.cmml">B</mi><msub id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.cmml"><mi id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3a" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.cmml"></mi><msub id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1.cmml"><mi id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1.2" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1.2.cmml">s</mi><mi id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1.3" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1.3.cmml">g</mi></msub></msub><mi id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.3" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.SS2.SSS1.p2.5.m5.1.1.1.3" xref="S3.SS2.SSS1.p2.5.m5.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.5.m5.1b"><set id="S3.SS2.SSS1.p2.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1"><apply id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.2">𝐵</ci><apply id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3"><apply id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1.1.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1.2.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1.2">𝑠</ci><ci id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1.3.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.2.3.1.3">𝑔</ci></apply></apply></apply><ci id="S3.SS2.SSS1.p2.5.m5.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.1.1.3">𝑖</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.5.m5.1c">\{B_{{}_{{s_{g}}}}^{i}\}</annotation></semantics></math>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2212.01231/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_img_landscape" width="180" height="64" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">The pipeline of slice feature fusion. Our fusion strategy contains two stages. The first stage is based on channel attention to merge local slices and global slices separately. The second stage is based on a dual-branch transformer, which explores the spatial attention.</span></figcaption>
</figure>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.10" class="ltx_p"><span id="S3.SS2.SSS1.p3.10.1" class="ltx_text ltx_font_bold">Local Slices</span> The goal of local slices is to emphasize the informative height ranges. We construct the local slices by sampling from the overall range <math id="S3.SS2.SSS1.p3.1.m1.2" class="ltx_Math" alttext="[-6,4]" display="inline"><semantics id="S3.SS2.SSS1.p3.1.m1.2a"><mrow id="S3.SS2.SSS1.p3.1.m1.2.2.1" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.1.m1.2.2.1.2" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.cmml">[</mo><mrow id="S3.SS2.SSS1.p3.1.m1.2.2.1.1" xref="S3.SS2.SSS1.p3.1.m1.2.2.1.1.cmml"><mo id="S3.SS2.SSS1.p3.1.m1.2.2.1.1a" xref="S3.SS2.SSS1.p3.1.m1.2.2.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p3.1.m1.2.2.1.1.2" xref="S3.SS2.SSS1.p3.1.m1.2.2.1.1.2.cmml">6</mn></mrow><mo id="S3.SS2.SSS1.p3.1.m1.2.2.1.3" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.cmml">,</mo><mn id="S3.SS2.SSS1.p3.1.m1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.cmml">4</mn><mo stretchy="false" id="S3.SS2.SSS1.p3.1.m1.2.2.1.4" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.1.m1.2b"><interval closure="closed" id="S3.SS2.SSS1.p3.1.m1.2.2.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.1"><apply id="S3.SS2.SSS1.p3.1.m1.2.2.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.1.1"><minus id="S3.SS2.SSS1.p3.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p3.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.1.1.2">6</cn></apply><cn type="integer" id="S3.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1">4</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.1.m1.2c">[-6,4]</annotation></semantics></math>. In order to sample reasonable local slices, we present a LiDAR-guided sampling strategy to determine the optimal heights of local slices. We transform the LiDAR points to BEV space and calculate the histogram along the height dimension as shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2.1 Global and Local Slices ‣ 3.2 Slice Attention Module ‣ 3 Methods ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We find that most LiDAR points are located around -2 and 0. However, those regions contain small objects while regions outside [-2,2] contain large objects. In order to sample more effective local slices, we design a novel strategy to consider the distribution differences between classes. Specifically, we accumulate the histogram and choose the local slices from the accumulated distribution. We slice the overall range <math id="S3.SS2.SSS1.p3.2.m2.2" class="ltx_Math" alttext="[-6,4]" display="inline"><semantics id="S3.SS2.SSS1.p3.2.m2.2a"><mrow id="S3.SS2.SSS1.p3.2.m2.2.2.1" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.2.m2.2.2.1.2" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.cmml">[</mo><mrow id="S3.SS2.SSS1.p3.2.m2.2.2.1.1" xref="S3.SS2.SSS1.p3.2.m2.2.2.1.1.cmml"><mo id="S3.SS2.SSS1.p3.2.m2.2.2.1.1a" xref="S3.SS2.SSS1.p3.2.m2.2.2.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p3.2.m2.2.2.1.1.2" xref="S3.SS2.SSS1.p3.2.m2.2.2.1.1.2.cmml">6</mn></mrow><mo id="S3.SS2.SSS1.p3.2.m2.2.2.1.3" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.cmml">,</mo><mn id="S3.SS2.SSS1.p3.2.m2.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.cmml">4</mn><mo stretchy="false" id="S3.SS2.SSS1.p3.2.m2.2.2.1.4" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.2.m2.2b"><interval closure="closed" id="S3.SS2.SSS1.p3.2.m2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.1"><apply id="S3.SS2.SSS1.p3.2.m2.2.2.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.1.1"><minus id="S3.SS2.SSS1.p3.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p3.2.m2.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.1.1.2">6</cn></apply><cn type="integer" id="S3.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1">4</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.2.m2.2c">[-6,4]</annotation></semantics></math> to six bins, including <math id="S3.SS2.SSS1.p3.3.m3.2" class="ltx_Math" alttext="[-6,-3]" display="inline"><semantics id="S3.SS2.SSS1.p3.3.m3.2a"><mrow id="S3.SS2.SSS1.p3.3.m3.2.2.2" xref="S3.SS2.SSS1.p3.3.m3.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.3.m3.2.2.2.3" xref="S3.SS2.SSS1.p3.3.m3.2.2.3.cmml">[</mo><mrow id="S3.SS2.SSS1.p3.3.m3.1.1.1.1" xref="S3.SS2.SSS1.p3.3.m3.1.1.1.1.cmml"><mo id="S3.SS2.SSS1.p3.3.m3.1.1.1.1a" xref="S3.SS2.SSS1.p3.3.m3.1.1.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p3.3.m3.1.1.1.1.2" xref="S3.SS2.SSS1.p3.3.m3.1.1.1.1.2.cmml">6</mn></mrow><mo id="S3.SS2.SSS1.p3.3.m3.2.2.2.4" xref="S3.SS2.SSS1.p3.3.m3.2.2.3.cmml">,</mo><mrow id="S3.SS2.SSS1.p3.3.m3.2.2.2.2" xref="S3.SS2.SSS1.p3.3.m3.2.2.2.2.cmml"><mo id="S3.SS2.SSS1.p3.3.m3.2.2.2.2a" xref="S3.SS2.SSS1.p3.3.m3.2.2.2.2.cmml">−</mo><mn id="S3.SS2.SSS1.p3.3.m3.2.2.2.2.2" xref="S3.SS2.SSS1.p3.3.m3.2.2.2.2.2.cmml">3</mn></mrow><mo stretchy="false" id="S3.SS2.SSS1.p3.3.m3.2.2.2.5" xref="S3.SS2.SSS1.p3.3.m3.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.3.m3.2b"><interval closure="closed" id="S3.SS2.SSS1.p3.3.m3.2.2.3.cmml" xref="S3.SS2.SSS1.p3.3.m3.2.2.2"><apply id="S3.SS2.SSS1.p3.3.m3.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.1.1"><minus id="S3.SS2.SSS1.p3.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p3.3.m3.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.1.1.2">6</cn></apply><apply id="S3.SS2.SSS1.p3.3.m3.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.3.m3.2.2.2.2"><minus id="S3.SS2.SSS1.p3.3.m3.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.2.2.2.2"></minus><cn type="integer" id="S3.SS2.SSS1.p3.3.m3.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.3.m3.2.2.2.2.2">3</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.3.m3.2c">[-6,-3]</annotation></semantics></math>, <math id="S3.SS2.SSS1.p3.4.m4.2" class="ltx_Math" alttext="[-3,-2]" display="inline"><semantics id="S3.SS2.SSS1.p3.4.m4.2a"><mrow id="S3.SS2.SSS1.p3.4.m4.2.2.2" xref="S3.SS2.SSS1.p3.4.m4.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.4.m4.2.2.2.3" xref="S3.SS2.SSS1.p3.4.m4.2.2.3.cmml">[</mo><mrow id="S3.SS2.SSS1.p3.4.m4.1.1.1.1" xref="S3.SS2.SSS1.p3.4.m4.1.1.1.1.cmml"><mo id="S3.SS2.SSS1.p3.4.m4.1.1.1.1a" xref="S3.SS2.SSS1.p3.4.m4.1.1.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p3.4.m4.1.1.1.1.2" xref="S3.SS2.SSS1.p3.4.m4.1.1.1.1.2.cmml">3</mn></mrow><mo id="S3.SS2.SSS1.p3.4.m4.2.2.2.4" xref="S3.SS2.SSS1.p3.4.m4.2.2.3.cmml">,</mo><mrow id="S3.SS2.SSS1.p3.4.m4.2.2.2.2" xref="S3.SS2.SSS1.p3.4.m4.2.2.2.2.cmml"><mo id="S3.SS2.SSS1.p3.4.m4.2.2.2.2a" xref="S3.SS2.SSS1.p3.4.m4.2.2.2.2.cmml">−</mo><mn id="S3.SS2.SSS1.p3.4.m4.2.2.2.2.2" xref="S3.SS2.SSS1.p3.4.m4.2.2.2.2.2.cmml">2</mn></mrow><mo stretchy="false" id="S3.SS2.SSS1.p3.4.m4.2.2.2.5" xref="S3.SS2.SSS1.p3.4.m4.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.4.m4.2b"><interval closure="closed" id="S3.SS2.SSS1.p3.4.m4.2.2.3.cmml" xref="S3.SS2.SSS1.p3.4.m4.2.2.2"><apply id="S3.SS2.SSS1.p3.4.m4.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.1.1"><minus id="S3.SS2.SSS1.p3.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p3.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.1.1.2">3</cn></apply><apply id="S3.SS2.SSS1.p3.4.m4.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.4.m4.2.2.2.2"><minus id="S3.SS2.SSS1.p3.4.m4.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.2.2.2.2"></minus><cn type="integer" id="S3.SS2.SSS1.p3.4.m4.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.4.m4.2.2.2.2.2">2</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.4.m4.2c">[-3,-2]</annotation></semantics></math>, <math id="S3.SS2.SSS1.p3.5.m5.2" class="ltx_Math" alttext="[-2,-1]" display="inline"><semantics id="S3.SS2.SSS1.p3.5.m5.2a"><mrow id="S3.SS2.SSS1.p3.5.m5.2.2.2" xref="S3.SS2.SSS1.p3.5.m5.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.5.m5.2.2.2.3" xref="S3.SS2.SSS1.p3.5.m5.2.2.3.cmml">[</mo><mrow id="S3.SS2.SSS1.p3.5.m5.1.1.1.1" xref="S3.SS2.SSS1.p3.5.m5.1.1.1.1.cmml"><mo id="S3.SS2.SSS1.p3.5.m5.1.1.1.1a" xref="S3.SS2.SSS1.p3.5.m5.1.1.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p3.5.m5.1.1.1.1.2" xref="S3.SS2.SSS1.p3.5.m5.1.1.1.1.2.cmml">2</mn></mrow><mo id="S3.SS2.SSS1.p3.5.m5.2.2.2.4" xref="S3.SS2.SSS1.p3.5.m5.2.2.3.cmml">,</mo><mrow id="S3.SS2.SSS1.p3.5.m5.2.2.2.2" xref="S3.SS2.SSS1.p3.5.m5.2.2.2.2.cmml"><mo id="S3.SS2.SSS1.p3.5.m5.2.2.2.2a" xref="S3.SS2.SSS1.p3.5.m5.2.2.2.2.cmml">−</mo><mn id="S3.SS2.SSS1.p3.5.m5.2.2.2.2.2" xref="S3.SS2.SSS1.p3.5.m5.2.2.2.2.2.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS2.SSS1.p3.5.m5.2.2.2.5" xref="S3.SS2.SSS1.p3.5.m5.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.5.m5.2b"><interval closure="closed" id="S3.SS2.SSS1.p3.5.m5.2.2.3.cmml" xref="S3.SS2.SSS1.p3.5.m5.2.2.2"><apply id="S3.SS2.SSS1.p3.5.m5.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1.1.1"><minus id="S3.SS2.SSS1.p3.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p3.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1.1.1.2">2</cn></apply><apply id="S3.SS2.SSS1.p3.5.m5.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.5.m5.2.2.2.2"><minus id="S3.SS2.SSS1.p3.5.m5.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p3.5.m5.2.2.2.2"></minus><cn type="integer" id="S3.SS2.SSS1.p3.5.m5.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.5.m5.2.2.2.2.2">1</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.5.m5.2c">[-2,-1]</annotation></semantics></math>,<math id="S3.SS2.SSS1.p3.6.m6.2" class="ltx_Math" alttext="[-1,0]" display="inline"><semantics id="S3.SS2.SSS1.p3.6.m6.2a"><mrow id="S3.SS2.SSS1.p3.6.m6.2.2.1" xref="S3.SS2.SSS1.p3.6.m6.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.6.m6.2.2.1.2" xref="S3.SS2.SSS1.p3.6.m6.2.2.2.cmml">[</mo><mrow id="S3.SS2.SSS1.p3.6.m6.2.2.1.1" xref="S3.SS2.SSS1.p3.6.m6.2.2.1.1.cmml"><mo id="S3.SS2.SSS1.p3.6.m6.2.2.1.1a" xref="S3.SS2.SSS1.p3.6.m6.2.2.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p3.6.m6.2.2.1.1.2" xref="S3.SS2.SSS1.p3.6.m6.2.2.1.1.2.cmml">1</mn></mrow><mo id="S3.SS2.SSS1.p3.6.m6.2.2.1.3" xref="S3.SS2.SSS1.p3.6.m6.2.2.2.cmml">,</mo><mn id="S3.SS2.SSS1.p3.6.m6.1.1" xref="S3.SS2.SSS1.p3.6.m6.1.1.cmml">0</mn><mo stretchy="false" id="S3.SS2.SSS1.p3.6.m6.2.2.1.4" xref="S3.SS2.SSS1.p3.6.m6.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.6.m6.2b"><interval closure="closed" id="S3.SS2.SSS1.p3.6.m6.2.2.2.cmml" xref="S3.SS2.SSS1.p3.6.m6.2.2.1"><apply id="S3.SS2.SSS1.p3.6.m6.2.2.1.1.cmml" xref="S3.SS2.SSS1.p3.6.m6.2.2.1.1"><minus id="S3.SS2.SSS1.p3.6.m6.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p3.6.m6.2.2.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p3.6.m6.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p3.6.m6.2.2.1.1.2">1</cn></apply><cn type="integer" id="S3.SS2.SSS1.p3.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p3.6.m6.1.1">0</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.6.m6.2c">[-1,0]</annotation></semantics></math>, <math id="S3.SS2.SSS1.p3.7.m7.2" class="ltx_Math" alttext="[0,2]" display="inline"><semantics id="S3.SS2.SSS1.p3.7.m7.2a"><mrow id="S3.SS2.SSS1.p3.7.m7.2.3.2" xref="S3.SS2.SSS1.p3.7.m7.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.7.m7.2.3.2.1" xref="S3.SS2.SSS1.p3.7.m7.2.3.1.cmml">[</mo><mn id="S3.SS2.SSS1.p3.7.m7.1.1" xref="S3.SS2.SSS1.p3.7.m7.1.1.cmml">0</mn><mo id="S3.SS2.SSS1.p3.7.m7.2.3.2.2" xref="S3.SS2.SSS1.p3.7.m7.2.3.1.cmml">,</mo><mn id="S3.SS2.SSS1.p3.7.m7.2.2" xref="S3.SS2.SSS1.p3.7.m7.2.2.cmml">2</mn><mo stretchy="false" id="S3.SS2.SSS1.p3.7.m7.2.3.2.3" xref="S3.SS2.SSS1.p3.7.m7.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.7.m7.2b"><interval closure="closed" id="S3.SS2.SSS1.p3.7.m7.2.3.1.cmml" xref="S3.SS2.SSS1.p3.7.m7.2.3.2"><cn type="integer" id="S3.SS2.SSS1.p3.7.m7.1.1.cmml" xref="S3.SS2.SSS1.p3.7.m7.1.1">0</cn><cn type="integer" id="S3.SS2.SSS1.p3.7.m7.2.2.cmml" xref="S3.SS2.SSS1.p3.7.m7.2.2">2</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.7.m7.2c">[0,2]</annotation></semantics></math>, and <math id="S3.SS2.SSS1.p3.8.m8.2" class="ltx_Math" alttext="[-2,4]" display="inline"><semantics id="S3.SS2.SSS1.p3.8.m8.2a"><mrow id="S3.SS2.SSS1.p3.8.m8.2.2.1" xref="S3.SS2.SSS1.p3.8.m8.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.8.m8.2.2.1.2" xref="S3.SS2.SSS1.p3.8.m8.2.2.2.cmml">[</mo><mrow id="S3.SS2.SSS1.p3.8.m8.2.2.1.1" xref="S3.SS2.SSS1.p3.8.m8.2.2.1.1.cmml"><mo id="S3.SS2.SSS1.p3.8.m8.2.2.1.1a" xref="S3.SS2.SSS1.p3.8.m8.2.2.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p3.8.m8.2.2.1.1.2" xref="S3.SS2.SSS1.p3.8.m8.2.2.1.1.2.cmml">2</mn></mrow><mo id="S3.SS2.SSS1.p3.8.m8.2.2.1.3" xref="S3.SS2.SSS1.p3.8.m8.2.2.2.cmml">,</mo><mn id="S3.SS2.SSS1.p3.8.m8.1.1" xref="S3.SS2.SSS1.p3.8.m8.1.1.cmml">4</mn><mo stretchy="false" id="S3.SS2.SSS1.p3.8.m8.2.2.1.4" xref="S3.SS2.SSS1.p3.8.m8.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.8.m8.2b"><interval closure="closed" id="S3.SS2.SSS1.p3.8.m8.2.2.2.cmml" xref="S3.SS2.SSS1.p3.8.m8.2.2.1"><apply id="S3.SS2.SSS1.p3.8.m8.2.2.1.1.cmml" xref="S3.SS2.SSS1.p3.8.m8.2.2.1.1"><minus id="S3.SS2.SSS1.p3.8.m8.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p3.8.m8.2.2.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p3.8.m8.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p3.8.m8.2.2.1.1.2">2</cn></apply><cn type="integer" id="S3.SS2.SSS1.p3.8.m8.1.1.cmml" xref="S3.SS2.SSS1.p3.8.m8.1.1">4</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.8.m8.2c">[-2,4]</annotation></semantics></math>. Similar to global slices, we also utilize the channel attention mechanism to reweight the local slices, which effectively aggregates the information of different heights. The local slices are denoted as <math id="S3.SS2.SSS1.p3.9.m9.1" class="ltx_Math" alttext="\{{s_{l}}\}" display="inline"><semantics id="S3.SS2.SSS1.p3.9.m9.1a"><mrow id="S3.SS2.SSS1.p3.9.m9.1.1.1" xref="S3.SS2.SSS1.p3.9.m9.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.9.m9.1.1.1.2" xref="S3.SS2.SSS1.p3.9.m9.1.1.2.cmml">{</mo><msub id="S3.SS2.SSS1.p3.9.m9.1.1.1.1" xref="S3.SS2.SSS1.p3.9.m9.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p3.9.m9.1.1.1.1.2" xref="S3.SS2.SSS1.p3.9.m9.1.1.1.1.2.cmml">s</mi><mi id="S3.SS2.SSS1.p3.9.m9.1.1.1.1.3" xref="S3.SS2.SSS1.p3.9.m9.1.1.1.1.3.cmml">l</mi></msub><mo stretchy="false" id="S3.SS2.SSS1.p3.9.m9.1.1.1.3" xref="S3.SS2.SSS1.p3.9.m9.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.9.m9.1b"><set id="S3.SS2.SSS1.p3.9.m9.1.1.2.cmml" xref="S3.SS2.SSS1.p3.9.m9.1.1.1"><apply id="S3.SS2.SSS1.p3.9.m9.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.9.m9.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.9.m9.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.9.m9.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.9.m9.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.9.m9.1.1.1.1.2">𝑠</ci><ci id="S3.SS2.SSS1.p3.9.m9.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p3.9.m9.1.1.1.1.3">𝑙</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.9.m9.1c">\{{s_{l}}\}</annotation></semantics></math> and the aggregated features are denoted as <math id="S3.SS2.SSS1.p3.10.m10.1" class="ltx_Math" alttext="\{B_{{s_{l}}}^{j}\}" display="inline"><semantics id="S3.SS2.SSS1.p3.10.m10.1a"><mrow id="S3.SS2.SSS1.p3.10.m10.1.1.1" xref="S3.SS2.SSS1.p3.10.m10.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.10.m10.1.1.1.2" xref="S3.SS2.SSS1.p3.10.m10.1.1.2.cmml">{</mo><msubsup id="S3.SS2.SSS1.p3.10.m10.1.1.1.1" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.2" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.2.cmml">B</mi><msub id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3.cmml"><mi id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3.2" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3.2.cmml">s</mi><mi id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3.3" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3.3.cmml">l</mi></msub><mi id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.3" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.3.cmml">j</mi></msubsup><mo stretchy="false" id="S3.SS2.SSS1.p3.10.m10.1.1.1.3" xref="S3.SS2.SSS1.p3.10.m10.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.10.m10.1b"><set id="S3.SS2.SSS1.p3.10.m10.1.1.2.cmml" xref="S3.SS2.SSS1.p3.10.m10.1.1.1"><apply id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.2">𝐵</ci><apply id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3">subscript</csymbol><ci id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3.2.cmml" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3.2">𝑠</ci><ci id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3.3.cmml" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.2.3.3">𝑙</ci></apply></apply><ci id="S3.SS2.SSS1.p3.10.m10.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p3.10.m10.1.1.1.1.3">𝑗</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.10.m10.1c">\{B_{{s_{l}}}^{j}\}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Fusion of Slice Features</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.6" class="ltx_p">After obtaining the global features <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\{B_{{}_{{s_{g}}}}^{i}\}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mrow id="S3.SS2.SSS2.p1.1.m1.1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.1.m1.1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml">{</mo><msubsup id="S3.SS2.SSS2.p1.1.m1.1.1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.2.cmml">B</mi><msub id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3a" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.cmml"></mi><msub id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1.2.cmml">s</mi><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1.3.cmml">g</mi></msub></msub><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.SS2.SSS2.p1.1.m1.1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><set id="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.2">𝐵</ci><apply id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1.2">𝑠</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.2.3.1.3">𝑔</ci></apply></apply></apply><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.3">𝑖</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">\{B_{{}_{{s_{g}}}}^{i}\}</annotation></semantics></math> and local features <math id="S3.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\{B_{{s_{l}}}^{j}\}" display="inline"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><mrow id="S3.SS2.SSS2.p1.2.m2.1.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.2.m2.1.1.1.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml">{</mo><msubsup id="S3.SS2.SSS2.p1.2.m2.1.1.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.cmml">B</mi><msub id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3.2.cmml">s</mi><mi id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3.3.cmml">l</mi></msub><mi id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.3.cmml">j</mi></msubsup><mo stretchy="false" id="S3.SS2.SSS2.p1.2.m2.1.1.1.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><set id="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1"><apply id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.2">𝐵</ci><apply id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3">subscript</csymbol><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3.2">𝑠</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.2.3.3">𝑙</ci></apply></apply><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.3">𝑗</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">\{B_{{s_{l}}}^{j}\}</annotation></semantics></math>, we can fuse them together into the feature map for task heads. Our method introduces a two-stage attention structure to progressively fuse the features as shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.1 Motivation ‣ 3 Methods ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In the first stage, we fuse the global features and local features via the attention mechanism. This will generate the global fused feature <math id="S3.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="{B_{g}}\in{R^{C\times{H_{e}}\times{W_{e}}}}" display="inline"><semantics id="S3.SS2.SSS2.p1.3.m3.1a"><mrow id="S3.SS2.SSS2.p1.3.m3.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.cmml"><msub id="S3.SS2.SSS2.p1.3.m3.1.1.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.1.1.2.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.2.cmml">B</mi><mi id="S3.SS2.SSS2.p1.3.m3.1.1.2.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.3.cmml">g</mi></msub><mo id="S3.SS2.SSS2.p1.3.m3.1.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS2.p1.3.m3.1.1.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.2.cmml">R</mi><mrow id="S3.SS2.SSS2.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3.2.cmml">H</mi><mi id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3.3.cmml">e</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.1a" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4.2.cmml">W</mi><mi id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4.3.cmml">e</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.1b"><apply id="S3.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1"><in id="S3.SS2.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.1"></in><apply id="S3.SS2.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.2">𝐵</ci><ci id="S3.SS2.SSS2.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.3">𝑔</ci></apply><apply id="S3.SS2.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.2">𝑅</ci><apply id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3"><times id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.1"></times><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.2">𝐶</ci><apply id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3.2">𝐻</ci><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.3.3">𝑒</ci></apply><apply id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4.2">𝑊</ci><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.4.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.1c">{B_{g}}\in{R^{C\times{H_{e}}\times{W_{e}}}}</annotation></semantics></math> and local fused feature <math id="S3.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="{B_{l}}\in{R^{C\times{H_{e}}\times{W_{e}}}}" display="inline"><semantics id="S3.SS2.SSS2.p1.4.m4.1a"><mrow id="S3.SS2.SSS2.p1.4.m4.1.1" xref="S3.SS2.SSS2.p1.4.m4.1.1.cmml"><msub id="S3.SS2.SSS2.p1.4.m4.1.1.2" xref="S3.SS2.SSS2.p1.4.m4.1.1.2.cmml"><mi id="S3.SS2.SSS2.p1.4.m4.1.1.2.2" xref="S3.SS2.SSS2.p1.4.m4.1.1.2.2.cmml">B</mi><mi id="S3.SS2.SSS2.p1.4.m4.1.1.2.3" xref="S3.SS2.SSS2.p1.4.m4.1.1.2.3.cmml">l</mi></msub><mo id="S3.SS2.SSS2.p1.4.m4.1.1.1" xref="S3.SS2.SSS2.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS2.p1.4.m4.1.1.3" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS2.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.2.cmml">R</mi><mrow id="S3.SS2.SSS2.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.2" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.1" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3.cmml"><mi id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3.2" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3.2.cmml">H</mi><mi id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3.3" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3.3.cmml">e</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.1a" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4.cmml"><mi id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4.2" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4.2.cmml">W</mi><mi id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4.3" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4.3.cmml">e</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.4.m4.1b"><apply id="S3.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1"><in id="S3.SS2.SSS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.1"></in><apply id="S3.SS2.SSS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.2.2">𝐵</ci><ci id="S3.SS2.SSS2.p1.4.m4.1.1.2.3.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.2.3">𝑙</ci></apply><apply id="S3.SS2.SSS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.2">𝑅</ci><apply id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3"><times id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.1"></times><ci id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.2">𝐶</ci><apply id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3.2.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3.2">𝐻</ci><ci id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.3.3">𝑒</ci></apply><apply id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4">subscript</csymbol><ci id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4.2.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4.2">𝑊</ci><ci id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4.3.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.4.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.4.m4.1c">{B_{l}}\in{R^{C\times{H_{e}}\times{W_{e}}}}</annotation></semantics></math>. In the second stage, we use a transformer to fuse <math id="S3.SS2.SSS2.p1.5.m5.1" class="ltx_Math" alttext="{B_{g}}" display="inline"><semantics id="S3.SS2.SSS2.p1.5.m5.1a"><msub id="S3.SS2.SSS2.p1.5.m5.1.1" xref="S3.SS2.SSS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS2.p1.5.m5.1.1.2" xref="S3.SS2.SSS2.p1.5.m5.1.1.2.cmml">B</mi><mi id="S3.SS2.SSS2.p1.5.m5.1.1.3" xref="S3.SS2.SSS2.p1.5.m5.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.5.m5.1b"><apply id="S3.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1.2">𝐵</ci><ci id="S3.SS2.SSS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.5.m5.1c">{B_{g}}</annotation></semantics></math> and <math id="S3.SS2.SSS2.p1.6.m6.1" class="ltx_Math" alttext="{B_{l}}" display="inline"><semantics id="S3.SS2.SSS2.p1.6.m6.1a"><msub id="S3.SS2.SSS2.p1.6.m6.1.1" xref="S3.SS2.SSS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS2.p1.6.m6.1.1.2" xref="S3.SS2.SSS2.p1.6.m6.1.1.2.cmml">B</mi><mi id="S3.SS2.SSS2.p1.6.m6.1.1.3" xref="S3.SS2.SSS2.p1.6.m6.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.6.m6.1b"><apply id="S3.SS2.SSS2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS2.p1.6.m6.1.1.2">𝐵</ci><ci id="S3.SS2.SSS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS2.p1.6.m6.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.6.m6.1c">{B_{l}}</annotation></semantics></math> and generate the feature map for task heads.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2212.01231/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_img_landscape" width="179" height="92" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">Illustration of the SE attention residual block for merging local and global slices separately.</span></figcaption>
</figure>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.10" class="ltx_p">To be more specific, in the first stage, we adopt the attention mechanism similar to the Squeeze-and-Excitation (SE) operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Taking local features as an example, the features of local slices are denoted as <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\{B_{{}_{{s_{l}}}}^{j}\}\in{R^{J\times C\times{H_{e}}\times{W_{e}}}}" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mrow id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS2.p2.1.m1.1.1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.2.cmml">B</mi><msub id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3a" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.cmml"></mi><msub id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1.2.cmml">s</mi><mi id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1.3.cmml">l</mi></msub></msub><mi id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.cmml">j</mi></msubsup><mo stretchy="false" id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.2.cmml">}</mo></mrow><mo id="S3.SS2.SSS2.p2.1.m1.1.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml">∈</mo><msup id="S3.SS2.SSS2.p2.1.m1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.3.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.2.cmml">R</mi><mrow id="S3.SS2.SSS2.p2.1.m1.1.1.3.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.2.cmml">J</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.3.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1a" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4.2.cmml">H</mi><mi id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4.3.cmml">e</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1b" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5.2.cmml">W</mi><mi id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5.3.cmml">e</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><apply id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1"><in id="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.2"></in><set id="S3.SS2.SSS2.p2.1.m1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1"><apply id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.2">𝐵</ci><apply id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3"><apply id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1.2">𝑠</ci><ci id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.1.3">𝑙</ci></apply></apply></apply><ci id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3">𝑗</ci></apply></set><apply id="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.2">𝑅</ci><apply id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3"><times id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.2">𝐽</ci><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.3">𝐶</ci><apply id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4">subscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4.2">𝐻</ci><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.4.3">𝑒</ci></apply><apply id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5">subscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5.2">𝑊</ci><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.5.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">\{B_{{}_{{s_{l}}}}^{j}\}\in{R^{J\times C\times{H_{e}}\times{W_{e}}}}</annotation></semantics></math>, where <math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="J" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><mi id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><ci id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">𝐽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">J</annotation></semantics></math> is the number of local slices. As shown in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.2.2 Fusion of Slice Features ‣ 3.2 Slice Attention Module ‣ 3 Methods ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we first use 1x1 convolution to reduce the channel number from <math id="S3.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="{J\times C}" display="inline"><semantics id="S3.SS2.SSS2.p2.3.m3.1a"><mrow id="S3.SS2.SSS2.p2.3.m3.1.1" xref="S3.SS2.SSS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p2.3.m3.1.1.2" xref="S3.SS2.SSS2.p2.3.m3.1.1.2.cmml">J</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p2.3.m3.1.1.1" xref="S3.SS2.SSS2.p2.3.m3.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS2.p2.3.m3.1.1.3" xref="S3.SS2.SSS2.p2.3.m3.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.3.m3.1b"><apply id="S3.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1"><times id="S3.SS2.SSS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.1"></times><ci id="S3.SS2.SSS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.2">𝐽</ci><ci id="S3.SS2.SSS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.3.m3.1c">{J\times C}</annotation></semantics></math> to <math id="S3.SS2.SSS2.p2.4.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.SSS2.p2.4.m4.1a"><mi id="S3.SS2.SSS2.p2.4.m4.1.1" xref="S3.SS2.SSS2.p2.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.4.m4.1b"><ci id="S3.SS2.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.4.m4.1c">C</annotation></semantics></math>. We use global average pooling to extract the <math id="S3.SS2.SSS2.p2.5.m5.1" class="ltx_Math" alttext="{J\times C}" display="inline"><semantics id="S3.SS2.SSS2.p2.5.m5.1a"><mrow id="S3.SS2.SSS2.p2.5.m5.1.1" xref="S3.SS2.SSS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.SSS2.p2.5.m5.1.1.2" xref="S3.SS2.SSS2.p2.5.m5.1.1.2.cmml">J</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p2.5.m5.1.1.1" xref="S3.SS2.SSS2.p2.5.m5.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS2.p2.5.m5.1.1.3" xref="S3.SS2.SSS2.p2.5.m5.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.5.m5.1b"><apply id="S3.SS2.SSS2.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p2.5.m5.1.1"><times id="S3.SS2.SSS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.SSS2.p2.5.m5.1.1.1"></times><ci id="S3.SS2.SSS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.SSS2.p2.5.m5.1.1.2">𝐽</ci><ci id="S3.SS2.SSS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.SSS2.p2.5.m5.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.5.m5.1c">{J\times C}</annotation></semantics></math> feature and reweight the input feature. Another 3x3 convolution is used to reduce the channel number from <math id="S3.SS2.SSS2.p2.6.m6.1" class="ltx_Math" alttext="{J\times C}" display="inline"><semantics id="S3.SS2.SSS2.p2.6.m6.1a"><mrow id="S3.SS2.SSS2.p2.6.m6.1.1" xref="S3.SS2.SSS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.SSS2.p2.6.m6.1.1.2" xref="S3.SS2.SSS2.p2.6.m6.1.1.2.cmml">J</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p2.6.m6.1.1.1" xref="S3.SS2.SSS2.p2.6.m6.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS2.p2.6.m6.1.1.3" xref="S3.SS2.SSS2.p2.6.m6.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.6.m6.1b"><apply id="S3.SS2.SSS2.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS2.p2.6.m6.1.1"><times id="S3.SS2.SSS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.SSS2.p2.6.m6.1.1.1"></times><ci id="S3.SS2.SSS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.SSS2.p2.6.m6.1.1.2">𝐽</ci><ci id="S3.SS2.SSS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.SSS2.p2.6.m6.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.6.m6.1c">{J\times C}</annotation></semantics></math> to <math id="S3.SS2.SSS2.p2.7.m7.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.SSS2.p2.7.m7.1a"><mi id="S3.SS2.SSS2.p2.7.m7.1.1" xref="S3.SS2.SSS2.p2.7.m7.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.7.m7.1b"><ci id="S3.SS2.SSS2.p2.7.m7.1.1.cmml" xref="S3.SS2.SSS2.p2.7.m7.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.7.m7.1c">C</annotation></semantics></math>. Finally, we add the two parts to deliver the fused feature <math id="S3.SS2.SSS2.p2.8.m8.1" class="ltx_Math" alttext="{B_{l}}\in{R^{C\times{H_{e}}\times{W_{e}}}}" display="inline"><semantics id="S3.SS2.SSS2.p2.8.m8.1a"><mrow id="S3.SS2.SSS2.p2.8.m8.1.1" xref="S3.SS2.SSS2.p2.8.m8.1.1.cmml"><msub id="S3.SS2.SSS2.p2.8.m8.1.1.2" xref="S3.SS2.SSS2.p2.8.m8.1.1.2.cmml"><mi id="S3.SS2.SSS2.p2.8.m8.1.1.2.2" xref="S3.SS2.SSS2.p2.8.m8.1.1.2.2.cmml">B</mi><mi id="S3.SS2.SSS2.p2.8.m8.1.1.2.3" xref="S3.SS2.SSS2.p2.8.m8.1.1.2.3.cmml">l</mi></msub><mo id="S3.SS2.SSS2.p2.8.m8.1.1.1" xref="S3.SS2.SSS2.p2.8.m8.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS2.p2.8.m8.1.1.3" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.cmml"><mi id="S3.SS2.SSS2.p2.8.m8.1.1.3.2" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.2.cmml">R</mi><mrow id="S3.SS2.SSS2.p2.8.m8.1.1.3.3" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.2" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.1" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3.cmml"><mi id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3.2" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3.2.cmml">H</mi><mi id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3.3" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3.3.cmml">e</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.1a" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4.cmml"><mi id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4.2" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4.2.cmml">W</mi><mi id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4.3" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4.3.cmml">e</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.8.m8.1b"><apply id="S3.SS2.SSS2.p2.8.m8.1.1.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1"><in id="S3.SS2.SSS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.1"></in><apply id="S3.SS2.SSS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.8.m8.1.1.2.1.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p2.8.m8.1.1.2.2.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.2.2">𝐵</ci><ci id="S3.SS2.SSS2.p2.8.m8.1.1.2.3.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.2.3">𝑙</ci></apply><apply id="S3.SS2.SSS2.p2.8.m8.1.1.3.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.8.m8.1.1.3.1.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p2.8.m8.1.1.3.2.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.2">𝑅</ci><apply id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3"><times id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.1"></times><ci id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.2">𝐶</ci><apply id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3.1.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3.2.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3.2">𝐻</ci><ci id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3.3.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.3.3">𝑒</ci></apply><apply id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4.1.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4">subscript</csymbol><ci id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4.2.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4.2">𝑊</ci><ci id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4.3.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.4.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.8.m8.1c">{B_{l}}\in{R^{C\times{H_{e}}\times{W_{e}}}}</annotation></semantics></math>. The features of global slices <math id="S3.SS2.SSS2.p2.9.m9.1" class="ltx_Math" alttext="\{B_{{}_{{s_{g}}}}^{i}\}" display="inline"><semantics id="S3.SS2.SSS2.p2.9.m9.1a"><mrow id="S3.SS2.SSS2.p2.9.m9.1.1.1" xref="S3.SS2.SSS2.p2.9.m9.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p2.9.m9.1.1.1.2" xref="S3.SS2.SSS2.p2.9.m9.1.1.2.cmml">{</mo><msubsup id="S3.SS2.SSS2.p2.9.m9.1.1.1.1" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.2" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.2.cmml">B</mi><msub id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.cmml"><mi id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3a" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.cmml"></mi><msub id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1.cmml"><mi id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1.2" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1.2.cmml">s</mi><mi id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1.3" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1.3.cmml">g</mi></msub></msub><mi id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.3" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.SS2.SSS2.p2.9.m9.1.1.1.3" xref="S3.SS2.SSS2.p2.9.m9.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.9.m9.1b"><set id="S3.SS2.SSS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1"><apply id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.2">𝐵</ci><apply id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3"><apply id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1.1.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1.2.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1.2">𝑠</ci><ci id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1.3.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.2.3.1.3">𝑔</ci></apply></apply></apply><ci id="S3.SS2.SSS2.p2.9.m9.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.1.1.3">𝑖</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.9.m9.1c">\{B_{{}_{{s_{g}}}}^{i}\}</annotation></semantics></math> can be fused into <math id="S3.SS2.SSS2.p2.10.m10.1" class="ltx_Math" alttext="{B_{g}}" display="inline"><semantics id="S3.SS2.SSS2.p2.10.m10.1a"><msub id="S3.SS2.SSS2.p2.10.m10.1.1" xref="S3.SS2.SSS2.p2.10.m10.1.1.cmml"><mi id="S3.SS2.SSS2.p2.10.m10.1.1.2" xref="S3.SS2.SSS2.p2.10.m10.1.1.2.cmml">B</mi><mi id="S3.SS2.SSS2.p2.10.m10.1.1.3" xref="S3.SS2.SSS2.p2.10.m10.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.10.m10.1b"><apply id="S3.SS2.SSS2.p2.10.m10.1.1.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.10.m10.1.1.1.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.10.m10.1.1.2.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1.2">𝐵</ci><ci id="S3.SS2.SSS2.p2.10.m10.1.1.3.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.10.m10.1c">{B_{g}}</annotation></semantics></math> in the same way.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.7" class="ltx_p">In the second stage, we need to fuse <math id="S3.SS2.SSS2.p3.1.m1.1" class="ltx_Math" alttext="{B_{g}}" display="inline"><semantics id="S3.SS2.SSS2.p3.1.m1.1a"><msub id="S3.SS2.SSS2.p3.1.m1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.1.1.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml">B</mi><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.1.m1.1b"><apply id="S3.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.2">𝐵</ci><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.1.m1.1c">{B_{g}}</annotation></semantics></math> and <math id="S3.SS2.SSS2.p3.2.m2.1" class="ltx_Math" alttext="{B_{l}}" display="inline"><semantics id="S3.SS2.SSS2.p3.2.m2.1a"><msub id="S3.SS2.SSS2.p3.2.m2.1.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p3.2.m2.1.1.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.2.cmml">B</mi><mi id="S3.SS2.SSS2.p3.2.m2.1.1.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.2.m2.1b"><apply id="S3.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.2">𝐵</ci><ci id="S3.SS2.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.2.m2.1c">{B_{l}}</annotation></semantics></math> with a transformer. As shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Relate work ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the transformer contains two branches (denoted as G2L and L2G) using <math id="S3.SS2.SSS2.p3.3.m3.1" class="ltx_Math" alttext="{B_{g}}" display="inline"><semantics id="S3.SS2.SSS2.p3.3.m3.1a"><msub id="S3.SS2.SSS2.p3.3.m3.1.1" xref="S3.SS2.SSS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p3.3.m3.1.1.2" xref="S3.SS2.SSS2.p3.3.m3.1.1.2.cmml">B</mi><mi id="S3.SS2.SSS2.p3.3.m3.1.1.3" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.3.m3.1b"><apply id="S3.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.2">𝐵</ci><ci id="S3.SS2.SSS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.3.m3.1c">{B_{g}}</annotation></semantics></math> and <math id="S3.SS2.SSS2.p3.4.m4.1" class="ltx_Math" alttext="{B_{l}}" display="inline"><semantics id="S3.SS2.SSS2.p3.4.m4.1a"><msub id="S3.SS2.SSS2.p3.4.m4.1.1" xref="S3.SS2.SSS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.SSS2.p3.4.m4.1.1.2" xref="S3.SS2.SSS2.p3.4.m4.1.1.2.cmml">B</mi><mi id="S3.SS2.SSS2.p3.4.m4.1.1.3" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.4.m4.1b"><apply id="S3.SS2.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.2">𝐵</ci><ci id="S3.SS2.SSS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.4.m4.1c">{B_{l}}</annotation></semantics></math> as the inputs. One feature will be transformed into a set of <span id="S3.SS2.SSS2.p3.7.1" class="ltx_text ltx_font_italic">Key/Value</span> pairs to interact with features from the other. For example, the <span id="S3.SS2.SSS2.p3.7.2" class="ltx_text ltx_font_italic">Query/Key/Value</span> pair in G2L Transformer is: <math id="S3.SS2.SSS2.p3.5.m5.2" class="ltx_Math" alttext="q=q^{L},k=k^{G},v=V^{G}" display="inline"><semantics id="S3.SS2.SSS2.p3.5.m5.2a"><mrow id="S3.SS2.SSS2.p3.5.m5.2.2.2" xref="S3.SS2.SSS2.p3.5.m5.2.2.3.cmml"><mrow id="S3.SS2.SSS2.p3.5.m5.1.1.1.1" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.2" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.2.cmml">q</mi><mo id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.1" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.1.cmml">=</mo><msup id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3.2" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3.2.cmml">q</mi><mi id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3.3" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3.3.cmml">L</mi></msup></mrow><mo id="S3.SS2.SSS2.p3.5.m5.2.2.2.3" xref="S3.SS2.SSS2.p3.5.m5.2.2.3a.cmml">,</mo><mrow id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.3.cmml"><mrow id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.2" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.2.cmml">k</mi><mo id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.1" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.1.cmml">=</mo><msup id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3.2" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3.2.cmml">k</mi><mi id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3.3" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3.3.cmml">G</mi></msup></mrow><mo id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.3" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.3a.cmml">,</mo><mrow id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.2" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.2.cmml">v</mi><mo id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.1" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.1.cmml">=</mo><msup id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3.2" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3.2.cmml">V</mi><mi id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3.3" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3.3.cmml">G</mi></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.5.m5.2b"><apply id="S3.SS2.SSS2.p3.5.m5.2.2.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.2.2.3a.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1"><eq id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.1"></eq><ci id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.2">𝑞</ci><apply id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3.2">𝑞</ci><ci id="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.1.3.3">𝐿</ci></apply></apply><apply id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.3a.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1"><eq id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.1"></eq><ci id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.2">𝑘</ci><apply id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3.2">𝑘</ci><ci id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.1.1.3.3">𝐺</ci></apply></apply><apply id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2"><eq id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.1"></eq><ci id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.2">𝑣</ci><apply id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3.2">𝑉</ci><ci id="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.2.2.2.2.3.3">𝐺</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.5.m5.2c">q=q^{L},k=k^{G},v=V^{G}</annotation></semantics></math> where <math id="S3.SS2.SSS2.p3.6.m6.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.SSS2.p3.6.m6.1a"><mi id="S3.SS2.SSS2.p3.6.m6.1.1" xref="S3.SS2.SSS2.p3.6.m6.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.6.m6.1b"><ci id="S3.SS2.SSS2.p3.6.m6.1.1.cmml" xref="S3.SS2.SSS2.p3.6.m6.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.6.m6.1c">L</annotation></semantics></math> stands for local-level and <math id="S3.SS2.SSS2.p3.7.m7.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS2.SSS2.p3.7.m7.1a"><mi id="S3.SS2.SSS2.p3.7.m7.1.1" xref="S3.SS2.SSS2.p3.7.m7.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.7.m7.1b"><ci id="S3.SS2.SSS2.p3.7.m7.1.1.cmml" xref="S3.SS2.SSS2.p3.7.m7.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.7.m7.1c">G</annotation></semantics></math> represents global-level. Finally, we sum up the outputs of the two branches to obtain the final feature map for task heads.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.3.2" class="ltx_text" style="font-size:90%;">3D Object Detection Results on nuScenes val set without CBGS</span></figcaption>
<table id="S3.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.4.1.1" class="ltx_tr">
<th id="S3.T2.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Method</th>
<th id="S3.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Voxel Range</th>
<th id="S3.T2.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Backbone</th>
<th id="S3.T2.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">NDS ↑</th>
<td id="S3.T2.4.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mAP ↑</td>
<td id="S3.T2.4.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mATE ↓</td>
<td id="S3.T2.4.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mASE ↓</td>
<td id="S3.T2.4.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mAOE ↓</td>
<td id="S3.T2.4.1.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mAVE ↓</td>
<td id="S3.T2.4.1.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mAAE ↓</td>
</tr>
<tr id="S3.T2.4.2.2" class="ltx_tr">
<th id="S3.T2.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">BEVDepth</th>
<th id="S3.T2.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">[-5,3]</th>
<th id="S3.T2.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">R50</th>
<th id="S3.T2.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.328</th>
<td id="S3.T2.4.2.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.293</td>
<td id="S3.T2.4.2.2.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.742</td>
<td id="S3.T2.4.2.2.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.283</td>
<td id="S3.T2.4.2.2.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.758</td>
<td id="S3.T2.4.2.2.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">1.216</td>
<td id="S3.T2.4.2.2.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.403</td>
</tr>
<tr id="S3.T2.4.3.3" class="ltx_tr">
<th id="S3.T2.4.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">BEVDepth</th>
<th id="S3.T2.4.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">[-4,2]</th>
<th id="S3.T2.4.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">R50</th>
<th id="S3.T2.4.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">0.330</th>
<td id="S3.T2.4.3.3.5" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.293</td>
<td id="S3.T2.4.3.3.6" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.740</td>
<td id="S3.T2.4.3.3.7" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.282</td>
<td id="S3.T2.4.3.3.8" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.745</td>
<td id="S3.T2.4.3.3.9" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">1.201</td>
<td id="S3.T2.4.3.3.10" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.397</td>
</tr>
<tr id="S3.T2.4.4.4" class="ltx_tr">
<th id="S3.T2.4.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">BEVDepth</th>
<th id="S3.T2.4.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">[-6,4]</th>
<th id="S3.T2.4.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">R50</th>
<th id="S3.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">0.336</th>
<td id="S3.T2.4.4.4.5" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.296</td>
<td id="S3.T2.4.4.4.6" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.732</td>
<td id="S3.T2.4.4.4.7" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.283</td>
<td id="S3.T2.4.4.4.8" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.713</td>
<td id="S3.T2.4.4.4.9" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">1.218</td>
<td id="S3.T2.4.4.4.10" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.396</td>
</tr>
<tr id="S3.T2.4.5.5" class="ltx_tr">
<th id="S3.T2.4.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">BEVDet</th>
<th id="S3.T2.4.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">[-5,3]</th>
<th id="S3.T2.4.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">R50</th>
<th id="S3.T2.4.5.5.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">0.298</th>
<td id="S3.T2.4.5.5.5" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.274</td>
<td id="S3.T2.4.5.5.6" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.754</td>
<td id="S3.T2.4.5.5.7" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.295</td>
<td id="S3.T2.4.5.5.8" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.881</td>
<td id="S3.T2.4.5.5.9" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">1.25</td>
<td id="S3.T2.4.5.5.10" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.418</td>
</tr>
<tr id="S3.T2.4.6.6" class="ltx_tr">
<th id="S3.T2.4.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">SANet(BEVDet)</th>
<th id="S3.T2.4.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">slice</th>
<th id="S3.T2.4.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">R50</th>
<th id="S3.T2.4.6.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">0.320</th>
<td id="S3.T2.4.6.6.5" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.292</td>
<td id="S3.T2.4.6.6.6" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.746</td>
<td id="S3.T2.4.6.6.7" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.286</td>
<td id="S3.T2.4.6.6.8" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.797</td>
<td id="S3.T2.4.6.6.9" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">1.167</td>
<td id="S3.T2.4.6.6.10" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.403</td>
</tr>
<tr id="S3.T2.4.7.7" class="ltx_tr">
<th id="S3.T2.4.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">SANet(BEVDepth)</th>
<th id="S3.T2.4.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">Slice</th>
<th id="S3.T2.4.7.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">R50</th>
<th id="S3.T2.4.7.7.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">0.366</th>
<td id="S3.T2.4.7.7.5" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.310</td>
<td id="S3.T2.4.7.7.6" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.705</td>
<td id="S3.T2.4.7.7.7" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.278</td>
<td id="S3.T2.4.7.7.8" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.608</td>
<td id="S3.T2.4.7.7.9" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">1.070</td>
<td id="S3.T2.4.7.7.10" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.300</td>
</tr>
<tr id="S3.T2.4.8.8" class="ltx_tr">
<th id="S3.T2.4.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">BEVDepth</th>
<th id="S3.T2.4.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">[-6,4]</th>
<th id="S3.T2.4.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">R101</th>
<th id="S3.T2.4.8.8.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">0.371</th>
<td id="S3.T2.4.8.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">0.313</td>
<td id="S3.T2.4.8.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">0.697</td>
<td id="S3.T2.4.8.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">0.278</td>
<td id="S3.T2.4.8.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">0.579</td>
<td id="S3.T2.4.8.8.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">1.086</td>
<td id="S3.T2.4.8.8.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">0.304</td>
</tr>
<tr id="S3.T2.4.9.9" class="ltx_tr">
<th id="S3.T2.4.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">SANet(BEVDepth)</th>
<th id="S3.T2.4.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">Slice</th>
<th id="S3.T2.4.9.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">R101</th>
<th id="S3.T2.4.9.9.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">0.379</th>
<td id="S3.T2.4.9.9.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.319</td>
<td id="S3.T2.4.9.9.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.681</td>
<td id="S3.T2.4.9.9.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.270</td>
<td id="S3.T2.4.9.9.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.567</td>
<td id="S3.T2.4.9.9.9" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.996</td>
<td id="S3.T2.4.9.9.10" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.290</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S3.T3.3.2" class="ltx_text" style="font-size:90%;">3D Object Detection Results on nuScenes val set with CBGS.</span></figcaption>
<table id="S3.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.4.1.1" class="ltx_tr">
<th id="S3.T3.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Method</th>
<th id="S3.T3.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Voxel Range</th>
<th id="S3.T3.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Backbone</th>
<th id="S3.T3.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">NDS ↑</th>
<th id="S3.T3.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mAP ↑</th>
<th id="S3.T3.4.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mATE ↓</th>
<th id="S3.T3.4.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mASE ↓</th>
<th id="S3.T3.4.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mAOE ↓</th>
<th id="S3.T3.4.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mAVE ↓</th>
<th id="S3.T3.4.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">mAAE ↓</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.4.2.1" class="ltx_tr">
<th id="S3.T3.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">BEVDet</th>
<th id="S3.T3.4.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">[-5,3]</th>
<th id="S3.T3.4.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">R50</th>
<th id="S3.T3.4.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.372</th>
<td id="S3.T3.4.2.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.299</td>
<td id="S3.T3.4.2.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.724</td>
<td id="S3.T3.4.2.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.273</td>
<td id="S3.T3.4.2.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.578</td>
<td id="S3.T3.4.2.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.929</td>
<td id="S3.T3.4.2.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.4pt;padding-right:3.4pt;">0.266</td>
</tr>
<tr id="S3.T3.4.3.2" class="ltx_tr">
<th id="S3.T3.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">PETR</th>
<th id="S3.T3.4.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">[-5,3]</th>
<th id="S3.T3.4.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">R50</th>
<th id="S3.T3.4.3.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">0.381</th>
<td id="S3.T3.4.3.2.5" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.313</td>
<td id="S3.T3.4.3.2.6" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.768</td>
<td id="S3.T3.4.3.2.7" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.278</td>
<td id="S3.T3.4.3.2.8" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.564</td>
<td id="S3.T3.4.3.2.9" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.923</td>
<td id="S3.T3.4.3.2.10" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.225</td>
</tr>
<tr id="S3.T3.4.4.3" class="ltx_tr">
<th id="S3.T3.4.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">BEVDepth</th>
<th id="S3.T3.4.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">[-5,3]</th>
<th id="S3.T3.4.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">R50</th>
<th id="S3.T3.4.4.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">0.470</th>
<td id="S3.T3.4.4.3.5" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.341</td>
<td id="S3.T3.4.4.3.6" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.619</td>
<td id="S3.T3.4.4.3.7" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.273</td>
<td id="S3.T3.4.4.3.8" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.451</td>
<td id="S3.T3.4.4.3.9" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.462</td>
<td id="S3.T3.4.4.3.10" class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">0.198</td>
</tr>
<tr id="S3.T3.4.5.4" class="ltx_tr">
<th id="S3.T3.4.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">SANet(BEVDepth)</th>
<th id="S3.T3.4.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">Slice</th>
<th id="S3.T3.4.5.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">R50</th>
<th id="S3.T3.4.5.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:3.4pt;padding-right:3.4pt;">0.482</th>
<td id="S3.T3.4.5.4.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.351</td>
<td id="S3.T3.4.5.4.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.618</td>
<td id="S3.T3.4.5.4.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.271</td>
<td id="S3.T3.4.5.4.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.434</td>
<td id="S3.T3.4.5.4.9" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.426</td>
<td id="S3.T3.4.5.4.10" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">0.192</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S3.T4.3.2" class="ltx_text" style="font-size:90%;">3D Object Detection Results of Each Object Class on nuScenes val set.</span></figcaption>
<table id="S3.T4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.4.1.1" class="ltx_tr">
<th id="S3.T4.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Method</th>
<th id="S3.T4.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Truck</th>
<th id="S3.T4.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">trailer</th>
<th id="S3.T4.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Car</th>
<th id="S3.T4.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Bus</th>
<th id="S3.T4.4.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Pedestrian</th>
<th id="S3.T4.4.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Motorcycle</th>
<th id="S3.T4.4.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Bicycle</th>
<th id="S3.T4.4.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Barrier</th>
<th id="S3.T4.4.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Traffic cone</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.4.2.1" class="ltx_tr">
<th id="S3.T4.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">BEVDepth</th>
<td id="S3.T4.4.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.237</td>
<td id="S3.T4.4.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.153</td>
<td id="S3.T4.4.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.466</td>
<td id="S3.T4.4.2.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.332</td>
<td id="S3.T4.4.2.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.247</td>
<td id="S3.T4.4.2.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.289</td>
<td id="S3.T4.4.2.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.267</td>
<td id="S3.T4.4.2.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.417</td>
<td id="S3.T4.4.2.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.465</td>
</tr>
<tr id="S3.T4.4.3.2" class="ltx_tr">
<th id="S3.T4.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">SANet</th>
<td id="S3.T4.4.3.2.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.244</td>
<td id="S3.T4.4.3.2.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.165</td>
<td id="S3.T4.4.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.491</td>
<td id="S3.T4.4.3.2.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.358</td>
<td id="S3.T4.4.3.2.6" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.265</td>
<td id="S3.T4.4.3.2.7" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.302</td>
<td id="S3.T4.4.3.2.8" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.272</td>
<td id="S3.T4.4.3.2.9" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.432</td>
<td id="S3.T4.4.3.2.10" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.503</td>
</tr>
<tr id="S3.T4.4.4.3" class="ltx_tr">
<th id="S3.T4.4.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">BEVDepth+CBGS</th>
<td id="S3.T4.4.4.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">0.269</td>
<td id="S3.T4.4.4.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">0.171</td>
<td id="S3.T4.4.4.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">0.545</td>
<td id="S3.T4.4.4.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">0.352</td>
<td id="S3.T4.4.4.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">0.351</td>
<td id="S3.T4.4.4.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">0.318</td>
<td id="S3.T4.4.4.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">0.250</td>
<td id="S3.T4.4.4.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">0.530</td>
<td id="S3.T4.4.4.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">0.559</td>
</tr>
<tr id="S3.T4.4.5.4" class="ltx_tr">
<th id="S3.T4.4.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">SANet+CBGS</th>
<td id="S3.T4.4.5.4.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">0.272</td>
<td id="S3.T4.4.5.4.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">0.166</td>
<td id="S3.T4.4.5.4.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">0.555</td>
<td id="S3.T4.4.5.4.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">0.358</td>
<td id="S3.T4.4.5.4.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">0.365</td>
<td id="S3.T4.4.5.4.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">0.315</td>
<td id="S3.T4.4.5.4.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">0.282</td>
<td id="S3.T4.4.5.4.9" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">0.544</td>
<td id="S3.T4.4.5.4.10" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">0.582</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we first give the experimental details in Sec. <a href="#S4.SS1" title="4.1 Experimental Details ‣ 4 Experiment ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. Then we evaluate the proposed SAN on nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and compare it with several baseline methods in Sec. <a href="#S4.SS2" title="4.2 Main Results ‣ 4 Experiment ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. Besides, we also conduct detailed ablation study to evaluate each component of our method in Sec. <a href="#S4.SS3" title="4.3 Ablation study ‣ 4 Experiment ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>. We further show the computational cost in Sec. <a href="#S4.SS4" title="4.4 Computational Cost ‣ 4 Experiment ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Details</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset</span> We use the nuScenes<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> dataset to evaluate the performance of our distillation framework. NuScenes contains 1k sequences, each of which is composed of six groups of surround-view camera images, one group of Lidar data, and their sensor information. The camera images are collected with the resolution of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="1600\times 900" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">900</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">1600</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">900</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">1600\times 900</annotation></semantics></math> at 12Hz and the LiDAR frequency for scanning is 20Hz. The dataset provides object annotations every 0.5 seconds, and the annotations include 3D bounding boxes for 10 classes {Car, Truck, Bus, Trailer, Construction vehicle, Pedestrian, Motorcycle, Bicycle, Barrier, Traffic cone }. We follow the official split that uses 750, 150, and 150 sequences as training, validation, and testing sets respectively. So total we get 28130 batches of data for training, 6019 batches for validation, and 6008 batches for testing.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Metrics</span> We use mean Average Precision(mAP) and Nuscenes Detection Score(NDS) as our main evaluation metrics. We also adopt other officially released metrics concluding Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE), and Average Attribute Error (AAE). Note that NDS is a weighted sum of mAP and other metric scores.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Implementation Details</span>
We use BEVDepth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> as the baseline. The image backbone is ResNet-50 and the input image size is [256,704]. Following BEVDepth, image augmentation includes random cropping, random scaling, random flipping, and random rotation. The BEV feature generated by the model is also augmented by random scaling, random flipping, and random rotation. The base learning rate is 2e-4, and the batch size is 6 for each GPU. During training, we use 8 V100 GPU and the training takes 40 epochs. We decay the learning rate on epochs 23 and 33 with ratio <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\alpha=1e-7" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">α</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml"><mrow id="S4.SS1.p3.1.m1.1.1.3.2" xref="S4.SS1.p3.1.m1.1.1.3.2.cmml"><mn id="S4.SS1.p3.1.m1.1.1.3.2.2" xref="S4.SS1.p3.1.m1.1.1.3.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p3.1.m1.1.1.3.2.1" xref="S4.SS1.p3.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.SS1.p3.1.m1.1.1.3.2.3" xref="S4.SS1.p3.1.m1.1.1.3.2.3.cmml">e</mi></mrow><mo id="S4.SS1.p3.1.m1.1.1.3.1" xref="S4.SS1.p3.1.m1.1.1.3.1.cmml">−</mo><mn id="S4.SS1.p3.1.m1.1.1.3.3" xref="S4.SS1.p3.1.m1.1.1.3.3.cmml">7</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><eq id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></eq><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">𝛼</ci><apply id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3"><minus id="S4.SS1.p3.1.m1.1.1.3.1.cmml" xref="S4.SS1.p3.1.m1.1.1.3.1"></minus><apply id="S4.SS1.p3.1.m1.1.1.3.2.cmml" xref="S4.SS1.p3.1.m1.1.1.3.2"><times id="S4.SS1.p3.1.m1.1.1.3.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1.3.2.1"></times><cn type="integer" id="S4.SS1.p3.1.m1.1.1.3.2.2.cmml" xref="S4.SS1.p3.1.m1.1.1.3.2.2">1</cn><ci id="S4.SS1.p3.1.m1.1.1.3.2.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3.2.3">𝑒</ci></apply><cn type="integer" id="S4.SS1.p3.1.m1.1.1.3.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3.3">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\alpha=1e-7</annotation></semantics></math>. To conduct a fair comparison, all methods share these settings. Apart from BEVDepth, we also evaluate the proposed method on the BEVDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2212.01231/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_img_landscape" width="523" height="413" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">The visualization result of baseline and the SAN. The red box denotes the ground truth, and the green box is the prediction. In this case, our method gives a more accurate prediction, and gives two correct predictions of pedestrians in the yellow circles that do not have labels.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Results on nuScenes val set</span> We first evaluate our method on nuScenes val set. The baseline methods are BEVDet and BEVDepth. We report the results of BEVDepth under different height ranges [-5,3], [-4,2], and [-6,4]. The default height range of BEVDet and BEVDepth is [-5,3]. As can be seen from Tab. <a href="#S3.T2" title="Table 2 ‣ 3.2.2 Fusion of Slice Features ‣ 3.2 Slice Attention Module ‣ 3 Methods ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our method can improve the baseline method by 0.03 in NDS and all the evaluation metrics are also improved. To further evaluate our method, we conduct the experiments with CBGS strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, which will take much longer training time. As can be seen from Tab. <a href="#S3.T3" title="Table 3 ‣ 3.2.2 Fusion of Slice Features ‣ 3.2 Slice Attention Module ‣ 3 Methods ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our method can still improve performance even with the CBGS strategy. We conduct this experiment based on ResNet-50 in consideration of computation cost.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Results of different object classes</span> Since the motivation of our method is to handle different object classes with different heights. Therefore, we show the results of different object classes in Tab. <a href="#S3.T4" title="Table 4 ‣ 3.2.2 Fusion of Slice Features ‣ 3.2 Slice Attention Module ‣ 3 Methods ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We compare the mAP of the proposed SAN and baseline methods. For the results without CBGS strategy, SAN outperforms the baseline BEVDepth in each object class. The performance gain of traffic cone even reaches 0.038. For the results with CBGS, the SAN also shows significant improvement. For example, our method improves the baseline BEVDepth by 0.032 in bicycles and 0.023 in traffic cones. These results show that our method gives different attention to objects with different shapes.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Qualitative results</span> We show the qualitative results of the baselines and our method. As can be seen from Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.1 Experimental Details ‣ 4 Experiment ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the proposed SAN improves the performance of 3D object detection. In this figure, we compare the results of SAN and BEVDepth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. We also show the feature visualization in Fig. <a href="#S4.F7" title="Figure 7 ‣ 4.3 Ablation study ‣ 4 Experiment ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. As can be seen from this figure, the original BEV feature does not capture the top left object, while our method fuses the features of different slices. Therefore, the enhanced BEV feature successfully captures the top left object.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.3.2" class="ltx_text" style="font-size:90%;">Ablation Study of Global and Local Slices.</span></figcaption>
<table id="S4.T5.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.4.1.1" class="ltx_tr">
<th id="S4.T5.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">Local</th>
<th id="S4.T5.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">Global</th>
<th id="S4.T5.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">NDS</th>
<th id="S4.T5.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">mAP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.4.2.1" class="ltx_tr">
<th id="S4.T5.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:14.2pt;padding-right:14.2pt;">.</th>
<th id="S4.T5.4.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:14.2pt;padding-right:14.2pt;">.</th>
<td id="S4.T5.4.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:14.2pt;padding-right:14.2pt;">0.330</td>
<td id="S4.T5.4.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:14.2pt;padding-right:14.2pt;">0.296</td>
</tr>
<tr id="S4.T5.4.3.2" class="ltx_tr">
<th id="S4.T5.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:14.2pt;padding-right:14.2pt;">✓</th>
<th id="S4.T5.4.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:14.2pt;padding-right:14.2pt;">.</th>
<td id="S4.T5.4.3.2.3" class="ltx_td ltx_align_center" style="padding-left:14.2pt;padding-right:14.2pt;">0.351</td>
<td id="S4.T5.4.3.2.4" class="ltx_td ltx_align_center" style="padding-left:14.2pt;padding-right:14.2pt;">0.310</td>
</tr>
<tr id="S4.T5.4.4.3" class="ltx_tr">
<th id="S4.T5.4.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:14.2pt;padding-right:14.2pt;">.</th>
<th id="S4.T5.4.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:14.2pt;padding-right:14.2pt;">✓</th>
<td id="S4.T5.4.4.3.3" class="ltx_td ltx_align_center" style="padding-left:14.2pt;padding-right:14.2pt;">0.343</td>
<td id="S4.T5.4.4.3.4" class="ltx_td ltx_align_center" style="padding-left:14.2pt;padding-right:14.2pt;">0.307</td>
</tr>
<tr id="S4.T5.4.5.4" class="ltx_tr">
<th id="S4.T5.4.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:14.2pt;padding-right:14.2pt;">✓</th>
<th id="S4.T5.4.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:14.2pt;padding-right:14.2pt;">✓</th>
<td id="S4.T5.4.5.4.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:14.2pt;padding-right:14.2pt;"><span id="S4.T5.4.5.4.3.1" class="ltx_text ltx_font_bold">0.366</span></td>
<td id="S4.T5.4.5.4.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:14.2pt;padding-right:14.2pt;"><span id="S4.T5.4.5.4.4.1" class="ltx_text ltx_font_bold">0.310</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation study</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Global and Local Slices</span> Our method uses both the global and local slices to construct the BEV feature. The global slices aim to cover the large ranges of BEV height while the local slices aim to emphasize the informative heights. Therefore, we conduct an ablation study to evaluate the contributions of global and local slices. As shown in Tab. <a href="#S4.T5" title="Table 5 ‣ 4.2 Main Results ‣ 4 Experiment ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, both types contribute to performance improvement.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">LiDAR-Guided Sampling</span>
In this paper, we propose to use LiDAR-guided sampling strategy to obtain the local slices. Therefore, we conduct the ablation study to evaluate the contribution of this component. For a fair comparison, we all use the global slices. As can be seen from Tab. <a href="#S4.T6" title="Table 6 ‣ 4.3 Ablation study ‣ 4 Experiment ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the LiDAR-guided sampling strategy can improve the NDS of average local sampling by 0.07, demonstrating the effectiveness of the proposed sampling strategy.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.2.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.3.2" class="ltx_text" style="font-size:90%;">Ablation Study of LiDAR-Guided Sampling.</span></figcaption>
<table id="S4.T6.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.4.1.1" class="ltx_tr">
<th id="S4.T6.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">      Statistics Local</th>
<th id="S4.T6.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">      NDS</th>
<th id="S4.T6.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">      mAP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.4.2.1" class="ltx_tr">
<th id="S4.T6.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:17.1pt;padding-right:17.1pt;">      .</th>
<td id="S4.T6.4.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:17.1pt;padding-right:17.1pt;">      0.359</td>
<td id="S4.T6.4.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:17.1pt;padding-right:17.1pt;">      0.310</td>
</tr>
<tr id="S4.T6.4.3.2" class="ltx_tr">
<th id="S4.T6.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">      ✓</th>
<td id="S4.T6.4.3.2.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:17.1pt;padding-right:17.1pt;">      <span id="S4.T6.4.3.2.2.1" class="ltx_text ltx_font_bold">0.366</span></td>
<td id="S4.T6.4.3.2.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:17.1pt;padding-right:17.1pt;">      <span id="S4.T6.4.3.2.3.1" class="ltx_text ltx_font_bold">0.310</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.2.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S4.T7.3.2" class="ltx_text" style="font-size:90%;">Ablation Study of Fusion Strategy.</span></figcaption>
<table id="S4.T7.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.4.1.1" class="ltx_tr">
<th id="S4.T7.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:7.1pt;padding-right:7.1pt;">Method</th>
<th id="S4.T7.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:7.1pt;padding-right:7.1pt;">Voxel Range</th>
<th id="S4.T7.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:7.1pt;padding-right:7.1pt;">NDS ↑</th>
<th id="S4.T7.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:7.1pt;padding-right:7.1pt;">mAP ↑</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.4.2.1" class="ltx_tr">
<td id="S4.T7.4.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:7.1pt;padding-right:7.1pt;">SA-Mean</td>
<td id="S4.T7.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:7.1pt;padding-right:7.1pt;">local Only</td>
<td id="S4.T7.4.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:7.1pt;padding-right:7.1pt;">0.332</td>
<td id="S4.T7.4.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.1pt;padding-right:7.1pt;">0.296</td>
</tr>
<tr id="S4.T7.4.3.2" class="ltx_tr">
<td id="S4.T7.4.3.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">SA-SE</td>
<td id="S4.T7.4.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">local Only</td>
<td id="S4.T7.4.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">0.350</td>
<td id="S4.T7.4.3.2.4" class="ltx_td ltx_align_center" style="padding-left:7.1pt;padding-right:7.1pt;">0.298</td>
</tr>
<tr id="S4.T7.4.4.3" class="ltx_tr">
<td id="S4.T7.4.4.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">SA-SE-Mean</td>
<td id="S4.T7.4.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">local + Global</td>
<td id="S4.T7.4.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">0.359</td>
<td id="S4.T7.4.4.3.4" class="ltx_td ltx_align_center" style="padding-left:7.1pt;padding-right:7.1pt;">0.311</td>
</tr>
<tr id="S4.T7.4.5.4" class="ltx_tr">
<td id="S4.T7.4.5.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">SA-SE-SE</td>
<td id="S4.T7.4.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">local + Global</td>
<td id="S4.T7.4.5.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">0.361</td>
<td id="S4.T7.4.5.4.4" class="ltx_td ltx_align_center" style="padding-left:7.1pt;padding-right:7.1pt;">0.310</td>
</tr>
<tr id="S4.T7.4.6.5" class="ltx_tr">
<td id="S4.T7.4.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">SA-SE-Trans</td>
<td id="S4.T7.4.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">local + Global</td>
<td id="S4.T7.4.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:7.1pt;padding-right:7.1pt;">0.366</td>
<td id="S4.T7.4.6.5.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:7.1pt;padding-right:7.1pt;">0.310</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2212.01231/assets/figs/bevvis_e.png" id="S4.F7.g1" class="ltx_graphics ltx_img_landscape" width="685" height="251" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">The visualization result of the baseline BEV feature and SAN BEV feature. As can be seen, the features of different slices can capture different objects. For example, the original feature fails to capture the top-left object, while our enhanced feature successfully capture this object.</span></figcaption>
</figure>
<figure id="S4.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T8.2.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="S4.T8.3.2" class="ltx_text" style="font-size:90%;">Computational cost. We compare the proposed SANet with the baseline method BEVDepth with ResNet-50 and ResNet-101 as the backbones. As can be seen, our method will introduce some additional computational cost. However, this is because we simply repeat the LSS operation many times to generate the features of slices. Careful engineering optimization can significantly improve the efficiency.</span></figcaption>
<table id="S4.T8.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.4.1.1" class="ltx_tr">
<th id="S4.T8.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Method</th>
<th id="S4.T8.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Backbone</th>
<th id="S4.T8.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">NDS</th>
<th id="S4.T8.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">FPS</th>
<th id="S4.T8.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Model Size(MB)</th>
<th id="S4.T8.4.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Image backbone(ms)</th>
<th id="S4.T8.4.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">pooling(ms)</th>
<th id="S4.T8.4.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Fusion(ms)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.4.2.1" class="ltx_tr">
<td id="S4.T8.4.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">SANet</td>
<td id="S4.T8.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">R50</td>
<td id="S4.T8.4.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.366</td>
<td id="S4.T8.4.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">15.4</td>
<td id="S4.T8.4.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">911.0</td>
<td id="S4.T8.4.2.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.53</td>
<td id="S4.T8.4.2.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">23.12</td>
<td id="S4.T8.4.2.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">0.50</td>
</tr>
<tr id="S4.T8.4.3.2" class="ltx_tr">
<td id="S4.T8.4.3.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">SANet</td>
<td id="S4.T8.4.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">R101</td>
<td id="S4.T8.4.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">0.379</td>
<td id="S4.T8.4.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">14.3</td>
<td id="S4.T8.4.3.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">1128.7</td>
<td id="S4.T8.4.3.2.6" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.55</td>
<td id="S4.T8.4.3.2.7" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">26.22</td>
<td id="S4.T8.4.3.2.8" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.45</td>
</tr>
<tr id="S4.T8.4.4.3" class="ltx_tr">
<td id="S4.T8.4.4.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">BEVDepth</td>
<td id="S4.T8.4.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">R50</td>
<td id="S4.T8.4.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">0.330</td>
<td id="S4.T8.4.4.3.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">24.3</td>
<td id="S4.T8.4.4.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">870.0</td>
<td id="S4.T8.4.4.3.6" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">0.54</td>
<td id="S4.T8.4.4.3.7" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">26.23</td>
<td id="S4.T8.4.4.3.8" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">.</td>
</tr>
<tr id="S4.T8.4.5.4" class="ltx_tr">
<td id="S4.T8.4.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">BEVDepth</td>
<td id="S4.T8.4.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">R101</td>
<td id="S4.T8.4.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">0.371</td>
<td id="S4.T8.4.5.4.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">19.6</td>
<td id="S4.T8.4.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">1087.1</td>
<td id="S4.T8.4.5.4.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">0.55</td>
<td id="S4.T8.4.5.4.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">26.26</td>
<td id="S4.T8.4.5.4.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.7pt;padding-right:5.7pt;">.</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Fusion Strategy</span>
The fusion strategy also plays an important role in merging the local and global slices. In short, our fusion strategy contains two stages. The first stage merges the local and global slices respectively. The second stage fuses the merged local and global features for task heads. In this part, we evaluate the fusion strategy based on BEVDepth with ResNet-50. Mean denotes adding the BEV features together. SE denotes the Squeeze-and-Excitation Attention residual block. Trans means the designed two branches transformer. As can be seen in Tab. <a href="#S4.T7" title="Table 7 ‣ 4.3 Ablation study ‣ 4 Experiment ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Using SE in the first stage and Trans in the second stage achieves the best performance compared with the alternatives. Nevertheless, all the fusion strategies can achieve considerable improvements compared with the baseline, demonstrating the effectiveness of the proposed SAN.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Computational Cost</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In this section, we report the computational cost of SAN. As shown in Tab. <a href="#S4.T8" title="Table 8 ‣ 4.3 Ablation study ‣ 4 Experiment ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, our method introduces additional computational and storage cost to the baseline methods. To be more specific, when the backbone is ResNet-101, our method introduces 41 MB storage cost and 27% slower than the BEVDepth baseline. The most time-consuming step is building the features of global and local slices. However, this is because our current implementation simply repeats the LSS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> operations. More careful engineering optimization can help to reduce the computational cost of SAN, which will be our future work.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Although the proposed SAN is simple yet effective, our method still has some limitations. One limitation is the additional computational and storage cost as mentioned above. However, we believe careful engineering optimization can solve this problem. Besides, our method follows the BEVDepth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> pipeline, which is sensitive to the accuracy of depth values or the depth distributions. How to apply SAN to baseline methods such as BEVFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is still a problem, which will also be our future work.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In summary, we propose a novel method named Slice Attention Network for BEV 3D object detection in this paper. Instead of summing up the frustum features that fall into the same flattened BEV grid, our method explores the benefit of different heights in BEV space. We extract the BEV features of global and local slices. The global slices aim at covering the large height ranges while the local slices aims at emphasizing informative local height ranges. To improve the performance, we propose to sample the local slices based on the histogram of LiDAR points along the height dimension. The features of local and global slices are fused by a two-stage strategy for task heads. We use BEVDepth as the baseline method and conduct detailed experiments to demonstrate the effectiveness of BEV-SAN.</p>
</div>
<figure id="S6.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T9.2.1.1" class="ltx_text" style="font-size:90%;">Table 9</span>: </span><span id="S6.T9.3.2" class="ltx_text" style="font-size:90%;">Comparisons of Generalization ability with different methods on the validation set of unseen environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The unseen environment includes night-time and rainy data. All methods utilize ResNet 50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> as backbone. </span></figcaption>
<table id="S6.T9.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T9.4.1.1" class="ltx_tr">
<th id="S6.T9.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">Test on</th>
<th id="S6.T9.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">Method</th>
<th id="S6.T9.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">Backbone</th>
<th id="S6.T9.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="background-color:#BFBFBF;padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.1.1.4.1" class="ltx_text" style="background-color:#BFBFBF;">NDS ↑</span></th>
<th id="S6.T9.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#BFBFBF;padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.1.1.5.1" class="ltx_text" style="background-color:#BFBFBF;">mAP ↑</span></th>
<th id="S6.T9.4.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">mATE ↓</th>
<th id="S6.T9.4.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">mASE ↓</th>
<th id="S6.T9.4.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">mAOE ↓</th>
<th id="S6.T9.4.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">mAVE ↓</th>
<th id="S6.T9.4.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">mAAE ↓</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T9.4.2.1" class="ltx_tr">
<th id="S6.T9.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.3pt;padding-right:2.3pt;" rowspan="2"><span id="S6.T9.4.2.1.1.1" class="ltx_text">Night</span></th>
<th id="S6.T9.4.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.3pt;padding-right:2.3pt;">BEVDepth<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<th id="S6.T9.4.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.3pt;padding-right:2.3pt;">R50</th>
<th id="S6.T9.4.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.3pt;padding-right:2.3pt;">0.170</th>
<td id="S6.T9.4.2.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.3pt;padding-right:2.3pt;">0.124</td>
<td id="S6.T9.4.2.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.3pt;padding-right:2.3pt;">0.847</td>
<td id="S6.T9.4.2.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.3pt;padding-right:2.3pt;">0.463</td>
<td id="S6.T9.4.2.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.3pt;padding-right:2.3pt;">0.906</td>
<td id="S6.T9.4.2.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.3pt;padding-right:2.3pt;">1.855</td>
<td id="S6.T9.4.2.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.3pt;padding-right:2.3pt;">0.696</td>
</tr>
<tr id="S6.T9.4.3.2" class="ltx_tr">
<th id="S6.T9.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">BEV-SAN</th>
<th id="S6.T9.4.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">R50</th>
<th id="S6.T9.4.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.3.2.3.1" class="ltx_text ltx_font_bold">0.210</span></th>
<td id="S6.T9.4.3.2.4" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.3.2.4.1" class="ltx_text ltx_font_bold">0.129</span></td>
<td id="S6.T9.4.3.2.5" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.3.2.5.1" class="ltx_text ltx_font_bold">0.827</span></td>
<td id="S6.T9.4.3.2.6" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.3.2.6.1" class="ltx_text ltx_font_bold">0.466</span></td>
<td id="S6.T9.4.3.2.7" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.3.2.7.1" class="ltx_text ltx_font_bold">0.670</span></td>
<td id="S6.T9.4.3.2.8" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.3.2.8.1" class="ltx_text ltx_font_bold">1.655</span></td>
<td id="S6.T9.4.3.2.9" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.3.2.9.1" class="ltx_text ltx_font_bold">0.584</span></td>
</tr>
<tr id="S6.T9.4.4.3" class="ltx_tr">
<th id="S6.T9.4.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;" rowspan="2"><span id="S6.T9.4.4.3.1.1" class="ltx_text">Rainy</span></th>
<th id="S6.T9.4.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">BEVDepth<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<th id="S6.T9.4.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">R50</th>
<th id="S6.T9.4.4.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.363</th>
<td id="S6.T9.4.4.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.305</td>
<td id="S6.T9.4.4.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.722</td>
<td id="S6.T9.4.4.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.298</td>
<td id="S6.T9.4.4.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.662</td>
<td id="S6.T9.4.4.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.915</td>
<td id="S6.T9.4.4.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.289</td>
</tr>
<tr id="S6.T9.4.5.4" class="ltx_tr">
<th id="S6.T9.4.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">BEV-SAN</th>
<th id="S6.T9.4.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">R50</th>
<th id="S6.T9.4.5.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.5.4.3.1" class="ltx_text ltx_font_bold">0.396</span></th>
<td id="S6.T9.4.5.4.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.5.4.4.1" class="ltx_text ltx_font_bold">0.314</span></td>
<td id="S6.T9.4.5.4.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.5.4.5.1" class="ltx_text ltx_font_bold">0.711</span></td>
<td id="S6.T9.4.5.4.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.5.4.6.1" class="ltx_text ltx_font_bold">0.296</span></td>
<td id="S6.T9.4.5.4.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.5.4.7.1" class="ltx_text ltx_font_bold">0.629</span></td>
<td id="S6.T9.4.5.4.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.5.4.8.1" class="ltx_text ltx_font_bold">0.664</span></td>
<td id="S6.T9.4.5.4.9" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S6.T9.4.5.4.9.1" class="ltx_text ltx_font_bold">0.242</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S6.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T10.2.1.1" class="ltx_text" style="font-size:90%;">Table 10</span>: </span><span id="S6.T10.3.2" class="ltx_text" style="font-size:90%;">Comparisons of the Robustness ability with different methods on the validation set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. We design a special experiment setting in which one camera breaks down or is occluded. And we occlude the front-view images in inference time.</span></figcaption>
<table id="S6.T10.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T10.4.1.1" class="ltx_tr">
<th id="S6.T10.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      Occlude</th>
<th id="S6.T10.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      Method</th>
<th id="S6.T10.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      Backbone</th>
<th id="S6.T10.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="background-color:#BFBFBF;padding-left:17.4pt;padding-right:17.4pt;">      <span id="S6.T10.4.1.1.4.1" class="ltx_text" style="background-color:#BFBFBF;"> NDS ↑</span>
</th>
<th id="S6.T10.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#BFBFBF;padding-left:17.4pt;padding-right:17.4pt;">      <span id="S6.T10.4.1.1.5.1" class="ltx_text" style="background-color:#BFBFBF;"> mAP ↑</span>
</th>
</tr>
<tr id="S6.T10.4.2.2" class="ltx_tr">
<th id="S6.T10.4.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:17.4pt;padding-right:17.4pt;"></th>
<th id="S6.T10.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:17.4pt;padding-right:17.4pt;">      BEVDepth<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></th>
<th id="S6.T10.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:17.4pt;padding-right:17.4pt;">      R50</th>
<th id="S6.T10.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:17.4pt;padding-right:17.4pt;">      0.336</th>
<th id="S6.T10.4.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:17.4pt;padding-right:17.4pt;">      0.296</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T10.4.3.1" class="ltx_tr">
<td id="S6.T10.4.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;" rowspan="2">      <span id="S6.T10.4.3.1.1.1" class="ltx_text">Front</span></td>
<td id="S6.T10.4.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      BEVDepth<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></td>
<td id="S6.T10.4.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      R50</td>
<td id="S6.T10.4.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      0.318</td>
<td id="S6.T10.4.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      0.228</td>
</tr>
<tr id="S6.T10.4.4.2" class="ltx_tr">
<td id="S6.T10.4.4.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:17.4pt;padding-right:17.4pt;">      Ours(BEVDepth)</td>
<td id="S6.T10.4.4.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:17.4pt;padding-right:17.4pt;">      R50</td>
<td id="S6.T10.4.4.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:17.4pt;padding-right:17.4pt;">      <span id="S6.T10.4.4.2.3.1" class="ltx_text ltx_font_bold">0.325</span></td>
<td id="S6.T10.4.4.2.4" class="ltx_td ltx_align_center" style="padding-left:17.4pt;padding-right:17.4pt;">      <span id="S6.T10.4.4.2.4.1" class="ltx_text ltx_font_bold">0.258</span></td>
</tr>
<tr id="S6.T10.4.5.3" class="ltx_tr">
<td id="S6.T10.4.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;" rowspan="2">      <span id="S6.T10.4.5.3.1.1" class="ltx_text">Front-Left</span></td>
<td id="S6.T10.4.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      BEVDepth<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></td>
<td id="S6.T10.4.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      R50</td>
<td id="S6.T10.4.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      0.331</td>
<td id="S6.T10.4.5.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      0.265</td>
</tr>
<tr id="S6.T10.4.6.4" class="ltx_tr">
<td id="S6.T10.4.6.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:17.4pt;padding-right:17.4pt;">      Ours(BEVDepth)</td>
<td id="S6.T10.4.6.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:17.4pt;padding-right:17.4pt;">      R50</td>
<td id="S6.T10.4.6.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:17.4pt;padding-right:17.4pt;">      <span id="S6.T10.4.6.4.3.1" class="ltx_text ltx_font_bold">0.332</span></td>
<td id="S6.T10.4.6.4.4" class="ltx_td ltx_align_center" style="padding-left:17.4pt;padding-right:17.4pt;">      <span id="S6.T10.4.6.4.4.1" class="ltx_text ltx_font_bold">0.279</span></td>
</tr>
<tr id="S6.T10.4.7.5" class="ltx_tr">
<td id="S6.T10.4.7.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;" rowspan="2">      <span id="S6.T10.4.7.5.1.1" class="ltx_text">Front-Right</span></td>
<td id="S6.T10.4.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      BEVDepth<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></td>
<td id="S6.T10.4.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      R50</td>
<td id="S6.T10.4.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      0.326</td>
<td id="S6.T10.4.7.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:17.4pt;padding-right:17.4pt;">      0.242</td>
</tr>
<tr id="S6.T10.4.8.6" class="ltx_tr">
<td id="S6.T10.4.8.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:17.4pt;padding-right:17.4pt;">      Ours(BEVDepth)</td>
<td id="S6.T10.4.8.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:17.4pt;padding-right:17.4pt;">      R50</td>
<td id="S6.T10.4.8.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:17.4pt;padding-right:17.4pt;">      <span id="S6.T10.4.8.6.3.1" class="ltx_text ltx_font_bold">0.330</span></td>
<td id="S6.T10.4.8.6.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:17.4pt;padding-right:17.4pt;">      <span id="S6.T10.4.8.6.4.1" class="ltx_text ltx_font_bold">0.271</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Appendix</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In the supplementary material, we first present additional related work of transformer network in Sec .<a href="#S7.SS1" title="7.1 Additional related works ‣ 7 Appendix ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.1</span></a> since we utilize dual-branch transformer module to fuse the global and local slices. In Sec .<a href="#S7.SS2" title="7.2 Additional implementation details ‣ 7 Appendix ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.2</span></a>, we then provide additional and detailed cross domain training strategy. In Sec .<a href="#S7.SS3" title="7.3 Additional generalization exploration ‣ 7 Appendix ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.3</span></a>, we explore the generalization ability of our proposed BEV-SAN by evaluating the performance on unseen and challenging data distribution. In Sec .<a href="#S7.SS4" title="7.4 Additional robustness exploration ‣ 7 Appendix ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.4</span></a>, we demonstrate the robustness of our method by comparing with baseline methods when encountering cameras malfunctioning.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Additional related works</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p"><span id="S7.SS1.p1.1.1" class="ltx_text ltx_font_bold">Vision transformer.</span>
Transformer network was first introduced for neural machine translation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and the encoder and decoder of transformer leverage self-attention mechanism to extract better feature representation and reserve contextual information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> first brings a transferring in backbone architectures for computer vision, which is transferred from CNNs to Transformers.
This seminal work has led to subsequent research that aims to improve its utility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
Meanwhile, Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is a practical backbone for various image recognition tasks, which adopts the inductive biases of locality, hierarchy and translation invariance. DeiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> focuses on improving the efficiency and practicality of transformer network, it proposes several training strategies that allows ViT to be effective when training on smaller image datasets. In this paper, we introduce a dual branches transformer block to fuse global an local-level BEV slices and generate the fused BEV feature map for task heads.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Additional implementation details</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.2" class="ltx_p">Our training process can be regarded as an end-to-end training. Firstly, in order to fully leverage the feature extraction ability of the model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, we load the backbone of ImageNet pretrained parameters. Then we train the model with slice-attention module for 28 epochs with <span id="S7.SS2.p1.2.1" class="ltx_text ltx_font_bold">CBGS</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and 40 epochs without. It should be noted that we freeze the backbone starting from epoch 23 and fine-tune the slice-attention module and detection head in the rest of the epochs. We adopt <math id="S7.SS2.p1.1.m1.1" class="ltx_Math" alttext="256\times 704" display="inline"><semantics id="S7.SS2.p1.1.m1.1a"><mrow id="S7.SS2.p1.1.m1.1.1" xref="S7.SS2.p1.1.m1.1.1.cmml"><mn id="S7.SS2.p1.1.m1.1.1.2" xref="S7.SS2.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS2.p1.1.m1.1.1.1" xref="S7.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S7.SS2.p1.1.m1.1.1.3" xref="S7.SS2.p1.1.m1.1.1.3.cmml">704</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.1.m1.1b"><apply id="S7.SS2.p1.1.m1.1.1.cmml" xref="S7.SS2.p1.1.m1.1.1"><times id="S7.SS2.p1.1.m1.1.1.1.cmml" xref="S7.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S7.SS2.p1.1.m1.1.1.2.cmml" xref="S7.SS2.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S7.SS2.p1.1.m1.1.1.3.cmml" xref="S7.SS2.p1.1.m1.1.1.3">704</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.1.m1.1c">256\times 704</annotation></semantics></math> as image input size and the same data augmentation methods as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. We apply AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> optimizer with 2e-4 learning rate. We decay the learning rate on epochs 19, 23, and 33 with ratio <math id="S7.SS2.p1.2.m2.1" class="ltx_Math" alttext="\alpha=1e-7" display="inline"><semantics id="S7.SS2.p1.2.m2.1a"><mrow id="S7.SS2.p1.2.m2.1.1" xref="S7.SS2.p1.2.m2.1.1.cmml"><mi id="S7.SS2.p1.2.m2.1.1.2" xref="S7.SS2.p1.2.m2.1.1.2.cmml">α</mi><mo id="S7.SS2.p1.2.m2.1.1.1" xref="S7.SS2.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S7.SS2.p1.2.m2.1.1.3" xref="S7.SS2.p1.2.m2.1.1.3.cmml"><mrow id="S7.SS2.p1.2.m2.1.1.3.2" xref="S7.SS2.p1.2.m2.1.1.3.2.cmml"><mn id="S7.SS2.p1.2.m2.1.1.3.2.2" xref="S7.SS2.p1.2.m2.1.1.3.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S7.SS2.p1.2.m2.1.1.3.2.1" xref="S7.SS2.p1.2.m2.1.1.3.2.1.cmml">​</mo><mi id="S7.SS2.p1.2.m2.1.1.3.2.3" xref="S7.SS2.p1.2.m2.1.1.3.2.3.cmml">e</mi></mrow><mo id="S7.SS2.p1.2.m2.1.1.3.1" xref="S7.SS2.p1.2.m2.1.1.3.1.cmml">−</mo><mn id="S7.SS2.p1.2.m2.1.1.3.3" xref="S7.SS2.p1.2.m2.1.1.3.3.cmml">7</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.2.m2.1b"><apply id="S7.SS2.p1.2.m2.1.1.cmml" xref="S7.SS2.p1.2.m2.1.1"><eq id="S7.SS2.p1.2.m2.1.1.1.cmml" xref="S7.SS2.p1.2.m2.1.1.1"></eq><ci id="S7.SS2.p1.2.m2.1.1.2.cmml" xref="S7.SS2.p1.2.m2.1.1.2">𝛼</ci><apply id="S7.SS2.p1.2.m2.1.1.3.cmml" xref="S7.SS2.p1.2.m2.1.1.3"><minus id="S7.SS2.p1.2.m2.1.1.3.1.cmml" xref="S7.SS2.p1.2.m2.1.1.3.1"></minus><apply id="S7.SS2.p1.2.m2.1.1.3.2.cmml" xref="S7.SS2.p1.2.m2.1.1.3.2"><times id="S7.SS2.p1.2.m2.1.1.3.2.1.cmml" xref="S7.SS2.p1.2.m2.1.1.3.2.1"></times><cn type="integer" id="S7.SS2.p1.2.m2.1.1.3.2.2.cmml" xref="S7.SS2.p1.2.m2.1.1.3.2.2">1</cn><ci id="S7.SS2.p1.2.m2.1.1.3.2.3.cmml" xref="S7.SS2.p1.2.m2.1.1.3.2.3">𝑒</ci></apply><cn type="integer" id="S7.SS2.p1.2.m2.1.1.3.3.cmml" xref="S7.SS2.p1.2.m2.1.1.3.3">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.2.m2.1c">\alpha=1e-7</annotation></semantics></math>. As for further detailed image augmentation process, we follow BEVDepth and adopt random cropping, random scaling, random flipping, and random rotation. The BEV feature generated by the model is also augmented by random scaling, random flipping, and random rotation. All experiments are conducted on NVIDIA Tesla V100 GPUs.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Additional generalization exploration</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">Slice-attention module leverages the attention mechanism of Transformer to fuse the features from different global information to construct a more comprehensive BEV feature. Therefore, BEV-SAN is of better generalization ability in more display scenarios after integrating multiple levels of information. We conduct further experiments on some particular scenarios like rainy and night in NuSences dataset to demonstrate the superiority generalization ability of BEV-SAN.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">As shown in Tab. <a href="#S6.T9" title="Table 9 ‣ 6 Conclusion ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, the baseline can only achieve 0.170 and 0.124 in NDS and mAP, respectively on the night validation set. Due to the faint light condition at night, the camera based method will encounter great challenges. However, we observe that BEV-SAN shows satisfying performance under such severe condition with 0.210 NDS and 0.129 mAP, respectively. As for rainy validation set, we notice that BEV-SAN also outperforms the baseline with significant margin by over 3% in NDS. These results verify the generalization ability of BEV-SAN.</p>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Additional robustness exploration</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.1" class="ltx_p">Though there are lots of recent works on autonomous driving systems, only a few of them <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> explore the robustness of the proposed methods. LSS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> presents the performance under extrinsic noises and camera dropout at test time. Following previous work, we aim to give a qualitative analysis of our method under camera missing condition.
Camera image missing occurs when one camera breaks down or is occluded. Multi-view images provide panoramic visual information, yet it can also face the condition when one of them is absent in the real-world. Therefore, it is necessary to evaluate the robustness of our method when encountering camera view missing.</p>
</div>
<div id="S7.SS4.p2" class="ltx_para">
<p id="S7.SS4.p2.1" class="ltx_p">As shown in Tab. <a href="#S6.T10" title="Table 10 ‣ 6 Conclusion ‣ BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, among six
cameras of nuScenes dataset, front-view data are the most important, and their absence leads
to a drop of 1.8% NDS and 6.8% mAP on BEVDepth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. In term of our proposed method, front-view camera missing only leads to a drop of 1.1% NDS and 3.8% mAP, which demonstrates that BEV-SAN has a great potential on robustness. For other view missing, the results show similar tendency.
<span id="S7.SS4.p2.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Garrick Brazil and Xiaoming Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">M3d-rpn: Monocular 3d region proposal network for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 9287–9296, 2019.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">nuscenes: A multimodal dataset for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 11621–11631, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Shaoyu Chen, Tianheng Cheng, Xinggang Wang, Wenming Meng, Qian Zhang, and Wenyu
Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Efficient and robust 2d-to-bev representation learning via
geometry-guided kernel transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.04584</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Chang Huang, and Wenyu
Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Polar parametrization for vision-based surround-view 3d detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.10965</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Multi-view 3d object detection network for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 1907–1915, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Yongjian Chen, Lei Tai, Kai Sun, and Mingyang Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Monopair: Monocular 3d object detection using pairwise spatial
relationships.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 12093–12102, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.11929</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Fast r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 1440–1448, 2015.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Rich feature hierarchies for accurate object detection and semantic
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 580–587, 2014.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 2961–2969, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Jie Hu, Li Shen, and Gang Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Squeeze-and-excitation networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 7132–7141, 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Junjie Huang and Guan Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Bevdet4d: Exploit temporal cues in multi-camera 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.17054</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Junjie Huang, Guan Huang, Zheng Zhu, and Dalong Du.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Bevdet: High-performance multi-camera 3d object detection in
bird-eye-view.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.11790</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Peixuan Li, Huaici Zhao, Pengfei Liu, and Feidao Cao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Rtm3d: Real-time monocular 3d detection from object keypoints for
autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 644–660.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yinhao Li, Han Bao, Zheng Ge, Jinrong Yang, Jianjian Sun, and Zeming Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Bevstereo: Enhancing depth estimation in multi-view 3d object
detection with dynamic temporal stereo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2209.10248</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran Wang, Yukang Shi,
Jianjian Sun, and Zeming Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Bevdepth: Acquisition of reliable depth for multi-view 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.10092</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu,
and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Bevformer: Learning bird’s-eye-view representation from multi-camera
images via spatiotemporal transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.17270</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">A structured self-attentive sentence embedding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1703.03130</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Petr: Position embedding transformation for multi-view 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.05625</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted
windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 10012–10022, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Video swin transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 3202–3211, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Zechen Liu, Zizhang Wu, and Roland Tóth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Smoke: Single-stage monocular 3d object detection via keypoint
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 996–997, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Decoupled weight decay regularization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1711.05101</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Yan Lu, Xinzhu Ma, Lei Yang, Tianzhu Zhang, Yating Liu, Qi Chu, Junjie Yan, and
Wanli Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Geometry uncertainty projection network for monocular 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 3111–3121, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Xinzhu Ma, Yinmin Zhang, Dan Xu, Dongzhan Zhou, Shuai Yi, Haojie Li, and Wanli
Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Delving into localization errors for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 4721–4730, 2021.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Arsalan Mousavian, Dragomir Anguelov, John Flynn, and Jana Kosecka.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">3d bounding box estimation using deep learning and geometry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 7074–7082, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Ankur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">A decomposable attention model for natural language inference.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.01933</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Jonah Philion and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Lift, splat, shoot: Encoding images from arbitrary camera rigs by
implicitly unprojecting to 3d.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 194–210.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Deep hough voting for 3d object detection in point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 9277–9286, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">You only look once: Unified, real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 779–788, 2016.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 28, 2015.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and
Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 10529–10538, 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
Sablayrolles, and Hervé Jégou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Training data-efficient image transformers &amp; distillation through
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages
10347–10357. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Tai Wang, ZHU Xinge, Jiangmiao Pang, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Probabilistic and geometric depth: Detecting objects in perspective.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Robot Learning</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 1475–1485. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Fcos3d: Fully convolutional one-stage monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 913–922, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao,
and Justin Solomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Detr3d: 3d object detection from multi-view images via 3d-to-2d
queries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Robot Learning</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 180–191. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Zengran Wang, Chen Min, Zheng Ge, Yinhao Li, Zeming Li, Hongyu Yang, and Di
Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Sts: Surround-view temporal stereo for multi-view 3d detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2208.10145</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Runsheng Xu, Zhengzhong Tu, Hao Xiang, Wei Shao, Bolei Zhou, and Jiaqi Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Cobevt: Cooperative bird’s eye view semantic segmentation with sparse
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.02202</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">3dssd: Point-based 3d single stage object detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, pages 11040–11048, 2020.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Xuanzhuo Xu, Yu Qiao, Peng Gao, and
Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Monodetr: Depth-guided transformer for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.13310</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Yunpeng Zhang, Jiwen Lu, and Jie Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Objects are different: Flexible monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 3289–3298, 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Class-balanced grouping and sampling for point cloud 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.09492</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.01229" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.01231" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.01231">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.01231" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.01232" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 08:15:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
