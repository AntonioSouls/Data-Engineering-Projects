<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2302.02858] TR3D: Towards Real-Time Indoor 3D Object Detection</title><meta property="og:description" content="Recently, sparse 3D convolutions have changed 3D object detection. Performing on par with the voting-based approaches, 3D CNNs are memory-efficient and scale to large scenes better. However, there is still room for imp…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TR3D: Towards Real-Time Indoor 3D Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="TR3D: Towards Real-Time Indoor 3D Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2302.02858">

<!--Generated on Fri Mar  1 03:06:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">TR3D: Towards Real-Time Indoor 3D Object Detection</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recently, sparse 3D convolutions have changed 3D object detection. Performing on par with the voting-based approaches, 3D CNNs are memory-efficient and scale to large scenes better. However, there is still room for improvement. With a conscious, practice-oriented approach to problem-solving, we analyze the performance of such methods and localize the weaknesses. Applying modifications that resolve the found issues one by one, we end up with TR3D: a fast fully-convolutional 3D object detection model trained end-to-end, that achieves state-of-the-art results on the standard benchmarks, ScanNet v2, SUN RGB-D, and S3DIS. Moreover, to take advantage of both point cloud and RGB inputs, we introduce an early fusion of 2D and 3D features. We employ our fusion module to make conventional 3D object detection methods multimodal and demonstrate an impressive boost in performance. Our model with early feature fusion, which we refer to as TR3D+FF, outperforms existing 3D object detection approaches on the SUN RGB-D dataset. Overall, besides being accurate, both TR3D and TR3D+FF models are lightweight, memory-efficient, and fast, thereby marking another milestone on the way toward real-time 3D object detection. Code is available at <a target="_blank" href="https://github.com/SamsungLabs/tr3d" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/SamsungLabs/tr3d</a>.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
3D object detection, indoor scene understanding, point clouds</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The recent emergence of self-driving, AR/VR applications, 3D modeling, and household robotics attracted attention to 3D object detection as a core scene understanding technology.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Modern 3D object detection methods can be categorized into voting-based, transformer-based, and 3D convolutional. Voting-based methods process points with a feature extractor network, use center votes to create an object proposal, and accumulate point features within each group. Many voting-based methods have poor scalability, limiting their usage.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Instead of domain-specific heuristics and hyperparameters, transformer-based methods use end-to-end learning and forward pass on inference. Being more generalized, they still have issues when processing larger scenes.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">3D convolutional methods represent point clouds as voxels, which allows processing sparse 3D data efficiently. Dense volumetric features require a lot of memory, so sparse representation and sparse 3D convolutions are used instead. Compared to other methods, 3D sparse convolutional methods are memory-efficient and scale to large scenes well without sacrificing point density. Up until very recently, such methods lacked accuracy, yet due to the recent advances in the field, fast and scalable yet accurate methods were developed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Overall, modern 3D detection methods demonstrate impressive results with only geometric inputs; yet, possibilities of leveraging data of other modalities for 3D object detection have been investigated as well. While point clouds are difficult to obtain and require additional equipment, RGB cameras are much more accessible. Typically being incorporated into capturing devices, they provide cheap, easy-to-use yet extremely informative data.
Existing approaches add RGB data in late stages; either they have a complicated, memory-consuming architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, rely on custom procedures limiting their usage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, or run slow iterative schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. On the contrary, we present an <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">early fusion strategy</span>, which can also be integrated into other point cloud-based models. Combined with the proposed simple yet efficient fully-convolutional pipeline, this strategy allows achieving state-of-the-art results in 3D object detection from point cloud and RGB data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">3D object detection in point clouds.</span> Voting-based methods pioneered the field, with VoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> being the first method that introduced point voting for 3D object detection. VoteNet extracts features from 3D points, assigns a group of points to each object candidate according to their voted center, and computes object features from each point group. BRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> refines voting results with the representative points from the vote centers, which improves capturing the fine local structural features.
H3DNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> improves the point group generation procedure by predicting a hybrid set of geometric primitives.
RBGNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> proposes a ray-based feature grouping module, which aggregates the point-wise features on object surfaces by uniformly emitting rays from cluster centers.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Transformer-based methods.</span> In transformer-based methods, grouping is not guided with a set of hyperparameters explicitly, which makes them less domain-specific. GroupFree <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> employs a transformer module to update object query locations iteratively and accumulate intermediate results. 3DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> was the first to solve the 3D object detection task with a transformer model.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Voxel-based methods.</span> Voxel-based 3D object detection methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> convert points into voxels and process them with 3D convolutional networks. However, dense volumetric features still consume much memory. GSDN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> alleviates this issue using sparse 3D convolutions, yet being notably less accurate than modern voting-based methods. Unlike GSDN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, FCAF3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> does not utilize anchors, and addresses 3D object detection in a data-driven manner, making it the first voxel-based approach that performs on par with state-of-the-art voting-based methods. In our work, we use FCAF3D as a strong baseline.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Point cloud and RGB fusion.</span>
Semantic data contained in images might provide additional clues for 3D object detection.
Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> generate initial region proposals and then refine them, using RGB features as guidance. ImVoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> leverages 2D detection results to perform voting.
EPNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> fuses image features with intermediate outputs of a feature extraction model.
MMTC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> uses cascades, including a 2D segmentation network between the first and second stages of a 3D object detection network. TokenFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> fuses point cloud and RGB information with a transformer model and learns to replace uninformative tokens with projected and aggregated inter-modal features.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">TR3D is based on FCAF3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and inherits its simple, fully-convolutional design (Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.1 TR3D: 3D Object Detection Method ‣ 3 Proposed Method ‣ TR3D: Towards Real-Time Indoor 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), yet with some updates improving its efficacy and accuracy. Moreover, we propose an early feature fusion for 3D object detection from RGB and point clouds, and incorporate a fusion module into TR3D to construct a multimodal TR3D+FF model.
</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>TR3D: 3D Object Detection Method</h3>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2302.02858/assets/x1.png" id="S3.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="362" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.3.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Scheme of TR3D in comparison with the FCAF3D baseline.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Considering FCAF3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> as a baseline, we introduce several modifications. TR3D is a result of all these modifications being applied jointly (Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.1 TR3D: 3D Object Detection Method ‣ 3 Proposed Method ‣ TR3D: Towards Real-Time Indoor 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). To ensure these modifications contribute to the final performance in the desired way, we apply them gradually, one at a time, and estimate the detection accuracy on S3DIS, memory footprint, and inference speed (hereinafter denoted as FPS, it is actually the number of scenes processed per second). We report the results of this series of experiments in Tab. <a href="#S3.T1" title="Table 1 ‣ 3.1 TR3D: 3D Object Detection Method ‣ 3 Proposed Method ‣ TR3D: Towards Real-Time Indoor 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.6" class="ltx_p"><span id="S3.SS1.p2.6.1" class="ltx_text ltx_font_bold">Efficacy.</span> First, we aim to turn the baseline model into a fast and lightweight one. The examination of performance revealed that a single generative transposed convolutional layer in the head at the first level consumes as much as one-third of the total memory. Accordingly, we drop the head on the first level: not only it has a major impact on the memory footprint, which decreases <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="1.5" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn type="float" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">1.5</annotation></semantics></math> times from <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="661" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">661</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn type="integer" id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">661</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">661</annotation></semantics></math> to <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="415" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mn id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">415</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><cn type="integer" id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">415</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">415</annotation></semantics></math> Mb, but also adds <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="+6" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mo id="S3.SS1.p2.4.m4.1.1a" xref="S3.SS1.p2.4.m4.1.1.cmml">+</mo><mn id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><plus id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"></plus><cn type="integer" id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">+6</annotation></semantics></math> FPS. Without this head, the pruning layer appears to be redundant, so we omit it. Then, we remove the head at the fourth level, since it focuses on processing large objects, which are rare in indoor scenes. Furthermore, if restricting the number of output channels in the backbone blocks, the number of parameters gets reduced dramatically from <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="68.3" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mn id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">68.3</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><cn type="float" id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">68.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">68.3</annotation></semantics></math> to <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="14.7" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mn id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">14.7</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><cn type="float" id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">14.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">14.7</annotation></semantics></math> M, and the memory consumption halves.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2302.02858/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="450" height="83" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>Our early feature fusion strategy and existing RGB and point cloud fusion approaches: ImVoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, EPNet++<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, MMTC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. With its simple and straightforward design, it appears to be more beneficial in terms of detection accuracy.
</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:230.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(47.2pt,-25.0pt) scale(1.27830196211703,1.27830196211703) ;">
<table id="S3.T1.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.2.1.1.1" class="ltx_tr">
<th id="S3.T1.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S3.T1.2.1.1.1.1.1" class="ltx_text">Modification</span></th>
<th id="S3.T1.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S3.T1.2.1.1.1.2.1" class="ltx_text">mAP</span></th>
<th id="S3.T1.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S3.T1.2.1.1.1.3.1" class="ltx_text">FPS</span></th>
<th id="S3.T1.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">#Params,</th>
<th id="S3.T1.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Memory,</th>
</tr>
<tr id="S3.T1.2.1.2.2" class="ltx_tr">
<th id="S3.T1.2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.0pt;padding-right:3.0pt;">M</th>
<th id="S3.T1.2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.0pt;padding-right:3.0pt;">Mb</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.2.1.3.1" class="ltx_tr">
<td id="S3.T1.2.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S3.T1.2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">66.7 (64.9)</td>
<td id="S3.T1.2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6FF;padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.3.1.3.1" class="ltx_text" style="background-color:#E6E6FF;">10.9</span></td>
<td id="S3.T1.2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">70.5</td>
<td id="S3.T1.2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6FF;padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.3.1.5.1" class="ltx_text" style="background-color:#E6E6FF;">661</span></td>
</tr>
<tr id="S3.T1.2.1.4.2" class="ltx_tr">
<td id="S3.T1.2.1.4.2.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">- head at 1st level</td>
<td id="S3.T1.2.1.4.2.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">62.9 (61.3)</td>
<td id="S3.T1.2.1.4.2.3" class="ltx_td ltx_align_center" style="background-color:#E6E6FF;padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.4.2.3.1" class="ltx_text" style="background-color:#E6E6FF;">16.9</span></td>
<td id="S3.T1.2.1.4.2.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">70.1</td>
<td id="S3.T1.2.1.4.2.5" class="ltx_td ltx_align_center" style="background-color:#E6E6FF;padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.4.2.5.1" class="ltx_text" style="background-color:#E6E6FF;">415</span></td>
</tr>
<tr id="S3.T1.2.1.5.3" class="ltx_tr">
<td id="S3.T1.2.1.5.3.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">- pruning</td>
<td id="S3.T1.2.1.5.3.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">62.9 (61.3)</td>
<td id="S3.T1.2.1.5.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">17.3</td>
<td id="S3.T1.2.1.5.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">70.1</td>
<td id="S3.T1.2.1.5.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">415</td>
</tr>
<tr id="S3.T1.2.1.6.4" class="ltx_tr">
<td id="S3.T1.2.1.6.4.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">- head at 4th level</td>
<td id="S3.T1.2.1.6.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">61.5 (59.6)</td>
<td id="S3.T1.2.1.6.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">17.3</td>
<td id="S3.T1.2.1.6.4.4" class="ltx_td ltx_align_center" style="background-color:#E6E6FF;padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.6.4.4.1" class="ltx_text" style="background-color:#E6E6FF;">68.3</span></td>
<td id="S3.T1.2.1.6.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">408</td>
</tr>
<tr id="S3.T1.2.1.7.5" class="ltx_tr">
<td id="S3.T1.2.1.7.5.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">- &gt;128 channels</td>
<td id="S3.T1.2.1.7.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">61.2 (58.8)</td>
<td id="S3.T1.2.1.7.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">20.8</td>
<td id="S3.T1.2.1.7.5.4" class="ltx_td ltx_align_center" style="background-color:#E6E6FF;padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.7.5.4.1" class="ltx_text" style="background-color:#E6E6FF;">14.7</span></td>
<td id="S3.T1.2.1.7.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">207</td>
</tr>
<tr id="S3.T1.2.1.8.6" class="ltx_tr">
<td id="S3.T1.2.1.8.6.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">- centerness</td>
<td id="S3.T1.2.1.8.6.2" class="ltx_td ltx_align_center" style="background-color:#E6E6FF;padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.8.6.2.1" class="ltx_text" style="background-color:#E6E6FF;">61.5 (59.8)</span></td>
<td id="S3.T1.2.1.8.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">21.0</td>
<td id="S3.T1.2.1.8.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14.7</td>
<td id="S3.T1.2.1.8.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">207</td>
</tr>
<tr id="S3.T1.2.1.9.7" class="ltx_tr">
<td id="S3.T1.2.1.9.7.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">+ TR3D assigner</td>
<td id="S3.T1.2.1.9.7.2" class="ltx_td ltx_align_center" style="background-color:#E6E6FF;padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.9.7.2.1" class="ltx_text" style="background-color:#E6E6FF;">72.9 (71.4)</span></td>
<td id="S3.T1.2.1.9.7.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">21.0</td>
<td id="S3.T1.2.1.9.7.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14.7</td>
<td id="S3.T1.2.1.9.7.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">207</td>
</tr>
<tr id="S3.T1.2.1.10.8" class="ltx_tr">
<td id="S3.T1.2.1.10.8.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">+ DIoU loss</td>
<td id="S3.T1.2.1.10.8.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.10.8.2.1" class="ltx_text ltx_font_bold">74.5 (72.1)</span></td>
<td id="S3.T1.2.1.10.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.10.8.3.1" class="ltx_text ltx_font_bold">21.0</span></td>
<td id="S3.T1.2.1.10.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.10.8.4.1" class="ltx_text ltx_font_bold">14.7</span></td>
<td id="S3.T1.2.1.10.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.10.8.5.1" class="ltx_text ltx_font_bold">207</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.3.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Results of a study of the TR3D components on S3DIS. Blue cells mark the most significant gains. Our approach has a 3x smaller memory footprint, has 4.5x fewer parameters, and is almost 2x faster on inference.</figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.6" class="ltx_p"><span id="S3.SS1.p3.6.1" class="ltx_text ltx_font_bold">Accuracy.</span> In the second round of improvements, we mainly focus on detection accuracy. Our experiments demonstrate that centerness does not contribute to the prediction quality, and can be harmlessly skipped. FCAF3D assigner considers only points inside 3D bounding boxes. Accordingly, this imposes a risk of missing thin or small objects (e.g., whiteboards), which might fall between locations and hence not get assigned with ground truth boxes. So, we introduce a TR3D assigner considering not inside points but the nearest ones, which might be located outside bounding boxes as well. Moreover, we pre-define the head level for each object category: typically large objects (e.g., <span id="S3.SS1.p3.6.2" class="ltx_text ltx_font_italic">bed</span> or <span id="S3.SS1.p3.6.3" class="ltx_text ltx_font_italic">sofa</span>) are processed at the third level, and smaller ones (e.g., <span id="S3.SS1.p3.6.4" class="ltx_text ltx_font_italic">chair</span> or <span id="S3.SS1.p3.6.5" class="ltx_text ltx_font_italic">night stand</span>) are handled at the second. Switching to a new multi-level assigner boosts the performance from <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="61.5" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mn id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">61.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><cn type="float" id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">61.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">61.5</annotation></semantics></math> to <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="72.9" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mn id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">72.9</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><cn type="float" id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">72.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">72.9</annotation></semantics></math> mAP. With a novel assigner, the assigned points might be outside the ground truth bounding box. Accordingly, IoU might be equal to zero, so, to enforce the training process, we replace IoU loss with DIoU loss, which resolves such cases successfully. This final update allows achieving <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="74.5" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mn id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">74.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><cn type="float" id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">74.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">74.5</annotation></semantics></math> mAP at <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="21" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mn id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">21</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><cn type="integer" id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">21</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">21</annotation></semantics></math> FPS, with a lightweight model with <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="14.7" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mn id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">14.7</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><cn type="float" id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">14.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">14.7</annotation></semantics></math>M parameters and a peak memory consumption of <math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="207" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><mn id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml">207</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><cn type="integer" id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">207</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">207</annotation></semantics></math> Mb. Overall, TR3D consumes 3x less memory than the baseline, has 4.5x fewer parameters, and requires 2x less time to proceed.
</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>TR3D+FF: RGB and Point Cloud Fusion</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">TR3D performs early 2D-3D feature fusion. First, RGB images are processed with a frozen ResNet50+FPN network pretrained to solve a 2D object detection task. Then, extracted 2D features are projected into 3D space, same as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Finally, the fusion is performed by summing up projected 2D features with 3D features element-wise. This strategy is notably more simple, time- and memory-efficient, than existing approaches (Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.1 TR3D: 3D Object Detection Method ‣ 3 Proposed Method ‣ TR3D: Towards Real-Time Indoor 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>); surprisingly, it ensures better results.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We incorporate our fusion module into VoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and TR3D, and evaluate the performance in point cloud-based and multimodal setting. In addition, we compare VoteNet+FF with ImVoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> implementing another feature fusion strategy on top of VoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. According to the metrics provided in Tab. <a href="#S3.T2" title="Table 2 ‣ 3.2 TR3D+FF: RGB and Point Cloud Fusion ‣ 3 Proposed Method ‣ TR3D: Towards Real-Time Indoor 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our early feature fusion boosts the detection accuracy of VoteNet by +6.8 mAP@0.25, while ImVoteNet demonstrates a smaller gain of 5.7 mAP@0.25. At the same time, the detection accuracy of TR3D is getting improved by +2.3 mAP@0.25 and +3.0 mAP@0.5 with feature fusion. This slight yet steady improvement evidences geometric data being the major source of information, while visual data is complementary, serving as guidance to alter the estimates.
</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:140.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(50.1pt,-16.2pt) scale(1.30080337690382,1.30080337690382) ;">
<table id="S3.T2.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.2.1.1.1" class="ltx_tr">
<th id="S3.T2.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Method</th>
<td id="S3.T2.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Inputs</td>
<td id="S3.T2.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">mAP@0.25</td>
<td id="S3.T2.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">mAP@0.5</td>
</tr>
<tr id="S3.T2.2.1.2.2" class="ltx_tr">
<th id="S3.T2.2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VoteNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<td id="S3.T2.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">PC</td>
<td id="S3.T2.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">57.7</td>
<td id="S3.T2.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T2.2.1.3.3" class="ltx_tr">
<th id="S3.T2.2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ImVoteNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</th>
<td id="S3.T2.2.1.3.3.2" class="ltx_td ltx_align_center">PC+RGB</td>
<td id="S3.T2.2.1.3.3.3" class="ltx_td ltx_align_center">63.4</td>
<td id="S3.T2.2.1.3.3.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.2.1.4.4" class="ltx_tr">
<th id="S3.T2.2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VoteNet+FF, ours</th>
<td id="S3.T2.2.1.4.4.2" class="ltx_td ltx_align_center">PC+RGB</td>
<td id="S3.T2.2.1.4.4.3" class="ltx_td ltx_align_center">64.5 (63.7)</td>
<td id="S3.T2.2.1.4.4.4" class="ltx_td ltx_align_center">39.2 (38.1)</td>
</tr>
<tr id="S3.T2.2.1.5.5" class="ltx_tr">
<th id="S3.T2.2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">TR3D, ours</th>
<td id="S3.T2.2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">PC</td>
<td id="S3.T2.2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">67.1 (66.3)</td>
<td id="S3.T2.2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">50.4 (49.6)</td>
</tr>
<tr id="S3.T2.2.1.6.6" class="ltx_tr">
<th id="S3.T2.2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">TR3D+FF, ours</th>
<td id="S3.T2.2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_bb">PC+RGB</td>
<td id="S3.T2.2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S3.T2.2.1.6.6.3.1" class="ltx_text ltx_font_bold">69.4</span> (68.7)</td>
<td id="S3.T2.2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S3.T2.2.1.6.6.4.1" class="ltx_text ltx_font_bold">53.4</span> (52.4)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.3.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Results of point cloud-based 3D object detection methods against multimodal methods with our early fusion, on SUN RGB-D.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Settings</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Datasets.</span> The experiments are conducted on SUN RGB-D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, ScanNet v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and S3DIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
Richly-annotated ScanNet v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> contains 1513 reconstructed scans: 1201 comprise the training subset and 312 are left for validation. We calculate axis-aligned bounding boxes of 18 object categories from semantic per-point annotation, as proposed in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. SUN RGB-D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> is a monocular dataset with 10 355 RGB-D images. The training and validation subsets contain 5285 and 5050 point clouds, respectively, with annotated oriented bounding boxes. We consider the 10 most common object categories for evaluation, as proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. S3DIS  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> features 271 scenes within 6 large areas. Following the standard evaluation protocol, we assess detection accuracy on scans from Area 5, using 5 semantic categories.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Metrics.</span> For all datasets, we use mean average precision (mAP) under IoU thresholds of 0.25 and 0.5 as a metric. To eliminate outliers caused by randomness and to obtain statistically significant results, we train our models five times and evaluate each trained model five times independently. For a fair comparison with existing methods that follow the same protocol, we report both the best and the average value (in brackets) across 25 trials for each metric.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:422.8pt;height:145.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-103.0pt,35.4pt) scale(0.672419052261039,0.672419052261039) ;">
<table id="S4.T3.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.2.1.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1.1" class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;" rowspan="2"><span id="S4.T3.2.1.1.1.1.1" class="ltx_text">Method</span></th>
<th id="S4.T3.2.1.1.1.2" class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;" rowspan="2"><span id="S4.T3.2.1.1.1.2.1" class="ltx_text">Presented at</span></th>
<td id="S4.T3.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;" colspan="3">ScanNet</td>
<td id="S4.T3.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;" colspan="3">SUN RGB-D</td>
<td id="S4.T3.2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;" colspan="3">S3DIS</td>
</tr>
<tr id="S4.T3.2.1.2.2" class="ltx_tr">
<td id="S4.T3.2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">mAP@0.25</td>
<td id="S4.T3.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">mAP@0.5</td>
<td id="S4.T3.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">FPS</td>
<td id="S4.T3.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">mAP@0.25</td>
<td id="S4.T3.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">mAP@0.5</td>
<td id="S4.T3.2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">FPS</td>
<td id="S4.T3.2.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">mAP@0.25</td>
<td id="S4.T3.2.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">mAP@0.5</td>
<td id="S4.T3.2.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">FPS</td>
</tr>
<tr id="S4.T3.2.1.3.3" class="ltx_tr">
<th id="S4.T3.2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">VoteNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<th id="S4.T3.2.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">ICCV’19</th>
<td id="S4.T3.2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">58.6</td>
<td id="S4.T3.2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">33.5</td>
<td id="S4.T3.2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">14.1</td>
<td id="S4.T3.2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">57.7</td>
<td id="S4.T3.2.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">24.9</td>
<td id="S4.T3.2.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
</tr>
<tr id="S4.T3.2.1.4.4" class="ltx_tr">
<th id="S4.T3.2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">GSDN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
<th id="S4.T3.2.1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">ECCV’20</th>
<td id="S4.T3.2.1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">62.8</td>
<td id="S4.T3.2.1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">34.8</td>
<td id="S4.T3.2.1.4.4.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.4.4.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.4.4.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.4.4.8" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.4.4.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">47.8</td>
<td id="S4.T3.2.1.4.4.10" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">25.1</td>
<td id="S4.T3.2.1.4.4.11" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
</tr>
<tr id="S4.T3.2.1.5.5" class="ltx_tr">
<th id="S4.T3.2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">BRNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<th id="S4.T3.2.1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">CVPR’21</th>
<td id="S4.T3.2.1.5.5.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">66.1</td>
<td id="S4.T3.2.1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">50.9</td>
<td id="S4.T3.2.1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">&lt;14</td>
<td id="S4.T3.2.1.5.5.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">61.1</td>
<td id="S4.T3.2.1.5.5.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">43.7</td>
<td id="S4.T3.2.1.5.5.8" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">&lt;24</td>
<td id="S4.T3.2.1.5.5.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.5.5.10" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.5.5.11" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
</tr>
<tr id="S4.T3.2.1.6.6" class="ltx_tr">
<th id="S4.T3.2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">3DETR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<th id="S4.T3.2.1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">ICCV’21</th>
<td id="S4.T3.2.1.6.6.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">65.0</td>
<td id="S4.T3.2.1.6.6.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">47.0</td>
<td id="S4.T3.2.1.6.6.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">&lt;7</td>
<td id="S4.T3.2.1.6.6.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">59.1</td>
<td id="S4.T3.2.1.6.6.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">32.7</td>
<td id="S4.T3.2.1.6.6.8" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">&lt;12</td>
<td id="S4.T3.2.1.6.6.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.6.6.10" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.6.6.11" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
</tr>
<tr id="S4.T3.2.1.7.7" class="ltx_tr">
<th id="S4.T3.2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">H3DNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<th id="S4.T3.2.1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">ECCV’20</th>
<td id="S4.T3.2.1.7.7.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">67.2</td>
<td id="S4.T3.2.1.7.7.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">48.1</td>
<td id="S4.T3.2.1.7.7.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">7.9</td>
<td id="S4.T3.2.1.7.7.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">60.1</td>
<td id="S4.T3.2.1.7.7.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">39.0</td>
<td id="S4.T3.2.1.7.7.8" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">&lt;24</td>
<td id="S4.T3.2.1.7.7.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.7.7.10" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.7.7.11" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
</tr>
<tr id="S4.T3.2.1.8.8" class="ltx_tr">
<th id="S4.T3.2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">GroupFree <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<th id="S4.T3.2.1.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">ICCV’21</th>
<td id="S4.T3.2.1.8.8.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">69.1 (68.6)</td>
<td id="S4.T3.2.1.8.8.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">52.8 (51.8)</td>
<td id="S4.T3.2.1.8.8.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">8.3</td>
<td id="S4.T3.2.1.8.8.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">63.0 (62.6)</td>
<td id="S4.T3.2.1.8.8.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">45.2 (44.4)</td>
<td id="S4.T3.2.1.8.8.8" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">&lt;24</td>
<td id="S4.T3.2.1.8.8.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.8.8.10" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.8.8.11" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
</tr>
<tr id="S4.T3.2.1.9.9" class="ltx_tr">
<th id="S4.T3.2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">RBGNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<th id="S4.T3.2.1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">CVPR’22</th>
<td id="S4.T3.2.1.9.9.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">70.6 (69.9)</td>
<td id="S4.T3.2.1.9.9.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">55.2 (54.7)</td>
<td id="S4.T3.2.1.9.9.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">&lt;14</td>
<td id="S4.T3.2.1.9.9.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">64.1 (63.6)</td>
<td id="S4.T3.2.1.9.9.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">47.2 (46.3)</td>
<td id="S4.T3.2.1.9.9.8" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">&lt;24</td>
<td id="S4.T3.2.1.9.9.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.9.9.10" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.9.9.11" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
</tr>
<tr id="S4.T3.2.1.10.10" class="ltx_tr">
<th id="S4.T3.2.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">HyperDet3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<th id="S4.T3.2.1.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">CVPR’22</th>
<td id="S4.T3.2.1.10.10.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">70.9</td>
<td id="S4.T3.2.1.10.10.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">57.2</td>
<td id="S4.T3.2.1.10.10.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">&lt;14</td>
<td id="S4.T3.2.1.10.10.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">63.5</td>
<td id="S4.T3.2.1.10.10.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">47.3</td>
<td id="S4.T3.2.1.10.10.8" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">&lt;24</td>
<td id="S4.T3.2.1.10.10.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.10.10.10" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td id="S4.T3.2.1.10.10.11" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
</tr>
<tr id="S4.T3.2.1.11.11" class="ltx_tr">
<th id="S4.T3.2.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">FCAF3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<th id="S4.T3.2.1.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">ECCV’22</th>
<td id="S4.T3.2.1.11.11.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">71.5 (70.7)</td>
<td id="S4.T3.2.1.11.11.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">57.3 (56.0)</td>
<td id="S4.T3.2.1.11.11.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">15.7</td>
<td id="S4.T3.2.1.11.11.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">64.2 (63.8)</td>
<td id="S4.T3.2.1.11.11.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">48.9 (48.2)</td>
<td id="S4.T3.2.1.11.11.8" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">17.9</td>
<td id="S4.T3.2.1.11.11.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">66.7 (64.9)</td>
<td id="S4.T3.2.1.11.11.10" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">45.9 (43.8)</td>
<td id="S4.T3.2.1.11.11.11" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">10.9</td>
</tr>
<tr id="S4.T3.2.1.12.12" class="ltx_tr">
<th id="S4.T3.2.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S4.T3.2.1.12.12.1.1" class="ltx_text ltx_font_bold">TR3D, ours</span></th>
<th id="S4.T3.2.1.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">-</th>
<td id="S4.T3.2.1.12.12.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="S4.T3.2.1.12.12.3.1" class="ltx_text ltx_font_bold">72.9</span> (72.0)</td>
<td id="S4.T3.2.1.12.12.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="S4.T3.2.1.12.12.4.1" class="ltx_text ltx_font_bold">59.3</span> (57.4)</td>
<td id="S4.T3.2.1.12.12.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S4.T3.2.1.12.12.5.1" class="ltx_text ltx_font_bold">23.7</span></td>
<td id="S4.T3.2.1.12.12.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="S4.T3.2.1.12.12.6.1" class="ltx_text ltx_font_bold">67.1</span> (66.3)</td>
<td id="S4.T3.2.1.12.12.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="S4.T3.2.1.12.12.7.1" class="ltx_text ltx_font_bold">50.4</span> (49.6)</td>
<td id="S4.T3.2.1.12.12.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S4.T3.2.1.12.12.8.1" class="ltx_text ltx_font_bold">27.5</span></td>
<td id="S4.T3.2.1.12.12.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="S4.T3.2.1.12.12.9.1" class="ltx_text ltx_font_bold">74.5</span> (72.1)</td>
<td id="S4.T3.2.1.12.12.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="S4.T3.2.1.12.12.10.1" class="ltx_text ltx_font_bold">51.7</span> (47.6)</td>
<td id="S4.T3.2.1.12.12.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S4.T3.2.1.12.12.11.1" class="ltx_text ltx_font_bold">21.0</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>Results of TR3D and existing 3D object detection methods based on geometric inputs only. TR3D outperforms the strong baseline FCAF3D in all benchmarks.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Implementation details.</span>
Our models are implemented using a mmdetection3d framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and trained and tested on a single NVidia 4090 GPU.
We follow the training procedure of FCAF3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, using the same losses, optimizer, learning schedule, and augmentations.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison to Prior Work</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Point cloud-based 3D object detection.</span> We report quantitative results of our TR3D on ScanNet v2, SUN RGB-D, and S3DIS in Tab. <a href="#S4.T3" title="Table 3 ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ TR3D: Towards Real-Time Indoor 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. As can be observed, TR3D demonstrates a solid superiority in all benchmarks, in terms of all metrics. The most notable accuracy gain is achieved for S3DIS, where the difference is as large as +7.8 mAP@0.25 and +5.8 mAP@0.5. Overall, we claim our approach to set a new state-of-the-art in indoor 3D object detection based on only geometric inputs.
</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Multimodal 3D object detection.</span> The results of multimodal methods on SUN RGB-D, including TR3D+FF, are listed in Tab. <a href="#S4.T3" title="Table 3 ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ TR3D: Towards Real-Time Indoor 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. TR3D+FF surpasses previous state-of-the-art MMTC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> by 4.1 mAP@0.25 and 4.8 mAP@0.5.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:144.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(28.2pt,-9.4pt) scale(1.14951601776027,1.14951601776027) ;">
<table id="S4.T4.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.2.1.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Method</th>
<th id="S4.T4.2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Presented at</th>
<td id="S4.T4.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">mAP@0.25</td>
<td id="S4.T4.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">mAP@0.5</td>
<td id="S4.T4.2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">FPS</td>
</tr>
<tr id="S4.T4.2.1.2.2" class="ltx_tr">
<th id="S4.T4.2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">ImVoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</th>
<th id="S4.T4.2.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">CVPR’20</th>
<td id="S4.T4.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">63.4</td>
<td id="S4.T4.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S4.T4.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">14.8</td>
</tr>
<tr id="S4.T4.2.1.3.3" class="ltx_tr">
<th id="S4.T4.2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">EPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</th>
<th id="S4.T4.2.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">ECCV’20</th>
<td id="S4.T4.2.1.3.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">64.6</td>
<td id="S4.T4.2.1.3.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S4.T4.2.1.3.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
</tr>
<tr id="S4.T4.2.1.4.4" class="ltx_tr">
<th id="S4.T4.2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">TokenFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</th>
<th id="S4.T4.2.1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">CVPR’22</th>
<td id="S4.T4.2.1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">64.9 (64.4)</td>
<td id="S4.T4.2.1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">48.3 (47.7)</td>
<td id="S4.T4.2.1.4.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
</tr>
<tr id="S4.T4.2.1.5.5" class="ltx_tr">
<th id="S4.T4.2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">EPNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</th>
<th id="S4.T4.2.1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">TPAMI’22</th>
<td id="S4.T4.2.1.5.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">65.3</td>
<td id="S4.T4.2.1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S4.T4.2.1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
</tr>
<tr id="S4.T4.2.1.6.6" class="ltx_tr">
<th id="S4.T4.2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">MMTC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<th id="S4.T4.2.1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">BMVC’21</th>
<td id="S4.T4.2.1.6.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">65.3 (64.7)</td>
<td id="S4.T4.2.1.6.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">48.6 (48.2)</td>
<td id="S4.T4.2.1.6.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">&lt;7</td>
</tr>
<tr id="S4.T4.2.1.7.7" class="ltx_tr">
<th id="S4.T4.2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T4.2.1.7.7.1.1" class="ltx_text ltx_font_bold">TR3D+FF, ours</span></th>
<th id="S4.T4.2.1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">-</th>
<td id="S4.T4.2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T4.2.1.7.7.3.1" class="ltx_text ltx_font_bold">69.4</span> (68.7)</td>
<td id="S4.T4.2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T4.2.1.7.7.4.1" class="ltx_text ltx_font_bold">53.4</span> (52.4)</td>
<td id="S4.T4.2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T4.2.1.7.7.5.1" class="ltx_text ltx_font_bold">17.5</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>Results of TR3D+FF and existing 3D object detection methods using both point clouds and RGB, on SUN RGB-D. TR3D+FF is a state-of-the-art in multimodal 3D object detection.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Qualitative Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The input point clouds, ground truth and predicted bounding boxes are depicted in Fig. <a href="#S4.F3" title="Figure 3 ‣ 4.3 Qualitative Results ‣ 4 Experiments ‣ TR3D: Towards Real-Time Indoor 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.SS3.6" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.SS3.6.6" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.SS3.6.6.7.1" class="ltx_tr">
<td id="S4.SS3.6.6.7.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
<th id="S4.SS3.6.6.7.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:1.0pt;padding-right:1.0pt;">Ground truth</th>
<th id="S4.SS3.6.6.7.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:1.0pt;padding-right:1.0pt;">Predicted</th>
</tr>
<tr id="S4.SS3.2.2.2" class="ltx_tr">
<th id="S4.SS3.2.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_row" style="padding-left:1.0pt;padding-right:1.0pt;">
<div id="S4.SS3.2.2.2.3.1" class="ltx_inline-block ltx_parbox ltx_align_top ltx_transformed_outer" style="width:5.7pt;height:54.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:54.4pt;transform:translate(-23.76pt,-23.76pt) rotate(-90deg) ;">
<p id="S4.SS3.2.2.2.3.1.1" class="ltx_p">Scannet</p>
</span></div>
</th>
<td id="S4.SS3.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><img src="/html/2302.02858/assets/images/scannet/scene0488_01_gt.png" id="S4.SS3.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="284" height="172" alt="[Uncaptioned image]"></td>
<td id="S4.SS3.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><img src="/html/2302.02858/assets/images/scannet/scene0488_01_pred.png" id="S4.SS3.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="284" height="172" alt="[Uncaptioned image]"></td>
</tr>
<tr id="S4.SS3.4.4.4" class="ltx_tr">
<th id="S4.SS3.4.4.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_row" style="padding-left:1.0pt;padding-right:1.0pt;">
<div id="S4.SS3.4.4.4.3.1" class="ltx_inline-block ltx_parbox ltx_align_top ltx_transformed_outer" style="width:5.7pt;height:65.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:65.4pt;transform:translate(-29.29pt,-29.29pt) rotate(-90deg) ;">
<p id="S4.SS3.4.4.4.3.1.1" class="ltx_p">SUN RGB-D</p>
</span></div>
</th>
<td id="S4.SS3.3.3.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><img src="/html/2302.02858/assets/images/sunrgbd/000967_gt.png" id="S4.SS3.3.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="299" height="181" alt="[Uncaptioned image]"></td>
<td id="S4.SS3.4.4.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><img src="/html/2302.02858/assets/images/sunrgbd/000967_pred.png" id="S4.SS3.4.4.4.2.g1" class="ltx_graphics ltx_img_landscape" width="299" height="181" alt="[Uncaptioned image]"></td>
</tr>
<tr id="S4.SS3.6.6.6" class="ltx_tr">
<th id="S4.SS3.6.6.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_row" style="padding-left:1.0pt;padding-right:1.0pt;">
<div id="S4.SS3.6.6.6.3.1" class="ltx_inline-block ltx_parbox ltx_align_top ltx_transformed_outer" style="width:5.7pt;height:44.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:44.4pt;transform:translate(-18.8pt,-18.8pt) rotate(-90deg) ;">
<p id="S4.SS3.6.6.6.3.1.1" class="ltx_p">S3DIS</p>
</span></div>
</th>
<td id="S4.SS3.5.5.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><img src="/html/2302.02858/assets/images/s3dis/Area_5_office_10_gt.png" id="S4.SS3.5.5.5.1.g1" class="ltx_graphics ltx_img_landscape" width="284" height="172" alt="[Uncaptioned image]"></td>
<td id="S4.SS3.6.6.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><img src="/html/2302.02858/assets/images/s3dis/Area_5_office_10_pred.png" id="S4.SS3.6.6.6.2.g1" class="ltx_graphics ltx_img_landscape" width="284" height="172" alt="[Uncaptioned image]"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F3" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text ltx_font_bold">Fig. 3</span>: </span><span id="S4.F3.4.2" class="ltx_text ltx_font_bold">Figure 2</span>: Ground truth objects and objects detected by our TR3D in the ScanNet, SUN RGB-D, and S3DIS point clouds.</figcaption>
</figure>
</div>
</div>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we introduced TR3D, a novel 3D object detection method. Moreover, we developed an early fusion strategy to incorporate visual features into a 3D processing pipeline and proposed a modification of TR3D called TR3D+FF, that leverages both point cloud and RGB inputs. We evaluated the proposed methods on the standard benchmarks: ScanNet v2, SUN RGB-D, and S3DIS. Through experiments, we demonstrated that TR3D outperforms existing methods in both accuracy and speed, thereby setting a new state-of-the-art in point cloud-based 3D object detection, while TR3D+FF achieves the best results among methods using RGB and point clouds.
</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Danila Rukhovich, Anna Vorontsova, and Anton Konushin,

</span>
<span class="ltx_bibblock">“Fcaf3d: fully convolutional anchor-free 3d object detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE/CVF European Conference on Computer Vision (ECCV)</span>.
Springer, 2022, pp. 477–493.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Danila Rukhovich, Anna Vorontsova, and Anton Konushin,

</span>
<span class="ltx_bibblock">“Imvoxelnet: Image to voxels projection for monocular and multi-view
general-purpose 3d object detection,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv</span>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jinhyung Park, Xinshuo Weng, Yunze Man, and Kris Kitani,

</span>
<span class="ltx_bibblock">“Multi-modality task cascade for 3d object detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">British Machine Vision Conference (BMVC)</span>, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, and Yunhe Wang,

</span>
<span class="ltx_bibblock">“Multimodal token fusion for vision transformers,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2022, pp. 12186–12195.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas,

</span>
<span class="ltx_bibblock">“Deep hough voting for 3d object detection in point clouds,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2019, pp. 9277–9286.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bowen Cheng, Lu Sheng, Shaoshuai Shi, Ming Yang, and Dong Xu,

</span>
<span class="ltx_bibblock">“Back-tracing representative points for voting-based 3d object
detection in point clouds,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2021, pp. 8963–8972.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang,

</span>
<span class="ltx_bibblock">“H3dnet: 3d object detection using hybrid geometric primitives,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE/CVF European Conference on Computer Vision (ECCV)</span>,
2020, pp. 311–329.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Haiyang Wang, Shaoshuai Shi, Ze Yang, Rongyao Fang, Qi Qian, Hongsheng Li,
Bernt Schiele, and Liwei Wang,

</span>
<span class="ltx_bibblock">“Rbgnet: Ray-based grouping for 3d object detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2022, pp. 1110–1119.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong,

</span>
<span class="ltx_bibblock">“Group-free 3d object detection via transformers,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2021, pp. 2949–2958.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ishan Misra, Rohit Girdhar, and Armand Joulin,

</span>
<span class="ltx_bibblock">“An end-to-end transformer model for 3d object detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">IEEE/CVF International Conference on Computer Vision (ICCV)</span>,
2021, pp. 2906–2917.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ji Hou, Angela Dai, and Matthias Nießner,

</span>
<span class="ltx_bibblock">“3d-sis: 3d semantic instance segmentation of rgb-d scans,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2019, pp. 4421–4430.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
JunYoung Gwak, Christopher Choy, and Silvio Savarese,

</span>
<span class="ltx_bibblock">“Generative sparse detection networks for 3d single-shot object
detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE/CVF European Conference on Computer Vision (ECCV)</span>.
Springer, 2020, pp. 297–313.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Charles R Qi, Xinlei Chen, Or Litany, and Leonidas J Guibas,

</span>
<span class="ltx_bibblock">“Imvotenet: Boosting 3d object detection in point clouds with image
votes,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2020, pp. 4404–4413.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Tengteng Huang, Zhe Liu, Xiwu Chen, and Xiang Bai,

</span>
<span class="ltx_bibblock">“Epnet: Enhancing point features with image semantics for 3d object
detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">IEEE/CVF European Conference on Computer Vision (ECCV)</span>,
Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, Eds.,
2020, pp. 35–52.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Zhe Liu, Tengteng Huang, Binglin Li, Xiwu Chen, Xi Wang, and Xiang Bai,

</span>
<span class="ltx_bibblock">“Epnet++: Cascade bi-directional fusion for multi-modal 3d object
detection,”

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv</span>, vol. abs/2112.11088, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao,

</span>
<span class="ltx_bibblock">“Sun rgb-d: A rgb-d scene understanding benchmark suite,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2015, pp. 567–576.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser,
and Matthias Nießner,

</span>
<span class="ltx_bibblock">“Scannet: Richly-annotated 3d reconstructions of indoor scenes,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2017.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioannis Brilakis, Martin
Fischer, and Silvio Savarese,

</span>
<span class="ltx_bibblock">“3d semantic parsing of large-scale indoor spaces,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2016, pp. 1534–1543.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Yu Zheng, Yueqi Duan, Jiwen Lu, Jie Zhou, and Qi Tian,

</span>
<span class="ltx_bibblock">“Hyperdet3d: Learning a scene-conditioned 3d object detector,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2022, pp. 5575–5584.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
MMDetection3D Contributors,

</span>
<span class="ltx_bibblock">“MMDetection3D: OpenMMLab next-generation platform for general
3D object detection,” <a target="_blank" href="https://github.com/open-mmlab/mmdetection3d" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/open-mmlab/mmdetection3d</a>,
2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2302.02857" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2302.02858" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2302.02858">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2302.02858" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2302.02859" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 03:06:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
