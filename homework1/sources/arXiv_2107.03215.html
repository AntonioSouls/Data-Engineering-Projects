<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2107.03215] FasterPose: A Faster Simple Baseline for Human Pose Estimation</title><meta property="og:description" content="The performance of human pose estimation depends on the spatial accuracy of keypoint localization.
Most existing methods pursue the spatial accuracy through learning the high-resolution (HR) representation from input iâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FasterPose: A Faster Simple Baseline for Human Pose Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FasterPose: A Faster Simple Baseline for Human Pose Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2107.03215">

<!--Generated on Tue Mar 19 12:22:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="pose estimation,  keypoint detection">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">FasterPose: A Faster Simple Baseline for Human Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hanbin Dai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:daihanbin.ac@gmail.com">daihanbin.ac@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">JD AI Research</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hailin Shi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:shihailin@jd.com">shihailin@jd.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">JD AI Research</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wu Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:liuwu1@jd.com">liuwu1@jd.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">JD AI Research</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Linfang Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wanglinfang@jd.com">wanglinfang@jd.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id10.1.id1" class="ltx_text ltx_affiliation_institution">JD AI Research</span><span id="id11.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id12.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yinglu Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:liuyinglu1@jd.com">liuyinglu1@jd.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">JD AI Research</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tao Mei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:tmei@live.com">tmei@live.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id16.1.id1" class="ltx_text ltx_affiliation_institution">JD AI Research</span><span id="id17.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id18.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
</div>
<div class="ltx_dates">(2021)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id19.id1" class="ltx_p">The performance of human pose estimation depends on the spatial accuracy of keypoint localization.
Most existing methods pursue the spatial accuracy through learning the high-resolution (HR) representation from input images. By the experimental analysis, we find that the HR representation leads to a sharp increase of computational cost, while the accuracy improvement remains marginal compared with the low-resolution (LR) representation.
In this paper, we propose a design paradigm for cost-effective network with LR representation for efficient pose estimation, named FasterPose.
Whereas the LR design largely shrinks the model complexity, yet how to effectively train the network with respect to the spatial accuracy is a concomitant challenge.
We study the training behavior of FasterPose, and formulate a novel regressive cross-entropy (RCE) loss function for accelerating the convergence and promoting the accuracy.
The RCE loss generalizes the ordinary cross-entropy loss from the binary supervision to a continuous range, thus the training of pose estimation network is able to benefit from the sigmoid function.
By doing so, the output heatmap can be inferred from the LR features without loss of spatial accuracy, while the computational cost and model size has been significantly reduced.
Compared with the previously dominant network of pose estimation, our method reduces 58% of the FLOPs and simultaneously gains 1.3% improvement of accuracy.
Extensive experiments show that FasterPose yields promising results on the common benchmarks, <span id="id19.id1.1" class="ltx_text ltx_font_italic">i.e.</span>,Â  COCO and MPII, consistently validating the effectiveness and efficiency for practical utilization, especially the low-latency and low-energy-budget applications in the non-GPU scenarios.</p>
</div>
<div class="ltx_keywords">pose estimation, keypoint detection
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journal: </span>TOMM</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span>2021</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_journalvolume"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalvolume: </span>0</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_journalnumber"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalnumber: </span>0</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_article"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">article: </span>0</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">publicationmonth: </span>0</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_price"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">doi: </span>00.0000/0000000</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Activity recognition and understanding</span></span></span><span id="id11" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Interest point and salient region detections</span></span></span><span id="id12" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Motion capture</span></span></span>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2107.03215/assets/figures/coco.jpg" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="426" height="190" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>
Qualitative evaluation of FasterPose on COCO. The red one is predicted by FasterPose, and the blue reference is by a state-of-the-art fast method, <span id="S0.F1.2.1" class="ltx_text ltx_font_italic">i.e.</span>,Â  SimpleBaseline.
The left side is original image, and the right side is zoomed image. FasterPose deals well with various adverse factors, such as twisted limb, abnormal pose, and view point change. It is worth noting that these results are yielded by FasterPose with 133 fps on a single NIVDIA 2080ti GPU.
</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Human pose estimation in image and video has been a long-standing yet challenging task in computer vision.
The goal is to localize human anatomical keypoints (wrists, knees, elbows, <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">etc</span>.) or parts in the input images.
This paper is interested in single-person pose estimation, which has been applied in many practical scenarios such as
action recognitionÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2013</span></a>; <span class="ltx_text" style="font-size:90%;">ChÃ©ron
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2015</span></a>)</cite>, pose trackingÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Cho
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2013</span></a>)</cite>, human-computer interactionÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shotton etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2011</span></a>)</cite>, <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">etc</span>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The recent literature shows that deep convolutional neural network (CNN) greatly improves the state-of-the-art performance in human pose estimation.
Many CNN-based methods are developed from Part-DetectorÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tompson
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite> that regresses the heatmaps corresponding to the keypoints.
In the regime of heatmap regression, one can find the high-resolution (HR) feature map provides great support to accurate keypoint localization.
Thus, many state-of-the-art methods pursue the spatial accuracy through learning the HR representation. TheyÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Newell
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Insafutdinov etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Yang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>; <span class="ltx_text" style="font-size:90%;">Cai etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> dispose the network with the high-to-low resolution subnetworks, and subsequently raise the resolution in the output stage.
For instance, HourglassÂ Â <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Newell
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite> and RSNÂ Â <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Cai etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> recovers the HR features through a symmetric low-to-high resolution process.
SimpleBaselineÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> adopts deconvolution layers for generating HR representations.
The dilated convolution plays a similar role inÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Insafutdinov etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Yang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>.
In addition, HRNetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> maintains the HR representations throughout the whole network.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.2" class="ltx_p">These HR feature based methods have achieved great progress for human pose estimation in recent years.
However, as shown in the experimental analysis (SectionÂ <a href="#S4.SS4" title="4.4. Ablation Study â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>), we find that the HR representation leads to a sharp increase of computational cost, while the accuracy improvement remains marginal.
For example, when the feature resolution increases from 8<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><times id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\times</annotation></semantics></math>6 to 64<math id="S1.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p3.2.m2.1a"><mo id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><times id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">\times</annotation></semantics></math>48, the computational cost increases from 3.8 to 9.0 GFLOPs, while the AP merely increases by less than 0.1% on COCO (TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.4.1. Influence of feature resolution â€£ 4.4. Ablation Study â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.3" class="ltx_p">Therefore, we put forward a question: is there a cost-effective solution that boosts the accuracy without any additional burden of computational complexity or even with lighter weights?
To deal with this challenge, we start from our design paradigm for efficient pose estimation, named <span id="S1.p4.3.1" class="ltx_text ltx_font_bold">FasterPose</span>.
The basic idea is to establish the thorough utilization of the low-resolution (LR) features.
FasterPose consists of an off-the-shelf backbone with high-to-low resolution, and a Low-to-High Regressor (LHR) module in succession. The design paradigm is particularly simple in structure and complexity. The backbone from any image classification routine can be directly used in FasterPose without any modification.
LHR adopts a group of 1<math id="S1.p4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p4.1.m1.1a"><mo id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><times id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">\times</annotation></semantics></math>1 convolution to each pixel of the LR feature maps that are output from the backbone to obtain multiple output values (<span id="S1.p4.3.2" class="ltx_text ltx_font_italic">e.g.</span>Â  <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S1.p4.2.m2.1a"><mn id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><cn type="integer" id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">64</annotation></semantics></math>), and then these multiple values are reshaped to form a region with a certain area (<span id="S1.p4.3.3" class="ltx_text ltx_font_italic">e.g.</span>Â  8<math id="S1.p4.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p4.3.m3.1a"><mo id="S1.p4.3.m3.1.1" xref="S1.p4.3.m3.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S1.p4.3.m3.1b"><times id="S1.p4.3.m3.1.1.cmml" xref="S1.p4.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.3.m3.1c">\times</annotation></semantics></math>8). After all the pixel points of LR feature maps are magnified many times by this operation, the LR feature maps is also magnified many times to obtain the HR heatmaps.
This LHR routine conducts more efficient inference than the deconvolution used in SimpleBaselineÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> which is currently the representative method for efficient post estimation. Given the same backbone, we find that the model size of FasterPose (ResNet-50) is 75% of SimpleBaseline, and the computational complexity (FLOPs) is only 42%.
This finding confirms that the exploitation of the LR feature is greatly useful towards the efficient human pose estimation, which nevertheless has been overlooked before.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Based on the efficient architecture of FasterPose,
the following objective is to effectively train the network and yield the accurate prediction for pose estimation.
Through the comparison analysis, we find the core of the problem is that the network with LR feature suffers from the slow converges, and consequently the accuracy is degraded (Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.4.1. Influence of feature resolution â€£ 4.4. Ablation Study â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).
To address the convergence problem, we inspect the distribution of the predicted values and the ground truth values on heatmap. For each person to be estimated, the true keypoints (<span id="S1.p5.1.1" class="ltx_text ltx_font_italic">i.e.</span>,Â  the positive samples with values greater than zero) are distributed sparsely and strongly correlated in the spatial dimension, while there are massive background pixels (<span id="S1.p5.1.2" class="ltx_text ltx_font_italic">i.e.</span>,Â  the negative samples with zero value) that fill the remaining space.
When the feature resolution changes from high to low, the parameters of the heatmap regression are dramatically reduced, most of which are dedicated to the negative sample regression under the supervision of mean squared error (MSE) loss. This causes the slow convergence on the positive sample regression that precisely forms the essential objective of pose estimation.
Given the massive negative samples with zero value, the cross-entropy (CE) style supervision is a more suitable choice, as it releases the parameters for the positive regression (SectionÂ <a href="#S3.SS2" title="3.2. RCE Loss â€£ 3. Proposed Method â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
The ordinary cross-entropy loss, however, cannot be utilized to supervise the learning of pose estimation in a straightforward manner, since the ground truth of heatmap regression consists of real numbers that ranges from 0 to 1, which does not match the binary target of cross-entropy supervision. To deal with this issue, we reshape the cross-entropy loss to a novel formulation, named <span id="S1.p5.1.3" class="ltx_text ltx_font_bold">Regressive Cross-Entropy</span> (RCE), which is able to supervise the regression toward real numbers between 0 and 1. Moreover, we find certain fine properties of RCE loss, such as the adaptive weighting of hard sample between positives and negatives. Benefiting from these advantages, RCE loss significantly improves the convergence speed and accuracy performance of FasterPose.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In summary, our contribution includes:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">It is the first attempt to analyze the impact of feature resolution in human pose estimation. The experimental results demonstrate that the LR features are also very useful and more efficient compared with the HR ones.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Resorting to the advantage of LR feature, we propose a design paradigm for cost-effective network of pose estimation, <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>,Â  FasterPose. Compared with the HR-based counterparts, FasterPose largely reduce the computational complexity, including the inference time, model size, and FLOPs, and thereby facilitates the low-latency and low-energy-budget applications on the non-GPU devices.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We formulate a novel loss function for heatmap regression, named RCE lossÂ <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The source code are released at <a target="_blank" href="https://github.com/hbin-ac/FasterPose" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/hbin-ac/FasterPose</a></span></span></span>, which is able to speed up the convergence of FasterPose and outperforms the counterparts in terms of test accuracy. These advantages are fully validated in the comprehensive experiments.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Feature Resolution in Pose Estimation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">HR features provide rich spatial information, many existing methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chen
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Newell
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Tang etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Ke
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Cai etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> pursue the accuracy through learning the HR representation.
The representative architectures can be categorized in three subsets:
(1) the backbone is followed by a low-to-high process (<span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span>Â  bilinear-upsampling or deconvolutions) to output HR representations, such as HourglassÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Newell
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>, CPNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chen
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, RSNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Cai etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, and SimpleBaselineÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>;
(2) the backbone is combined with dilated convolutionsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lifshitz
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Insafutdinov etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Pishchulin etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>;
(3) the backbone maintains the HR representation throughout the network, such as HRNetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
In fact, we find that the HR representation leads to a sharp increase of computational cost, while the accuracy improvement remains marginal.
Till now, there are few tools besides the bilinear interpolation and deconvolution, that dedicated to the effective upsampling from LR features for efficient pose estimation. Therefore, only one method SimpleBaselineÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> carries out pose estimation that utilizes mere the LR features.
In the field of super-resolution, ESPCNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shi etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite> propose the PixelShuffle routine, however, which cannot be directly utilized for pose estimation, since it is incompatible with the keypoint configuration.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Supervision for Heatmap Regression</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The MSE (mean square error) loss is the most widely used supervision in heatmap-based pose estimationÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">He
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Cao
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>; <span class="ltx_text" style="font-size:90%;">Chen
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Papandreou etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>; <span class="ltx_text" style="font-size:90%;">Insafutdinov etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Zhang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>.
It optimizes the pixel-wise similarity between the output and ground truth heatmap subject to the Euclidean metric.
However, the training objective of MSE is not consistent with the evaluation metric of pose estimation.
The discrepancy mainly comes from that the a prior distributions of keypoint and background pixels have significant gap. One can refer to SectionÂ <a href="#S3.SS2.SSS1" title="3.2.1. The drawback of MSE for pose estimation â€£ 3.2. RCE Loss â€£ 3. Proposed Method â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a> for the detailed discussion.
In such situation, the CE style supervision is more suitable for learning the regression towards 0 and 1.
However, the ground truth values are real numbers that range from 0 to 1, which is incompatible with the binary objective of CE loss.
PoseFixÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Moon
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, G-RMIÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Papandreou etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite> and UDPÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Huang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> deal with this issue by the alternative of one-hot or mask.
The experiments show that such modification on heatmap will cause bias and performance drop in SectionÂ <a href="#S4.SS4" title="4.4. Ablation Study â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.
To deal with this obstacle, we reshape the CE loss to a novel formulation that supervises the regression towards real numbers between 0 and 1.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Proposed Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first describe the design paradigm for efficient network, and its essential element, <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>,Â  low-to-high regressor; then, we discuss the limitation of MSE and plain CE loss in condition of LR feature, and present the formulation and application of the RCE supervision.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2107.03215/assets/figures/pipeline.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="265" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The overview of FasterPose. The input image is passed through a backbone, and then through the LHR module, and finally approaches the heatmap regression step that supervised by the RCE Loss.
Each group of 1<math id="S3.F2.5.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.F2.5.m1.1b"><mo id="S3.F2.5.m1.1.1" xref="S3.F2.5.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.F2.5.m1.1c"><times id="S3.F2.5.m1.1.1.cmml" xref="S3.F2.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.m1.1d">\times</annotation></semantics></math>1 convolution contains <math id="S3.F2.6.m2.1" class="ltx_Math" alttext="L^{2}" display="inline"><semantics id="S3.F2.6.m2.1b"><msup id="S3.F2.6.m2.1.1" xref="S3.F2.6.m2.1.1.cmml"><mi id="S3.F2.6.m2.1.1.2" xref="S3.F2.6.m2.1.1.2.cmml">L</mi><mn id="S3.F2.6.m2.1.1.3" xref="S3.F2.6.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.F2.6.m2.1c"><apply id="S3.F2.6.m2.1.1.cmml" xref="S3.F2.6.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.6.m2.1.1.1.cmml" xref="S3.F2.6.m2.1.1">superscript</csymbol><ci id="S3.F2.6.m2.1.1.2.cmml" xref="S3.F2.6.m2.1.1.2">ğ¿</ci><cn type="integer" id="S3.F2.6.m2.1.1.3.cmml" xref="S3.F2.6.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.m2.1d">L^{2}</annotation></semantics></math> kernels, and <math id="S3.F2.7.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.F2.7.m3.1b"><mi id="S3.F2.7.m3.1.1" xref="S3.F2.7.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.F2.7.m3.1c"><ci id="S3.F2.7.m3.1.1.cmml" xref="S3.F2.7.m3.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.7.m3.1d">L</annotation></semantics></math> is the upsampling ratio (<span id="S3.F2.10.1" class="ltx_text ltx_font_italic">e.g.</span>Â  <math id="S3.F2.8.m4.1" class="ltx_Math" alttext="L=8" display="inline"><semantics id="S3.F2.8.m4.1b"><mrow id="S3.F2.8.m4.1.1" xref="S3.F2.8.m4.1.1.cmml"><mi id="S3.F2.8.m4.1.1.2" xref="S3.F2.8.m4.1.1.2.cmml">L</mi><mo id="S3.F2.8.m4.1.1.1" xref="S3.F2.8.m4.1.1.1.cmml">=</mo><mn id="S3.F2.8.m4.1.1.3" xref="S3.F2.8.m4.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.8.m4.1c"><apply id="S3.F2.8.m4.1.1.cmml" xref="S3.F2.8.m4.1.1"><eq id="S3.F2.8.m4.1.1.1.cmml" xref="S3.F2.8.m4.1.1.1"></eq><ci id="S3.F2.8.m4.1.1.2.cmml" xref="S3.F2.8.m4.1.1.2">ğ¿</ci><cn type="integer" id="S3.F2.8.m4.1.1.3.cmml" xref="S3.F2.8.m4.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.m4.1d">L=8</annotation></semantics></math> in this figure).
The number of groups equals to the categories of keypoint.
</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Fast Paradigm</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The objective of heatmap-based human pose estimation is to accomplish the regression of output heatmaps that correspond to each keypoint.
Typically, the input image is passed through a backbone that consists of high-to-low resolution subnetworks, and then through the subsequent resolution-raising subnetworks, and finally approaches the heatmap regression stage.
The high-to-low stage aims to generate the LR and highly-semantic representations, and the low-to-high stage subsequently increases the resolution to support the accurate keypoint localization in finalÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bulat and
Tzimiropoulos</span>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Chen
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Hu and Ramanan</span>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Newell
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Tang etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>.
Therefore, this set of methods highly depends on the learning and inference of HR representation, suffering from the problem of heavy computational cost that in proportion to the resolution.
An effective alternative consists in cutting out the low-to-high stage, and inferring the heatmap from the LR representation directly. Without any modification of the remaining steps, the network is built in a straightforward way, while the critical challenge comes from the regressor which recovers the spatial resolution from the LR representation to the output heatmap and simultaneously regresses the prediction on the heatmap.
Previously, SimpleBaselineÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> deals with this challenge by using multiple layers of deconvolution, which improves the regression accuracy but also leads to significant augmentation of computational cost. For example, when the backbone (<span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>,Â  the high-to-low stage) is ResNet-50, the additional FLOPs that brought by the deconvolution layers will take 59% budget of the entire network.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.13" class="ltx_p">Rather than sticking to the framework of upsampling from spatial-neighboring site, we assume that the LR features have great capacity of semantic information along the channel dimension, and we show how to make use of the cross-channel abundant semantic information for regressing the heatmap via 1<math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mo id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><times id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\times</annotation></semantics></math>1 convolutions.
In other words, the high-level semantic features are sufficient to locally infers the prediction in the low-frequency domain.
This idea encourages us to build the scheme of LHR (<span id="S3.SS1.p2.13.1" class="ltx_text ltx_font_bold">Low-to-high Regressor</span>) in FasterPose.
Specifically, LHR applies a number of groups of 1<math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mo id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><times id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\times</annotation></semantics></math>1 convolution following the backbone; each group contains the kernels of which the number equals to the desired square upsampling ratio.
Without loss of generality, we take an example that the ratio is <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">L</annotation></semantics></math>, and the target has <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">N</annotation></semantics></math> keypoints, then LHR employs <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">N</annotation></semantics></math> groups, each of which owns <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="L^{2}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><msup id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">L</mi><mn id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">superscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">L^{2}</annotation></semantics></math> kernels of <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="1\times 1\times M" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mn id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.1a" xref="S3.SS1.p2.7.m7.1.1.1.cmml">Ã—</mo><mi id="S3.SS1.p2.7.m7.1.1.4" xref="S3.SS1.p2.7.m7.1.1.4.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><times id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1"></times><cn type="integer" id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">1</cn><cn type="integer" id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">1</cn><ci id="S3.SS1.p2.7.m7.1.1.4.cmml" xref="S3.SS1.p2.7.m7.1.1.4">ğ‘€</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">1\times 1\times M</annotation></semantics></math>, where <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><mi id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><ci id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">M</annotation></semantics></math> denotes the channel number of the LR feature maps. So, each group is in charge of projecting the LR feature to the heatmap with <math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="L^{2}" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><msup id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml"><mi id="S3.SS1.p2.9.m9.1.1.2" xref="S3.SS1.p2.9.m9.1.1.2.cmml">L</mi><mn id="S3.SS1.p2.9.m9.1.1.3" xref="S3.SS1.p2.9.m9.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><apply id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.1.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">superscript</csymbol><ci id="S3.SS1.p2.9.m9.1.1.2.cmml" xref="S3.SS1.p2.9.m9.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS1.p2.9.m9.1.1.3.cmml" xref="S3.SS1.p2.9.m9.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">L^{2}</annotation></semantics></math> channels, or equally the heatmap with <math id="S3.SS1.p2.10.m10.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS1.p2.10.m10.1a"><mi id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><ci id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">L</annotation></semantics></math> upsampling ratio in width and height; the total <math id="S3.SS1.p2.11.m11.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p2.11.m11.1a"><mi id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><ci id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">N</annotation></semantics></math> groups enable to output <math id="S3.SS1.p2.12.m12.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p2.12.m12.1a"><mi id="S3.SS1.p2.12.m12.1.1" xref="S3.SS1.p2.12.m12.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.1b"><ci id="S3.SS1.p2.12.m12.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.1c">N</annotation></semantics></math> heatmaps corresponding to the <math id="S3.SS1.p2.13.m13.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p2.13.m13.1a"><mi id="S3.SS1.p2.13.m13.1.1" xref="S3.SS1.p2.13.m13.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m13.1b"><ci id="S3.SS1.p2.13.m13.1.1.cmml" xref="S3.SS1.p2.13.m13.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m13.1c">N</annotation></semantics></math> keypoints, respectively.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.6" class="ltx_p">LHR is currently the simplest structure to infer HR heatmaps from LR features for pose estimation.
Its parameter number only takes <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="M\times N\times L^{2}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">M</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">Ã—</mo><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.1a" xref="S3.SS1.p3.1.m1.1.1.1.cmml">Ã—</mo><msup id="S3.SS1.p3.1.m1.1.1.4" xref="S3.SS1.p3.1.m1.1.1.4.cmml"><mi id="S3.SS1.p3.1.m1.1.1.4.2" xref="S3.SS1.p3.1.m1.1.1.4.2.cmml">L</mi><mn id="S3.SS1.p3.1.m1.1.1.4.3" xref="S3.SS1.p3.1.m1.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><times id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></times><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">ğ‘€</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">ğ‘</ci><apply id="S3.SS1.p3.1.m1.1.1.4.cmml" xref="S3.SS1.p3.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.4.1.cmml" xref="S3.SS1.p3.1.m1.1.1.4">superscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.4.2.cmml" xref="S3.SS1.p3.1.m1.1.1.4.2">ğ¿</ci><cn type="integer" id="S3.SS1.p3.1.m1.1.1.4.3.cmml" xref="S3.SS1.p3.1.m1.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">M\times N\times L^{2}</annotation></semantics></math>, while the deconvolutions in SimpleBaseline carry parameters of <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="(M+2\times F)\times F\times K^{2}+F\times N" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mrow id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.cmml"><mrow id="S3.SS1.p3.2.m2.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p3.2.m2.1.1.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.2.m2.1.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.1.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.2.cmml">M</mi><mo id="S3.SS1.p3.2.m2.1.1.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.SS1.p3.2.m2.1.1.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.cmml"><mn id="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.1.cmml">Ã—</mo><mi id="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.3.cmml">F</mi></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S3.SS1.p3.2.m2.1.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.SS1.p3.2.m2.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.2.cmml">Ã—</mo><mi id="S3.SS1.p3.2.m2.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.3.cmml">F</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.2.m2.1.1.1.2a" xref="S3.SS1.p3.2.m2.1.1.1.2.cmml">Ã—</mo><msup id="S3.SS1.p3.2.m2.1.1.1.4" xref="S3.SS1.p3.2.m2.1.1.1.4.cmml"><mi id="S3.SS1.p3.2.m2.1.1.1.4.2" xref="S3.SS1.p3.2.m2.1.1.1.4.2.cmml">K</mi><mn id="S3.SS1.p3.2.m2.1.1.1.4.3" xref="S3.SS1.p3.2.m2.1.1.1.4.3.cmml">2</mn></msup></mrow><mo id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">+</mo><mrow id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml"><mi id="S3.SS1.p3.2.m2.1.1.3.2" xref="S3.SS1.p3.2.m2.1.1.3.2.cmml">F</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.2.m2.1.1.3.1" xref="S3.SS1.p3.2.m2.1.1.3.1.cmml">Ã—</mo><mi id="S3.SS1.p3.2.m2.1.1.3.3" xref="S3.SS1.p3.2.m2.1.1.3.3.cmml">N</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><plus id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2"></plus><apply id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1"><times id="S3.SS1.p3.2.m2.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.2"></times><apply id="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1"><plus id="S3.SS1.p3.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.1"></plus><ci id="S3.SS1.p3.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.2">ğ‘€</ci><apply id="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.3"><times id="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.1"></times><cn type="integer" id="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.2">2</cn><ci id="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.3">ğ¹</ci></apply></apply><ci id="S3.SS1.p3.2.m2.1.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.1.3">ğ¹</ci><apply id="S3.SS1.p3.2.m2.1.1.1.4.cmml" xref="S3.SS1.p3.2.m2.1.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.4.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.4">superscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.1.4.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.4.2">ğ¾</ci><cn type="integer" id="S3.SS1.p3.2.m2.1.1.1.4.3.cmml" xref="S3.SS1.p3.2.m2.1.1.1.4.3">2</cn></apply></apply><apply id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3"><times id="S3.SS1.p3.2.m2.1.1.3.1.cmml" xref="S3.SS1.p3.2.m2.1.1.3.1"></times><ci id="S3.SS1.p3.2.m2.1.1.3.2.cmml" xref="S3.SS1.p3.2.m2.1.1.3.2">ğ¹</ci><ci id="S3.SS1.p3.2.m2.1.1.3.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">(M+2\times F)\times F\times K^{2}+F\times N</annotation></semantics></math>, where <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">K</annotation></semantics></math> and <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">F</annotation></semantics></math> are the kernel size and kernel number of the deconvolutions.
Resorting to LHR, we can reduce 79% of the regressor weights and 25% of the total weights compared with the counterpart subject to ResNet-50, <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="K=4" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mrow id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><mi id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml">K</mi><mo id="S3.SS1.p3.5.m5.1.1.1" xref="S3.SS1.p3.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><eq id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1.1"></eq><ci id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2">ğ¾</ci><cn type="integer" id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">K=4</annotation></semantics></math> and <math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="F=256" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><mrow id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml"><mi id="S3.SS1.p3.6.m6.1.1.2" xref="S3.SS1.p3.6.m6.1.1.2.cmml">F</mi><mo id="S3.SS1.p3.6.m6.1.1.1" xref="S3.SS1.p3.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS1.p3.6.m6.1.1.3" xref="S3.SS1.p3.6.m6.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><eq id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1.1"></eq><ci id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2">ğ¹</ci><cn type="integer" id="S3.SS1.p3.6.m6.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">F=256</annotation></semantics></math> in SimpleBaseline.
Fig.Â <a href="#S3.F3" title="Figure 3 â€£ 3.1. Fast Paradigm â€£ 3. Proposed Method â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that LHR significantly reduces the FLOPs as well.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.5" class="ltx_p">It is noteworthy that the PixelShuffle method proposed by ESPCNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shi etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>,
which is initially designed for the super-resolution task, is similar to LHR in terms of the LR inputs. ESPCN adopts an 3<math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mo id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><times id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\times</annotation></semantics></math>3 convolution to generate the features with <math id="S3.SS1.p4.2.m2.3" class="ltx_Math" alttext="(L^{2},W,H)" display="inline"><semantics id="S3.SS1.p4.2.m2.3a"><mrow id="S3.SS1.p4.2.m2.3.3.1" xref="S3.SS1.p4.2.m2.3.3.2.cmml"><mo stretchy="false" id="S3.SS1.p4.2.m2.3.3.1.2" xref="S3.SS1.p4.2.m2.3.3.2.cmml">(</mo><msup id="S3.SS1.p4.2.m2.3.3.1.1" xref="S3.SS1.p4.2.m2.3.3.1.1.cmml"><mi id="S3.SS1.p4.2.m2.3.3.1.1.2" xref="S3.SS1.p4.2.m2.3.3.1.1.2.cmml">L</mi><mn id="S3.SS1.p4.2.m2.3.3.1.1.3" xref="S3.SS1.p4.2.m2.3.3.1.1.3.cmml">2</mn></msup><mo id="S3.SS1.p4.2.m2.3.3.1.3" xref="S3.SS1.p4.2.m2.3.3.2.cmml">,</mo><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">W</mi><mo id="S3.SS1.p4.2.m2.3.3.1.4" xref="S3.SS1.p4.2.m2.3.3.2.cmml">,</mo><mi id="S3.SS1.p4.2.m2.2.2" xref="S3.SS1.p4.2.m2.2.2.cmml">H</mi><mo stretchy="false" id="S3.SS1.p4.2.m2.3.3.1.5" xref="S3.SS1.p4.2.m2.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.3b"><vector id="S3.SS1.p4.2.m2.3.3.2.cmml" xref="S3.SS1.p4.2.m2.3.3.1"><apply id="S3.SS1.p4.2.m2.3.3.1.1.cmml" xref="S3.SS1.p4.2.m2.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.3.3.1.1.1.cmml" xref="S3.SS1.p4.2.m2.3.3.1.1">superscript</csymbol><ci id="S3.SS1.p4.2.m2.3.3.1.1.2.cmml" xref="S3.SS1.p4.2.m2.3.3.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS1.p4.2.m2.3.3.1.1.3.cmml" xref="S3.SS1.p4.2.m2.3.3.1.1.3">2</cn></apply><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">ğ‘Š</ci><ci id="S3.SS1.p4.2.m2.2.2.cmml" xref="S3.SS1.p4.2.m2.2.2">ğ»</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.3c">(L^{2},W,H)</annotation></semantics></math> shape firstly, and then shuffle to the up-sampled maps of <math id="S3.SS1.p4.3.m3.3" class="ltx_Math" alttext="(1,W\times L,H\times L)" display="inline"><semantics id="S3.SS1.p4.3.m3.3a"><mrow id="S3.SS1.p4.3.m3.3.3.2" xref="S3.SS1.p4.3.m3.3.3.3.cmml"><mo stretchy="false" id="S3.SS1.p4.3.m3.3.3.2.3" xref="S3.SS1.p4.3.m3.3.3.3.cmml">(</mo><mn id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">1</mn><mo id="S3.SS1.p4.3.m3.3.3.2.4" xref="S3.SS1.p4.3.m3.3.3.3.cmml">,</mo><mrow id="S3.SS1.p4.3.m3.2.2.1.1" xref="S3.SS1.p4.3.m3.2.2.1.1.cmml"><mi id="S3.SS1.p4.3.m3.2.2.1.1.2" xref="S3.SS1.p4.3.m3.2.2.1.1.2.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.3.m3.2.2.1.1.1" xref="S3.SS1.p4.3.m3.2.2.1.1.1.cmml">Ã—</mo><mi id="S3.SS1.p4.3.m3.2.2.1.1.3" xref="S3.SS1.p4.3.m3.2.2.1.1.3.cmml">L</mi></mrow><mo id="S3.SS1.p4.3.m3.3.3.2.5" xref="S3.SS1.p4.3.m3.3.3.3.cmml">,</mo><mrow id="S3.SS1.p4.3.m3.3.3.2.2" xref="S3.SS1.p4.3.m3.3.3.2.2.cmml"><mi id="S3.SS1.p4.3.m3.3.3.2.2.2" xref="S3.SS1.p4.3.m3.3.3.2.2.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.3.m3.3.3.2.2.1" xref="S3.SS1.p4.3.m3.3.3.2.2.1.cmml">Ã—</mo><mi id="S3.SS1.p4.3.m3.3.3.2.2.3" xref="S3.SS1.p4.3.m3.3.3.2.2.3.cmml">L</mi></mrow><mo stretchy="false" id="S3.SS1.p4.3.m3.3.3.2.6" xref="S3.SS1.p4.3.m3.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.3b"><vector id="S3.SS1.p4.3.m3.3.3.3.cmml" xref="S3.SS1.p4.3.m3.3.3.2"><cn type="integer" id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">1</cn><apply id="S3.SS1.p4.3.m3.2.2.1.1.cmml" xref="S3.SS1.p4.3.m3.2.2.1.1"><times id="S3.SS1.p4.3.m3.2.2.1.1.1.cmml" xref="S3.SS1.p4.3.m3.2.2.1.1.1"></times><ci id="S3.SS1.p4.3.m3.2.2.1.1.2.cmml" xref="S3.SS1.p4.3.m3.2.2.1.1.2">ğ‘Š</ci><ci id="S3.SS1.p4.3.m3.2.2.1.1.3.cmml" xref="S3.SS1.p4.3.m3.2.2.1.1.3">ğ¿</ci></apply><apply id="S3.SS1.p4.3.m3.3.3.2.2.cmml" xref="S3.SS1.p4.3.m3.3.3.2.2"><times id="S3.SS1.p4.3.m3.3.3.2.2.1.cmml" xref="S3.SS1.p4.3.m3.3.3.2.2.1"></times><ci id="S3.SS1.p4.3.m3.3.3.2.2.2.cmml" xref="S3.SS1.p4.3.m3.3.3.2.2.2">ğ»</ci><ci id="S3.SS1.p4.3.m3.3.3.2.2.3.cmml" xref="S3.SS1.p4.3.m3.3.3.2.2.3">ğ¿</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.3c">(1,W\times L,H\times L)</annotation></semantics></math>. Such routine cannot be used in most cases of pose estimation without customization, since the number of output channel of backbone (generally 512 or 2048) divided by the category of human keypoint (<span id="S3.SS1.p4.5.1" class="ltx_text ltx_font_italic">e.g.</span>Â  17 in COCO setting) does not match the multiples of the number <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="L^{2}" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><msup id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2" xref="S3.SS1.p4.4.m4.1.1.2.cmml">L</mi><mn id="S3.SS1.p4.4.m4.1.1.3" xref="S3.SS1.p4.4.m4.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">superscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS1.p4.4.m4.1.1.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">L^{2}</annotation></semantics></math>; instead, LHR gets rid of such limit, and can be implemented with any off-the-shelf architectures for pose estimation. Besides, by taking the advantage of abundant local information in the LR features, LHR accomplishes the mapping from LR feature to heatmap with 1<math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><mo id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><times id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">\times</annotation></semantics></math>1 convolution of which the computational cost is only 1/9 of PixelShuffle.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2107.03215/assets/figures/ap_gflops.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="274" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>The detailed comparison between FasterPose (FP) and SimpleBaseline (SB) with various backbone architectures and input size, in terms of FLOPs, model size and COCO AP.
The horizontal axis indicates the choice of backbone and input size. For example, â€œR50-256â€ denotes the ResNet-50 with input size of 256<math id="S3.F3.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.F3.2.m1.1b"><mo id="S3.F3.2.m1.1.1" xref="S3.F3.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.F3.2.m1.1c"><times id="S3.F3.2.m1.1.1.cmml" xref="S3.F3.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.m1.1d">\times</annotation></semantics></math>192 RGB.
</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>RCE Loss</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>The drawback of MSE for pose estimation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The MSE loss is employed by most of the heatmap regression methods to supervise the training of the backbone and regressor.
It minimizes the pixel-wise discrepancy between the predicted heatmap and the ground truth heatmap.
While we adopt MSE for training FasterPose, we find the network suffers from slow convergence and inferior accuracy (Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.4.1. Influence of feature resolution â€£ 4.4. Ablation Study â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) compared with the HR-style counterpart.
To identify the reason, we inspect the distribution of the heatmap values (TableÂ <a href="#S3.T1" title="Table 1 â€£ 3.2.1. The drawback of MSE for pose estimation â€£ 3.2. RCE Loss â€£ 3. Proposed Method â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
It is obvious that, for each pose to be estimated, the true keypoints, whose values are greater than zero in the ground truth heatmap, are distributed sparsely in the spatial dimension, while the remaining massive background pixels are with zero value.
The parameters of the heatmap regressor are significantly reduced when the feature becomes LR, and most of the parameters are dedicated to the regression towards zero value in the background under the supervision of MSE.
This is the reason that causes the convergence issue with respect to the positive samples (<span id="S3.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">a.k.a.</span>Â  keypoints) which is precisely the primary objective of pose estimation.
Moreover, the inferior accuracy is also caused by the reason that MSE is inconsistent with the evaluation metric of pose estimation. For example, a minor mis-prediction in the keypoint site affects the test accuracy (<span id="S3.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_italic">e.g.</span>Â  AP) remarkably, while a minor mis-prediction in background barely matters; on the other hand, both the predictions in background and keypoint site contribute equally to the MSE value.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.4" class="ltx_p">To solve this problem, the sigmoid operation is a more suitable choice than the Euclidean metric.
As shown in Fig.Â <a href="#S3.F4" title="Figure 4 â€£ 3.2.1. The drawback of MSE for pose estimation â€£ 3.2. RCE Loss â€£ 3. Proposed Method â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the regression value <math id="S3.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mi id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><ci id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">x</annotation></semantics></math> is squeezed to 0 or 1 by the sigmoid mapping. For example, the values <math id="S3.SS2.SSS1.p2.2.m2.2" class="ltx_Math" alttext="x\in\left(-\infty,-5\right]" display="inline"><semantics id="S3.SS2.SSS1.p2.2.m2.2a"><mrow id="S3.SS2.SSS1.p2.2.m2.2.2" xref="S3.SS2.SSS1.p2.2.m2.2.2.cmml"><mi id="S3.SS2.SSS1.p2.2.m2.2.2.4" xref="S3.SS2.SSS1.p2.2.m2.2.2.4.cmml">x</mi><mo id="S3.SS2.SSS1.p2.2.m2.2.2.3" xref="S3.SS2.SSS1.p2.2.m2.2.2.3.cmml">âˆˆ</mo><mrow id="S3.SS2.SSS1.p2.2.m2.2.2.2.2" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.3.cmml"><mo id="S3.SS2.SSS1.p2.2.m2.2.2.2.2.3" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.3.cmml">(</mo><mrow id="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1a" xref="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1.cmml">âˆ’</mo><mi mathvariant="normal" id="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1.2" xref="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1.2.cmml">âˆ</mi></mrow><mo id="S3.SS2.SSS1.p2.2.m2.2.2.2.2.4" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.3.cmml">,</mo><mrow id="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2.cmml"><mo id="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2a" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2.cmml">âˆ’</mo><mn id="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2.2" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2.2.cmml">5</mn></mrow><mo id="S3.SS2.SSS1.p2.2.m2.2.2.2.2.5" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.2b"><apply id="S3.SS2.SSS1.p2.2.m2.2.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2"><in id="S3.SS2.SSS1.p2.2.m2.2.2.3.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.3"></in><ci id="S3.SS2.SSS1.p2.2.m2.2.2.4.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.4">ğ‘¥</ci><interval closure="open-closed" id="S3.SS2.SSS1.p2.2.m2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.2"><apply id="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1"><minus id="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1"></minus><infinity id="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.1.1.1.2"></infinity></apply><apply id="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2"><minus id="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2"></minus><cn type="integer" id="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.2.2.2.2">5</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.2c">x\in\left(-\infty,-5\right]</annotation></semantics></math> are mapped sufficiently close to zero.
Thus, the regressor does not have to fit the background with large portion of parameters.
Instead, the regressor is released for the primary objective, <span id="S3.SS2.SSS1.p2.4.1" class="ltx_text ltx_font_italic">i.e.</span>,Â  the regression towards positive samples.
A side affect, however, is raised when MSE applied to <math id="S3.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="y=sigmoid(x)" display="inline"><semantics id="S3.SS2.SSS1.p2.3.m3.1a"><mrow id="S3.SS2.SSS1.p2.3.m3.1.2" xref="S3.SS2.SSS1.p2.3.m3.1.2.cmml"><mi id="S3.SS2.SSS1.p2.3.m3.1.2.2" xref="S3.SS2.SSS1.p2.3.m3.1.2.2.cmml">y</mi><mo id="S3.SS2.SSS1.p2.3.m3.1.2.1" xref="S3.SS2.SSS1.p2.3.m3.1.2.1.cmml">=</mo><mrow id="S3.SS2.SSS1.p2.3.m3.1.2.3" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.cmml"><mi id="S3.SS2.SSS1.p2.3.m3.1.2.3.2" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.3.m3.1.2.3.1" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p2.3.m3.1.2.3.3" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.3.m3.1.2.3.1a" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p2.3.m3.1.2.3.4" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.3.m3.1.2.3.1b" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p2.3.m3.1.2.3.5" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.3.m3.1.2.3.1c" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p2.3.m3.1.2.3.6" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.3.m3.1.2.3.1d" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p2.3.m3.1.2.3.7" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.3.m3.1.2.3.1e" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p2.3.m3.1.2.3.8" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.8.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.3.m3.1.2.3.1f" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.1.cmml">â€‹</mo><mrow id="S3.SS2.SSS1.p2.3.m3.1.2.3.9.2" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p2.3.m3.1.2.3.9.2.1" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.cmml">(</mo><mi id="S3.SS2.SSS1.p2.3.m3.1.1" xref="S3.SS2.SSS1.p2.3.m3.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.SSS1.p2.3.m3.1.2.3.9.2.2" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.3.m3.1b"><apply id="S3.SS2.SSS1.p2.3.m3.1.2.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2"><eq id="S3.SS2.SSS1.p2.3.m3.1.2.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2.1"></eq><ci id="S3.SS2.SSS1.p2.3.m3.1.2.2.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2.2">ğ‘¦</ci><apply id="S3.SS2.SSS1.p2.3.m3.1.2.3.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2.3"><times id="S3.SS2.SSS1.p2.3.m3.1.2.3.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.1"></times><ci id="S3.SS2.SSS1.p2.3.m3.1.2.3.2.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.2">ğ‘ </ci><ci id="S3.SS2.SSS1.p2.3.m3.1.2.3.3.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.3">ğ‘–</ci><ci id="S3.SS2.SSS1.p2.3.m3.1.2.3.4.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.4">ğ‘”</ci><ci id="S3.SS2.SSS1.p2.3.m3.1.2.3.5.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.5">ğ‘š</ci><ci id="S3.SS2.SSS1.p2.3.m3.1.2.3.6.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.6">ğ‘œ</ci><ci id="S3.SS2.SSS1.p2.3.m3.1.2.3.7.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.7">ğ‘–</ci><ci id="S3.SS2.SSS1.p2.3.m3.1.2.3.8.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.2.3.8">ğ‘‘</ci><ci id="S3.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.3.m3.1c">y=sigmoid(x)</annotation></semantics></math>: the gradient rapidly vanishes in most range of <math id="S3.SS2.SSS1.p2.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS1.p2.4.m4.1a"><mi id="S3.SS2.SSS1.p2.4.m4.1.1" xref="S3.SS2.SSS1.p2.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.4.m4.1b"><ci id="S3.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.4.m4.1c">x</annotation></semantics></math> (Fig.Â <a href="#S3.F4" title="Figure 4 â€£ 3.2.1. The drawback of MSE for pose estimation â€£ 3.2. RCE Loss â€£ 3. Proposed Method â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). We tackle this problem with the RCE loss.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>
Statistics of the positive (P) and negative (N) samples in heatmap.
Most of the pixels in heatmap are negative samples.
<math id="S3.T1.2.2.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.T1.2.2.m1.1b"><mi id="S3.T1.2.2.m1.1.1" xref="S3.T1.2.2.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.m1.1c"><ci id="S3.T1.2.2.m1.1.1.cmml" xref="S3.T1.2.2.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.m1.1d">t</annotation></semantics></math> is the variance of Gaussian diffusion.
</figcaption>
<table id="S3.T1.5.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.3.3.1" class="ltx_tr">
<th id="S3.T1.3.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Heatmap size</th>
<th id="S3.T1.3.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><math id="S3.T1.3.3.1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.T1.3.3.1.1.m1.1a"><mi id="S3.T1.3.3.1.1.m1.1.1" xref="S3.T1.3.3.1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.1.m1.1b"><ci id="S3.T1.3.3.1.1.m1.1.1.cmml" xref="S3.T1.3.3.1.1.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.1.m1.1c">t</annotation></semantics></math></th>
<th id="S3.T1.3.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">P</th>
<th id="S3.T1.3.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">N</th>
<th id="S3.T1.3.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Ratio of P:N</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.4.2" class="ltx_tr">
<td id="S3.T1.4.4.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">64<math id="S3.T1.4.4.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.4.4.2.1.m1.1a"><mo id="S3.T1.4.4.2.1.m1.1.1" xref="S3.T1.4.4.2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.2.1.m1.1b"><times id="S3.T1.4.4.2.1.m1.1.1.cmml" xref="S3.T1.4.4.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.2.1.m1.1c">\times</annotation></semantics></math>48</td>
<td id="S3.T1.4.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">2.0</td>
<td id="S3.T1.4.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">113</td>
<td id="S3.T1.4.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">2,959</td>
<td id="S3.T1.4.4.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">1:26.2</td>
</tr>
<tr id="S3.T1.5.5.3" class="ltx_tr">
<td id="S3.T1.5.5.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">96<math id="S3.T1.5.5.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.5.5.3.1.m1.1a"><mo id="S3.T1.5.5.3.1.m1.1.1" xref="S3.T1.5.5.3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.3.1.m1.1b"><times id="S3.T1.5.5.3.1.m1.1.1.cmml" xref="S3.T1.5.5.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.3.1.m1.1c">\times</annotation></semantics></math>72</td>
<td id="S3.T1.5.5.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">3.0</td>
<td id="S3.T1.5.5.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">261</td>
<td id="S3.T1.5.5.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">6,651</td>
<td id="S3.T1.5.5.3.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;">1:25.5</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.03215/assets/figures/loss_a.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="210" height="157" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.03215/assets/figures/loss_b.png" id="S3.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="199" height="157" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.03215/assets/figures/loss_c.png" id="S3.F4.3.g1" class="ltx_graphics ltx_img_landscape" width="210" height="155" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.03215/assets/figures/loss_d.png" id="S3.F4.4.g1" class="ltx_graphics ltx_img_landscape" width="204" height="153" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>
Analysis of loss function. (a) The sigmoid function, where <math id="S3.F4.11.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.F4.11.m1.1b"><mi id="S3.F4.11.m1.1.1" xref="S3.F4.11.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F4.11.m1.1c"><ci id="S3.F4.11.m1.1.1.cmml" xref="S3.F4.11.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.11.m1.1d">x</annotation></semantics></math> (horizontal axis) represents the regression value, and <math id="S3.F4.12.m2.1" class="ltx_Math" alttext="y=sigmoid(x)" display="inline"><semantics id="S3.F4.12.m2.1b"><mrow id="S3.F4.12.m2.1.2" xref="S3.F4.12.m2.1.2.cmml"><mi id="S3.F4.12.m2.1.2.2" xref="S3.F4.12.m2.1.2.2.cmml">y</mi><mo id="S3.F4.12.m2.1.2.1" xref="S3.F4.12.m2.1.2.1.cmml">=</mo><mrow id="S3.F4.12.m2.1.2.3" xref="S3.F4.12.m2.1.2.3.cmml"><mi id="S3.F4.12.m2.1.2.3.2" xref="S3.F4.12.m2.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.F4.12.m2.1.2.3.1" xref="S3.F4.12.m2.1.2.3.1.cmml">â€‹</mo><mi id="S3.F4.12.m2.1.2.3.3" xref="S3.F4.12.m2.1.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.F4.12.m2.1.2.3.1b" xref="S3.F4.12.m2.1.2.3.1.cmml">â€‹</mo><mi id="S3.F4.12.m2.1.2.3.4" xref="S3.F4.12.m2.1.2.3.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.F4.12.m2.1.2.3.1c" xref="S3.F4.12.m2.1.2.3.1.cmml">â€‹</mo><mi id="S3.F4.12.m2.1.2.3.5" xref="S3.F4.12.m2.1.2.3.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.F4.12.m2.1.2.3.1d" xref="S3.F4.12.m2.1.2.3.1.cmml">â€‹</mo><mi id="S3.F4.12.m2.1.2.3.6" xref="S3.F4.12.m2.1.2.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.F4.12.m2.1.2.3.1e" xref="S3.F4.12.m2.1.2.3.1.cmml">â€‹</mo><mi id="S3.F4.12.m2.1.2.3.7" xref="S3.F4.12.m2.1.2.3.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.F4.12.m2.1.2.3.1f" xref="S3.F4.12.m2.1.2.3.1.cmml">â€‹</mo><mi id="S3.F4.12.m2.1.2.3.8" xref="S3.F4.12.m2.1.2.3.8.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.F4.12.m2.1.2.3.1g" xref="S3.F4.12.m2.1.2.3.1.cmml">â€‹</mo><mrow id="S3.F4.12.m2.1.2.3.9.2" xref="S3.F4.12.m2.1.2.3.cmml"><mo stretchy="false" id="S3.F4.12.m2.1.2.3.9.2.1" xref="S3.F4.12.m2.1.2.3.cmml">(</mo><mi id="S3.F4.12.m2.1.1" xref="S3.F4.12.m2.1.1.cmml">x</mi><mo stretchy="false" id="S3.F4.12.m2.1.2.3.9.2.2" xref="S3.F4.12.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.12.m2.1c"><apply id="S3.F4.12.m2.1.2.cmml" xref="S3.F4.12.m2.1.2"><eq id="S3.F4.12.m2.1.2.1.cmml" xref="S3.F4.12.m2.1.2.1"></eq><ci id="S3.F4.12.m2.1.2.2.cmml" xref="S3.F4.12.m2.1.2.2">ğ‘¦</ci><apply id="S3.F4.12.m2.1.2.3.cmml" xref="S3.F4.12.m2.1.2.3"><times id="S3.F4.12.m2.1.2.3.1.cmml" xref="S3.F4.12.m2.1.2.3.1"></times><ci id="S3.F4.12.m2.1.2.3.2.cmml" xref="S3.F4.12.m2.1.2.3.2">ğ‘ </ci><ci id="S3.F4.12.m2.1.2.3.3.cmml" xref="S3.F4.12.m2.1.2.3.3">ğ‘–</ci><ci id="S3.F4.12.m2.1.2.3.4.cmml" xref="S3.F4.12.m2.1.2.3.4">ğ‘”</ci><ci id="S3.F4.12.m2.1.2.3.5.cmml" xref="S3.F4.12.m2.1.2.3.5">ğ‘š</ci><ci id="S3.F4.12.m2.1.2.3.6.cmml" xref="S3.F4.12.m2.1.2.3.6">ğ‘œ</ci><ci id="S3.F4.12.m2.1.2.3.7.cmml" xref="S3.F4.12.m2.1.2.3.7">ğ‘–</ci><ci id="S3.F4.12.m2.1.2.3.8.cmml" xref="S3.F4.12.m2.1.2.3.8">ğ‘‘</ci><ci id="S3.F4.12.m2.1.1.cmml" xref="S3.F4.12.m2.1.1">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.12.m2.1d">y=sigmoid(x)</annotation></semantics></math> squeezes most values to 0 and 1.
(b) MSE and RCE with respect to the error <math id="S3.F4.13.m3.1" class="ltx_Math" alttext="e=y-\hat{y}" display="inline"><semantics id="S3.F4.13.m3.1b"><mrow id="S3.F4.13.m3.1.1" xref="S3.F4.13.m3.1.1.cmml"><mi id="S3.F4.13.m3.1.1.2" xref="S3.F4.13.m3.1.1.2.cmml">e</mi><mo id="S3.F4.13.m3.1.1.1" xref="S3.F4.13.m3.1.1.1.cmml">=</mo><mrow id="S3.F4.13.m3.1.1.3" xref="S3.F4.13.m3.1.1.3.cmml"><mi id="S3.F4.13.m3.1.1.3.2" xref="S3.F4.13.m3.1.1.3.2.cmml">y</mi><mo id="S3.F4.13.m3.1.1.3.1" xref="S3.F4.13.m3.1.1.3.1.cmml">âˆ’</mo><mover accent="true" id="S3.F4.13.m3.1.1.3.3" xref="S3.F4.13.m3.1.1.3.3.cmml"><mi id="S3.F4.13.m3.1.1.3.3.2" xref="S3.F4.13.m3.1.1.3.3.2.cmml">y</mi><mo id="S3.F4.13.m3.1.1.3.3.1" xref="S3.F4.13.m3.1.1.3.3.1.cmml">^</mo></mover></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.13.m3.1c"><apply id="S3.F4.13.m3.1.1.cmml" xref="S3.F4.13.m3.1.1"><eq id="S3.F4.13.m3.1.1.1.cmml" xref="S3.F4.13.m3.1.1.1"></eq><ci id="S3.F4.13.m3.1.1.2.cmml" xref="S3.F4.13.m3.1.1.2">ğ‘’</ci><apply id="S3.F4.13.m3.1.1.3.cmml" xref="S3.F4.13.m3.1.1.3"><minus id="S3.F4.13.m3.1.1.3.1.cmml" xref="S3.F4.13.m3.1.1.3.1"></minus><ci id="S3.F4.13.m3.1.1.3.2.cmml" xref="S3.F4.13.m3.1.1.3.2">ğ‘¦</ci><apply id="S3.F4.13.m3.1.1.3.3.cmml" xref="S3.F4.13.m3.1.1.3.3"><ci id="S3.F4.13.m3.1.1.3.3.1.cmml" xref="S3.F4.13.m3.1.1.3.3.1">^</ci><ci id="S3.F4.13.m3.1.1.3.3.2.cmml" xref="S3.F4.13.m3.1.1.3.3.2">ğ‘¦</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.13.m3.1d">e=y-\hat{y}</annotation></semantics></math>, where <math id="S3.F4.14.m4.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S3.F4.14.m4.1b"><mover accent="true" id="S3.F4.14.m4.1.1" xref="S3.F4.14.m4.1.1.cmml"><mi id="S3.F4.14.m4.1.1.2" xref="S3.F4.14.m4.1.1.2.cmml">y</mi><mo id="S3.F4.14.m4.1.1.1" xref="S3.F4.14.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.F4.14.m4.1c"><apply id="S3.F4.14.m4.1.1.cmml" xref="S3.F4.14.m4.1.1"><ci id="S3.F4.14.m4.1.1.1.cmml" xref="S3.F4.14.m4.1.1.1">^</ci><ci id="S3.F4.14.m4.1.1.2.cmml" xref="S3.F4.14.m4.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.14.m4.1d">\hat{y}</annotation></semantics></math> denotes the ground truth.
(c) MSE with respect to <math id="S3.F4.15.m5.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.F4.15.m5.1b"><mi id="S3.F4.15.m5.1.1" xref="S3.F4.15.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F4.15.m5.1c"><ci id="S3.F4.15.m5.1.1.cmml" xref="S3.F4.15.m5.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.15.m5.1d">x</annotation></semantics></math>, whose gradient vanishes in most range of <math id="S3.F4.16.m6.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.F4.16.m6.1b"><mi id="S3.F4.16.m6.1.1" xref="S3.F4.16.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F4.16.m6.1c"><ci id="S3.F4.16.m6.1.1.cmml" xref="S3.F4.16.m6.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.16.m6.1d">x</annotation></semantics></math>.
(d) RCE alleviates the gradient vanishing problem.
Best viewed in color.
</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Formulation of RCE</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.8" class="ltx_p">There are only two prior worksÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Moon
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Papandreou etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite> that use the CE loss for training human pose estimation.
The major challenge is that the ground truth heatmap consists of real numbers that ranges from 0 to 1, which is conflict to the binary objective of CE loss:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.6" class="ltx_Math" alttext="CE(y,\hat{y})=\begin{cases}-log(y)&amp;\text{if $\hat{y}=1,$}\\
-log(1-y)&amp;\text{if $\hat{y}=0,$}\end{cases}" display="block"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.7" xref="S3.E1.m1.6.7.cmml"><mrow id="S3.E1.m1.6.7.2" xref="S3.E1.m1.6.7.2.cmml"><mi id="S3.E1.m1.6.7.2.2" xref="S3.E1.m1.6.7.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.7.2.1" xref="S3.E1.m1.6.7.2.1.cmml">â€‹</mo><mi id="S3.E1.m1.6.7.2.3" xref="S3.E1.m1.6.7.2.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.7.2.1a" xref="S3.E1.m1.6.7.2.1.cmml">â€‹</mo><mrow id="S3.E1.m1.6.7.2.4.2" xref="S3.E1.m1.6.7.2.4.1.cmml"><mo stretchy="false" id="S3.E1.m1.6.7.2.4.2.1" xref="S3.E1.m1.6.7.2.4.1.cmml">(</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">y</mi><mo id="S3.E1.m1.6.7.2.4.2.2" xref="S3.E1.m1.6.7.2.4.1.cmml">,</mo><mover accent="true" id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml"><mi id="S3.E1.m1.6.6.2" xref="S3.E1.m1.6.6.2.cmml">y</mi><mo id="S3.E1.m1.6.6.1" xref="S3.E1.m1.6.6.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E1.m1.6.7.2.4.2.3" xref="S3.E1.m1.6.7.2.4.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.6.7.1" xref="S3.E1.m1.6.7.1.cmml">=</mo><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.6.7.3.1.cmml"><mo id="S3.E1.m1.4.4.5" xref="S3.E1.m1.6.7.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E1.m1.4.4.4" xref="S3.E1.m1.6.7.3.1.cmml"><mtr id="S3.E1.m1.4.4.4a" xref="S3.E1.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.4.4.4b" xref="S3.E1.m1.6.7.3.1.cmml"><mrow id="S3.E1.m1.3.3.3.3.2.1" xref="S3.E1.m1.3.3.3.3.2.1.cmml"><mo id="S3.E1.m1.3.3.3.3.2.1a" xref="S3.E1.m1.3.3.3.3.2.1.cmml">âˆ’</mo><mrow id="S3.E1.m1.3.3.3.3.2.1.3" xref="S3.E1.m1.3.3.3.3.2.1.3.cmml"><mi id="S3.E1.m1.3.3.3.3.2.1.3.2" xref="S3.E1.m1.3.3.3.3.2.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.2.1.3.1" xref="S3.E1.m1.3.3.3.3.2.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.3.3.3.3.2.1.3.3" xref="S3.E1.m1.3.3.3.3.2.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.2.1.3.1a" xref="S3.E1.m1.3.3.3.3.2.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.3.3.3.3.2.1.3.4" xref="S3.E1.m1.3.3.3.3.2.1.3.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.2.1.3.1b" xref="S3.E1.m1.3.3.3.3.2.1.3.1.cmml">â€‹</mo><mrow id="S3.E1.m1.3.3.3.3.2.1.3.5.2" xref="S3.E1.m1.3.3.3.3.2.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.3.2.1.3.5.2.1" xref="S3.E1.m1.3.3.3.3.2.1.3.cmml">(</mo><mi id="S3.E1.m1.3.3.3.3.2.1.1" xref="S3.E1.m1.3.3.3.3.2.1.1.cmml">y</mi><mo stretchy="false" id="S3.E1.m1.3.3.3.3.2.1.3.5.2.2" xref="S3.E1.m1.3.3.3.3.2.1.3.cmml">)</mo></mrow></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.4.4.4c" xref="S3.E1.m1.6.7.3.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1b.cmml"><mtext id="S3.E1.m1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1b.cmml">ifÂ </mtext><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.2.cmml">y</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.cmml">=</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">,</mo></mrow></mrow></mtd></mtr><mtr id="S3.E1.m1.4.4.4d" xref="S3.E1.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.4.4.4e" xref="S3.E1.m1.6.7.3.1.cmml"><mrow id="S3.E1.m1.4.4.4.4.2.1" xref="S3.E1.m1.4.4.4.4.2.1.cmml"><mo id="S3.E1.m1.4.4.4.4.2.1a" xref="S3.E1.m1.4.4.4.4.2.1.cmml">âˆ’</mo><mrow id="S3.E1.m1.4.4.4.4.2.1.1" xref="S3.E1.m1.4.4.4.4.2.1.1.cmml"><mi id="S3.E1.m1.4.4.4.4.2.1.1.3" xref="S3.E1.m1.4.4.4.4.2.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.2.1.1.2" xref="S3.E1.m1.4.4.4.4.2.1.1.2.cmml">â€‹</mo><mi id="S3.E1.m1.4.4.4.4.2.1.1.4" xref="S3.E1.m1.4.4.4.4.2.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.2.1.1.2a" xref="S3.E1.m1.4.4.4.4.2.1.1.2.cmml">â€‹</mo><mi id="S3.E1.m1.4.4.4.4.2.1.1.5" xref="S3.E1.m1.4.4.4.4.2.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.2.1.1.2b" xref="S3.E1.m1.4.4.4.4.2.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.4.4.4.4.2.1.1.1.1" xref="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.4.4.2.1.1.1.1.2" xref="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.4.4.2.1.1.1.1.1" xref="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.cmml"><mn id="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.2" xref="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.1" xref="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.3" xref="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.3.cmml">y</mi></mrow><mo stretchy="false" id="S3.E1.m1.4.4.4.4.2.1.1.1.1.3" xref="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.4.4.4f" xref="S3.E1.m1.6.7.3.1.cmml"><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1b.cmml"><mtext id="S3.E1.m1.2.2.2.2.1.1a" xref="S3.E1.m1.2.2.2.2.1.1b.cmml">ifÂ </mtext><mrow id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.cmml"><mrow id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2.2" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2.2.cmml">y</mi><mo id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.cmml">=</mo><mn id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.3.cmml">0</mn></mrow><mo id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.cmml">,</mo></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.7.cmml" xref="S3.E1.m1.6.7"><eq id="S3.E1.m1.6.7.1.cmml" xref="S3.E1.m1.6.7.1"></eq><apply id="S3.E1.m1.6.7.2.cmml" xref="S3.E1.m1.6.7.2"><times id="S3.E1.m1.6.7.2.1.cmml" xref="S3.E1.m1.6.7.2.1"></times><ci id="S3.E1.m1.6.7.2.2.cmml" xref="S3.E1.m1.6.7.2.2">ğ¶</ci><ci id="S3.E1.m1.6.7.2.3.cmml" xref="S3.E1.m1.6.7.2.3">ğ¸</ci><interval closure="open" id="S3.E1.m1.6.7.2.4.1.cmml" xref="S3.E1.m1.6.7.2.4.2"><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">ğ‘¦</ci><apply id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"><ci id="S3.E1.m1.6.6.1.cmml" xref="S3.E1.m1.6.6.1">^</ci><ci id="S3.E1.m1.6.6.2.cmml" xref="S3.E1.m1.6.6.2">ğ‘¦</ci></apply></interval></apply><apply id="S3.E1.m1.6.7.3.1.cmml" xref="S3.E1.m1.4.4"><csymbol cd="latexml" id="S3.E1.m1.6.7.3.1.1.cmml" xref="S3.E1.m1.4.4.5">cases</csymbol><apply id="S3.E1.m1.3.3.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.3.2.1"><minus id="S3.E1.m1.3.3.3.3.2.1.2.cmml" xref="S3.E1.m1.3.3.3.3.2.1"></minus><apply id="S3.E1.m1.3.3.3.3.2.1.3.cmml" xref="S3.E1.m1.3.3.3.3.2.1.3"><times id="S3.E1.m1.3.3.3.3.2.1.3.1.cmml" xref="S3.E1.m1.3.3.3.3.2.1.3.1"></times><ci id="S3.E1.m1.3.3.3.3.2.1.3.2.cmml" xref="S3.E1.m1.3.3.3.3.2.1.3.2">ğ‘™</ci><ci id="S3.E1.m1.3.3.3.3.2.1.3.3.cmml" xref="S3.E1.m1.3.3.3.3.2.1.3.3">ğ‘œ</ci><ci id="S3.E1.m1.3.3.3.3.2.1.3.4.cmml" xref="S3.E1.m1.3.3.3.3.2.1.3.4">ğ‘”</ci><ci id="S3.E1.m1.3.3.3.3.2.1.1.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1">ğ‘¦</ci></apply></apply><ci id="S3.E1.m1.1.1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><mrow id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><mtext id="S3.E1.m1.1.1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1.1.1.1">ifÂ </mtext><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1"><mover accent="true" id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.2">y</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.2.1">^</mo></mover><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1">=</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.3">1</mn></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1">,</mo></mrow></mrow></ci><apply id="S3.E1.m1.4.4.4.4.2.1.cmml" xref="S3.E1.m1.4.4.4.4.2.1"><minus id="S3.E1.m1.4.4.4.4.2.1.2.cmml" xref="S3.E1.m1.4.4.4.4.2.1"></minus><apply id="S3.E1.m1.4.4.4.4.2.1.1.cmml" xref="S3.E1.m1.4.4.4.4.2.1.1"><times id="S3.E1.m1.4.4.4.4.2.1.1.2.cmml" xref="S3.E1.m1.4.4.4.4.2.1.1.2"></times><ci id="S3.E1.m1.4.4.4.4.2.1.1.3.cmml" xref="S3.E1.m1.4.4.4.4.2.1.1.3">ğ‘™</ci><ci id="S3.E1.m1.4.4.4.4.2.1.1.4.cmml" xref="S3.E1.m1.4.4.4.4.2.1.1.4">ğ‘œ</ci><ci id="S3.E1.m1.4.4.4.4.2.1.1.5.cmml" xref="S3.E1.m1.4.4.4.4.2.1.1.5">ğ‘”</ci><apply id="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.2.1.1.1.1"><minus id="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.2">1</cn><ci id="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.4.4.2.1.1.1.1.1.3">ğ‘¦</ci></apply></apply></apply><ci id="S3.E1.m1.2.2.2.2.1.1b.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><mrow id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><mtext id="S3.E1.m1.2.2.2.2.1.1a.cmml" xref="S3.E1.m1.2.2.2.2.1.1">ifÂ </mtext><mrow id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1"><mrow id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1"><mover accent="true" id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2"><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2.2">y</mi><mo id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.2.1">^</mo></mover><mo id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1">=</mo><mn id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.3">0</mn></mrow><mo id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.1">,</mo></mrow></mrow></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">CE(y,\hat{y})=\begin{cases}-log(y)&amp;\text{if $\hat{y}=1,$}\\
-log(1-y)&amp;\text{if $\hat{y}=0,$}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p1.3" class="ltx_p">where <math id="S3.SS2.SSS2.p1.1.m1.2" class="ltx_Math" alttext="\hat{y}\in\{0,1\}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.2a"><mrow id="S3.SS2.SSS2.p1.1.m1.2.3" xref="S3.SS2.SSS2.p1.1.m1.2.3.cmml"><mover accent="true" id="S3.SS2.SSS2.p1.1.m1.2.3.2" xref="S3.SS2.SSS2.p1.1.m1.2.3.2.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.2.3.2.2" xref="S3.SS2.SSS2.p1.1.m1.2.3.2.2.cmml">y</mi><mo id="S3.SS2.SSS2.p1.1.m1.2.3.2.1" xref="S3.SS2.SSS2.p1.1.m1.2.3.2.1.cmml">^</mo></mover><mo id="S3.SS2.SSS2.p1.1.m1.2.3.1" xref="S3.SS2.SSS2.p1.1.m1.2.3.1.cmml">âˆˆ</mo><mrow id="S3.SS2.SSS2.p1.1.m1.2.3.3.2" xref="S3.SS2.SSS2.p1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.1.m1.2.3.3.2.1" xref="S3.SS2.SSS2.p1.1.m1.2.3.3.1.cmml">{</mo><mn id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.SSS2.p1.1.m1.2.3.3.2.2" xref="S3.SS2.SSS2.p1.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS2.SSS2.p1.1.m1.2.2" xref="S3.SS2.SSS2.p1.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.SSS2.p1.1.m1.2.3.3.2.3" xref="S3.SS2.SSS2.p1.1.m1.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.2b"><apply id="S3.SS2.SSS2.p1.1.m1.2.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.3"><in id="S3.SS2.SSS2.p1.1.m1.2.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.3.1"></in><apply id="S3.SS2.SSS2.p1.1.m1.2.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.3.2"><ci id="S3.SS2.SSS2.p1.1.m1.2.3.2.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.3.2.1">^</ci><ci id="S3.SS2.SSS2.p1.1.m1.2.3.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.3.2.2">ğ‘¦</ci></apply><set id="S3.SS2.SSS2.p1.1.m1.2.3.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.3.3.2"><cn type="integer" id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">0</cn><cn type="integer" id="S3.SS2.SSS2.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.2c">\hat{y}\in\{0,1\}</annotation></semantics></math> is the objective, and <math id="S3.SS2.SSS2.p1.2.m2.2" class="ltx_Math" alttext="y\in\left[0,1\right]" display="inline"><semantics id="S3.SS2.SSS2.p1.2.m2.2a"><mrow id="S3.SS2.SSS2.p1.2.m2.2.3" xref="S3.SS2.SSS2.p1.2.m2.2.3.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.2.3.2" xref="S3.SS2.SSS2.p1.2.m2.2.3.2.cmml">y</mi><mo id="S3.SS2.SSS2.p1.2.m2.2.3.1" xref="S3.SS2.SSS2.p1.2.m2.2.3.1.cmml">âˆˆ</mo><mrow id="S3.SS2.SSS2.p1.2.m2.2.3.3.2" xref="S3.SS2.SSS2.p1.2.m2.2.3.3.1.cmml"><mo id="S3.SS2.SSS2.p1.2.m2.2.3.3.2.1" xref="S3.SS2.SSS2.p1.2.m2.2.3.3.1.cmml">[</mo><mn id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml">0</mn><mo id="S3.SS2.SSS2.p1.2.m2.2.3.3.2.2" xref="S3.SS2.SSS2.p1.2.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS2.SSS2.p1.2.m2.2.2" xref="S3.SS2.SSS2.p1.2.m2.2.2.cmml">1</mn><mo id="S3.SS2.SSS2.p1.2.m2.2.3.3.2.3" xref="S3.SS2.SSS2.p1.2.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.2b"><apply id="S3.SS2.SSS2.p1.2.m2.2.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.3"><in id="S3.SS2.SSS2.p1.2.m2.2.3.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.3.1"></in><ci id="S3.SS2.SSS2.p1.2.m2.2.3.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.3.2">ğ‘¦</ci><interval closure="closed" id="S3.SS2.SSS2.p1.2.m2.2.3.3.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.3.3.2"><cn type="integer" id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1">0</cn><cn type="integer" id="S3.SS2.SSS2.p1.2.m2.2.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.2c">y\in\left[0,1\right]</annotation></semantics></math> is the prediction.
G-RMIÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Papandreou etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite> and PoseFixÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Moon
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> dispose a roundabout that quantifies all the positive samples to 1.
However, such quantifing practice will introduce bias away from the ground truth and bring sub-optimal training (TableÂ <a href="#S4.T6" title="Table 6 â€£ 4.4.2. Advantage of RCE loss â€£ 4.4. Ablation Study â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).
To deal with this obstacle, we reshape the CE loss to a novel formulation, RCE, which is able to supervise the training toward real numbers between 0 and 1.
Specifically, given the prediction error <math id="S3.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="e=y-\hat{y}" display="inline"><semantics id="S3.SS2.SSS2.p1.3.m3.1a"><mrow id="S3.SS2.SSS2.p1.3.m3.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.1.1.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.cmml">e</mi><mo id="S3.SS2.SSS2.p1.3.m3.1.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.1.cmml">=</mo><mrow id="S3.SS2.SSS2.p1.3.m3.1.1.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.2.cmml">y</mi><mo id="S3.SS2.SSS2.p1.3.m3.1.1.3.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.1.cmml">âˆ’</mo><mover accent="true" id="S3.SS2.SSS2.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.2.cmml">y</mi><mo id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.1.cmml">^</mo></mover></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.1b"><apply id="S3.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1"><eq id="S3.SS2.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.1"></eq><ci id="S3.SS2.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.2">ğ‘’</ci><apply id="S3.SS2.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3"><minus id="S3.SS2.SSS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.1"></minus><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.2">ğ‘¦</ci><apply id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3"><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.1">^</ci><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.3.2">ğ‘¦</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.1c">e=y-\hat{y}</annotation></semantics></math>, the RCE loss is formulated by</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.5" class="ltx_Math" alttext="RCE(y,\hat{y})=RCE(e)=-log(1-|e|)." display="block"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.1.cmml"><mrow id="S3.E2.m1.5.5.1.1" xref="S3.E2.m1.5.5.1.1.cmml"><mrow id="S3.E2.m1.5.5.1.1.3" xref="S3.E2.m1.5.5.1.1.3.cmml"><mi id="S3.E2.m1.5.5.1.1.3.2" xref="S3.E2.m1.5.5.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.3.1" xref="S3.E2.m1.5.5.1.1.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.5.5.1.1.3.3" xref="S3.E2.m1.5.5.1.1.3.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.3.1a" xref="S3.E2.m1.5.5.1.1.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.5.5.1.1.3.4" xref="S3.E2.m1.5.5.1.1.3.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.3.1b" xref="S3.E2.m1.5.5.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E2.m1.5.5.1.1.3.5.2" xref="S3.E2.m1.5.5.1.1.3.5.1.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.3.5.2.1" xref="S3.E2.m1.5.5.1.1.3.5.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">y</mi><mo id="S3.E2.m1.5.5.1.1.3.5.2.2" xref="S3.E2.m1.5.5.1.1.3.5.1.cmml">,</mo><mover accent="true" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mi id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">y</mi><mo id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E2.m1.5.5.1.1.3.5.2.3" xref="S3.E2.m1.5.5.1.1.3.5.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.5.5.1.1.4" xref="S3.E2.m1.5.5.1.1.4.cmml">=</mo><mrow id="S3.E2.m1.5.5.1.1.5" xref="S3.E2.m1.5.5.1.1.5.cmml"><mi id="S3.E2.m1.5.5.1.1.5.2" xref="S3.E2.m1.5.5.1.1.5.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.5.1" xref="S3.E2.m1.5.5.1.1.5.1.cmml">â€‹</mo><mi id="S3.E2.m1.5.5.1.1.5.3" xref="S3.E2.m1.5.5.1.1.5.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.5.1a" xref="S3.E2.m1.5.5.1.1.5.1.cmml">â€‹</mo><mi id="S3.E2.m1.5.5.1.1.5.4" xref="S3.E2.m1.5.5.1.1.5.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.5.1b" xref="S3.E2.m1.5.5.1.1.5.1.cmml">â€‹</mo><mrow id="S3.E2.m1.5.5.1.1.5.5.2" xref="S3.E2.m1.5.5.1.1.5.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.5.5.2.1" xref="S3.E2.m1.5.5.1.1.5.cmml">(</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">e</mi><mo stretchy="false" id="S3.E2.m1.5.5.1.1.5.5.2.2" xref="S3.E2.m1.5.5.1.1.5.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.5.5.1.1.6" xref="S3.E2.m1.5.5.1.1.6.cmml">=</mo><mrow id="S3.E2.m1.5.5.1.1.1" xref="S3.E2.m1.5.5.1.1.1.cmml"><mo id="S3.E2.m1.5.5.1.1.1a" xref="S3.E2.m1.5.5.1.1.1.cmml">âˆ’</mo><mrow id="S3.E2.m1.5.5.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.5.5.1.1.1.1.4" xref="S3.E2.m1.5.5.1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.1.2a" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.5.5.1.1.1.1.5" xref="S3.E2.m1.5.5.1.1.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.1.2b" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.1.1.cmml">|</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">e</mi><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.1.1.cmml">|</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m1.5.5.1.2" xref="S3.E2.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.1.1.cmml" xref="S3.E2.m1.5.5.1"><and id="S3.E2.m1.5.5.1.1a.cmml" xref="S3.E2.m1.5.5.1"></and><apply id="S3.E2.m1.5.5.1.1b.cmml" xref="S3.E2.m1.5.5.1"><eq id="S3.E2.m1.5.5.1.1.4.cmml" xref="S3.E2.m1.5.5.1.1.4"></eq><apply id="S3.E2.m1.5.5.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.3"><times id="S3.E2.m1.5.5.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.3.1"></times><ci id="S3.E2.m1.5.5.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2">ğ‘…</ci><ci id="S3.E2.m1.5.5.1.1.3.3.cmml" xref="S3.E2.m1.5.5.1.1.3.3">ğ¶</ci><ci id="S3.E2.m1.5.5.1.1.3.4.cmml" xref="S3.E2.m1.5.5.1.1.3.4">ğ¸</ci><interval closure="open" id="S3.E2.m1.5.5.1.1.3.5.1.cmml" xref="S3.E2.m1.5.5.1.1.3.5.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ğ‘¦</ci><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><ci id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1">^</ci><ci id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2">ğ‘¦</ci></apply></interval></apply><apply id="S3.E2.m1.5.5.1.1.5.cmml" xref="S3.E2.m1.5.5.1.1.5"><times id="S3.E2.m1.5.5.1.1.5.1.cmml" xref="S3.E2.m1.5.5.1.1.5.1"></times><ci id="S3.E2.m1.5.5.1.1.5.2.cmml" xref="S3.E2.m1.5.5.1.1.5.2">ğ‘…</ci><ci id="S3.E2.m1.5.5.1.1.5.3.cmml" xref="S3.E2.m1.5.5.1.1.5.3">ğ¶</ci><ci id="S3.E2.m1.5.5.1.1.5.4.cmml" xref="S3.E2.m1.5.5.1.1.5.4">ğ¸</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">ğ‘’</ci></apply></apply><apply id="S3.E2.m1.5.5.1.1c.cmml" xref="S3.E2.m1.5.5.1"><eq id="S3.E2.m1.5.5.1.1.6.cmml" xref="S3.E2.m1.5.5.1.1.6"></eq><share href="#S3.E2.m1.5.5.1.1.5.cmml" id="S3.E2.m1.5.5.1.1d.cmml" xref="S3.E2.m1.5.5.1"></share><apply id="S3.E2.m1.5.5.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1"><minus id="S3.E2.m1.5.5.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1"></minus><apply id="S3.E2.m1.5.5.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1"><times id="S3.E2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2"></times><ci id="S3.E2.m1.5.5.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3">ğ‘™</ci><ci id="S3.E2.m1.5.5.1.1.1.1.4.cmml" xref="S3.E2.m1.5.5.1.1.1.1.4">ğ‘œ</ci><ci id="S3.E2.m1.5.5.1.1.1.1.5.cmml" xref="S3.E2.m1.5.5.1.1.1.1.5">ğ‘”</ci><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1"><minus id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.2"><abs id="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.2.1"></abs><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">ğ‘’</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">RCE(y,\hat{y})=RCE(e)=-log(1-|e|).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p1.7" class="ltx_p">The RCE loss has three advantages.
Firstly, the objective can be generalized to the real numbers <math id="S3.SS2.SSS2.p1.4.m1.2" class="ltx_Math" alttext="\hat{y}\in\left[0,1\right]" display="inline"><semantics id="S3.SS2.SSS2.p1.4.m1.2a"><mrow id="S3.SS2.SSS2.p1.4.m1.2.3" xref="S3.SS2.SSS2.p1.4.m1.2.3.cmml"><mover accent="true" id="S3.SS2.SSS2.p1.4.m1.2.3.2" xref="S3.SS2.SSS2.p1.4.m1.2.3.2.cmml"><mi id="S3.SS2.SSS2.p1.4.m1.2.3.2.2" xref="S3.SS2.SSS2.p1.4.m1.2.3.2.2.cmml">y</mi><mo id="S3.SS2.SSS2.p1.4.m1.2.3.2.1" xref="S3.SS2.SSS2.p1.4.m1.2.3.2.1.cmml">^</mo></mover><mo id="S3.SS2.SSS2.p1.4.m1.2.3.1" xref="S3.SS2.SSS2.p1.4.m1.2.3.1.cmml">âˆˆ</mo><mrow id="S3.SS2.SSS2.p1.4.m1.2.3.3.2" xref="S3.SS2.SSS2.p1.4.m1.2.3.3.1.cmml"><mo id="S3.SS2.SSS2.p1.4.m1.2.3.3.2.1" xref="S3.SS2.SSS2.p1.4.m1.2.3.3.1.cmml">[</mo><mn id="S3.SS2.SSS2.p1.4.m1.1.1" xref="S3.SS2.SSS2.p1.4.m1.1.1.cmml">0</mn><mo id="S3.SS2.SSS2.p1.4.m1.2.3.3.2.2" xref="S3.SS2.SSS2.p1.4.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS2.SSS2.p1.4.m1.2.2" xref="S3.SS2.SSS2.p1.4.m1.2.2.cmml">1</mn><mo id="S3.SS2.SSS2.p1.4.m1.2.3.3.2.3" xref="S3.SS2.SSS2.p1.4.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.4.m1.2b"><apply id="S3.SS2.SSS2.p1.4.m1.2.3.cmml" xref="S3.SS2.SSS2.p1.4.m1.2.3"><in id="S3.SS2.SSS2.p1.4.m1.2.3.1.cmml" xref="S3.SS2.SSS2.p1.4.m1.2.3.1"></in><apply id="S3.SS2.SSS2.p1.4.m1.2.3.2.cmml" xref="S3.SS2.SSS2.p1.4.m1.2.3.2"><ci id="S3.SS2.SSS2.p1.4.m1.2.3.2.1.cmml" xref="S3.SS2.SSS2.p1.4.m1.2.3.2.1">^</ci><ci id="S3.SS2.SSS2.p1.4.m1.2.3.2.2.cmml" xref="S3.SS2.SSS2.p1.4.m1.2.3.2.2">ğ‘¦</ci></apply><interval closure="closed" id="S3.SS2.SSS2.p1.4.m1.2.3.3.1.cmml" xref="S3.SS2.SSS2.p1.4.m1.2.3.3.2"><cn type="integer" id="S3.SS2.SSS2.p1.4.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.4.m1.1.1">0</cn><cn type="integer" id="S3.SS2.SSS2.p1.4.m1.2.2.cmml" xref="S3.SS2.SSS2.p1.4.m1.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.4.m1.2c">\hat{y}\in\left[0,1\right]</annotation></semantics></math>, thereby the pose estimation training can benefit from RCE loss. One may notice that the CE loss becomes a special case of RCE if the objective <math id="S3.SS2.SSS2.p1.5.m2.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S3.SS2.SSS2.p1.5.m2.1a"><mover accent="true" id="S3.SS2.SSS2.p1.5.m2.1.1" xref="S3.SS2.SSS2.p1.5.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p1.5.m2.1.1.2" xref="S3.SS2.SSS2.p1.5.m2.1.1.2.cmml">y</mi><mo id="S3.SS2.SSS2.p1.5.m2.1.1.1" xref="S3.SS2.SSS2.p1.5.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.5.m2.1b"><apply id="S3.SS2.SSS2.p1.5.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.5.m2.1.1"><ci id="S3.SS2.SSS2.p1.5.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p1.5.m2.1.1.1">^</ci><ci id="S3.SS2.SSS2.p1.5.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p1.5.m2.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.5.m2.1c">\hat{y}</annotation></semantics></math> is restricted to the integers of 0 and 1.
Secondly, the RCE loss alleviates the gradient vanishing issue that caused by the sigmoid mapping (Fig.Â <a href="#S3.F4" title="Figure 4 â€£ 3.2.1. The drawback of MSE for pose estimation â€£ 3.2. RCE Loss â€£ 3. Proposed Method â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
Thirdly, the loss value goes to infinity when <math id="S3.SS2.SSS2.p1.6.m3.1" class="ltx_Math" alttext="|e|" display="inline"><semantics id="S3.SS2.SSS2.p1.6.m3.1a"><mrow id="S3.SS2.SSS2.p1.6.m3.1.2.2" xref="S3.SS2.SSS2.p1.6.m3.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.6.m3.1.2.2.1" xref="S3.SS2.SSS2.p1.6.m3.1.2.1.1.cmml">|</mo><mi id="S3.SS2.SSS2.p1.6.m3.1.1" xref="S3.SS2.SSS2.p1.6.m3.1.1.cmml">e</mi><mo stretchy="false" id="S3.SS2.SSS2.p1.6.m3.1.2.2.2" xref="S3.SS2.SSS2.p1.6.m3.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.6.m3.1b"><apply id="S3.SS2.SSS2.p1.6.m3.1.2.1.cmml" xref="S3.SS2.SSS2.p1.6.m3.1.2.2"><abs id="S3.SS2.SSS2.p1.6.m3.1.2.1.1.cmml" xref="S3.SS2.SSS2.p1.6.m3.1.2.2.1"></abs><ci id="S3.SS2.SSS2.p1.6.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.6.m3.1.1">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.6.m3.1c">|e|</annotation></semantics></math> approaches 1 (Fig.Â <a href="#S3.F4" title="Figure 4 â€£ 3.2.1. The drawback of MSE for pose estimation â€£ 3.2. RCE Loss â€£ 3. Proposed Method â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), and such trend provides the prerequisite for learning from the hard samples with large loss values. In contrast, the MSE loss cannot yield such large loss value on hard samples within the finite support of <math id="S3.SS2.SSS2.p1.7.m4.1" class="ltx_Math" alttext="e" display="inline"><semantics id="S3.SS2.SSS2.p1.7.m4.1a"><mi id="S3.SS2.SSS2.p1.7.m4.1.1" xref="S3.SS2.SSS2.p1.7.m4.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.7.m4.1b"><ci id="S3.SS2.SSS2.p1.7.m4.1.1.cmml" xref="S3.SS2.SSS2.p1.7.m4.1.1">ğ‘’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.7.m4.1c">e</annotation></semantics></math>.
In the following part, we will take this advantage and further develop the RCE loss with the hard sample learning.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span>RCE against hard sample</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.2" class="ltx_p">Hard sample (<span id="S3.SS2.SSS3.p1.2.1" class="ltx_text ltx_font_italic">a.k.a.</span>Â  outlier pixel) is a common issue in training pose estimation network.
The common practice for hard sample learning is to introduce a weighting factor in the loss formulation, where the weighting factor is set proportional to the hard extent.
Following the common practice, we adopt the absolute prediction error <math id="S3.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="|e|" display="inline"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mrow id="S3.SS2.SSS3.p1.1.m1.1.2.2" xref="S3.SS2.SSS3.p1.1.m1.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p1.1.m1.1.2.2.1" xref="S3.SS2.SSS3.p1.1.m1.1.2.1.1.cmml">|</mo><mi id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml">e</mi><mo stretchy="false" id="S3.SS2.SSS3.p1.1.m1.1.2.2.2" xref="S3.SS2.SSS3.p1.1.m1.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><apply id="S3.SS2.SSS3.p1.1.m1.1.2.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.2.2"><abs id="S3.SS2.SSS3.p1.1.m1.1.2.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.2.2.1"></abs><ci id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">|e|</annotation></semantics></math> as the measurement of hard extent.
Owing to the advantage described above, when <math id="S3.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="|e|" display="inline"><semantics id="S3.SS2.SSS3.p1.2.m2.1a"><mrow id="S3.SS2.SSS3.p1.2.m2.1.2.2" xref="S3.SS2.SSS3.p1.2.m2.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p1.2.m2.1.2.2.1" xref="S3.SS2.SSS3.p1.2.m2.1.2.1.1.cmml">|</mo><mi id="S3.SS2.SSS3.p1.2.m2.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.cmml">e</mi><mo stretchy="false" id="S3.SS2.SSS3.p1.2.m2.1.2.2.2" xref="S3.SS2.SSS3.p1.2.m2.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.2.m2.1b"><apply id="S3.SS2.SSS3.p1.2.m2.1.2.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.2.2"><abs id="S3.SS2.SSS3.p1.2.m2.1.2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.2.2.1"></abs><ci id="S3.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.2.m2.1c">|e|</annotation></semantics></math> increases, the large loss value of RCE provides the support of hard sample emphasis.
The final RCE loss is developed in a focal style:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.4" class="ltx_Math" alttext="RCE(e)=-|e|^{\gamma}log(1-|e|)," display="block"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1.3" xref="S3.E3.m1.4.4.1.1.3.cmml"><mi id="S3.E3.m1.4.4.1.1.3.2" xref="S3.E3.m1.4.4.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.3.1" xref="S3.E3.m1.4.4.1.1.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.4.4.1.1.3.3" xref="S3.E3.m1.4.4.1.1.3.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.3.1a" xref="S3.E3.m1.4.4.1.1.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.4.4.1.1.3.4" xref="S3.E3.m1.4.4.1.1.3.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.3.1b" xref="S3.E3.m1.4.4.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E3.m1.4.4.1.1.3.5.2" xref="S3.E3.m1.4.4.1.1.3.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.1.1.3.5.2.1" xref="S3.E3.m1.4.4.1.1.3.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">e</mi><mo stretchy="false" id="S3.E3.m1.4.4.1.1.3.5.2.2" xref="S3.E3.m1.4.4.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.1.1.2" xref="S3.E3.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.4.4.1.1.1" xref="S3.E3.m1.4.4.1.1.1.cmml"><mo id="S3.E3.m1.4.4.1.1.1a" xref="S3.E3.m1.4.4.1.1.1.cmml">âˆ’</mo><mrow id="S3.E3.m1.4.4.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.cmml"><msup id="S3.E3.m1.4.4.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.3.cmml"><mrow id="S3.E3.m1.4.4.1.1.1.1.3.2.2" xref="S3.E3.m1.4.4.1.1.1.1.3.2.1.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.1.1.1.1.3.2.2.1" xref="S3.E3.m1.4.4.1.1.1.1.3.2.1.1.cmml">|</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">e</mi><mo stretchy="false" id="S3.E3.m1.4.4.1.1.1.1.3.2.2.2" xref="S3.E3.m1.4.4.1.1.1.1.3.2.1.1.cmml">|</mo></mrow><mi id="S3.E3.m1.4.4.1.1.1.1.3.3" xref="S3.E3.m1.4.4.1.1.1.1.3.3.cmml">Î³</mi></msup><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E3.m1.4.4.1.1.1.1.4" xref="S3.E3.m1.4.4.1.1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.1.1.2a" xref="S3.E3.m1.4.4.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E3.m1.4.4.1.1.1.1.5" xref="S3.E3.m1.4.4.1.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.1.1.2b" xref="S3.E3.m1.4.4.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E3.m1.4.4.1.1.1.1.6" xref="S3.E3.m1.4.4.1.1.1.1.6.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.1.1.2c" xref="S3.E3.m1.4.4.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.cmml"><mn id="S3.E3.m1.4.4.1.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.1.1.1.1.1.1.1.3.2.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.3.1.1.cmml">|</mo><mi id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml">e</mi><mo stretchy="false" id="S3.E3.m1.4.4.1.1.1.1.1.1.1.3.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.3.1.1.cmml">|</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m1.4.4.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.4.4.1.2" xref="S3.E3.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1"><eq id="S3.E3.m1.4.4.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.2"></eq><apply id="S3.E3.m1.4.4.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.3"><times id="S3.E3.m1.4.4.1.1.3.1.cmml" xref="S3.E3.m1.4.4.1.1.3.1"></times><ci id="S3.E3.m1.4.4.1.1.3.2.cmml" xref="S3.E3.m1.4.4.1.1.3.2">ğ‘…</ci><ci id="S3.E3.m1.4.4.1.1.3.3.cmml" xref="S3.E3.m1.4.4.1.1.3.3">ğ¶</ci><ci id="S3.E3.m1.4.4.1.1.3.4.cmml" xref="S3.E3.m1.4.4.1.1.3.4">ğ¸</ci><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğ‘’</ci></apply><apply id="S3.E3.m1.4.4.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1"><minus id="S3.E3.m1.4.4.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1"></minus><apply id="S3.E3.m1.4.4.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1"><times id="S3.E3.m1.4.4.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.2"></times><apply id="S3.E3.m1.4.4.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.3.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.3">superscript</csymbol><apply id="S3.E3.m1.4.4.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.3.2.2"><abs id="S3.E3.m1.4.4.1.1.1.1.3.2.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.3.2.2.1"></abs><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">ğ‘’</ci></apply><ci id="S3.E3.m1.4.4.1.1.1.1.3.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.3.3">ğ›¾</ci></apply><ci id="S3.E3.m1.4.4.1.1.1.1.4.cmml" xref="S3.E3.m1.4.4.1.1.1.1.4">ğ‘™</ci><ci id="S3.E3.m1.4.4.1.1.1.1.5.cmml" xref="S3.E3.m1.4.4.1.1.1.1.5">ğ‘œ</ci><ci id="S3.E3.m1.4.4.1.1.1.1.6.cmml" xref="S3.E3.m1.4.4.1.1.1.1.6">ğ‘”</ci><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1"><minus id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.3.2"><abs id="S3.E3.m1.4.4.1.1.1.1.1.1.1.3.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.3.2.1"></abs><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">ğ‘’</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">RCE(e)=-|e|^{\gamma}log(1-|e|),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS3.p1.6" class="ltx_p">where the power <math id="S3.SS2.SSS3.p1.3.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS2.SSS3.p1.3.m1.1a"><mi id="S3.SS2.SSS3.p1.3.m1.1.1" xref="S3.SS2.SSS3.p1.3.m1.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.3.m1.1b"><ci id="S3.SS2.SSS3.p1.3.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m1.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.3.m1.1c">\gamma</annotation></semantics></math> is a hyperparameter for modulating the hard sample weighting. In the experiments, we find that a trivial setting of <math id="S3.SS2.SSS3.p1.4.m2.1" class="ltx_Math" alttext="\gamma=1.0" display="inline"><semantics id="S3.SS2.SSS3.p1.4.m2.1a"><mrow id="S3.SS2.SSS3.p1.4.m2.1.1" xref="S3.SS2.SSS3.p1.4.m2.1.1.cmml"><mi id="S3.SS2.SSS3.p1.4.m2.1.1.2" xref="S3.SS2.SSS3.p1.4.m2.1.1.2.cmml">Î³</mi><mo id="S3.SS2.SSS3.p1.4.m2.1.1.1" xref="S3.SS2.SSS3.p1.4.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS3.p1.4.m2.1.1.3" xref="S3.SS2.SSS3.p1.4.m2.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.4.m2.1b"><apply id="S3.SS2.SSS3.p1.4.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m2.1.1"><eq id="S3.SS2.SSS3.p1.4.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m2.1.1.1"></eq><ci id="S3.SS2.SSS3.p1.4.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p1.4.m2.1.1.2">ğ›¾</ci><cn type="float" id="S3.SS2.SSS3.p1.4.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p1.4.m2.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.4.m2.1c">\gamma=1.0</annotation></semantics></math> is enough to bring consistent improvement.
Also, as suggested by Focal lossÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lin
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>, a further boost can be obtained by modulating the weighting factor with respect to positive and negative, <span id="S3.SS2.SSS3.p1.6.1" class="ltx_text ltx_font_italic">e.g.</span>Â  adopting <math id="S3.SS2.SSS3.p1.5.m3.1" class="ltx_Math" alttext="\alpha|e|^{\gamma}" display="inline"><semantics id="S3.SS2.SSS3.p1.5.m3.1a"><mrow id="S3.SS2.SSS3.p1.5.m3.1.2" xref="S3.SS2.SSS3.p1.5.m3.1.2.cmml"><mi id="S3.SS2.SSS3.p1.5.m3.1.2.2" xref="S3.SS2.SSS3.p1.5.m3.1.2.2.cmml">Î±</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.5.m3.1.2.1" xref="S3.SS2.SSS3.p1.5.m3.1.2.1.cmml">â€‹</mo><msup id="S3.SS2.SSS3.p1.5.m3.1.2.3" xref="S3.SS2.SSS3.p1.5.m3.1.2.3.cmml"><mrow id="S3.SS2.SSS3.p1.5.m3.1.2.3.2.2" xref="S3.SS2.SSS3.p1.5.m3.1.2.3.2.1.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p1.5.m3.1.2.3.2.2.1" xref="S3.SS2.SSS3.p1.5.m3.1.2.3.2.1.1.cmml">|</mo><mi id="S3.SS2.SSS3.p1.5.m3.1.1" xref="S3.SS2.SSS3.p1.5.m3.1.1.cmml">e</mi><mo stretchy="false" id="S3.SS2.SSS3.p1.5.m3.1.2.3.2.2.2" xref="S3.SS2.SSS3.p1.5.m3.1.2.3.2.1.1.cmml">|</mo></mrow><mi id="S3.SS2.SSS3.p1.5.m3.1.2.3.3" xref="S3.SS2.SSS3.p1.5.m3.1.2.3.3.cmml">Î³</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.5.m3.1b"><apply id="S3.SS2.SSS3.p1.5.m3.1.2.cmml" xref="S3.SS2.SSS3.p1.5.m3.1.2"><times id="S3.SS2.SSS3.p1.5.m3.1.2.1.cmml" xref="S3.SS2.SSS3.p1.5.m3.1.2.1"></times><ci id="S3.SS2.SSS3.p1.5.m3.1.2.2.cmml" xref="S3.SS2.SSS3.p1.5.m3.1.2.2">ğ›¼</ci><apply id="S3.SS2.SSS3.p1.5.m3.1.2.3.cmml" xref="S3.SS2.SSS3.p1.5.m3.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.5.m3.1.2.3.1.cmml" xref="S3.SS2.SSS3.p1.5.m3.1.2.3">superscript</csymbol><apply id="S3.SS2.SSS3.p1.5.m3.1.2.3.2.1.cmml" xref="S3.SS2.SSS3.p1.5.m3.1.2.3.2.2"><abs id="S3.SS2.SSS3.p1.5.m3.1.2.3.2.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m3.1.2.3.2.2.1"></abs><ci id="S3.SS2.SSS3.p1.5.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m3.1.1">ğ‘’</ci></apply><ci id="S3.SS2.SSS3.p1.5.m3.1.2.3.3.cmml" xref="S3.SS2.SSS3.p1.5.m3.1.2.3.3">ğ›¾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.5.m3.1c">\alpha|e|^{\gamma}</annotation></semantics></math> for positive samples, and <math id="S3.SS2.SSS3.p1.6.m4.2" class="ltx_Math" alttext="(1-\alpha)|e|^{\gamma}" display="inline"><semantics id="S3.SS2.SSS3.p1.6.m4.2a"><mrow id="S3.SS2.SSS3.p1.6.m4.2.2" xref="S3.SS2.SSS3.p1.6.m4.2.2.cmml"><mrow id="S3.SS2.SSS3.p1.6.m4.2.2.1.1" xref="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p1.6.m4.2.2.1.1.2" xref="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1" xref="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.cmml"><mn id="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.2" xref="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.2.cmml">1</mn><mo id="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.1" xref="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.1.cmml">âˆ’</mo><mi id="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.3" xref="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.3.cmml">Î±</mi></mrow><mo stretchy="false" id="S3.SS2.SSS3.p1.6.m4.2.2.1.1.3" xref="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.6.m4.2.2.2" xref="S3.SS2.SSS3.p1.6.m4.2.2.2.cmml">â€‹</mo><msup id="S3.SS2.SSS3.p1.6.m4.2.2.3" xref="S3.SS2.SSS3.p1.6.m4.2.2.3.cmml"><mrow id="S3.SS2.SSS3.p1.6.m4.2.2.3.2.2" xref="S3.SS2.SSS3.p1.6.m4.2.2.3.2.1.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p1.6.m4.2.2.3.2.2.1" xref="S3.SS2.SSS3.p1.6.m4.2.2.3.2.1.1.cmml">|</mo><mi id="S3.SS2.SSS3.p1.6.m4.1.1" xref="S3.SS2.SSS3.p1.6.m4.1.1.cmml">e</mi><mo stretchy="false" id="S3.SS2.SSS3.p1.6.m4.2.2.3.2.2.2" xref="S3.SS2.SSS3.p1.6.m4.2.2.3.2.1.1.cmml">|</mo></mrow><mi id="S3.SS2.SSS3.p1.6.m4.2.2.3.3" xref="S3.SS2.SSS3.p1.6.m4.2.2.3.3.cmml">Î³</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.6.m4.2b"><apply id="S3.SS2.SSS3.p1.6.m4.2.2.cmml" xref="S3.SS2.SSS3.p1.6.m4.2.2"><times id="S3.SS2.SSS3.p1.6.m4.2.2.2.cmml" xref="S3.SS2.SSS3.p1.6.m4.2.2.2"></times><apply id="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m4.2.2.1.1"><minus id="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.1"></minus><cn type="integer" id="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.2">1</cn><ci id="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.6.m4.2.2.1.1.1.3">ğ›¼</ci></apply><apply id="S3.SS2.SSS3.p1.6.m4.2.2.3.cmml" xref="S3.SS2.SSS3.p1.6.m4.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.6.m4.2.2.3.1.cmml" xref="S3.SS2.SSS3.p1.6.m4.2.2.3">superscript</csymbol><apply id="S3.SS2.SSS3.p1.6.m4.2.2.3.2.1.cmml" xref="S3.SS2.SSS3.p1.6.m4.2.2.3.2.2"><abs id="S3.SS2.SSS3.p1.6.m4.2.2.3.2.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m4.2.2.3.2.2.1"></abs><ci id="S3.SS2.SSS3.p1.6.m4.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m4.1.1">ğ‘’</ci></apply><ci id="S3.SS2.SSS3.p1.6.m4.2.2.3.3.cmml" xref="S3.SS2.SSS3.p1.6.m4.2.2.3.3">ğ›¾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.6.m4.2c">(1-\alpha)|e|^{\gamma}</annotation></semantics></math> for negative samples.
It should be noted that the RCE loss is not limited with any specific focal routine, but is friendly to various formulation for hard sample learning.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Experimental Setting</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Datasets</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">We utilized two popular human pose estimation datasets, COCOÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lin etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite> and MPIIÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Andriluka etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite>. The COCO keypoint datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lin etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite> presents naturally challenging imagery data with various human poses, unconstrained environments, different body scales and occlusion patterns. The total objective involves both detecting the person instances and localizing the keypoints, while this work focus on the latter one.
It contains 200k images and 250k person samples. Each person instance is labeled with 17 keypoints. The annotations of training and validation sets are publicly benchmarked. In the evaluation, we follow the commonly used protocol of train2017/val2017/test-dev2017 split. The MPII human pose datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Andriluka etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite> contains 40k person samples, each of which is labeled with 16 keypoints. We follow the standard train/val/test split as inÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tompson
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite>.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Training details</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.9" class="ltx_p">The weights of the backbone part are initialized with the publicly released ResNet model that pre-trained on the ImageNet datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Russakovsky
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2015</span></a>)</cite>. The weights are updated by the Adam optimizer with the mini-batch size of 128. The initial learning rate is set to 1<math id="S4.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\times 10^{-3}" display="inline"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mrow id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p1.1.m1.1.1.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p1.1.m1.1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.cmml">Ã—</mo><msup id="S4.SS1.SSS2.p1.1.m1.1.1.3" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.cmml"><mn id="S4.SS1.SSS2.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.SSS2.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3.cmml"><mo id="S4.SS1.SSS2.p1.1.m1.1.1.3.3a" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="S4.SS1.SSS2.p1.1.m1.1.1.3.3.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><apply id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1"><times id="S4.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S4.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.2">absent</csymbol><apply id="S4.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.2">10</cn><apply id="S4.SS1.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3"><minus id="S4.SS1.SSS2.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS1.SSS2.p1.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">\times 10^{-3}</annotation></semantics></math>, and divided by 10 at 90 and 120th epoch, respectively. The training process is terminated within 140 epochs.
We perform data augmentation including scaling (<math id="S4.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mo id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><csymbol cd="latexml" id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">\pm</annotation></semantics></math> 35%), rotation (<math id="S4.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\pm 45^{\circ}" display="inline"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><mrow id="S4.SS1.SSS2.p1.3.m3.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml"><mo id="S4.SS1.SSS2.p1.3.m3.1.1a" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml">Â±</mo><msup id="S4.SS1.SSS2.p1.3.m3.1.1.2" xref="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml"><mn id="S4.SS1.SSS2.p1.3.m3.1.1.2.2" xref="S4.SS1.SSS2.p1.3.m3.1.1.2.2.cmml">45</mn><mo id="S4.SS1.SSS2.p1.3.m3.1.1.2.3" xref="S4.SS1.SSS2.p1.3.m3.1.1.2.3.cmml">âˆ˜</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.3.m3.1b"><apply id="S4.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1">plus-or-minus</csymbol><apply id="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.3.m3.1.1.2.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.2">superscript</csymbol><cn type="integer" id="S4.SS1.SSS2.p1.3.m3.1.1.2.2.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.2.2">45</cn><compose id="S4.SS1.SSS2.p1.3.m3.1.1.2.3.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.2.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.3.m3.1c">\pm 45^{\circ}</annotation></semantics></math>), and flip on COCO. FollowingÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wang etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, half body data augmentation is involved.
We use two different input sizes (256<math id="S4.SS1.SSS2.p1.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.SSS2.p1.4.m4.1a"><mo id="S4.SS1.SSS2.p1.4.m4.1.1" xref="S4.SS1.SSS2.p1.4.m4.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.4.m4.1b"><times id="S4.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.4.m4.1c">\times</annotation></semantics></math>192, 384<math id="S4.SS1.SSS2.p1.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.SSS2.p1.5.m5.1a"><mo id="S4.SS1.SSS2.p1.5.m5.1.1" xref="S4.SS1.SSS2.p1.5.m5.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.5.m5.1b"><times id="S4.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.5.m5.1c">\times</annotation></semantics></math>288) in our experiments, and adopt the same data preprocessing as inÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>.
Samples from the MPII are augmented during training by using the same scheme as in HourglassÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Newell
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>.
In addition, we transform the plain regression output <math id="S4.SS1.SSS2.p1.6.m6.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS1.SSS2.p1.6.m6.1a"><mi id="S4.SS1.SSS2.p1.6.m6.1.1" xref="S4.SS1.SSS2.p1.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.6.m6.1b"><ci id="S4.SS1.SSS2.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS2.p1.6.m6.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.6.m6.1c">x</annotation></semantics></math> with <math id="S4.SS1.SSS2.p1.7.m7.1" class="ltx_Math" alttext="sigmoid(x)" display="inline"><semantics id="S4.SS1.SSS2.p1.7.m7.1a"><mrow id="S4.SS1.SSS2.p1.7.m7.1.2" xref="S4.SS1.SSS2.p1.7.m7.1.2.cmml"><mi id="S4.SS1.SSS2.p1.7.m7.1.2.2" xref="S4.SS1.SSS2.p1.7.m7.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p1.7.m7.1.2.1" xref="S4.SS1.SSS2.p1.7.m7.1.2.1.cmml">â€‹</mo><mi id="S4.SS1.SSS2.p1.7.m7.1.2.3" xref="S4.SS1.SSS2.p1.7.m7.1.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p1.7.m7.1.2.1a" xref="S4.SS1.SSS2.p1.7.m7.1.2.1.cmml">â€‹</mo><mi id="S4.SS1.SSS2.p1.7.m7.1.2.4" xref="S4.SS1.SSS2.p1.7.m7.1.2.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p1.7.m7.1.2.1b" xref="S4.SS1.SSS2.p1.7.m7.1.2.1.cmml">â€‹</mo><mi id="S4.SS1.SSS2.p1.7.m7.1.2.5" xref="S4.SS1.SSS2.p1.7.m7.1.2.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p1.7.m7.1.2.1c" xref="S4.SS1.SSS2.p1.7.m7.1.2.1.cmml">â€‹</mo><mi id="S4.SS1.SSS2.p1.7.m7.1.2.6" xref="S4.SS1.SSS2.p1.7.m7.1.2.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p1.7.m7.1.2.1d" xref="S4.SS1.SSS2.p1.7.m7.1.2.1.cmml">â€‹</mo><mi id="S4.SS1.SSS2.p1.7.m7.1.2.7" xref="S4.SS1.SSS2.p1.7.m7.1.2.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p1.7.m7.1.2.1e" xref="S4.SS1.SSS2.p1.7.m7.1.2.1.cmml">â€‹</mo><mi id="S4.SS1.SSS2.p1.7.m7.1.2.8" xref="S4.SS1.SSS2.p1.7.m7.1.2.8.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p1.7.m7.1.2.1f" xref="S4.SS1.SSS2.p1.7.m7.1.2.1.cmml">â€‹</mo><mrow id="S4.SS1.SSS2.p1.7.m7.1.2.9.2" xref="S4.SS1.SSS2.p1.7.m7.1.2.cmml"><mo stretchy="false" id="S4.SS1.SSS2.p1.7.m7.1.2.9.2.1" xref="S4.SS1.SSS2.p1.7.m7.1.2.cmml">(</mo><mi id="S4.SS1.SSS2.p1.7.m7.1.1" xref="S4.SS1.SSS2.p1.7.m7.1.1.cmml">x</mi><mo stretchy="false" id="S4.SS1.SSS2.p1.7.m7.1.2.9.2.2" xref="S4.SS1.SSS2.p1.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.7.m7.1b"><apply id="S4.SS1.SSS2.p1.7.m7.1.2.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.2"><times id="S4.SS1.SSS2.p1.7.m7.1.2.1.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.2.1"></times><ci id="S4.SS1.SSS2.p1.7.m7.1.2.2.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.2.2">ğ‘ </ci><ci id="S4.SS1.SSS2.p1.7.m7.1.2.3.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.2.3">ğ‘–</ci><ci id="S4.SS1.SSS2.p1.7.m7.1.2.4.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.2.4">ğ‘”</ci><ci id="S4.SS1.SSS2.p1.7.m7.1.2.5.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.2.5">ğ‘š</ci><ci id="S4.SS1.SSS2.p1.7.m7.1.2.6.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.2.6">ğ‘œ</ci><ci id="S4.SS1.SSS2.p1.7.m7.1.2.7.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.2.7">ğ‘–</ci><ci id="S4.SS1.SSS2.p1.7.m7.1.2.8.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.2.8">ğ‘‘</ci><ci id="S4.SS1.SSS2.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.1">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.7.m7.1c">sigmoid(x)</annotation></semantics></math> when using RCE loss. In this case, we fix <math id="S4.SS1.SSS2.p1.8.m8.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.SSS2.p1.8.m8.1a"><mi id="S4.SS1.SSS2.p1.8.m8.1.1" xref="S4.SS1.SSS2.p1.8.m8.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.8.m8.1b"><ci id="S4.SS1.SSS2.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS2.p1.8.m8.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.8.m8.1c">\alpha</annotation></semantics></math>=0.7, <math id="S4.SS1.SSS2.p1.9.m9.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S4.SS1.SSS2.p1.9.m9.1a"><mi id="S4.SS1.SSS2.p1.9.m9.1.1" xref="S4.SS1.SSS2.p1.9.m9.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.9.m9.1b"><ci id="S4.SS1.SSS2.p1.9.m9.1.1.cmml" xref="S4.SS1.SSS2.p1.9.m9.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.9.m9.1c">\gamma</annotation></semantics></math>=1.0 in Eq.Â <a href="#S3.E3" title="In 3.2.3. RCE against hard sample â€£ 3.2. RCE Loss â€£ 3. Proposed Method â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> according to the validation result, and find it stably performs well throughout the experiments.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Evaluation</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">We use the standard average precision (AP) on COCO and the percentage of correct keypoints (PCK) on MPII to evaluate the accuracy.
The model efficiency is assessed in multiple terms, including the FLOPs, the parameter number.
and the inference time per image measured on a single NVIDIA 2080ti GPU.
The top-down paradigm is used on COCO: it detects the person instance with a human detector, and then predicts the keypoints. We use the same person detectorsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Faster</span>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2015</span></a>)</cite> provided by SimpleBaseline for both validation set and test-dev set.
Also the same with SimpleBaseline, we compute the heatmap by averaging the heatmap of the original and flipped images.
Then we implement a quarter offset from the highest response to the second highest one to get the locations of keypoints. The final confidence score is obtained by the multiplication of the averaged score of keypoints and the bounding box score.
The testing procedure of MPII is generally the same to that in COCO except that we follow the standard testing strategy with the provided person boxes instead of detected person boxes.</p>
</div>
</section>
<section id="S4.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4. </span>Contrast methods</h4>

<div id="S4.SS1.SSS4.p1" class="ltx_para">
<p id="S4.SS1.SSS4.p1.1" class="ltx_p">The success of SimpleBaselineÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> has provided the prior knowledge on how to design an efficient network for human pose estimation.
Inspired by this design, LPNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> moves further towards the lightweight models.
Apart from them, the remaining pose estimation methods in literature pursue the accuracy with heavy weights, and seldom notices the efficiency issue.
Therefore, our experiments focus on the comparison with SimpleBaseline and LPN, in order to validate the advantages of FasterPose with respect to the efficiency.
Besides, many state-of-the-art methods are involved in the comparison experiments, such as HRNetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, RSNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, G-RMIÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Papandreou etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>, Integral Pose RegressionÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, RMPEÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Fang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>, CPNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chen
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, Lite-HRNetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, <span id="S4.SS1.SSS4.p1.1.1" class="ltx_text ltx_font_italic">etc</span>.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>COCO Keypoint Detection</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Results on validation set</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.3" class="ltx_p">The comparison between SimpleBaseline and FasterPose is shown in TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.2.1. Results on validation set â€£ 4.2. COCO Keypoint Detection â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. It is obvious that FasterPose reduces the computational cost, model size and inference time dramatically, while improving the accuracy.
FasterPose benefits from:
(1) the LHR module directly uses LR features for regression, greatly reduces the computational cost;
(2) the RCE loss significantly improve the convergence.
Given the input size of either 256 <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mo id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><times id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">\times</annotation></semantics></math>192 or 384<math id="S4.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mo id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><times id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">\times</annotation></semantics></math>288, FasterPose consistently yields higher AP than the counterpart with various backbone architectures.
Meanwhile, the FLOPs, parameter number, and inference time are significantly reduced by FasterPose.
In order to verify the generalization ability of FasterPose, we also carried out experiments on LPNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
Different with SimpleBaseline, LPNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> replace the standard bottleneck blocks used in the backbone with a lightweight bottleneck blocks when downsampling. During the upsampling process, LPN replace each deconvolutional layer with a combination of a group deconvolutional layer. Hence, LPN has much less model size and computational complexity than other top-performing networks.
The results of the comparison with LPN and FasterPose are shown in the TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.2.1. Results on validation set â€£ 4.2. COCO Keypoint Detection â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, FasterPose achieves the AP increase of 0.8% (67.6<math id="S4.SS2.SSS1.p1.3.m3.1" class="ltx_math_unparsed" alttext="\%\to" display="inline"><semantics id="S4.SS2.SSS1.p1.3.m3.1a"><mrow id="S4.SS2.SSS1.p1.3.m3.1b"><mo id="S4.SS2.SSS1.p1.3.m3.1.1">%</mo><mo stretchy="false" id="S4.SS2.SSS1.p1.3.m3.1.2">â†’</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m3.1c">\%\to</annotation></semantics></math> 68.2%), and only needs 80% FLOPs than that of LPN.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>
Comparison of LPN, SimpleBaseline and FasterPose on COCO validation set.
The results show that FasterPose has consistent advantage of accuracy and efficiency with various backbone architectures and input size.
</figcaption>
<table id="S4.T2.14.14" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.14.14.15.1" class="ltx_tr">
<th id="S4.T2.14.14.15.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Method</th>
<th id="S4.T2.14.14.15.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Input Size</th>
<th id="S4.T2.14.14.15.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Backbone</th>
<th id="S4.T2.14.14.15.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">GFLOPs</th>
<th id="S4.T2.14.14.15.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">#Params</th>
<th id="S4.T2.14.14.15.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Infer. time(ms)</th>
<th id="S4.T2.14.14.15.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP(%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">LPN</th>
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">256 <math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><times id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\times</annotation></semantics></math> 192</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">lpn-50</th>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">1.0</td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">2.91M</td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">4.8</td>
<td id="S4.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">67.6</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">256 <math id="S4.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.2.2.2.1.m1.1a"><mo id="S4.T2.2.2.2.1.m1.1.1" xref="S4.T2.2.2.2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><times id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">\times</annotation></semantics></math> 192</th>
<th id="S4.T2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">lpn-50</th>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.2.2.2.4.1" class="ltx_text ltx_font_bold">0.8</span></td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.2.2.2.5.1" class="ltx_text ltx_font_bold">2.90M</span></td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.2.2.2.6.1" class="ltx_text ltx_font_bold">4.6</span></td>
<td id="S4.T2.2.2.2.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.2.2.2.7.1" class="ltx_text ltx_font_bold">68.2</span></td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<th id="S4.T2.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">SimpleBaseline</th>
<th id="S4.T2.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">256 <math id="S4.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.3.3.3.1.m1.1a"><mo id="S4.T2.3.3.3.1.m1.1.1" xref="S4.T2.3.3.3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><times id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">\times</annotation></semantics></math> 192</th>
<th id="S4.T2.3.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-50</th>
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">9.0</td>
<td id="S4.T2.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">34.0M</td>
<td id="S4.T2.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">7.5</td>
<td id="S4.T2.3.3.3.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">70.4</td>
</tr>
<tr id="S4.T2.4.4.4" class="ltx_tr">
<th id="S4.T2.4.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SimpleBaseline</th>
<th id="S4.T2.4.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">256 <math id="S4.T2.4.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.4.4.4.1.m1.1a"><mo id="S4.T2.4.4.4.1.m1.1.1" xref="S4.T2.4.4.4.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.1b"><times id="S4.T2.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.1c">\times</annotation></semantics></math> 192</th>
<th id="S4.T2.4.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-101</th>
<td id="S4.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">12.4</td>
<td id="S4.T2.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">53.0M</td>
<td id="S4.T2.4.4.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">15.0</td>
<td id="S4.T2.4.4.4.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">71.4</td>
</tr>
<tr id="S4.T2.5.5.5" class="ltx_tr">
<th id="S4.T2.5.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SimpleBaseline</th>
<th id="S4.T2.5.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">256 <math id="S4.T2.5.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.5.5.5.1.m1.1a"><mo id="S4.T2.5.5.5.1.m1.1.1" xref="S4.T2.5.5.5.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.1.m1.1b"><times id="S4.T2.5.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.1.m1.1c">\times</annotation></semantics></math> 192</th>
<th id="S4.T2.5.5.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-152</th>
<td id="S4.T2.5.5.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">15.8</td>
<td id="S4.T2.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.6M</td>
<td id="S4.T2.5.5.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">20.9</td>
<td id="S4.T2.5.5.5.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">72.0</td>
</tr>
<tr id="S4.T2.6.6.6" class="ltx_tr">
<th id="S4.T2.6.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T2.6.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">256 <math id="S4.T2.6.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.6.6.6.1.m1.1a"><mo id="S4.T2.6.6.6.1.m1.1.1" xref="S4.T2.6.6.6.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.1.m1.1b"><times id="S4.T2.6.6.6.1.m1.1.1.cmml" xref="S4.T2.6.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.1.m1.1c">\times</annotation></semantics></math> 192</th>
<th id="S4.T2.6.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-50</th>
<td id="S4.T2.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.6.6.6.4.1" class="ltx_text ltx_font_bold">3.8</span></td>
<td id="S4.T2.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.6.6.6.5.1" class="ltx_text ltx_font_bold">25.7M</span></td>
<td id="S4.T2.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.6.6.6.6.1" class="ltx_text ltx_font_bold">6.4</span></td>
<td id="S4.T2.6.6.6.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.6.6.6.7.1" class="ltx_text ltx_font_bold">71.7</span></td>
</tr>
<tr id="S4.T2.7.7.7" class="ltx_tr">
<th id="S4.T2.7.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T2.7.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">256 <math id="S4.T2.7.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.7.7.7.1.m1.1a"><mo id="S4.T2.7.7.7.1.m1.1.1" xref="S4.T2.7.7.7.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.1.m1.1b"><times id="S4.T2.7.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.1.m1.1c">\times</annotation></semantics></math> 192</th>
<th id="S4.T2.7.7.7.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-101</th>
<td id="S4.T2.7.7.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.7.7.7.4.1" class="ltx_text ltx_font_bold">7.2</span></td>
<td id="S4.T2.7.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.7.7.7.5.1" class="ltx_text ltx_font_bold">44.7M</span></td>
<td id="S4.T2.7.7.7.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.7.7.7.6.1" class="ltx_text ltx_font_bold">13.0</span></td>
<td id="S4.T2.7.7.7.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.7.7.7.7.1" class="ltx_text ltx_font_bold">73.5</span></td>
</tr>
<tr id="S4.T2.8.8.8" class="ltx_tr">
<th id="S4.T2.8.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T2.8.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">256 <math id="S4.T2.8.8.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.8.8.8.1.m1.1a"><mo id="S4.T2.8.8.8.1.m1.1.1" xref="S4.T2.8.8.8.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.1.m1.1b"><times id="S4.T2.8.8.8.1.m1.1.1.cmml" xref="S4.T2.8.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.1.m1.1c">\times</annotation></semantics></math> 192</th>
<th id="S4.T2.8.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-152</th>
<td id="S4.T2.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.8.8.8.4.1" class="ltx_text ltx_font_bold">10.6</span></td>
<td id="S4.T2.8.8.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.8.8.8.5.1" class="ltx_text ltx_font_bold">60.4M</span></td>
<td id="S4.T2.8.8.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.8.8.8.6.1" class="ltx_text ltx_font_bold">18.7</span></td>
<td id="S4.T2.8.8.8.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.8.8.8.7.1" class="ltx_text ltx_font_bold">74.2</span></td>
</tr>
<tr id="S4.T2.9.9.9" class="ltx_tr">
<th id="S4.T2.9.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">SimpleBaseline</th>
<th id="S4.T2.9.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">384 <math id="S4.T2.9.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.9.9.9.1.m1.1a"><mo id="S4.T2.9.9.9.1.m1.1.1" xref="S4.T2.9.9.9.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.1.m1.1b"><times id="S4.T2.9.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.1.m1.1c">\times</annotation></semantics></math> 288</th>
<th id="S4.T2.9.9.9.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-50</th>
<td id="S4.T2.9.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">20.2</td>
<td id="S4.T2.9.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">34.0M</td>
<td id="S4.T2.9.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">9.4</td>
<td id="S4.T2.9.9.9.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">72.2</td>
</tr>
<tr id="S4.T2.10.10.10" class="ltx_tr">
<th id="S4.T2.10.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SimpleBaseline</th>
<th id="S4.T2.10.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">384 <math id="S4.T2.10.10.10.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.10.10.10.1.m1.1a"><mo id="S4.T2.10.10.10.1.m1.1.1" xref="S4.T2.10.10.10.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.1.m1.1b"><times id="S4.T2.10.10.10.1.m1.1.1.cmml" xref="S4.T2.10.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.1.m1.1c">\times</annotation></semantics></math> 288</th>
<th id="S4.T2.10.10.10.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-101</th>
<td id="S4.T2.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">27.9</td>
<td id="S4.T2.10.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">53.0M</td>
<td id="S4.T2.10.10.10.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">17.5</td>
<td id="S4.T2.10.10.10.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">73.6</td>
</tr>
<tr id="S4.T2.11.11.11" class="ltx_tr">
<th id="S4.T2.11.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SimpleBaseline</th>
<th id="S4.T2.11.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">384 <math id="S4.T2.11.11.11.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.11.11.11.1.m1.1a"><mo id="S4.T2.11.11.11.1.m1.1.1" xref="S4.T2.11.11.11.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.11.1.m1.1b"><times id="S4.T2.11.11.11.1.m1.1.1.cmml" xref="S4.T2.11.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.11.1.m1.1c">\times</annotation></semantics></math> 288</th>
<th id="S4.T2.11.11.11.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-152</th>
<td id="S4.T2.11.11.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">35.5</td>
<td id="S4.T2.11.11.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.6M</td>
<td id="S4.T2.11.11.11.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">24.1</td>
<td id="S4.T2.11.11.11.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">74.3</td>
</tr>
<tr id="S4.T2.12.12.12" class="ltx_tr">
<th id="S4.T2.12.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T2.12.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">384 <math id="S4.T2.12.12.12.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.12.12.12.1.m1.1a"><mo id="S4.T2.12.12.12.1.m1.1.1" xref="S4.T2.12.12.12.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.12.1.m1.1b"><times id="S4.T2.12.12.12.1.m1.1.1.cmml" xref="S4.T2.12.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.12.1.m1.1c">\times</annotation></semantics></math> 288</th>
<th id="S4.T2.12.12.12.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-50</th>
<td id="S4.T2.12.12.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.12.12.12.4.1" class="ltx_text ltx_font_bold">8.6</span></td>
<td id="S4.T2.12.12.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.12.12.12.5.1" class="ltx_text ltx_font_bold">25.7M</span></td>
<td id="S4.T2.12.12.12.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.12.12.12.6.1" class="ltx_text ltx_font_bold">7.5</span></td>
<td id="S4.T2.12.12.12.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.12.12.12.7.1" class="ltx_text ltx_font_bold">72.7</span></td>
</tr>
<tr id="S4.T2.13.13.13" class="ltx_tr">
<th id="S4.T2.13.13.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T2.13.13.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">384 <math id="S4.T2.13.13.13.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.13.13.13.1.m1.1a"><mo id="S4.T2.13.13.13.1.m1.1.1" xref="S4.T2.13.13.13.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.13.13.13.1.m1.1b"><times id="S4.T2.13.13.13.1.m1.1.1.cmml" xref="S4.T2.13.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.13.13.1.m1.1c">\times</annotation></semantics></math> 288</th>
<th id="S4.T2.13.13.13.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-101</th>
<td id="S4.T2.13.13.13.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.13.13.13.4.1" class="ltx_text ltx_font_bold">16.2</span></td>
<td id="S4.T2.13.13.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.13.13.13.5.1" class="ltx_text ltx_font_bold">44.7M</span></td>
<td id="S4.T2.13.13.13.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.13.13.13.6.1" class="ltx_text ltx_font_bold">14.3</span></td>
<td id="S4.T2.13.13.13.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.13.13.13.7.1" class="ltx_text ltx_font_bold">75.0</span></td>
</tr>
<tr id="S4.T2.14.14.14" class="ltx_tr">
<th id="S4.T2.14.14.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T2.14.14.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">384 <math id="S4.T2.14.14.14.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.14.14.14.1.m1.1a"><mo id="S4.T2.14.14.14.1.m1.1.1" xref="S4.T2.14.14.14.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T2.14.14.14.1.m1.1b"><times id="S4.T2.14.14.14.1.m1.1.1.cmml" xref="S4.T2.14.14.14.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.14.14.1.m1.1c">\times</annotation></semantics></math> 288</th>
<th id="S4.T2.14.14.14.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-152</th>
<td id="S4.T2.14.14.14.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.14.14.14.4.1" class="ltx_text ltx_font_bold">23.9</span></td>
<td id="S4.T2.14.14.14.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.14.14.14.5.1" class="ltx_text ltx_font_bold">60.4M</span></td>
<td id="S4.T2.14.14.14.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.14.14.14.6.1" class="ltx_text ltx_font_bold">21.5</span></td>
<td id="S4.T2.14.14.14.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T2.14.14.14.7.1" class="ltx_text ltx_font_bold">75.6</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>
Comparison with state of the art on COCO test-dev set. FasterPose reduces the computational cost dramatically, while keeping the accuracy competitive.
Note that RSN involves the heavy complexity that far outweighs the others. The accuracy is reported in percentage.
</figcaption>
<div id="S4.T3.21.21" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:171.7pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-148.2pt,58.5pt) scale(0.593992109220544,0.593992109220544) ;">
<table id="S4.T3.21.21.21" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.4.4.4.4" class="ltx_tr">
<th id="S4.T3.4.4.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Method</th>
<th id="S4.T3.4.4.4.4.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Backbone</th>
<td id="S4.T3.4.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">lightning</td>
<td id="S4.T3.4.4.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Input size</td>
<td id="S4.T3.4.4.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">#Params</td>
<td id="S4.T3.4.4.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">GFLOPs</td>
<td id="S4.T3.4.4.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP</td>
<td id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP<sup id="S4.T3.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T3.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sup>
</td>
<td id="S4.T3.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP<sup id="S4.T3.2.2.2.2.2.1" class="ltx_sup"><span id="S4.T3.2.2.2.2.2.1.1" class="ltx_text ltx_font_italic">75</span></sup>
</td>
<td id="S4.T3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP<sup id="S4.T3.3.3.3.3.3.1" class="ltx_sup"><span id="S4.T3.3.3.3.3.3.1.1" class="ltx_text ltx_font_italic">M</span></sup>
</td>
<td id="S4.T3.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP<sup id="S4.T3.4.4.4.4.4.1" class="ltx_sup"><span id="S4.T3.4.4.4.4.4.1.1" class="ltx_text ltx_font_italic">L</span></sup>
</td>
<td id="S4.T3.4.4.4.4.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AR</td>
</tr>
<tr id="S4.T3.5.5.5.5" class="ltx_tr">
<th id="S4.T3.5.5.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">CPNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chen
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>
</th>
<th id="S4.T3.5.5.5.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-Inc</th>
<td id="S4.T3.5.5.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.5.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">384<math id="S4.T3.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.5.5.5.5.1.m1.1a"><mo id="S4.T3.5.5.5.5.1.m1.1.1" xref="S4.T3.5.5.5.5.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.5.1.m1.1b"><times id="S4.T3.5.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.5.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.5.5.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.5.5.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">72.6</td>
<td id="S4.T3.5.5.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">86.1</td>
<td id="S4.T3.5.5.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">69.7</td>
<td id="S4.T3.5.5.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">78.3</td>
<td id="S4.T3.5.5.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">64.1</td>
<td id="S4.T3.5.5.5.5.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
</tr>
<tr id="S4.T3.6.6.6.6" class="ltx_tr">
<th id="S4.T3.6.6.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">CPNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chen
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> (ensemble)</th>
<th id="S4.T3.6.6.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-Inc</th>
<td id="S4.T3.6.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.6.6.6.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">384<math id="S4.T3.6.6.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.6.6.6.6.1.m1.1a"><mo id="S4.T3.6.6.6.6.1.m1.1.1" xref="S4.T3.6.6.6.6.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.6.1.m1.1b"><times id="S4.T3.6.6.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.6.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.6.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.6.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.6.6.6.6.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">73.0</td>
<td id="S4.T3.6.6.6.6.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">91.7</td>
<td id="S4.T3.6.6.6.6.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">80.9</td>
<td id="S4.T3.6.6.6.6.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.5</td>
<td id="S4.T3.6.6.6.6.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.1</td>
<td id="S4.T3.6.6.6.6.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">79.0</td>
</tr>
<tr id="S4.T3.8.8.8.8" class="ltx_tr">
<th id="S4.T3.8.8.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">RSNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>
</th>
<th id="S4.T3.7.7.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">4<math id="S4.T3.7.7.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.7.7.7.7.1.m1.1a"><mo id="S4.T3.7.7.7.7.1.m1.1.1" xref="S4.T3.7.7.7.7.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.7.1.m1.1b"><times id="S4.T3.7.7.7.7.1.m1.1.1.cmml" xref="S4.T3.7.7.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.7.1.m1.1c">\times</annotation></semantics></math> RSN-50</th>
<td id="S4.T3.8.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.8.8.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">384<math id="S4.T3.8.8.8.8.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.8.8.8.8.2.m1.1a"><mo id="S4.T3.8.8.8.8.2.m1.1.1" xref="S4.T3.8.8.8.8.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.8.8.2.m1.1b"><times id="S4.T3.8.8.8.8.2.m1.1.1.cmml" xref="S4.T3.8.8.8.8.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.8.8.2.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.8.8.8.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">111.8M</td>
<td id="S4.T3.8.8.8.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">65.9</td>
<td id="S4.T3.8.8.8.8.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.6</td>
<td id="S4.T3.8.8.8.8.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">94.3</td>
<td id="S4.T3.8.8.8.8.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">86.6</td>
<td id="S4.T3.8.8.8.8.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">75.5</td>
<td id="S4.T3.8.8.8.8.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">83.3</td>
<td id="S4.T3.8.8.8.8.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">83.8</td>
</tr>
<tr id="S4.T3.9.9.9.9" class="ltx_tr">
<th id="S4.T3.9.9.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">G-RMIÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Papandreou etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>
</th>
<th id="S4.T3.9.9.9.9.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-101</th>
<td id="S4.T3.9.9.9.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.9.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">353<math id="S4.T3.9.9.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.9.9.9.9.1.m1.1a"><mo id="S4.T3.9.9.9.9.1.m1.1.1" xref="S4.T3.9.9.9.9.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.9.9.9.9.1.m1.1b"><times id="S4.T3.9.9.9.9.1.m1.1.1.cmml" xref="S4.T3.9.9.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.9.9.9.1.m1.1c">\times</annotation></semantics></math>257</td>
<td id="S4.T3.9.9.9.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">42.6M</td>
<td id="S4.T3.9.9.9.9.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">57.0</td>
<td id="S4.T3.9.9.9.9.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">64.9</td>
<td id="S4.T3.9.9.9.9.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">85.5</td>
<td id="S4.T3.9.9.9.9.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.3</td>
<td id="S4.T3.9.9.9.9.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">62.3</td>
<td id="S4.T3.9.9.9.9.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.0</td>
<td id="S4.T3.9.9.9.9.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">69.7</td>
</tr>
<tr id="S4.T3.10.10.10.10" class="ltx_tr">
<th id="S4.T3.10.10.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">G-RMIÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Papandreou etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>+extra data</th>
<th id="S4.T3.10.10.10.10.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-101</th>
<td id="S4.T3.10.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.10.10.10.10.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">353<math id="S4.T3.10.10.10.10.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.10.10.10.10.1.m1.1a"><mo id="S4.T3.10.10.10.10.1.m1.1.1" xref="S4.T3.10.10.10.10.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.10.10.10.10.1.m1.1b"><times id="S4.T3.10.10.10.10.1.m1.1.1.cmml" xref="S4.T3.10.10.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.10.10.10.1.m1.1c">\times</annotation></semantics></math>257</td>
<td id="S4.T3.10.10.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">42.6M</td>
<td id="S4.T3.10.10.10.10.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">57.0</td>
<td id="S4.T3.10.10.10.10.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.5</td>
<td id="S4.T3.10.10.10.10.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">87.1</td>
<td id="S4.T3.10.10.10.10.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">75.5</td>
<td id="S4.T3.10.10.10.10.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">65.8</td>
<td id="S4.T3.10.10.10.10.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">73.3</td>
<td id="S4.T3.10.10.10.10.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">73.3</td>
</tr>
<tr id="S4.T3.11.11.11.11" class="ltx_tr">
<th id="S4.T3.11.11.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SimpleBaselineÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>
</th>
<th id="S4.T3.11.11.11.11.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-152</th>
<td id="S4.T3.11.11.11.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.11.11.11.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">384<math id="S4.T3.11.11.11.11.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.11.11.11.11.1.m1.1a"><mo id="S4.T3.11.11.11.11.1.m1.1.1" xref="S4.T3.11.11.11.11.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.11.11.11.11.1.m1.1b"><times id="S4.T3.11.11.11.11.1.m1.1.1.cmml" xref="S4.T3.11.11.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.11.11.11.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.11.11.11.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.6M</td>
<td id="S4.T3.11.11.11.11.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">35.6</td>
<td id="S4.T3.11.11.11.11.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">73.7</td>
<td id="S4.T3.11.11.11.11.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">91.9</td>
<td id="S4.T3.11.11.11.11.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">81.1</td>
<td id="S4.T3.11.11.11.11.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.3</td>
<td id="S4.T3.11.11.11.11.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">80.0</td>
<td id="S4.T3.11.11.11.11.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">79.0</td>
</tr>
<tr id="S4.T3.12.12.12.12" class="ltx_tr">
<th id="S4.T3.12.12.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">HRNetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>
</th>
<th id="S4.T3.12.12.12.12.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">HRNet-W48</th>
<td id="S4.T3.12.12.12.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.12.12.12.12.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">384<math id="S4.T3.12.12.12.12.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.12.12.12.12.1.m1.1a"><mo id="S4.T3.12.12.12.12.1.m1.1.1" xref="S4.T3.12.12.12.12.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.12.12.12.12.1.m1.1b"><times id="S4.T3.12.12.12.12.1.m1.1.1.cmml" xref="S4.T3.12.12.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.12.12.12.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.12.12.12.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">63.6M</td>
<td id="S4.T3.12.12.12.12.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">32.9</td>
<td id="S4.T3.12.12.12.12.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">75.5</td>
<td id="S4.T3.12.12.12.12.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">92.5</td>
<td id="S4.T3.12.12.12.12.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">83.3</td>
<td id="S4.T3.12.12.12.12.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.9</td>
<td id="S4.T3.12.12.12.12.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">81.5</td>
<td id="S4.T3.12.12.12.12.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">80.5</td>
</tr>
<tr id="S4.T3.14.14.14.14" class="ltx_tr">
<th id="S4.T3.14.14.14.14.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">RSNÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>
</th>
<th id="S4.T3.13.13.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">4<math id="S4.T3.13.13.13.13.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.13.13.13.13.1.m1.1a"><mo id="S4.T3.13.13.13.13.1.m1.1.1" xref="S4.T3.13.13.13.13.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.13.13.13.13.1.m1.1b"><times id="S4.T3.13.13.13.13.1.m1.1.1.cmml" xref="S4.T3.13.13.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.13.13.13.1.m1.1c">\times</annotation></semantics></math> RSN-50</th>
<td id="S4.T3.14.14.14.14.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.14.14.14.14.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">256<math id="S4.T3.14.14.14.14.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.14.14.14.14.2.m1.1a"><mo id="S4.T3.14.14.14.14.2.m1.1.1" xref="S4.T3.14.14.14.14.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.14.14.14.14.2.m1.1b"><times id="S4.T3.14.14.14.14.2.m1.1.1.cmml" xref="S4.T3.14.14.14.14.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.14.14.14.2.m1.1c">\times</annotation></semantics></math>192</td>
<td id="S4.T3.14.14.14.14.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">111.8M</td>
<td id="S4.T3.14.14.14.14.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">29.3</td>
<td id="S4.T3.14.14.14.14.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.0</td>
<td id="S4.T3.14.14.14.14.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">94.2</td>
<td id="S4.T3.14.14.14.14.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">86.5</td>
<td id="S4.T3.14.14.14.14.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">75.3</td>
<td id="S4.T3.14.14.14.14.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">82.2</td>
<td id="S4.T3.14.14.14.14.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">83.4</td>
</tr>
<tr id="S4.T3.15.15.15.15" class="ltx_tr">
<th id="S4.T3.15.15.15.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">RMPEÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Fang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>
</th>
<th id="S4.T3.15.15.15.15.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">PyraNet</th>
<td id="S4.T3.15.15.15.15.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.15.15.15.15.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">320<math id="S4.T3.15.15.15.15.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.15.15.15.15.1.m1.1a"><mo id="S4.T3.15.15.15.15.1.m1.1.1" xref="S4.T3.15.15.15.15.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.15.15.15.15.1.m1.1b"><times id="S4.T3.15.15.15.15.1.m1.1.1.cmml" xref="S4.T3.15.15.15.15.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.15.15.15.1.m1.1c">\times</annotation></semantics></math>256</td>
<td id="S4.T3.15.15.15.15.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">28.1M</td>
<td id="S4.T3.15.15.15.15.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">26.7</td>
<td id="S4.T3.15.15.15.15.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.3</td>
<td id="S4.T3.15.15.15.15.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">89.2</td>
<td id="S4.T3.15.15.15.15.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">79.1</td>
<td id="S4.T3.15.15.15.15.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.0</td>
<td id="S4.T3.15.15.15.15.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.6</td>
<td id="S4.T3.15.15.15.15.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
</tr>
<tr id="S4.T3.16.16.16.16" class="ltx_tr">
<th id="S4.T3.16.16.16.16.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">HRNetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>
</th>
<th id="S4.T3.16.16.16.16.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">HRNet-W32</th>
<td id="S4.T3.16.16.16.16.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.16.16.16.16.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">384<math id="S4.T3.16.16.16.16.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.16.16.16.16.1.m1.1a"><mo id="S4.T3.16.16.16.16.1.m1.1.1" xref="S4.T3.16.16.16.16.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.16.16.16.16.1.m1.1b"><times id="S4.T3.16.16.16.16.1.m1.1.1.cmml" xref="S4.T3.16.16.16.16.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.16.16.16.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.16.16.16.16.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">28.5M</td>
<td id="S4.T3.16.16.16.16.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">16.0</td>
<td id="S4.T3.16.16.16.16.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">74.9</td>
<td id="S4.T3.16.16.16.16.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">92.5</td>
<td id="S4.T3.16.16.16.16.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">82.8</td>
<td id="S4.T3.16.16.16.16.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.3</td>
<td id="S4.T3.16.16.16.16.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">80.9</td>
<td id="S4.T3.16.16.16.16.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">80.1</td>
</tr>
<tr id="S4.T3.17.17.17.17" class="ltx_tr">
<th id="S4.T3.17.17.17.17.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">Integral Pose RegressionÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>
</th>
<th id="S4.T3.17.17.17.17.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-101</th>
<td id="S4.T3.17.17.17.17.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.17.17.17.17.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">256<math id="S4.T3.17.17.17.17.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.17.17.17.17.1.m1.1a"><mo id="S4.T3.17.17.17.17.1.m1.1.1" xref="S4.T3.17.17.17.17.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.17.17.17.17.1.m1.1b"><times id="S4.T3.17.17.17.17.1.m1.1.1.cmml" xref="S4.T3.17.17.17.17.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.17.17.17.17.1.m1.1c">\times</annotation></semantics></math>256</td>
<td id="S4.T3.17.17.17.17.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">45.0M</td>
<td id="S4.T3.17.17.17.17.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">11.0</td>
<td id="S4.T3.17.17.17.17.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">67.8</td>
<td id="S4.T3.17.17.17.17.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">88.2</td>
<td id="S4.T3.17.17.17.17.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">74.8</td>
<td id="S4.T3.17.17.17.17.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">63.9</td>
<td id="S4.T3.17.17.17.17.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">74.0</td>
<td id="S4.T3.17.17.17.17.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
</tr>
<tr id="S4.T3.18.18.18.18" class="ltx_tr">
<th id="S4.T3.18.18.18.18.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SimpleBaselineÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>
</th>
<th id="S4.T3.18.18.18.18.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-50</th>
<td id="S4.T3.18.18.18.18.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.18.18.18.18.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">256<math id="S4.T3.18.18.18.18.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.18.18.18.18.1.m1.1a"><mo id="S4.T3.18.18.18.18.1.m1.1.1" xref="S4.T3.18.18.18.18.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.18.18.18.18.1.m1.1b"><times id="S4.T3.18.18.18.18.1.m1.1.1.cmml" xref="S4.T3.18.18.18.18.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.18.18.18.18.1.m1.1c">\times</annotation></semantics></math>192</td>
<td id="S4.T3.18.18.18.18.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">34.0M</td>
<td id="S4.T3.18.18.18.18.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">9.0</td>
<td id="S4.T3.18.18.18.18.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.7</td>
<td id="S4.T3.18.18.18.18.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">90.9</td>
<td id="S4.T3.18.18.18.18.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.5</td>
<td id="S4.T3.18.18.18.18.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">67.4</td>
<td id="S4.T3.18.18.18.18.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">75.0</td>
<td id="S4.T3.18.18.18.18.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">75.9</td>
</tr>
<tr id="S4.T3.19.19.19.19" class="ltx_tr">
<th id="S4.T3.19.19.19.19.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">Lite-HRNetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>
</th>
<th id="S4.T3.19.19.19.19.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">Lite-HRNet-18</th>
<td id="S4.T3.19.19.19.19.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">âœ“</td>
<td id="S4.T3.19.19.19.19.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">256<math id="S4.T3.19.19.19.19.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.19.19.19.19.1.m1.1a"><mo id="S4.T3.19.19.19.19.1.m1.1.1" xref="S4.T3.19.19.19.19.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.19.19.19.19.1.m1.1b"><times id="S4.T3.19.19.19.19.1.m1.1.1.cmml" xref="S4.T3.19.19.19.19.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.19.19.19.19.1.m1.1c">\times</annotation></semantics></math>192</td>
<td id="S4.T3.19.19.19.19.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">1.13M</td>
<td id="S4.T3.19.19.19.19.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">0.21</td>
<td id="S4.T3.19.19.19.19.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">64.1</td>
<td id="S4.T3.19.19.19.19.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">88.7</td>
<td id="S4.T3.19.19.19.19.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.0</td>
<td id="S4.T3.19.19.19.19.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">61.6</td>
<td id="S4.T3.19.19.19.19.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.0</td>
<td id="S4.T3.19.19.19.19.12" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">70.2</td>
</tr>
<tr id="S4.T3.20.20.20.20" class="ltx_tr">
<th id="S4.T3.20.20.20.20.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T3.20.20.20.20.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-50</th>
<td id="S4.T3.20.20.20.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S4.T3.20.20.20.20.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">256<math id="S4.T3.20.20.20.20.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.20.20.20.20.1.m1.1a"><mo id="S4.T3.20.20.20.20.1.m1.1.1" xref="S4.T3.20.20.20.20.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.20.20.20.20.1.m1.1b"><times id="S4.T3.20.20.20.20.1.m1.1.1.cmml" xref="S4.T3.20.20.20.20.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.20.20.20.20.1.m1.1c">\times</annotation></semantics></math>192</td>
<td id="S4.T3.20.20.20.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">25.7M</td>
<td id="S4.T3.20.20.20.20.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">3.8</td>
<td id="S4.T3.20.20.20.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">70.8</td>
<td id="S4.T3.20.20.20.20.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">91.3</td>
<td id="S4.T3.20.20.20.20.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">78.8</td>
<td id="S4.T3.20.20.20.20.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">67.2</td>
<td id="S4.T3.20.20.20.20.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">76.8</td>
<td id="S4.T3.20.20.20.20.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">76.4</td>
</tr>
<tr id="S4.T3.21.21.21.21" class="ltx_tr">
<th id="S4.T3.21.21.21.21.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T3.21.21.21.21.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">Lite-HRNet-18</th>
<td id="S4.T3.21.21.21.21.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">âœ“</td>
<td id="S4.T3.21.21.21.21.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">256<math id="S4.T3.21.21.21.21.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.21.21.21.21.1.m1.1a"><mo id="S4.T3.21.21.21.21.1.m1.1.1" xref="S4.T3.21.21.21.21.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.21.21.21.21.1.m1.1b"><times id="S4.T3.21.21.21.21.1.m1.1.1.cmml" xref="S4.T3.21.21.21.21.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.21.21.21.21.1.m1.1c">\times</annotation></semantics></math>192</td>
<td id="S4.T3.21.21.21.21.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">1.13M</td>
<td id="S4.T3.21.21.21.21.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">0.21</td>
<td id="S4.T3.21.21.21.21.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">64.5</td>
<td id="S4.T3.21.21.21.21.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">89.0</td>
<td id="S4.T3.21.21.21.21.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.4</td>
<td id="S4.T3.21.21.21.21.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">62.2</td>
<td id="S4.T3.21.21.21.21.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.3</td>
<td id="S4.T3.21.21.21.21.12" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;">70.6</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Results on test-dev set</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">We further evaluate the proposed method on the COCO test-dev set, and make the comparison with the state-of-the-art methods. TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.2.1. Results on validation set â€£ 4.2. COCO Keypoint Detection â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> compares the accuracy results of state-of-the-art methods and FasterPose.
Following the conventional routine, we use the human bounding boxes provided byÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
We can find that SimpleBaseline is the most efficient among the prior networks that do not use any weight-lightning strategy (<span id="S4.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>,Â  weight-lightning blocks like MobileNetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sandler etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> or ShuffleNetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>), while FasterPose is able to further improve the efficiency with competitive accuracy.
FasterPose achieves the AP increase of 1.1% (69.7% <math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mo stretchy="false" id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><ci id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">\to</annotation></semantics></math> 70.8%) compared to SimpleBaseline, and only needs 42.2% FLOPs than that of SimpleBaseline.
Beyond the SimpleBaseline, weight-lightning practiceÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> currently helps HRNet greatly reduce model complexity, <span id="S4.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_italic">a.k.a.</span>Â  Lite-HRNetÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu
etÂ al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.
So, we adopt Lite-HRNet as backbone to establish an extra version of FasterPose, and yield higher accuracy than the Lite-HRNet (bottom line in TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.2.1. Results on validation set â€£ 4.2. COCO Keypoint Detection â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). As no need of LHR module in this case, it indicates the advantage of RCE loss.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>MPII Keypoint Detection</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.3. MPII Keypoint Detection â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the accuracy in terms of PCKh@0.5, and the FLOPs of SimpleBaseline and FasterPose.
Overall, we obtain a similar comparison result with that on COCO. In particular,
FasterPose exhibits the superior accuracy over the counterparts in every backbone configuration, while constantly reducing the FLOPs.
It is noteworthy that MPII provides a much smaller training dataset than COCO, suggesting that our method generalizes well across training data sizes.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span>
Comparison of FasterPose and SimpleBaseline (PCKh@0.5 with single-scale test) on the MPII validation set. The input size is 256 <math id="S4.T4.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.2.2.m1.1b"><mo id="S4.T4.2.2.m1.1.1" xref="S4.T4.2.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.m1.1c"><times id="S4.T4.2.2.m1.1.1.cmml" xref="S4.T4.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.m1.1d">\times</annotation></semantics></math> 256 RGB.
</figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.3.1.1" class="ltx_tr">
<th id="S4.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Method</th>
<th id="S4.T4.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Backbone</th>
<th id="S4.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">GFLOPs</th>
<th id="S4.T4.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Hea</th>
<th id="S4.T4.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Sho</th>
<th id="S4.T4.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Elb</th>
<th id="S4.T4.3.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Wri</th>
<th id="S4.T4.3.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Hip</th>
<th id="S4.T4.3.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Kne</th>
<th id="S4.T4.3.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Ank</th>
<th id="S4.T4.3.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Mean</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.3.2.1" class="ltx_tr">
<th id="S4.T4.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">SimpleBaseline</th>
<th id="S4.T4.3.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-50</th>
<td id="S4.T4.3.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">12.0</td>
<td id="S4.T4.3.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">96.4</td>
<td id="S4.T4.3.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">95.3</td>
<td id="S4.T4.3.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">89.0</td>
<td id="S4.T4.3.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">83.2</td>
<td id="S4.T4.3.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">88.4</td>
<td id="S4.T4.3.2.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">84.0</td>
<td id="S4.T4.3.2.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">79.6</td>
<td id="S4.T4.3.2.1.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">88.5</td>
</tr>
<tr id="S4.T4.3.3.2" class="ltx_tr">
<th id="S4.T4.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SimpleBaseline</th>
<th id="S4.T4.3.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-101</th>
<td id="S4.T4.3.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">16.5</td>
<td id="S4.T4.3.3.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">96.9</td>
<td id="S4.T4.3.3.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">95.9</td>
<td id="S4.T4.3.3.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">89.5</td>
<td id="S4.T4.3.3.2.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">84.4</td>
<td id="S4.T4.3.3.2.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">88.4</td>
<td id="S4.T4.3.3.2.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">84.5</td>
<td id="S4.T4.3.3.2.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">80.7</td>
<td id="S4.T4.3.3.2.11" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">89.1</td>
</tr>
<tr id="S4.T4.3.4.3" class="ltx_tr">
<th id="S4.T4.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SimpleBaseline</th>
<th id="S4.T4.3.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-152</th>
<td id="S4.T4.3.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">21.0</td>
<td id="S4.T4.3.4.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">97.0</td>
<td id="S4.T4.3.4.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">95.9</td>
<td id="S4.T4.3.4.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">90.0</td>
<td id="S4.T4.3.4.3.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">85.0</td>
<td id="S4.T4.3.4.3.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">89.2</td>
<td id="S4.T4.3.4.3.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">85.3</td>
<td id="S4.T4.3.4.3.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">81.3</td>
<td id="S4.T4.3.4.3.11" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">89.6</td>
</tr>
<tr id="S4.T4.3.5.4" class="ltx_tr">
<th id="S4.T4.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T4.3.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-50</th>
<td id="S4.T4.3.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.3.5.4.3.1" class="ltx_text ltx_font_bold">5.1</span></td>
<td id="S4.T4.3.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">96.5</td>
<td id="S4.T4.3.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">95.4</td>
<td id="S4.T4.3.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">88.9</td>
<td id="S4.T4.3.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">83.5</td>
<td id="S4.T4.3.5.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">88.4</td>
<td id="S4.T4.3.5.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">84.1</td>
<td id="S4.T4.3.5.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">79.7</td>
<td id="S4.T4.3.5.4.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.3.5.4.11.1" class="ltx_text ltx_font_bold">88.6</span></td>
</tr>
<tr id="S4.T4.3.6.5" class="ltx_tr">
<th id="S4.T4.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T4.3.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-101</th>
<td id="S4.T4.3.6.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.3.6.5.3.1" class="ltx_text ltx_font_bold">9.6</span></td>
<td id="S4.T4.3.6.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">96.7</td>
<td id="S4.T4.3.6.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">95.9</td>
<td id="S4.T4.3.6.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">89.5</td>
<td id="S4.T4.3.6.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">84.4</td>
<td id="S4.T4.3.6.5.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">88.6</td>
<td id="S4.T4.3.6.5.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">85.8</td>
<td id="S4.T4.3.6.5.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">81.0</td>
<td id="S4.T4.3.6.5.11" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.3.6.5.11.1" class="ltx_text ltx_font_bold">89.3</span></td>
</tr>
<tr id="S4.T4.3.7.6" class="ltx_tr">
<th id="S4.T4.3.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FasterPose</th>
<th id="S4.T4.3.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">ResNet-152</th>
<td id="S4.T4.3.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.3.7.6.3.1" class="ltx_text ltx_font_bold">14.1</span></td>
<td id="S4.T4.3.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">96.8</td>
<td id="S4.T4.3.7.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">95.7</td>
<td id="S4.T4.3.7.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">90.1</td>
<td id="S4.T4.3.7.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">85.0</td>
<td id="S4.T4.3.7.6.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">89.2</td>
<td id="S4.T4.3.7.6.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">86.1</td>
<td id="S4.T4.3.7.6.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">82.7</td>
<td id="S4.T4.3.7.6.11" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.3.7.6.11.1" class="ltx_text ltx_font_bold">89.8</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Ablation Study</h3>

<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span>Influence of feature resolution</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.5" class="ltx_p">We study how the representation resolution affects the accuracy of human pose estimation.
First, we verify the performence of SimpleBaseline with the various feature resolutions. The input image of 256<math id="S4.SS4.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.SSS1.p1.1.m1.1a"><mo id="S4.SS4.SSS1.p1.1.m1.1.1" xref="S4.SS4.SSS1.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.1.m1.1b"><times id="S4.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.1.m1.1c">\times</annotation></semantics></math>192 RGB is passed through the ResNet-50 backbone to extract the LR features of 8<math id="S4.SS4.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.SSS1.p1.2.m2.1a"><mo id="S4.SS4.SSS1.p1.2.m2.1.1" xref="S4.SS4.SSS1.p1.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.2.m2.1b"><times id="S4.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.2.m2.1c">\times</annotation></semantics></math>6.
Then, we employ one, two and three layers of deconvolution to generate features with resolutions of 16<math id="S4.SS4.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.SSS1.p1.3.m3.1a"><mo id="S4.SS4.SSS1.p1.3.m3.1.1" xref="S4.SS4.SSS1.p1.3.m3.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.3.m3.1b"><times id="S4.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.3.m3.1c">\times</annotation></semantics></math>12, 32<math id="S4.SS4.SSS1.p1.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.SSS1.p1.4.m4.1a"><mo id="S4.SS4.SSS1.p1.4.m4.1.1" xref="S4.SS4.SSS1.p1.4.m4.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.4.m4.1b"><times id="S4.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.4.m4.1c">\times</annotation></semantics></math>24 and 64<math id="S4.SS4.SSS1.p1.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.SSS1.p1.5.m5.1a"><mo id="S4.SS4.SSS1.p1.5.m5.1.1" xref="S4.SS4.SSS1.p1.5.m5.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.5.m5.1b"><times id="S4.SS4.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS4.SSS1.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.5.m5.1c">\times</annotation></semantics></math>48, respectively. As a result, we obtain four representations with different resolutions.
Subsequently, they are used to regress the output heatmap.
The comparison is summarized in TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.4.1. Influence of feature resolution â€£ 4.4. Ablation Study â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, including the evaluation of AP on COCO, the FLOPs, and the models size.
It is observed that when the resolution of the feature increases, the computational cost of the model also increases greatly. The improvement of accuracy, however, is only 0.1%.
This observation confirms that the high efficiency of LR features is valid for human pose estimation, although it has been neglected in the past.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5. </span>
The ablation study on the feature resolution.
With the increase of resolution, the accuracy barely changes, while the computational cost significantly increases.
The backbone is ResNet-50, and the input size is 256 <math id="S4.T5.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T5.2.2.m1.1b"><mo id="S4.T5.2.2.m1.1.1" xref="S4.T5.2.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.m1.1c"><times id="S4.T5.2.2.m1.1.1.cmml" xref="S4.T5.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.m1.1d">\times</annotation></semantics></math> 192 RGB. The same data augmentation as SimpleBaseline are used.
</figcaption>
<table id="S4.T5.7.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.3.3.1" class="ltx_tr">
<th id="S4.T5.3.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Feature size</th>
<th id="S4.T5.3.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP (%)</th>
<th id="S4.T5.3.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP<sup id="S4.T5.3.3.1.1.1" class="ltx_sup"><span id="S4.T5.3.3.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sup> (%)</th>
<th id="S4.T5.3.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">GFLOPs</th>
<th id="S4.T5.3.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">#Params</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.4.4.2" class="ltx_tr">
<th id="S4.T5.4.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">8<math id="S4.T5.4.4.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T5.4.4.2.1.m1.1a"><mo id="S4.T5.4.4.2.1.m1.1.1" xref="S4.T5.4.4.2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.2.1.m1.1b"><times id="S4.T5.4.4.2.1.m1.1.1.cmml" xref="S4.T5.4.4.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.2.1.m1.1c">\times</annotation></semantics></math>6</th>
<td id="S4.T5.4.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">70.3</td>
<td id="S4.T5.4.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">88.6</td>
<td id="S4.T5.4.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.4.4.2.4.1" class="ltx_text ltx_font_bold">3.8</span></td>
<td id="S4.T5.4.4.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.4.4.2.5.1" class="ltx_text ltx_font_bold">25.6M</span></td>
</tr>
<tr id="S4.T5.5.5.3" class="ltx_tr">
<th id="S4.T5.5.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">16<math id="S4.T5.5.5.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T5.5.5.3.1.m1.1a"><mo id="S4.T5.5.5.3.1.m1.1.1" xref="S4.T5.5.5.3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.3.1.m1.1b"><times id="S4.T5.5.5.3.1.m1.1.1.cmml" xref="S4.T5.5.5.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.3.1.m1.1c">\times</annotation></semantics></math>12</th>
<td id="S4.T5.5.5.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.4</td>
<td id="S4.T5.5.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">88.7</td>
<td id="S4.T5.5.5.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">5.2</td>
<td id="S4.T5.5.5.3.5" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">31.9M</td>
</tr>
<tr id="S4.T5.6.6.4" class="ltx_tr">
<th id="S4.T5.6.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">32<math id="S4.T5.6.6.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T5.6.6.4.1.m1.1a"><mo id="S4.T5.6.6.4.1.m1.1.1" xref="S4.T5.6.6.4.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.4.1.m1.1b"><times id="S4.T5.6.6.4.1.m1.1.1.cmml" xref="S4.T5.6.6.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.4.1.m1.1c">\times</annotation></semantics></math>24</th>
<td id="S4.T5.6.6.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.4</td>
<td id="S4.T5.6.6.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">88.7</td>
<td id="S4.T5.6.6.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">6.0</td>
<td id="S4.T5.6.6.4.5" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">33.0M</td>
</tr>
<tr id="S4.T5.7.7.5" class="ltx_tr">
<th id="S4.T5.7.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">64<math id="S4.T5.7.7.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T5.7.7.5.1.m1.1a"><mo id="S4.T5.7.7.5.1.m1.1.1" xref="S4.T5.7.7.5.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T5.7.7.5.1.m1.1b"><times id="S4.T5.7.7.5.1.m1.1.1.cmml" xref="S4.T5.7.7.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.7.5.1.m1.1c">\times</annotation></semantics></math>48</th>
<td id="S4.T5.7.7.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.4</td>
<td id="S4.T5.7.7.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">88.6</td>
<td id="S4.T5.7.7.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">9.0</td>
<td id="S4.T5.7.7.5.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;">34.0M</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2107.03215/assets/figures/training_curve2.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="473" height="295" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>
The convergence of the HR-based and LR-based networks, measured by the AP on COCO validation.
The green curve denotes the training with RCE supervision, while the others are supervised by MSE.
All of them have the same backbone (ResNet-50) and input resolution (256<math id="S4.F5.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.F5.2.m1.1b"><mo id="S4.F5.2.m1.1.1" xref="S4.F5.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.F5.2.m1.1c"><times id="S4.F5.2.m1.1.1.cmml" xref="S4.F5.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.2.m1.1d">\times</annotation></semantics></math> 192 RGB).
The HR-based network employs three deconvolution layers to recover the HR features, while the LR-based network employs the LHR module to regress the heatmap from the LR features directly.
It is obvious that the convergence of LR-based network (blue curve) is slower than that of the HR-based network (red curve).
The RCE loss promotes the LR-based network in terms of convergence and accuracy (green curve).
</figcaption>
</figure>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span>Advantage of RCE loss</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">Based on the efficient architecture of FasterrPose, the next goal is to further promote the spatial accuracy for pose estimation. We visualize the convergence process and find that the convergence of LR-based network is slower than that of HR-based network, therefore resulting in inferior performance. This verifies the conclusion: the reduction of parameters over the feature layers will cause the model unable to accurately learn from the samples with imbalance classes.
To deal with this problem, the RCE supervision is developed to promote the training convergence and the final accuracy.</p>
</div>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<p id="S4.SS4.SSS2.p2.1" class="ltx_p">As mentioned in SectionÂ <a href="#S3.SS2" title="3.2. RCE Loss â€£ 3. Proposed Method â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, the standard CE loss can only supervise the learning with ground truth of 0-1 binary values, while RCE loss can be applied to the heatmap regression learning with the ground truth of continuous real values. In this experiment, we compare the RCE loss with not only the CE loss, but also the MSE loss.
The APs are shown in TableÂ <a href="#S4.T6" title="Table 6 â€£ 4.4.2. Advantage of RCE loss â€£ 4.4. Ablation Study â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. It can be clearly observed that the RCE loss outperforms both MSE loss and CE loss.
Besides, in Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.4.1. Influence of feature resolution â€£ 4.4. Ablation Study â€£ 4. Experiments â€£ FasterPose: A Faster Simple Baseline for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the green curve shows the advantage of RCE loss for improving the convergence, as well as the accuracy.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6. </span>
The RCE supervision consistently improves the accuracy compared with MSE and CE on COCO validation set. The model is FasterPose (ResNet-50), the input size is 256<math id="S4.T6.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.2.2.m1.1b"><mo id="S4.T6.2.2.m1.1.1" xref="S4.T6.2.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.m1.1c"><times id="S4.T6.2.2.m1.1.1.cmml" xref="S4.T6.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.m1.1d">\times</annotation></semantics></math>192 RGB, The accuracy is reported in percentage.
</figcaption>
<table id="S4.T6.6.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.6.6.4" class="ltx_tr">
<th id="S4.T6.6.6.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Loss</th>
<th id="S4.T6.6.6.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP</th>
<th id="S4.T6.3.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP<sup id="S4.T6.3.3.1.1.1" class="ltx_sup"><span id="S4.T6.3.3.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sup>
</th>
<th id="S4.T6.4.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP<sup id="S4.T6.4.4.2.2.1" class="ltx_sup"><span id="S4.T6.4.4.2.2.1.1" class="ltx_text ltx_font_italic">75</span></sup>
</th>
<th id="S4.T6.5.5.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP<sup id="S4.T6.5.5.3.3.1" class="ltx_sup"><span id="S4.T6.5.5.3.3.1.1" class="ltx_text ltx_font_italic">M</span></sup>
</th>
<th id="S4.T6.6.6.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AP<sup id="S4.T6.6.6.4.4.1" class="ltx_sup"><span id="S4.T6.6.6.4.4.1.1" class="ltx_text ltx_font_italic">L</span></sup>
</th>
<th id="S4.T6.6.6.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AR</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.6.6.5.1" class="ltx_tr">
<th id="S4.T6.6.6.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">MSE</th>
<td id="S4.T6.6.6.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">70.3</td>
<td id="S4.T6.6.6.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">88.6</td>
<td id="S4.T6.6.6.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">78.8</td>
<td id="S4.T6.6.6.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">67.3</td>
<td id="S4.T6.6.6.5.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">77.8</td>
<td id="S4.T6.6.6.5.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">76.6</td>
</tr>
<tr id="S4.T6.6.6.6.2" class="ltx_tr">
<th id="S4.T6.6.6.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">CE (one-hot)</th>
<td id="S4.T6.6.6.6.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.7</td>
<td id="S4.T6.6.6.6.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">89.1</td>
<td id="S4.T6.6.6.6.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.1</td>
<td id="S4.T6.6.6.6.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">67.3</td>
<td id="S4.T6.6.6.6.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">77.2</td>
<td id="S4.T6.6.6.6.2.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">76.2</td>
</tr>
<tr id="S4.T6.6.6.7.3" class="ltx_tr">
<th id="S4.T6.6.6.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">CE (mask)</th>
<td id="S4.T6.6.6.7.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.0</td>
<td id="S4.T6.6.6.7.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">88.9</td>
<td id="S4.T6.6.6.7.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">76.9</td>
<td id="S4.T6.6.6.7.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">66.6</td>
<td id="S4.T6.6.6.7.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">76.5</td>
<td id="S4.T6.6.6.7.3.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">76.6</td>
</tr>
<tr id="S4.T6.6.6.8.4" class="ltx_tr">
<th id="S4.T6.6.6.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">RCE</th>
<td id="S4.T6.6.6.8.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T6.6.6.8.4.2.1" class="ltx_text ltx_font_bold">70.9</span></td>
<td id="S4.T6.6.6.8.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T6.6.6.8.4.3.1" class="ltx_text ltx_font_bold">89.5</span></td>
<td id="S4.T6.6.6.8.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T6.6.6.8.4.4.1" class="ltx_text ltx_font_bold">78.9</span></td>
<td id="S4.T6.6.6.8.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T6.6.6.8.4.5.1" class="ltx_text ltx_font_bold">67.8</span></td>
<td id="S4.T6.6.6.8.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T6.6.6.8.4.6.1" class="ltx_text ltx_font_bold">77.9</span></td>
<td id="S4.T6.6.6.8.4.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T6.6.6.8.4.7.1" class="ltx_text ltx_font_bold">76.8</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we present a novel design paradigm, named FasterPose, to accomplish the efficient network for human pose estimation, and a novel loss function for effectively training such efficient network.
To the best of our knowledge, it is the first attempt to analyze the impact of feature resolution for human pose estimation.
Based on the analysis, FasterPose is devised with the design of LR feature and thereby largely reduces the computational cost.
Furthermore, we study the convergence behavior of LR based networks, and formulate a novel loss function for accelerating its convergence and raising its accuracy.
Our method facilitate the low-latency and low-energy-budget application as required in the non-GPU scenarios.
The extensive experiments verify the effectiveness and efficiency of FasterPose.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="font-size:90%;">(1)</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">        



</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.6.6.1" class="ltx_text" style="font-size:90%;">Andriluka etÂ al</span><span id="bib.bib2.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib2.8.8.3" class="ltx_text" style="font-size:90%;"> (2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.10.1" class="ltx_text" style="font-size:90%;">
Mykhaylo Andriluka, Leonid
Pishchulin, Peter Gehler, and Bernt
Schiele. 2014.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.11.1" class="ltx_text" style="font-size:90%;">2d human pose estimation: New benchmark and state
of the art analysis. In </span><em id="bib.bib2.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE
Conference on computer Vision and Pattern Recognition</em><span id="bib.bib2.13.3" class="ltx_text" style="font-size:90%;">.
3686â€“3693.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Bulat and
Tzimiropoulos (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Adrian Bulat and
Georgios Tzimiropoulos. 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Human pose estimation via convolutional part
heatmap regression.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.6.6.1" class="ltx_text" style="font-size:90%;">Cai etÂ al</span><span id="bib.bib4.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib4.8.8.3" class="ltx_text" style="font-size:90%;"> (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.10.1" class="ltx_text" style="font-size:90%;">
Yuanhao Cai, Zhicheng
Wang, Zhengxiong Luo, Binyi Yin,
Angang Du, Haoqian Wang,
Xinyu Zhou, Erjin Zhou,
Xiangyu Zhang, and Jian Sun.
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.11.1" class="ltx_text" style="font-size:90%;">Learning Delicate Local Representations for
Multi-Person Pose Estimation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.12.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2003.04030</em><span id="bib.bib4.13.2" class="ltx_text" style="font-size:90%;">
(2020).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.6.6.1" class="ltx_text" style="font-size:90%;">Cao
etÂ al</span><span id="bib.bib5.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib5.8.8.3" class="ltx_text" style="font-size:90%;"> (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.10.1" class="ltx_text" style="font-size:90%;">
Zhe Cao, Tomas Simon,
Shih-En Wei, and Yaser Sheikh.
2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.11.1" class="ltx_text" style="font-size:90%;">Realtime multi-person 2d pose estimation using part
affinity fields. In </span><em id="bib.bib5.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE
conference on computer vision and pattern recognition</em><span id="bib.bib5.13.3" class="ltx_text" style="font-size:90%;">.
7291â€“7299.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.6.6.1" class="ltx_text" style="font-size:90%;">Chen
etÂ al</span><span id="bib.bib6.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib6.8.8.3" class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.10.1" class="ltx_text" style="font-size:90%;">
Yilun Chen, Zhicheng
Wang, Yuxiang Peng, Zhiqiang Zhang,
Gang Yu, and Jian Sun.
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.11.1" class="ltx_text" style="font-size:90%;">Cascaded pyramid network for multi-person pose
estimation. In </span><em id="bib.bib6.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference
on computer vision and pattern recognition</em><span id="bib.bib6.13.3" class="ltx_text" style="font-size:90%;">. 7103â€“7112.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.6.6.1" class="ltx_text" style="font-size:90%;">ChÃ©ron
etÂ al</span><span id="bib.bib7.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib7.8.8.3" class="ltx_text" style="font-size:90%;"> (2015)</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.10.1" class="ltx_text" style="font-size:90%;">
Guilhem ChÃ©ron, Ivan
Laptev, and Cordelia Schmid.
2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.11.1" class="ltx_text" style="font-size:90%;">P-cnn: Pose-based cnn features for action
recognition. In </span><em id="bib.bib7.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE
international conference on computer vision</em><span id="bib.bib7.13.3" class="ltx_text" style="font-size:90%;">. 3218â€“3226.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.6.6.1" class="ltx_text" style="font-size:90%;">Cho
etÂ al</span><span id="bib.bib8.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib8.8.8.3" class="ltx_text" style="font-size:90%;"> (2013)</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.10.1" class="ltx_text" style="font-size:90%;">
Nam-Gyu Cho, AlanÂ L
Yuille, and Seong-Whan Lee.
2013.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.11.1" class="ltx_text" style="font-size:90%;">Adaptive occlusion state estimation for human pose
tracking under self-occlusions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.12.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Pattern Recognition</em><span id="bib.bib8.13.2" class="ltx_text" style="font-size:90%;"> 46,
3 (2013), 649â€“661.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.6.6.1" class="ltx_text" style="font-size:90%;">Fang
etÂ al</span><span id="bib.bib9.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib9.8.8.3" class="ltx_text" style="font-size:90%;"> (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.10.1" class="ltx_text" style="font-size:90%;">
Hao-Shu Fang, Shuqin Xie,
Yu-Wing Tai, and Cewu Lu.
2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.11.1" class="ltx_text" style="font-size:90%;">Rmpe: Regional multi-person pose estimation. In
</span><em id="bib.bib9.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on
Computer Vision</em><span id="bib.bib9.13.3" class="ltx_text" style="font-size:90%;">. 2334â€“2343.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Faster (2015)</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
RCNN Faster.
2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Towards real-time object detection with region
proposal networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing
systems</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;"> (2015), 9199.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.6.6.1" class="ltx_text" style="font-size:90%;">He
etÂ al</span><span id="bib.bib11.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib11.8.8.3" class="ltx_text" style="font-size:90%;"> (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.10.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu
Zhang, Shaoqing Ren, and Jian Sun.
2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.11.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition. In
</span><em id="bib.bib11.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer
vision and pattern recognition</em><span id="bib.bib11.13.3" class="ltx_text" style="font-size:90%;">. 770â€“778.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Hu and Ramanan (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Peiyun Hu and Deva
Ramanan. 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down reasoning with hierarchical
rectified gaussians. In </span><em id="bib.bib12.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib12.10.3" class="ltx_text" style="font-size:90%;">.
5600â€“5609.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.6.6.1" class="ltx_text" style="font-size:90%;">Huang
etÂ al</span><span id="bib.bib13.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib13.8.8.3" class="ltx_text" style="font-size:90%;"> (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.10.1" class="ltx_text" style="font-size:90%;">
Junjie Huang, Zheng Zhu,
Feng Guo, and Guan Huang.
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.11.1" class="ltx_text" style="font-size:90%;">The Devil Is in the Details: Delving Into Unbiased
Data Processing for Human Pose Estimation. In </span><em id="bib.bib13.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib13.13.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.6.6.1" class="ltx_text" style="font-size:90%;">Insafutdinov etÂ al</span><span id="bib.bib14.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib14.8.8.3" class="ltx_text" style="font-size:90%;"> (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.10.1" class="ltx_text" style="font-size:90%;">
Eldar Insafutdinov, Leonid
Pishchulin, Bjoern Andres, Mykhaylo
Andriluka, and Bernt Schiele.
2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.11.1" class="ltx_text" style="font-size:90%;">Deepercut: A deeper, stronger, and faster
multi-person pose estimation model. In </span><em id="bib.bib14.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European
Conference on Computer Vision</em><span id="bib.bib14.13.3" class="ltx_text" style="font-size:90%;">. Springer, 34â€“50.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.6.6.1" class="ltx_text" style="font-size:90%;">Ke
etÂ al</span><span id="bib.bib15.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib15.8.8.3" class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.10.1" class="ltx_text" style="font-size:90%;">
Lipeng Ke, Ming-Ching
Chang, Honggang Qi, and Siwei Lyu.
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.11.1" class="ltx_text" style="font-size:90%;">Multi-Scale Structure-Aware Network for Human Pose
Estimation.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.6.6.1" class="ltx_text" style="font-size:90%;">Lifshitz
etÂ al</span><span id="bib.bib16.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib16.8.8.3" class="ltx_text" style="font-size:90%;"> (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.10.1" class="ltx_text" style="font-size:90%;">
Ita Lifshitz, Ethan
Fetaya, and Shimon Ullman.
2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.11.1" class="ltx_text" style="font-size:90%;">Human pose estimation using deep consensus voting.
In </span><em id="bib.bib16.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</em><span id="bib.bib16.13.3" class="ltx_text" style="font-size:90%;">.
Springer, 246â€“260.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.6.6.1" class="ltx_text" style="font-size:90%;">Lin
etÂ al</span><span id="bib.bib17.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib17.8.8.3" class="ltx_text" style="font-size:90%;"> (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.10.1" class="ltx_text" style="font-size:90%;">
TsungÂ Yi Lin, Priya
Goyal, Ross Girshick, Kaiming He, and
Piotr DollÃ¡r. 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.11.1" class="ltx_text" style="font-size:90%;">Focal Loss for Dense Object Detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.12.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis &amp;
Machine Intelligence</em><span id="bib.bib17.13.2" class="ltx_text" style="font-size:90%;"> PP, 99
(2017), 2999â€“3007.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.6.6.1" class="ltx_text" style="font-size:90%;">Lin etÂ al</span><span id="bib.bib18.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib18.8.8.3" class="ltx_text" style="font-size:90%;"> (2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.10.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael
Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan,
Piotr DollÃ¡r, and CÂ Lawrence
Zitnick. 2014.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.11.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context. In
</span><em id="bib.bib18.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European conference on computer vision</em><span id="bib.bib18.13.3" class="ltx_text" style="font-size:90%;">. Springer,
740â€“755.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.6.6.1" class="ltx_text" style="font-size:90%;">Moon
etÂ al</span><span id="bib.bib19.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib19.8.8.3" class="ltx_text" style="font-size:90%;"> (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.10.1" class="ltx_text" style="font-size:90%;">
Gyeongsik Moon, JuÂ Yong
Chang, and KyoungÂ Mu Lee.
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.11.1" class="ltx_text" style="font-size:90%;">PoseFix: Model-Agnostic General Human Pose
Refinement Network. In </span><em id="bib.bib19.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib19.13.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.6.6.1" class="ltx_text" style="font-size:90%;">Newell
etÂ al</span><span id="bib.bib20.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib20.8.8.3" class="ltx_text" style="font-size:90%;"> (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.10.1" class="ltx_text" style="font-size:90%;">
Alejandro Newell, Kaiyu
Yang, and Jia Deng. 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.11.1" class="ltx_text" style="font-size:90%;">Stacked hourglass networks for human pose
estimation. In </span><em id="bib.bib20.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European conference on computer
vision</em><span id="bib.bib20.13.3" class="ltx_text" style="font-size:90%;">. Springer, 483â€“499.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.6.6.1" class="ltx_text" style="font-size:90%;">Papandreou etÂ al</span><span id="bib.bib21.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib21.8.8.3" class="ltx_text" style="font-size:90%;"> (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.10.1" class="ltx_text" style="font-size:90%;">
George Papandreou, Tyler
Zhu, Nori Kanazawa, Alexander Toshev,
Jonathan Tompson, Chris Bregler, and
Kevin Murphy. 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.11.1" class="ltx_text" style="font-size:90%;">Towards accurate multi-person pose estimation in
the wild. In </span><em id="bib.bib21.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition</em><span id="bib.bib21.13.3" class="ltx_text" style="font-size:90%;">. 4903â€“4911.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.6.6.1" class="ltx_text" style="font-size:90%;">Pishchulin etÂ al</span><span id="bib.bib22.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib22.8.8.3" class="ltx_text" style="font-size:90%;"> (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.10.1" class="ltx_text" style="font-size:90%;">
Leonid Pishchulin, Eldar
Insafutdinov, Siyu Tang, Bjoern Andres,
Mykhaylo Andriluka, PeterÂ V Gehler, and
Bernt Schiele. 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.11.1" class="ltx_text" style="font-size:90%;">Deepcut: Joint subset partition and labeling for
multi person pose estimation. In </span><em id="bib.bib22.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of
the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib22.13.3" class="ltx_text" style="font-size:90%;">.
4929â€“4937.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.7.7.1" class="ltx_text" style="font-size:90%;">Russakovsky
etÂ al</span><span id="bib.bib23.8.8.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib23.9.9.3" class="ltx_text" style="font-size:90%;"> (2015)</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.11.1" class="ltx_text" style="font-size:90%;">
Olga Russakovsky, Jia
Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein,
etÂ al</span><span id="bib.bib23.12.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib23.13.3" class="ltx_text" style="font-size:90%;"> 2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.14.1" class="ltx_text" style="font-size:90%;">Imagenet large scale visual recognition challenge.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib23.15.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International journal of computer vision</em><span id="bib.bib23.16.2" class="ltx_text" style="font-size:90%;">
115, 3 (2015),
211â€“252.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.6.6.1" class="ltx_text" style="font-size:90%;">Sandler etÂ al</span><span id="bib.bib24.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib24.8.8.3" class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.10.1" class="ltx_text" style="font-size:90%;">
Mark Sandler, Andrew
Howard, Menglong Zhu, Andrey Zhmoginov,
and Liang-Chieh Chen. 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.11.1" class="ltx_text" style="font-size:90%;">Mobilenetv2: Inverted residuals and linear
bottlenecks. In </span><em id="bib.bib24.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference
on computer vision and pattern recognition</em><span id="bib.bib24.13.3" class="ltx_text" style="font-size:90%;">. 4510â€“4520.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.6.6.1" class="ltx_text" style="font-size:90%;">Shi etÂ al</span><span id="bib.bib25.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib25.8.8.3" class="ltx_text" style="font-size:90%;"> (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.10.1" class="ltx_text" style="font-size:90%;">
Wenzhe Shi, Jose
Caballero, Ferenc HuszÃ¡r, Johannes
Totz, AndrewÂ P Aitken, Rob Bishop,
Daniel Rueckert, and Zehan Wang.
2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.11.1" class="ltx_text" style="font-size:90%;">Real-time single image and video super-resolution
using an efficient sub-pixel convolutional neural network. In
</span><em id="bib.bib25.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer
vision and pattern recognition</em><span id="bib.bib25.13.3" class="ltx_text" style="font-size:90%;">. 1874â€“1883.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.6.6.1" class="ltx_text" style="font-size:90%;">Shotton etÂ al</span><span id="bib.bib26.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib26.8.8.3" class="ltx_text" style="font-size:90%;"> (2011)</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.10.1" class="ltx_text" style="font-size:90%;">
Jamie Shotton, Andrew
Fitzgibbon, Mat Cook, Toby Sharp,
Mark Finocchio, Richard Moore,
Alex Kipman, and Andrew Blake.
2011.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.11.1" class="ltx_text" style="font-size:90%;">Real-time human pose recognition in parts from
single depth images. In </span><em id="bib.bib26.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR 2011</em><span id="bib.bib26.13.3" class="ltx_text" style="font-size:90%;">. Ieee,
1297â€“1304.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.6.6.1" class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span id="bib.bib27.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib27.8.8.3" class="ltx_text" style="font-size:90%;"> (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.10.1" class="ltx_text" style="font-size:90%;">
Ke Sun, Bin Xiao,
Dong Liu, and Jingdong Wang.
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.11.1" class="ltx_text" style="font-size:90%;">Deep high-resolution representation learning for
human pose estimation. In </span><em id="bib.bib27.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE
conference on computer vision and pattern recognition</em><span id="bib.bib27.13.3" class="ltx_text" style="font-size:90%;">.
5693â€“5703.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.6.6.1" class="ltx_text" style="font-size:90%;">Sun
etÂ al</span><span id="bib.bib28.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib28.8.8.3" class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.10.1" class="ltx_text" style="font-size:90%;">
Xiao Sun, Bin Xiao,
Fangyin Wei, Shuang Liang, and
Yichen Wei. 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.11.1" class="ltx_text" style="font-size:90%;">Integral human pose regression. In
</span><em id="bib.bib28.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer
Vision (ECCV)</em><span id="bib.bib28.13.3" class="ltx_text" style="font-size:90%;">. 529â€“545.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.6.6.1" class="ltx_text" style="font-size:90%;">Tang etÂ al</span><span id="bib.bib29.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib29.8.8.3" class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.10.1" class="ltx_text" style="font-size:90%;">
Wei Tang, Pei Yu, and
Ying Wu. 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.11.1" class="ltx_text" style="font-size:90%;">Deeply Learned Compositional Models for Human Pose
Estimation.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.6.6.1" class="ltx_text" style="font-size:90%;">Tompson
etÂ al</span><span id="bib.bib30.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib30.8.8.3" class="ltx_text" style="font-size:90%;"> (2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.10.1" class="ltx_text" style="font-size:90%;">
JonathanÂ J Tompson, Arjun
Jain, Yann LeCun, and Christoph
Bregler. 2014.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.11.1" class="ltx_text" style="font-size:90%;">Joint training of a convolutional network and a
graphical model for human pose estimation.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.6.6.1" class="ltx_text" style="font-size:90%;">Wang
etÂ al</span><span id="bib.bib31.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib31.8.8.3" class="ltx_text" style="font-size:90%;"> (2013)</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.10.1" class="ltx_text" style="font-size:90%;">
Chunyu Wang, Yizhou Wang,
and AlanÂ L Yuille. 2013.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.11.1" class="ltx_text" style="font-size:90%;">An approach to pose-based action recognition. In
</span><em id="bib.bib31.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer
vision and pattern recognition</em><span id="bib.bib31.13.3" class="ltx_text" style="font-size:90%;">. 915â€“922.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.6.6.1" class="ltx_text" style="font-size:90%;">Wang etÂ al</span><span id="bib.bib32.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib32.8.8.3" class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.10.1" class="ltx_text" style="font-size:90%;">
Zhicheng Wang, Wenbo Li,
Binyi Yin, Qixiang Peng,
Tianzi Xiao, Yuming Du,
Zeming Li, Xiangyu Zhang,
Gang Yu, and Jian Sun.
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.11.1" class="ltx_text" style="font-size:90%;">Mscoco keypoints challenge 2018. In
</span><em id="bib.bib32.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Joint Recognition Challenge Workshop at ECCV
2018</em><span id="bib.bib32.13.3" class="ltx_text" style="font-size:90%;">, Vol.Â 5.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.6.6.1" class="ltx_text" style="font-size:90%;">Xiao etÂ al</span><span id="bib.bib33.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib33.8.8.3" class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.10.1" class="ltx_text" style="font-size:90%;">
Bin Xiao, Haiping Wu,
and Yichen Wei. 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.11.1" class="ltx_text" style="font-size:90%;">Simple baselines for human pose estimation and
tracking. In </span><em id="bib.bib33.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European
conference on computer vision (ECCV)</em><span id="bib.bib33.13.3" class="ltx_text" style="font-size:90%;">. 466â€“481.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.6.6.1" class="ltx_text" style="font-size:90%;">Yang
etÂ al</span><span id="bib.bib34.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib34.8.8.3" class="ltx_text" style="font-size:90%;"> (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.10.1" class="ltx_text" style="font-size:90%;">
Wei Yang, Shuang Li,
Wanli Ouyang, Hongsheng Li, and
Xiaogang Wang. 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.11.1" class="ltx_text" style="font-size:90%;">Learning feature pyramids for human pose
estimation. In </span><em id="bib.bib34.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">proceedings of the IEEE
international conference on computer vision</em><span id="bib.bib34.13.3" class="ltx_text" style="font-size:90%;">. 1281â€“1290.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.6.6.1" class="ltx_text" style="font-size:90%;">Yu
etÂ al</span><span id="bib.bib35.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib35.8.8.3" class="ltx_text" style="font-size:90%;"> (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.10.1" class="ltx_text" style="font-size:90%;">
Changqian Yu, Bin Xiao,
Changxin Gao, Lu Yuan,
Lei Zhang, Nong Sang, and
Jingdong Wang. 2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.11.1" class="ltx_text" style="font-size:90%;">Lite-HRNet: A Lightweight High-Resolution Network.
In </span><em id="bib.bib35.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib35.13.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.6.6.1" class="ltx_text" style="font-size:90%;">Zhang
etÂ al</span><span id="bib.bib36.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib36.8.8.3" class="ltx_text" style="font-size:90%;"> (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.10.1" class="ltx_text" style="font-size:90%;">
Feng Zhang, Xiatian Zhu,
Hanbin Dai, Mao Ye, and
Ce Zhu. 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.11.1" class="ltx_text" style="font-size:90%;">Distribution-Aware Coordinate Representation for
Human Pose Estimation. In </span><em id="bib.bib36.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib36.13.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.6.6.1" class="ltx_text" style="font-size:90%;">Zhang
etÂ al</span><span id="bib.bib37.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib37.8.8.3" class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.10.1" class="ltx_text" style="font-size:90%;">
Xiangyu Zhang, Xinyu
Zhou, Mengxiao Lin, and Jian Sun.
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.11.1" class="ltx_text" style="font-size:90%;">Shufflenet: An extremely efficient convolutional
neural network for mobile devices. In </span><em id="bib.bib37.12.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings
of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib37.13.3" class="ltx_text" style="font-size:90%;">.
6848â€“6856.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.6.6.1" class="ltx_text" style="font-size:90%;">Zhang
etÂ al</span><span id="bib.bib38.7.7.2" class="ltx_text" style="font-size:90%;">.</span><span id="bib.bib38.8.8.3" class="ltx_text" style="font-size:90%;"> (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.10.1" class="ltx_text" style="font-size:90%;">
Zhe Zhang, Jie Tang,
and Gangshan Wu. 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.11.1" class="ltx_text" style="font-size:90%;">Simple and Lightweight Human Pose Estimation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib38.12.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.10346</em><span id="bib.bib38.13.2" class="ltx_text" style="font-size:90%;">
(2019).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2107.03214" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2107.03215" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2107.03215">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2107.03215" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2107.03216" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 12:22:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
