<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2006.01423] Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods</title><meta property="og:description" content="Vision-based monocular human pose estimation, as one of the most fundamental and challenging problems in computer vision, aims to obtain posture of the human body from input images or video sequences. The recent develo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2006.01423">

<!--Generated on Wed Mar  6 09:44:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yucheng Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:chenyucheng@mail.nwpu.edu.cn">chenyucheng@mail.nwpu.edu.cn</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yingli Tian
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:ytian@ccny.cuny.edu">ytian@ccny.cuny.edu</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mingyi He
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:myhe@nwpu.edu.cn">myhe@nwpu.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_address">Northwestern Polytechnical University, Xi’an, China
</span>
<span class="ltx_contact ltx_role_address">The City College, City University of New York, NY 10031, USA
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Vision-based monocular human pose estimation, as one of the most fundamental and challenging problems in computer vision, aims to obtain posture of the human body from input images or video sequences. The recent developments of deep learning techniques have been brought significant progress and remarkable breakthroughs in the field of human pose estimation. This survey extensively reviews the recent deep learning-based 2D and 3D human pose estimation methods published since 2014. This paper summarizes the challenges, main frameworks, benchmark datasets, evaluation metrics, performance comparison, and discusses some promising future research directions.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>

</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">MSC: </h6>41A05, 41A10, 65D05, 65D17
<span id="id2.id1" class="ltx_ERROR undefined">\KWD</span>Keyword1, Keyword2, Keyword3
deep learning; human pose estimation; survey;

</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>Computer Vision and Image Understanding</span></span></span>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The human pose estimation (HPE) task, which has been developed for decades, aims to obtain posture of the human body from given sensor inputs. Vision-based approaches are often used to provide such a solution by using cameras. In recent years, with deep learning shows good performance on many computer version tasks such as image classification <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky et al., <a href="#bib.bib78" title="" class="ltx_ref">2012</a>)</cite>, object detection <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib138" title="" class="ltx_ref">2015</a>)</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_citep">(Long et al., <a href="#bib.bib95" title="" class="ltx_ref">2015</a>)</cite>, etc., HPE also achieves rapid progress by employing deep learning technology. The main developments include well-designed networks with great estimation capability, richer datasets <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib93" title="" class="ltx_ref">2014</a>; Joo et al., <a href="#bib.bib70" title="" class="ltx_ref">2017</a>; Mehta et al., <a href="#bib.bib104" title="" class="ltx_ref">2017a</a>)</cite> for feeding networks and more practical exploration of body models <cite class="ltx_cite ltx_citemacro_citep">(Loper et al., <a href="#bib.bib96" title="" class="ltx_ref">2015</a>; Kanazawa et al., <a href="#bib.bib73" title="" class="ltx_ref">2018</a>)</cite>. Although there are some existing reviews for HPE, however, there still lacks a survey to summarize the most recent deep learning-based achievements. This paper extensively reviews deep learning-based 2D/3D human pose estimation methods from monocular images or video footage of humans. Algorithms relied on other sensors such as depth <cite class="ltx_cite ltx_citemacro_citep">(Shotton et al., <a href="#bib.bib147" title="" class="ltx_ref">2012</a>)</cite>, infrared light source <cite class="ltx_cite ltx_citemacro_citep">(Faessler et al., <a href="#bib.bib36" title="" class="ltx_ref">2014</a>)</cite>, radio frequency signal <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a href="#bib.bib183" title="" class="ltx_ref">2018</a>)</cite>, and multi-view inputs <cite class="ltx_cite ltx_citemacro_citep">(Rhodin et al., <a href="#bib.bib140" title="" class="ltx_ref">2018b</a>)</cite> are not included in this survey.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As one of the fundamental computer vision tasks, HPE is a very important research field and can be applied to many applications such as action/activity recognition <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib81" title="" class="ltx_ref">2017b</a>; Luvizon et al., <a href="#bib.bib98" title="" class="ltx_ref">2018</a>; Li et al., <a href="#bib.bib83" title="" class="ltx_ref">2018b</a>)</cite>, action detection <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib80" title="" class="ltx_ref">2017a</a>)</cite>, human tracking <cite class="ltx_cite ltx_citemacro_citep">(Insafutdinov et al., <a href="#bib.bib57" title="" class="ltx_ref">2017</a>)</cite>, Movies and animation, Virtual reality, Human-computer interaction, Video surveillance, Medical assistance, Self-driving, Sports motion analysis, etc.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text ltx_font_italic">Movies and animation:</span> The generation of various vivid digital characters is inseparable from the capture of human movements. Cheap and accurate human motion capture system can better promote the development of the digital entertainment industry.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text ltx_font_italic">Virtual reality:</span> Virtual reality is a very promising technology that can be applied in both education and entertainment. Estimation of human posture can further clarify the relation between human and virtual reality world and enhance the interactive experience.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_italic">Human–computer interaction (HCI):</span> HPE is very important for computers and robots to better understand the identification, location, and action of people. With the posture of human (e.g. gesture), computers and robots can execute instructions in an easy way and be more intelligent.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_italic">Video surveillance:</span> Video surveillance is one of the early applications to adopt HPE technology in tracking, action recognition, re-identification people within a specific range.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_italic">Medical assistance:</span> In the application of medical assistance, HPE can provide physicians with quantitative human motion information especially for rehabilitation training and physical therapy.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text ltx_font_italic">Self-driving:</span> Advanced self-driving has been developed rapidly. With HPE, self-driving cars can respond more appropriately to pedestrians and offer more comprehensive interaction with traffic coordinators.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p"><span id="S1.p9.1.1" class="ltx_text ltx_font_italic">Sport motion analysis:</span> Estimating players’ posture in sport videos can further obtain the statistics of athletes’ indicators (e.g. running distance, number of jumps). During training, HPE can provide a quantitative analysis of action details. In physical education, instructors can make more objective evaluations of students with HPE.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2006.01423/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="211" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 1: </span>Typical challenges of HPE in monocular images or videos. Example images are from Max Planck Institute for Informatics (MPII) dataset <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite>.</figcaption>
</figure>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">Monocular human pose estimation has some unique characteristics and challenges. As shown in Fig. <a href="#S1.F1" title="Fig. 1 ‣ 1 Introduction ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the challenges of human pose estimation mainly fall in three aspects:</p>
</div>
<div id="S1.p11" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Flexible body configuration indicates complex interdependent joints and high degree-of-freedom limbs, which may cause self-occlusions or rare/complex poses.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Diverse body appearance includes different clothing and self-similar parts.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Complex environment may cause foreground occlusion, occlusion or similar parts from nearby persons, various viewing angles, and truncation in the camera view.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p12" class="ltx_para">
<p id="S1.p12.1" class="ltx_p">The papers of human pose estimation can be categorized in different ways. Based on whether to use designed human body models or not, the methods can be categorized into generative methods (model-based) and discriminative methods (model-free). According to from which level (high-level abstraction or low-level pixel evidence) to start the processing, they can be classified into top-down methods and bottom-up methods. More details of different category strategies for HPE approaches are summarized in Table <a href="#S2.T2" title="Table 2 ‣ 2 Categories of HPE Methods and Human Body Models ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and described in Section <a href="#S2.SS1" title="2.1 HPE Method Categories ‣ 2 Categories of HPE Methods and Human Body Models ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>.</p>
</div>
<div id="S1.p13" class="ltx_para">
<p id="S1.p13.1" class="ltx_p">As listed in Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, with the development of human pose estimation in the past decades, several notable surveys summarized the research work in this area. The surveys <cite class="ltx_cite ltx_citemacro_citep">(Aggarwal and Cai, <a href="#bib.bib1" title="" class="ltx_ref">1999</a>; Gavrila, <a href="#bib.bib42" title="" class="ltx_ref">1999</a>; Poppe, <a href="#bib.bib134" title="" class="ltx_ref">2007</a>; Ji and Liu, <a href="#bib.bib67" title="" class="ltx_ref">2010</a>; Moeslund et al., <a href="#bib.bib111" title="" class="ltx_ref">2011</a>)</cite> reviewed the early work of human motion analysis in many aspects (e.g., detection and tracking, pose estimation, recognition) and described the relation between human pose estimation and other related tasks.
While <cite class="ltx_cite ltx_citemacro_citet">Hu et al. (<a href="#bib.bib54" title="" class="ltx_ref">2004</a>)</cite> summarized the research of human motion analysis for video surveillance application, the reviews <cite class="ltx_cite ltx_citemacro_citep">(Moeslund and Granum, <a href="#bib.bib109" title="" class="ltx_ref">2001</a>; Moeslund et al., <a href="#bib.bib110" title="" class="ltx_ref">2006</a>)</cite> focused on the human motion capture systems. More recent surveys were mainly focusing on relatively narrow directions, such as RGB-D-based action recognition<cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib18" title="" class="ltx_ref">2013</a>; Wang et al., <a href="#bib.bib172" title="" class="ltx_ref">2018b</a>)</cite>, 3D HPE <cite class="ltx_cite ltx_citemacro_citep">(Sminchisescu, <a href="#bib.bib150" title="" class="ltx_ref">2008</a>; Holte et al., <a href="#bib.bib52" title="" class="ltx_ref">2012</a>; Sarafianos et al., <a href="#bib.bib145" title="" class="ltx_ref">2016</a>)</cite>, model-based HPE <cite class="ltx_cite ltx_citemacro_citep">(Holte et al., <a href="#bib.bib52" title="" class="ltx_ref">2012</a>; Perez-Sala et al., <a href="#bib.bib128" title="" class="ltx_ref">2014</a>)</cite>, body parts-based HPE <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib94" title="" class="ltx_ref">2015</a>)</cite>, and monocular-based HPE <cite class="ltx_cite ltx_citemacro_citep">(Sminchisescu, <a href="#bib.bib150" title="" class="ltx_ref">2008</a>; Gong et al., <a href="#bib.bib47" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
<div id="S1.p14" class="ltx_para">
<p id="S1.p14.1" class="ltx_p">Different from existing review papers, this survey extensively summarizes the recent milestone work of deep learning-based human pose estimation methods, which were mainly published from 2014. In order to provide a comprehensive summary, this survey includes a few research work which has been discussed in some surveys <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib94" title="" class="ltx_ref">2015</a>; Gong et al., <a href="#bib.bib47" title="" class="ltx_ref">2016</a>; Sarafianos et al., <a href="#bib.bib145" title="" class="ltx_ref">2016</a>)</cite>, but most of the recent advances are not been presented in any survey before.</p>
</div>
<div id="S1.p15" class="ltx_para">
<p id="S1.p15.1" class="ltx_p">The remainder of this paper is organized as follows.
Section <a href="#S2" title="2 Categories of HPE Methods and Human Body Models ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> introduces the existing review papers for human motion analysis and HPE, different ways to category HPE methods, and the widely used human body models. Sections <a href="#S3" title="3 2D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S4" title="4 3D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> describe 2D HPE and 3D HPE approaches respectively. In each section, we further describe HPE approaches for both single person pose estimation and multi-person pose estimation. Since data are a very important and fundamental element for deep learning-based methods, the recent HPE datasets and the evaluation metrics are summarized in Section <a href="#S5" title="5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Finally, Section <a href="#S6" title="6 Conclusion and Future Research Directions ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> concludes the paper and discusses several promising future research directions.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of the related surveys of human motion analysis and HPE.</figcaption>
<table id="S1.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.1.2.1" class="ltx_text" style="font-size:80%;">No.</span></th>
<th id="S1.T1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Survey </span><math id="S1.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="S1.T1.1.1.1.1.1.m1.1a"><mo mathsize="80%" id="S1.T1.1.1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.1.1.m1.1b"><and id="S1.T1.1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.1.1.m1.1c">\&amp;</annotation></semantics></math><span id="S1.T1.1.1.1.1.1.2" class="ltx_text" style="font-size:80%;"> Reference</span></span>
</span>
</th>
<th id="S1.T1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T1.1.1.3.1" class="ltx_text" style="font-size:80%;">Venue</span></th>
<th id="S1.T1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.1.4.1.1.1" class="ltx_text" style="font-size:80%;">Content</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.2.1" class="ltx_tr">
<td id="S1.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S1.T1.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.2.1.1.1.1" class="ltx_tr">
<td id="S1.T1.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.2.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">1</span></td>
</tr>
</table>
</td>
<td id="S1.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.1.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.2.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Human motion analysis: A review </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.2.1.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Aggarwal and Cai<span id="S1.T1.1.2.1.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib1" title="" class="ltx_ref">1999</a><span id="S1.T1.1.2.1.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.2.1.3.1" class="ltx_text" style="font-size:80%;">CVIU</span></td>
<td id="S1.T1.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.1.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.2.1.4.1.1.1" class="ltx_text" style="font-size:80%;">A review of human motion analysis including body structure analysis, motion tracking and action recognition.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.3.2" class="ltx_tr">
<td id="S1.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">2</span></td>
<td id="S1.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.2.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.3.2.2.1.1.1" class="ltx_text" style="font-size:80%;">The visual analysis of human movement: A survey </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.3.2.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Gavrila<span id="S1.T1.1.3.2.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib42" title="" class="ltx_ref">1999</a><span id="S1.T1.1.3.2.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.3.2.3.1" class="ltx_text" style="font-size:80%;">CVIU</span></td>
<td id="S1.T1.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.2.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.3.2.4.1.1.1" class="ltx_text" style="font-size:80%;">A survey of whole-body and hand motion analysis.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.4.3" class="ltx_tr">
<td id="S1.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.4.3.1.1" class="ltx_text" style="font-size:80%;">3</span></td>
<td id="S1.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.3.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.4.3.2.1.1.1" class="ltx_text" style="font-size:80%;">A survey of computer vision-based human motion capture </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.4.3.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Moeslund and Granum<span id="S1.T1.1.4.3.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib109" title="" class="ltx_ref">2001</a><span id="S1.T1.1.4.3.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.4.3.3.1" class="ltx_text" style="font-size:80%;">CVIU</span></td>
<td id="S1.T1.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.3.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.4.3.4.1.1.1" class="ltx_text" style="font-size:80%;">An overview based on motion capture system, including initialization, tracking, pose estimation, and recognition.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.5.4" class="ltx_tr">
<td id="S1.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.5.4.1.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="S1.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.4.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.5.4.2.1.1.1" class="ltx_text" style="font-size:80%;">A survey on visual surveillance of object motion and behaviors </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.5.4.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Hu et al.<span id="S1.T1.1.5.4.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib54" title="" class="ltx_ref">2004</a><span id="S1.T1.1.5.4.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.5.4.3.1" class="ltx_text" style="font-size:80%;">TSMCS</span></td>
<td id="S1.T1.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.4.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.5.4.4.1.1.1" class="ltx_text" style="font-size:80%;">A summary of human motion analysis based one the framework of visual surveillance in dynamic scenes.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.6.5" class="ltx_tr">
<td id="S1.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.6.5.1.1" class="ltx_text" style="font-size:80%;">5</span></td>
<td id="S1.T1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.6.5.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.6.5.2.1.1.1" class="ltx_text" style="font-size:80%;">A survey of advances in vision-based human motion capture and analysis </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.6.5.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Moeslund et al.<span id="S1.T1.1.6.5.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib110" title="" class="ltx_ref">2006</a><span id="S1.T1.1.6.5.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.6.5.3.1" class="ltx_text" style="font-size:80%;">CVIU</span></td>
<td id="S1.T1.1.6.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.6.5.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.6.5.4.1.1.1" class="ltx_text" style="font-size:80%;">Further summary of human motion capture and analysis from 2000 to 2006, following </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.6.5.4.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Moeslund and Granum<span id="S1.T1.1.6.5.4.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib109" title="" class="ltx_ref">2001</a><span id="S1.T1.1.6.5.4.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S1.T1.1.6.5.4.1.1.5" class="ltx_text" style="font-size:80%;">.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.7.6" class="ltx_tr">
<td id="S1.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.7.6.1.1" class="ltx_text" style="font-size:80%;">6</span></td>
<td id="S1.T1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.7.6.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.7.6.2.1.1.1" class="ltx_text" style="font-size:80%;">Vision-based human motion analysis: An overview </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.7.6.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Poppe<span id="S1.T1.1.7.6.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib134" title="" class="ltx_ref">2007</a><span id="S1.T1.1.7.6.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.7.6.3.1" class="ltx_text" style="font-size:80%;">CVIU</span></td>
<td id="S1.T1.1.7.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.7.6.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.7.6.4.1.1.1" class="ltx_text" style="font-size:80%;">A summary of vision-based human motion analysis with markerless data.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.8.7" class="ltx_tr">
<td id="S1.T1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.8.7.1.1" class="ltx_text" style="font-size:80%;">7</span></td>
<td id="S1.T1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.7.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.8.7.2.1.1.1" class="ltx_text" style="font-size:80%;">3D human motion analysis in monocular video: techniques and challenges </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.8.7.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Sminchisescu<span id="S1.T1.1.8.7.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib150" title="" class="ltx_ref">2008</a><span id="S1.T1.1.8.7.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S1.T1.1.8.7.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.8.7.3.1.1" class="ltx_tr">
<td id="S1.T1.1.8.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.8.7.3.1.1.1.1" class="ltx_text" style="font-size:80%;">Book</span></td>
</tr>
<tr id="S1.T1.1.8.7.3.1.2" class="ltx_tr">
<td id="S1.T1.1.8.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.8.7.3.1.2.1.1" class="ltx_text" style="font-size:80%;">Chapter</span></td>
</tr>
</table>
</td>
<td id="S1.T1.1.8.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.7.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.8.7.4.1.1.1" class="ltx_text" style="font-size:80%;">An overview of reconstructing 3D human motion with video sequences from single-view camera.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.9.8" class="ltx_tr">
<td id="S1.T1.1.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.9.8.1.1" class="ltx_text" style="font-size:80%;">8</span></td>
<td id="S1.T1.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.9.8.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.9.8.2.1.1.1" class="ltx_text" style="font-size:80%;">Advances in view-invariant human motion analysis: A review </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.9.8.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Ji and Liu<span id="S1.T1.1.9.8.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib67" title="" class="ltx_ref">2010</a><span id="S1.T1.1.9.8.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.9.8.3.1" class="ltx_text" style="font-size:80%;">TSMCS</span></td>
<td id="S1.T1.1.9.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.9.8.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.9.8.4.1.1.1" class="ltx_text" style="font-size:80%;">A summary of human motion analysis, including human detection, view-invariant pose representation and estimation, and behavior understanding.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.10.9" class="ltx_tr">
<td id="S1.T1.1.10.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.10.9.1.1" class="ltx_text" style="font-size:80%;">9</span></td>
<td id="S1.T1.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.10.9.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.10.9.2.1.1.1" class="ltx_text" style="font-size:80%;">Visual analysis of humans </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.10.9.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Moeslund et al.<span id="S1.T1.1.10.9.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib111" title="" class="ltx_ref">2011</a><span id="S1.T1.1.10.9.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.10.9.3.1" class="ltx_text" style="font-size:80%;">Book</span></td>
<td id="S1.T1.1.10.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.10.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.10.9.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.10.9.4.1.1.1" class="ltx_text" style="font-size:80%;">A comprehensive overview of human analysis, including detection and tracking, pose estimation, recognition, and applications with human body and face.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.11.10" class="ltx_tr">
<td id="S1.T1.1.11.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.11.10.1.1" class="ltx_text" style="font-size:80%;">10</span></td>
<td id="S1.T1.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.11.10.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.11.10.2.1.1.1" class="ltx_text" style="font-size:80%;">Human pose estimation and activity recognition from multi-view videos: Comparative explorations of recent developments </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.11.10.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Holte et al.<span id="S1.T1.1.11.10.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib52" title="" class="ltx_ref">2012</a><span id="S1.T1.1.11.10.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.11.10.3.1" class="ltx_text" style="font-size:80%;">JSTSP</span></td>
<td id="S1.T1.1.11.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.11.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.11.10.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.11.10.4.1.1.1" class="ltx_text" style="font-size:80%;">A review of model-based 3D HPE and action recognition methods under multi-view.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.12.11" class="ltx_tr">
<td id="S1.T1.1.12.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.12.11.1.1" class="ltx_text" style="font-size:80%;">11</span></td>
<td id="S1.T1.1.12.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.12.11.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.12.11.2.1.1.1" class="ltx_text" style="font-size:80%;">A survey of human motion analysis using depth imagery </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.12.11.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Chen et al.<span id="S1.T1.1.12.11.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib18" title="" class="ltx_ref">2013</a><span id="S1.T1.1.12.11.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.12.11.3.1" class="ltx_text" style="font-size:80%;">PRL</span></td>
<td id="S1.T1.1.12.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.12.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.12.11.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.12.11.4.1.1.1" class="ltx_text" style="font-size:80%;">A survey of traditional RGB-D-based human action recognition methods, including description of sensors, corresponding datasets, and approaches.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.13.12" class="ltx_tr">
<td id="S1.T1.1.13.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.13.12.1.1" class="ltx_text" style="font-size:80%;">12</span></td>
<td id="S1.T1.1.13.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.13.12.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.13.12.2.1.1.1" class="ltx_text" style="font-size:80%;">A survey on model based approaches for 2D and 3D visual human pose recovery </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.13.12.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Perez-Sala et al.<span id="S1.T1.1.13.12.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib128" title="" class="ltx_ref">2014</a><span id="S1.T1.1.13.12.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.13.12.3.1" class="ltx_text" style="font-size:80%;">Sensors</span></td>
<td id="S1.T1.1.13.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.13.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.13.12.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.13.12.4.1.1.1" class="ltx_text" style="font-size:80%;">A survey of model-based approaches for HPE, grouped in five main modules: appearance, viewpoint, spatial relations, temporal consistence, and behavior.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.14.13" class="ltx_tr">
<td id="S1.T1.1.14.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.14.13.1.1" class="ltx_text" style="font-size:80%;">13</span></td>
<td id="S1.T1.1.14.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.14.13.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.14.13.2.1.1.1" class="ltx_text" style="font-size:80%;">A survey of human pose estimation: the body parts parsing based methods </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.14.13.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Liu et al.<span id="S1.T1.1.14.13.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib94" title="" class="ltx_ref">2015</a><span id="S1.T1.1.14.13.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.14.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.14.13.3.1" class="ltx_text" style="font-size:80%;">JVCIR</span></td>
<td id="S1.T1.1.14.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.14.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.14.13.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.14.13.4.1.1.1" class="ltx_text" style="font-size:80%;">A survey of body parts parsing-based HPE methods under both single-view and multiple-view from different input sources(images, videos, depth).</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.15.14" class="ltx_tr">
<td id="S1.T1.1.15.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.15.14.1.1" class="ltx_text" style="font-size:80%;">14</span></td>
<td id="S1.T1.1.15.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.15.14.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.15.14.2.1.1.1" class="ltx_text" style="font-size:80%;">Human pose estimation from monocular images: A comprehensive survey </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.15.14.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Gong et al.<span id="S1.T1.1.15.14.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib47" title="" class="ltx_ref">2016</a><span id="S1.T1.1.15.14.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.15.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.15.14.3.1" class="ltx_text" style="font-size:80%;">Sensors</span></td>
<td id="S1.T1.1.15.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.15.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.15.14.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.15.14.4.1.1.1" class="ltx_text" style="font-size:80%;">A survey of monocular-based traditional HPE methods with a few deep learning-based methods.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.16.15" class="ltx_tr">
<td id="S1.T1.1.16.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.16.15.1.1" class="ltx_text" style="font-size:80%;">15</span></td>
<td id="S1.T1.1.16.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.16.15.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.16.15.2.1.1.1" class="ltx_text" style="font-size:80%;">3d human pose estimation: A review of the literature and analysis of covariates </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.16.15.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Sarafianos et al.<span id="S1.T1.1.16.15.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib145" title="" class="ltx_ref">2016</a><span id="S1.T1.1.16.15.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.16.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.16.15.3.1" class="ltx_text" style="font-size:80%;">CVIU</span></td>
<td id="S1.T1.1.16.15.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.16.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.16.15.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.16.15.4.1.1.1" class="ltx_text" style="font-size:80%;">A review of 3D HPE methods with different type of inputs(e.g., single image or video, monocular or multi-view).</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.17.16" class="ltx_tr">
<td id="S1.T1.1.17.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.17.16.1.1" class="ltx_text" style="font-size:80%;">16</span></td>
<td id="S1.T1.1.17.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.17.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.17.16.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.17.16.2.1.1.1" class="ltx_text" style="font-size:80%;">RGB-D-based human motion recognition with deep learning: A survey </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.17.16.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Wang et al.<span id="S1.T1.1.17.16.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib172" title="" class="ltx_ref">2018b</a><span id="S1.T1.1.17.16.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td id="S1.T1.1.17.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.17.16.3.1" class="ltx_text" style="font-size:80%;">CVIU</span></td>
<td id="S1.T1.1.17.16.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.17.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.17.16.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.17.16.4.1.1.1" class="ltx_text" style="font-size:80%;">A survey of RGB-D-based motion recognition in four categories: RGB-based, depth-based, skeleton-based, and RGB-D-based.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.18.17" class="ltx_tr">
<td id="S1.T1.1.18.17.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.18.17.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">17</span></td>
<td id="S1.T1.1.18.17.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S1.T1.1.18.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.18.17.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S1.T1.1.18.17.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods</span></span>
</span>
</td>
<td id="S1.T1.1.18.17.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S1.T1.1.18.17.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Ours</span></td>
<td id="S1.T1.1.18.17.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S1.T1.1.18.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.18.17.4.1.1" class="ltx_p" style="width:241.8pt;"><span id="S1.T1.1.18.17.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A comprehensive survey of deep learning-based monocular HPE research and human pose datasets, organized into four groups: 2D single HPE, 2D multi-HPE, 3D single HPE and 3D multi-HPE</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Categories of HPE Methods and Human Body Models</h2>

<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>The Categories of deep learning-based monocular human pose estimation.</figcaption>
<table id="S2.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.3.1.1" class="ltx_tr">
<td id="S2.T2.3.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T2.3.1.1.1.1" class="ltx_text" style="font-size:80%;">Direction</span></td>
<td id="S2.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.3.1.1.2.1" class="ltx_text" style="font-size:80%;">Sub-direction</span></td>
<td id="S2.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.3.1.1.3.1" class="ltx_text" style="font-size:80%;">Categories</span></td>
<td id="S2.T2.3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.1.1.4.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.1.1.4.1.1.1" class="ltx_text" style="font-size:80%;">Sub-categories</span></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.2.2" class="ltx_tr">
<td id="S2.T2.3.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="12"><span id="S2.T2.3.2.2.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-113.8pt;">2D HPE</span></td>
<td id="S2.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="8"><span id="S2.T2.3.2.2.2.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-71.1pt;">2D Single</span></td>
<td id="S2.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S2.T2.3.2.2.3.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-22.8pt;">Regression-based</span></td>
<td id="S2.T2.3.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.3.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.2.2.4.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.2.2.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(1) Direct prediction</span><span id="S2.T2.3.2.2.4.1.1.2" class="ltx_text" style="font-size:80%;">: </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.2.2.4.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Krizhevsky et al.<span id="S2.T2.3.2.2.4.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib78" title="" class="ltx_ref">2012</a><span id="S2.T2.3.2.2.4.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.2.2.4.1.1.6" class="ltx_text" style="font-size:80%;">, on video </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.2.2.4.1.1.7.1" class="ltx_text" style="font-size:80%;">(</span>Pfister et al.<span id="S2.T2.3.2.2.4.1.1.8.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib130" title="" class="ltx_ref">2014</a><span id="S2.T2.3.2.2.4.1.1.9.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.3.3" class="ltx_tr">
<td id="S2.T2.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T2.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.3.3.1.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.3.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(2) Supervision improvement</span><span id="S2.T2.3.3.3.1.1.1.2" class="ltx_text" style="font-size:80%;">: transform heatmaps to joint coordinates </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.3.3.1.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Luvizon et al.<span id="S2.T2.3.3.3.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib99" title="" class="ltx_ref">2017</a>; Nibali et al.<span id="S2.T2.3.3.3.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib116" title="" class="ltx_ref">2018</a><span id="S2.T2.3.3.3.1.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.3.3.1.1.1.6" class="ltx_text" style="font-size:80%;">, recursive refinement </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.3.3.1.1.1.7.1" class="ltx_text" style="font-size:80%;">(</span>Carreira et al.<span id="S2.T2.3.3.3.1.1.1.8.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib14" title="" class="ltx_ref">2016</a><span id="S2.T2.3.3.3.1.1.1.9.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.3.3.1.1.1.10" class="ltx_text" style="font-size:80%;">, bone-based constraint </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.3.3.1.1.1.11.1" class="ltx_text" style="font-size:80%;">(</span>Sun et al.<span id="S2.T2.3.3.3.1.1.1.12.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib152" title="" class="ltx_ref">2017</a><span id="S2.T2.3.3.3.1.1.1.13.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.4.4" class="ltx_tr">
<td id="S2.T2.3.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T2.3.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.4.4.1.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.4.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(3) Multi-task</span><span id="S2.T2.3.4.4.1.1.1.2" class="ltx_text" style="font-size:80%;">: with body part detection </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.4.4.1.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Li et al.<span id="S2.T2.3.4.4.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib88" title="" class="ltx_ref">2014</a><span id="S2.T2.3.4.4.1.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.4.4.1.1.1.6" class="ltx_text" style="font-size:80%;">, with person detection and action classification </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.4.4.1.1.1.7.1" class="ltx_text" style="font-size:80%;">(</span>Gkioxari et al.<span id="S2.T2.3.4.4.1.1.1.8.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib44" title="" class="ltx_ref">2014a</a><span id="S2.T2.3.4.4.1.1.1.9.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.4.4.1.1.1.10" class="ltx_text" style="font-size:80%;">, with heatmap-based joint detection </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.4.4.1.1.1.11.1" class="ltx_text" style="font-size:80%;">(</span>Fan et al.<span id="S2.T2.3.4.4.1.1.1.12.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib37" title="" class="ltx_ref">2015</a><span id="S2.T2.3.4.4.1.1.1.13.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.4.4.1.1.1.14" class="ltx_text" style="font-size:80%;">, with action recognition on video sequences </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.4.4.1.1.1.15.1" class="ltx_text" style="font-size:80%;">(</span>Luvizon et al.<span id="S2.T2.3.4.4.1.1.1.16.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib98" title="" class="ltx_ref">2018</a><span id="S2.T2.3.4.4.1.1.1.17.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.5.5" class="ltx_tr">
<td id="S2.T2.3.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="5"><span id="S2.T2.3.5.5.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-45.5pt;">Detection-based</span></td>
<td id="S2.T2.3.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.3.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.5.5.2.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.5.5.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(1) Patch-based</span><span id="S2.T2.3.5.5.2.1.1.2" class="ltx_text" style="font-size:80%;">: </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.5.5.2.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Jain et al.<span id="S2.T2.3.5.5.2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib63" title="" class="ltx_ref">2013</a>; Chen and Yuille<span id="S2.T2.3.5.5.2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib20" title="" class="ltx_ref">2014</a>; Ramakrishna et al.<span id="S2.T2.3.5.5.2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib137" title="" class="ltx_ref">2014</a><span id="S2.T2.3.5.5.2.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.6.6" class="ltx_tr">
<td id="S2.T2.3.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T2.3.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.6.6.1.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.6.6.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(2) Network design</span><span id="S2.T2.3.6.6.1.1.1.2" class="ltx_text" style="font-size:80%;">: </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.6.6.1.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Tompson et al.<span id="S2.T2.3.6.6.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib163" title="" class="ltx_ref">2015</a>; Bulat and Tzimiropoulos<span id="S2.T2.3.6.6.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib12" title="" class="ltx_ref">2016</a>; Xiao et al.<span id="S2.T2.3.6.6.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib176" title="" class="ltx_ref">2018</a><span id="S2.T2.3.6.6.1.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.6.6.1.1.1.6" class="ltx_text" style="font-size:80%;">, multi-scale inputs </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.6.6.1.1.1.7.1" class="ltx_text" style="font-size:80%;">(</span>Rafi et al.<span id="S2.T2.3.6.6.1.1.1.8.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib136" title="" class="ltx_ref">2016</a><span id="S2.T2.3.6.6.1.1.1.9.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.6.6.1.1.1.10" class="ltx_text" style="font-size:80%;">, heatmap-based improvement </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.6.6.1.1.1.11.1" class="ltx_text" style="font-size:80%;">(</span>Papandreou et al.<span id="S2.T2.3.6.6.1.1.1.12.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib123" title="" class="ltx_ref">2017</a><span id="S2.T2.3.6.6.1.1.1.13.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.6.6.1.1.1.14" class="ltx_text" style="font-size:80%;">, Hourglass </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.6.6.1.1.1.15.1" class="ltx_text" style="font-size:80%;">(</span>Newell et al.<span id="S2.T2.3.6.6.1.1.1.16.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib115" title="" class="ltx_ref">2016</a><span id="S2.T2.3.6.6.1.1.1.17.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.6.6.1.1.1.18" class="ltx_text" style="font-size:80%;">, CPM </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.6.6.1.1.1.19.1" class="ltx_text" style="font-size:80%;">(</span>Wei et al.<span id="S2.T2.3.6.6.1.1.1.20.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib174" title="" class="ltx_ref">2016</a><span id="S2.T2.3.6.6.1.1.1.21.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.6.6.1.1.1.22" class="ltx_text" style="font-size:80%;">, PRM </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.6.6.1.1.1.23.1" class="ltx_text" style="font-size:80%;">(</span>Yang et al.<span id="S2.T2.3.6.6.1.1.1.24.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib177" title="" class="ltx_ref">2017</a><span id="S2.T2.3.6.6.1.1.1.25.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.6.6.1.1.1.26" class="ltx_text" style="font-size:80%;">, feed forward module </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.6.6.1.1.1.27.1" class="ltx_text" style="font-size:80%;">(</span>Belagiannis and Zisserman<span id="S2.T2.3.6.6.1.1.1.28.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib7" title="" class="ltx_ref">2017</a><span id="S2.T2.3.6.6.1.1.1.29.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.6.6.1.1.1.30" class="ltx_text" style="font-size:80%;">, HRNet </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.6.6.1.1.1.31.1" class="ltx_text" style="font-size:80%;">(</span>Sun et al.<span id="S2.T2.3.6.6.1.1.1.32.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib151" title="" class="ltx_ref">2019</a><span id="S2.T2.3.6.6.1.1.1.33.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.6.6.1.1.1.34" class="ltx_text" style="font-size:80%;">, GAN </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.6.6.1.1.1.35.1" class="ltx_text" style="font-size:80%;">(</span>Chou et al.<span id="S2.T2.3.6.6.1.1.1.36.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib23" title="" class="ltx_ref">2017</a>; Chen et al.<span id="S2.T2.3.6.6.1.1.1.36.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib21" title="" class="ltx_ref">2017</a>; Peng et al.<span id="S2.T2.3.6.6.1.1.1.36.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib127" title="" class="ltx_ref">2018</a><span id="S2.T2.3.6.6.1.1.1.37.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.7.7" class="ltx_tr">
<td id="S2.T2.3.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T2.3.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.7.7.1.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.7.7.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(3) Body structure constraint</span><span id="S2.T2.3.7.7.1.1.1.2" class="ltx_text" style="font-size:80%;">: </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.7.7.1.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Tompson et al.<span id="S2.T2.3.7.7.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib164" title="" class="ltx_ref">2014</a>; Lifshitz et al.<span id="S2.T2.3.7.7.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib91" title="" class="ltx_ref">2016</a>; Yang et al.<span id="S2.T2.3.7.7.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib178" title="" class="ltx_ref">2016</a>; Gkioxari et al.<span id="S2.T2.3.7.7.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib46" title="" class="ltx_ref">2016</a>; Chu et al.<span id="S2.T2.3.7.7.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib24" title="" class="ltx_ref">2016</a>, <a href="#bib.bib25" title="" class="ltx_ref">2017</a>; Ning et al.<span id="S2.T2.3.7.7.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib119" title="" class="ltx_ref">2018</a>; Ke et al.<span id="S2.T2.3.7.7.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib74" title="" class="ltx_ref">2018</a>; Tang et al.<span id="S2.T2.3.7.7.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib157" title="" class="ltx_ref">2018a</a>; Tang and Wu<span id="S2.T2.3.7.7.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib156" title="" class="ltx_ref">2019</a><span id="S2.T2.3.7.7.1.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.8.8" class="ltx_tr">
<td id="S2.T2.3.8.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T2.3.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.8.8.1.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.8.8.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(4) Temporal constraint</span><span id="S2.T2.3.8.8.1.1.1.2" class="ltx_text" style="font-size:80%;">: </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.8.8.1.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Jain et al.<span id="S2.T2.3.8.8.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib64" title="" class="ltx_ref">2014</a>; Pfister et al.<span id="S2.T2.3.8.8.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib129" title="" class="ltx_ref">2015</a>; Luo et al.<span id="S2.T2.3.8.8.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib97" title="" class="ltx_ref">2018</a><span id="S2.T2.3.8.8.1.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.9.9" class="ltx_tr">
<td id="S2.T2.3.9.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T2.3.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.9.9.1.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.9.9.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(5) Network compression</span><span id="S2.T2.3.9.9.1.1.1.2" class="ltx_text" style="font-size:80%;">: </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.9.9.1.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Tang et al.<span id="S2.T2.3.9.9.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib158" title="" class="ltx_ref">2018b</a>; Debnath et al.<span id="S2.T2.3.9.9.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib28" title="" class="ltx_ref">2018</a>; Feng et al.<span id="S2.T2.3.9.9.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib40" title="" class="ltx_ref">2019</a><span id="S2.T2.3.9.9.1.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.10.10" class="ltx_tr">
<td id="S2.T2.3.10.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S2.T2.3.10.10.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-22.8pt;">2D Multiple</span></td>
<td id="S2.T2.3.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.3.10.10.2.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-5.7pt;">Top-down</span></td>
<td id="S2.T2.3.10.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.3.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.10.10.3.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.10.10.3.1.1.1" class="ltx_text" style="font-size:80%;">coarse-to-fine </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.10.10.3.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Iqbal and Gall<span id="S2.T2.3.10.10.3.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib60" title="" class="ltx_ref">2016</a>; Huang et al.<span id="S2.T2.3.10.10.3.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib55" title="" class="ltx_ref">2017</a><span id="S2.T2.3.10.10.3.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.10.10.3.1.1.5" class="ltx_text" style="font-size:80%;">, bounding box refinement </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.10.10.3.1.1.6.1" class="ltx_text" style="font-size:80%;">(</span>Fang et al.<span id="S2.T2.3.10.10.3.1.1.7.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib38" title="" class="ltx_ref">2017</a><span id="S2.T2.3.10.10.3.1.1.8.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.10.10.3.1.1.9" class="ltx_text" style="font-size:80%;">, multi-level feature fusion </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.10.10.3.1.1.10.1" class="ltx_text" style="font-size:80%;">(</span>Xiao et al.<span id="S2.T2.3.10.10.3.1.1.11.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib176" title="" class="ltx_ref">2018</a>; Chen et al.<span id="S2.T2.3.10.10.3.1.1.11.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib22" title="" class="ltx_ref">2018</a><span id="S2.T2.3.10.10.3.1.1.12.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.10.10.3.1.1.13" class="ltx_text" style="font-size:80%;">, results refinement </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.10.10.3.1.1.14.1" class="ltx_text" style="font-size:80%;">(</span>Moon et al.<span id="S2.T2.3.10.10.3.1.1.15.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib112" title="" class="ltx_ref">2019</a><span id="S2.T2.3.10.10.3.1.1.16.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.11.11" class="ltx_tr">
<td id="S2.T2.3.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S2.T2.3.11.11.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-11.4pt;">Bottom-up</span></td>
<td id="S2.T2.3.11.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.3.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.11.11.2.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.11.11.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(1) Two-stage</span><span id="S2.T2.3.11.11.2.1.1.2" class="ltx_text" style="font-size:80%;">: DeepCut </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.11.11.2.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Pishchulin et al.<span id="S2.T2.3.11.11.2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib131" title="" class="ltx_ref">2016</a><span id="S2.T2.3.11.11.2.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.11.11.2.1.1.6" class="ltx_text" style="font-size:80%;">, DeeperCut </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.11.11.2.1.1.7.1" class="ltx_text" style="font-size:80%;">(</span>Insafutdinov et al.<span id="S2.T2.3.11.11.2.1.1.8.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib58" title="" class="ltx_ref">2016</a><span id="S2.T2.3.11.11.2.1.1.9.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.11.11.2.1.1.10" class="ltx_text" style="font-size:80%;">, OpenPose </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.11.11.2.1.1.11.1" class="ltx_text" style="font-size:80%;">(</span>Cao et al.<span id="S2.T2.3.11.11.2.1.1.12.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib13" title="" class="ltx_ref">2016</a><span id="S2.T2.3.11.11.2.1.1.13.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.11.11.2.1.1.14" class="ltx_text" style="font-size:80%;">, PPN </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.11.11.2.1.1.15.1" class="ltx_text" style="font-size:80%;">(</span>Nie et al.<span id="S2.T2.3.11.11.2.1.1.16.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib118" title="" class="ltx_ref">2018</a><span id="S2.T2.3.11.11.2.1.1.17.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.11.11.2.1.1.18" class="ltx_text" style="font-size:80%;">, PifPafNet </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.11.11.2.1.1.19.1" class="ltx_text" style="font-size:80%;">(</span>Kreiss et al.<span id="S2.T2.3.11.11.2.1.1.20.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib77" title="" class="ltx_ref">2019</a><span id="S2.T2.3.11.11.2.1.1.21.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.12.12" class="ltx_tr">
<td id="S2.T2.3.12.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T2.3.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.12.12.1.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.12.12.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(2) Single-stage</span><span id="S2.T2.3.12.12.1.1.1.2" class="ltx_text" style="font-size:80%;">: heatmaps and associative embedding maps </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.12.12.1.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Newell et al.<span id="S2.T2.3.12.12.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib114" title="" class="ltx_ref">2017</a><span id="S2.T2.3.12.12.1.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.13.13" class="ltx_tr">
<td id="S2.T2.3.13.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T2.3.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.13.13.1.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.13.13.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(3) Multi-task</span><span id="S2.T2.3.13.13.1.1.1.2" class="ltx_text" style="font-size:80%;">: instance segmentation </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.13.13.1.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Papandreou et al.<span id="S2.T2.3.13.13.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib122" title="" class="ltx_ref">2018</a><span id="S2.T2.3.13.13.1.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.13.13.1.1.1.6" class="ltx_text" style="font-size:80%;">, keypoint detection and semantic segmentation </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.13.13.1.1.1.7.1" class="ltx_text" style="font-size:80%;">(</span>Kocabas et al.<span id="S2.T2.3.13.13.1.1.1.8.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib76" title="" class="ltx_ref">2018</a><span id="S2.T2.3.13.13.1.1.1.9.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.14.14" class="ltx_tr">
<td id="S2.T2.3.14.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="5"><span id="S2.T2.3.14.14.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-51.2pt;">3D HPE</span></td>
<td id="S2.T2.3.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S2.T2.3.14.14.2.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-37.0pt;">3D Single</span></td>
<td id="S2.T2.3.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.3.14.14.3.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">Model-free</span></td>
<td id="S2.T2.3.14.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.3.14.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.14.14.4.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.14.14.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(1) Single-stage</span><span id="S2.T2.3.14.14.4.1.1.2" class="ltx_text" style="font-size:80%;">: direct prediction </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.14.14.4.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Li and Chan<span id="S2.T2.3.14.14.4.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib87" title="" class="ltx_ref">2014</a>; Pavlakos et al.<span id="S2.T2.3.14.14.4.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib125" title="" class="ltx_ref">2017</a><span id="S2.T2.3.14.14.4.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.14.14.4.1.1.6" class="ltx_text" style="font-size:80%;">, body structure constraint </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.14.14.4.1.1.7.1" class="ltx_text" style="font-size:80%;">(</span>Li et al.<span id="S2.T2.3.14.14.4.1.1.8.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib89" title="" class="ltx_ref">2015b</a>; Tekin et al.<span id="S2.T2.3.14.14.4.1.1.8.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib159" title="" class="ltx_ref">2016</a>; Sun et al.<span id="S2.T2.3.14.14.4.1.1.8.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib152" title="" class="ltx_ref">2017</a>; Pavlakos et al.<span id="S2.T2.3.14.14.4.1.1.8.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib124" title="" class="ltx_ref">2018a</a><span id="S2.T2.3.14.14.4.1.1.9.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.15.15" class="ltx_tr">
<td id="S2.T2.3.15.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T2.3.15.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.15.15.1.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.15.15.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(2) 2D-to-3D</span><span id="S2.T2.3.15.15.1.1.1.2" class="ltx_text" style="font-size:80%;">: </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.15.15.1.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Martinez et al.<span id="S2.T2.3.15.15.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib103" title="" class="ltx_ref">2017</a>; Zhou et al.<span id="S2.T2.3.15.15.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib184" title="" class="ltx_ref">2017</a>; Tekin et al.<span id="S2.T2.3.15.15.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib160" title="" class="ltx_ref">2017</a>; Li and Lee<span id="S2.T2.3.15.15.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib85" title="" class="ltx_ref">2019</a>; Qammaz and Argyros<span id="S2.T2.3.15.15.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib135" title="" class="ltx_ref">2019</a>; Chen and Ramanan<span id="S2.T2.3.15.15.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib17" title="" class="ltx_ref">2017</a>; Moreno-Noguer<span id="S2.T2.3.15.15.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib113" title="" class="ltx_ref">2017</a>; Wang et al.<span id="S2.T2.3.15.15.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib171" title="" class="ltx_ref">2018a</a>; Yang et al.<span id="S2.T2.3.15.15.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib179" title="" class="ltx_ref">2018</a><span id="S2.T2.3.15.15.1.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.16.16" class="ltx_tr">
<td id="S2.T2.3.16.16.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.3.16.16.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-14.2pt;">Model-based</span></td>
<td id="S2.T2.3.16.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.3.16.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.16.16.2.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.16.16.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(1) SMPL-based</span><span id="S2.T2.3.16.16.2.1.1.2" class="ltx_text" style="font-size:80%;">: </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.16.16.2.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Bogo et al.<span id="S2.T2.3.16.16.2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib8" title="" class="ltx_ref">2016</a>; Tan et al.<span id="S2.T2.3.16.16.2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib155" title="" class="ltx_ref">2017</a>; Pavlakos et al.<span id="S2.T2.3.16.16.2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib126" title="" class="ltx_ref">2018b</a>; Omran et al.<span id="S2.T2.3.16.16.2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib120" title="" class="ltx_ref">2018</a>; Varol et al.<span id="S2.T2.3.16.16.2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib167" title="" class="ltx_ref">2018</a>; Kanazawa et al.<span id="S2.T2.3.16.16.2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib73" title="" class="ltx_ref">2018</a>; Arnab et al.<span id="S2.T2.3.16.16.2.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib6" title="" class="ltx_ref">2019</a><span id="S2.T2.3.16.16.2.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.17.17" class="ltx_tr">
<td id="S2.T2.3.17.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T2.3.17.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.17.17.1.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.17.17.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(2) Kinematic model-based</span><span id="S2.T2.3.17.17.1.1.1.2" class="ltx_text" style="font-size:80%;">: </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.17.17.1.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Mehta et al.<span id="S2.T2.3.17.17.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib104" title="" class="ltx_ref">2017a</a>; Nie et al.<span id="S2.T2.3.17.17.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib117" title="" class="ltx_ref">2017</a>; Zhou et al.<span id="S2.T2.3.17.17.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib185" title="" class="ltx_ref">2016</a>; Mehta et al.<span id="S2.T2.3.17.17.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib107" title="" class="ltx_ref">2017c</a>; Rhodin et al.<span id="S2.T2.3.17.17.1.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib139" title="" class="ltx_ref">2018a</a><span id="S2.T2.3.17.17.1.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.18.18" class="ltx_tr">
<td id="S2.T2.3.18.18.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T2.3.18.18.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T2.3.18.18.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T2.3.18.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.18.18.3.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.18.18.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">(3) Other model-based</span><span id="S2.T2.3.18.18.3.1.1.2" class="ltx_text" style="font-size:80%;">: probabilistic model </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.18.18.3.1.1.3.1" class="ltx_text" style="font-size:80%;">(</span>Tome et al.<span id="S2.T2.3.18.18.3.1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib162" title="" class="ltx_ref">2017</a><span id="S2.T2.3.18.18.3.1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.3.19.19" class="ltx_tr">
<td id="S2.T2.3.19.19.1" class="ltx_td ltx_border_b ltx_border_l ltx_border_r"></td>
<td id="S2.T2.3.19.19.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T2.3.19.19.2.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-2.8pt;">3D Multiple</span></td>
<td id="S2.T2.3.19.19.3" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S2.T2.3.19.19.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T2.3.19.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.19.19.4.1.1" class="ltx_p" style="width:321.5pt;"><span id="S2.T2.3.19.19.4.1.1.1" class="ltx_text" style="font-size:80%;">bottom-up </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.19.19.4.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Mehta et al.<span id="S2.T2.3.19.19.4.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib106" title="" class="ltx_ref">2017b</a><span id="S2.T2.3.19.19.4.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.19.19.4.1.1.5" class="ltx_text" style="font-size:80%;">, top-down </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.19.19.4.1.1.6.1" class="ltx_text" style="font-size:80%;">(</span>Rogez et al.<span id="S2.T2.3.19.19.4.1.1.7.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib141" title="" class="ltx_ref">2017</a><span id="S2.T2.3.19.19.4.1.1.8.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.19.19.4.1.1.9" class="ltx_text" style="font-size:80%;">, SMPL-based </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.19.19.4.1.1.10.1" class="ltx_text" style="font-size:80%;">(</span>Zanfir et al.<span id="S2.T2.3.19.19.4.1.1.11.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib181" title="" class="ltx_ref">2018</a><span id="S2.T2.3.19.19.4.1.1.12.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.T2.3.19.19.4.1.1.13" class="ltx_text" style="font-size:80%;">, real-time </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.T2.3.19.19.4.1.1.14.1" class="ltx_text" style="font-size:80%;">(</span>Mehta et al.<span id="S2.T2.3.19.19.4.1.1.15.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib105" title="" class="ltx_ref">2019</a><span id="S2.T2.3.19.19.4.1.1.16.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>HPE Method Categories</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">This section summarizes the different categories of deep learning-based HPE methods based on different characteristics: 1) generative (human body model-based) and discriminative (human body model-free);
2) top-down (from high-level abstraction to low-level pixel evidence) and bottom-up (from low-level pixel evidence to high-level abstraction);
3) regression-based (directly mapping from input images to body joint positions) and detection-based (generating intermediate image patches or heatmaps of joint locations);
and 4) one-stage (end-to-end training) and multi-stage (stage-by-stage training).</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Generative V.S. Discriminative:</span>
The main difference between generative and discriminative methods is whether a method uses human body models or not. Based on the different representations of human body models, generative methods can be processed in different ways such as prior beliefs about the structure of the body model, geometrically projection from different views to 2D or 3D space, high-dimensional parametric space optimization in regression manners. More details of human body model representation can be found in Section <a href="#S2.SS2" title="2.2 Human Body models ‣ 2 Categories of HPE Methods and Human Body Models ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
Discriminative methods directly learn a mapping from input sources to human pose space (learning-based) or search in existing examples (example-based) without using human body models. Discriminative methods are usually faster than generative methods but may have less robustness for poses never trained with.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Top-down V.S. Bottom-up:</span>
For multi-person pose estimation, HPE methods can generally be classified as top-down and bottom-up methods according to the starting point of the prediction: high-level abstraction or low-level pixel evidence. Top-down methods start from high-level abstraction to first detect persons and generate the person locations in bounding boxes. Then pose estimation is conducted for each person. In contrast, bottom-up methods first predict all body parts of every person in the input image and then group them either by human body model fitting or other algorithms. Note that body parts could be joints, limbs, or small template patches depending on different methods.
With an increased number of people in an image, the computation cost of top-down methods significantly increases, while keeps stable for bottom-up methods. However, if there are some people with a large overlap, bottom-up methods face challenges to group corresponding body parts.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_bold">Regression-based V.S. Detection-based:</span>
Based on the different problem formulations, deep learning-based human pose estimation methods can be split into regression-based or detection-based methods. The regression-based methods directly map the input image to the coordinates of body joints or the parameters of human body models. The detection-based methods treat the body parts as detection targets based on two widely used representations: image patches and heatmaps of joint locations.
Direct mapping from images to joint coordinates is very difficult since it is a highly nonlinear problem, while small-region representation provides dense pixel information with stronger robustness. Compared to the original image size, the detected results of small-region representation limit the accuracy of the final joint coordinates.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p"><span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_bold">One-stage V.S. Multi-stage:</span>
The deep learning-based one-stage methods aim to map the input image to human poses by employing end-to-end networks, while multi-stage methods usually predict human pose in multiple stages and are accompanied by intermediate supervision.
For example, some multi-person pose estimation methods first detect the locations of people and then estimate the human pose for each detected person. Other 3D human pose estimation methods first predict joint locations in the 2D surface, then extend them to 3D space.
The training of one-stage methods is easier than multi-stage methods, but with less intermediate constraints.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p">This survey reviews the recent work in two main sections: 2D human pose estimation (Section 3) and 3D human pose estimation (Section 4). For each section, we further divide them into subsections based on their respective characteristics (see a summary of all the categories and the corresponding papers in Table <a href="#S2.T2" title="Table 2 ‣ 2 Categories of HPE Methods and Human Body Models ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.)
<span id="S2.SS1.p6.1.1" class="ltx_text" style="color:#0000FF;"> </span></p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Human Body models</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Human body modeling is a key component of HPE. Human body is a flexible and complex non-rigid object and has many specific characteristics like kinematic structure, body shape, surface texture, the position of body parts or body joints, etc. A mature model for human body is not necessary to contain all human body attributes but should satisfy the requirements for specific tasks to build and describe human body pose.
Based on different levels of representations and application scenarios, as shown in Fig. <a href="#S2.F2" title="Fig. 2 ‣ 2.2 Human Body models ‣ 2 Categories of HPE Methods and Human Body Models ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, there are three types of commonly used human body models in HPE: skeleton-based model, contour-based model, and volume-based model. For more detailed descriptions of human body models, we refer interested readers to two well-summarized papers <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib94" title="" class="ltx_ref">2015</a>; Gong et al., <a href="#bib.bib47" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2006.01423/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="212" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 2: </span>Commonly used human body models. (a) skeleton-based model; (b) contour-based models; (c) volume-based models.</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Skeleton-based Model:</span>
Skeleton-based model, also known as stick-figure or kinematic model, represents a set of joint (typically between 10 to 30) locations and the corresponding limb orientations following the human body skeletal structure. The skeleton-based model can also be described as a graph where vertices indicating joints and edges encoding constraints or prior connections of joints within the skeleton structure <cite class="ltx_cite ltx_citemacro_citep">(Felzenszwalb and Huttenlocher, <a href="#bib.bib39" title="" class="ltx_ref">2005</a>)</cite>. This human body topology is very simple and flexible which is widely utilized in both 2D and 3D HPE <cite class="ltx_cite ltx_citemacro_citep">(Cao et al., <a href="#bib.bib13" title="" class="ltx_ref">2016</a>; Mehta et al., <a href="#bib.bib107" title="" class="ltx_ref">2017c</a>)</cite> and human pose datasets <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>; Wu et al., <a href="#bib.bib175" title="" class="ltx_ref">2017</a>)</cite>.
With obvious advantages of simple and flexible representing, it also has many shortcomings such as lacking texture information which indicates there is no width and contour information of human body.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Contour-based Model:</span>
The contour-based model is widely used in earlier HPE methods which contains the rough width and contour information of body limbs and torso. Human body parts are approximately represented with rectangles or boundaries of person silhouette. Widely used contour-based models include cardboard models <cite class="ltx_cite ltx_citemacro_citep">(Ju et al., <a href="#bib.bib72" title="" class="ltx_ref">1996</a>)</cite> and Active Shape Models (ASMs) <cite class="ltx_cite ltx_citemacro_citep">(Cootes et al., <a href="#bib.bib26" title="" class="ltx_ref">1995</a>)</cite>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Volume-based Model:</span>
3D human body shapes and poses are generally represented by volume-based models with geometric shapes or meshes. Earlier geometric shapes for modeling body parts include cylinders, conics, etc. <cite class="ltx_cite ltx_citemacro_citep">(Sidenbladh et al., <a href="#bib.bib148" title="" class="ltx_ref">2000</a>)</cite>. Modern volume-based models are represented in mesh form, normally captured with 3D scans. Widely used volume-based models includes Shape Completion and Animation of People (SCAPE) <cite class="ltx_cite ltx_citemacro_citep">(Anguelov et al., <a href="#bib.bib5" title="" class="ltx_ref">2005</a>)</cite>, Skinned Multi-Person Linear model (SMPL) <cite class="ltx_cite ltx_citemacro_citep">(Loper et al., <a href="#bib.bib96" title="" class="ltx_ref">2015</a>)</cite>, and a unified deformation model <cite class="ltx_cite ltx_citemacro_citep">(Joo et al., <a href="#bib.bib71" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>2D Human Pose Estimation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">2D human pose estimation calculates the locations of human joints from monocular images or videos. Before deep learning brings a huge impact on vision-based human pose estimation, traditional 2D HPE algorithms adopt hand-craft feature extraction and sophisticated body models to obtain local representations and global pose structures <cite class="ltx_cite ltx_citemacro_citep">(Dantone et al., <a href="#bib.bib27" title="" class="ltx_ref">2013</a>; Chen and Yuille, <a href="#bib.bib20" title="" class="ltx_ref">2014</a>; Gkioxari et al., <a href="#bib.bib45" title="" class="ltx_ref">2014b</a>)</cite>. Here, the recent deep learning-based 2D human pose estimation methods are categorized into ”single person pose estimation” and ”multi-person pose estimation.”</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>2D single person pose estimation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">2D single person pose estimation is to localize body joint positions of a single person in an input image. For images with more persons, pre-processing is needed to crop the original image so that there is only one person in the input image such as using an upper-body detector <cite class="ltx_cite ltx_citemacro_citep">(Eichner and Ferrari, <a href="#bib.bib30" title="" class="ltx_ref">2012a</a>)</cite> or full-body detector <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib138" title="" class="ltx_ref">2015</a>)</cite>, and cropping from original images based on the annotated person center and body scale <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>; Newell et al., <a href="#bib.bib115" title="" class="ltx_ref">2016</a>)</cite>. Early work of introducing deep learning into human pose estimation mainly extended traditional HPE methods by simply replaced some components of frameworks by neural networks <cite class="ltx_cite ltx_citemacro_citep">(Jain et al., <a href="#bib.bib63" title="" class="ltx_ref">2013</a>; Ouyang et al., <a href="#bib.bib121" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Based on the different formulations of human pose estimation task, the proposed methods using CNNs can be classified into two categories: regression-based methods and detection-based methods. Regression-based methods attempt to learn a mapping from image to kinematic body joint coordinates by an end-to-end framework and generally directly produce joint coordinates <cite class="ltx_cite ltx_citemacro_citep">(Toshev and Szegedy, <a href="#bib.bib165" title="" class="ltx_ref">2014</a>)</cite>. Detection-based methods are intended to predict approximate locations of body parts <cite class="ltx_cite ltx_citemacro_citep">(Chen and Yuille, <a href="#bib.bib20" title="" class="ltx_ref">2014</a>)</cite> or joints <cite class="ltx_cite ltx_citemacro_citep">(Newell et al., <a href="#bib.bib115" title="" class="ltx_ref">2016</a>)</cite>, usually are supervised by a sequence of rectangular windows (each including a specific body part) <cite class="ltx_cite ltx_citemacro_citep">(Jain et al., <a href="#bib.bib63" title="" class="ltx_ref">2013</a>; Chen and Yuille, <a href="#bib.bib20" title="" class="ltx_ref">2014</a>)</cite> or heatmaps (each indicating one joint position by a 2D Gaussian distribution centered at the joint location) <cite class="ltx_cite ltx_citemacro_citep">(Newell et al., <a href="#bib.bib115" title="" class="ltx_ref">2016</a>; Wei et al., <a href="#bib.bib174" title="" class="ltx_ref">2016</a>)</cite>. Each of these two kinds of methods has its advantages and disadvantages. Direct regression learning of only one single point is a difficulty since it is a highly nonlinear problem and lacks robustness, while heatmap learning is supervised by dense pixel information which results in better robustness. Compared to the original image size, heatmap representation has much lower resolution due to the pooling operation in CNNs, which limits the accuracy of joint coordinate estimation. And obtaining joint coordinates from heatmap is normally a non-differentiable process that blocks the network to be trained end-to-end. The recent representative work for 2D single person pose estimation are summarized in Table <a href="#S3.T3" title="Table 3 ‣ 3.1 2D single person pose estimation ‣ 3 2D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the last column is the comparisons of PCKh@0.5 scores on the MPII testing set. More details of datasets and evaluation metrics are described in Section <a href="#S5" title="5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Summary of 2D single person pose estimation methods. Note that the last column shows the PCKh@0.5 scores on the Max Planck Institute for Informatics (MPII) Human Pose testing set.</figcaption>
<table id="S3.T3.16" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.16.17.1" class="ltx_tr">
<td id="S3.T3.16.17.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T3.16.17.1.1.1" class="ltx_text" style="font-size:80%;">Methods</span></td>
<td id="S3.T3.16.17.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.16.17.1.2.1" class="ltx_text" style="font-size:80%;">Backbone</span></td>
<td id="S3.T3.16.17.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.16.17.1.3.1" class="ltx_text" style="font-size:80%;">Input size</span></td>
<td id="S3.T3.16.17.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.16.17.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.16.17.1.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.16.17.1.4.1.1.1" class="ltx_text" style="font-size:80%;">Highlights</span></span>
</span>
</td>
<td id="S3.T3.16.17.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.16.17.1.5.1" class="ltx_text" style="font-size:80%;">PCKh (%)</span></td>
</tr>
<tr id="S3.T3.16.18.2" class="ltx_tr">
<td id="S3.T3.16.18.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="5"><span id="S3.T3.16.18.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Regression-based</span></td>
</tr>
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.1.1.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Toshev and Szegedy<span id="S3.T3.1.1.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib165" title="" class="ltx_ref">2014</a><span id="S3.T3.1.1.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S3.T3.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.1.1.3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T3.1.1.3.1.1.1.1" class="ltx_text" style="font-size:80%;">AlexNet</span></td>
</tr>
</table>
</td>
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.1.1.1.1" class="ltx_text" style="font-size:80%;">220</span><math id="S3.T3.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.1.1.1.m1.1a"><mo mathsize="80%" id="S3.T3.1.1.1.m1.1.1" xref="S3.T3.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><times id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.1.1.1.2" class="ltx_text" style="font-size:80%;">220</span>
</td>
<td id="S3.T3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.1.1.4.1.1.1" class="ltx_text" style="font-size:80%;">Direct regression, multi-stage refinement</span></span>
</span>
</td>
<td id="S3.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.1.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S3.T3.2.2" class="ltx_tr">
<td id="S3.T3.2.2.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.2.2.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Carreira et al.<span id="S3.T3.2.2.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib14" title="" class="ltx_ref">2016</a><span id="S3.T3.2.2.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.2.2.3.1" class="ltx_text" style="font-size:80%;">GoogleNet</span></td>
<td id="S3.T3.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.2.2.1.1" class="ltx_text" style="font-size:80%;">224</span><math id="S3.T3.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.2.2.1.m1.1a"><mo mathsize="80%" id="S3.T3.2.2.1.m1.1.1" xref="S3.T3.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.1.m1.1b"><times id="S3.T3.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.2.2.1.2" class="ltx_text" style="font-size:80%;">224</span>
</td>
<td id="S3.T3.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.2.2.4.1.1.1" class="ltx_text" style="font-size:80%;">Iterative error feedback refinement from initial pose.</span></span>
</span>
</td>
<td id="S3.T3.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.2.2.5.1" class="ltx_text" style="font-size:80%;">81.3</span></td>
</tr>
<tr id="S3.T3.3.3" class="ltx_tr">
<td id="S3.T3.3.3.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.3.3.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Sun et al.<span id="S3.T3.3.3.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib152" title="" class="ltx_ref">2017</a><span id="S3.T3.3.3.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.3.3.3.1" class="ltx_text" style="font-size:80%;">ResNet-50</span></td>
<td id="S3.T3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.3.3.1.1" class="ltx_text" style="font-size:80%;">224</span><math id="S3.T3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.3.3.1.m1.1a"><mo mathsize="80%" id="S3.T3.3.3.1.m1.1.1" xref="S3.T3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.1.m1.1b"><times id="S3.T3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.3.3.1.2" class="ltx_text" style="font-size:80%;">224</span>
</td>
<td id="S3.T3.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">Bone based representation as additional constraint, general for both 2D/3D HPE</span></span>
</span>
</td>
<td id="S3.T3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.3.3.5.1" class="ltx_text" style="font-size:80%;">86.4</span></td>
</tr>
<tr id="S3.T3.4.4" class="ltx_tr">
<td id="S3.T3.4.4.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.4.4.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Luvizon et al.<span id="S3.T3.4.4.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib99" title="" class="ltx_ref">2017</a><span id="S3.T3.4.4.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S3.T3.4.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.4.4.3.1.1" class="ltx_tr">
<td id="S3.T3.4.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T3.4.4.3.1.1.1.1" class="ltx_text" style="font-size:80%;">Inception-v4+</span></td>
</tr>
<tr id="S3.T3.4.4.3.1.2" class="ltx_tr">
<td id="S3.T3.4.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T3.4.4.3.1.2.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></td>
</tr>
</table>
</td>
<td id="S3.T3.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.4.4.1.1" class="ltx_text" style="font-size:80%;">256</span><math id="S3.T3.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.4.4.1.m1.1a"><mo mathsize="80%" id="S3.T3.4.4.1.m1.1.1" xref="S3.T3.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.1.m1.1b"><times id="S3.T3.4.4.1.m1.1.1.cmml" xref="S3.T3.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.4.4.1.2" class="ltx_text" style="font-size:80%;">256</span>
</td>
<td id="S3.T3.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.4.4.4.1.1.1" class="ltx_text" style="font-size:80%;">Multi-stage architecture, proposed soft-argmax function to convert heatmaps into joint locations</span></span>
</span>
</td>
<td id="S3.T3.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.4.4.5.1" class="ltx_text" style="font-size:80%;">91.2</span></td>
</tr>
<tr id="S3.T3.16.19.3" class="ltx_tr">
<td id="S3.T3.16.19.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="5"><span id="S3.T3.16.19.3.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Detection-based</span></td>
</tr>
<tr id="S3.T3.5.5" class="ltx_tr">
<td id="S3.T3.5.5.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.5.5.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Tompson et al.<span id="S3.T3.5.5.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib164" title="" class="ltx_ref">2014</a><span id="S3.T3.5.5.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.5.5.3.1" class="ltx_text" style="font-size:80%;">AlexNet</span></td>
<td id="S3.T3.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.5.5.1.1" class="ltx_text" style="font-size:80%;">320</span><math id="S3.T3.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.5.5.1.m1.1a"><mo mathsize="80%" id="S3.T3.5.5.1.m1.1.1" xref="S3.T3.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.1.m1.1b"><times id="S3.T3.5.5.1.m1.1.1.cmml" xref="S3.T3.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.5.5.1.2" class="ltx_text" style="font-size:80%;">240</span>
</td>
<td id="S3.T3.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.5.5.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.5.5.4.1.1.1" class="ltx_text" style="font-size:80%;">Heatmap representation, multi-scale input, MRF-like Spatial-Model</span></span>
</span>
</td>
<td id="S3.T3.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.5.5.5.1" class="ltx_text" style="font-size:80%;">79.6</span></td>
</tr>
<tr id="S3.T3.6.6" class="ltx_tr">
<td id="S3.T3.6.6.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.6.6.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Yang et al.<span id="S3.T3.6.6.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib178" title="" class="ltx_ref">2016</a><span id="S3.T3.6.6.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.6.6.3.1" class="ltx_text" style="font-size:80%;">VGG</span></td>
<td id="S3.T3.6.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.6.6.1.1" class="ltx_text" style="font-size:80%;">112</span><math id="S3.T3.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.6.6.1.m1.1a"><mo mathsize="80%" id="S3.T3.6.6.1.m1.1.1" xref="S3.T3.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.1.m1.1b"><times id="S3.T3.6.6.1.m1.1.1.cmml" xref="S3.T3.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.6.6.1.2" class="ltx_text" style="font-size:80%;">112</span>
</td>
<td id="S3.T3.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.6.6.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.6.6.4.1.1.1" class="ltx_text" style="font-size:80%;">Jointly learning DCNNs with deformable mixture of parts models</span></span>
</span>
</td>
<td id="S3.T3.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.6.6.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S3.T3.7.7" class="ltx_tr">
<td id="S3.T3.7.7.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.7.7.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Newell et al.<span id="S3.T3.7.7.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib115" title="" class="ltx_ref">2016</a><span id="S3.T3.7.7.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.7.7.3.1" class="ltx_text" style="font-size:80%;">Hourglass</span></td>
<td id="S3.T3.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.7.7.1.1" class="ltx_text" style="font-size:80%;">256</span><math id="S3.T3.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.7.7.1.m1.1a"><mo mathsize="80%" id="S3.T3.7.7.1.m1.1.1" xref="S3.T3.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.1.m1.1b"><times id="S3.T3.7.7.1.m1.1.1.cmml" xref="S3.T3.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.7.7.1.2" class="ltx_text" style="font-size:80%;">256</span>
</td>
<td id="S3.T3.7.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.7.7.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.7.7.4.1.1.1" class="ltx_text" style="font-size:80%;">Proposed stacked Hourglass architecture with intermediate supervision.</span></span>
</span>
</td>
<td id="S3.T3.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.7.7.5.1" class="ltx_text" style="font-size:80%;">90.9</span></td>
</tr>
<tr id="S3.T3.8.8" class="ltx_tr">
<td id="S3.T3.8.8.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.8.8.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Wei et al.<span id="S3.T3.8.8.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib174" title="" class="ltx_ref">2016</a><span id="S3.T3.8.8.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.8.8.3.1" class="ltx_text" style="font-size:80%;">CPM</span></td>
<td id="S3.T3.8.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.8.8.1.1" class="ltx_text" style="font-size:80%;">368</span><math id="S3.T3.8.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.8.8.1.m1.1a"><mo mathsize="80%" id="S3.T3.8.8.1.m1.1.1" xref="S3.T3.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.1.m1.1b"><times id="S3.T3.8.8.1.m1.1.1.cmml" xref="S3.T3.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.8.8.1.2" class="ltx_text" style="font-size:80%;">368</span>
</td>
<td id="S3.T3.8.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.8.8.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.8.8.4.1.1.1" class="ltx_text" style="font-size:80%;">Proposed Convolutional Pose Machines (CPM) with intermediate input and supervision, learn spatial correlations among body parts</span></span>
</span>
</td>
<td id="S3.T3.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.8.8.5.1" class="ltx_text" style="font-size:80%;">88.5</span></td>
</tr>
<tr id="S3.T3.9.9" class="ltx_tr">
<td id="S3.T3.9.9.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.9.9.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Chu et al.<span id="S3.T3.9.9.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib25" title="" class="ltx_ref">2017</a><span id="S3.T3.9.9.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.9.9.3.1" class="ltx_text" style="font-size:80%;">Hourglass</span></td>
<td id="S3.T3.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.9.9.1.1" class="ltx_text" style="font-size:80%;">256</span><math id="S3.T3.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.9.9.1.m1.1a"><mo mathsize="80%" id="S3.T3.9.9.1.m1.1.1" xref="S3.T3.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.9.9.1.m1.1b"><times id="S3.T3.9.9.1.m1.1.1.cmml" xref="S3.T3.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.9.9.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.9.9.1.2" class="ltx_text" style="font-size:80%;">256</span>
</td>
<td id="S3.T3.9.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.9.9.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.9.9.4.1.1.1" class="ltx_text" style="font-size:80%;">Multi-resolution attention maps from multi-scale features, proposed micro hourglass residual units to increase the receptive field</span></span>
</span>
</td>
<td id="S3.T3.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.9.9.5.1" class="ltx_text" style="font-size:80%;">91.5</span></td>
</tr>
<tr id="S3.T3.10.10" class="ltx_tr">
<td id="S3.T3.10.10.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.10.10.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Yang et al.<span id="S3.T3.10.10.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib177" title="" class="ltx_ref">2017</a><span id="S3.T3.10.10.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.10.10.3.1" class="ltx_text" style="font-size:80%;">Hourglass</span></td>
<td id="S3.T3.10.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.10.10.1.1" class="ltx_text" style="font-size:80%;">256</span><math id="S3.T3.10.10.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.10.10.1.m1.1a"><mo mathsize="80%" id="S3.T3.10.10.1.m1.1.1" xref="S3.T3.10.10.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.10.10.1.m1.1b"><times id="S3.T3.10.10.1.m1.1.1.cmml" xref="S3.T3.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.10.10.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.10.10.1.2" class="ltx_text" style="font-size:80%;">256</span>
</td>
<td id="S3.T3.10.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.10.10.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.10.10.4.1.1.1" class="ltx_text" style="font-size:80%;">Proposed Pyramid Residual Module (PRM) learns filters for input features with different resolutions</span></span>
</span>
</td>
<td id="S3.T3.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.10.10.5.1" class="ltx_text" style="font-size:80%;">92.0</span></td>
</tr>
<tr id="S3.T3.11.11" class="ltx_tr">
<td id="S3.T3.11.11.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.11.11.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Chen et al.<span id="S3.T3.11.11.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib21" title="" class="ltx_ref">2017</a><span id="S3.T3.11.11.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.11.11.3.1" class="ltx_text" style="font-size:80%;">conv-deconv</span></td>
<td id="S3.T3.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.11.11.1.1" class="ltx_text" style="font-size:80%;">256</span><math id="S3.T3.11.11.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.11.11.1.m1.1a"><mo mathsize="80%" id="S3.T3.11.11.1.m1.1.1" xref="S3.T3.11.11.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.11.11.1.m1.1b"><times id="S3.T3.11.11.1.m1.1.1.cmml" xref="S3.T3.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.11.11.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.11.11.1.2" class="ltx_text" style="font-size:80%;">256</span>
</td>
<td id="S3.T3.11.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.11.11.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.11.11.4.1.1.1" class="ltx_text" style="font-size:80%;">GAN, stacked conv-deconv architecture, multi-task for pose and occlusion, two discriminators for distinguishing whether the pose is ’real’ and the confidence is strong</span></span>
</span>
</td>
<td id="S3.T3.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.11.11.5.1" class="ltx_text" style="font-size:80%;">91.9</span></td>
</tr>
<tr id="S3.T3.12.12" class="ltx_tr">
<td id="S3.T3.12.12.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.12.12.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Peng et al.<span id="S3.T3.12.12.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib127" title="" class="ltx_ref">2018</a><span id="S3.T3.12.12.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.3.1" class="ltx_text" style="font-size:80%;">Hourglass</span></td>
<td id="S3.T3.12.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.12.12.1.1" class="ltx_text" style="font-size:80%;">256</span><math id="S3.T3.12.12.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.12.12.1.m1.1a"><mo mathsize="80%" id="S3.T3.12.12.1.m1.1.1" xref="S3.T3.12.12.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.12.12.1.m1.1b"><times id="S3.T3.12.12.1.m1.1.1.cmml" xref="S3.T3.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.12.12.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.12.12.1.2" class="ltx_text" style="font-size:80%;">256</span>
</td>
<td id="S3.T3.12.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.12.12.4.1.1.1" class="ltx_text" style="font-size:80%;">GAN, proposed augmentation network to generate data augmentations without looking for more data</span></span>
</span>
</td>
<td id="S3.T3.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.5.1" class="ltx_text" style="font-size:80%;">91.5</span></td>
</tr>
<tr id="S3.T3.13.13" class="ltx_tr">
<td id="S3.T3.13.13.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.13.13.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Ke et al.<span id="S3.T3.13.13.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib74" title="" class="ltx_ref">2018</a><span id="S3.T3.13.13.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.13.13.3.1" class="ltx_text" style="font-size:80%;">Hourglass</span></td>
<td id="S3.T3.13.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.13.13.1.1" class="ltx_text" style="font-size:80%;">256</span><math id="S3.T3.13.13.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.13.13.1.m1.1a"><mo mathsize="80%" id="S3.T3.13.13.1.m1.1.1" xref="S3.T3.13.13.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.13.13.1.m1.1b"><times id="S3.T3.13.13.1.m1.1.1.cmml" xref="S3.T3.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.13.13.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.13.13.1.2" class="ltx_text" style="font-size:80%;">256</span>
</td>
<td id="S3.T3.13.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.13.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.13.13.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.13.13.4.1.1.1" class="ltx_text" style="font-size:80%;">Improved Hourglass network with multi-scale intermediate supervision, multi-scale feature combination, structure-aware loss and data augmentation of joints masking</span></span>
</span>
</td>
<td id="S3.T3.13.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.13.13.5.1" class="ltx_text" style="font-size:80%;">92.1</span></td>
</tr>
<tr id="S3.T3.14.14" class="ltx_tr">
<td id="S3.T3.14.14.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.14.14.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Tang et al.<span id="S3.T3.14.14.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib157" title="" class="ltx_ref">2018a</a><span id="S3.T3.14.14.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.14.14.3.1" class="ltx_text" style="font-size:80%;">Hourglass</span></td>
<td id="S3.T3.14.14.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.14.14.1.1" class="ltx_text" style="font-size:80%;">256</span><math id="S3.T3.14.14.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.14.14.1.m1.1a"><mo mathsize="80%" id="S3.T3.14.14.1.m1.1.1" xref="S3.T3.14.14.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.14.14.1.m1.1b"><times id="S3.T3.14.14.1.m1.1.1.cmml" xref="S3.T3.14.14.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.14.14.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.14.14.1.2" class="ltx_text" style="font-size:80%;">256</span>
</td>
<td id="S3.T3.14.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.14.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.14.14.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.14.14.4.1.1.1" class="ltx_text" style="font-size:80%;">Compositional model, hierarchical representation of body parts for intermediate supervision</span></span>
</span>
</td>
<td id="S3.T3.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.14.14.5.1" class="ltx_text" style="font-size:80%;">92.3</span></td>
</tr>
<tr id="S3.T3.15.15" class="ltx_tr">
<td id="S3.T3.15.15.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.15.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Sun et al.<span id="S3.T3.15.15.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib151" title="" class="ltx_ref">2019</a><span id="S3.T3.15.15.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.15.15.3.1" class="ltx_text" style="font-size:80%;">HRNet</span></td>
<td id="S3.T3.15.15.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T3.15.15.1.1" class="ltx_text" style="font-size:80%;">256</span><math id="S3.T3.15.15.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.15.15.1.m1.1a"><mo mathsize="80%" id="S3.T3.15.15.1.m1.1.1" xref="S3.T3.15.15.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.15.15.1.m1.1b"><times id="S3.T3.15.15.1.m1.1.1.cmml" xref="S3.T3.15.15.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.15.15.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.15.15.1.2" class="ltx_text" style="font-size:80%;">256</span>
</td>
<td id="S3.T3.15.15.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.15.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.15.15.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.15.15.4.1.1.1" class="ltx_text" style="font-size:80%;">high-resolution representations of features across the whole network, multi-scale fusion.</span></span>
</span>
</td>
<td id="S3.T3.15.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.15.15.5.1" class="ltx_text" style="font-size:80%;">92.3</span></td>
</tr>
<tr id="S3.T3.16.16" class="ltx_tr">
<td id="S3.T3.16.16.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.16.16.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Tang and Wu<span id="S3.T3.16.16.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib156" title="" class="ltx_ref">2019</a><span id="S3.T3.16.16.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T3.16.16.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T3.16.16.3.1" class="ltx_text" style="font-size:80%;">Hourglass</span></td>
<td id="S3.T3.16.16.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T3.16.16.1.1" class="ltx_text" style="font-size:80%;">256</span><math id="S3.T3.16.16.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T3.16.16.1.m1.1a"><mo mathsize="80%" id="S3.T3.16.16.1.m1.1.1" xref="S3.T3.16.16.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.16.16.1.m1.1b"><times id="S3.T3.16.16.1.m1.1.1.cmml" xref="S3.T3.16.16.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.16.16.1.m1.1c">\times</annotation></semantics></math><span id="S3.T3.16.16.1.2" class="ltx_text" style="font-size:80%;">256</span>
</td>
<td id="S3.T3.16.16.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T3.16.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.16.16.4.1.1" class="ltx_p" style="width:256.1pt;"><span id="S3.T3.16.16.4.1.1.1" class="ltx_text" style="font-size:80%;">data-driven joint grouping, proposed part-based branching network (PBN) to learn representations specific to each part group.</span></span>
</span>
</td>
<td id="S3.T3.16.16.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T3.16.16.5.1" class="ltx_text" style="font-size:80%;">92.7</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Regression-based methods</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">AlexNet <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky et al., <a href="#bib.bib78" title="" class="ltx_ref">2012</a>)</cite> was one of the early networks for deep learning-based HPE methods due to its simple architecture and impressive performance. <cite class="ltx_cite ltx_citemacro_citet">Toshev and Szegedy (<a href="#bib.bib165" title="" class="ltx_ref">2014</a>)</cite> firstly attempted to train an AlexNet-like deep neural network to learn joint coordinates from full images in a very straightforward manner without using any body model or part detectors as shown in Fig. <a href="#S3.F3" title="Fig. 3 ‣ 3.1.1 Regression-based methods ‣ 3.1 2D single person pose estimation ‣ 3 2D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Moreover, a cascade architecture of multi-stage refining regressors is employed to refine the cropped images from the previous stage and show improved performance. <cite class="ltx_cite ltx_citemacro_citet">Pfister et al. (<a href="#bib.bib130" title="" class="ltx_ref">2014</a>)</cite> also applied an AlexNet-like network using a sequence of concatenated frames as input to predict the human pose in the videos.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2006.01423/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="212" height="49" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 3: </span>The framework of DeepPose <cite class="ltx_cite ltx_citemacro_citep">(Toshev and Szegedy, <a href="#bib.bib165" title="" class="ltx_ref">2014</a>)</cite>.</figcaption>
</figure>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Only using joints without the surrounding information lacks robustness. Converting heatmap supervision to numerical joint positions supervision can retain the advantages of both representations. <cite class="ltx_cite ltx_citemacro_citet">Luvizon et al. (<a href="#bib.bib99" title="" class="ltx_ref">2017</a>)</cite> proposed a Soft-argmax function to transform heatmaps to joint coordinates which can convert a detection-based network to a differentiable regression-based one. <cite class="ltx_cite ltx_citemacro_citet">Nibali et al. (<a href="#bib.bib116" title="" class="ltx_ref">2018</a>)</cite> designed a differentiable spatial to numerical transform (DSNT) layer to calculate joint coordinates from heatmaps, which worked well with low-resolution heatmaps.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">Prediction of joint coordinates directly from input images with few constrains is very hard, therefore more powerful networks were introduced with a refinement or body model structure.
<cite class="ltx_cite ltx_citemacro_citet">Carreira et al. (<a href="#bib.bib14" title="" class="ltx_ref">2016</a>)</cite> proposed an Iterative Error Feedback network based on GoogleNet which recursively processes the combination of the input image and output results. The final pose is improved from an initial mean pose after iterations.
<cite class="ltx_cite ltx_citemacro_citet">Sun et al. (<a href="#bib.bib152" title="" class="ltx_ref">2017</a>)</cite> proposed a structure-aware regression approach based on a ResNet-50. Instead of using joints to represent pose, a bone-based representation is designed by involving body structure information to achieve more stable results than only using joint positions. The bone-based representation also works on 3D HPE.</p>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.1" class="ltx_p">Networks handling multiple closely related tasks of human body may learn diverse features to improve the prediction of joint coordinates.
<cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib88" title="" class="ltx_ref">2014</a>)</cite> employed an AlexNet-like multi-task framework to handle the joint coordinate prediction task from full images in a regression way, and the body part detection task from image patches obtained by a sliding-window.
<cite class="ltx_cite ltx_citemacro_citet">Gkioxari et al. (<a href="#bib.bib44" title="" class="ltx_ref">2014a</a>)</cite> used a R-CNN architecture to synchronously detect person, estimate pose, and classify action.
<cite class="ltx_cite ltx_citemacro_citet">Fan et al. (<a href="#bib.bib37" title="" class="ltx_ref">2015</a>)</cite> proposed a dual-source deep CNNs which take image patches and full images as inputs and output heatmap represented joint detection results of sliding windows together with coordinate represented joint localization results. The final estimated posture is obtained from the combination of the two results.
<cite class="ltx_cite ltx_citemacro_citet">Luvizon et al. (<a href="#bib.bib98" title="" class="ltx_ref">2018</a>)</cite> designed a network that can jointly handle 2D/3D pose estimation and action recognition from video sequences. The pose estimated in the middle of the network can be used as a reference for action recognition.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Detection-based methods</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Detection-based methods are developed from body part detection methods. In traditional part-based HPE methods, body parts are first detected from image patch candidates and then are assembled to fit a human body model. The detected body parts in early work are relatively big and generally represented by rectangular sliding windows or patches. We refer to <cite class="ltx_cite ltx_citemacro_citep">(Poppe, <a href="#bib.bib134" title="" class="ltx_ref">2007</a>; Gong et al., <a href="#bib.bib47" title="" class="ltx_ref">2016</a>)</cite> for a more detailed introduction. Some early methods use neural networks as body part detectors to distinguish whether a candidate patch is a specific body part <cite class="ltx_cite ltx_citemacro_citep">(Jain et al., <a href="#bib.bib63" title="" class="ltx_ref">2013</a>)</cite>, classify a candidate patch among predefined templates <cite class="ltx_cite ltx_citemacro_citep">(Chen and Yuille, <a href="#bib.bib20" title="" class="ltx_ref">2014</a>)</cite> or predict the confidence map belonging to multiple classes <cite class="ltx_cite ltx_citemacro_citep">(Ramakrishna et al., <a href="#bib.bib137" title="" class="ltx_ref">2014</a>)</cite>. Body part detection methods are usually sensitive to complexity background and body occlusions. Therefore the independent image patches with only local appearance may not be sufficiently discriminative for body part detection.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">In order to provide more supervision information than just joint coordinates and to facilitate the training of CNNs, more recent work employed heatmap to indicate the ground truth of the joint location <cite class="ltx_cite ltx_citemacro_citep">(Tompson et al., <a href="#bib.bib164" title="" class="ltx_ref">2014</a>; Jain et al., <a href="#bib.bib64" title="" class="ltx_ref">2014</a>)</cite>. As shown in Fig. <a href="#S3.F4" title="Fig. 4 ‣ 3.1.2 Detection-based methods ‣ 3.1 2D single person pose estimation ‣ 3 2D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, each joint occupies a heatmap channel with a 2D Gaussian distribution centered at the target joint location. Moreover, <cite class="ltx_cite ltx_citemacro_citet">Papandreou et al. (<a href="#bib.bib123" title="" class="ltx_ref">2017</a>)</cite> proposed an improved representation of the joint location, which is a combination of binary activation heatmap and corresponding offset. Since heatmap representation is more robust than coordinate representation, most of the recent research is based on heatmap representation.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2006.01423/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="212" height="42" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 4: </span>Heatmap representation of different joints.</figcaption>
</figure>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">The neural network architecture is very important to make better use of input information.
Some approaches are mainly based on classic networks with appropriate improvements, such as GoogLeNet-based network with multi-scale inputs <cite class="ltx_cite ltx_citemacro_citep">(Rafi et al., <a href="#bib.bib136" title="" class="ltx_ref">2016</a>)</cite>, ResNet-based network with deconvolutional layers <cite class="ltx_cite ltx_citemacro_citet">Xiao et al. (<a href="#bib.bib176" title="" class="ltx_ref">2018</a>)</cite>.
In terms of iterative refinement, some work designed networks in a multi-stage style to refine results from coarse prediction via end-to-end learning <cite class="ltx_cite ltx_citemacro_citep">(Tompson et al., <a href="#bib.bib163" title="" class="ltx_ref">2015</a>; Bulat and Tzimiropoulos, <a href="#bib.bib12" title="" class="ltx_ref">2016</a>; Newell et al., <a href="#bib.bib115" title="" class="ltx_ref">2016</a>; Wei et al., <a href="#bib.bib174" title="" class="ltx_ref">2016</a>; Yang et al., <a href="#bib.bib177" title="" class="ltx_ref">2017</a>; Belagiannis and Zisserman, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>. Such networks generally use intermediate supervision to address vanishing gradients.
<cite class="ltx_cite ltx_citemacro_citet">Newell et al. (<a href="#bib.bib115" title="" class="ltx_ref">2016</a>)</cite> proposed a novel <span id="S3.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_italic">stacked hourglass</span> architecture by using a residual module as the component unit.
<cite class="ltx_cite ltx_citemacro_citet">Wei et al. (<a href="#bib.bib174" title="" class="ltx_ref">2016</a>)</cite> proposed a multi-stage prediction framework with input image for each stage.
<cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a href="#bib.bib177" title="" class="ltx_ref">2017</a>)</cite> designed a Pyramid Residual Module (PRMs) to replace the residual module of the Hourglass network to enhance the invariance across scales of DCNNs by learning features on various scales.
<cite class="ltx_cite ltx_citemacro_citet">Belagiannis and Zisserman (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite> combined a 7 layers feedforward module with a recurrent module to iteratively refine the results. This model learns to predict location heatmaps for both joints and body limbs. Also, they analyzed keypoint visibility with unbalanced ground truth distribution.
To keep high-resolution representations of features across the whole network, <cite class="ltx_cite ltx_citemacro_citet">Sun et al. (<a href="#bib.bib151" title="" class="ltx_ref">2019</a>)</cite> proposed a novel High-Resolution Net (HRNet) with multi-scale feature fusion.</p>
</div>
<div id="S3.SS1.SSS2.p4" class="ltx_para">
<p id="S3.SS1.SSS2.p4.1" class="ltx_p">Different from earlier work which attempted to fit detected body parts into body models, some recent work tried to encode human body structure information into networks.
<cite class="ltx_cite ltx_citemacro_citet">Tompson et al. (<a href="#bib.bib164" title="" class="ltx_ref">2014</a>)</cite> jointly trained a network with a MRF-like spatial-model for learning typical spatial relations between joints.
<cite class="ltx_cite ltx_citemacro_citet">Lifshitz et al. (<a href="#bib.bib91" title="" class="ltx_ref">2016</a>)</cite> discretized an image into log-polar bins centered around each joint and employed a VGG-based network to predict joint category confident for each pair-wise joints (binary terms). With all relative confident scores, the final heatmap for each joint can be generated by a deconvolutional network.
<cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a href="#bib.bib178" title="" class="ltx_ref">2016</a>)</cite> designed a two-stage network. Stage one is a convolutional neural network to predict joint locations in heatmap representation. Stage two is a message-passing model connected manually according to the human body structure to find optimal joint locations with a max-sum algorithm.
<cite class="ltx_cite ltx_citemacro_citet">Gkioxari et al. (<a href="#bib.bib46" title="" class="ltx_ref">2016</a>)</cite> proposed a convolutional Recurrent Neural Network to output joint location one by one following a chain model. The output of each step depends on both the input image and the previously predicted output. The network can handle both images and videos with different connection strategy.
<cite class="ltx_cite ltx_citemacro_citet">Chu et al. (<a href="#bib.bib24" title="" class="ltx_ref">2016</a>)</cite> proposed to transform kernels by a bi-directional tree to pass information between corresponding joints in a tree body model.
<cite class="ltx_cite ltx_citemacro_citet">Chu et al. (<a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite> replaced the residual modules of the Hourglass network with more sophisticated ones. The Conditional Random Field (CRF) is utilized for attention maps as intermediate supervisions for learning body structure information.
<cite class="ltx_cite ltx_citemacro_citep">(Ning et al., <a href="#bib.bib119" title="" class="ltx_ref">2018</a>)</cite> designed a fractal network to impose body prior knowledge to guide the network. The external knowledge visual features are encoded into the basic network by using a learned projection matrix.
<cite class="ltx_cite ltx_citemacro_citet">Ke et al. (<a href="#bib.bib74" title="" class="ltx_ref">2018</a>)</cite> proposed a multi-scale structure-aware network based on Hourglass network with multi-scale supervision, multi-scale feature combination, structure-aware loss, and data augmentation of joints masking.
On the basic framework of Hourglass network, <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a href="#bib.bib157" title="" class="ltx_ref">2018a</a>)</cite> designed a hierarchical representation of body parts for intermediate supervision to replace heatmap for each joint. Thus the network learns the bottom-up/top-down body structure, rather than only scattered joints.
<cite class="ltx_cite ltx_citemacro_citet">Tang and Wu (<a href="#bib.bib156" title="" class="ltx_ref">2019</a>)</cite> proposed a part-based branching network (PBN) to learn specific representations of each part group rather than predict all joint heatmaps from one branch. The data-driven part groups are then split by calculating mutual information of joints.</p>
</div>
<div id="S3.SS1.SSS2.p5" class="ltx_para">
<p id="S3.SS1.SSS2.p5.1" class="ltx_p">Generative Adversarial Networks (GANs) are also employed to provide adversarial supervision for learning body structure or network training.
<cite class="ltx_cite ltx_citemacro_citet">Chou et al. (<a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite> introduced adversarial learning with two same Hourglass networks as generator and discriminator respectively. The generator predicts heatmap location of each joint, while the discriminator distinguishes ground truth heatmaps from generated heatmaps.
<cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> proposed a structure-aware convolutional network with one generator and two discriminators to incorporate priors of human body structure. The generator is designed from the Hourglass network to predict joint heatmaps as well as occlusion heatmaps. The pose discriminator can discriminate against reasonable body configuration from unreasonable body configuration. The confidence discriminator shows the confidence score of predictions.
<cite class="ltx_cite ltx_citemacro_citet">Peng et al. (<a href="#bib.bib127" title="" class="ltx_ref">2018</a>)</cite> studied how to jointly optimize data augmentation and network training without looking for more data. Instead of using random data augmentation, they applied augmentations to increase the network loss while the pose network learns from the generated augmentations.</p>
</div>
<div id="S3.SS1.SSS2.p6" class="ltx_para">
<p id="S3.SS1.SSS2.p6.1" class="ltx_p">Utilization of temporal information is also very important to estimate 2D human poses in monocular video sequences. <cite class="ltx_cite ltx_citemacro_citet">Jain et al. (<a href="#bib.bib64" title="" class="ltx_ref">2014</a>)</cite> designed a framework contains two-branch CNNs taking multi-scale RGB frames and optical-flow maps as inputs. The extracted features are concatenated before the last convolutional layers. <cite class="ltx_cite ltx_citemacro_citet">Pfister et al. (<a href="#bib.bib129" title="" class="ltx_ref">2015</a>)</cite> used optical-flow maps as a guide to align predicted heatmaps from neighboring frames based on the temporal context of videos. <cite class="ltx_cite ltx_citemacro_citet">Luo et al. (<a href="#bib.bib97" title="" class="ltx_ref">2018</a>)</cite> exploited temporal information with a Recurrent Neural Network redesigned from CPM by changing multi-stage architecture with LSTM structure.</p>
</div>
<div id="S3.SS1.SSS2.p7" class="ltx_para">
<p id="S3.SS1.SSS2.p7.1" class="ltx_p">In order to estimate human poses on low-capacity devices, network parameters can be reduced while still maintaining competitive performance. <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a href="#bib.bib158" title="" class="ltx_ref">2018b</a>)</cite> committed to improving the network structure by proposing a densely connected U-Nets and efficient usage of memory. This network is similar to the idea of the Hourglass network while utilizing U-Net as each component with a more optimized global connection across each stage resulting in fewer parameters and small model size. <cite class="ltx_cite ltx_citemacro_citet">Debnath et al. (<a href="#bib.bib28" title="" class="ltx_ref">2018</a>)</cite> adapted MobileNets <cite class="ltx_cite ltx_citemacro_citep">(Howard et al., <a href="#bib.bib53" title="" class="ltx_ref">2017</a>)</cite> for pose estimation by designing a split stream architecture at the final two layers of the MobileNets. <cite class="ltx_cite ltx_citemacro_citet">Feng et al. (<a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite> designed a lightweight variant of Hourglass network and trained it with a full teacher Hourglass network by a Fast Pose Distillation (FPD) training strategy.</p>
</div>
<div id="S3.SS1.SSS2.p8" class="ltx_para">
<p id="S3.SS1.SSS2.p8.1" class="ltx_p">In summary, the heatmap representation is more suitable for network training than coordinate representation from detection-based methods in deep learning-based 2D single person pose estimation.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>2D multi-person pose estimation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Different from single person pose estimation, multi-person pose estimation needs to handle both detection and localization tasks since there is no prompt of how many persons in the input images. According to from which level (high-level abstraction or low-level pixel evidence) to start the calculation, human pose estimation methods can be classified into top-down methods and bottom-up methods.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Top-down methods generally employ person detectors to obtain a set of the bounding box of people in the input image and then directly leverage existing single-person pose estimators to predict human poses. The predicted poses heavily depend on the precision of the person detection. The runtime for the whole system is proportional based on the number of persons. While bottom-up methods directly predict all the 2D joints of all persons and then assemble them into independent skeletons. Correct grouping of joint points in a complex environment is a challenging research task. Table <a href="#S3.T4" title="Table 4 ‣ 3.2 2D multi-person pose estimation ‣ 3 2D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> summarizes recent deep learning-based work about 2D multi-person pose estimation methods in both top-down and bottom-up categories. The last column of Table <a href="#S3.T4" title="Table 4 ‣ 3.2 2D multi-person pose estimation ‣ 3 2D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> is the Average Precision (AP) scores on the COCO test-dev dataset. More details of datasets and evaluation metrics are described in Section <a href="#S5" title="5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of 2D multi-person pose estimation methods. Note that the last column shows the Average Precision (AP) scores on the COCO test-dev set. The results with * were obtained with COCO16 training set, while others with COCO17 training set.</figcaption>
<table id="S3.T4.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.5.6.1" class="ltx_tr">
<td id="S3.T4.5.6.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T4.5.6.1.1.1" class="ltx_text" style="font-size:80%;">Methods</span></td>
<td id="S3.T4.5.6.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.6.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.6.1.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.6.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Network type</span></span>
</span>
</td>
<td id="S3.T4.5.6.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.6.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.6.1.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.6.1.3.1.1.1" class="ltx_text" style="font-size:80%;">Highlights</span></span>
</span>
</td>
<td id="S3.T4.5.6.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.6.1.4.1" class="ltx_text" style="font-size:80%;">AP Score (%)</span></td>
</tr>
<tr id="S3.T4.5.7.2" class="ltx_tr">
<td id="S3.T4.5.7.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="4"><span id="S3.T4.5.7.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Top-down</span></td>
</tr>
<tr id="S3.T4.5.8.3" class="ltx_tr">
<td id="S3.T4.5.8.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.8.3.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Iqbal and Gall<span id="S3.T4.5.8.3.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib60" title="" class="ltx_ref">2016</a><span id="S3.T4.5.8.3.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.8.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.8.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.8.3.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.8.3.2.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN + CPM</span></span>
</span>
</td>
<td id="S3.T4.5.8.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.8.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.8.3.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.8.3.3.1.1.1" class="ltx_text" style="font-size:80%;">After person detection and single HPE, refines detected local joint candidates with Integer Linear Programming (ILP).</span></span>
</span>
</td>
<td id="S3.T4.5.8.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.8.3.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S3.T4.1.1" class="ltx_tr">
<td id="S3.T4.1.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.1.1.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Fang et al.<span id="S3.T4.1.1.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib38" title="" class="ltx_ref">2017</a><span id="S3.T4.1.1.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.1.1.3.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN + Hourglass</span></span>
</span>
</td>
<td id="S3.T4.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.4.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.1.1.4.1.1.1" class="ltx_text" style="font-size:80%;">Combines symmetric spatial transformer network (SSTN) and Hourglass model to do SPPE on detected results; proposes a parametric pose NMS for refining pose proposals; designs a pose-guided proposals generator to augment the existing training samples</span></span>
</span>
</td>
<td id="S3.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T4.1.1.1.m1.1" class="ltx_Math" alttext="63.3^{*}" display="inline"><semantics id="S3.T4.1.1.1.m1.1a"><msup id="S3.T4.1.1.1.m1.1.1" xref="S3.T4.1.1.1.m1.1.1.cmml"><mn mathsize="80%" id="S3.T4.1.1.1.m1.1.1.2" xref="S3.T4.1.1.1.m1.1.1.2.cmml">63.3</mn><mo mathsize="80%" id="S3.T4.1.1.1.m1.1.1.3" xref="S3.T4.1.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.m1.1b"><apply id="S3.T4.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T4.1.1.1.m1.1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1">superscript</csymbol><cn type="float" id="S3.T4.1.1.1.m1.1.1.2.cmml" xref="S3.T4.1.1.1.m1.1.1.2">63.3</cn><times id="S3.T4.1.1.1.m1.1.1.3.cmml" xref="S3.T4.1.1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.m1.1c">63.3^{*}</annotation></semantics></math></td>
</tr>
<tr id="S3.T4.2.2" class="ltx_tr">
<td id="S3.T4.2.2.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.2.2.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Papandreou et al.<span id="S3.T4.2.2.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib123" title="" class="ltx_ref">2017</a><span id="S3.T4.2.2.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.2.2.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.2.2.3.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN + ResNet-101</span></span>
</span>
</td>
<td id="S3.T4.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.2.2.4.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.2.2.4.1.1.1" class="ltx_text" style="font-size:80%;">Produces heatmap and offset map of each joint for SPPE and combines them with an aggregation procedure; uses keypoint-based NMS to avoid duplicate poses</span></span>
</span>
</td>
<td id="S3.T4.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T4.2.2.1.m1.1" class="ltx_Math" alttext="64.9^{*}" display="inline"><semantics id="S3.T4.2.2.1.m1.1a"><msup id="S3.T4.2.2.1.m1.1.1" xref="S3.T4.2.2.1.m1.1.1.cmml"><mn mathsize="80%" id="S3.T4.2.2.1.m1.1.1.2" xref="S3.T4.2.2.1.m1.1.1.2.cmml">64.9</mn><mo mathsize="80%" id="S3.T4.2.2.1.m1.1.1.3" xref="S3.T4.2.2.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.1.m1.1b"><apply id="S3.T4.2.2.1.m1.1.1.cmml" xref="S3.T4.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T4.2.2.1.m1.1.1.1.cmml" xref="S3.T4.2.2.1.m1.1.1">superscript</csymbol><cn type="float" id="S3.T4.2.2.1.m1.1.1.2.cmml" xref="S3.T4.2.2.1.m1.1.1.2">64.9</cn><times id="S3.T4.2.2.1.m1.1.1.3.cmml" xref="S3.T4.2.2.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.1.m1.1c">64.9^{*}</annotation></semantics></math></td>
</tr>
<tr id="S3.T4.3.3" class="ltx_tr">
<td id="S3.T4.3.3.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.3.3.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Huang et al.<span id="S3.T4.3.3.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib55" title="" class="ltx_ref">2017</a><span id="S3.T4.3.3.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.3.3.3.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN + Inception-v2</span></span>
</span>
</td>
<td id="S3.T4.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.4.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">Produces coarse and fine poses for SPPE with multi-level supervisions; multi-scale features fusion</span></span>
</span>
</td>
<td id="S3.T4.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T4.3.3.1.m1.1" class="ltx_Math" alttext="72.2^{*}" display="inline"><semantics id="S3.T4.3.3.1.m1.1a"><msup id="S3.T4.3.3.1.m1.1.1" xref="S3.T4.3.3.1.m1.1.1.cmml"><mn mathsize="80%" id="S3.T4.3.3.1.m1.1.1.2" xref="S3.T4.3.3.1.m1.1.1.2.cmml">72.2</mn><mo mathsize="80%" id="S3.T4.3.3.1.m1.1.1.3" xref="S3.T4.3.3.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.1.m1.1b"><apply id="S3.T4.3.3.1.m1.1.1.cmml" xref="S3.T4.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T4.3.3.1.m1.1.1.1.cmml" xref="S3.T4.3.3.1.m1.1.1">superscript</csymbol><cn type="float" id="S3.T4.3.3.1.m1.1.1.2.cmml" xref="S3.T4.3.3.1.m1.1.1.2">72.2</cn><times id="S3.T4.3.3.1.m1.1.1.3.cmml" xref="S3.T4.3.3.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.1.m1.1c">72.2^{*}</annotation></semantics></math></td>
</tr>
<tr id="S3.T4.4.4" class="ltx_tr">
<td id="S3.T4.4.4.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.4.4.2.1.1" class="ltx_text" style="font-size:80%;">(</span>He et al.<span id="S3.T4.4.4.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib51" title="" class="ltx_ref">2017</a><span id="S3.T4.4.4.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.4.4.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.4.4.3.1.1.1" class="ltx_text" style="font-size:80%;">Mask R-CNN + ResNet-FPN</span></span>
</span>
</td>
<td id="S3.T4.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.4.4.4.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.4.4.4.1.1.1" class="ltx_text" style="font-size:80%;">An extension of Mask R-CNN framework; predicts keypoints and human mask synchronously</span></span>
</span>
</td>
<td id="S3.T4.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T4.4.4.1.m1.1" class="ltx_Math" alttext="63.1^{*}" display="inline"><semantics id="S3.T4.4.4.1.m1.1a"><msup id="S3.T4.4.4.1.m1.1.1" xref="S3.T4.4.4.1.m1.1.1.cmml"><mn mathsize="80%" id="S3.T4.4.4.1.m1.1.1.2" xref="S3.T4.4.4.1.m1.1.1.2.cmml">63.1</mn><mo mathsize="80%" id="S3.T4.4.4.1.m1.1.1.3" xref="S3.T4.4.4.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T4.4.4.1.m1.1b"><apply id="S3.T4.4.4.1.m1.1.1.cmml" xref="S3.T4.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T4.4.4.1.m1.1.1.1.cmml" xref="S3.T4.4.4.1.m1.1.1">superscript</csymbol><cn type="float" id="S3.T4.4.4.1.m1.1.1.2.cmml" xref="S3.T4.4.4.1.m1.1.1.2">63.1</cn><times id="S3.T4.4.4.1.m1.1.1.3.cmml" xref="S3.T4.4.4.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.4.1.m1.1c">63.1^{*}</annotation></semantics></math></td>
</tr>
<tr id="S3.T4.5.9.4" class="ltx_tr">
<td id="S3.T4.5.9.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.9.4.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Xiao et al.<span id="S3.T4.5.9.4.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib176" title="" class="ltx_ref">2018</a><span id="S3.T4.5.9.4.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.9.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.9.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.9.4.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.9.4.2.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN + ResNet</span></span>
</span>
</td>
<td id="S3.T4.5.9.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.9.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.9.4.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.9.4.3.1.1.1" class="ltx_text" style="font-size:80%;">Simply adds a few deconvolutional layers after ResNet to generate heatmaps from deep and low resolution features</span></span>
</span>
</td>
<td id="S3.T4.5.9.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.9.4.4.1" class="ltx_text" style="font-size:80%;">73.7</span></td>
</tr>
<tr id="S3.T4.5.10.5" class="ltx_tr">
<td id="S3.T4.5.10.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.10.5.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Chen et al.<span id="S3.T4.5.10.5.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib22" title="" class="ltx_ref">2018</a><span id="S3.T4.5.10.5.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.10.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.10.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.10.5.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.10.5.2.1.1.1" class="ltx_text" style="font-size:80%;">FPN + CPN</span></span>
</span>
</td>
<td id="S3.T4.5.10.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.10.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.10.5.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.10.5.3.1.1.1" class="ltx_text" style="font-size:80%;">Proposes CPN with feature pyramid; two-stage network; online hard keypoints mining</span></span>
</span>
</td>
<td id="S3.T4.5.10.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.10.5.4.1" class="ltx_text" style="font-size:80%;">73.0</span></td>
</tr>
<tr id="S3.T4.5.11.6" class="ltx_tr">
<td id="S3.T4.5.11.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.11.6.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Moon et al.<span id="S3.T4.5.11.6.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib112" title="" class="ltx_ref">2019</a><span id="S3.T4.5.11.6.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.11.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.11.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.11.6.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.11.6.2.1.1.1" class="ltx_text" style="font-size:80%;">ResNet + upsampling</span></span>
</span>
</td>
<td id="S3.T4.5.11.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.11.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.11.6.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.11.6.3.1.1.1" class="ltx_text" style="font-size:80%;">proposes PoseFix net to refine estimated pose from any HPE methods based on pose error distributions</span></span>
</span>
</td>
<td id="S3.T4.5.11.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.11.6.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S3.T4.5.12.7" class="ltx_tr">
<td id="S3.T4.5.12.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.12.7.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Sun et al.<span id="S3.T4.5.12.7.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib151" title="" class="ltx_ref">2019</a><span id="S3.T4.5.12.7.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.12.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.12.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.12.7.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.12.7.2.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN + HRNet</span></span>
</span>
</td>
<td id="S3.T4.5.12.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.12.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.12.7.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.12.7.3.1.1.1" class="ltx_text" style="font-size:80%;">high-resolution representations of features across the whole network, multi-scale fusion</span></span>
</span>
</td>
<td id="S3.T4.5.12.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.12.7.4.1" class="ltx_text" style="font-size:80%;">75.5</span></td>
</tr>
<tr id="S3.T4.5.13.8" class="ltx_tr">
<td id="S3.T4.5.13.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="4"><span id="S3.T4.5.13.8.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Bottom-up</span></td>
</tr>
<tr id="S3.T4.5.14.9" class="ltx_tr">
<td id="S3.T4.5.14.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.14.9.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Pishchulin et al.<span id="S3.T4.5.14.9.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib131" title="" class="ltx_ref">2016</a><span id="S3.T4.5.14.9.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.14.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.14.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.14.9.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.14.9.2.1.1.1" class="ltx_text" style="font-size:80%;">Fast R-CNN</span></span>
</span>
</td>
<td id="S3.T4.5.14.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.14.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.14.9.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.14.9.3.1.1.1" class="ltx_text" style="font-size:80%;">Formulate the distinguishing different persons as an ILP problem; cluster detected part candidates; combine person clusters and labeled parts to obtain final poses</span></span>
</span>
</td>
<td id="S3.T4.5.14.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.14.9.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S3.T4.5.15.10" class="ltx_tr">
<td id="S3.T4.5.15.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.15.10.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Insafutdinov et al.<span id="S3.T4.5.15.10.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib58" title="" class="ltx_ref">2016</a><span id="S3.T4.5.15.10.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.15.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.15.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.15.10.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.15.10.2.1.1.1" class="ltx_text" style="font-size:80%;">ResNet</span></span>
</span>
</td>
<td id="S3.T4.5.15.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.15.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.15.10.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.15.10.3.1.1.1" class="ltx_text" style="font-size:80%;">Employs image-conditioned pairwise terms to assemble the part proposals</span></span>
</span>
</td>
<td id="S3.T4.5.15.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.15.10.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S3.T4.5.5" class="ltx_tr">
<td id="S3.T4.5.5.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.5.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Cao et al.<span id="S3.T4.5.5.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib13" title="" class="ltx_ref">2016</a><span id="S3.T4.5.5.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.5.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.5.3.1.1.1" class="ltx_text" style="font-size:80%;">VGG-19 + CPM</span></span>
</span>
</td>
<td id="S3.T4.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.5.4.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.5.4.1.1.1" class="ltx_text" style="font-size:80%;">OpenPose; real-time; Simultaneous joints detection and association in a two-branch architecture; propose Part Affinity Fields (PAFs) to encode the location and orientation of limbs</span></span>
</span>
</td>
<td id="S3.T4.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T4.5.5.1.m1.1" class="ltx_Math" alttext="61.8^{*}" display="inline"><semantics id="S3.T4.5.5.1.m1.1a"><msup id="S3.T4.5.5.1.m1.1.1" xref="S3.T4.5.5.1.m1.1.1.cmml"><mn mathsize="80%" id="S3.T4.5.5.1.m1.1.1.2" xref="S3.T4.5.5.1.m1.1.1.2.cmml">61.8</mn><mo mathsize="80%" id="S3.T4.5.5.1.m1.1.1.3" xref="S3.T4.5.5.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T4.5.5.1.m1.1b"><apply id="S3.T4.5.5.1.m1.1.1.cmml" xref="S3.T4.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T4.5.5.1.m1.1.1.1.cmml" xref="S3.T4.5.5.1.m1.1.1">superscript</csymbol><cn type="float" id="S3.T4.5.5.1.m1.1.1.2.cmml" xref="S3.T4.5.5.1.m1.1.1.2">61.8</cn><times id="S3.T4.5.5.1.m1.1.1.3.cmml" xref="S3.T4.5.5.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.5.5.1.m1.1c">61.8^{*}</annotation></semantics></math></td>
</tr>
<tr id="S3.T4.5.16.11" class="ltx_tr">
<td id="S3.T4.5.16.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.16.11.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Newell et al.<span id="S3.T4.5.16.11.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib114" title="" class="ltx_ref">2017</a><span id="S3.T4.5.16.11.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.16.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.16.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.16.11.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.16.11.2.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S3.T4.5.16.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.16.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.16.11.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.16.11.3.1.1.1" class="ltx_text" style="font-size:80%;">Simultaneous joints detection and association in one branch; propose dense associative embedding tags for detected joints grouping</span></span>
</span>
</td>
<td id="S3.T4.5.16.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.16.11.4.1" class="ltx_text" style="font-size:80%;">65.5</span></td>
</tr>
<tr id="S3.T4.5.17.12" class="ltx_tr">
<td id="S3.T4.5.17.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.17.12.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Nie et al.<span id="S3.T4.5.17.12.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib118" title="" class="ltx_ref">2018</a><span id="S3.T4.5.17.12.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.17.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.17.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.17.12.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.17.12.2.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S3.T4.5.17.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.17.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.17.12.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.17.12.3.1.1.1" class="ltx_text" style="font-size:80%;">Simultaneous joints detection and association in a two-branch architecture; generate partitions in the embedding space parameterized by person centroids over joint candidates; estimate pose instances by a local greedy inference approach</span></span>
</span>
</td>
<td id="S3.T4.5.17.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.17.12.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S3.T4.5.18.13" class="ltx_tr">
<td id="S3.T4.5.18.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.18.13.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Papandreou et al.<span id="S3.T4.5.18.13.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib122" title="" class="ltx_ref">2018</a><span id="S3.T4.5.18.13.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.18.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.18.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.18.13.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.18.13.2.1.1.1" class="ltx_text" style="font-size:80%;">ResNet</span></span>
</span>
</td>
<td id="S3.T4.5.18.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.18.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.18.13.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.18.13.3.1.1.1" class="ltx_text" style="font-size:80%;">Multi-task (pose estimation and instance segmentation) network; simultaneous joints detection and association in a multi-branch architecture; multi-range joint offsets following tree-structured kinematic graph to guide joints grouping</span></span>
</span>
</td>
<td id="S3.T4.5.18.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.18.13.4.1" class="ltx_text" style="font-size:80%;">68.7</span></td>
</tr>
<tr id="S3.T4.5.19.14" class="ltx_tr">
<td id="S3.T4.5.19.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.19.14.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Kocabas et al.<span id="S3.T4.5.19.14.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib76" title="" class="ltx_ref">2018</a><span id="S3.T4.5.19.14.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.19.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.19.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.19.14.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.19.14.2.1.1.1" class="ltx_text" style="font-size:80%;">ResNet-FPN + RetinaNet</span></span>
</span>
</td>
<td id="S3.T4.5.19.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.5.19.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.19.14.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.19.14.3.1.1.1" class="ltx_text" style="font-size:80%;">Multi-task (pose estimation, person detection and person segmentation) network; simultaneous keypoint detection and person detection in a two-branch architecture; proposes a Pose Residual Network (PRN) to assign keypoint detection to person instances</span></span>
</span>
</td>
<td id="S3.T4.5.19.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.5.19.14.4.1" class="ltx_text" style="font-size:80%;">69.6</span></td>
</tr>
<tr id="S3.T4.5.20.15" class="ltx_tr">
<td id="S3.T4.5.20.15.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T4.5.20.15.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Kreiss et al.<span id="S3.T4.5.20.15.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib77" title="" class="ltx_ref">2019</a><span id="S3.T4.5.20.15.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S3.T4.5.20.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T4.5.20.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.20.15.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T4.5.20.15.2.1.1.1" class="ltx_text" style="font-size:80%;">ResNet-50</span></span>
</span>
</td>
<td id="S3.T4.5.20.15.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T4.5.20.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.20.15.3.1.1" class="ltx_p" style="width:298.8pt;"><span id="S3.T4.5.20.15.3.1.1.1" class="ltx_text" style="font-size:80%;">predicts Part Intensity Fields (PIF) and Part Association Fields (PAF) to represent body joints location and body joints association; works well under low-resolution</span></span>
</span>
</td>
<td id="S3.T4.5.20.15.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T4.5.20.15.4.1" class="ltx_text" style="font-size:80%;">66.7</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Top-down methods</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The two most important components of top-down HPE methods are human body region candidate detector and a single person pose estimator. Most of the research focused on human part estimation based on existing human detectors such as Faster R-CNN <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib138" title="" class="ltx_ref">2015</a>)</cite>, Mask R-CNN <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib51" title="" class="ltx_ref">2017</a>)</cite>, FPN <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib92" title="" class="ltx_ref">2017</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Iqbal and Gall (<a href="#bib.bib60" title="" class="ltx_ref">2016</a>)</cite> utilized a convolutional pose machine-based pose estimator to generate initial poses. Then integer linear programming (ILP) is applied to obtain the final poses. <cite class="ltx_cite ltx_citemacro_citet">Fang et al. (<a href="#bib.bib38" title="" class="ltx_ref">2017</a>)</cite> adopted spatial transformer network (STN) <cite class="ltx_cite ltx_citemacro_citep">(Jaderberg et al., <a href="#bib.bib62" title="" class="ltx_ref">2015</a>)</cite>, Non-Maximum-Suppression (NMS), and Hourglass network to facilitate pose estimation in the presence of inaccurate human bounding boxes.
<cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a href="#bib.bib55" title="" class="ltx_ref">2017</a>)</cite> developed a coarse-fine network (CFN) with Inception-v2 network <cite class="ltx_cite ltx_citemacro_citep">(Szegedy et al., <a href="#bib.bib154" title="" class="ltx_ref">2016</a>)</cite> as the backbone. The network is supervised in multiple levels for learning coarse and fine prediction.
<cite class="ltx_cite ltx_citemacro_citet">Xiao et al. (<a href="#bib.bib176" title="" class="ltx_ref">2018</a>)</cite> added several deconvolutional layers over the last convolution layer of ResNet to generate heatmaps from deep and low-resolution features.
<cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite> proposed a cascade pyramid network (CPN) by employing multi-scale feature maps from different layers to obtain more inference from local and global features with an online hard keypoint mining loss for difficulty joints.
Based on similar pose error distributions of different HPE approaches, <cite class="ltx_cite ltx_citemacro_citet">Moon et al. (<a href="#bib.bib112" title="" class="ltx_ref">2019</a>)</cite> designed PoseFix net to refine estimated poses from any methods.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">Top-down HPE methods can be easily implemented by combining existing detection networks and single HPE networks. Meanwhile, the performance of this kind of methods is affected by person detection results and the operation speed is usually not real-time.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Bottom-up methods</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The main components of bottom-up HPE methods include body joint detection and joint candidate grouping. Most algorithms handle these two components separately.
DeepCut <cite class="ltx_cite ltx_citemacro_citep">(Pishchulin et al., <a href="#bib.bib131" title="" class="ltx_ref">2016</a>)</cite> employed a Fast R-CNN based body part detector to first detect all the body part candidates, then labeled each part to its corresponding part category, and assembled these parts with integer linear programming to a complete skeleton.
DeeperCut <cite class="ltx_cite ltx_citemacro_citep">(Insafutdinov et al., <a href="#bib.bib58" title="" class="ltx_ref">2016</a>)</cite> improved the DeepCut by using a stronger part detector based on ResNet and a better incremental optimization strategy exploring geometric and appearance constraints among joint candidates.
OpenPose <cite class="ltx_cite ltx_citemacro_citep">(Cao et al., <a href="#bib.bib13" title="" class="ltx_ref">2016</a>)</cite> used CPM to predict candidates of all body joints with Part Affinity Fields (PAFs). The proposed PAFs can encode locations and orientations of limbs to assemble the estimated joints into different poses of persons.
<cite class="ltx_cite ltx_citemacro_citet">Nie et al. (<a href="#bib.bib118" title="" class="ltx_ref">2018</a>)</cite> proposed a Pose Partition Network (PPN) to conduct both joint detection and dense regression for joint partition. Then PPN performs local inference for joint configurations with joint partition.
Similar to OpenPose, <cite class="ltx_cite ltx_citemacro_citet">Kreiss et al. (<a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite> designed a PifPaf net to predict a Part Intensity Field (PIF) and a Part Association Field (PAF) to represent body joint locations and body joint association. It works well on low-resolution images due to the fine-grained PAF and the utilization of Laplace loss.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">The above methods are all following a separation of joint detection and joint grouping. Recently, some methods can do the prediction in one stage.
<cite class="ltx_cite ltx_citemacro_citet">Newell et al. (<a href="#bib.bib114" title="" class="ltx_ref">2017</a>)</cite> introduced a single-stage deep network architecture to simultaneously perform both detection and grouping. This network can produce detection heatmaps for each joint, and associative embedding maps that contain the grouping tags of each joint.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">Some methods employed multi-task structures.
<cite class="ltx_cite ltx_citemacro_citet">Papandreou et al. (<a href="#bib.bib122" title="" class="ltx_ref">2018</a>)</cite> proposed a box-free multi-task network for pose estimation and instance segmentation. The ResNet-based network can synchronously predict joint heatmaps of all keypoints for every person and their relative displacements. Then the grouping starts from the most confident detection with a greedy decoding process based on a tree-structured kinematic graph.
The network proposed by <cite class="ltx_cite ltx_citemacro_citet">Kocabas et al. (<a href="#bib.bib76" title="" class="ltx_ref">2018</a>)</cite> combines a multi-task model with a novel assignment method to handle human keypoint estimation, detection, and semantic segmentation tasks altogether. Its backbone network is a combination of ResNet and FPN with shared features for keypoints and person detection subnets. The human detection results are used as constraints of the spatial position of people.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">Currently, the processing speed of bottom-up methods is very fast, and some <cite class="ltx_cite ltx_citemacro_citep">(Cao et al., <a href="#bib.bib13" title="" class="ltx_ref">2016</a>; Nie et al., <a href="#bib.bib118" title="" class="ltx_ref">2018</a>)</cite> can run in real-time. However, the performance can be very influenced by the complex background and human occlusions. The top-down approaches achieved state-of-the-art performance in almost all benchmark datasets while the processing speed is limited by the number of detected people.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>3D Human Pose Estimation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">3D human pose estimation is to predict locations of body joints in 3D space from images or other input sources. Although commercial products such as Kinect <cite class="ltx_cite ltx_citemacro_citep">(Kinect, <a href="#bib.bib75" title="" class="ltx_ref">accessed on 2019</a>)</cite> with depth sensor, VICON <cite class="ltx_cite ltx_citemacro_citep">(Vicon, <a href="#bib.bib169" title="" class="ltx_ref">accessed on 2019</a>)</cite> with optical sensor and TheCaptury <cite class="ltx_cite ltx_citemacro_citep">(TheCaptury, <a href="#bib.bib161" title="" class="ltx_ref">accessed on 2019</a>)</cite> with multiple cameras have been employed for 3D body pose estimation, all these systems work in very constrained environments or need special markers on human body. Monocular camera, as the most widely used sensor, is very important for 3D human pose estimation. Deep neural networks have the capability to estimate the dense depth <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib84" title="" class="ltx_ref">2015a</a>, <a href="#bib.bib82" title="" class="ltx_ref">2018a</a>, <a href="#bib.bib90" title="" class="ltx_ref">2019</a>)</cite> and sparse depth points (joints) as well from monocular images. Moreover, the progress of 3D human pose estimation from monocular inputs can further improve multi-view 3D human pose estimation in constrained environments. Thus, this section focuses on the deep learning-based methods that estimate 3D human pose from monocular RGB images and videos including 3D single person pose estimation and 3D multi-person pose estimation.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>3D single person pose estimation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Compared to 2D HPE, 3D HPE is more challenging since it needs to predict the depth information of body joints. In addition, the training data for 3D HPE are not easy to obtain as 2D HPE. Most existing datasets are obtained under constrained environments with limited generalizability. For single person pose estimation, the bounding box of the person in the image is normally provided, and hence it is not necessary to combine the process of person detection. In this section, we divide the methods of 3D single person pose estimation into model-free and model-based categories and summarize the recent work in Table <a href="#S4.T5" title="Table 5 ‣ 4.1 3D single person pose estimation ‣ 4 3D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The last column of Table <a href="#S4.T5" title="Table 5 ‣ 4.1 3D single person pose estimation ‣ 4 3D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> is the comparisons of Mean Per Joint Position Error (MPJPE) in millimeter on Human3.6M dataset under protocol #1. More details of datasets and evaluation metrics are described in Section <a href="#S5" title="5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of 3D single person pose estimation methods. Here “E.” stands for “Extra data” and “T.” indicates “Temporal info”. The last column is the Mean Per Joint Position Error (MPJPE) in millimeter on Human3.6M dataset under protocol #1. The results with <math id="S4.T5.4.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S4.T5.4.m1.1b"><mo id="S4.T5.4.m1.1.1" xref="S4.T5.4.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S4.T5.4.m1.1c"><times id="S4.T5.4.m1.1.1.cmml" xref="S4.T5.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.m1.1d">*</annotation></semantics></math> were reported from 6 actions in testing set, while others from all 17 actions. The results with <math id="S4.T5.5.m2.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S4.T5.5.m2.1b"><mo id="S4.T5.5.m2.1.1" xref="S4.T5.5.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T5.5.m2.1c"><ci id="S4.T5.5.m2.1.1.cmml" xref="S4.T5.5.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.m2.1d">\dagger</annotation></semantics></math> were reported with 2D joint ground truth. The methods with <math id="S4.T5.6.m3.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="S4.T5.6.m3.1b"><mi mathvariant="normal" id="S4.T5.6.m3.1.1" xref="S4.T5.6.m3.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S4.T5.6.m3.1c"><ci id="S4.T5.6.m3.1.1.cmml" xref="S4.T5.6.m3.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.m3.1d">\#</annotation></semantics></math> report joint rotation as well.</figcaption>
<table id="S4.T5.42" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.42.37.1" class="ltx_tr">
<td id="S4.T5.42.37.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T5.42.37.1.1.1" class="ltx_text" style="font-size:80%;">Methods</span></td>
<td id="S4.T5.42.37.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.42.37.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.37.1.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.42.37.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Backbone</span></span>
</span>
</td>
<td id="S4.T5.42.37.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.42.37.1.3.1" class="ltx_text" style="font-size:80%;">E.</span></td>
<td id="S4.T5.42.37.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.42.37.1.4.1" class="ltx_text" style="font-size:80%;">T.</span></td>
<td id="S4.T5.42.37.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.42.37.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.37.1.5.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.42.37.1.5.1.1.1" class="ltx_text" style="font-size:80%;">Highlights</span></span>
</span>
</td>
<td id="S4.T5.42.37.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.42.37.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.37.1.6.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T5.42.37.1.6.1.1.1" class="ltx_text" style="font-size:80%;">MPJPE (mm)</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.42.38.2" class="ltx_tr">
<td id="S4.T5.42.38.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="6"><span id="S4.T5.42.38.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model-free</span></td>
</tr>
<tr id="S4.T5.7.1" class="ltx_tr">
<td id="S4.T5.7.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.7.1.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Li and Chan<span id="S4.T5.7.1.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib87" title="" class="ltx_ref">2014</a><span id="S4.T5.7.1.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.7.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.7.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.7.1.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.7.1.3.1.1.1" class="ltx_text" style="font-size:80%;">shallow CNNs</span></span>
</span>
</td>
<td id="S4.T5.7.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.7.1.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.7.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.7.1.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.7.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.7.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.7.1.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.7.1.6.1.1.1" class="ltx_text" style="font-size:80%;">A multi-task network to predict of body part detection with sliding windows and 3D pose estimation jointly</span></span>
</span>
</td>
<td id="S4.T5.7.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.7.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.7.1.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.7.1.1.1.1.m1.1" class="ltx_Math" alttext="132.2^{*}" display="inline"><semantics id="S4.T5.7.1.1.1.1.m1.1a"><msup id="S4.T5.7.1.1.1.1.m1.1.1" xref="S4.T5.7.1.1.1.1.m1.1.1.cmml"><mn mathsize="80%" id="S4.T5.7.1.1.1.1.m1.1.1.2" xref="S4.T5.7.1.1.1.1.m1.1.1.2.cmml">132.2</mn><mo mathsize="80%" id="S4.T5.7.1.1.1.1.m1.1.1.3" xref="S4.T5.7.1.1.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T5.7.1.1.1.1.m1.1b"><apply id="S4.T5.7.1.1.1.1.m1.1.1.cmml" xref="S4.T5.7.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.7.1.1.1.1.m1.1.1.1.cmml" xref="S4.T5.7.1.1.1.1.m1.1.1">superscript</csymbol><cn type="float" id="S4.T5.7.1.1.1.1.m1.1.1.2.cmml" xref="S4.T5.7.1.1.1.1.m1.1.1.2">132.2</cn><times id="S4.T5.7.1.1.1.1.m1.1.1.3.cmml" xref="S4.T5.7.1.1.1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.1.1.1.1.m1.1c">132.2^{*}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.8.2" class="ltx_tr">
<td id="S4.T5.8.2.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.8.2.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Li et al.<span id="S4.T5.8.2.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib89" title="" class="ltx_ref">2015b</a><span id="S4.T5.8.2.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.8.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.8.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.8.2.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.8.2.3.1.1.1" class="ltx_text" style="font-size:80%;">shallow CNNs</span></span>
</span>
</td>
<td id="S4.T5.8.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.8.2.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.8.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.8.2.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.8.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.8.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.8.2.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.8.2.6.1.1.1" class="ltx_text" style="font-size:80%;">Compute matching score of image-pose pairs</span></span>
</span>
</td>
<td id="S4.T5.8.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.8.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.8.2.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.8.2.1.1.1.m1.1" class="ltx_Math" alttext="120.2^{*}" display="inline"><semantics id="S4.T5.8.2.1.1.1.m1.1a"><msup id="S4.T5.8.2.1.1.1.m1.1.1" xref="S4.T5.8.2.1.1.1.m1.1.1.cmml"><mn mathsize="80%" id="S4.T5.8.2.1.1.1.m1.1.1.2" xref="S4.T5.8.2.1.1.1.m1.1.1.2.cmml">120.2</mn><mo mathsize="80%" id="S4.T5.8.2.1.1.1.m1.1.1.3" xref="S4.T5.8.2.1.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T5.8.2.1.1.1.m1.1b"><apply id="S4.T5.8.2.1.1.1.m1.1.1.cmml" xref="S4.T5.8.2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.8.2.1.1.1.m1.1.1.1.cmml" xref="S4.T5.8.2.1.1.1.m1.1.1">superscript</csymbol><cn type="float" id="S4.T5.8.2.1.1.1.m1.1.1.2.cmml" xref="S4.T5.8.2.1.1.1.m1.1.1.2">120.2</cn><times id="S4.T5.8.2.1.1.1.m1.1.1.3.cmml" xref="S4.T5.8.2.1.1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.2.1.1.1.m1.1c">120.2^{*}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.9.3" class="ltx_tr">
<td id="S4.T5.9.3.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.9.3.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Tekin et al.<span id="S4.T5.9.3.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib159" title="" class="ltx_ref">2016</a><span id="S4.T5.9.3.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.9.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.9.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.9.3.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.9.3.3.1.1.1" class="ltx_text" style="font-size:80%;">auto-encoder+ shallow CNNs</span></span>
</span>
</td>
<td id="S4.T5.9.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.9.3.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.9.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.9.3.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.9.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.9.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.9.3.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.9.3.6.1.1.1" class="ltx_text" style="font-size:80%;">Employ an auto-encoder to learn a high-dimensional representation of 3D pose; use a shallow CNNs network to learn the high-dimensional pose representation</span></span>
</span>
</td>
<td id="S4.T5.9.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.9.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.9.3.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.9.3.1.1.1.m1.1" class="ltx_Math" alttext="116.8^{*}" display="inline"><semantics id="S4.T5.9.3.1.1.1.m1.1a"><msup id="S4.T5.9.3.1.1.1.m1.1.1" xref="S4.T5.9.3.1.1.1.m1.1.1.cmml"><mn mathsize="80%" id="S4.T5.9.3.1.1.1.m1.1.1.2" xref="S4.T5.9.3.1.1.1.m1.1.1.2.cmml">116.8</mn><mo mathsize="80%" id="S4.T5.9.3.1.1.1.m1.1.1.3" xref="S4.T5.9.3.1.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T5.9.3.1.1.1.m1.1b"><apply id="S4.T5.9.3.1.1.1.m1.1.1.cmml" xref="S4.T5.9.3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.9.3.1.1.1.m1.1.1.1.cmml" xref="S4.T5.9.3.1.1.1.m1.1.1">superscript</csymbol><cn type="float" id="S4.T5.9.3.1.1.1.m1.1.1.2.cmml" xref="S4.T5.9.3.1.1.1.m1.1.1.2">116.8</cn><times id="S4.T5.9.3.1.1.1.m1.1.1.3.cmml" xref="S4.T5.9.3.1.1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.3.1.1.1.m1.1c">116.8^{*}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.10.4" class="ltx_tr">
<td id="S4.T5.10.4.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.10.4.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Tekin et al.<span id="S4.T5.10.4.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib160" title="" class="ltx_ref">2017</a><span id="S4.T5.10.4.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.10.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.10.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.10.4.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.10.4.3.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S4.T5.10.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.10.4.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.10.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.10.4.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.10.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.10.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.10.4.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.10.4.6.1.1.1" class="ltx_text" style="font-size:80%;">Predict 2D heatmaps for joints first; then use a trainable fusion architecture to combine 2D heatmaps and extracted features; 2D module is pre-trained with MPII</span></span>
</span>
</td>
<td id="S4.T5.10.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.10.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.10.4.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.10.4.1.1.1.m1.1" class="ltx_Math" alttext="69.7" display="inline"><semantics id="S4.T5.10.4.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.10.4.1.1.1.m1.1.1" xref="S4.T5.10.4.1.1.1.m1.1.1.cmml">69.7</mn><annotation-xml encoding="MathML-Content" id="S4.T5.10.4.1.1.1.m1.1b"><cn type="float" id="S4.T5.10.4.1.1.1.m1.1.1.cmml" xref="S4.T5.10.4.1.1.1.m1.1.1">69.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.10.4.1.1.1.m1.1c">69.7</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.12.6" class="ltx_tr">
<td id="S4.T5.12.6.3" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.12.6.3.1.1" class="ltx_text" style="font-size:80%;">(</span>Chen and Ramanan<span id="S4.T5.12.6.3.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib17" title="" class="ltx_ref">2017</a><span id="S4.T5.12.6.3.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.12.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.12.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.12.6.4.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.12.6.4.1.1.1" class="ltx_text" style="font-size:80%;">CPM</span></span>
</span>
</td>
<td id="S4.T5.12.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.12.6.5.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.12.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.12.6.6.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.12.6.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.12.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.12.6.7.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.12.6.7.1.1.1" class="ltx_text" style="font-size:80%;">Estimate 2D poses from images first; then estimate depth of them by matching to a library of 3D poses; 2D module is pre-trained with MPII</span></span>
</span>
</td>
<td id="S4.T5.12.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.12.6.2.2" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.12.6.2.2.2" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.11.5.1.1.1.m1.1" class="ltx_Math" alttext="82.7" display="inline"><semantics id="S4.T5.11.5.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.11.5.1.1.1.m1.1.1" xref="S4.T5.11.5.1.1.1.m1.1.1.cmml">82.7</mn><annotation-xml encoding="MathML-Content" id="S4.T5.11.5.1.1.1.m1.1b"><cn type="float" id="S4.T5.11.5.1.1.1.m1.1.1.cmml" xref="S4.T5.11.5.1.1.1.m1.1.1">82.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.11.5.1.1.1.m1.1c">82.7</annotation></semantics></math><span id="S4.T5.12.6.2.2.2.1" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T5.12.6.2.2.2.m2.1" class="ltx_Math" alttext="/57.5^{\dagger}" display="inline"><semantics id="S4.T5.12.6.2.2.2.m2.1a"><mrow id="S4.T5.12.6.2.2.2.m2.1.1" xref="S4.T5.12.6.2.2.2.m2.1.1.cmml"><mi id="S4.T5.12.6.2.2.2.m2.1.1.2" xref="S4.T5.12.6.2.2.2.m2.1.1.2.cmml"></mi><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S4.T5.12.6.2.2.2.m2.1.1.1" xref="S4.T5.12.6.2.2.2.m2.1.1.1.cmml">/</mo><msup id="S4.T5.12.6.2.2.2.m2.1.1.3" xref="S4.T5.12.6.2.2.2.m2.1.1.3.cmml"><mn mathsize="80%" id="S4.T5.12.6.2.2.2.m2.1.1.3.2" xref="S4.T5.12.6.2.2.2.m2.1.1.3.2.cmml">57.5</mn><mo mathsize="80%" id="S4.T5.12.6.2.2.2.m2.1.1.3.3" xref="S4.T5.12.6.2.2.2.m2.1.1.3.3.cmml">†</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.12.6.2.2.2.m2.1b"><apply id="S4.T5.12.6.2.2.2.m2.1.1.cmml" xref="S4.T5.12.6.2.2.2.m2.1.1"><divide id="S4.T5.12.6.2.2.2.m2.1.1.1.cmml" xref="S4.T5.12.6.2.2.2.m2.1.1.1"></divide><csymbol cd="latexml" id="S4.T5.12.6.2.2.2.m2.1.1.2.cmml" xref="S4.T5.12.6.2.2.2.m2.1.1.2">absent</csymbol><apply id="S4.T5.12.6.2.2.2.m2.1.1.3.cmml" xref="S4.T5.12.6.2.2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.T5.12.6.2.2.2.m2.1.1.3.1.cmml" xref="S4.T5.12.6.2.2.2.m2.1.1.3">superscript</csymbol><cn type="float" id="S4.T5.12.6.2.2.2.m2.1.1.3.2.cmml" xref="S4.T5.12.6.2.2.2.m2.1.1.3.2">57.5</cn><ci id="S4.T5.12.6.2.2.2.m2.1.1.3.3.cmml" xref="S4.T5.12.6.2.2.2.m2.1.1.3.3">†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.12.6.2.2.2.m2.1c">/57.5^{\dagger}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.13.7" class="ltx_tr">
<td id="S4.T5.13.7.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.13.7.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Moreno-Noguer<span id="S4.T5.13.7.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib113" title="" class="ltx_ref">2017</a><span id="S4.T5.13.7.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.13.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.13.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.13.7.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.13.7.3.1.1.1" class="ltx_text" style="font-size:80%;">CPM</span></span>
</span>
</td>
<td id="S4.T5.13.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.13.7.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.13.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.13.7.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.13.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.13.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.13.7.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.13.7.6.1.1.1" class="ltx_text" style="font-size:80%;">Use Euclidean Distance Matrices (EDMs) to encoding pairwise distances of 2D and 3D body joints; train a network to learn 2D-to-3D EDM regression; jointly trained with other 3D (Humaneva-I) dataset</span></span>
</span>
</td>
<td id="S4.T5.13.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.13.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.13.7.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.13.7.1.1.1.m1.1" class="ltx_Math" alttext="87.3" display="inline"><semantics id="S4.T5.13.7.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.13.7.1.1.1.m1.1.1" xref="S4.T5.13.7.1.1.1.m1.1.1.cmml">87.3</mn><annotation-xml encoding="MathML-Content" id="S4.T5.13.7.1.1.1.m1.1b"><cn type="float" id="S4.T5.13.7.1.1.1.m1.1.1.cmml" xref="S4.T5.13.7.1.1.1.m1.1.1">87.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.13.7.1.1.1.m1.1c">87.3</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.14.8" class="ltx_tr">
<td id="S4.T5.14.8.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.14.8.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Pavlakos et al.<span id="S4.T5.14.8.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib125" title="" class="ltx_ref">2017</a><span id="S4.T5.14.8.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.14.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.14.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.14.8.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.14.8.3.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S4.T5.14.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.14.8.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.14.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.14.8.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.14.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.14.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.14.8.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.14.8.6.1.1.1" class="ltx_text" style="font-size:80%;">Volumetric representation for 3D human pose; a coarse-to-fine prediction scheme; 2D module is pre-trained with MPII</span></span>
</span>
</td>
<td id="S4.T5.14.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.14.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.14.8.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.14.8.1.1.1.m1.1" class="ltx_Math" alttext="71.9" display="inline"><semantics id="S4.T5.14.8.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.14.8.1.1.1.m1.1.1" xref="S4.T5.14.8.1.1.1.m1.1.1.cmml">71.9</mn><annotation-xml encoding="MathML-Content" id="S4.T5.14.8.1.1.1.m1.1b"><cn type="float" id="S4.T5.14.8.1.1.1.m1.1.1.cmml" xref="S4.T5.14.8.1.1.1.m1.1.1">71.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.14.8.1.1.1.m1.1c">71.9</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.15.9" class="ltx_tr">
<td id="S4.T5.15.9.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.15.9.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Zhou et al.<span id="S4.T5.15.9.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib184" title="" class="ltx_ref">2017</a><span id="S4.T5.15.9.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.15.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.15.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.15.9.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.15.9.3.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S4.T5.15.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.15.9.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.15.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.15.9.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.15.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.15.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.15.9.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.15.9.6.1.1.1" class="ltx_text" style="font-size:80%;">A proposed loss induced from a geometric constraint for 2D data; bone-length constraints; jointly trained with 2D (MPII) dataset</span></span>
</span>
</td>
<td id="S4.T5.15.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.15.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.15.9.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.15.9.1.1.1.m1.1" class="ltx_Math" alttext="64.9" display="inline"><semantics id="S4.T5.15.9.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.15.9.1.1.1.m1.1.1" xref="S4.T5.15.9.1.1.1.m1.1.1.cmml">64.9</mn><annotation-xml encoding="MathML-Content" id="S4.T5.15.9.1.1.1.m1.1b"><cn type="float" id="S4.T5.15.9.1.1.1.m1.1.1.cmml" xref="S4.T5.15.9.1.1.1.m1.1.1">64.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.15.9.1.1.1.m1.1c">64.9</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.17.11" class="ltx_tr">
<td id="S4.T5.17.11.3" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.17.11.3.1.1" class="ltx_text" style="font-size:80%;">(</span>Martinez et al.<span id="S4.T5.17.11.3.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib103" title="" class="ltx_ref">2017</a><span id="S4.T5.17.11.3.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.17.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.17.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.17.11.4.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.17.11.4.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S4.T5.17.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.17.11.5.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.17.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.17.11.6.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.17.11.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.17.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.17.11.7.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.17.11.7.1.1.1" class="ltx_text" style="font-size:80%;">Directly map predicted 2D poses to 3D poses with two linear layers; 2D module is pre-trained with MPII; process in real-time</span></span>
</span>
</td>
<td id="S4.T5.17.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.17.11.2.2" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.17.11.2.2.2" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.16.10.1.1.1.m1.1" class="ltx_Math" alttext="62.9" display="inline"><semantics id="S4.T5.16.10.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.16.10.1.1.1.m1.1.1" xref="S4.T5.16.10.1.1.1.m1.1.1.cmml">62.9</mn><annotation-xml encoding="MathML-Content" id="S4.T5.16.10.1.1.1.m1.1b"><cn type="float" id="S4.T5.16.10.1.1.1.m1.1.1.cmml" xref="S4.T5.16.10.1.1.1.m1.1.1">62.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.16.10.1.1.1.m1.1c">62.9</annotation></semantics></math><span id="S4.T5.17.11.2.2.2.1" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T5.17.11.2.2.2.m2.1" class="ltx_Math" alttext="/45.5^{\dagger}" display="inline"><semantics id="S4.T5.17.11.2.2.2.m2.1a"><mrow id="S4.T5.17.11.2.2.2.m2.1.1" xref="S4.T5.17.11.2.2.2.m2.1.1.cmml"><mi id="S4.T5.17.11.2.2.2.m2.1.1.2" xref="S4.T5.17.11.2.2.2.m2.1.1.2.cmml"></mi><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S4.T5.17.11.2.2.2.m2.1.1.1" xref="S4.T5.17.11.2.2.2.m2.1.1.1.cmml">/</mo><msup id="S4.T5.17.11.2.2.2.m2.1.1.3" xref="S4.T5.17.11.2.2.2.m2.1.1.3.cmml"><mn mathsize="80%" id="S4.T5.17.11.2.2.2.m2.1.1.3.2" xref="S4.T5.17.11.2.2.2.m2.1.1.3.2.cmml">45.5</mn><mo mathsize="80%" id="S4.T5.17.11.2.2.2.m2.1.1.3.3" xref="S4.T5.17.11.2.2.2.m2.1.1.3.3.cmml">†</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.17.11.2.2.2.m2.1b"><apply id="S4.T5.17.11.2.2.2.m2.1.1.cmml" xref="S4.T5.17.11.2.2.2.m2.1.1"><divide id="S4.T5.17.11.2.2.2.m2.1.1.1.cmml" xref="S4.T5.17.11.2.2.2.m2.1.1.1"></divide><csymbol cd="latexml" id="S4.T5.17.11.2.2.2.m2.1.1.2.cmml" xref="S4.T5.17.11.2.2.2.m2.1.1.2">absent</csymbol><apply id="S4.T5.17.11.2.2.2.m2.1.1.3.cmml" xref="S4.T5.17.11.2.2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.T5.17.11.2.2.2.m2.1.1.3.1.cmml" xref="S4.T5.17.11.2.2.2.m2.1.1.3">superscript</csymbol><cn type="float" id="S4.T5.17.11.2.2.2.m2.1.1.3.2.cmml" xref="S4.T5.17.11.2.2.2.m2.1.1.3.2">45.5</cn><ci id="S4.T5.17.11.2.2.2.m2.1.1.3.3.cmml" xref="S4.T5.17.11.2.2.2.m2.1.1.3.3">†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.17.11.2.2.2.m2.1c">/45.5^{\dagger}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.19.13" class="ltx_tr">
<td id="S4.T5.18.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.18.12.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Sun et al.<span id="S4.T5.18.12.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib152" title="" class="ltx_ref">2017</a><span id="S4.T5.18.12.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite><sup id="S4.T5.18.12.1.4" class="ltx_sup"><span id="S4.T5.18.12.1.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">#</span></sup>
</td>
<td id="S4.T5.19.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.19.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.19.13.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.19.13.3.1.1.1" class="ltx_text" style="font-size:80%;">ResNet</span></span>
</span>
</td>
<td id="S4.T5.19.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.19.13.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.19.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.19.13.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.19.13.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.19.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.19.13.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.19.13.6.1.1.1" class="ltx_text" style="font-size:80%;">A bone-based representation involving body structure information to enhance robustness; bone-length constraints; jointly trained with 2D (MPII) dataset</span></span>
</span>
</td>
<td id="S4.T5.19.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.19.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.19.13.2.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.19.13.2.1.1.m1.1" class="ltx_Math" alttext="48.3" display="inline"><semantics id="S4.T5.19.13.2.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.19.13.2.1.1.m1.1.1" xref="S4.T5.19.13.2.1.1.m1.1.1.cmml">48.3</mn><annotation-xml encoding="MathML-Content" id="S4.T5.19.13.2.1.1.m1.1b"><cn type="float" id="S4.T5.19.13.2.1.1.m1.1.1.cmml" xref="S4.T5.19.13.2.1.1.m1.1.1">48.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.19.13.2.1.1.m1.1c">48.3</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.20.14" class="ltx_tr">
<td id="S4.T5.20.14.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.20.14.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Yang et al.<span id="S4.T5.20.14.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib179" title="" class="ltx_ref">2018</a><span id="S4.T5.20.14.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.20.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.20.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.20.14.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.20.14.3.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S4.T5.20.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.20.14.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.20.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.20.14.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.20.14.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.20.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.20.14.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.20.14.6.1.1.1" class="ltx_text" style="font-size:80%;">Adversarial learning for domain adaptation of 2D/3D datasets; adopted generator from </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.20.14.6.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Zhou et al.<span id="S4.T5.20.14.6.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib184" title="" class="ltx_ref">2017</a><span id="S4.T5.20.14.6.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S4.T5.20.14.6.1.1.5" class="ltx_text" style="font-size:80%;">; multi-source discriminator with image, pairwise geometric structure and joint location; jointly trained with 2D (MPII) dataset</span></span>
</span>
</td>
<td id="S4.T5.20.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.20.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.20.14.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.20.14.1.1.1.m1.1" class="ltx_Math" alttext="58.6" display="inline"><semantics id="S4.T5.20.14.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.20.14.1.1.1.m1.1.1" xref="S4.T5.20.14.1.1.1.m1.1.1.cmml">58.6</mn><annotation-xml encoding="MathML-Content" id="S4.T5.20.14.1.1.1.m1.1b"><cn type="float" id="S4.T5.20.14.1.1.1.m1.1.1.cmml" xref="S4.T5.20.14.1.1.1.m1.1.1">58.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.20.14.1.1.1.m1.1c">58.6</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.21.15" class="ltx_tr">
<td id="S4.T5.21.15.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citet">Pavlakos et al. <span id="S4.T5.21.15.2.1.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib124" title="" class="ltx_ref">2018a</a><span id="S4.T5.21.15.2.2.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.21.15.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.21.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.21.15.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.21.15.3.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S4.T5.21.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.21.15.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.21.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.21.15.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.21.15.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.21.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.21.15.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.21.15.6.1.1.1" class="ltx_text" style="font-size:80%;">Volumetric representation for 3D human pose; additional ordinal depths annotations for human joints; jointly trained with 2D (MPII) and 3D (Humaneva-I) datasets</span></span>
</span>
</td>
<td id="S4.T5.21.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.21.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.21.15.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.21.15.1.1.1.m1.1" class="ltx_Math" alttext="56.2" display="inline"><semantics id="S4.T5.21.15.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.21.15.1.1.1.m1.1.1" xref="S4.T5.21.15.1.1.1.m1.1.1.cmml">56.2</mn><annotation-xml encoding="MathML-Content" id="S4.T5.21.15.1.1.1.m1.1b"><cn type="float" id="S4.T5.21.15.1.1.1.m1.1.1.cmml" xref="S4.T5.21.15.1.1.1.m1.1.1">56.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.21.15.1.1.1.m1.1c">56.2</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.22.16" class="ltx_tr">
<td id="S4.T5.22.16.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.22.16.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Sun et al.<span id="S4.T5.22.16.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib153" title="" class="ltx_ref">2018</a><span id="S4.T5.22.16.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.22.16.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.22.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.22.16.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.22.16.3.1.1.1" class="ltx_text" style="font-size:80%;">Mask R-CNN</span></span>
</span>
</td>
<td id="S4.T5.22.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.22.16.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.22.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.22.16.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.22.16.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.22.16.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.22.16.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.22.16.6.1.1.1" class="ltx_text" style="font-size:80%;">Volumetric representation for 3D human pose; integral operation unifies the heat map representation and joint regression; jointly trained with 2D (MPII) dataset</span></span>
</span>
</td>
<td id="S4.T5.22.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.22.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.22.16.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.22.16.1.1.1.m1.1" class="ltx_Math" alttext="40.6" display="inline"><semantics id="S4.T5.22.16.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.22.16.1.1.1.m1.1.1" xref="S4.T5.22.16.1.1.1.m1.1.1.cmml">40.6</mn><annotation-xml encoding="MathML-Content" id="S4.T5.22.16.1.1.1.m1.1b"><cn type="float" id="S4.T5.22.16.1.1.1.m1.1.1.cmml" xref="S4.T5.22.16.1.1.1.m1.1.1">40.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.22.16.1.1.1.m1.1c">40.6</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.23.17" class="ltx_tr">
<td id="S4.T5.23.17.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.23.17.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Li and Lee<span id="S4.T5.23.17.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib85" title="" class="ltx_ref">2019</a><span id="S4.T5.23.17.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.23.17.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.23.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.23.17.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.23.17.3.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S4.T5.23.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.23.17.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.23.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.23.17.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.23.17.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.23.17.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.23.17.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.23.17.6.1.1.1" class="ltx_text" style="font-size:80%;">Multiple hypotheses of 3D poses are generated from 2D poses; the best one is chosen by 2D reprojections; 2D module is pre-trained with MPII</span></span>
</span>
</td>
<td id="S4.T5.23.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.23.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.23.17.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.23.17.1.1.1.m1.1" class="ltx_Math" alttext="52.7" display="inline"><semantics id="S4.T5.23.17.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.23.17.1.1.1.m1.1.1" xref="S4.T5.23.17.1.1.1.m1.1.1.cmml">52.7</mn><annotation-xml encoding="MathML-Content" id="S4.T5.23.17.1.1.1.m1.1b"><cn type="float" id="S4.T5.23.17.1.1.1.m1.1.1.cmml" xref="S4.T5.23.17.1.1.1.m1.1.1">52.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.23.17.1.1.1.m1.1c">52.7</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.42.39.3" class="ltx_tr">
<td id="S4.T5.42.39.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="6"><span id="S4.T5.42.39.3.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model-based</span></td>
</tr>
<tr id="S4.T5.25.19" class="ltx_tr">
<td id="S4.T5.24.18.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.24.18.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Bogo et al.<span id="S4.T5.24.18.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib8" title="" class="ltx_ref">2016</a><span id="S4.T5.24.18.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite><sup id="S4.T5.24.18.1.4" class="ltx_sup"><span id="S4.T5.24.18.1.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">#</span></sup>
</td>
<td id="S4.T5.25.19.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.25.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.25.19.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.25.19.3.1.1.1" class="ltx_text" style="font-size:80%;">DeepCut</span></span>
</span>
</td>
<td id="S4.T5.25.19.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.25.19.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.25.19.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.25.19.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.25.19.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.25.19.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.25.19.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.25.19.6.1.1.1" class="ltx_text" style="font-size:80%;">SMPL model; fit SMPL model to 2D joints by minimizing the distance between 2D joints and projected 3D model joints</span></span>
</span>
</td>
<td id="S4.T5.25.19.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.25.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.25.19.2.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.25.19.2.1.1.m1.1" class="ltx_Math" alttext="82.3" display="inline"><semantics id="S4.T5.25.19.2.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.25.19.2.1.1.m1.1.1" xref="S4.T5.25.19.2.1.1.m1.1.1.cmml">82.3</mn><annotation-xml encoding="MathML-Content" id="S4.T5.25.19.2.1.1.m1.1b"><cn type="float" id="S4.T5.25.19.2.1.1.m1.1.1.cmml" xref="S4.T5.25.19.2.1.1.m1.1.1">82.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.25.19.2.1.1.m1.1c">82.3</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.27.21" class="ltx_tr">
<td id="S4.T5.26.20.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.26.20.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Zhou et al.<span id="S4.T5.26.20.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib185" title="" class="ltx_ref">2016</a><span id="S4.T5.26.20.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite><sup id="S4.T5.26.20.1.4" class="ltx_sup"><span id="S4.T5.26.20.1.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">#</span></sup>
</td>
<td id="S4.T5.27.21.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.27.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.27.21.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.27.21.3.1.1.1" class="ltx_text" style="font-size:80%;">ResNet</span></span>
</span>
</td>
<td id="S4.T5.27.21.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.27.21.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.27.21.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.27.21.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.27.21.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.27.21.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.27.21.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.27.21.6.1.1.1" class="ltx_text" style="font-size:80%;">kinematic model; embedded a kinematic object model into network for general articulated object pose estimation; orientation and rotational constrains</span></span>
</span>
</td>
<td id="S4.T5.27.21.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.27.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.27.21.2.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.27.21.2.1.1.m1.1" class="ltx_Math" alttext="107.3" display="inline"><semantics id="S4.T5.27.21.2.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.27.21.2.1.1.m1.1.1" xref="S4.T5.27.21.2.1.1.m1.1.1.cmml">107.3</mn><annotation-xml encoding="MathML-Content" id="S4.T5.27.21.2.1.1.m1.1b"><cn type="float" id="S4.T5.27.21.2.1.1.m1.1.1.cmml" xref="S4.T5.27.21.2.1.1.m1.1.1">107.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.27.21.2.1.1.m1.1c">107.3</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.29.23" class="ltx_tr">
<td id="S4.T5.28.22.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.28.22.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Mehta et al.<span id="S4.T5.28.22.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib107" title="" class="ltx_ref">2017c</a><span id="S4.T5.28.22.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite><sup id="S4.T5.28.22.1.4" class="ltx_sup"><span id="S4.T5.28.22.1.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">#</span></sup>
</td>
<td id="S4.T5.29.23.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.29.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.29.23.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.29.23.3.1.1.1" class="ltx_text" style="font-size:80%;">ResNet</span></span>
</span>
</td>
<td id="S4.T5.29.23.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.29.23.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.29.23.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.29.23.5.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.29.23.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.29.23.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.29.23.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.29.23.6.1.1.1" class="ltx_text" style="font-size:80%;">A real-time pipeline with temporal smooth filter and model-based kinematic skeleton fitting; 2D module is pre-trained with MPII and LSP; process in real-time; provide body height</span></span>
</span>
</td>
<td id="S4.T5.29.23.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.29.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.29.23.2.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.29.23.2.1.1.m1.1" class="ltx_Math" alttext="80.5" display="inline"><semantics id="S4.T5.29.23.2.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.29.23.2.1.1.m1.1.1" xref="S4.T5.29.23.2.1.1.m1.1.1.cmml">80.5</mn><annotation-xml encoding="MathML-Content" id="S4.T5.29.23.2.1.1.m1.1b"><cn type="float" id="S4.T5.29.23.2.1.1.m1.1.1.cmml" xref="S4.T5.29.23.2.1.1.m1.1.1">80.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.29.23.2.1.1.m1.1c">80.5</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.42.40.4" class="ltx_tr">
<td id="S4.T5.42.40.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.42.40.4.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Tan et al.<span id="S4.T5.42.40.4.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib155" title="" class="ltx_ref">2017</a><span id="S4.T5.42.40.4.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.42.40.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.42.40.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.40.4.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.42.40.4.2.1.1.1" class="ltx_text" style="font-size:80%;">shallow CNNs</span></span>
</span>
</td>
<td id="S4.T5.42.40.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.42.40.4.3.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.42.40.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.42.40.4.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.42.40.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.42.40.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.40.4.5.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.42.40.4.5.1.1.1" class="ltx_text" style="font-size:80%;">SMPL model; first train a decoder to predict a 2D body silhouette from parameters of SMPL; then train a encoder-decoder network with images and corresponding silhouettes; the trained encoder can predict parameters of SMPL from images</span></span>
</span>
</td>
<td id="S4.T5.42.40.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.42.40.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.40.4.6.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T5.42.40.4.6.1.1.1" class="ltx_text" style="font-size:80%;">-</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.30.24" class="ltx_tr">
<td id="S4.T5.30.24.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.30.24.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Mehta et al.<span id="S4.T5.30.24.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib104" title="" class="ltx_ref">2017a</a><span id="S4.T5.30.24.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.30.24.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.30.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.30.24.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.30.24.3.1.1.1" class="ltx_text" style="font-size:80%;">Resnet</span></span>
</span>
</td>
<td id="S4.T5.30.24.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.30.24.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.30.24.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.30.24.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.30.24.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.30.24.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.30.24.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.30.24.6.1.1.1" class="ltx_text" style="font-size:80%;">Kinematic model; transfer learning from features learned for 2D pose estimation; 2D pose prediction as auxiliary task; predict relative joint locations following the kinematic tree body model; jointly trained with 2D (MPII and LSP) datasets</span></span>
</span>
</td>
<td id="S4.T5.30.24.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.30.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.30.24.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.30.24.1.1.1.m1.1" class="ltx_Math" alttext="74.1" display="inline"><semantics id="S4.T5.30.24.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.30.24.1.1.1.m1.1.1" xref="S4.T5.30.24.1.1.1.m1.1.1.cmml">74.1</mn><annotation-xml encoding="MathML-Content" id="S4.T5.30.24.1.1.1.m1.1b"><cn type="float" id="S4.T5.30.24.1.1.1.m1.1.1.cmml" xref="S4.T5.30.24.1.1.1.m1.1.1">74.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.30.24.1.1.1.m1.1c">74.1</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.31.25" class="ltx_tr">
<td id="S4.T5.31.25.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.31.25.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Nie et al.<span id="S4.T5.31.25.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib117" title="" class="ltx_ref">2017</a><span id="S4.T5.31.25.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.31.25.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.31.25.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.31.25.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.31.25.3.1.1.1" class="ltx_text" style="font-size:80%;">RMPE + LSTM</span></span>
</span>
</td>
<td id="S4.T5.31.25.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.31.25.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.31.25.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.31.25.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.31.25.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.31.25.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.31.25.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.31.25.6.1.1.1" class="ltx_text" style="font-size:80%;">Kinematic model; joint depth estimation from global 2D pose with skeleton-LSTM and local body parts with patch-LSTM; 2D module is pre-trained with MPII</span></span>
</span>
</td>
<td id="S4.T5.31.25.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.31.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.31.25.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.31.25.1.1.1.m1.1" class="ltx_Math" alttext="79.5" display="inline"><semantics id="S4.T5.31.25.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.31.25.1.1.1.m1.1.1" xref="S4.T5.31.25.1.1.1.m1.1.1.cmml">79.5</mn><annotation-xml encoding="MathML-Content" id="S4.T5.31.25.1.1.1.m1.1b"><cn type="float" id="S4.T5.31.25.1.1.1.m1.1.1.cmml" xref="S4.T5.31.25.1.1.1.m1.1.1">79.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.31.25.1.1.1.m1.1c">79.5</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.33.27" class="ltx_tr">
<td id="S4.T5.32.26.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.32.26.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Kanazawa et al.<span id="S4.T5.32.26.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib73" title="" class="ltx_ref">2018</a><span id="S4.T5.32.26.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite><sup id="S4.T5.32.26.1.4" class="ltx_sup"><span id="S4.T5.32.26.1.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">#</span></sup>
</td>
<td id="S4.T5.33.27.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.33.27.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.33.27.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.33.27.3.1.1.1" class="ltx_text" style="font-size:80%;">ResNet</span></span>
</span>
</td>
<td id="S4.T5.33.27.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.33.27.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.33.27.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.33.27.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.33.27.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.33.27.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.33.27.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.33.27.6.1.1.1" class="ltx_text" style="font-size:80%;">SMPL model; adversarial learning for domain adaptation of 2D images and 3D human body model; propose a framework to learn parameters of SMPL; jointly trained with 2D (LSP, MPII and COCO) datasets; process in real-time</span></span>
</span>
</td>
<td id="S4.T5.33.27.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.33.27.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.33.27.2.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.33.27.2.1.1.m1.1" class="ltx_Math" alttext="88.0" display="inline"><semantics id="S4.T5.33.27.2.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.33.27.2.1.1.m1.1.1" xref="S4.T5.33.27.2.1.1.m1.1.1.cmml">88.0</mn><annotation-xml encoding="MathML-Content" id="S4.T5.33.27.2.1.1.m1.1b"><cn type="float" id="S4.T5.33.27.2.1.1.m1.1.1.cmml" xref="S4.T5.33.27.2.1.1.m1.1.1">88.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.33.27.2.1.1.m1.1c">88.0</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.35.29" class="ltx_tr">
<td id="S4.T5.34.28.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.34.28.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Pavlakos et al.<span id="S4.T5.34.28.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib126" title="" class="ltx_ref">2018b</a><span id="S4.T5.34.28.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite><sup id="S4.T5.34.28.1.4" class="ltx_sup"><span id="S4.T5.34.28.1.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">#</span></sup>
</td>
<td id="S4.T5.35.29.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.35.29.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.35.29.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.35.29.3.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S4.T5.35.29.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.35.29.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.35.29.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.35.29.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.35.29.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.35.29.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.35.29.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.35.29.6.1.1.1" class="ltx_text" style="font-size:80%;">SMPL model; first predict 2D heatmaps of joint and human silhouette; second generate parameters of SMPL; 2D module is trained with MPII and LSP</span></span>
</span>
</td>
<td id="S4.T5.35.29.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.35.29.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.35.29.2.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.35.29.2.1.1.m1.1" class="ltx_Math" alttext="75.9" display="inline"><semantics id="S4.T5.35.29.2.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.35.29.2.1.1.m1.1.1" xref="S4.T5.35.29.2.1.1.m1.1.1.cmml">75.9</mn><annotation-xml encoding="MathML-Content" id="S4.T5.35.29.2.1.1.m1.1b"><cn type="float" id="S4.T5.35.29.2.1.1.m1.1.1.cmml" xref="S4.T5.35.29.2.1.1.m1.1.1">75.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.35.29.2.1.1.m1.1c">75.9</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.37.31" class="ltx_tr">
<td id="S4.T5.36.30.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.36.30.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Omran et al.<span id="S4.T5.36.30.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib120" title="" class="ltx_ref">2018</a><span id="S4.T5.36.30.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite><sup id="S4.T5.36.30.1.4" class="ltx_sup"><span id="S4.T5.36.30.1.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">#</span></sup>
</td>
<td id="S4.T5.37.31.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.37.31.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.37.31.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.37.31.3.1.1.1" class="ltx_text" style="font-size:80%;">RefineNet</span></span>
</span>
</td>
<td id="S4.T5.37.31.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.37.31.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.37.31.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.37.31.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.37.31.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.37.31.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.37.31.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.37.31.6.1.1.1" class="ltx_text" style="font-size:80%;">SMPL model; first predict 2D body parts segmentation from the RGB image; second take this segmentation to predict the parameters of SMPL</span></span>
</span>
</td>
<td id="S4.T5.37.31.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.37.31.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.37.31.2.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.37.31.2.1.1.m1.1" class="ltx_Math" alttext="59.9" display="inline"><semantics id="S4.T5.37.31.2.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.37.31.2.1.1.m1.1.1" xref="S4.T5.37.31.2.1.1.m1.1.1.cmml">59.9</mn><annotation-xml encoding="MathML-Content" id="S4.T5.37.31.2.1.1.m1.1b"><cn type="float" id="S4.T5.37.31.2.1.1.m1.1.1.cmml" xref="S4.T5.37.31.2.1.1.m1.1.1">59.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.37.31.2.1.1.m1.1c">59.9</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.38.32" class="ltx_tr">
<td id="S4.T5.38.32.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.38.32.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Varol et al.<span id="S4.T5.38.32.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib167" title="" class="ltx_ref">2018</a><span id="S4.T5.38.32.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.38.32.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.38.32.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.38.32.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.38.32.3.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S4.T5.38.32.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.38.32.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.38.32.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.38.32.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.38.32.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.38.32.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.38.32.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.38.32.6.1.1.1" class="ltx_text" style="font-size:80%;">SMPL model; first predict 2D pose and 2D body parts segmentation; second predict 3D pose; finally predict volumetric shape to fit SMPL model; 2D modules are trained with MPII and SURREAL</span></span>
</span>
</td>
<td id="S4.T5.38.32.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.38.32.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.38.32.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.38.32.1.1.1.m1.1" class="ltx_Math" alttext="49.0" display="inline"><semantics id="S4.T5.38.32.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.38.32.1.1.1.m1.1.1" xref="S4.T5.38.32.1.1.1.m1.1.1.cmml">49.0</mn><annotation-xml encoding="MathML-Content" id="S4.T5.38.32.1.1.1.m1.1b"><cn type="float" id="S4.T5.38.32.1.1.1.m1.1.1.cmml" xref="S4.T5.38.32.1.1.1.m1.1.1">49.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.38.32.1.1.1.m1.1c">49.0</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.41.35" class="ltx_tr">
<td id="S4.T5.39.33.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.39.33.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Arnab et al.<span id="S4.T5.39.33.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib6" title="" class="ltx_ref">2019</a><span id="S4.T5.39.33.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite><sup id="S4.T5.39.33.1.4" class="ltx_sup"><span id="S4.T5.39.33.1.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">#</span></sup>
</td>
<td id="S4.T5.41.35.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.41.35.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.41.35.4.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.41.35.4.1.1.1" class="ltx_text" style="font-size:80%;">ResNet</span></span>
</span>
</td>
<td id="S4.T5.41.35.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.41.35.5.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.41.35.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.41.35.6.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.41.35.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.41.35.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.41.35.7.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.41.35.7.1.1.1" class="ltx_text" style="font-size:80%;">SMPL model; 2D keypoints, SMPL and camera parameters estimation; off-line bundle adjustment with temporal constraints; 2D module is trained with COCO</span></span>
</span>
</td>
<td id="S4.T5.41.35.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.41.35.3.2" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.41.35.3.2.2" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.40.34.2.1.1.m1.1" class="ltx_Math" alttext="77.8" display="inline"><semantics id="S4.T5.40.34.2.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.40.34.2.1.1.m1.1.1" xref="S4.T5.40.34.2.1.1.m1.1.1.cmml">77.8</mn><annotation-xml encoding="MathML-Content" id="S4.T5.40.34.2.1.1.m1.1b"><cn type="float" id="S4.T5.40.34.2.1.1.m1.1.1.cmml" xref="S4.T5.40.34.2.1.1.m1.1.1">77.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.40.34.2.1.1.m1.1c">77.8</annotation></semantics></math><span id="S4.T5.41.35.3.2.2.1" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T5.41.35.3.2.2.m2.1" class="ltx_Math" alttext="/63.3^{\dagger}" display="inline"><semantics id="S4.T5.41.35.3.2.2.m2.1a"><mrow id="S4.T5.41.35.3.2.2.m2.1.1" xref="S4.T5.41.35.3.2.2.m2.1.1.cmml"><mi id="S4.T5.41.35.3.2.2.m2.1.1.2" xref="S4.T5.41.35.3.2.2.m2.1.1.2.cmml"></mi><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S4.T5.41.35.3.2.2.m2.1.1.1" xref="S4.T5.41.35.3.2.2.m2.1.1.1.cmml">/</mo><msup id="S4.T5.41.35.3.2.2.m2.1.1.3" xref="S4.T5.41.35.3.2.2.m2.1.1.3.cmml"><mn mathsize="80%" id="S4.T5.41.35.3.2.2.m2.1.1.3.2" xref="S4.T5.41.35.3.2.2.m2.1.1.3.2.cmml">63.3</mn><mo mathsize="80%" id="S4.T5.41.35.3.2.2.m2.1.1.3.3" xref="S4.T5.41.35.3.2.2.m2.1.1.3.3.cmml">†</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.41.35.3.2.2.m2.1b"><apply id="S4.T5.41.35.3.2.2.m2.1.1.cmml" xref="S4.T5.41.35.3.2.2.m2.1.1"><divide id="S4.T5.41.35.3.2.2.m2.1.1.1.cmml" xref="S4.T5.41.35.3.2.2.m2.1.1.1"></divide><csymbol cd="latexml" id="S4.T5.41.35.3.2.2.m2.1.1.2.cmml" xref="S4.T5.41.35.3.2.2.m2.1.1.2">absent</csymbol><apply id="S4.T5.41.35.3.2.2.m2.1.1.3.cmml" xref="S4.T5.41.35.3.2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.T5.41.35.3.2.2.m2.1.1.3.1.cmml" xref="S4.T5.41.35.3.2.2.m2.1.1.3">superscript</csymbol><cn type="float" id="S4.T5.41.35.3.2.2.m2.1.1.3.2.cmml" xref="S4.T5.41.35.3.2.2.m2.1.1.3.2">63.3</cn><ci id="S4.T5.41.35.3.2.2.m2.1.1.3.3.cmml" xref="S4.T5.41.35.3.2.2.m2.1.1.3.3">†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.41.35.3.2.2.m2.1c">/63.3^{\dagger}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.42.36" class="ltx_tr">
<td id="S4.T5.42.36.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.42.36.2.1.1" class="ltx_text" style="font-size:80%;">(</span>Tome et al.<span id="S4.T5.42.36.2.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib162" title="" class="ltx_ref">2017</a><span id="S4.T5.42.36.2.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.42.36.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.42.36.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.36.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.42.36.3.1.1.1" class="ltx_text" style="font-size:80%;">CPM</span></span>
</span>
</td>
<td id="S4.T5.42.36.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.42.36.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T5.42.36.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.42.36.5.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.42.36.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.42.36.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.36.6.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.42.36.6.1.1.1" class="ltx_text" style="font-size:80%;">Pre-trained probabilistic 3D pose model; 3D lifting and projection by probabilistic model within the CPM-like network; 2D module is pre-trained with MPII; process in real-time</span></span>
</span>
</td>
<td id="S4.T5.42.36.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.42.36.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.36.1.1.1" class="ltx_p" style="width:22.8pt;"><math id="S4.T5.42.36.1.1.1.m1.1" class="ltx_Math" alttext="88.4" display="inline"><semantics id="S4.T5.42.36.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.42.36.1.1.1.m1.1.1" xref="S4.T5.42.36.1.1.1.m1.1.1.cmml">88.4</mn><annotation-xml encoding="MathML-Content" id="S4.T5.42.36.1.1.1.m1.1b"><cn type="float" id="S4.T5.42.36.1.1.1.m1.1.1.cmml" xref="S4.T5.42.36.1.1.1.m1.1.1">88.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.42.36.1.1.1.m1.1c">88.4</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.42.41.5" class="ltx_tr">
<td id="S4.T5.42.41.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.42.41.5.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Rhodin et al.<span id="S4.T5.42.41.5.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib139" title="" class="ltx_ref">2018a</a><span id="S4.T5.42.41.5.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></td>
<td id="S4.T5.42.41.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.42.41.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.41.5.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T5.42.41.5.2.1.1.1" class="ltx_text" style="font-size:80%;">Hourglass</span></span>
</span>
</td>
<td id="S4.T5.42.41.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.42.41.5.3.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.42.41.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.42.41.5.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T5.42.41.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.42.41.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.41.5.5.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T5.42.41.5.5.1.1.1" class="ltx_text" style="font-size:80%;">A latent variable body model learned from multi-view images; an encoder-decoder to predict a novel view image from a given one; the pre-trained encoder with additional shallow layers to predict 3D poses from images</span></span>
</span>
</td>
<td id="S4.T5.42.41.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.42.41.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.42.41.5.6.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T5.42.41.5.6.1.1.1" class="ltx_text" style="font-size:80%;">-</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Model-free methods</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">The model-free methods do not employ human body models as the predicted target or intermediate cues. They can be roughly categorized into two types: 1) directly map an image to 3D pose, and 2) estimate depth following intermediately predicted 2D pose from 2D pose estimation methods.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">Approaches that directly estimate the 3D pose from image features usually contain very few constraints.
<cite class="ltx_cite ltx_citemacro_citet">Li and Chan (<a href="#bib.bib87" title="" class="ltx_ref">2014</a>)</cite> employed a shallow network to regress 3D joint coordinates directly with synchronous task of body part detection with sliding windows.
<cite class="ltx_cite ltx_citemacro_citet">Pavlakos et al. (<a href="#bib.bib125" title="" class="ltx_ref">2017</a>)</cite> proposed a volumetric representation for 3D human pose and employed a coarse-to-fine prediction scheme to refine predictions with a multi-stage structure.
Some researchers attempted to add body structure information or the dependencies between human joints to the deep learning networks.
<cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib89" title="" class="ltx_ref">2015b</a>)</cite> designed an embedding sub-network learning latent pose structure information to guide the 3D joint coordinates mapping. The sub-network can assign matching scores for input image-pose pairs with a maximum-margin cost function.
<cite class="ltx_cite ltx_citemacro_citet">Tekin et al. (<a href="#bib.bib159" title="" class="ltx_ref">2016</a>)</cite> pre-trained an unsupervised auto-encoder to learn a high-dimensional latent pose representation of 3D pose for adding implicit constraints about the human body and then used a shallow network to learn the high-dimensional pose representation.
<cite class="ltx_cite ltx_citemacro_citet">Sun et al. (<a href="#bib.bib152" title="" class="ltx_ref">2017</a>)</cite> proposed a structure-aware regression approach. They designed a bone-based representation involving body structure information which is more stable than only using joint positions.
<cite class="ltx_cite ltx_citemacro_citet">Pavlakos et al. (<a href="#bib.bib124" title="" class="ltx_ref">2018a</a>)</cite> trained the network with additional ordinal depths of human joints as constraints, by which the 2D human datasets can also be feed in with ordinal depths annotations.</p>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p id="S4.SS1.SSS1.p3.1" class="ltx_p">The 3D HPE methods which intermediately estimate 2D poses gain the advantages of 2D HPE, and can easily utilize images from 2D human datasets. Some of them adopt off-the-shelf 2D HPE modules to first estimate 2D poses, then extend to 3D poses.
<cite class="ltx_cite ltx_citemacro_citep">(Martinez et al., <a href="#bib.bib103" title="" class="ltx_ref">2017</a>)</cite> designed a 2D-to-3D pose predictor with only two linear layers.
<cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a href="#bib.bib184" title="" class="ltx_ref">2017</a>)</cite> presented a depth regression module to predict 3D pose from 2D heatmaps with a proposed geometric constraint loss for 2D data.
<cite class="ltx_cite ltx_citemacro_citet">Tekin et al. (<a href="#bib.bib160" title="" class="ltx_ref">2017</a>)</cite> proposed a two-branch framework to predict 2D heatmaps and extract features from images. The extracted features are fused with 2D heatmaps by a trainable fusion scheme instead of being hand-crafted to obtain the final 3D joint coordinates.
<cite class="ltx_cite ltx_citemacro_citet">Li and Lee (<a href="#bib.bib85" title="" class="ltx_ref">2019</a>)</cite> considered 3D HPE as an inverse problem with multiple feasible solutions. Multiple feasible hypotheses of 3D poses are generated from 2D poses and the best one is chosen by 2D reprojections.
<cite class="ltx_cite ltx_citemacro_citet">Qammaz and Argyros (<a href="#bib.bib135" title="" class="ltx_ref">2019</a>)</cite> proposed MocapNET directly encoding 2D poses into the 3D BVH <cite class="ltx_cite ltx_citemacro_citep">(Meredith et al., <a href="#bib.bib108" title="" class="ltx_ref">2001</a>)</cite> format for subsequent rendering. By consolidating OpenPose <cite class="ltx_cite ltx_citemacro_citep">(Cao et al., <a href="#bib.bib13" title="" class="ltx_ref">2016</a>)</cite> the architecture estimated and rendered 3D human pose in real-time using only CPU processing.</p>
</div>
<div id="S4.SS1.SSS1.p4" class="ltx_para">
<p id="S4.SS1.SSS1.p4.1" class="ltx_p">When mapping 2D pose to 3D pose, different strategies may be applied. <cite class="ltx_cite ltx_citemacro_citet">Chen and Ramanan (<a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite> used a matching strategy for an estimated 2D pose and 3D pose from a library. <cite class="ltx_cite ltx_citemacro_citet">Moreno-Noguer (<a href="#bib.bib113" title="" class="ltx_ref">2017</a>)</cite> encoded pairwise distances of 2D and 3D body joints into two Euclidean Distance Matrices (EDMs) and trained a regression network to learn the mapping of the two matrices. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a href="#bib.bib171" title="" class="ltx_ref">2018a</a>)</cite> predicted depth rankings of human joints as a cue to infer 3D joint positions from a 2D pose. <cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a href="#bib.bib179" title="" class="ltx_ref">2018</a>)</cite> adopted a generator from <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a href="#bib.bib184" title="" class="ltx_ref">2017</a>)</cite> and designed a multi-source discriminator with image, pairwise geometric structure, and joint location information.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Model-based methods</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">Model-based methods generally employ a parametric body model or template to estimate human pose and shape from images. Early geometric-based models are not included in this paper. More recent models are estimated from multiple scans of diverse people <cite class="ltx_cite ltx_citemacro_citep">(Hasler et al., <a href="#bib.bib50" title="" class="ltx_ref">2009</a>; Loper et al., <a href="#bib.bib96" title="" class="ltx_ref">2015</a>; Pons-Moll et al., <a href="#bib.bib132" title="" class="ltx_ref">2015</a>; Zuffi and Black, <a href="#bib.bib186" title="" class="ltx_ref">2015</a>)</cite> or combination of different body models <cite class="ltx_cite ltx_citemacro_citep">(Joo et al., <a href="#bib.bib71" title="" class="ltx_ref">2018</a>)</cite>. These models are typically parameterized by separate body pose and shape components.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">Some work employed the body model of SMPL <cite class="ltx_cite ltx_citemacro_citep">(Loper et al., <a href="#bib.bib96" title="" class="ltx_ref">2015</a>)</cite> and attempted to estimate the 3D parameters from images. For example,
<cite class="ltx_cite ltx_citemacro_citet">Bogo et al. (<a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite> fit SMPL model to estimated 2D joints and proposed an optimization-based method to recover SMPL parameters from 2D joints.
<cite class="ltx_cite ltx_citemacro_citet">Tan et al. (<a href="#bib.bib155" title="" class="ltx_ref">2017</a>)</cite> inferred SMPL parameters by first training a decoder to predict silhouettes from SMPL parameters with synthetic data, and then learning an image encoder with the trained decoder. The trained encoder can predict SMPL parameters from input images.
Directly learning parameters of SMPL is hard, some work predicted intermediate cues as constrains. For example, intermediate 2D pose and human body segmentation <cite class="ltx_cite ltx_citemacro_citep">(Pavlakos et al., <a href="#bib.bib126" title="" class="ltx_ref">2018b</a>)</cite>, body parts segmentation <cite class="ltx_cite ltx_citemacro_citep">(Omran et al., <a href="#bib.bib120" title="" class="ltx_ref">2018</a>)</cite>, 2D pose and body parts segmentation <cite class="ltx_cite ltx_citemacro_citep">(Varol et al., <a href="#bib.bib167" title="" class="ltx_ref">2018</a>)</cite>.
In order to overcome the problem of lacking training data for the human body model, <cite class="ltx_cite ltx_citemacro_citep">(Kanazawa et al., <a href="#bib.bib73" title="" class="ltx_ref">2018</a>)</cite> employed adversarial learning by using a generator to predict parameters of SMPL, and a discriminator to distinguish the real SMPL model and the predicted ones.
<cite class="ltx_cite ltx_citemacro_citep">(Arnab et al., <a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite> reconstructed person from video sequences which explored the multiple views information.</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">Kinematic model is widely used for 3D HPE. <cite class="ltx_cite ltx_citemacro_citep">(Mehta et al., <a href="#bib.bib104" title="" class="ltx_ref">2017a</a>)</cite> predicted relative joint locations from 2D heatmaps following the kinematic tree body model.
<cite class="ltx_cite ltx_citemacro_citep">(Nie et al., <a href="#bib.bib117" title="" class="ltx_ref">2017</a>)</cite> employed LSTM to exploit global 2D joint locations and
local body part images following kinematic tree body model which are two cues for joint depth estimation.
<cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a href="#bib.bib185" title="" class="ltx_ref">2016</a>)</cite> embedded a kinematic object model into a network for general articulated object pose estimation which provides orientation and rotational constrains.
<cite class="ltx_cite ltx_citemacro_citet">Mehta et al. (<a href="#bib.bib107" title="" class="ltx_ref">2017c</a>)</cite> proposed a pipeline for 3D single HPE running in real-time. The temporal information and kinematic body model are used as a smooth filter and skeleton fitting respectively.
<cite class="ltx_cite ltx_citemacro_citet">Rhodin et al. (<a href="#bib.bib139" title="" class="ltx_ref">2018a</a>)</cite> used an encoder-decoder network to learn a latent variable body model without 2D or 3D annotations under self-supervision, then employed the pre-trained encoder to predict 3D poses.</p>
</div>
<div id="S4.SS1.SSS2.p4" class="ltx_para">
<p id="S4.SS1.SSS2.p4.1" class="ltx_p">Additional to those typical body models, latent 3D pose model learned from data is also used for 3D HPE.
<cite class="ltx_cite ltx_citemacro_citet">Tome et al. (<a href="#bib.bib162" title="" class="ltx_ref">2017</a>)</cite> proposed a multi-stage CPM-like network including a pre-trained probabilistic 3D pose model layer which can generate 3D pose from 2D heatmaps.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>Summary of 3D multi-person pose estimation methods.</figcaption>
<table id="S4.T6.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.3.1.1" class="ltx_tr">
<th id="S4.T6.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T6.3.1.1.1.1" class="ltx_text" style="font-size:80%;">Methods</span></th>
<th id="S4.T6.3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T6.3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.1.1.2.1.1" class="ltx_p" style="width:54.1pt;"><span id="S4.T6.3.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Network type</span></span>
</span>
</th>
<th id="S4.T6.3.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T6.3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.1.1.3.1.1" class="ltx_p" style="width:327.2pt;"><span id="S4.T6.3.1.1.3.1.1.1" class="ltx_text" style="font-size:80%;">Highlights</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.3.2.1" class="ltx_tr">
<th id="S4.T6.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.3.2.1.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Mehta et al.<span id="S4.T6.3.2.1.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib106" title="" class="ltx_ref">2017b</a><span id="S4.T6.3.2.1.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></th>
<td id="S4.T6.3.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.3.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.2.1.2.1.1" class="ltx_p" style="width:54.1pt;"><span id="S4.T6.3.2.1.2.1.1.1" class="ltx_text" style="font-size:80%;">ResNet</span></span>
</span>
</td>
<td id="S4.T6.3.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.3.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.2.1.3.1.1" class="ltx_p" style="width:327.2pt;"><span id="S4.T6.3.2.1.3.1.1.1" class="ltx_text" style="font-size:80%;">Propose an occlusion-robust pose-maps (ORPM) for full-body pose inference even under (self-)occlusions; combine 2D pose and part affinity fields to infer person instances</span></span>
</span>
</td>
</tr>
<tr id="S4.T6.3.3.2" class="ltx_tr">
<th id="S4.T6.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.3.3.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Rogez et al.<span id="S4.T6.3.3.2.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib141" title="" class="ltx_ref">2017</a><span id="S4.T6.3.3.2.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></th>
<td id="S4.T6.3.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.3.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.3.2.2.1.1" class="ltx_p" style="width:54.1pt;">
<span id="S4.T6.3.3.2.2.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.3.3.2.2.1.1.1.1" class="ltx_tr">
<span id="S4.T6.3.3.2.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T6.3.3.2.2.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN</span></span></span>
<span id="S4.T6.3.3.2.2.1.1.1.2" class="ltx_tr">
<span id="S4.T6.3.3.2.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T6.3.3.2.2.1.1.1.2.1.1" class="ltx_text" style="font-size:80%;">+ VGG-16</span></span></span>
</span></span>
</span>
</td>
<td id="S4.T6.3.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.3.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.3.2.3.1.1" class="ltx_p" style="width:327.2pt;"><span id="S4.T6.3.3.2.3.1.1.1" class="ltx_text" style="font-size:80%;">Localize human bounding boxes with Faster R-CNN; classify the closest anchor-pose for each proposal; regress anchor-pose to get final pose</span></span>
</span>
</td>
</tr>
<tr id="S4.T6.3.4.3" class="ltx_tr">
<th id="S4.T6.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.3.4.3.1.1.1" class="ltx_text" style="font-size:80%;">(</span>Zanfir et al.<span id="S4.T6.3.4.3.1.2.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib181" title="" class="ltx_ref">2018</a><span id="S4.T6.3.4.3.1.3.3" class="ltx_text" style="font-size:80%;">)</span></cite></th>
<td id="S4.T6.3.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.3.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.4.3.2.1.1" class="ltx_p" style="width:54.1pt;"><span id="S4.T6.3.4.3.2.1.1.1" class="ltx_text" style="font-size:80%;">DMHS</span></span>
</span>
</td>
<td id="S4.T6.3.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.3.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.4.3.3.1.1" class="ltx_p" style="width:327.2pt;"><span id="S4.T6.3.4.3.3.1.1.1" class="ltx_text" style="font-size:80%;">Feed forward process of body parts semantic segmentation and 3d pose estimates; feed backward process of refining pose and shape parameters of a body model SMPL</span></span>
</span>
</td>
</tr>
<tr id="S4.T6.3.5.4" class="ltx_tr">
<th id="S4.T6.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citet">Mehta et al. <span id="S4.T6.3.5.4.1.1.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib105" title="" class="ltx_ref">2019</a><span id="S4.T6.3.5.4.1.2.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite></th>
<td id="S4.T6.3.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T6.3.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.5.4.2.1.1" class="ltx_p" style="width:54.1pt;"><span id="S4.T6.3.5.4.2.1.1.1" class="ltx_text" style="font-size:80%;">SelecSLS Net</span></span>
</span>
</td>
<td id="S4.T6.3.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T6.3.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.5.4.3.1.1" class="ltx_p" style="width:327.2pt;"><span id="S4.T6.3.5.4.3.1.1.1" class="ltx_text" style="font-size:80%;">Real-time; a new CNN architecture that uses selective long and short range skip connections; 2D and 3D pose features prediction along with identity assignments for all visible joints of all individuals; complete 3D pose reconstruction including occluded joints; temporal stability refinement and kinematic skeleton fitting.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>3D multi-person pose estimation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The achievements of monocular 3D multi-person pose estimation are based on 3D single person pose estimation and other deep learning methods. This research field is pretty new and only a few methods are proposed. Table <a href="#S4.T6" title="Table 6 ‣ 4.1.2 Model-based methods ‣ 4.1 3D single person pose estimation ‣ 4 3D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes these methods.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Mehta et al. (<a href="#bib.bib106" title="" class="ltx_ref">2017b</a>)</cite> proposed a bottom-up method by using 2D pose and part affinity fields to infer person instances. An occlusion-robust pose-maps (ORPM) is proposed to provide multi-style occlusion information regardless of the number of people. <cite class="ltx_cite ltx_citemacro_citet">Rogez et al. (<a href="#bib.bib141" title="" class="ltx_ref">2017</a>)</cite> proposed a Localization-Classification-Regression Network (LCR-Net) following three-stage processing. First, Faster R-CNN is employed to detect people locations. Second, each pose proposal is assigned with the closest anchor-pose scored by a classifier. The final poses are refined with a regressor respectively.
<cite class="ltx_cite ltx_citemacro_citet">Zanfir et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>)</cite> proposed a framework with feed forward and feed backward stages for 3D multi-person pose and shape estimation. The feed forward process includes semantic segmentation of body parts and 3D pose estimates based on DMHS <cite class="ltx_cite ltx_citemacro_citep">(Popa et al., <a href="#bib.bib133" title="" class="ltx_ref">2017</a>)</cite>. Then the feed backward process refines the pose and shape parameters of SMPL <cite class="ltx_cite ltx_citemacro_citep">(Loper et al., <a href="#bib.bib96" title="" class="ltx_ref">2015</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Mehta et al. (<a href="#bib.bib105" title="" class="ltx_ref">2019</a>)</cite> estimated multiple poses in real-time with three stages. First, SelecSLS Net infers 2D pose and intermediate 3D pose encoding for visible body joints. Then based on each detected person, it reconstructs the complete 3D pose, including occluded joints. Finally, refinement is provided for temporal stability and kinematic skeleton fitting.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Datasets and evaluation protocols</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Datasets play an important role in deep learning-baed human pose estimation. Datasets not only are essential for fair comparison of different algorithms but also bring more challenges and complexity through their expansion and improvement. With the maturity of the commercial motion capture systems and crowdsourcing services, recent datasets are no longer limited by the data quantity or lab environments.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">This section discusses the popular publicly available human pose datasets for 2D and 3D human pose estimation, introduces the characteristics and the evaluation methods, as well as the performance of recent state-of-the-art work on several popular datasets. In addition to these basic datasets, some researchers have extended the existing datasets in their own way <cite class="ltx_cite ltx_citemacro_citep">(Pavlakos et al., <a href="#bib.bib124" title="" class="ltx_ref">2018a</a>; Lassner et al., <a href="#bib.bib79" title="" class="ltx_ref">2017</a>)</cite>. In addition, some relevant human datasets are also within the scope of this section <cite class="ltx_cite ltx_citemacro_citep">(Güler et al., <a href="#bib.bib49" title="" class="ltx_ref">2018</a>)</cite>. A brief description of how researchers collected all the annotated images of each dataset is also provided to bring inspiration to readers who want to generate their own datasets.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets for 2D human pose estimation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Before deep learning brings significant progress for 2D HPE, there are many 2D human pose datasets for specific scenarios and tasks. Upper body pose datasets include Buffy Stickmen <cite class="ltx_cite ltx_citemacro_citep">(Ferrari et al., <a href="#bib.bib41" title="" class="ltx_ref">2008</a>)</cite> (frontal-facing view, from indoor TV show), ETHZ PASCAL Stickmen <cite class="ltx_cite ltx_citemacro_citep">(Eichner et al., <a href="#bib.bib32" title="" class="ltx_ref">2009</a>)</cite> (frontal-facing view, from PASCAL VOC <cite class="ltx_cite ltx_citemacro_citep">(Everingham et al., <a href="#bib.bib34" title="" class="ltx_ref">2010</a>)</cite>), We Are Family <cite class="ltx_cite ltx_citemacro_citep">(Eichner and Ferrari, <a href="#bib.bib29" title="" class="ltx_ref">2010</a>)</cite> (Group photo scenario), Video Pose 2 <cite class="ltx_cite ltx_citemacro_citep">(Sapp et al., <a href="#bib.bib144" title="" class="ltx_ref">2011</a>)</cite> (from indoor TV show), Sync. Activities <cite class="ltx_cite ltx_citemacro_citep">(Eichner and Ferrari, <a href="#bib.bib31" title="" class="ltx_ref">2012b</a>)</cite> (sports, full-body image, upper body annotation). full-body pose datasets include PASCAL Person Layout <cite class="ltx_cite ltx_citemacro_citep">(Everingham et al., <a href="#bib.bib34" title="" class="ltx_ref">2010</a>)</cite> (daily scene, from PASCAL VOC <cite class="ltx_cite ltx_citemacro_citep">(Everingham et al., <a href="#bib.bib34" title="" class="ltx_ref">2010</a>)</cite>), Sport <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib173" title="" class="ltx_ref">2011</a>)</cite> (sport scenes) and UIUC people <cite class="ltx_cite ltx_citemacro_citep">(Li and Fei-fei, <a href="#bib.bib86" title="" class="ltx_ref">2007</a>)</cite> (sport scenes). For detailed description of these datasets, we refer interested readers to several well-summarized papers <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Gong et al., <a href="#bib.bib47" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Above earlier datasets for 2D human pose estimation have many shortcomings such as few scenes, monotonous view angle, lack of diverse activities, and limited number of images. The scale is the most important aspect of a dataset for deep learning-based methods. Small training sets are insufficient for learning robust features, unsuitable for networks with deep layers and complex design, and may easily cause overfitting. Thus in this section, we only introduce 2D human pose datasets with the number of images for training over 1,000. The features of these selected 2D HPE datasets are summarized in Table <a href="#S5.T7" title="Table 7 ‣ 5.1 Datasets for 2D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and some sample images with annotations are illustrated in Fig. <a href="#S5.F5" title="Fig. 5 ‣ 5.1 Datasets for 2D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Popular 2D databases for human pose estimation. Selected example images with annotations are shown in Fig. <a href="#S5.F5" title="Fig. 5 ‣ 5.1 Datasets for 2D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Here Jnt. indicates the number of joints</figcaption>
<table id="S5.T7.24" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T7.24.25.1" class="ltx_tr">
<td id="S5.T7.24.25.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T7.24.25.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.24.25.1.1.1.1" class="ltx_p" style="width:31.3pt;"><span id="S5.T7.24.25.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Dataset</span></span>
</span>
</td>
<td id="S5.T7.24.25.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T7.24.25.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.24.25.1.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S5.T7.24.25.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Single/</span></span>
</span>
</td>
<td id="S5.T7.24.25.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T7.24.25.1.3.1" class="ltx_text" style="font-size:80%;">Jnt.</span></td>
<td id="S5.T7.24.25.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S5.T7.24.25.1.4.1" class="ltx_text" style="font-size:80%;">Number of images/videos</span></td>
<td id="S5.T7.24.25.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.24.25.1.5.1" class="ltx_text" style="font-size:80%;">Evaluation</span></td>
<td id="S5.T7.24.25.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" rowspan="2">
<span id="S5.T7.24.25.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.24.25.1.6.1.1" class="ltx_p" style="width:216.2pt;"><span id="S5.T7.24.25.1.6.1.1.1" class="ltx_text" style="font-size:80%;">Highlights</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.24.26.2" class="ltx_tr">
<td id="S5.T7.24.26.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S5.T7.24.26.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.24.26.2.1.1.1" class="ltx_p" style="width:31.3pt;"><span id="S5.T7.24.26.2.1.1.1.1" class="ltx_text" style="font-size:80%;">name</span></span>
</span>
</td>
<td id="S5.T7.24.26.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T7.24.26.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.24.26.2.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S5.T7.24.26.2.2.1.1.1" class="ltx_text" style="font-size:80%;">Multiple</span></span>
</span>
</td>
<td id="S5.T7.24.26.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.24.26.2.3.1" class="ltx_text" style="font-size:80%;">Train</span></td>
<td id="S5.T7.24.26.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.24.26.2.4.1" class="ltx_text" style="font-size:80%;">Val</span></td>
<td id="S5.T7.24.26.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.24.26.2.5.1" class="ltx_text" style="font-size:80%;">Test</span></td>
<td id="S5.T7.24.26.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T7.24.26.2.6.1" class="ltx_text" style="font-size:80%;">protocol</span></td>
</tr>
<tr id="S5.T7.24.27.3" class="ltx_tr">
<td id="S5.T7.24.27.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" colspan="8"><span id="S5.T7.24.27.3.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Image-based</span></td>
</tr>
<tr id="S5.T7.2.2" class="ltx_tr">
<td id="S5.T7.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;">
<span id="S5.T7.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.3.1.1" class="ltx_p" style="width:31.3pt;"><span id="S5.T7.2.2.3.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">FLIC</span></span>
</span>
</td>
<td id="S5.T7.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;" rowspan="3">
<span id="S5.T7.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.4.1.1" class="ltx_p" style="width:28.5pt;"><span id="S5.T7.2.2.4.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-19.9pt;">single</span></span>
</span>
</td>
<td id="S5.T7.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;" rowspan="3"><span id="S5.T7.2.2.5.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-19.9pt;">10</span></td>
<td id="S5.T7.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T7.1.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.1.1.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.1.1.1.1.m1.1a"><mo id="S5.T7.1.1.1.1.m1.1.1" xref="S5.T7.1.1.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.1.1.1.1.m1.1b"><approx id="S5.T7.1.1.1.1.m1.1.1.cmml" xref="S5.T7.1.1.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.1.1.1.1.m1.1c">\approx</annotation></semantics></math>5k</span></td>
<td id="S5.T7.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T7.2.2.6.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">0</span></td>
<td id="S5.T7.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T7.2.2.2.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.2.2.2.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.2.2.2.1.m1.1a"><mo id="S5.T7.2.2.2.1.m1.1.1" xref="S5.T7.2.2.2.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.2.2.2.1.m1.1b"><approx id="S5.T7.2.2.2.1.m1.1.1.cmml" xref="S5.T7.2.2.2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.2.2.2.1.m1.1c">\approx</annotation></semantics></math>1k</span></td>
<td id="S5.T7.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;" rowspan="3"><span id="S5.T7.2.2.7.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-22.8pt;">PCP&amp;PCK</span></td>
<td id="S5.T7.2.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;" rowspan="3">
<span id="S5.T7.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.2.2.8.1.1" class="ltx_p" style="width:216.2pt;"><span id="S5.T7.2.2.8.1.1.1" class="ltx_text" style="font-size:80%;">Upper body poses; Sampled from movies; FLIC-full is complete version <cite class="ltx_cite ltx_citemacro_citep">(Sapp and Taskar, <a href="#bib.bib143" title="" class="ltx_ref">2013</a>)</cite>; FLIC-plus is cleaned version <cite class="ltx_cite ltx_citemacro_citep">(Tompson et al., <a href="#bib.bib164" title="" class="ltx_ref">2014</a>)</cite>; FLIC is a simple version with no difficult poses.</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.3.3" class="ltx_tr">
<td id="S5.T7.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;">
<span id="S5.T7.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.3.3.2.1.1" class="ltx_p" style="width:31.3pt;"><span id="S5.T7.3.3.2.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">
<span id="S5.T7.3.3.2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T7.3.3.2.1.1.1.1.1" class="ltx_tr">
<span id="S5.T7.3.3.2.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">FLIC-full</span></span>
</span></span></span>
</span>
</td>
<td id="S5.T7.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T7.3.3.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.3.3.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.3.3.1.1.m1.1a"><mo id="S5.T7.3.3.1.1.m1.1.1" xref="S5.T7.3.3.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.3.3.1.1.m1.1b"><approx id="S5.T7.3.3.1.1.m1.1.1.cmml" xref="S5.T7.3.3.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.3.3.1.1.m1.1c">\approx</annotation></semantics></math>20k</span></td>
<td id="S5.T7.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T7.3.3.3.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">0</span></td>
<td id="S5.T7.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T7.3.3.4.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">0</span></td>
</tr>
<tr id="S5.T7.4.4" class="ltx_tr">
<td id="S5.T7.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;">
<span id="S5.T7.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.4.4.2.1.1" class="ltx_p" style="width:31.3pt;"><span id="S5.T7.4.4.2.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">
<span id="S5.T7.4.4.2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T7.4.4.2.1.1.1.1.1" class="ltx_tr">
<span id="S5.T7.4.4.2.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">FLIC-plus</span></span>
</span></span></span>
</span>
</td>
<td id="S5.T7.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T7.4.4.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.4.4.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.4.4.1.1.m1.1a"><mo id="S5.T7.4.4.1.1.m1.1.1" xref="S5.T7.4.4.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.4.4.1.1.m1.1b"><approx id="S5.T7.4.4.1.1.m1.1.1.cmml" xref="S5.T7.4.4.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.4.4.1.1.m1.1c">\approx</annotation></semantics></math>17k</span></td>
<td id="S5.T7.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T7.4.4.3.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">0</span></td>
<td id="S5.T7.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T7.4.4.4.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">0</span></td>
</tr>
<tr id="S5.T7.6.6" class="ltx_tr">
<td id="S5.T7.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;">
<span id="S5.T7.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.6.6.3.1.1" class="ltx_p" style="width:31.3pt;"><span id="S5.T7.6.6.3.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">LSP</span></span>
</span>
</td>
<td id="S5.T7.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;" rowspan="2">
<span id="S5.T7.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.6.6.4.1.1" class="ltx_p" style="width:28.5pt;"><span id="S5.T7.6.6.4.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-19.9pt;">single</span></span>
</span>
</td>
<td id="S5.T7.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;" rowspan="2"><span id="S5.T7.6.6.5.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-19.9pt;">14</span></td>
<td id="S5.T7.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.5.5.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.5.5.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.5.5.1.1.m1.1a"><mo id="S5.T7.5.5.1.1.m1.1.1" xref="S5.T7.5.5.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.5.5.1.1.m1.1b"><approx id="S5.T7.5.5.1.1.m1.1.1.cmml" xref="S5.T7.5.5.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.5.5.1.1.m1.1c">\approx</annotation></semantics></math>1k</span></td>
<td id="S5.T7.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.6.6.6.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">0</span></td>
<td id="S5.T7.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.6.6.2.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.6.6.2.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.6.6.2.1.m1.1a"><mo id="S5.T7.6.6.2.1.m1.1.1" xref="S5.T7.6.6.2.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.6.6.2.1.m1.1b"><approx id="S5.T7.6.6.2.1.m1.1.1.cmml" xref="S5.T7.6.6.2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.6.6.2.1.m1.1c">\approx</annotation></semantics></math>1k</span></td>
<td id="S5.T7.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;" rowspan="2"><span id="S5.T7.6.6.7.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-19.9pt;">PCP</span></td>
<td id="S5.T7.6.6.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;" rowspan="2">
<span id="S5.T7.6.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.6.6.8.1.1" class="ltx_p" style="width:216.2pt;"><span id="S5.T7.6.6.8.1.1.1" class="ltx_text" style="font-size:80%;">full-body poses; From Flickr with 8 sports tags <cite class="ltx_cite ltx_citemacro_citep">(Johnson and Everingham, <a href="#bib.bib68" title="" class="ltx_ref">2010</a>)</cite>; Extended by adding most challenging poses lie in 3 tags <cite class="ltx_cite ltx_citemacro_citep">(Johnson and Everingham, <a href="#bib.bib69" title="" class="ltx_ref">2011</a>)</cite>.</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.7.7" class="ltx_tr">
<td id="S5.T7.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;">
<span id="S5.T7.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.7.7.2.1.1" class="ltx_p" style="width:31.3pt;"><span id="S5.T7.7.7.2.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">
<span id="S5.T7.7.7.2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T7.7.7.2.1.1.1.1.1" class="ltx_tr">
<span id="S5.T7.7.7.2.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">LSP-</span></span>
<span id="S5.T7.7.7.2.1.1.1.1.2" class="ltx_tr">
<span id="S5.T7.7.7.2.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">extended</span></span>
</span></span></span>
</span>
</td>
<td id="S5.T7.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.7.7.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.7.7.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.7.7.1.1.m1.1a"><mo id="S5.T7.7.7.1.1.m1.1.1" xref="S5.T7.7.7.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.7.7.1.1.m1.1b"><approx id="S5.T7.7.7.1.1.m1.1.1.cmml" xref="S5.T7.7.7.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.7.7.1.1.m1.1c">\approx</annotation></semantics></math>10k</span></td>
<td id="S5.T7.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.7.7.3.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">0</span></td>
<td id="S5.T7.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.7.7.4.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">0</span></td>
</tr>
<tr id="S5.T7.9.9" class="ltx_tr">
<td id="S5.T7.9.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;" rowspan="2">
<span id="S5.T7.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.9.9.3.1.1" class="ltx_p" style="width:31.3pt;"><span id="S5.T7.9.9.3.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-19.9pt;">MPII</span></span>
</span>
</td>
<td id="S5.T7.9.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;">
<span id="S5.T7.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.9.9.4.1.1" class="ltx_p" style="width:28.5pt;"><span id="S5.T7.9.9.4.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">single</span></span>
</span>
</td>
<td id="S5.T7.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;" rowspan="2"><span id="S5.T7.9.9.5.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-19.9pt;">16</span></td>
<td id="S5.T7.8.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.8.8.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.8.8.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.8.8.1.1.m1.1a"><mo id="S5.T7.8.8.1.1.m1.1.1" xref="S5.T7.8.8.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.8.8.1.1.m1.1b"><approx id="S5.T7.8.8.1.1.m1.1.1.cmml" xref="S5.T7.8.8.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.8.8.1.1.m1.1c">\approx</annotation></semantics></math>29k</span></td>
<td id="S5.T7.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.9.9.6.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">0</span></td>
<td id="S5.T7.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.9.9.2.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.9.9.2.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.9.9.2.1.m1.1a"><mo id="S5.T7.9.9.2.1.m1.1.1" xref="S5.T7.9.9.2.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.9.9.2.1.m1.1b"><approx id="S5.T7.9.9.2.1.m1.1.1.cmml" xref="S5.T7.9.9.2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.9.9.2.1.m1.1c">\approx</annotation></semantics></math>12k</span></td>
<td id="S5.T7.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.9.9.7.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">PCPm/PCKh</span></td>
<td id="S5.T7.9.9.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;" rowspan="2">
<span id="S5.T7.9.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.9.9.8.1.1" class="ltx_p" style="width:216.2pt;"><span id="S5.T7.9.9.8.1.1.1" class="ltx_text" style="font-size:80%;">Various body poses; Downloaded videos from YouTube; Multiple annotations (bounding boxes, 3D viewpoint of the head and torso, position of the eyes and nose, joint locations); <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite>.</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.11.11" class="ltx_tr">
<td id="S5.T7.11.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;">
<span id="S5.T7.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.11.11.3.1.1" class="ltx_p" style="width:28.5pt;"><span id="S5.T7.11.11.3.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">multiple</span></span>
</span>
</td>
<td id="S5.T7.10.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.10.10.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.10.10.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.10.10.1.1.m1.1a"><mo id="S5.T7.10.10.1.1.m1.1.1" xref="S5.T7.10.10.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.10.10.1.1.m1.1b"><approx id="S5.T7.10.10.1.1.m1.1.1.cmml" xref="S5.T7.10.10.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.10.10.1.1.m1.1c">\approx</annotation></semantics></math>3.8k</span></td>
<td id="S5.T7.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.11.11.4.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">0</span></td>
<td id="S5.T7.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.11.11.2.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.11.11.2.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.11.11.2.1.m1.1a"><mo id="S5.T7.11.11.2.1.m1.1.1" xref="S5.T7.11.11.2.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.11.11.2.1.m1.1b"><approx id="S5.T7.11.11.2.1.m1.1.1.cmml" xref="S5.T7.11.11.2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.11.11.2.1.m1.1c">\approx</annotation></semantics></math>1.7k</span></td>
<td id="S5.T7.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.11.11.5.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">mAP</span></td>
</tr>
<tr id="S5.T7.14.14" class="ltx_tr">
<td id="S5.T7.14.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;">
<span id="S5.T7.14.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.14.14.4.1.1" class="ltx_p" style="width:31.3pt;"><span id="S5.T7.14.14.4.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">COCO16</span></span>
</span>
</td>
<td id="S5.T7.14.14.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;" rowspan="2">
<span id="S5.T7.14.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.14.14.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S5.T7.14.14.5.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-19.9pt;">multiple</span></span>
</span>
</td>
<td id="S5.T7.14.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;" rowspan="2"><span id="S5.T7.14.14.6.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-19.9pt;">17</span></td>
<td id="S5.T7.12.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.12.12.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.12.12.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.12.12.1.1.m1.1a"><mo id="S5.T7.12.12.1.1.m1.1.1" xref="S5.T7.12.12.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.12.12.1.1.m1.1b"><approx id="S5.T7.12.12.1.1.m1.1.1.cmml" xref="S5.T7.12.12.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.12.12.1.1.m1.1c">\approx</annotation></semantics></math>45k</span></td>
<td id="S5.T7.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.13.13.2.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.13.13.2.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.13.13.2.1.m1.1a"><mo id="S5.T7.13.13.2.1.m1.1.1" xref="S5.T7.13.13.2.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.13.13.2.1.m1.1b"><approx id="S5.T7.13.13.2.1.m1.1.1.cmml" xref="S5.T7.13.13.2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.13.13.2.1.m1.1c">\approx</annotation></semantics></math>22k</span></td>
<td id="S5.T7.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.14.14.3.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.14.14.3.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.14.14.3.1.m1.1a"><mo id="S5.T7.14.14.3.1.m1.1.1" xref="S5.T7.14.14.3.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.14.14.3.1.m1.1b"><approx id="S5.T7.14.14.3.1.m1.1.1.cmml" xref="S5.T7.14.14.3.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.14.14.3.1.m1.1c">\approx</annotation></semantics></math>80k</span></td>
<td id="S5.T7.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;" rowspan="2"><span id="S5.T7.14.14.7.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-19.9pt;">AP</span></td>
<td id="S5.T7.14.14.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;" rowspan="2">
<span id="S5.T7.14.14.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.14.14.8.1.1" class="ltx_p" style="width:216.2pt;"><span id="S5.T7.14.14.8.1.1.1" class="ltx_text" style="font-size:80%;">Various body poses; From Google, Bing and Flickr; Multiple annotations (bounding boxes, human body masks, joint locations); With about 120K unlabeled images for semi-supervised learning; <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib93" title="" class="ltx_ref">2014</a>)</cite></span></span>
</span>
</td>
</tr>
<tr id="S5.T7.17.17" class="ltx_tr">
<td id="S5.T7.17.17.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;">
<span id="S5.T7.17.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.17.17.4.1.1" class="ltx_p" style="width:31.3pt;"><span id="S5.T7.17.17.4.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;">COCO17</span></span>
</span>
</td>
<td id="S5.T7.15.15.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.15.15.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.15.15.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.15.15.1.1.m1.1a"><mo id="S5.T7.15.15.1.1.m1.1.1" xref="S5.T7.15.15.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.15.15.1.1.m1.1b"><approx id="S5.T7.15.15.1.1.m1.1.1.cmml" xref="S5.T7.15.15.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.15.15.1.1.m1.1c">\approx</annotation></semantics></math>64k</span></td>
<td id="S5.T7.16.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.16.16.2.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.16.16.2.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.16.16.2.1.m1.1a"><mo id="S5.T7.16.16.2.1.m1.1.1" xref="S5.T7.16.16.2.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.16.16.2.1.m1.1b"><approx id="S5.T7.16.16.2.1.m1.1.1.cmml" xref="S5.T7.16.16.2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.16.16.2.1.m1.1c">\approx</annotation></semantics></math>2.7k</span></td>
<td id="S5.T7.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:13.0pt;"><span id="S5.T7.17.17.3.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-8.5pt;"><math id="S5.T7.17.17.3.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.17.17.3.1.m1.1a"><mo id="S5.T7.17.17.3.1.m1.1.1" xref="S5.T7.17.17.3.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.17.17.3.1.m1.1b"><approx id="S5.T7.17.17.3.1.m1.1.1.cmml" xref="S5.T7.17.17.3.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.17.17.3.1.m1.1c">\approx</annotation></semantics></math>40k</span></td>
</tr>
<tr id="S5.T7.20.20" class="ltx_tr">
<td id="S5.T7.20.20.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T7.20.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.20.20.4.1.1" class="ltx_p" style="width:31.3pt;">
<span id="S5.T7.20.20.4.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T7.20.20.4.1.1.1.1" class="ltx_tr">
<span id="S5.T7.20.20.4.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T7.20.20.4.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">AIC-</span></span></span>
<span id="S5.T7.20.20.4.1.1.1.2" class="ltx_tr">
<span id="S5.T7.20.20.4.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T7.20.20.4.1.1.1.2.1.1" class="ltx_text" style="font-size:80%;">HKD</span></span></span>
</span></span>
</span>
</td>
<td id="S5.T7.20.20.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T7.20.20.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.20.20.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S5.T7.20.20.5.1.1.1" class="ltx_text" style="font-size:80%;">multiple</span></span>
</span>
</td>
<td id="S5.T7.20.20.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.20.20.6.1" class="ltx_text" style="font-size:80%;">14</span></td>
<td id="S5.T7.18.18.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T7.18.18.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.18.18.1.m1.1a"><mo mathsize="80%" id="S5.T7.18.18.1.m1.1.1" xref="S5.T7.18.18.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.18.18.1.m1.1b"><approx id="S5.T7.18.18.1.m1.1.1.cmml" xref="S5.T7.18.18.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.18.18.1.m1.1c">\approx</annotation></semantics></math><span id="S5.T7.18.18.1.1" class="ltx_text" style="font-size:80%;">210k</span>
</td>
<td id="S5.T7.19.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T7.19.19.2.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.19.19.2.m1.1a"><mo mathsize="80%" id="S5.T7.19.19.2.m1.1.1" xref="S5.T7.19.19.2.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.19.19.2.m1.1b"><approx id="S5.T7.19.19.2.m1.1.1.cmml" xref="S5.T7.19.19.2.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.19.19.2.m1.1c">\approx</annotation></semantics></math><span id="S5.T7.19.19.2.1" class="ltx_text" style="font-size:80%;">30k</span>
</td>
<td id="S5.T7.20.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T7.20.20.3.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.20.20.3.m1.1a"><mo mathsize="80%" id="S5.T7.20.20.3.m1.1.1" xref="S5.T7.20.20.3.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.20.20.3.m1.1b"><approx id="S5.T7.20.20.3.m1.1.1.cmml" xref="S5.T7.20.20.3.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.20.20.3.m1.1c">\approx</annotation></semantics></math><span id="S5.T7.20.20.3.1" class="ltx_text" style="font-size:80%;">60k</span>
</td>
<td id="S5.T7.20.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.20.20.7.1" class="ltx_text" style="font-size:80%;">AP</span></td>
<td id="S5.T7.20.20.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T7.20.20.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.20.20.8.1.1" class="ltx_p" style="width:216.2pt;"><span id="S5.T7.20.20.8.1.1.1" class="ltx_text" style="font-size:80%;">Various body poses; From Internet search engines; Multiple annotations (bounding boxes, joint locations); </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T7.20.20.8.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Wu et al.<span id="S5.T7.20.20.8.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib175" title="" class="ltx_ref">2017</a><span id="S5.T7.20.20.8.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S5.T7.24.28.4" class="ltx_tr">
<td id="S5.T7.24.28.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" colspan="8"><span id="S5.T7.24.28.4.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Video-based</span></td>
</tr>
<tr id="S5.T7.22.22" class="ltx_tr">
<td id="S5.T7.22.22.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T7.22.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.22.22.3.1.1" class="ltx_p" style="width:31.3pt;">
<span id="S5.T7.22.22.3.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T7.22.22.3.1.1.1.1" class="ltx_tr">
<span id="S5.T7.22.22.3.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T7.22.22.3.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Penn</span></span></span>
<span id="S5.T7.22.22.3.1.1.1.2" class="ltx_tr">
<span id="S5.T7.22.22.3.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T7.22.22.3.1.1.1.2.1.1" class="ltx_text" style="font-size:80%;">Action</span></span></span>
</span></span>
</span>
</td>
<td id="S5.T7.22.22.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T7.22.22.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.22.22.4.1.1" class="ltx_p" style="width:28.5pt;"><span id="S5.T7.22.22.4.1.1.1" class="ltx_text" style="font-size:80%;">single</span></span>
</span>
</td>
<td id="S5.T7.22.22.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.22.22.5.1" class="ltx_text" style="font-size:80%;">13</span></td>
<td id="S5.T7.21.21.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T7.21.21.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.21.21.1.m1.1a"><mo mathsize="80%" id="S5.T7.21.21.1.m1.1.1" xref="S5.T7.21.21.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.21.21.1.m1.1b"><approx id="S5.T7.21.21.1.m1.1.1.cmml" xref="S5.T7.21.21.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.21.21.1.m1.1c">\approx</annotation></semantics></math><span id="S5.T7.21.21.1.1" class="ltx_text" style="font-size:80%;">1k</span>
</td>
<td id="S5.T7.22.22.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.22.22.6.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T7.22.22.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T7.22.22.2.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.22.22.2.m1.1a"><mo mathsize="80%" id="S5.T7.22.22.2.m1.1.1" xref="S5.T7.22.22.2.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.22.22.2.m1.1b"><approx id="S5.T7.22.22.2.m1.1.1.cmml" xref="S5.T7.22.22.2.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.22.22.2.m1.1c">\approx</annotation></semantics></math><span id="S5.T7.22.22.2.1" class="ltx_text" style="font-size:80%;">1k</span>
</td>
<td id="S5.T7.22.22.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.22.22.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T7.22.22.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T7.22.22.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.22.22.8.1.1" class="ltx_p" style="width:216.2pt;">
<span id="S5.T7.22.22.8.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T7.22.22.8.1.1.1.1" class="ltx_tr">
<span id="S5.T7.22.22.8.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T7.22.22.8.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">full-body poses; From YouTube; 15 actions; Multiple annotations</span></span></span>
<span id="S5.T7.22.22.8.1.1.1.2" class="ltx_tr">
<span id="S5.T7.22.22.8.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T7.22.22.8.1.1.1.2.1.1" class="ltx_text" style="font-size:80%;">(joint locations, bounding boxes, action classes) </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T7.22.22.8.1.1.1.2.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Zhang et al.<span id="S5.T7.22.22.8.1.1.1.2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib182" title="" class="ltx_ref">2013</a><span id="S5.T7.22.22.8.1.1.1.2.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S5.T7.22.22.8.1.1.1.2.1.5" class="ltx_text" style="font-size:80%;">.</span></span></span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.24.24" class="ltx_tr">
<td id="S5.T7.24.24.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T7.24.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.24.24.3.1.1" class="ltx_p" style="width:31.3pt;"><span id="S5.T7.24.24.3.1.1.1" class="ltx_text" style="font-size:80%;">J-HMDB</span></span>
</span>
</td>
<td id="S5.T7.24.24.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T7.24.24.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.24.24.4.1.1" class="ltx_p" style="width:28.5pt;"><span id="S5.T7.24.24.4.1.1.1" class="ltx_text" style="font-size:80%;">single</span></span>
</span>
</td>
<td id="S5.T7.24.24.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.24.24.5.1" class="ltx_text" style="font-size:80%;">15</span></td>
<td id="S5.T7.23.23.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T7.23.23.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.23.23.1.m1.1a"><mo mathsize="80%" id="S5.T7.23.23.1.m1.1.1" xref="S5.T7.23.23.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.23.23.1.m1.1b"><approx id="S5.T7.23.23.1.m1.1.1.cmml" xref="S5.T7.23.23.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.23.23.1.m1.1c">\approx</annotation></semantics></math><span id="S5.T7.23.23.1.1" class="ltx_text" style="font-size:80%;">0.6k</span>
</td>
<td id="S5.T7.24.24.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.24.24.6.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T7.24.24.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T7.24.24.2.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T7.24.24.2.m1.1a"><mo mathsize="80%" id="S5.T7.24.24.2.m1.1.1" xref="S5.T7.24.24.2.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T7.24.24.2.m1.1b"><approx id="S5.T7.24.24.2.m1.1.1.cmml" xref="S5.T7.24.24.2.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.24.24.2.m1.1c">\approx</annotation></semantics></math><span id="S5.T7.24.24.2.1" class="ltx_text" style="font-size:80%;">0.3k</span>
</td>
<td id="S5.T7.24.24.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.24.24.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T7.24.24.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T7.24.24.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.24.24.8.1.1" class="ltx_p" style="width:216.2pt;">
<span id="S5.T7.24.24.8.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T7.24.24.8.1.1.1.1" class="ltx_tr">
<span id="S5.T7.24.24.8.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T7.24.24.8.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">full-body poses; Generated from action recognition dataset; 21</span></span></span>
<span id="S5.T7.24.24.8.1.1.1.2" class="ltx_tr">
<span id="S5.T7.24.24.8.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T7.24.24.8.1.1.1.2.1.1" class="ltx_text" style="font-size:80%;">actions; Multiple annotations (joint positions and relations, optical</span></span></span>
<span id="S5.T7.24.24.8.1.1.1.3" class="ltx_tr">
<span id="S5.T7.24.24.8.1.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T7.24.24.8.1.1.1.3.1.1" class="ltx_text" style="font-size:80%;">flows, segmentation masks) </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T7.24.24.8.1.1.1.3.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Jhuang et al.<span id="S5.T7.24.24.8.1.1.1.3.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib65" title="" class="ltx_ref">2013</a><span id="S5.T7.24.24.8.1.1.1.3.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S5.T7.24.24.8.1.1.1.3.1.5" class="ltx_text" style="font-size:80%;">.</span></span></span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.24.29.5" class="ltx_tr">
<td id="S5.T7.24.29.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T7.24.29.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.24.29.5.1.1.1" class="ltx_p" style="width:31.3pt;">
<span id="S5.T7.24.29.5.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T7.24.29.5.1.1.1.1.1" class="ltx_tr">
<span id="S5.T7.24.29.5.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T7.24.29.5.1.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">PoseTrack</span></span></span>
</span></span>
</span>
</td>
<td id="S5.T7.24.29.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T7.24.29.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.24.29.5.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S5.T7.24.29.5.2.1.1.1" class="ltx_text" style="font-size:80%;">multiple</span></span>
</span>
</td>
<td id="S5.T7.24.29.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T7.24.29.5.3.1" class="ltx_text" style="font-size:80%;">15</span></td>
<td id="S5.T7.24.29.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T7.24.29.5.4.1" class="ltx_text" style="font-size:80%;">292</span></td>
<td id="S5.T7.24.29.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T7.24.29.5.5.1" class="ltx_text" style="font-size:80%;">50</span></td>
<td id="S5.T7.24.29.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T7.24.29.5.6.1" class="ltx_text" style="font-size:80%;">208</span></td>
<td id="S5.T7.24.29.5.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T7.24.29.5.7.1" class="ltx_text" style="font-size:80%;">mAP</span></td>
<td id="S5.T7.24.29.5.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T7.24.29.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.24.29.5.8.1.1" class="ltx_p" style="width:216.2pt;">
<span id="S5.T7.24.29.5.8.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T7.24.29.5.8.1.1.1.1" class="ltx_tr">
<span id="S5.T7.24.29.5.8.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T7.24.29.5.8.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Various body poses; Extended from MPII; Dense annotations</span></span></span>
<span id="S5.T7.24.29.5.8.1.1.1.2" class="ltx_tr">
<span id="S5.T7.24.29.5.8.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T7.24.29.5.8.1.1.1.2.1.1" class="ltx_text" style="font-size:80%;">(joint locations, head bounding boxes) </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T7.24.29.5.8.1.1.1.2.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Andriluka et al.<span id="S5.T7.24.29.5.8.1.1.1.2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib3" title="" class="ltx_ref">2018</a><span id="S5.T7.24.29.5.8.1.1.1.2.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S5.T7.24.29.5.8.1.1.1.2.1.5" class="ltx_text" style="font-size:80%;">.</span></span></span>
</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2006.01423/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="447" height="473" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 5: </span>Some selected example images with annotations from typical 2D human pose estimation datasets.</figcaption>
</figure>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.2" class="ltx_p"><span id="S5.SS1.p3.2.1" class="ltx_text ltx_font_bold">Frames Labeled In Cinema (FLIC) Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Sapp and Taskar, <a href="#bib.bib143" title="" class="ltx_ref">2013</a>)</cite> contains <math id="S5.SS1.p3.1.m1.2" class="ltx_Math" alttext="5,003" display="inline"><semantics id="S5.SS1.p3.1.m1.2a"><mrow id="S5.SS1.p3.1.m1.2.3.2" xref="S5.SS1.p3.1.m1.2.3.1.cmml"><mn id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">5</mn><mo id="S5.SS1.p3.1.m1.2.3.2.1" xref="S5.SS1.p3.1.m1.2.3.1.cmml">,</mo><mn id="S5.SS1.p3.1.m1.2.2" xref="S5.SS1.p3.1.m1.2.2.cmml">003</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.2b"><list id="S5.SS1.p3.1.m1.2.3.1.cmml" xref="S5.SS1.p3.1.m1.2.3.2"><cn type="integer" id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">5</cn><cn type="integer" id="S5.SS1.p3.1.m1.2.2.cmml" xref="S5.SS1.p3.1.m1.2.2">003</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.2c">5,003</annotation></semantics></math> images collected from popular Hollywood movies. For every tenth frame of 30 movies, a person detector <cite class="ltx_cite ltx_citemacro_citep">(Bourdev and Malik, <a href="#bib.bib11" title="" class="ltx_ref">2009</a>)</cite> was run to obtain about 20K person candidates. Then all candidates are sent to Amazon Mechanical Turk to obtain ground truth labeling for 10 upper body joints. Finally, images with person occluded or severely non-frontal views are manually deleted. The undeleted original set called FLIC-full consisting of occluded, non-frontal, or just plain mislabeled examples (<math id="S5.SS1.p3.2.m2.2" class="ltx_Math" alttext="20,928" display="inline"><semantics id="S5.SS1.p3.2.m2.2a"><mrow id="S5.SS1.p3.2.m2.2.3.2" xref="S5.SS1.p3.2.m2.2.3.1.cmml"><mn id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">20</mn><mo id="S5.SS1.p3.2.m2.2.3.2.1" xref="S5.SS1.p3.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS1.p3.2.m2.2.2" xref="S5.SS1.p3.2.m2.2.2.cmml">928</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.2b"><list id="S5.SS1.p3.2.m2.2.3.1.cmml" xref="S5.SS1.p3.2.m2.2.3.2"><cn type="integer" id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">20</cn><cn type="integer" id="S5.SS1.p3.2.m2.2.2.cmml" xref="S5.SS1.p3.2.m2.2.2">928</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.2c">20,928</annotation></semantics></math> examples) is also available. Moreover, in <cite class="ltx_cite ltx_citemacro_citep">(Tompson et al., <a href="#bib.bib164" title="" class="ltx_ref">2014</a>)</cite>, the FLIC-full dataset is further cleaned to FLIC-plus to make sure that the training subset does not include any images from the same scene as the test subset.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.2" class="ltx_p"><span id="S5.SS1.p4.2.1" class="ltx_text ltx_font_bold">Leeds Sports Pose (LSP) Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Johnson and Everingham, <a href="#bib.bib68" title="" class="ltx_ref">2010</a>)</cite> contains <math id="S5.SS1.p4.1.m1.2" class="ltx_Math" alttext="2,000" display="inline"><semantics id="S5.SS1.p4.1.m1.2a"><mrow id="S5.SS1.p4.1.m1.2.3.2" xref="S5.SS1.p4.1.m1.2.3.1.cmml"><mn id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml">2</mn><mo id="S5.SS1.p4.1.m1.2.3.2.1" xref="S5.SS1.p4.1.m1.2.3.1.cmml">,</mo><mn id="S5.SS1.p4.1.m1.2.2" xref="S5.SS1.p4.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.2b"><list id="S5.SS1.p4.1.m1.2.3.1.cmml" xref="S5.SS1.p4.1.m1.2.3.2"><cn type="integer" id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1">2</cn><cn type="integer" id="S5.SS1.p4.1.m1.2.2.cmml" xref="S5.SS1.p4.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.2c">2,000</annotation></semantics></math> images of full-body poses collected from Flickr by downloading with 8 sports tags (athletics, badminton, baseball, gymnastics, parkour, soccer, tennis, and volleyball). Each image is annotated with up to 14 visible joint locations. Further, the extension version Leeds Sports Pose Extended (LSP-extended) training dataset <cite class="ltx_cite ltx_citemacro_citep">(Johnson and Everingham, <a href="#bib.bib69" title="" class="ltx_ref">2011</a>)</cite> is gathered to extend the LSP dataset only for training. It contains <math id="S5.SS1.p4.2.m2.2" class="ltx_Math" alttext="10,000" display="inline"><semantics id="S5.SS1.p4.2.m2.2a"><mrow id="S5.SS1.p4.2.m2.2.3.2" xref="S5.SS1.p4.2.m2.2.3.1.cmml"><mn id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml">10</mn><mo id="S5.SS1.p4.2.m2.2.3.2.1" xref="S5.SS1.p4.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS1.p4.2.m2.2.2" xref="S5.SS1.p4.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.2b"><list id="S5.SS1.p4.2.m2.2.3.1.cmml" xref="S5.SS1.p4.2.m2.2.3.2"><cn type="integer" id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1">10</cn><cn type="integer" id="S5.SS1.p4.2.m2.2.2.cmml" xref="S5.SS1.p4.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.2c">10,000</annotation></semantics></math> images collected from Flickr searches with 3 most challenging tags (parkour, gymnastics, and athletics). The annotations were conducted through Amazon Mechanical Turk and the accuracy cannot be guaranteed.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.3" class="ltx_p"><span id="S5.SS1.p5.3.1" class="ltx_text ltx_font_bold">Max Planck Institute for Informatics (MPII) Human Pose Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite> is one of current the state-of-the-art benchmarks for evaluation of articulated human pose estimation with rich annotations. First, with guidance from a two-level hierarchy of human activities from <cite class="ltx_cite ltx_citemacro_citep">(Ainsworth et al., <a href="#bib.bib2" title="" class="ltx_ref">2011</a>)</cite>, <math id="S5.SS1.p5.1.m1.2" class="ltx_Math" alttext="3,913" display="inline"><semantics id="S5.SS1.p5.1.m1.2a"><mrow id="S5.SS1.p5.1.m1.2.3.2" xref="S5.SS1.p5.1.m1.2.3.1.cmml"><mn id="S5.SS1.p5.1.m1.1.1" xref="S5.SS1.p5.1.m1.1.1.cmml">3</mn><mo id="S5.SS1.p5.1.m1.2.3.2.1" xref="S5.SS1.p5.1.m1.2.3.1.cmml">,</mo><mn id="S5.SS1.p5.1.m1.2.2" xref="S5.SS1.p5.1.m1.2.2.cmml">913</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.1.m1.2b"><list id="S5.SS1.p5.1.m1.2.3.1.cmml" xref="S5.SS1.p5.1.m1.2.3.2"><cn type="integer" id="S5.SS1.p5.1.m1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1">3</cn><cn type="integer" id="S5.SS1.p5.1.m1.2.2.cmml" xref="S5.SS1.p5.1.m1.2.2">913</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.1.m1.2c">3,913</annotation></semantics></math> videos spanning <math id="S5.SS1.p5.2.m2.1" class="ltx_Math" alttext="491" display="inline"><semantics id="S5.SS1.p5.2.m2.1a"><mn id="S5.SS1.p5.2.m2.1.1" xref="S5.SS1.p5.2.m2.1.1.cmml">491</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.2.m2.1b"><cn type="integer" id="S5.SS1.p5.2.m2.1.1.cmml" xref="S5.SS1.p5.2.m2.1.1">491</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.2.m2.1c">491</annotation></semantics></math> different activities are downloaded from YouTube. Then frames that either contains different people in the video or the same person in a very different pose were manually selected which results in <math id="S5.SS1.p5.3.m3.2" class="ltx_Math" alttext="24,920" display="inline"><semantics id="S5.SS1.p5.3.m3.2a"><mrow id="S5.SS1.p5.3.m3.2.3.2" xref="S5.SS1.p5.3.m3.2.3.1.cmml"><mn id="S5.SS1.p5.3.m3.1.1" xref="S5.SS1.p5.3.m3.1.1.cmml">24</mn><mo id="S5.SS1.p5.3.m3.2.3.2.1" xref="S5.SS1.p5.3.m3.2.3.1.cmml">,</mo><mn id="S5.SS1.p5.3.m3.2.2" xref="S5.SS1.p5.3.m3.2.2.cmml">920</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.3.m3.2b"><list id="S5.SS1.p5.3.m3.2.3.1.cmml" xref="S5.SS1.p5.3.m3.2.3.2"><cn type="integer" id="S5.SS1.p5.3.m3.1.1.cmml" xref="S5.SS1.p5.3.m3.1.1">24</cn><cn type="integer" id="S5.SS1.p5.3.m3.2.2.cmml" xref="S5.SS1.p5.3.m3.2.2">920</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.3.m3.2c">24,920</annotation></semantics></math> frames. Rich annotations including 16 body joints, the 3D viewpoint of the head and torso and position of the eyes and nose are labeled by in-house workers and on Amazon Mechanical Turk. For corresponding joints, visibility and left/right labels are also annotated in a person-centric way. Images in MPII have various body poses and are suitable for many tasks such as 2D single/multiple human pose estimation, action recognition, etc.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.2" class="ltx_p"><span id="S5.SS1.p6.2.1" class="ltx_text ltx_font_bold">Microsoft Common Objects in Context (COCO) Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib93" title="" class="ltx_ref">2014</a>)</cite> is a large-scale dataset that was originally proposed for daily object detection and segmentation in natural environments. With improvements and extensions, the usage of COCO covers image captioning and keypoint detection. Images are collected from Google, Bing, and Flickr image search with isolated or pairwise object categories. Annotations were conducted on Amazon Mechanical Turk. The whole set contains more than <math id="S5.SS1.p6.1.m1.2" class="ltx_Math" alttext="200,000" display="inline"><semantics id="S5.SS1.p6.1.m1.2a"><mrow id="S5.SS1.p6.1.m1.2.3.2" xref="S5.SS1.p6.1.m1.2.3.1.cmml"><mn id="S5.SS1.p6.1.m1.1.1" xref="S5.SS1.p6.1.m1.1.1.cmml">200</mn><mo id="S5.SS1.p6.1.m1.2.3.2.1" xref="S5.SS1.p6.1.m1.2.3.1.cmml">,</mo><mn id="S5.SS1.p6.1.m1.2.2" xref="S5.SS1.p6.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.1.m1.2b"><list id="S5.SS1.p6.1.m1.2.3.1.cmml" xref="S5.SS1.p6.1.m1.2.3.2"><cn type="integer" id="S5.SS1.p6.1.m1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1">200</cn><cn type="integer" id="S5.SS1.p6.1.m1.2.2.cmml" xref="S5.SS1.p6.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.1.m1.2c">200,000</annotation></semantics></math> images and <math id="S5.SS1.p6.2.m2.2" class="ltx_Math" alttext="250,000" display="inline"><semantics id="S5.SS1.p6.2.m2.2a"><mrow id="S5.SS1.p6.2.m2.2.3.2" xref="S5.SS1.p6.2.m2.2.3.1.cmml"><mn id="S5.SS1.p6.2.m2.1.1" xref="S5.SS1.p6.2.m2.1.1.cmml">250</mn><mo id="S5.SS1.p6.2.m2.2.3.2.1" xref="S5.SS1.p6.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS1.p6.2.m2.2.2" xref="S5.SS1.p6.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.2.m2.2b"><list id="S5.SS1.p6.2.m2.2.3.1.cmml" xref="S5.SS1.p6.2.m2.2.3.2"><cn type="integer" id="S5.SS1.p6.2.m2.1.1.cmml" xref="S5.SS1.p6.2.m2.1.1">250</cn><cn type="integer" id="S5.SS1.p6.2.m2.2.2.cmml" xref="S5.SS1.p6.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.2.m2.2c">250,000</annotation></semantics></math> labeled person instances. Suitable examples are selected for human pose estimation, thus forming two datasets: COCO keypoints 2016 and COCO keypoints 2017, corresponding to two public keypoint detection challenges respectively. The only difference between these two versions is the train/val/test splitting strategy based on community feedback (shown in Table <a href="#S5.T7" title="Table 7 ‣ 5.1 Datasets for 2D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>), and cross-year results can be compared directly since the images in the test set are same. The COCO Keypoint Detection Challenge aims to localize keypoints of people in uncontrolled images. The annotations for each person include 17 body joints with visibility and left/right labels, and instance human body segmentation. Note that COCO dataset contains about 120K unlabeled images following the same class distribution as the labeled images which can be used for unsupervised or semi-supervised learning.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.4" class="ltx_p"><span id="S5.SS1.p7.4.1" class="ltx_text ltx_font_bold">AI Challenger Human Keypoint Detection (AIC-HKD) Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a href="#bib.bib175" title="" class="ltx_ref">2017</a>)</cite> has the largest number of training examples. It contains <math id="S5.SS1.p7.1.m1.2" class="ltx_Math" alttext="210,000" display="inline"><semantics id="S5.SS1.p7.1.m1.2a"><mrow id="S5.SS1.p7.1.m1.2.3.2" xref="S5.SS1.p7.1.m1.2.3.1.cmml"><mn id="S5.SS1.p7.1.m1.1.1" xref="S5.SS1.p7.1.m1.1.1.cmml">210</mn><mo id="S5.SS1.p7.1.m1.2.3.2.1" xref="S5.SS1.p7.1.m1.2.3.1.cmml">,</mo><mn id="S5.SS1.p7.1.m1.2.2" xref="S5.SS1.p7.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.1.m1.2b"><list id="S5.SS1.p7.1.m1.2.3.1.cmml" xref="S5.SS1.p7.1.m1.2.3.2"><cn type="integer" id="S5.SS1.p7.1.m1.1.1.cmml" xref="S5.SS1.p7.1.m1.1.1">210</cn><cn type="integer" id="S5.SS1.p7.1.m1.2.2.cmml" xref="S5.SS1.p7.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.1.m1.2c">210,000</annotation></semantics></math>, <math id="S5.SS1.p7.2.m2.2" class="ltx_Math" alttext="30,000" display="inline"><semantics id="S5.SS1.p7.2.m2.2a"><mrow id="S5.SS1.p7.2.m2.2.3.2" xref="S5.SS1.p7.2.m2.2.3.1.cmml"><mn id="S5.SS1.p7.2.m2.1.1" xref="S5.SS1.p7.2.m2.1.1.cmml">30</mn><mo id="S5.SS1.p7.2.m2.2.3.2.1" xref="S5.SS1.p7.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS1.p7.2.m2.2.2" xref="S5.SS1.p7.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.2.m2.2b"><list id="S5.SS1.p7.2.m2.2.3.1.cmml" xref="S5.SS1.p7.2.m2.2.3.2"><cn type="integer" id="S5.SS1.p7.2.m2.1.1.cmml" xref="S5.SS1.p7.2.m2.1.1">30</cn><cn type="integer" id="S5.SS1.p7.2.m2.2.2.cmml" xref="S5.SS1.p7.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.2.m2.2c">30,000</annotation></semantics></math>, <math id="S5.SS1.p7.3.m3.2" class="ltx_Math" alttext="30,000" display="inline"><semantics id="S5.SS1.p7.3.m3.2a"><mrow id="S5.SS1.p7.3.m3.2.3.2" xref="S5.SS1.p7.3.m3.2.3.1.cmml"><mn id="S5.SS1.p7.3.m3.1.1" xref="S5.SS1.p7.3.m3.1.1.cmml">30</mn><mo id="S5.SS1.p7.3.m3.2.3.2.1" xref="S5.SS1.p7.3.m3.2.3.1.cmml">,</mo><mn id="S5.SS1.p7.3.m3.2.2" xref="S5.SS1.p7.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.3.m3.2b"><list id="S5.SS1.p7.3.m3.2.3.1.cmml" xref="S5.SS1.p7.3.m3.2.3.2"><cn type="integer" id="S5.SS1.p7.3.m3.1.1.cmml" xref="S5.SS1.p7.3.m3.1.1">30</cn><cn type="integer" id="S5.SS1.p7.3.m3.2.2.cmml" xref="S5.SS1.p7.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.3.m3.2c">30,000</annotation></semantics></math>, and <math id="S5.SS1.p7.4.m4.2" class="ltx_Math" alttext="30,000" display="inline"><semantics id="S5.SS1.p7.4.m4.2a"><mrow id="S5.SS1.p7.4.m4.2.3.2" xref="S5.SS1.p7.4.m4.2.3.1.cmml"><mn id="S5.SS1.p7.4.m4.1.1" xref="S5.SS1.p7.4.m4.1.1.cmml">30</mn><mo id="S5.SS1.p7.4.m4.2.3.2.1" xref="S5.SS1.p7.4.m4.2.3.1.cmml">,</mo><mn id="S5.SS1.p7.4.m4.2.2" xref="S5.SS1.p7.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.4.m4.2b"><list id="S5.SS1.p7.4.m4.2.3.1.cmml" xref="S5.SS1.p7.4.m4.2.3.2"><cn type="integer" id="S5.SS1.p7.4.m4.1.1.cmml" xref="S5.SS1.p7.4.m4.1.1">30</cn><cn type="integer" id="S5.SS1.p7.4.m4.2.2.cmml" xref="S5.SS1.p7.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.4.m4.2c">30,000</annotation></semantics></math> images for training, validation, test A, and test B respectively. The images, focusing on the daily life of people, were collected from Internet search engines. Then, after removing inappropriate examples (e.g. with the political, constabulary, violent and sexual contents; too small or too crowded human figures), each person in the images were annotated with a bounding box and 14 keypoints. Each keypoint has the visibility and left/right labels.</p>
</div>
<div id="S5.SS1.p8" class="ltx_para">
<p id="S5.SS1.p8.1" class="ltx_p">In addition to the datasets described above which are in static image style, datasets with densely annotated video frames are collected in closer to real-life application scenarios which offer the possibility to utilize temporal information and can be used for action recognition. Some of them focus on single individuals <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib182" title="" class="ltx_ref">2013</a>; Jhuang et al., <a href="#bib.bib65" title="" class="ltx_ref">2013</a>; Charles et al., <a href="#bib.bib16" title="" class="ltx_ref">2016</a>)</cite> and others have pose annotations for multiple people <cite class="ltx_cite ltx_citemacro_citep">(Insafutdinov et al., <a href="#bib.bib57" title="" class="ltx_ref">2017</a>; Iqbal et al., <a href="#bib.bib61" title="" class="ltx_ref">2016</a>; Andriluka et al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S5.SS1.p9" class="ltx_para">
<p id="S5.SS1.p9.1" class="ltx_p"><span id="S5.SS1.p9.1.1" class="ltx_text ltx_font_bold">Penn Action Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib182" title="" class="ltx_ref">2013</a>)</cite> consists of <math id="S5.SS1.p9.1.m1.2" class="ltx_Math" alttext="2,326" display="inline"><semantics id="S5.SS1.p9.1.m1.2a"><mrow id="S5.SS1.p9.1.m1.2.3.2" xref="S5.SS1.p9.1.m1.2.3.1.cmml"><mn id="S5.SS1.p9.1.m1.1.1" xref="S5.SS1.p9.1.m1.1.1.cmml">2</mn><mo id="S5.SS1.p9.1.m1.2.3.2.1" xref="S5.SS1.p9.1.m1.2.3.1.cmml">,</mo><mn id="S5.SS1.p9.1.m1.2.2" xref="S5.SS1.p9.1.m1.2.2.cmml">326</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p9.1.m1.2b"><list id="S5.SS1.p9.1.m1.2.3.1.cmml" xref="S5.SS1.p9.1.m1.2.3.2"><cn type="integer" id="S5.SS1.p9.1.m1.1.1.cmml" xref="S5.SS1.p9.1.m1.1.1">2</cn><cn type="integer" id="S5.SS1.p9.1.m1.2.2.cmml" xref="S5.SS1.p9.1.m1.2.2">326</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p9.1.m1.2c">2,326</annotation></semantics></math> videos downloaded from YouTube covering 15 actions: baseball pitch, baseball swing, bench press, bowling, clean and jerk, golf swing, jump rope, jumping jacks, pull up, push up, sit up, squat, strum guitar, tennis forehand, and tennis serve. Annotations for each frame were labeled by VATIC <cite class="ltx_cite ltx_citemacro_citep">(Vondrick et al., <a href="#bib.bib170" title="" class="ltx_ref">2013</a>)</cite> (an annotation tool) on Amazon Mechanical Turk. Each video involves an action class label and each video frame contains a bounding box of human and 13 joint locations with the visibility and left/right labels.</p>
</div>
<div id="S5.SS1.p10" class="ltx_para">
<p id="S5.SS1.p10.4" class="ltx_p"><span id="S5.SS1.p10.4.1" class="ltx_text ltx_font_bold">Joint-annotated Human Motion Database (J-HMDB)</span> <cite class="ltx_cite ltx_citemacro_citep">(Jhuang et al., <a href="#bib.bib65" title="" class="ltx_ref">2013</a>)</cite> is based on the HMDB51 <cite class="ltx_cite ltx_citemacro_citep">(Jhuang et al., <a href="#bib.bib66" title="" class="ltx_ref">2011</a>)</cite> which is originally collected for action recognition. First, 21 action categories with relatively large body movements were selected from original 51 actions in HMDB51, including: brush hair, catch, clap, climb stairs, golf, jump, kick ball, pick, pour, pull-up, push, run, shoot ball, shoot bow, shoot gun, sit, stand, swing baseball, throw, walk, and wave. Then, after a selection-and-cleaning process, <math id="S5.SS1.p10.1.m1.1" class="ltx_Math" alttext="928" display="inline"><semantics id="S5.SS1.p10.1.m1.1a"><mn id="S5.SS1.p10.1.m1.1.1" xref="S5.SS1.p10.1.m1.1.1.cmml">928</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p10.1.m1.1b"><cn type="integer" id="S5.SS1.p10.1.m1.1.1.cmml" xref="S5.SS1.p10.1.m1.1.1">928</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p10.1.m1.1c">928</annotation></semantics></math> clips comprising <math id="S5.SS1.p10.2.m2.2" class="ltx_Math" alttext="31,838" display="inline"><semantics id="S5.SS1.p10.2.m2.2a"><mrow id="S5.SS1.p10.2.m2.2.3.2" xref="S5.SS1.p10.2.m2.2.3.1.cmml"><mn id="S5.SS1.p10.2.m2.1.1" xref="S5.SS1.p10.2.m2.1.1.cmml">31</mn><mo id="S5.SS1.p10.2.m2.2.3.2.1" xref="S5.SS1.p10.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS1.p10.2.m2.2.2" xref="S5.SS1.p10.2.m2.2.2.cmml">838</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p10.2.m2.2b"><list id="S5.SS1.p10.2.m2.2.3.1.cmml" xref="S5.SS1.p10.2.m2.2.3.2"><cn type="integer" id="S5.SS1.p10.2.m2.1.1.cmml" xref="S5.SS1.p10.2.m2.1.1">31</cn><cn type="integer" id="S5.SS1.p10.2.m2.2.2.cmml" xref="S5.SS1.p10.2.m2.2.2">838</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p10.2.m2.2c">31,838</annotation></semantics></math> annotated frames are selected. Finally, a 2D articulated human puppet model <cite class="ltx_cite ltx_citemacro_citep">(Zuffi et al., <a href="#bib.bib187" title="" class="ltx_ref">2012</a>)</cite> is employed to generate all the needed annotations using Amazon Mechanical Turk. The 2D puppet model is an articulated human body model that provides scale, pose, segmentation, coarse viewpoint, and dense optical flow for the humans in actions. The annotations include 15 joint positions and relations, 2D optical flow corresponding to the human motion, human body segmentation mask. The 70<math id="S5.SS1.p10.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS1.p10.3.m3.1a"><mo id="S5.SS1.p10.3.m3.1.1" xref="S5.SS1.p10.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p10.3.m3.1b"><csymbol cd="latexml" id="S5.SS1.p10.3.m3.1.1.cmml" xref="S5.SS1.p10.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p10.3.m3.1c">\%</annotation></semantics></math> images are used for training and the 30<math id="S5.SS1.p10.4.m4.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS1.p10.4.m4.1a"><mo id="S5.SS1.p10.4.m4.1.1" xref="S5.SS1.p10.4.m4.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p10.4.m4.1b"><csymbol cd="latexml" id="S5.SS1.p10.4.m4.1.1.cmml" xref="S5.SS1.p10.4.m4.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p10.4.m4.1c">\%</annotation></semantics></math> images for testing. J-HMDB can also be used for action recognition and human detection tasks.</p>
</div>
<div id="S5.SS1.p11" class="ltx_para">
<p id="S5.SS1.p11.7" class="ltx_p">There are several video datasets annotated with human upper body pose. <span id="S5.SS1.p11.7.1" class="ltx_text ltx_font_bold">BBC Pose</span> <cite class="ltx_cite ltx_citemacro_citep">(Charles et al., <a href="#bib.bib15" title="" class="ltx_ref">2014</a>)</cite> contains 20 videos (10/5/5 for train/val/test, 1.5 million frames in total) with 9 sign language signers. <math id="S5.SS1.p11.1.m1.2" class="ltx_Math" alttext="2,000" display="inline"><semantics id="S5.SS1.p11.1.m1.2a"><mrow id="S5.SS1.p11.1.m1.2.3.2" xref="S5.SS1.p11.1.m1.2.3.1.cmml"><mn id="S5.SS1.p11.1.m1.1.1" xref="S5.SS1.p11.1.m1.1.1.cmml">2</mn><mo id="S5.SS1.p11.1.m1.2.3.2.1" xref="S5.SS1.p11.1.m1.2.3.1.cmml">,</mo><mn id="S5.SS1.p11.1.m1.2.2" xref="S5.SS1.p11.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p11.1.m1.2b"><list id="S5.SS1.p11.1.m1.2.3.1.cmml" xref="S5.SS1.p11.1.m1.2.3.2"><cn type="integer" id="S5.SS1.p11.1.m1.1.1.cmml" xref="S5.SS1.p11.1.m1.1.1">2</cn><cn type="integer" id="S5.SS1.p11.1.m1.2.2.cmml" xref="S5.SS1.p11.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p11.1.m1.2c">2,000</annotation></semantics></math> frames for validation and test are manually annotated and the rest of the frames are annotated with a semi-automatic method. <span id="S5.SS1.p11.7.2" class="ltx_text ltx_font_bold">Extended BBC Pose</span> dataset <cite class="ltx_cite ltx_citemacro_citep">(Pfister et al., <a href="#bib.bib130" title="" class="ltx_ref">2014</a>)</cite> adds <math id="S5.SS1.p11.2.m2.1" class="ltx_Math" alttext="72" display="inline"><semantics id="S5.SS1.p11.2.m2.1a"><mn id="S5.SS1.p11.2.m2.1.1" xref="S5.SS1.p11.2.m2.1.1.cmml">72</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p11.2.m2.1b"><cn type="integer" id="S5.SS1.p11.2.m2.1.1.cmml" xref="S5.SS1.p11.2.m2.1.1">72</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p11.2.m2.1c">72</annotation></semantics></math> additional training videos for <span id="S5.SS1.p11.7.3" class="ltx_text ltx_font_bold">BBC Pose</span> which has about <math id="S5.SS1.p11.3.m3.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S5.SS1.p11.3.m3.1a"><mn id="S5.SS1.p11.3.m3.1.1" xref="S5.SS1.p11.3.m3.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p11.3.m3.1b"><cn type="integer" id="S5.SS1.p11.3.m3.1.1.cmml" xref="S5.SS1.p11.3.m3.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p11.3.m3.1c">7</annotation></semantics></math> million frames in total. <span id="S5.SS1.p11.7.4" class="ltx_text ltx_font_bold">MPII Cooking</span> <cite class="ltx_cite ltx_citemacro_citep">(Rohrbach et al., <a href="#bib.bib142" title="" class="ltx_ref">2012</a>)</cite> dataset contains <math id="S5.SS1.p11.4.m4.2" class="ltx_Math" alttext="1,071" display="inline"><semantics id="S5.SS1.p11.4.m4.2a"><mrow id="S5.SS1.p11.4.m4.2.3.2" xref="S5.SS1.p11.4.m4.2.3.1.cmml"><mn id="S5.SS1.p11.4.m4.1.1" xref="S5.SS1.p11.4.m4.1.1.cmml">1</mn><mo id="S5.SS1.p11.4.m4.2.3.2.1" xref="S5.SS1.p11.4.m4.2.3.1.cmml">,</mo><mn id="S5.SS1.p11.4.m4.2.2" xref="S5.SS1.p11.4.m4.2.2.cmml">071</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p11.4.m4.2b"><list id="S5.SS1.p11.4.m4.2.3.1.cmml" xref="S5.SS1.p11.4.m4.2.3.2"><cn type="integer" id="S5.SS1.p11.4.m4.1.1.cmml" xref="S5.SS1.p11.4.m4.1.1">1</cn><cn type="integer" id="S5.SS1.p11.4.m4.2.2.cmml" xref="S5.SS1.p11.4.m4.2.2">071</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p11.4.m4.2c">1,071</annotation></semantics></math> frames for training and <math id="S5.SS1.p11.5.m5.2" class="ltx_Math" alttext="1,277" display="inline"><semantics id="S5.SS1.p11.5.m5.2a"><mrow id="S5.SS1.p11.5.m5.2.3.2" xref="S5.SS1.p11.5.m5.2.3.1.cmml"><mn id="S5.SS1.p11.5.m5.1.1" xref="S5.SS1.p11.5.m5.1.1.cmml">1</mn><mo id="S5.SS1.p11.5.m5.2.3.2.1" xref="S5.SS1.p11.5.m5.2.3.1.cmml">,</mo><mn id="S5.SS1.p11.5.m5.2.2" xref="S5.SS1.p11.5.m5.2.2.cmml">277</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p11.5.m5.2b"><list id="S5.SS1.p11.5.m5.2.3.1.cmml" xref="S5.SS1.p11.5.m5.2.3.2"><cn type="integer" id="S5.SS1.p11.5.m5.1.1.cmml" xref="S5.SS1.p11.5.m5.1.1">1</cn><cn type="integer" id="S5.SS1.p11.5.m5.2.2.cmml" xref="S5.SS1.p11.5.m5.2.2">277</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p11.5.m5.2c">1,277</annotation></semantics></math> frames for testing with manually annotated joint locations for cooking activities. <span id="S5.SS1.p11.7.5" class="ltx_text ltx_font_bold">YouTube Pose</span> dataset <cite class="ltx_cite ltx_citemacro_citep">(Charles et al., <a href="#bib.bib16" title="" class="ltx_ref">2016</a>)</cite> contains <math id="S5.SS1.p11.6.m6.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S5.SS1.p11.6.m6.1a"><mn id="S5.SS1.p11.6.m6.1.1" xref="S5.SS1.p11.6.m6.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p11.6.m6.1b"><cn type="integer" id="S5.SS1.p11.6.m6.1.1.cmml" xref="S5.SS1.p11.6.m6.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p11.6.m6.1c">50</annotation></semantics></math> YouTube videos with single person in. The activities cover dancing, stand-up comedy, how-to, sports, disk jockeys, performing arts and dancing, and sign language signers. <math id="S5.SS1.p11.7.m7.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S5.SS1.p11.7.m7.1a"><mn id="S5.SS1.p11.7.m7.1.1" xref="S5.SS1.p11.7.m7.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p11.7.m7.1b"><cn type="integer" id="S5.SS1.p11.7.m7.1.1.cmml" xref="S5.SS1.p11.7.m7.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p11.7.m7.1c">100</annotation></semantics></math> frames of each video are manually annotated with joint locations of the upper body. The scenes of these datasets are relatively simple, with static views and the characters are normally in a small motion range.</p>
</div>
<div id="S5.SS1.p12" class="ltx_para">
<p id="S5.SS1.p12.6" class="ltx_p">From unlabeled MPII Human Pose <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite> video data, there are several extended versions result in dense annotations of video frames. The general approach is to extend the original labeled frame with the connected frames both forward and backward and annotate unlabeled frames in the same way as the labeled frame. <span id="S5.SS1.p12.6.1" class="ltx_text ltx_font_bold">MPII Video Pose</span> dataset <cite class="ltx_cite ltx_citemacro_citep">(Insafutdinov et al., <a href="#bib.bib57" title="" class="ltx_ref">2017</a>)</cite> provides 28 videos containing 21 frames each by selecting the challenging labeled images and unlabeled neighboring +/-10 frames from the MPII dataset. In <span id="S5.SS1.p12.6.2" class="ltx_text ltx_font_bold">Multi-Person PoseTrack</span> dataset <cite class="ltx_cite ltx_citemacro_citep">(Iqbal et al., <a href="#bib.bib61" title="" class="ltx_ref">2016</a>)</cite>, each selected labeled frame is extended with unlabeled clips ranging +/-20 frames, and each person has a unique ID. Also, additional videos of more than 41 frames are provided for longer and variable-length sequences. In total, it contains 60 videos with additional videos with more than 41 frames for longer and variable-length sequences. <span id="S5.SS1.p12.6.3" class="ltx_text ltx_font_bold">PoseTrack</span> dataset <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite> is the integrated expansion of the above two datasets and is the current largest multi-person pose estimation and tracking dataset. Each person in the video has a unique track ID with annotations of a head bounding box and 15 body joint locations. All pose annotations are labeled with VATIC <cite class="ltx_cite ltx_citemacro_citep">(Vondrick et al., <a href="#bib.bib170" title="" class="ltx_ref">2013</a>)</cite>. PoseTrack contains <math id="S5.SS1.p12.1.m1.1" class="ltx_Math" alttext="550" display="inline"><semantics id="S5.SS1.p12.1.m1.1a"><mn id="S5.SS1.p12.1.m1.1.1" xref="S5.SS1.p12.1.m1.1.1.cmml">550</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p12.1.m1.1b"><cn type="integer" id="S5.SS1.p12.1.m1.1.1.cmml" xref="S5.SS1.p12.1.m1.1.1">550</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p12.1.m1.1c">550</annotation></semantics></math> video sequences with the frames mainly ranging between <math id="S5.SS1.p12.2.m2.1" class="ltx_Math" alttext="41" display="inline"><semantics id="S5.SS1.p12.2.m2.1a"><mn id="S5.SS1.p12.2.m2.1.1" xref="S5.SS1.p12.2.m2.1.1.cmml">41</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p12.2.m2.1b"><cn type="integer" id="S5.SS1.p12.2.m2.1.1.cmml" xref="S5.SS1.p12.2.m2.1.1">41</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p12.2.m2.1c">41</annotation></semantics></math> and <math id="S5.SS1.p12.3.m3.1" class="ltx_Math" alttext="151" display="inline"><semantics id="S5.SS1.p12.3.m3.1a"><mn id="S5.SS1.p12.3.m3.1.1" xref="S5.SS1.p12.3.m3.1.1.cmml">151</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p12.3.m3.1b"><cn type="integer" id="S5.SS1.p12.3.m3.1.1.cmml" xref="S5.SS1.p12.3.m3.1.1">151</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p12.3.m3.1c">151</annotation></semantics></math> frames in a wide variety of everyday human activities and is divided into <math id="S5.SS1.p12.4.m4.1" class="ltx_Math" alttext="292" display="inline"><semantics id="S5.SS1.p12.4.m4.1a"><mn id="S5.SS1.p12.4.m4.1.1" xref="S5.SS1.p12.4.m4.1.1.cmml">292</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p12.4.m4.1b"><cn type="integer" id="S5.SS1.p12.4.m4.1.1.cmml" xref="S5.SS1.p12.4.m4.1.1">292</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p12.4.m4.1c">292</annotation></semantics></math>, <math id="S5.SS1.p12.5.m5.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S5.SS1.p12.5.m5.1a"><mn id="S5.SS1.p12.5.m5.1.1" xref="S5.SS1.p12.5.m5.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p12.5.m5.1b"><cn type="integer" id="S5.SS1.p12.5.m5.1.1.cmml" xref="S5.SS1.p12.5.m5.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p12.5.m5.1c">50</annotation></semantics></math>, and <math id="S5.SS1.p12.6.m6.1" class="ltx_Math" alttext="208" display="inline"><semantics id="S5.SS1.p12.6.m6.1a"><mn id="S5.SS1.p12.6.m6.1.1" xref="S5.SS1.p12.6.m6.1.1.cmml">208</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p12.6.m6.1b"><cn type="integer" id="S5.SS1.p12.6.m6.1.1.cmml" xref="S5.SS1.p12.6.m6.1.1">208</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p12.6.m6.1c">208</annotation></semantics></math> videos for training, validation, and testing, following original MPII split strategy.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation Metrics of 2D human pose estimation</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Different datasets have different features (e.g. various range of human body sizes, upper/full human body) and different task requirements (single/multiple pose estimation), so there are several evaluation metrics for 2D human pose estimation. The summary of different evaluation metrics which are commonly used are listed in Table <a href="#S5.T8" title="Table 8 ‣ 5.2 Evaluation Metrics of 2D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 8: </span>Summary of commonly used evaluation metrics for 2D HPE.</figcaption>
<table id="S5.T8.32" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T8.32.33.1" class="ltx_tr">
<td id="S5.T8.32.33.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T8.32.33.1.1.1" class="ltx_text" style="font-size:80%;">Metric</span></td>
<td id="S5.T8.32.33.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.32.33.1.2.1" class="ltx_text" style="font-size:80%;">Meaning</span></td>
<td id="S5.T8.32.33.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T8.32.33.1.3.1" class="ltx_text" style="font-size:80%;">Typical datasets and Description</span></td>
</tr>
<tr id="S5.T8.32.34.2" class="ltx_tr">
<td id="S5.T8.32.34.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="4"><span id="S5.T8.32.34.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Single person</span></td>
</tr>
<tr id="S5.T8.32.35.3" class="ltx_tr">
<td id="S5.T8.32.35.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T8.32.35.3.1.1" class="ltx_text" style="font-size:80%;">PCP</span></td>
<td id="S5.T8.32.35.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S5.T8.32.35.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.32.35.3.2.1.1" class="ltx_tr">
<td id="S5.T8.32.35.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.35.3.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Percentage</span></td>
</tr>
<tr id="S5.T8.32.35.3.2.1.2" class="ltx_tr">
<td id="S5.T8.32.35.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.35.3.2.1.2.1.1" class="ltx_text" style="font-size:80%;">of Correct</span></td>
</tr>
<tr id="S5.T8.32.35.3.2.1.3" class="ltx_tr">
<td id="S5.T8.32.35.3.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.35.3.2.1.3.1.1" class="ltx_text" style="font-size:80%;">Parts</span></td>
</tr>
</table>
</td>
<td id="S5.T8.32.35.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T8.32.35.3.3.1" class="ltx_text" style="font-size:80%;">LSP</span></td>
<td id="S5.T8.32.35.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.32.35.3.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.32.35.3.4.1.1" class="ltx_tr">
<td id="S5.T8.32.35.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.35.3.4.1.1.1.1" class="ltx_text" style="font-size:80%;">Percentage of correct predicted Parts which their end points fall within a threshold</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.32.36.4" class="ltx_tr">
<td id="S5.T8.32.36.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T8.32.36.4.1.1" class="ltx_text" style="font-size:80%;">PCK</span></td>
<td id="S5.T8.32.36.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S5.T8.32.36.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.32.36.4.2.1.1" class="ltx_tr">
<td id="S5.T8.32.36.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.36.4.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Percentage</span></td>
</tr>
<tr id="S5.T8.32.36.4.2.1.2" class="ltx_tr">
<td id="S5.T8.32.36.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.36.4.2.1.2.1.1" class="ltx_text" style="font-size:80%;">of Correct</span></td>
</tr>
<tr id="S5.T8.32.36.4.2.1.3" class="ltx_tr">
<td id="S5.T8.32.36.4.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.36.4.2.1.3.1.1" class="ltx_text" style="font-size:80%;">Keypoints</span></td>
</tr>
</table>
</td>
<td id="S5.T8.32.36.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.32.36.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.32.36.4.3.1.1" class="ltx_tr">
<td id="S5.T8.32.36.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.36.4.3.1.1.1.1" class="ltx_text" style="font-size:80%;">LSP</span></td>
</tr>
<tr id="S5.T8.32.36.4.3.1.2" class="ltx_tr">
<td id="S5.T8.32.36.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.36.4.3.1.2.1.1" class="ltx_text" style="font-size:80%;">MPII</span></td>
</tr>
</table>
</td>
<td id="S5.T8.32.36.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.32.36.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.32.36.4.4.1.1" class="ltx_tr">
<td id="S5.T8.32.36.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.36.4.4.1.1.1.1" class="ltx_text" style="font-size:80%;">Percentage of correct predicted joints which fall within a threshold</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.32.37.5" class="ltx_tr">
<td id="S5.T8.32.37.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="4"><span id="S5.T8.32.37.5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Multiple person</span></td>
</tr>
<tr id="S5.T8.32.38.6" class="ltx_tr">
<td id="S5.T8.32.38.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="6"><span id="S5.T8.32.38.6.1.1" class="ltx_text" style="font-size:80%;">AP</span></td>
<td id="S5.T8.32.38.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="6"><span id="S5.T8.32.38.6.2.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T8.32.38.6.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T8.32.38.6.2.1.1.1" class="ltx_tr">
<span id="S5.T8.32.38.6.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Average</span></span>
<span id="S5.T8.32.38.6.2.1.1.2" class="ltx_tr">
<span id="S5.T8.32.38.6.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Precision</span></span>
</span></span></td>
<td id="S5.T8.32.38.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.32.38.6.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.32.38.6.3.1.1" class="ltx_tr">
<td id="S5.T8.32.38.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.38.6.3.1.1.1.1" class="ltx_text" style="font-size:80%;">MPII</span></td>
</tr>
<tr id="S5.T8.32.38.6.3.1.2" class="ltx_tr">
<td id="S5.T8.32.38.6.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.38.6.3.1.2.1.1" class="ltx_text" style="font-size:80%;">PoseTrack</span></td>
</tr>
</table>
</td>
<td id="S5.T8.32.38.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.32.38.6.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.32.38.6.4.1.1" class="ltx_tr">
<td id="S5.T8.32.38.6.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T8.32.38.6.4.1.1.1.1" class="ltx_text" style="font-size:80%;">mean AP (mAP) is reported by AP for each body part after assigning predicted</span></td>
</tr>
<tr id="S5.T8.32.38.6.4.1.2" class="ltx_tr">
<td id="S5.T8.32.38.6.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T8.32.38.6.4.1.2.1.1" class="ltx_text" style="font-size:80%;">pose to the ground truth pose by PCKh score.</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.2.2" class="ltx_tr">
<td id="S5.T8.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="5"><span id="S5.T8.2.2.3.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T8.2.2.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T8.2.2.3.1.1.1" class="ltx_tr">
<span id="S5.T8.2.2.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">COCO</span></span>
</span></span></td>
<td id="S5.T8.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<math id="S5.T8.1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T8.1.1.1.m1.1a"><mo mathsize="80%" id="S5.T8.1.1.1.m1.1.1" xref="S5.T8.1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T8.1.1.1.m1.1b"><ci id="S5.T8.1.1.1.m1.1.1.cmml" xref="S5.T8.1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.1.1.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.T8.2.2.2.1" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.2.2.2.m2.1" class="ltx_Math" alttext="AP_{coco}" display="inline"><semantics id="S5.T8.2.2.2.m2.1a"><mrow id="S5.T8.2.2.2.m2.1.1" xref="S5.T8.2.2.2.m2.1.1.cmml"><mi mathsize="80%" id="S5.T8.2.2.2.m2.1.1.2" xref="S5.T8.2.2.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.T8.2.2.2.m2.1.1.1" xref="S5.T8.2.2.2.m2.1.1.1.cmml">​</mo><msub id="S5.T8.2.2.2.m2.1.1.3" xref="S5.T8.2.2.2.m2.1.1.3.cmml"><mi mathsize="80%" id="S5.T8.2.2.2.m2.1.1.3.2" xref="S5.T8.2.2.2.m2.1.1.3.2.cmml">P</mi><mrow id="S5.T8.2.2.2.m2.1.1.3.3" xref="S5.T8.2.2.2.m2.1.1.3.3.cmml"><mi mathsize="80%" id="S5.T8.2.2.2.m2.1.1.3.3.2" xref="S5.T8.2.2.2.m2.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.2.2.2.m2.1.1.3.3.1" xref="S5.T8.2.2.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.2.2.2.m2.1.1.3.3.3" xref="S5.T8.2.2.2.m2.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.T8.2.2.2.m2.1.1.3.3.1a" xref="S5.T8.2.2.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.2.2.2.m2.1.1.3.3.4" xref="S5.T8.2.2.2.m2.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.2.2.2.m2.1.1.3.3.1b" xref="S5.T8.2.2.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.2.2.2.m2.1.1.3.3.5" xref="S5.T8.2.2.2.m2.1.1.3.3.5.cmml">o</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.2.2.2.m2.1b"><apply id="S5.T8.2.2.2.m2.1.1.cmml" xref="S5.T8.2.2.2.m2.1.1"><times id="S5.T8.2.2.2.m2.1.1.1.cmml" xref="S5.T8.2.2.2.m2.1.1.1"></times><ci id="S5.T8.2.2.2.m2.1.1.2.cmml" xref="S5.T8.2.2.2.m2.1.1.2">𝐴</ci><apply id="S5.T8.2.2.2.m2.1.1.3.cmml" xref="S5.T8.2.2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.2.2.2.m2.1.1.3.1.cmml" xref="S5.T8.2.2.2.m2.1.1.3">subscript</csymbol><ci id="S5.T8.2.2.2.m2.1.1.3.2.cmml" xref="S5.T8.2.2.2.m2.1.1.3.2">𝑃</ci><apply id="S5.T8.2.2.2.m2.1.1.3.3.cmml" xref="S5.T8.2.2.2.m2.1.1.3.3"><times id="S5.T8.2.2.2.m2.1.1.3.3.1.cmml" xref="S5.T8.2.2.2.m2.1.1.3.3.1"></times><ci id="S5.T8.2.2.2.m2.1.1.3.3.2.cmml" xref="S5.T8.2.2.2.m2.1.1.3.3.2">𝑐</ci><ci id="S5.T8.2.2.2.m2.1.1.3.3.3.cmml" xref="S5.T8.2.2.2.m2.1.1.3.3.3">𝑜</ci><ci id="S5.T8.2.2.2.m2.1.1.3.3.4.cmml" xref="S5.T8.2.2.2.m2.1.1.3.3.4">𝑐</ci><ci id="S5.T8.2.2.2.m2.1.1.3.3.5.cmml" xref="S5.T8.2.2.2.m2.1.1.3.3.5">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.2.2.2.m2.1c">AP_{coco}</annotation></semantics></math><span id="S5.T8.2.2.2.2" class="ltx_text" style="font-size:80%;">: at OKS=.50:.05:.95 (primary metric)</span>
</td>
</tr>
<tr id="S5.T8.4.4" class="ltx_tr">
<td id="S5.T8.4.4.2" class="ltx_td ltx_align_left ltx_border_r">
<math id="S5.T8.3.3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T8.3.3.1.m1.1a"><mo mathsize="80%" id="S5.T8.3.3.1.m1.1.1" xref="S5.T8.3.3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T8.3.3.1.m1.1b"><ci id="S5.T8.3.3.1.m1.1.1.cmml" xref="S5.T8.3.3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.3.3.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.T8.4.4.2.1" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.4.4.2.m2.1" class="ltx_Math" alttext="AP^{OKS=.50}_{coco}" display="inline"><semantics id="S5.T8.4.4.2.m2.1a"><mrow id="S5.T8.4.4.2.m2.1.1" xref="S5.T8.4.4.2.m2.1.1.cmml"><mi mathsize="80%" id="S5.T8.4.4.2.m2.1.1.2" xref="S5.T8.4.4.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.T8.4.4.2.m2.1.1.1" xref="S5.T8.4.4.2.m2.1.1.1.cmml">​</mo><msubsup id="S5.T8.4.4.2.m2.1.1.3" xref="S5.T8.4.4.2.m2.1.1.3.cmml"><mi mathsize="80%" id="S5.T8.4.4.2.m2.1.1.3.2.2" xref="S5.T8.4.4.2.m2.1.1.3.2.2.cmml">P</mi><mrow id="S5.T8.4.4.2.m2.1.1.3.3" xref="S5.T8.4.4.2.m2.1.1.3.3.cmml"><mi mathsize="80%" id="S5.T8.4.4.2.m2.1.1.3.3.2" xref="S5.T8.4.4.2.m2.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.4.4.2.m2.1.1.3.3.1" xref="S5.T8.4.4.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.4.4.2.m2.1.1.3.3.3" xref="S5.T8.4.4.2.m2.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.T8.4.4.2.m2.1.1.3.3.1a" xref="S5.T8.4.4.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.4.4.2.m2.1.1.3.3.4" xref="S5.T8.4.4.2.m2.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.4.4.2.m2.1.1.3.3.1b" xref="S5.T8.4.4.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.4.4.2.m2.1.1.3.3.5" xref="S5.T8.4.4.2.m2.1.1.3.3.5.cmml">o</mi></mrow><mrow id="S5.T8.4.4.2.m2.1.1.3.2.3" xref="S5.T8.4.4.2.m2.1.1.3.2.3.cmml"><mrow id="S5.T8.4.4.2.m2.1.1.3.2.3.2" xref="S5.T8.4.4.2.m2.1.1.3.2.3.2.cmml"><mi mathsize="80%" id="S5.T8.4.4.2.m2.1.1.3.2.3.2.2" xref="S5.T8.4.4.2.m2.1.1.3.2.3.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S5.T8.4.4.2.m2.1.1.3.2.3.2.1" xref="S5.T8.4.4.2.m2.1.1.3.2.3.2.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.4.4.2.m2.1.1.3.2.3.2.3" xref="S5.T8.4.4.2.m2.1.1.3.2.3.2.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S5.T8.4.4.2.m2.1.1.3.2.3.2.1a" xref="S5.T8.4.4.2.m2.1.1.3.2.3.2.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.4.4.2.m2.1.1.3.2.3.2.4" xref="S5.T8.4.4.2.m2.1.1.3.2.3.2.4.cmml">S</mi></mrow><mo mathsize="80%" id="S5.T8.4.4.2.m2.1.1.3.2.3.1" xref="S5.T8.4.4.2.m2.1.1.3.2.3.1.cmml">=</mo><mn mathsize="80%" id="S5.T8.4.4.2.m2.1.1.3.2.3.3" xref="S5.T8.4.4.2.m2.1.1.3.2.3.3.cmml">.50</mn></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.4.4.2.m2.1b"><apply id="S5.T8.4.4.2.m2.1.1.cmml" xref="S5.T8.4.4.2.m2.1.1"><times id="S5.T8.4.4.2.m2.1.1.1.cmml" xref="S5.T8.4.4.2.m2.1.1.1"></times><ci id="S5.T8.4.4.2.m2.1.1.2.cmml" xref="S5.T8.4.4.2.m2.1.1.2">𝐴</ci><apply id="S5.T8.4.4.2.m2.1.1.3.cmml" xref="S5.T8.4.4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.4.4.2.m2.1.1.3.1.cmml" xref="S5.T8.4.4.2.m2.1.1.3">subscript</csymbol><apply id="S5.T8.4.4.2.m2.1.1.3.2.cmml" xref="S5.T8.4.4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.4.4.2.m2.1.1.3.2.1.cmml" xref="S5.T8.4.4.2.m2.1.1.3">superscript</csymbol><ci id="S5.T8.4.4.2.m2.1.1.3.2.2.cmml" xref="S5.T8.4.4.2.m2.1.1.3.2.2">𝑃</ci><apply id="S5.T8.4.4.2.m2.1.1.3.2.3.cmml" xref="S5.T8.4.4.2.m2.1.1.3.2.3"><eq id="S5.T8.4.4.2.m2.1.1.3.2.3.1.cmml" xref="S5.T8.4.4.2.m2.1.1.3.2.3.1"></eq><apply id="S5.T8.4.4.2.m2.1.1.3.2.3.2.cmml" xref="S5.T8.4.4.2.m2.1.1.3.2.3.2"><times id="S5.T8.4.4.2.m2.1.1.3.2.3.2.1.cmml" xref="S5.T8.4.4.2.m2.1.1.3.2.3.2.1"></times><ci id="S5.T8.4.4.2.m2.1.1.3.2.3.2.2.cmml" xref="S5.T8.4.4.2.m2.1.1.3.2.3.2.2">𝑂</ci><ci id="S5.T8.4.4.2.m2.1.1.3.2.3.2.3.cmml" xref="S5.T8.4.4.2.m2.1.1.3.2.3.2.3">𝐾</ci><ci id="S5.T8.4.4.2.m2.1.1.3.2.3.2.4.cmml" xref="S5.T8.4.4.2.m2.1.1.3.2.3.2.4">𝑆</ci></apply><cn type="float" id="S5.T8.4.4.2.m2.1.1.3.2.3.3.cmml" xref="S5.T8.4.4.2.m2.1.1.3.2.3.3">.50</cn></apply></apply><apply id="S5.T8.4.4.2.m2.1.1.3.3.cmml" xref="S5.T8.4.4.2.m2.1.1.3.3"><times id="S5.T8.4.4.2.m2.1.1.3.3.1.cmml" xref="S5.T8.4.4.2.m2.1.1.3.3.1"></times><ci id="S5.T8.4.4.2.m2.1.1.3.3.2.cmml" xref="S5.T8.4.4.2.m2.1.1.3.3.2">𝑐</ci><ci id="S5.T8.4.4.2.m2.1.1.3.3.3.cmml" xref="S5.T8.4.4.2.m2.1.1.3.3.3">𝑜</ci><ci id="S5.T8.4.4.2.m2.1.1.3.3.4.cmml" xref="S5.T8.4.4.2.m2.1.1.3.3.4">𝑐</ci><ci id="S5.T8.4.4.2.m2.1.1.3.3.5.cmml" xref="S5.T8.4.4.2.m2.1.1.3.3.5">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.4.4.2.m2.1c">AP^{OKS=.50}_{coco}</annotation></semantics></math><span id="S5.T8.4.4.2.2" class="ltx_text" style="font-size:80%;">: at OKS=.50 (loose metric)</span>
</td>
</tr>
<tr id="S5.T8.6.6" class="ltx_tr">
<td id="S5.T8.6.6.2" class="ltx_td ltx_align_left ltx_border_r">
<math id="S5.T8.5.5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T8.5.5.1.m1.1a"><mo mathsize="80%" id="S5.T8.5.5.1.m1.1.1" xref="S5.T8.5.5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T8.5.5.1.m1.1b"><ci id="S5.T8.5.5.1.m1.1.1.cmml" xref="S5.T8.5.5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.5.5.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.T8.6.6.2.1" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.6.6.2.m2.1" class="ltx_Math" alttext="AP^{OKS=.75}_{coco}" display="inline"><semantics id="S5.T8.6.6.2.m2.1a"><mrow id="S5.T8.6.6.2.m2.1.1" xref="S5.T8.6.6.2.m2.1.1.cmml"><mi mathsize="80%" id="S5.T8.6.6.2.m2.1.1.2" xref="S5.T8.6.6.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.T8.6.6.2.m2.1.1.1" xref="S5.T8.6.6.2.m2.1.1.1.cmml">​</mo><msubsup id="S5.T8.6.6.2.m2.1.1.3" xref="S5.T8.6.6.2.m2.1.1.3.cmml"><mi mathsize="80%" id="S5.T8.6.6.2.m2.1.1.3.2.2" xref="S5.T8.6.6.2.m2.1.1.3.2.2.cmml">P</mi><mrow id="S5.T8.6.6.2.m2.1.1.3.3" xref="S5.T8.6.6.2.m2.1.1.3.3.cmml"><mi mathsize="80%" id="S5.T8.6.6.2.m2.1.1.3.3.2" xref="S5.T8.6.6.2.m2.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.6.6.2.m2.1.1.3.3.1" xref="S5.T8.6.6.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.6.6.2.m2.1.1.3.3.3" xref="S5.T8.6.6.2.m2.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.T8.6.6.2.m2.1.1.3.3.1a" xref="S5.T8.6.6.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.6.6.2.m2.1.1.3.3.4" xref="S5.T8.6.6.2.m2.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.6.6.2.m2.1.1.3.3.1b" xref="S5.T8.6.6.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.6.6.2.m2.1.1.3.3.5" xref="S5.T8.6.6.2.m2.1.1.3.3.5.cmml">o</mi></mrow><mrow id="S5.T8.6.6.2.m2.1.1.3.2.3" xref="S5.T8.6.6.2.m2.1.1.3.2.3.cmml"><mrow id="S5.T8.6.6.2.m2.1.1.3.2.3.2" xref="S5.T8.6.6.2.m2.1.1.3.2.3.2.cmml"><mi mathsize="80%" id="S5.T8.6.6.2.m2.1.1.3.2.3.2.2" xref="S5.T8.6.6.2.m2.1.1.3.2.3.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S5.T8.6.6.2.m2.1.1.3.2.3.2.1" xref="S5.T8.6.6.2.m2.1.1.3.2.3.2.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.6.6.2.m2.1.1.3.2.3.2.3" xref="S5.T8.6.6.2.m2.1.1.3.2.3.2.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S5.T8.6.6.2.m2.1.1.3.2.3.2.1a" xref="S5.T8.6.6.2.m2.1.1.3.2.3.2.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.6.6.2.m2.1.1.3.2.3.2.4" xref="S5.T8.6.6.2.m2.1.1.3.2.3.2.4.cmml">S</mi></mrow><mo mathsize="80%" id="S5.T8.6.6.2.m2.1.1.3.2.3.1" xref="S5.T8.6.6.2.m2.1.1.3.2.3.1.cmml">=</mo><mn mathsize="80%" id="S5.T8.6.6.2.m2.1.1.3.2.3.3" xref="S5.T8.6.6.2.m2.1.1.3.2.3.3.cmml">.75</mn></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.6.6.2.m2.1b"><apply id="S5.T8.6.6.2.m2.1.1.cmml" xref="S5.T8.6.6.2.m2.1.1"><times id="S5.T8.6.6.2.m2.1.1.1.cmml" xref="S5.T8.6.6.2.m2.1.1.1"></times><ci id="S5.T8.6.6.2.m2.1.1.2.cmml" xref="S5.T8.6.6.2.m2.1.1.2">𝐴</ci><apply id="S5.T8.6.6.2.m2.1.1.3.cmml" xref="S5.T8.6.6.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.6.6.2.m2.1.1.3.1.cmml" xref="S5.T8.6.6.2.m2.1.1.3">subscript</csymbol><apply id="S5.T8.6.6.2.m2.1.1.3.2.cmml" xref="S5.T8.6.6.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.6.6.2.m2.1.1.3.2.1.cmml" xref="S5.T8.6.6.2.m2.1.1.3">superscript</csymbol><ci id="S5.T8.6.6.2.m2.1.1.3.2.2.cmml" xref="S5.T8.6.6.2.m2.1.1.3.2.2">𝑃</ci><apply id="S5.T8.6.6.2.m2.1.1.3.2.3.cmml" xref="S5.T8.6.6.2.m2.1.1.3.2.3"><eq id="S5.T8.6.6.2.m2.1.1.3.2.3.1.cmml" xref="S5.T8.6.6.2.m2.1.1.3.2.3.1"></eq><apply id="S5.T8.6.6.2.m2.1.1.3.2.3.2.cmml" xref="S5.T8.6.6.2.m2.1.1.3.2.3.2"><times id="S5.T8.6.6.2.m2.1.1.3.2.3.2.1.cmml" xref="S5.T8.6.6.2.m2.1.1.3.2.3.2.1"></times><ci id="S5.T8.6.6.2.m2.1.1.3.2.3.2.2.cmml" xref="S5.T8.6.6.2.m2.1.1.3.2.3.2.2">𝑂</ci><ci id="S5.T8.6.6.2.m2.1.1.3.2.3.2.3.cmml" xref="S5.T8.6.6.2.m2.1.1.3.2.3.2.3">𝐾</ci><ci id="S5.T8.6.6.2.m2.1.1.3.2.3.2.4.cmml" xref="S5.T8.6.6.2.m2.1.1.3.2.3.2.4">𝑆</ci></apply><cn type="float" id="S5.T8.6.6.2.m2.1.1.3.2.3.3.cmml" xref="S5.T8.6.6.2.m2.1.1.3.2.3.3">.75</cn></apply></apply><apply id="S5.T8.6.6.2.m2.1.1.3.3.cmml" xref="S5.T8.6.6.2.m2.1.1.3.3"><times id="S5.T8.6.6.2.m2.1.1.3.3.1.cmml" xref="S5.T8.6.6.2.m2.1.1.3.3.1"></times><ci id="S5.T8.6.6.2.m2.1.1.3.3.2.cmml" xref="S5.T8.6.6.2.m2.1.1.3.3.2">𝑐</ci><ci id="S5.T8.6.6.2.m2.1.1.3.3.3.cmml" xref="S5.T8.6.6.2.m2.1.1.3.3.3">𝑜</ci><ci id="S5.T8.6.6.2.m2.1.1.3.3.4.cmml" xref="S5.T8.6.6.2.m2.1.1.3.3.4">𝑐</ci><ci id="S5.T8.6.6.2.m2.1.1.3.3.5.cmml" xref="S5.T8.6.6.2.m2.1.1.3.3.5">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.6.6.2.m2.1c">AP^{OKS=.75}_{coco}</annotation></semantics></math><span id="S5.T8.6.6.2.2" class="ltx_text" style="font-size:80%;">: at OKS=.75 (strict metric)</span>
</td>
</tr>
<tr id="S5.T8.12.12" class="ltx_tr">
<td id="S5.T8.12.12.6" class="ltx_td ltx_align_left ltx_border_r">
<math id="S5.T8.7.7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T8.7.7.1.m1.1a"><mo mathsize="80%" id="S5.T8.7.7.1.m1.1.1" xref="S5.T8.7.7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T8.7.7.1.m1.1b"><ci id="S5.T8.7.7.1.m1.1.1.cmml" xref="S5.T8.7.7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.7.7.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.T8.12.12.6.1" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.8.8.2.m2.1" class="ltx_Math" alttext="AP^{medium}_{coco}" display="inline"><semantics id="S5.T8.8.8.2.m2.1a"><mrow id="S5.T8.8.8.2.m2.1.1" xref="S5.T8.8.8.2.m2.1.1.cmml"><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.2" xref="S5.T8.8.8.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.T8.8.8.2.m2.1.1.1" xref="S5.T8.8.8.2.m2.1.1.1.cmml">​</mo><msubsup id="S5.T8.8.8.2.m2.1.1.3" xref="S5.T8.8.8.2.m2.1.1.3.cmml"><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.3.2.2" xref="S5.T8.8.8.2.m2.1.1.3.2.2.cmml">P</mi><mrow id="S5.T8.8.8.2.m2.1.1.3.3" xref="S5.T8.8.8.2.m2.1.1.3.3.cmml"><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.3.3.2" xref="S5.T8.8.8.2.m2.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.8.8.2.m2.1.1.3.3.1" xref="S5.T8.8.8.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.3.3.3" xref="S5.T8.8.8.2.m2.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.T8.8.8.2.m2.1.1.3.3.1a" xref="S5.T8.8.8.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.3.3.4" xref="S5.T8.8.8.2.m2.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.8.8.2.m2.1.1.3.3.1b" xref="S5.T8.8.8.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.3.3.5" xref="S5.T8.8.8.2.m2.1.1.3.3.5.cmml">o</mi></mrow><mrow id="S5.T8.8.8.2.m2.1.1.3.2.3" xref="S5.T8.8.8.2.m2.1.1.3.2.3.cmml"><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.3.2.3.2" xref="S5.T8.8.8.2.m2.1.1.3.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.T8.8.8.2.m2.1.1.3.2.3.1" xref="S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.3.2.3.3" xref="S5.T8.8.8.2.m2.1.1.3.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.T8.8.8.2.m2.1.1.3.2.3.1a" xref="S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.3.2.3.4" xref="S5.T8.8.8.2.m2.1.1.3.2.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S5.T8.8.8.2.m2.1.1.3.2.3.1b" xref="S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.3.2.3.5" xref="S5.T8.8.8.2.m2.1.1.3.2.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.T8.8.8.2.m2.1.1.3.2.3.1c" xref="S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.3.2.3.6" xref="S5.T8.8.8.2.m2.1.1.3.2.3.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.T8.8.8.2.m2.1.1.3.2.3.1d" xref="S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.8.8.2.m2.1.1.3.2.3.7" xref="S5.T8.8.8.2.m2.1.1.3.2.3.7.cmml">m</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.8.8.2.m2.1b"><apply id="S5.T8.8.8.2.m2.1.1.cmml" xref="S5.T8.8.8.2.m2.1.1"><times id="S5.T8.8.8.2.m2.1.1.1.cmml" xref="S5.T8.8.8.2.m2.1.1.1"></times><ci id="S5.T8.8.8.2.m2.1.1.2.cmml" xref="S5.T8.8.8.2.m2.1.1.2">𝐴</ci><apply id="S5.T8.8.8.2.m2.1.1.3.cmml" xref="S5.T8.8.8.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.8.8.2.m2.1.1.3.1.cmml" xref="S5.T8.8.8.2.m2.1.1.3">subscript</csymbol><apply id="S5.T8.8.8.2.m2.1.1.3.2.cmml" xref="S5.T8.8.8.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.8.8.2.m2.1.1.3.2.1.cmml" xref="S5.T8.8.8.2.m2.1.1.3">superscript</csymbol><ci id="S5.T8.8.8.2.m2.1.1.3.2.2.cmml" xref="S5.T8.8.8.2.m2.1.1.3.2.2">𝑃</ci><apply id="S5.T8.8.8.2.m2.1.1.3.2.3.cmml" xref="S5.T8.8.8.2.m2.1.1.3.2.3"><times id="S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml" xref="S5.T8.8.8.2.m2.1.1.3.2.3.1"></times><ci id="S5.T8.8.8.2.m2.1.1.3.2.3.2.cmml" xref="S5.T8.8.8.2.m2.1.1.3.2.3.2">𝑚</ci><ci id="S5.T8.8.8.2.m2.1.1.3.2.3.3.cmml" xref="S5.T8.8.8.2.m2.1.1.3.2.3.3">𝑒</ci><ci id="S5.T8.8.8.2.m2.1.1.3.2.3.4.cmml" xref="S5.T8.8.8.2.m2.1.1.3.2.3.4">𝑑</ci><ci id="S5.T8.8.8.2.m2.1.1.3.2.3.5.cmml" xref="S5.T8.8.8.2.m2.1.1.3.2.3.5">𝑖</ci><ci id="S5.T8.8.8.2.m2.1.1.3.2.3.6.cmml" xref="S5.T8.8.8.2.m2.1.1.3.2.3.6">𝑢</ci><ci id="S5.T8.8.8.2.m2.1.1.3.2.3.7.cmml" xref="S5.T8.8.8.2.m2.1.1.3.2.3.7">𝑚</ci></apply></apply><apply id="S5.T8.8.8.2.m2.1.1.3.3.cmml" xref="S5.T8.8.8.2.m2.1.1.3.3"><times id="S5.T8.8.8.2.m2.1.1.3.3.1.cmml" xref="S5.T8.8.8.2.m2.1.1.3.3.1"></times><ci id="S5.T8.8.8.2.m2.1.1.3.3.2.cmml" xref="S5.T8.8.8.2.m2.1.1.3.3.2">𝑐</ci><ci id="S5.T8.8.8.2.m2.1.1.3.3.3.cmml" xref="S5.T8.8.8.2.m2.1.1.3.3.3">𝑜</ci><ci id="S5.T8.8.8.2.m2.1.1.3.3.4.cmml" xref="S5.T8.8.8.2.m2.1.1.3.3.4">𝑐</ci><ci id="S5.T8.8.8.2.m2.1.1.3.3.5.cmml" xref="S5.T8.8.8.2.m2.1.1.3.3.5">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.8.8.2.m2.1c">AP^{medium}_{coco}</annotation></semantics></math><span id="S5.T8.12.12.6.2" class="ltx_text" style="font-size:80%;">: for medium objects: </span><math id="S5.T8.9.9.3.m3.1" class="ltx_Math" alttext="32^{2}" display="inline"><semantics id="S5.T8.9.9.3.m3.1a"><msup id="S5.T8.9.9.3.m3.1.1" xref="S5.T8.9.9.3.m3.1.1.cmml"><mn mathsize="80%" id="S5.T8.9.9.3.m3.1.1.2" xref="S5.T8.9.9.3.m3.1.1.2.cmml">32</mn><mn mathsize="80%" id="S5.T8.9.9.3.m3.1.1.3" xref="S5.T8.9.9.3.m3.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T8.9.9.3.m3.1b"><apply id="S5.T8.9.9.3.m3.1.1.cmml" xref="S5.T8.9.9.3.m3.1.1"><csymbol cd="ambiguous" id="S5.T8.9.9.3.m3.1.1.1.cmml" xref="S5.T8.9.9.3.m3.1.1">superscript</csymbol><cn type="integer" id="S5.T8.9.9.3.m3.1.1.2.cmml" xref="S5.T8.9.9.3.m3.1.1.2">32</cn><cn type="integer" id="S5.T8.9.9.3.m3.1.1.3.cmml" xref="S5.T8.9.9.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.9.9.3.m3.1c">32^{2}</annotation></semantics></math><span id="S5.T8.12.12.6.3" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.10.10.4.m4.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.T8.10.10.4.m4.1a"><mo mathsize="80%" id="S5.T8.10.10.4.m4.1.1" xref="S5.T8.10.10.4.m4.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.T8.10.10.4.m4.1b"><lt id="S5.T8.10.10.4.m4.1.1.cmml" xref="S5.T8.10.10.4.m4.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.10.10.4.m4.1c">&lt;</annotation></semantics></math><span id="S5.T8.12.12.6.4" class="ltx_text" style="font-size:80%;"> area </span><math id="S5.T8.11.11.5.m5.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.T8.11.11.5.m5.1a"><mo mathsize="80%" id="S5.T8.11.11.5.m5.1.1" xref="S5.T8.11.11.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.T8.11.11.5.m5.1b"><lt id="S5.T8.11.11.5.m5.1.1.cmml" xref="S5.T8.11.11.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.11.11.5.m5.1c">&lt;</annotation></semantics></math><span id="S5.T8.12.12.6.5" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.12.12.6.m6.1" class="ltx_Math" alttext="96^{2}" display="inline"><semantics id="S5.T8.12.12.6.m6.1a"><msup id="S5.T8.12.12.6.m6.1.1" xref="S5.T8.12.12.6.m6.1.1.cmml"><mn mathsize="80%" id="S5.T8.12.12.6.m6.1.1.2" xref="S5.T8.12.12.6.m6.1.1.2.cmml">96</mn><mn mathsize="80%" id="S5.T8.12.12.6.m6.1.1.3" xref="S5.T8.12.12.6.m6.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T8.12.12.6.m6.1b"><apply id="S5.T8.12.12.6.m6.1.1.cmml" xref="S5.T8.12.12.6.m6.1.1"><csymbol cd="ambiguous" id="S5.T8.12.12.6.m6.1.1.1.cmml" xref="S5.T8.12.12.6.m6.1.1">superscript</csymbol><cn type="integer" id="S5.T8.12.12.6.m6.1.1.2.cmml" xref="S5.T8.12.12.6.m6.1.1.2">96</cn><cn type="integer" id="S5.T8.12.12.6.m6.1.1.3.cmml" xref="S5.T8.12.12.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.12.12.6.m6.1c">96^{2}</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T8.16.16" class="ltx_tr">
<td id="S5.T8.16.16.4" class="ltx_td ltx_align_left ltx_border_r">
<math id="S5.T8.13.13.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T8.13.13.1.m1.1a"><mo mathsize="80%" id="S5.T8.13.13.1.m1.1.1" xref="S5.T8.13.13.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T8.13.13.1.m1.1b"><ci id="S5.T8.13.13.1.m1.1.1.cmml" xref="S5.T8.13.13.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.13.13.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.T8.16.16.4.1" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.14.14.2.m2.1" class="ltx_Math" alttext="AP^{large}_{coco}" display="inline"><semantics id="S5.T8.14.14.2.m2.1a"><mrow id="S5.T8.14.14.2.m2.1.1" xref="S5.T8.14.14.2.m2.1.1.cmml"><mi mathsize="80%" id="S5.T8.14.14.2.m2.1.1.2" xref="S5.T8.14.14.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.T8.14.14.2.m2.1.1.1" xref="S5.T8.14.14.2.m2.1.1.1.cmml">​</mo><msubsup id="S5.T8.14.14.2.m2.1.1.3" xref="S5.T8.14.14.2.m2.1.1.3.cmml"><mi mathsize="80%" id="S5.T8.14.14.2.m2.1.1.3.2.2" xref="S5.T8.14.14.2.m2.1.1.3.2.2.cmml">P</mi><mrow id="S5.T8.14.14.2.m2.1.1.3.3" xref="S5.T8.14.14.2.m2.1.1.3.3.cmml"><mi mathsize="80%" id="S5.T8.14.14.2.m2.1.1.3.3.2" xref="S5.T8.14.14.2.m2.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.14.14.2.m2.1.1.3.3.1" xref="S5.T8.14.14.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.14.14.2.m2.1.1.3.3.3" xref="S5.T8.14.14.2.m2.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.T8.14.14.2.m2.1.1.3.3.1a" xref="S5.T8.14.14.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.14.14.2.m2.1.1.3.3.4" xref="S5.T8.14.14.2.m2.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.14.14.2.m2.1.1.3.3.1b" xref="S5.T8.14.14.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.14.14.2.m2.1.1.3.3.5" xref="S5.T8.14.14.2.m2.1.1.3.3.5.cmml">o</mi></mrow><mrow id="S5.T8.14.14.2.m2.1.1.3.2.3" xref="S5.T8.14.14.2.m2.1.1.3.2.3.cmml"><mi mathsize="80%" id="S5.T8.14.14.2.m2.1.1.3.2.3.2" xref="S5.T8.14.14.2.m2.1.1.3.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.T8.14.14.2.m2.1.1.3.2.3.1" xref="S5.T8.14.14.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.14.14.2.m2.1.1.3.2.3.3" xref="S5.T8.14.14.2.m2.1.1.3.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.T8.14.14.2.m2.1.1.3.2.3.1a" xref="S5.T8.14.14.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.14.14.2.m2.1.1.3.2.3.4" xref="S5.T8.14.14.2.m2.1.1.3.2.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.T8.14.14.2.m2.1.1.3.2.3.1b" xref="S5.T8.14.14.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.14.14.2.m2.1.1.3.2.3.5" xref="S5.T8.14.14.2.m2.1.1.3.2.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S5.T8.14.14.2.m2.1.1.3.2.3.1c" xref="S5.T8.14.14.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.14.14.2.m2.1.1.3.2.3.6" xref="S5.T8.14.14.2.m2.1.1.3.2.3.6.cmml">e</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.14.14.2.m2.1b"><apply id="S5.T8.14.14.2.m2.1.1.cmml" xref="S5.T8.14.14.2.m2.1.1"><times id="S5.T8.14.14.2.m2.1.1.1.cmml" xref="S5.T8.14.14.2.m2.1.1.1"></times><ci id="S5.T8.14.14.2.m2.1.1.2.cmml" xref="S5.T8.14.14.2.m2.1.1.2">𝐴</ci><apply id="S5.T8.14.14.2.m2.1.1.3.cmml" xref="S5.T8.14.14.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.14.14.2.m2.1.1.3.1.cmml" xref="S5.T8.14.14.2.m2.1.1.3">subscript</csymbol><apply id="S5.T8.14.14.2.m2.1.1.3.2.cmml" xref="S5.T8.14.14.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.14.14.2.m2.1.1.3.2.1.cmml" xref="S5.T8.14.14.2.m2.1.1.3">superscript</csymbol><ci id="S5.T8.14.14.2.m2.1.1.3.2.2.cmml" xref="S5.T8.14.14.2.m2.1.1.3.2.2">𝑃</ci><apply id="S5.T8.14.14.2.m2.1.1.3.2.3.cmml" xref="S5.T8.14.14.2.m2.1.1.3.2.3"><times id="S5.T8.14.14.2.m2.1.1.3.2.3.1.cmml" xref="S5.T8.14.14.2.m2.1.1.3.2.3.1"></times><ci id="S5.T8.14.14.2.m2.1.1.3.2.3.2.cmml" xref="S5.T8.14.14.2.m2.1.1.3.2.3.2">𝑙</ci><ci id="S5.T8.14.14.2.m2.1.1.3.2.3.3.cmml" xref="S5.T8.14.14.2.m2.1.1.3.2.3.3">𝑎</ci><ci id="S5.T8.14.14.2.m2.1.1.3.2.3.4.cmml" xref="S5.T8.14.14.2.m2.1.1.3.2.3.4">𝑟</ci><ci id="S5.T8.14.14.2.m2.1.1.3.2.3.5.cmml" xref="S5.T8.14.14.2.m2.1.1.3.2.3.5">𝑔</ci><ci id="S5.T8.14.14.2.m2.1.1.3.2.3.6.cmml" xref="S5.T8.14.14.2.m2.1.1.3.2.3.6">𝑒</ci></apply></apply><apply id="S5.T8.14.14.2.m2.1.1.3.3.cmml" xref="S5.T8.14.14.2.m2.1.1.3.3"><times id="S5.T8.14.14.2.m2.1.1.3.3.1.cmml" xref="S5.T8.14.14.2.m2.1.1.3.3.1"></times><ci id="S5.T8.14.14.2.m2.1.1.3.3.2.cmml" xref="S5.T8.14.14.2.m2.1.1.3.3.2">𝑐</ci><ci id="S5.T8.14.14.2.m2.1.1.3.3.3.cmml" xref="S5.T8.14.14.2.m2.1.1.3.3.3">𝑜</ci><ci id="S5.T8.14.14.2.m2.1.1.3.3.4.cmml" xref="S5.T8.14.14.2.m2.1.1.3.3.4">𝑐</ci><ci id="S5.T8.14.14.2.m2.1.1.3.3.5.cmml" xref="S5.T8.14.14.2.m2.1.1.3.3.5">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.14.14.2.m2.1c">AP^{large}_{coco}</annotation></semantics></math><span id="S5.T8.16.16.4.2" class="ltx_text" style="font-size:80%;">: for large objects: area </span><math id="S5.T8.15.15.3.m3.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.T8.15.15.3.m3.1a"><mo mathsize="80%" id="S5.T8.15.15.3.m3.1.1" xref="S5.T8.15.15.3.m3.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.T8.15.15.3.m3.1b"><gt id="S5.T8.15.15.3.m3.1.1.cmml" xref="S5.T8.15.15.3.m3.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.15.15.3.m3.1c">&gt;</annotation></semantics></math><span id="S5.T8.16.16.4.3" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.16.16.4.m4.1" class="ltx_Math" alttext="96^{2}" display="inline"><semantics id="S5.T8.16.16.4.m4.1a"><msup id="S5.T8.16.16.4.m4.1.1" xref="S5.T8.16.16.4.m4.1.1.cmml"><mn mathsize="80%" id="S5.T8.16.16.4.m4.1.1.2" xref="S5.T8.16.16.4.m4.1.1.2.cmml">96</mn><mn mathsize="80%" id="S5.T8.16.16.4.m4.1.1.3" xref="S5.T8.16.16.4.m4.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T8.16.16.4.m4.1b"><apply id="S5.T8.16.16.4.m4.1.1.cmml" xref="S5.T8.16.16.4.m4.1.1"><csymbol cd="ambiguous" id="S5.T8.16.16.4.m4.1.1.1.cmml" xref="S5.T8.16.16.4.m4.1.1">superscript</csymbol><cn type="integer" id="S5.T8.16.16.4.m4.1.1.2.cmml" xref="S5.T8.16.16.4.m4.1.1.2">96</cn><cn type="integer" id="S5.T8.16.16.4.m4.1.1.3.cmml" xref="S5.T8.16.16.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.16.16.4.m4.1c">96^{2}</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T8.18.18" class="ltx_tr">
<td id="S5.T8.18.18.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="5"><span id="S5.T8.18.18.3.1" class="ltx_text" style="font-size:80%;">AR</span></td>
<td id="S5.T8.18.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="5"><span id="S5.T8.18.18.4.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T8.18.18.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T8.18.18.4.1.1.1" class="ltx_tr">
<span id="S5.T8.18.18.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Average</span></span>
<span id="S5.T8.18.18.4.1.1.2" class="ltx_tr">
<span id="S5.T8.18.18.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Recall</span></span>
</span></span></td>
<td id="S5.T8.18.18.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="5"><span id="S5.T8.18.18.5.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T8.18.18.5.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T8.18.18.5.1.1.1" class="ltx_tr">
<span id="S5.T8.18.18.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">COCO</span></span>
</span></span></td>
<td id="S5.T8.18.18.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<math id="S5.T8.17.17.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T8.17.17.1.m1.1a"><mo mathsize="80%" id="S5.T8.17.17.1.m1.1.1" xref="S5.T8.17.17.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T8.17.17.1.m1.1b"><ci id="S5.T8.17.17.1.m1.1.1.cmml" xref="S5.T8.17.17.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.17.17.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.T8.18.18.2.1" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.18.18.2.m2.1" class="ltx_Math" alttext="AR_{coco}" display="inline"><semantics id="S5.T8.18.18.2.m2.1a"><mrow id="S5.T8.18.18.2.m2.1.1" xref="S5.T8.18.18.2.m2.1.1.cmml"><mi mathsize="80%" id="S5.T8.18.18.2.m2.1.1.2" xref="S5.T8.18.18.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.T8.18.18.2.m2.1.1.1" xref="S5.T8.18.18.2.m2.1.1.1.cmml">​</mo><msub id="S5.T8.18.18.2.m2.1.1.3" xref="S5.T8.18.18.2.m2.1.1.3.cmml"><mi mathsize="80%" id="S5.T8.18.18.2.m2.1.1.3.2" xref="S5.T8.18.18.2.m2.1.1.3.2.cmml">R</mi><mrow id="S5.T8.18.18.2.m2.1.1.3.3" xref="S5.T8.18.18.2.m2.1.1.3.3.cmml"><mi mathsize="80%" id="S5.T8.18.18.2.m2.1.1.3.3.2" xref="S5.T8.18.18.2.m2.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.18.18.2.m2.1.1.3.3.1" xref="S5.T8.18.18.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.18.18.2.m2.1.1.3.3.3" xref="S5.T8.18.18.2.m2.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.T8.18.18.2.m2.1.1.3.3.1a" xref="S5.T8.18.18.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.18.18.2.m2.1.1.3.3.4" xref="S5.T8.18.18.2.m2.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.18.18.2.m2.1.1.3.3.1b" xref="S5.T8.18.18.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.18.18.2.m2.1.1.3.3.5" xref="S5.T8.18.18.2.m2.1.1.3.3.5.cmml">o</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.18.18.2.m2.1b"><apply id="S5.T8.18.18.2.m2.1.1.cmml" xref="S5.T8.18.18.2.m2.1.1"><times id="S5.T8.18.18.2.m2.1.1.1.cmml" xref="S5.T8.18.18.2.m2.1.1.1"></times><ci id="S5.T8.18.18.2.m2.1.1.2.cmml" xref="S5.T8.18.18.2.m2.1.1.2">𝐴</ci><apply id="S5.T8.18.18.2.m2.1.1.3.cmml" xref="S5.T8.18.18.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.18.18.2.m2.1.1.3.1.cmml" xref="S5.T8.18.18.2.m2.1.1.3">subscript</csymbol><ci id="S5.T8.18.18.2.m2.1.1.3.2.cmml" xref="S5.T8.18.18.2.m2.1.1.3.2">𝑅</ci><apply id="S5.T8.18.18.2.m2.1.1.3.3.cmml" xref="S5.T8.18.18.2.m2.1.1.3.3"><times id="S5.T8.18.18.2.m2.1.1.3.3.1.cmml" xref="S5.T8.18.18.2.m2.1.1.3.3.1"></times><ci id="S5.T8.18.18.2.m2.1.1.3.3.2.cmml" xref="S5.T8.18.18.2.m2.1.1.3.3.2">𝑐</ci><ci id="S5.T8.18.18.2.m2.1.1.3.3.3.cmml" xref="S5.T8.18.18.2.m2.1.1.3.3.3">𝑜</ci><ci id="S5.T8.18.18.2.m2.1.1.3.3.4.cmml" xref="S5.T8.18.18.2.m2.1.1.3.3.4">𝑐</ci><ci id="S5.T8.18.18.2.m2.1.1.3.3.5.cmml" xref="S5.T8.18.18.2.m2.1.1.3.3.5">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.18.18.2.m2.1c">AR_{coco}</annotation></semantics></math><span id="S5.T8.18.18.2.2" class="ltx_text" style="font-size:80%;">: at OKS=.50:.05:.95</span>
</td>
</tr>
<tr id="S5.T8.20.20" class="ltx_tr">
<td id="S5.T8.20.20.2" class="ltx_td ltx_align_left ltx_border_r">
<math id="S5.T8.19.19.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T8.19.19.1.m1.1a"><mo mathsize="80%" id="S5.T8.19.19.1.m1.1.1" xref="S5.T8.19.19.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T8.19.19.1.m1.1b"><ci id="S5.T8.19.19.1.m1.1.1.cmml" xref="S5.T8.19.19.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.19.19.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.T8.20.20.2.1" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.20.20.2.m2.1" class="ltx_Math" alttext="AR^{OKS=.50}_{coco}" display="inline"><semantics id="S5.T8.20.20.2.m2.1a"><mrow id="S5.T8.20.20.2.m2.1.1" xref="S5.T8.20.20.2.m2.1.1.cmml"><mi mathsize="80%" id="S5.T8.20.20.2.m2.1.1.2" xref="S5.T8.20.20.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.T8.20.20.2.m2.1.1.1" xref="S5.T8.20.20.2.m2.1.1.1.cmml">​</mo><msubsup id="S5.T8.20.20.2.m2.1.1.3" xref="S5.T8.20.20.2.m2.1.1.3.cmml"><mi mathsize="80%" id="S5.T8.20.20.2.m2.1.1.3.2.2" xref="S5.T8.20.20.2.m2.1.1.3.2.2.cmml">R</mi><mrow id="S5.T8.20.20.2.m2.1.1.3.3" xref="S5.T8.20.20.2.m2.1.1.3.3.cmml"><mi mathsize="80%" id="S5.T8.20.20.2.m2.1.1.3.3.2" xref="S5.T8.20.20.2.m2.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.20.20.2.m2.1.1.3.3.1" xref="S5.T8.20.20.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.20.20.2.m2.1.1.3.3.3" xref="S5.T8.20.20.2.m2.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.T8.20.20.2.m2.1.1.3.3.1a" xref="S5.T8.20.20.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.20.20.2.m2.1.1.3.3.4" xref="S5.T8.20.20.2.m2.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.20.20.2.m2.1.1.3.3.1b" xref="S5.T8.20.20.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.20.20.2.m2.1.1.3.3.5" xref="S5.T8.20.20.2.m2.1.1.3.3.5.cmml">o</mi></mrow><mrow id="S5.T8.20.20.2.m2.1.1.3.2.3" xref="S5.T8.20.20.2.m2.1.1.3.2.3.cmml"><mrow id="S5.T8.20.20.2.m2.1.1.3.2.3.2" xref="S5.T8.20.20.2.m2.1.1.3.2.3.2.cmml"><mi mathsize="80%" id="S5.T8.20.20.2.m2.1.1.3.2.3.2.2" xref="S5.T8.20.20.2.m2.1.1.3.2.3.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S5.T8.20.20.2.m2.1.1.3.2.3.2.1" xref="S5.T8.20.20.2.m2.1.1.3.2.3.2.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.20.20.2.m2.1.1.3.2.3.2.3" xref="S5.T8.20.20.2.m2.1.1.3.2.3.2.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S5.T8.20.20.2.m2.1.1.3.2.3.2.1a" xref="S5.T8.20.20.2.m2.1.1.3.2.3.2.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.20.20.2.m2.1.1.3.2.3.2.4" xref="S5.T8.20.20.2.m2.1.1.3.2.3.2.4.cmml">S</mi></mrow><mo mathsize="80%" id="S5.T8.20.20.2.m2.1.1.3.2.3.1" xref="S5.T8.20.20.2.m2.1.1.3.2.3.1.cmml">=</mo><mn mathsize="80%" id="S5.T8.20.20.2.m2.1.1.3.2.3.3" xref="S5.T8.20.20.2.m2.1.1.3.2.3.3.cmml">.50</mn></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.20.20.2.m2.1b"><apply id="S5.T8.20.20.2.m2.1.1.cmml" xref="S5.T8.20.20.2.m2.1.1"><times id="S5.T8.20.20.2.m2.1.1.1.cmml" xref="S5.T8.20.20.2.m2.1.1.1"></times><ci id="S5.T8.20.20.2.m2.1.1.2.cmml" xref="S5.T8.20.20.2.m2.1.1.2">𝐴</ci><apply id="S5.T8.20.20.2.m2.1.1.3.cmml" xref="S5.T8.20.20.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.20.20.2.m2.1.1.3.1.cmml" xref="S5.T8.20.20.2.m2.1.1.3">subscript</csymbol><apply id="S5.T8.20.20.2.m2.1.1.3.2.cmml" xref="S5.T8.20.20.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.20.20.2.m2.1.1.3.2.1.cmml" xref="S5.T8.20.20.2.m2.1.1.3">superscript</csymbol><ci id="S5.T8.20.20.2.m2.1.1.3.2.2.cmml" xref="S5.T8.20.20.2.m2.1.1.3.2.2">𝑅</ci><apply id="S5.T8.20.20.2.m2.1.1.3.2.3.cmml" xref="S5.T8.20.20.2.m2.1.1.3.2.3"><eq id="S5.T8.20.20.2.m2.1.1.3.2.3.1.cmml" xref="S5.T8.20.20.2.m2.1.1.3.2.3.1"></eq><apply id="S5.T8.20.20.2.m2.1.1.3.2.3.2.cmml" xref="S5.T8.20.20.2.m2.1.1.3.2.3.2"><times id="S5.T8.20.20.2.m2.1.1.3.2.3.2.1.cmml" xref="S5.T8.20.20.2.m2.1.1.3.2.3.2.1"></times><ci id="S5.T8.20.20.2.m2.1.1.3.2.3.2.2.cmml" xref="S5.T8.20.20.2.m2.1.1.3.2.3.2.2">𝑂</ci><ci id="S5.T8.20.20.2.m2.1.1.3.2.3.2.3.cmml" xref="S5.T8.20.20.2.m2.1.1.3.2.3.2.3">𝐾</ci><ci id="S5.T8.20.20.2.m2.1.1.3.2.3.2.4.cmml" xref="S5.T8.20.20.2.m2.1.1.3.2.3.2.4">𝑆</ci></apply><cn type="float" id="S5.T8.20.20.2.m2.1.1.3.2.3.3.cmml" xref="S5.T8.20.20.2.m2.1.1.3.2.3.3">.50</cn></apply></apply><apply id="S5.T8.20.20.2.m2.1.1.3.3.cmml" xref="S5.T8.20.20.2.m2.1.1.3.3"><times id="S5.T8.20.20.2.m2.1.1.3.3.1.cmml" xref="S5.T8.20.20.2.m2.1.1.3.3.1"></times><ci id="S5.T8.20.20.2.m2.1.1.3.3.2.cmml" xref="S5.T8.20.20.2.m2.1.1.3.3.2">𝑐</ci><ci id="S5.T8.20.20.2.m2.1.1.3.3.3.cmml" xref="S5.T8.20.20.2.m2.1.1.3.3.3">𝑜</ci><ci id="S5.T8.20.20.2.m2.1.1.3.3.4.cmml" xref="S5.T8.20.20.2.m2.1.1.3.3.4">𝑐</ci><ci id="S5.T8.20.20.2.m2.1.1.3.3.5.cmml" xref="S5.T8.20.20.2.m2.1.1.3.3.5">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.20.20.2.m2.1c">AR^{OKS=.50}_{coco}</annotation></semantics></math><span id="S5.T8.20.20.2.2" class="ltx_text" style="font-size:80%;">: at OKS=.50</span>
</td>
</tr>
<tr id="S5.T8.22.22" class="ltx_tr">
<td id="S5.T8.22.22.2" class="ltx_td ltx_align_left ltx_border_r">
<math id="S5.T8.21.21.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T8.21.21.1.m1.1a"><mo mathsize="80%" id="S5.T8.21.21.1.m1.1.1" xref="S5.T8.21.21.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T8.21.21.1.m1.1b"><ci id="S5.T8.21.21.1.m1.1.1.cmml" xref="S5.T8.21.21.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.21.21.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.T8.22.22.2.1" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.22.22.2.m2.1" class="ltx_Math" alttext="AR^{OKS=.75}_{coco}" display="inline"><semantics id="S5.T8.22.22.2.m2.1a"><mrow id="S5.T8.22.22.2.m2.1.1" xref="S5.T8.22.22.2.m2.1.1.cmml"><mi mathsize="80%" id="S5.T8.22.22.2.m2.1.1.2" xref="S5.T8.22.22.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.T8.22.22.2.m2.1.1.1" xref="S5.T8.22.22.2.m2.1.1.1.cmml">​</mo><msubsup id="S5.T8.22.22.2.m2.1.1.3" xref="S5.T8.22.22.2.m2.1.1.3.cmml"><mi mathsize="80%" id="S5.T8.22.22.2.m2.1.1.3.2.2" xref="S5.T8.22.22.2.m2.1.1.3.2.2.cmml">R</mi><mrow id="S5.T8.22.22.2.m2.1.1.3.3" xref="S5.T8.22.22.2.m2.1.1.3.3.cmml"><mi mathsize="80%" id="S5.T8.22.22.2.m2.1.1.3.3.2" xref="S5.T8.22.22.2.m2.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.22.22.2.m2.1.1.3.3.1" xref="S5.T8.22.22.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.22.22.2.m2.1.1.3.3.3" xref="S5.T8.22.22.2.m2.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.T8.22.22.2.m2.1.1.3.3.1a" xref="S5.T8.22.22.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.22.22.2.m2.1.1.3.3.4" xref="S5.T8.22.22.2.m2.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.22.22.2.m2.1.1.3.3.1b" xref="S5.T8.22.22.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.22.22.2.m2.1.1.3.3.5" xref="S5.T8.22.22.2.m2.1.1.3.3.5.cmml">o</mi></mrow><mrow id="S5.T8.22.22.2.m2.1.1.3.2.3" xref="S5.T8.22.22.2.m2.1.1.3.2.3.cmml"><mrow id="S5.T8.22.22.2.m2.1.1.3.2.3.2" xref="S5.T8.22.22.2.m2.1.1.3.2.3.2.cmml"><mi mathsize="80%" id="S5.T8.22.22.2.m2.1.1.3.2.3.2.2" xref="S5.T8.22.22.2.m2.1.1.3.2.3.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S5.T8.22.22.2.m2.1.1.3.2.3.2.1" xref="S5.T8.22.22.2.m2.1.1.3.2.3.2.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.22.22.2.m2.1.1.3.2.3.2.3" xref="S5.T8.22.22.2.m2.1.1.3.2.3.2.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S5.T8.22.22.2.m2.1.1.3.2.3.2.1a" xref="S5.T8.22.22.2.m2.1.1.3.2.3.2.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.22.22.2.m2.1.1.3.2.3.2.4" xref="S5.T8.22.22.2.m2.1.1.3.2.3.2.4.cmml">S</mi></mrow><mo mathsize="80%" id="S5.T8.22.22.2.m2.1.1.3.2.3.1" xref="S5.T8.22.22.2.m2.1.1.3.2.3.1.cmml">=</mo><mn mathsize="80%" id="S5.T8.22.22.2.m2.1.1.3.2.3.3" xref="S5.T8.22.22.2.m2.1.1.3.2.3.3.cmml">.75</mn></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.22.22.2.m2.1b"><apply id="S5.T8.22.22.2.m2.1.1.cmml" xref="S5.T8.22.22.2.m2.1.1"><times id="S5.T8.22.22.2.m2.1.1.1.cmml" xref="S5.T8.22.22.2.m2.1.1.1"></times><ci id="S5.T8.22.22.2.m2.1.1.2.cmml" xref="S5.T8.22.22.2.m2.1.1.2">𝐴</ci><apply id="S5.T8.22.22.2.m2.1.1.3.cmml" xref="S5.T8.22.22.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.22.22.2.m2.1.1.3.1.cmml" xref="S5.T8.22.22.2.m2.1.1.3">subscript</csymbol><apply id="S5.T8.22.22.2.m2.1.1.3.2.cmml" xref="S5.T8.22.22.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.22.22.2.m2.1.1.3.2.1.cmml" xref="S5.T8.22.22.2.m2.1.1.3">superscript</csymbol><ci id="S5.T8.22.22.2.m2.1.1.3.2.2.cmml" xref="S5.T8.22.22.2.m2.1.1.3.2.2">𝑅</ci><apply id="S5.T8.22.22.2.m2.1.1.3.2.3.cmml" xref="S5.T8.22.22.2.m2.1.1.3.2.3"><eq id="S5.T8.22.22.2.m2.1.1.3.2.3.1.cmml" xref="S5.T8.22.22.2.m2.1.1.3.2.3.1"></eq><apply id="S5.T8.22.22.2.m2.1.1.3.2.3.2.cmml" xref="S5.T8.22.22.2.m2.1.1.3.2.3.2"><times id="S5.T8.22.22.2.m2.1.1.3.2.3.2.1.cmml" xref="S5.T8.22.22.2.m2.1.1.3.2.3.2.1"></times><ci id="S5.T8.22.22.2.m2.1.1.3.2.3.2.2.cmml" xref="S5.T8.22.22.2.m2.1.1.3.2.3.2.2">𝑂</ci><ci id="S5.T8.22.22.2.m2.1.1.3.2.3.2.3.cmml" xref="S5.T8.22.22.2.m2.1.1.3.2.3.2.3">𝐾</ci><ci id="S5.T8.22.22.2.m2.1.1.3.2.3.2.4.cmml" xref="S5.T8.22.22.2.m2.1.1.3.2.3.2.4">𝑆</ci></apply><cn type="float" id="S5.T8.22.22.2.m2.1.1.3.2.3.3.cmml" xref="S5.T8.22.22.2.m2.1.1.3.2.3.3">.75</cn></apply></apply><apply id="S5.T8.22.22.2.m2.1.1.3.3.cmml" xref="S5.T8.22.22.2.m2.1.1.3.3"><times id="S5.T8.22.22.2.m2.1.1.3.3.1.cmml" xref="S5.T8.22.22.2.m2.1.1.3.3.1"></times><ci id="S5.T8.22.22.2.m2.1.1.3.3.2.cmml" xref="S5.T8.22.22.2.m2.1.1.3.3.2">𝑐</ci><ci id="S5.T8.22.22.2.m2.1.1.3.3.3.cmml" xref="S5.T8.22.22.2.m2.1.1.3.3.3">𝑜</ci><ci id="S5.T8.22.22.2.m2.1.1.3.3.4.cmml" xref="S5.T8.22.22.2.m2.1.1.3.3.4">𝑐</ci><ci id="S5.T8.22.22.2.m2.1.1.3.3.5.cmml" xref="S5.T8.22.22.2.m2.1.1.3.3.5">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.22.22.2.m2.1c">AR^{OKS=.75}_{coco}</annotation></semantics></math><span id="S5.T8.22.22.2.2" class="ltx_text" style="font-size:80%;">: at OKS=.75</span>
</td>
</tr>
<tr id="S5.T8.28.28" class="ltx_tr">
<td id="S5.T8.28.28.6" class="ltx_td ltx_align_left ltx_border_r">
<math id="S5.T8.23.23.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T8.23.23.1.m1.1a"><mo mathsize="80%" id="S5.T8.23.23.1.m1.1.1" xref="S5.T8.23.23.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T8.23.23.1.m1.1b"><ci id="S5.T8.23.23.1.m1.1.1.cmml" xref="S5.T8.23.23.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.23.23.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.T8.28.28.6.1" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.24.24.2.m2.1" class="ltx_Math" alttext="AR^{medium}_{coco}" display="inline"><semantics id="S5.T8.24.24.2.m2.1a"><mrow id="S5.T8.24.24.2.m2.1.1" xref="S5.T8.24.24.2.m2.1.1.cmml"><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.2" xref="S5.T8.24.24.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.T8.24.24.2.m2.1.1.1" xref="S5.T8.24.24.2.m2.1.1.1.cmml">​</mo><msubsup id="S5.T8.24.24.2.m2.1.1.3" xref="S5.T8.24.24.2.m2.1.1.3.cmml"><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.3.2.2" xref="S5.T8.24.24.2.m2.1.1.3.2.2.cmml">R</mi><mrow id="S5.T8.24.24.2.m2.1.1.3.3" xref="S5.T8.24.24.2.m2.1.1.3.3.cmml"><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.3.3.2" xref="S5.T8.24.24.2.m2.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.24.24.2.m2.1.1.3.3.1" xref="S5.T8.24.24.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.3.3.3" xref="S5.T8.24.24.2.m2.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.T8.24.24.2.m2.1.1.3.3.1a" xref="S5.T8.24.24.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.3.3.4" xref="S5.T8.24.24.2.m2.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.24.24.2.m2.1.1.3.3.1b" xref="S5.T8.24.24.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.3.3.5" xref="S5.T8.24.24.2.m2.1.1.3.3.5.cmml">o</mi></mrow><mrow id="S5.T8.24.24.2.m2.1.1.3.2.3" xref="S5.T8.24.24.2.m2.1.1.3.2.3.cmml"><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.3.2.3.2" xref="S5.T8.24.24.2.m2.1.1.3.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.T8.24.24.2.m2.1.1.3.2.3.1" xref="S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.3.2.3.3" xref="S5.T8.24.24.2.m2.1.1.3.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.T8.24.24.2.m2.1.1.3.2.3.1a" xref="S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.3.2.3.4" xref="S5.T8.24.24.2.m2.1.1.3.2.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S5.T8.24.24.2.m2.1.1.3.2.3.1b" xref="S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.3.2.3.5" xref="S5.T8.24.24.2.m2.1.1.3.2.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.T8.24.24.2.m2.1.1.3.2.3.1c" xref="S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.3.2.3.6" xref="S5.T8.24.24.2.m2.1.1.3.2.3.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.T8.24.24.2.m2.1.1.3.2.3.1d" xref="S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.24.24.2.m2.1.1.3.2.3.7" xref="S5.T8.24.24.2.m2.1.1.3.2.3.7.cmml">m</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.24.24.2.m2.1b"><apply id="S5.T8.24.24.2.m2.1.1.cmml" xref="S5.T8.24.24.2.m2.1.1"><times id="S5.T8.24.24.2.m2.1.1.1.cmml" xref="S5.T8.24.24.2.m2.1.1.1"></times><ci id="S5.T8.24.24.2.m2.1.1.2.cmml" xref="S5.T8.24.24.2.m2.1.1.2">𝐴</ci><apply id="S5.T8.24.24.2.m2.1.1.3.cmml" xref="S5.T8.24.24.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.24.24.2.m2.1.1.3.1.cmml" xref="S5.T8.24.24.2.m2.1.1.3">subscript</csymbol><apply id="S5.T8.24.24.2.m2.1.1.3.2.cmml" xref="S5.T8.24.24.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.24.24.2.m2.1.1.3.2.1.cmml" xref="S5.T8.24.24.2.m2.1.1.3">superscript</csymbol><ci id="S5.T8.24.24.2.m2.1.1.3.2.2.cmml" xref="S5.T8.24.24.2.m2.1.1.3.2.2">𝑅</ci><apply id="S5.T8.24.24.2.m2.1.1.3.2.3.cmml" xref="S5.T8.24.24.2.m2.1.1.3.2.3"><times id="S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml" xref="S5.T8.24.24.2.m2.1.1.3.2.3.1"></times><ci id="S5.T8.24.24.2.m2.1.1.3.2.3.2.cmml" xref="S5.T8.24.24.2.m2.1.1.3.2.3.2">𝑚</ci><ci id="S5.T8.24.24.2.m2.1.1.3.2.3.3.cmml" xref="S5.T8.24.24.2.m2.1.1.3.2.3.3">𝑒</ci><ci id="S5.T8.24.24.2.m2.1.1.3.2.3.4.cmml" xref="S5.T8.24.24.2.m2.1.1.3.2.3.4">𝑑</ci><ci id="S5.T8.24.24.2.m2.1.1.3.2.3.5.cmml" xref="S5.T8.24.24.2.m2.1.1.3.2.3.5">𝑖</ci><ci id="S5.T8.24.24.2.m2.1.1.3.2.3.6.cmml" xref="S5.T8.24.24.2.m2.1.1.3.2.3.6">𝑢</ci><ci id="S5.T8.24.24.2.m2.1.1.3.2.3.7.cmml" xref="S5.T8.24.24.2.m2.1.1.3.2.3.7">𝑚</ci></apply></apply><apply id="S5.T8.24.24.2.m2.1.1.3.3.cmml" xref="S5.T8.24.24.2.m2.1.1.3.3"><times id="S5.T8.24.24.2.m2.1.1.3.3.1.cmml" xref="S5.T8.24.24.2.m2.1.1.3.3.1"></times><ci id="S5.T8.24.24.2.m2.1.1.3.3.2.cmml" xref="S5.T8.24.24.2.m2.1.1.3.3.2">𝑐</ci><ci id="S5.T8.24.24.2.m2.1.1.3.3.3.cmml" xref="S5.T8.24.24.2.m2.1.1.3.3.3">𝑜</ci><ci id="S5.T8.24.24.2.m2.1.1.3.3.4.cmml" xref="S5.T8.24.24.2.m2.1.1.3.3.4">𝑐</ci><ci id="S5.T8.24.24.2.m2.1.1.3.3.5.cmml" xref="S5.T8.24.24.2.m2.1.1.3.3.5">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.24.24.2.m2.1c">AR^{medium}_{coco}</annotation></semantics></math><span id="S5.T8.28.28.6.2" class="ltx_text" style="font-size:80%;">: for medium objects: </span><math id="S5.T8.25.25.3.m3.1" class="ltx_Math" alttext="32^{2}" display="inline"><semantics id="S5.T8.25.25.3.m3.1a"><msup id="S5.T8.25.25.3.m3.1.1" xref="S5.T8.25.25.3.m3.1.1.cmml"><mn mathsize="80%" id="S5.T8.25.25.3.m3.1.1.2" xref="S5.T8.25.25.3.m3.1.1.2.cmml">32</mn><mn mathsize="80%" id="S5.T8.25.25.3.m3.1.1.3" xref="S5.T8.25.25.3.m3.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T8.25.25.3.m3.1b"><apply id="S5.T8.25.25.3.m3.1.1.cmml" xref="S5.T8.25.25.3.m3.1.1"><csymbol cd="ambiguous" id="S5.T8.25.25.3.m3.1.1.1.cmml" xref="S5.T8.25.25.3.m3.1.1">superscript</csymbol><cn type="integer" id="S5.T8.25.25.3.m3.1.1.2.cmml" xref="S5.T8.25.25.3.m3.1.1.2">32</cn><cn type="integer" id="S5.T8.25.25.3.m3.1.1.3.cmml" xref="S5.T8.25.25.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.25.25.3.m3.1c">32^{2}</annotation></semantics></math><span id="S5.T8.28.28.6.3" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.26.26.4.m4.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.T8.26.26.4.m4.1a"><mo mathsize="80%" id="S5.T8.26.26.4.m4.1.1" xref="S5.T8.26.26.4.m4.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.T8.26.26.4.m4.1b"><lt id="S5.T8.26.26.4.m4.1.1.cmml" xref="S5.T8.26.26.4.m4.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.26.26.4.m4.1c">&lt;</annotation></semantics></math><span id="S5.T8.28.28.6.4" class="ltx_text" style="font-size:80%;"> area </span><math id="S5.T8.27.27.5.m5.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.T8.27.27.5.m5.1a"><mo mathsize="80%" id="S5.T8.27.27.5.m5.1.1" xref="S5.T8.27.27.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.T8.27.27.5.m5.1b"><lt id="S5.T8.27.27.5.m5.1.1.cmml" xref="S5.T8.27.27.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.27.27.5.m5.1c">&lt;</annotation></semantics></math><span id="S5.T8.28.28.6.5" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.28.28.6.m6.1" class="ltx_Math" alttext="96^{2}" display="inline"><semantics id="S5.T8.28.28.6.m6.1a"><msup id="S5.T8.28.28.6.m6.1.1" xref="S5.T8.28.28.6.m6.1.1.cmml"><mn mathsize="80%" id="S5.T8.28.28.6.m6.1.1.2" xref="S5.T8.28.28.6.m6.1.1.2.cmml">96</mn><mn mathsize="80%" id="S5.T8.28.28.6.m6.1.1.3" xref="S5.T8.28.28.6.m6.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T8.28.28.6.m6.1b"><apply id="S5.T8.28.28.6.m6.1.1.cmml" xref="S5.T8.28.28.6.m6.1.1"><csymbol cd="ambiguous" id="S5.T8.28.28.6.m6.1.1.1.cmml" xref="S5.T8.28.28.6.m6.1.1">superscript</csymbol><cn type="integer" id="S5.T8.28.28.6.m6.1.1.2.cmml" xref="S5.T8.28.28.6.m6.1.1.2">96</cn><cn type="integer" id="S5.T8.28.28.6.m6.1.1.3.cmml" xref="S5.T8.28.28.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.28.28.6.m6.1c">96^{2}</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T8.32.32" class="ltx_tr">
<td id="S5.T8.32.32.4" class="ltx_td ltx_align_left ltx_border_r">
<math id="S5.T8.29.29.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T8.29.29.1.m1.1a"><mo mathsize="80%" id="S5.T8.29.29.1.m1.1.1" xref="S5.T8.29.29.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T8.29.29.1.m1.1b"><ci id="S5.T8.29.29.1.m1.1.1.cmml" xref="S5.T8.29.29.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.29.29.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.T8.32.32.4.1" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.30.30.2.m2.1" class="ltx_Math" alttext="AR^{large}_{coco}" display="inline"><semantics id="S5.T8.30.30.2.m2.1a"><mrow id="S5.T8.30.30.2.m2.1.1" xref="S5.T8.30.30.2.m2.1.1.cmml"><mi mathsize="80%" id="S5.T8.30.30.2.m2.1.1.2" xref="S5.T8.30.30.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.T8.30.30.2.m2.1.1.1" xref="S5.T8.30.30.2.m2.1.1.1.cmml">​</mo><msubsup id="S5.T8.30.30.2.m2.1.1.3" xref="S5.T8.30.30.2.m2.1.1.3.cmml"><mi mathsize="80%" id="S5.T8.30.30.2.m2.1.1.3.2.2" xref="S5.T8.30.30.2.m2.1.1.3.2.2.cmml">R</mi><mrow id="S5.T8.30.30.2.m2.1.1.3.3" xref="S5.T8.30.30.2.m2.1.1.3.3.cmml"><mi mathsize="80%" id="S5.T8.30.30.2.m2.1.1.3.3.2" xref="S5.T8.30.30.2.m2.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.30.30.2.m2.1.1.3.3.1" xref="S5.T8.30.30.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.30.30.2.m2.1.1.3.3.3" xref="S5.T8.30.30.2.m2.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.T8.30.30.2.m2.1.1.3.3.1a" xref="S5.T8.30.30.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.30.30.2.m2.1.1.3.3.4" xref="S5.T8.30.30.2.m2.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T8.30.30.2.m2.1.1.3.3.1b" xref="S5.T8.30.30.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.30.30.2.m2.1.1.3.3.5" xref="S5.T8.30.30.2.m2.1.1.3.3.5.cmml">o</mi></mrow><mrow id="S5.T8.30.30.2.m2.1.1.3.2.3" xref="S5.T8.30.30.2.m2.1.1.3.2.3.cmml"><mi mathsize="80%" id="S5.T8.30.30.2.m2.1.1.3.2.3.2" xref="S5.T8.30.30.2.m2.1.1.3.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.T8.30.30.2.m2.1.1.3.2.3.1" xref="S5.T8.30.30.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.30.30.2.m2.1.1.3.2.3.3" xref="S5.T8.30.30.2.m2.1.1.3.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.T8.30.30.2.m2.1.1.3.2.3.1a" xref="S5.T8.30.30.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.30.30.2.m2.1.1.3.2.3.4" xref="S5.T8.30.30.2.m2.1.1.3.2.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.T8.30.30.2.m2.1.1.3.2.3.1b" xref="S5.T8.30.30.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.30.30.2.m2.1.1.3.2.3.5" xref="S5.T8.30.30.2.m2.1.1.3.2.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S5.T8.30.30.2.m2.1.1.3.2.3.1c" xref="S5.T8.30.30.2.m2.1.1.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S5.T8.30.30.2.m2.1.1.3.2.3.6" xref="S5.T8.30.30.2.m2.1.1.3.2.3.6.cmml">e</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.30.30.2.m2.1b"><apply id="S5.T8.30.30.2.m2.1.1.cmml" xref="S5.T8.30.30.2.m2.1.1"><times id="S5.T8.30.30.2.m2.1.1.1.cmml" xref="S5.T8.30.30.2.m2.1.1.1"></times><ci id="S5.T8.30.30.2.m2.1.1.2.cmml" xref="S5.T8.30.30.2.m2.1.1.2">𝐴</ci><apply id="S5.T8.30.30.2.m2.1.1.3.cmml" xref="S5.T8.30.30.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.30.30.2.m2.1.1.3.1.cmml" xref="S5.T8.30.30.2.m2.1.1.3">subscript</csymbol><apply id="S5.T8.30.30.2.m2.1.1.3.2.cmml" xref="S5.T8.30.30.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.T8.30.30.2.m2.1.1.3.2.1.cmml" xref="S5.T8.30.30.2.m2.1.1.3">superscript</csymbol><ci id="S5.T8.30.30.2.m2.1.1.3.2.2.cmml" xref="S5.T8.30.30.2.m2.1.1.3.2.2">𝑅</ci><apply id="S5.T8.30.30.2.m2.1.1.3.2.3.cmml" xref="S5.T8.30.30.2.m2.1.1.3.2.3"><times id="S5.T8.30.30.2.m2.1.1.3.2.3.1.cmml" xref="S5.T8.30.30.2.m2.1.1.3.2.3.1"></times><ci id="S5.T8.30.30.2.m2.1.1.3.2.3.2.cmml" xref="S5.T8.30.30.2.m2.1.1.3.2.3.2">𝑙</ci><ci id="S5.T8.30.30.2.m2.1.1.3.2.3.3.cmml" xref="S5.T8.30.30.2.m2.1.1.3.2.3.3">𝑎</ci><ci id="S5.T8.30.30.2.m2.1.1.3.2.3.4.cmml" xref="S5.T8.30.30.2.m2.1.1.3.2.3.4">𝑟</ci><ci id="S5.T8.30.30.2.m2.1.1.3.2.3.5.cmml" xref="S5.T8.30.30.2.m2.1.1.3.2.3.5">𝑔</ci><ci id="S5.T8.30.30.2.m2.1.1.3.2.3.6.cmml" xref="S5.T8.30.30.2.m2.1.1.3.2.3.6">𝑒</ci></apply></apply><apply id="S5.T8.30.30.2.m2.1.1.3.3.cmml" xref="S5.T8.30.30.2.m2.1.1.3.3"><times id="S5.T8.30.30.2.m2.1.1.3.3.1.cmml" xref="S5.T8.30.30.2.m2.1.1.3.3.1"></times><ci id="S5.T8.30.30.2.m2.1.1.3.3.2.cmml" xref="S5.T8.30.30.2.m2.1.1.3.3.2">𝑐</ci><ci id="S5.T8.30.30.2.m2.1.1.3.3.3.cmml" xref="S5.T8.30.30.2.m2.1.1.3.3.3">𝑜</ci><ci id="S5.T8.30.30.2.m2.1.1.3.3.4.cmml" xref="S5.T8.30.30.2.m2.1.1.3.3.4">𝑐</ci><ci id="S5.T8.30.30.2.m2.1.1.3.3.5.cmml" xref="S5.T8.30.30.2.m2.1.1.3.3.5">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.30.30.2.m2.1c">AR^{large}_{coco}</annotation></semantics></math><span id="S5.T8.32.32.4.2" class="ltx_text" style="font-size:80%;">: for large objects: area </span><math id="S5.T8.31.31.3.m3.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.T8.31.31.3.m3.1a"><mo mathsize="80%" id="S5.T8.31.31.3.m3.1.1" xref="S5.T8.31.31.3.m3.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.T8.31.31.3.m3.1b"><gt id="S5.T8.31.31.3.m3.1.1.cmml" xref="S5.T8.31.31.3.m3.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.31.31.3.m3.1c">&gt;</annotation></semantics></math><span id="S5.T8.32.32.4.3" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T8.32.32.4.m4.1" class="ltx_Math" alttext="96^{2}" display="inline"><semantics id="S5.T8.32.32.4.m4.1a"><msup id="S5.T8.32.32.4.m4.1.1" xref="S5.T8.32.32.4.m4.1.1.cmml"><mn mathsize="80%" id="S5.T8.32.32.4.m4.1.1.2" xref="S5.T8.32.32.4.m4.1.1.2.cmml">96</mn><mn mathsize="80%" id="S5.T8.32.32.4.m4.1.1.3" xref="S5.T8.32.32.4.m4.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T8.32.32.4.m4.1b"><apply id="S5.T8.32.32.4.m4.1.1.cmml" xref="S5.T8.32.32.4.m4.1.1"><csymbol cd="ambiguous" id="S5.T8.32.32.4.m4.1.1.1.cmml" xref="S5.T8.32.32.4.m4.1.1">superscript</csymbol><cn type="integer" id="S5.T8.32.32.4.m4.1.1.2.cmml" xref="S5.T8.32.32.4.m4.1.1.2">96</cn><cn type="integer" id="S5.T8.32.32.4.m4.1.1.3.cmml" xref="S5.T8.32.32.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.32.32.4.m4.1c">96^{2}</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T8.32.39.7" class="ltx_tr">
<td id="S5.T8.32.39.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T8.32.39.7.1.1" class="ltx_text" style="font-size:80%;">OKS</span></td>
<td id="S5.T8.32.39.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<table id="S5.T8.32.39.7.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.32.39.7.2.1.1" class="ltx_tr">
<td id="S5.T8.32.39.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.39.7.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Object</span></td>
</tr>
<tr id="S5.T8.32.39.7.2.1.2" class="ltx_tr">
<td id="S5.T8.32.39.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.39.7.2.1.2.1.1" class="ltx_text" style="font-size:80%;">Keypoint</span></td>
</tr>
<tr id="S5.T8.32.39.7.2.1.3" class="ltx_tr">
<td id="S5.T8.32.39.7.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.39.7.2.1.3.1.1" class="ltx_text" style="font-size:80%;">Similarity</span></td>
</tr>
</table>
</td>
<td id="S5.T8.32.39.7.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T8.32.39.7.3.1" class="ltx_text" style="font-size:80%;">COCO</span></td>
<td id="S5.T8.32.39.7.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="S5.T8.32.39.7.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.32.39.7.4.1.1" class="ltx_tr">
<td id="S5.T8.32.39.7.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T8.32.39.7.4.1.1.1.1" class="ltx_text" style="font-size:80%;">A similar role as the Intersection over Union (IoU) for AP/AR.</span></td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.2" class="ltx_p"><span id="S5.SS2.p2.2.1" class="ltx_text ltx_font_bold">Percentage of Correct Parts (PCP)</span> <cite class="ltx_cite ltx_citemacro_citep">(Ferrari et al., <a href="#bib.bib41" title="" class="ltx_ref">2008</a>)</cite> is widely used in early research. It reports the localization accuracy for limbs. A limb is correctly localized if its two endpoints are within a threshold from the corresponding ground truth endpoints. The threshold can be <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mn id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">50</mn><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">50\%</annotation></semantics></math> of the limb length. Besides a mean PCP, some limbs PCP (torso, upper legs, lower legs, upper arms, forearms, head) normally are also reported <cite class="ltx_cite ltx_citemacro_citep">(Johnson and Everingham, <a href="#bib.bib68" title="" class="ltx_ref">2010</a>)</cite>. And percentage curves for each limb can be obtained with the variation of threshold in the metric <cite class="ltx_cite ltx_citemacro_citep">(Gkioxari et al., <a href="#bib.bib43" title="" class="ltx_ref">2013</a>)</cite>. The similar metrics PCPm from <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite> use <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mrow id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml"><mn id="S5.SS2.p2.2.m2.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml">50</mn><mo id="S5.SS2.p2.2.m2.1.1.1" xref="S5.SS2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><apply id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">50\%</annotation></semantics></math> of the mean ground-truth segment length over the entire test set as a matching threshold.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Percentage of Correct Keypoints (PCK)</span> <cite class="ltx_cite ltx_citemacro_citep">(Yang and Ramanan, <a href="#bib.bib180" title="" class="ltx_ref">2013</a>)</cite> measures the accuracy of the localization of the body joints. A candidate body joint is considered as correct if it falls within the threshold pixels of the ground-truth joint. The threshold can be a fraction of the person bounding box size <cite class="ltx_cite ltx_citemacro_citep">(Yang and Ramanan, <a href="#bib.bib180" title="" class="ltx_ref">2013</a>)</cite>, pixel radius that normalized by the torso height of each test sample <cite class="ltx_cite ltx_citemacro_citep">(Sapp and Taskar, <a href="#bib.bib143" title="" class="ltx_ref">2013</a>)</cite> (denoted as Percent of Detected Joints (PDJ) in <cite class="ltx_cite ltx_citemacro_citep">(Toshev and Szegedy, <a href="#bib.bib165" title="" class="ltx_ref">2014</a>)</cite>), <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mrow id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><mn id="S5.SS2.p3.1.m1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.2.cmml">50</mn><mo id="S5.SS2.p3.1.m1.1.1.1" xref="S5.SS2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.p3.1.m1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">50\%</annotation></semantics></math> of the head segment length of each test image (denoted as <span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_bold">PCKh@0.5</span> in <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite>). Also, with the variation of a threshold, Area Under the Curve (AUC) can be generated for further analysis.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">The Average Precision (AP)</span>. For systems in which there are only joint locations but no annotated bounding boxes for human bodies/heads or number of people in the image as ground truth at testing, the detection problem must be addressed as well. Similar to object detection, an Average Precision (AP) evaluation method is proposed, which is first called Average Precision of Keypoints (APK) in <cite class="ltx_cite ltx_citemacro_citep">(Yang and Ramanan, <a href="#bib.bib180" title="" class="ltx_ref">2013</a>)</cite>. In AP measure, if a predicted joint falls within a threshold of the ground-truth joint location, it is counted as a true positive. Note that correspondence between candidates and ground-truth poses are established separately for each keypoint. For multi-person pose evaluation, all predicted poses are assigned to the ground truth poses one by one based on the PCKh score order, while unassigned predictions are counted as false positives <cite class="ltx_cite ltx_citemacro_citep">(Pishchulin et al., <a href="#bib.bib131" title="" class="ltx_ref">2016</a>)</cite>. The mean average precision (mAP) is reported from the AP of each body joint.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p"><span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_bold">Average Precision (AP), Average Recall (AR) and their variants</span>. In <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib93" title="" class="ltx_ref">2014</a>)</cite>, evaluating multi-person pose estimation results as an object detection problem is further designed. AP, AR, and their variants are reported based on an analogous similarity measure: object keypoint similarity (OKS) which plays the same role as the Intersection over Union (IoU). Additional, AP/AR with different human body scales are also reported in COCO dataset. Table <a href="#S5.T8" title="Table 8 ‣ 5.2 Evaluation Metrics of 2D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> summarizes all above evaluation metrics.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p"><span id="S5.SS2.p6.1.1" class="ltx_text ltx_font_bold">Frame Rate, Number of Weights and Giga Floating-point Operations Per Second (GFLOPs)</span>. The computational performance metrics are also very important for HPE. Frame Rate indicates the processing speed of input data, generally expressed by Frames Per Second (FPS) or seconds per image (s/image) <cite class="ltx_cite ltx_citemacro_citep">(Cao et al., <a href="#bib.bib13" title="" class="ltx_ref">2016</a>)</cite>. Number of Weights and GFLOPs show the efficiency of the network, mainly related to the network design and the specific used GPUs/CPUs <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a href="#bib.bib151" title="" class="ltx_ref">2019</a>)</cite>. These computational performance metrics are suitable for 3D HPE as well.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Datasets for 3D human pose estimation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">For a better understanding of the human body in 3D space, there are many kinds of body representations with different modern equipment. 3D human body shape scans, such as <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_bold">SCAPE</span> <cite class="ltx_cite ltx_citemacro_citep">(Anguelov et al., <a href="#bib.bib5" title="" class="ltx_ref">2005</a>)</cite>, <span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_bold">INRIA4D</span> <cite class="ltx_cite ltx_citemacro_citep">(INRIA4D, <a href="#bib.bib56" title="" class="ltx_ref">accessed on 2019</a>)</cite> and <span id="S5.SS3.p1.1.3" class="ltx_text ltx_font_bold">FAUST</span> <cite class="ltx_cite ltx_citemacro_citep">(Bogo et al., <a href="#bib.bib9" title="" class="ltx_ref">2014</a>, <a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite>, 3D human body surface cloud points with time of flight (TOF) depth sensors <cite class="ltx_cite ltx_citemacro_citep">(Shahroudy et al., <a href="#bib.bib146" title="" class="ltx_ref">2016</a>)</cite>, 3D human body reflective markers capture with motion capture systems (MoCap) <cite class="ltx_cite ltx_citemacro_citep">(Sigal et al., <a href="#bib.bib149" title="" class="ltx_ref">2010</a>; Ionescu et al., <a href="#bib.bib59" title="" class="ltx_ref">2014</a>)</cite>, orientation and acceleration of 3D human body data with Inertial Measurement Unit (IMU) <cite class="ltx_cite ltx_citemacro_citep">(von Marcard et al., <a href="#bib.bib102" title="" class="ltx_ref">2016</a>, <a href="#bib.bib101" title="" class="ltx_ref">2018</a>)</cite>. It is difficult to summarize them all, this paper summarizes the datasets that involve RGB images and 3D joint coordinates. The details of the selected 3D datasets are summarized in Table <a href="#S5.T9" title="Table 9 ‣ 5.3 Datasets for 3D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> and some example images with annotations are shown in Fig. <a href="#S5.F6" title="Fig. 6 ‣ 5.3 Datasets for 3D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S5.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 9: </span>Popular databases for 3D human pose estimation. Selected example images with annotations are shown in Fig. <a href="#S5.F6" title="Fig. 6 ‣ 5.3 Datasets for 3D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (Cams. indicates the number of cameras; Jnt. indicates the number of joints)</figcaption>
<table id="S5.T9.11" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T9.11.12.1" class="ltx_tr">
<td id="S5.T9.11.12.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T9.11.12.1.1.1" class="ltx_text" style="font-size:90%;">Dataset</span></td>
<td id="S5.T9.11.12.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" rowspan="2">
<span id="S5.T9.11.12.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.12.1.2.1.1" class="ltx_p" style="width:19.9pt;"><span id="S5.T9.11.12.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Cams.</span></span>
</span>
</td>
<td id="S5.T9.11.12.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" rowspan="2">
<span id="S5.T9.11.12.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.12.1.3.1.1" class="ltx_p" style="width:14.2pt;"><span id="S5.T9.11.12.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Jnt.</span></span>
</span>
</td>
<td id="S5.T9.11.12.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S5.T9.11.12.1.4.1" class="ltx_text" style="font-size:90%;">Number of frames/videos</span></td>
<td id="S5.T9.11.12.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.11.12.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.12.1.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S5.T9.11.12.1.5.1.1.1" class="ltx_text" style="font-size:90%;">Evaluation</span></span>
</span>
</td>
<td id="S5.T9.11.12.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" rowspan="2">
<span id="S5.T9.11.12.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.12.1.6.1.1" class="ltx_p" style="width:213.4pt;"><span id="S5.T9.11.12.1.6.1.1.1" class="ltx_text" style="font-size:90%;">Highlights</span></span>
</span>
</td>
</tr>
<tr id="S5.T9.11.13.2" class="ltx_tr">
<td id="S5.T9.11.13.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S5.T9.11.13.2.1.1" class="ltx_text" style="font-size:90%;">name</span></td>
<td id="S5.T9.11.13.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.11.13.2.2.1" class="ltx_text" style="font-size:90%;">Train</span></td>
<td id="S5.T9.11.13.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.11.13.2.3.1" class="ltx_text" style="font-size:90%;">Val</span></td>
<td id="S5.T9.11.13.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.11.13.2.4.1" class="ltx_text" style="font-size:90%;">Test</span></td>
<td id="S5.T9.11.13.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T9.11.13.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.13.2.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S5.T9.11.13.2.5.1.1.1" class="ltx_text" style="font-size:90%;">protocol</span></span>
</span>
</td>
</tr>
<tr id="S5.T9.11.14.3" class="ltx_tr">
<td id="S5.T9.11.14.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="8"><span id="S5.T9.11.14.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Single person</span></td>
</tr>
<tr id="S5.T9.3.3" class="ltx_tr">
<td id="S5.T9.3.3.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T9.3.3.4.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-8.5pt;">HumanEva-I</span></td>
<td id="S5.T9.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;">
<span id="S5.T9.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.3.3.5.1.1" class="ltx_p" style="width:19.9pt;"><span id="S5.T9.3.3.5.1.1.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-8.5pt;">7</span></span>
</span>
</td>
<td id="S5.T9.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;" rowspan="2">
<span id="S5.T9.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.3.3.6.1.1" class="ltx_p" style="width:14.2pt;"><span id="S5.T9.3.3.6.1.1.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-14.2pt;">15</span></span>
</span>
</td>
<td id="S5.T9.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T9.1.1.1.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-8.5pt;"><math id="S5.T9.1.1.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T9.1.1.1.1.m1.1a"><mo id="S5.T9.1.1.1.1.m1.1.1" xref="S5.T9.1.1.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T9.1.1.1.1.m1.1b"><approx id="S5.T9.1.1.1.1.m1.1.1.cmml" xref="S5.T9.1.1.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.1.1.1.1.m1.1c">\approx</annotation></semantics></math>6.8k</span></td>
<td id="S5.T9.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T9.2.2.2.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-8.5pt;"><math id="S5.T9.2.2.2.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T9.2.2.2.1.m1.1a"><mo id="S5.T9.2.2.2.1.m1.1.1" xref="S5.T9.2.2.2.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T9.2.2.2.1.m1.1b"><approx id="S5.T9.2.2.2.1.m1.1.1.cmml" xref="S5.T9.2.2.2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.2.2.2.1.m1.1c">\approx</annotation></semantics></math>6.8k</span></td>
<td id="S5.T9.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T9.3.3.3.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-8.5pt;"><math id="S5.T9.3.3.3.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T9.3.3.3.1.m1.1a"><mo id="S5.T9.3.3.3.1.m1.1.1" xref="S5.T9.3.3.3.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T9.3.3.3.1.m1.1b"><approx id="S5.T9.3.3.3.1.m1.1.1.cmml" xref="S5.T9.3.3.3.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.3.3.3.1.m1.1c">\approx</annotation></semantics></math>24k</span></td>
<td id="S5.T9.3.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;" rowspan="2">
<span id="S5.T9.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.3.3.7.1.1" class="ltx_p" style="width:42.7pt;"><span id="S5.T9.3.3.7.1.1.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-14.2pt;">    MPJPE</span></span>
</span>
</td>
<td id="S5.T9.3.3.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;" rowspan="2">
<span id="S5.T9.3.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.3.3.8.1.1" class="ltx_p" style="width:213.4pt;"><span id="S5.T9.3.3.8.1.1.1" class="ltx_text" style="font-size:90%;">4/2 (I/II) subjects, 6/1 (I/II) actions, Vicon data, indoor environment. <cite class="ltx_cite ltx_citemacro_citep">(Sigal et al., <a href="#bib.bib149" title="" class="ltx_ref">2010</a>)</cite></span></span>
</span>
</td>
</tr>
<tr id="S5.T9.4.4" class="ltx_tr">
<td id="S5.T9.4.4.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T9.4.4.2.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-8.5pt;">
<span id="S5.T9.4.4.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.4.4.2.1.1.1" class="ltx_tr">
<span id="S5.T9.4.4.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">HumanEva-II</span></span>
</span></span></td>
<td id="S5.T9.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;">
<span id="S5.T9.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.4.4.3.1.1" class="ltx_p" style="width:19.9pt;"><span id="S5.T9.4.4.3.1.1.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-8.5pt;">4</span></span>
</span>
</td>
<td id="S5.T9.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T9.4.4.4.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-8.5pt;">0</span></td>
<td id="S5.T9.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T9.4.4.5.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-8.5pt;">0</span></td>
<td id="S5.T9.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:9.0pt;"><span id="S5.T9.4.4.1.1" class="ltx_text" style="font-size:90%;position:relative; bottom:-8.5pt;"><math id="S5.T9.4.4.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T9.4.4.1.1.m1.1a"><mo id="S5.T9.4.4.1.1.m1.1.1" xref="S5.T9.4.4.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T9.4.4.1.1.m1.1b"><approx id="S5.T9.4.4.1.1.m1.1.1.cmml" xref="S5.T9.4.4.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.4.4.1.1.m1.1c">\approx</annotation></semantics></math>2.5k</span></td>
</tr>
<tr id="S5.T9.7.7" class="ltx_tr">
<td id="S5.T9.7.7.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T9.7.7.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T9.7.7.4.1.1" class="ltx_tr">
<td id="S5.T9.7.7.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.7.7.4.1.1.1.1" class="ltx_text" style="font-size:90%;">Human3.6M</span></td>
</tr>
</table>
</td>
<td id="S5.T9.7.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.7.7.5.1.1" class="ltx_p" style="width:19.9pt;"><span id="S5.T9.7.7.5.1.1.1" class="ltx_text" style="font-size:90%;">4</span></span>
</span>
</td>
<td id="S5.T9.7.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.7.7.6.1.1" class="ltx_p" style="width:14.2pt;"><span id="S5.T9.7.7.6.1.1.1" class="ltx_text" style="font-size:90%;">17</span></span>
</span>
</td>
<td id="S5.T9.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T9.5.5.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T9.5.5.1.m1.1a"><mo mathsize="90%" id="S5.T9.5.5.1.m1.1.1" xref="S5.T9.5.5.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T9.5.5.1.m1.1b"><approx id="S5.T9.5.5.1.m1.1.1.cmml" xref="S5.T9.5.5.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.5.5.1.m1.1c">\approx</annotation></semantics></math><span id="S5.T9.5.5.1.1" class="ltx_text" style="font-size:90%;">1.5M</span>
</td>
<td id="S5.T9.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T9.6.6.2.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T9.6.6.2.m1.1a"><mo mathsize="90%" id="S5.T9.6.6.2.m1.1.1" xref="S5.T9.6.6.2.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T9.6.6.2.m1.1b"><approx id="S5.T9.6.6.2.m1.1.1.cmml" xref="S5.T9.6.6.2.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.6.6.2.m1.1c">\approx</annotation></semantics></math><span id="S5.T9.6.6.2.1" class="ltx_text" style="font-size:90%;">0.6M</span>
</td>
<td id="S5.T9.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T9.7.7.3.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T9.7.7.3.m1.1a"><mo mathsize="90%" id="S5.T9.7.7.3.m1.1.1" xref="S5.T9.7.7.3.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T9.7.7.3.m1.1b"><approx id="S5.T9.7.7.3.m1.1.1.cmml" xref="S5.T9.7.7.3.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.7.7.3.m1.1c">\approx</annotation></semantics></math><span id="S5.T9.7.7.3.1" class="ltx_text" style="font-size:90%;">1.5M</span>
</td>
<td id="S5.T9.7.7.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.7.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.7.7.7.1.1" class="ltx_p" style="width:42.7pt;"><span id="S5.T9.7.7.7.1.1.1" class="ltx_text" style="font-size:90%;">MPJPE</span></span>
</span>
</td>
<td id="S5.T9.7.7.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.7.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.7.7.8.1.1" class="ltx_p" style="width:213.4pt;"><span id="S5.T9.7.7.8.1.1.1" class="ltx_text" style="font-size:90%;">11 subjects, 17 actions, Vicon data, multi-annotation (3D joints, person bounding boxes, depth data, 3D body scans), indoor environment. </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T9.7.7.8.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Ionescu et al.<span id="S5.T9.7.7.8.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib59" title="" class="ltx_ref">2014</a><span id="S5.T9.7.7.8.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S5.T9.8.8" class="ltx_tr">
<td id="S5.T9.8.8.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T9.8.8.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T9.8.8.2.1.1" class="ltx_tr">
<td id="S5.T9.8.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.8.8.2.1.1.1.1" class="ltx_text" style="font-size:90%;">TNT15</span></td>
</tr>
</table>
</td>
<td id="S5.T9.8.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.8.8.3.1.1" class="ltx_p" style="width:19.9pt;"><span id="S5.T9.8.8.3.1.1.1" class="ltx_text" style="font-size:90%;">8</span></span>
</span>
</td>
<td id="S5.T9.8.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.8.8.4.1.1" class="ltx_p" style="width:14.2pt;"><span id="S5.T9.8.8.4.1.1.1" class="ltx_text" style="font-size:90%;">15</span></span>
</span>
</td>
<td id="S5.T9.8.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">
<math id="S5.T9.8.8.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T9.8.8.1.m1.1a"><mo mathsize="90%" id="S5.T9.8.8.1.m1.1.1" xref="S5.T9.8.8.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T9.8.8.1.m1.1b"><approx id="S5.T9.8.8.1.m1.1.1.cmml" xref="S5.T9.8.8.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.8.8.1.m1.1c">\approx</annotation></semantics></math><span id="S5.T9.8.8.1.1" class="ltx_text" style="font-size:90%;">13k</span>
</td>
<td id="S5.T9.8.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.8.8.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S5.T9.8.8.5.1.1.1" class="ltx_text" style="font-size:90%;">HumanEva</span></span>
</span>
</td>
<td id="S5.T9.8.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.8.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.8.8.6.1.1" class="ltx_p" style="width:213.4pt;"><span id="S5.T9.8.8.6.1.1.1" class="ltx_text" style="font-size:90%;">4 subjects, 5 actions, IMU data, 3D body scans, indoor environment. </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T9.8.8.6.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>von Marcard et al.<span id="S5.T9.8.8.6.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib102" title="" class="ltx_ref">2016</a><span id="S5.T9.8.8.6.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S5.T9.9.9" class="ltx_tr">
<td id="S5.T9.9.9.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T9.9.9.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T9.9.9.2.1.1" class="ltx_tr">
<td id="S5.T9.9.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.9.9.2.1.1.1.1" class="ltx_text" style="font-size:90%;">MPI-INF-3DHP</span></td>
</tr>
</table>
</td>
<td id="S5.T9.9.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.9.9.3.1.1" class="ltx_p" style="width:19.9pt;"><span id="S5.T9.9.9.3.1.1.1" class="ltx_text" style="font-size:90%;">14</span></span>
</span>
</td>
<td id="S5.T9.9.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.9.9.4.1.1" class="ltx_p" style="width:14.2pt;"><span id="S5.T9.9.9.4.1.1.1" class="ltx_text" style="font-size:90%;">15</span></span>
</span>
</td>
<td id="S5.T9.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">
<math id="S5.T9.9.9.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T9.9.9.1.m1.1a"><mo mathsize="90%" id="S5.T9.9.9.1.m1.1.1" xref="S5.T9.9.9.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T9.9.9.1.m1.1b"><approx id="S5.T9.9.9.1.m1.1.1.cmml" xref="S5.T9.9.9.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.9.9.1.m1.1c">\approx</annotation></semantics></math><span id="S5.T9.9.9.1.1" class="ltx_text" style="font-size:90%;">1.3M</span>
</td>
<td id="S5.T9.9.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.9.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.9.9.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S5.T9.9.9.5.1.1.1" class="ltx_text" style="font-size:90%;">3DPCK</span></span>
</span>
</td>
<td id="S5.T9.9.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.9.9.6.1.1" class="ltx_p" style="width:213.4pt;"><span id="S5.T9.9.9.6.1.1.1" class="ltx_text" style="font-size:90%;">8 subjects, 8 actions, commercial markerless system, indoor and outdoor scenes. </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T9.9.9.6.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Mehta et al.<span id="S5.T9.9.9.6.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib104" title="" class="ltx_ref">2017a</a><span id="S5.T9.9.9.6.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S5.T9.10.10" class="ltx_tr">
<td id="S5.T9.10.10.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T9.10.10.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T9.10.10.2.1.1" class="ltx_tr">
<td id="S5.T9.10.10.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.10.10.2.1.1.1.1" class="ltx_text" style="font-size:90%;">TotalCapture</span></td>
</tr>
</table>
</td>
<td id="S5.T9.10.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.10.10.3.1.1" class="ltx_p" style="width:19.9pt;"><span id="S5.T9.10.10.3.1.1.1" class="ltx_text" style="font-size:90%;">8</span></span>
</span>
</td>
<td id="S5.T9.10.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.10.10.4.1.1" class="ltx_p" style="width:14.2pt;"><span id="S5.T9.10.10.4.1.1.1" class="ltx_text" style="font-size:90%;">26</span></span>
</span>
</td>
<td id="S5.T9.10.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">
<math id="S5.T9.10.10.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T9.10.10.1.m1.1a"><mo mathsize="90%" id="S5.T9.10.10.1.m1.1.1" xref="S5.T9.10.10.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T9.10.10.1.m1.1b"><approx id="S5.T9.10.10.1.m1.1.1.cmml" xref="S5.T9.10.10.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.10.10.1.m1.1c">\approx</annotation></semantics></math><span id="S5.T9.10.10.1.1" class="ltx_text" style="font-size:90%;">1.9M</span>
</td>
<td id="S5.T9.10.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.10.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.10.10.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S5.T9.10.10.5.1.1.1" class="ltx_text" style="font-size:90%;">MPJPE</span></span>
</span>
</td>
<td id="S5.T9.10.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.10.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.10.10.6.1.1" class="ltx_p" style="width:213.4pt;"><span id="S5.T9.10.10.6.1.1.1" class="ltx_text" style="font-size:90%;">5 subjects, 5 actions, IMU and Vicon data, indoors environment. </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T9.10.10.6.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Trumble et al.<span id="S5.T9.10.10.6.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib166" title="" class="ltx_ref">2017</a><span id="S5.T9.10.10.6.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S5.T9.11.15.4" class="ltx_tr">
<td id="S5.T9.11.15.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="8"><span id="S5.T9.11.15.4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Multiple person</span></td>
</tr>
<tr id="S5.T9.11.16.5" class="ltx_tr">
<td id="S5.T9.11.16.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T9.11.16.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T9.11.16.5.1.1.1" class="ltx_tr">
<td id="S5.T9.11.16.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.11.16.5.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Panoptic</span></td>
</tr>
</table>
</td>
<td id="S5.T9.11.16.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.11.16.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.16.5.2.1.1" class="ltx_p" style="width:19.9pt;"><span id="S5.T9.11.16.5.2.1.1.1" class="ltx_text" style="font-size:90%;">521</span></span>
</span>
</td>
<td id="S5.T9.11.16.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.11.16.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.16.5.3.1.1" class="ltx_p" style="width:14.2pt;"><span id="S5.T9.11.16.5.3.1.1.1" class="ltx_text" style="font-size:90%;">15</span></span>
</span>
</td>
<td id="S5.T9.11.16.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S5.T9.11.16.5.4.1" class="ltx_text" style="font-size:90%;">65 videos (5.5 hours)</span></td>
<td id="S5.T9.11.16.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.11.16.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.16.5.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S5.T9.11.16.5.5.1.1.1" class="ltx_text" style="font-size:90%;">3DPCK</span></span>
</span>
</td>
<td id="S5.T9.11.16.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T9.11.16.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.16.5.6.1.1" class="ltx_p" style="width:213.4pt;"><span id="S5.T9.11.16.5.6.1.1.1" class="ltx_text" style="font-size:90%;">up to 8 subjects in each video, social interactions, markerless studio, multi-annotation (3D joints, cloud points, optical flow), indoors environment. </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T9.11.16.5.6.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Joo et al.<span id="S5.T9.11.16.5.6.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib70" title="" class="ltx_ref">2017</a><span id="S5.T9.11.16.5.6.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
</tr>
<tr id="S5.T9.11.11" class="ltx_tr">
<td id="S5.T9.11.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T9.11.11.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T9.11.11.2.1.1" class="ltx_tr">
<td id="S5.T9.11.11.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.11.11.2.1.1.1.1" class="ltx_text" style="font-size:90%;">3DPW</span></td>
</tr>
</table>
</td>
<td id="S5.T9.11.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T9.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.11.3.1.1" class="ltx_p" style="width:19.9pt;"><span id="S5.T9.11.11.3.1.1.1" class="ltx_text" style="font-size:90%;">1</span></span>
</span>
</td>
<td id="S5.T9.11.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T9.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.11.4.1.1" class="ltx_p" style="width:14.2pt;"><span id="S5.T9.11.11.4.1.1.1" class="ltx_text" style="font-size:90%;">18</span></span>
</span>
</td>
<td id="S5.T9.11.11.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="3">
<span id="S5.T9.11.11.1.1" class="ltx_text" style="font-size:90%;">60 videos (</span><math id="S5.T9.11.11.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T9.11.11.1.m1.1a"><mo mathsize="90%" id="S5.T9.11.11.1.m1.1.1" xref="S5.T9.11.11.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T9.11.11.1.m1.1b"><approx id="S5.T9.11.11.1.m1.1.1.cmml" xref="S5.T9.11.11.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.11.11.1.m1.1c">\approx</annotation></semantics></math><span id="S5.T9.11.11.1.2" class="ltx_text" style="font-size:90%;">51k frames)</span>
</td>
<td id="S5.T9.11.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T9.11.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.11.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S5.T9.11.11.5.1.1.1" class="ltx_text" style="font-size:90%;">MPJPE MPJAE</span></span>
</span>
</td>
<td id="S5.T9.11.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T9.11.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T9.11.11.6.1.1" class="ltx_p" style="width:213.4pt;"><span id="S5.T9.11.11.6.1.1.1" class="ltx_text" style="font-size:90%;">7 subjects(up to 2), daily actions, estimated 3D poses from videos and attached IMUs, 3D body scans, SMPL model fitting, in the wild. </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T9.11.11.6.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>von Marcard et al.<span id="S5.T9.11.11.6.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib101" title="" class="ltx_ref">2018</a><span id="S5.T9.11.11.6.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2006.01423/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="446" height="403" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 6: </span>Some selected example images with annotations from typical 3D human pose estimation datasets.</figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_bold">HumanEva-I&amp;II Datasets</span> <cite class="ltx_cite ltx_citemacro_citep">(Sigal et al., <a href="#bib.bib149" title="" class="ltx_ref">2010</a>)</cite>. The ground truth annotations of both datasets were captured with a commercial MoCap system from ViconPeak. The HumanEva-I dataset contains 7-view video sequences (4 grayscales and 3 colors) which are synchronized with 3D body poses. There are 4 subjects with markers on their bodies performing 6 common actions (e.g. walking, jogging, gesturing, throwing and catching a ball, boxing, combo) in an 3m x 2m capture area. HumanEva-II is an extension of HumanEva-I dataset for testing, which contains 2 subjects performing the action combo.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p"><span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">Human3.6M Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Ionescu et al., <a href="#bib.bib59" title="" class="ltx_ref">2014</a>)</cite> was collected using accurate marker-based MoCap systems <cite class="ltx_cite ltx_citemacro_citep">(Vicon, <a href="#bib.bib169" title="" class="ltx_ref">accessed on 2019</a>)</cite> in an indoor laboratory setup with 11 professional actors (5 females and 6 males) dressing moderately realistic clothing. It contains 3.6 million 3D human poses and corresponding images from 4 different views. The performed <math id="S5.SS3.p3.1.m1.1" class="ltx_Math" alttext="17" display="inline"><semantics id="S5.SS3.p3.1.m1.1a"><mn id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml">17</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><cn type="integer" id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1">17</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">17</annotation></semantics></math> daily activities include discussion, smoking, taking photos, talking on the phone, etc. Main capturing devices include 4 digital video cameras, 1 time-of-flight sensor, 10 motion cameras working synchronously. The capture area is about 4m x 3m. The provided annotations include 3D joint positions, joint angles, person bounding boxes, and 3D laser scans of each actor. For evaluation, there are three protocols with different training and testing data splits (protocol #1, protocol #2 and protocol #3.)</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.3" class="ltx_p"><span id="S5.SS3.p4.3.1" class="ltx_text ltx_font_bold">TNT15 Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(von Marcard et al., <a href="#bib.bib102" title="" class="ltx_ref">2016</a>)</cite> consists of synchronized data streams from <math id="S5.SS3.p4.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S5.SS3.p4.1.m1.1a"><mn id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><cn type="integer" id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">8</annotation></semantics></math> RGB-cameras and <math id="S5.SS3.p4.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.SS3.p4.2.m2.1a"><mn id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><cn type="integer" id="S5.SS3.p4.2.m2.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">10</annotation></semantics></math> IMUs. It has been recorded in an office environment. The dataset records 4 actors performing five activities (e.g. walking, running on the spot, rotating arms, jumping and skiing exercises, dynamic punching.) and contains about <math id="S5.SS3.p4.3.m3.1" class="ltx_Math" alttext="13" display="inline"><semantics id="S5.SS3.p4.3.m3.1a"><mn id="S5.SS3.p4.3.m3.1.1" xref="S5.SS3.p4.3.m3.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.3.m3.1b"><cn type="integer" id="S5.SS3.p4.3.m3.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.3.m3.1c">13</annotation></semantics></math>k frames including binary segmented images obtained by background subtraction, 3D laser scans and registered meshes of each actor.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.3" class="ltx_p"><span id="S5.SS3.p5.3.1" class="ltx_text ltx_font_bold">MPI-INF-3DHP</span> <cite class="ltx_cite ltx_citemacro_citep">(Mehta et al., <a href="#bib.bib104" title="" class="ltx_ref">2017a</a>)</cite> was collected with a markerless multi-camera MoCap system <cite class="ltx_cite ltx_citemacro_citep">(TheCaptury, <a href="#bib.bib161" title="" class="ltx_ref">accessed on 2019</a>)</cite> in both indoor and outdoor scenes. It contains over <math id="S5.SS3.p5.1.m1.1" class="ltx_Math" alttext="1.3" display="inline"><semantics id="S5.SS3.p5.1.m1.1a"><mn id="S5.SS3.p5.1.m1.1.1" xref="S5.SS3.p5.1.m1.1.1.cmml">1.3</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.1.m1.1b"><cn type="float" id="S5.SS3.p5.1.m1.1.1.cmml" xref="S5.SS3.p5.1.m1.1.1">1.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.1.m1.1c">1.3</annotation></semantics></math>M frames from <math id="S5.SS3.p5.2.m2.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S5.SS3.p5.2.m2.1a"><mn id="S5.SS3.p5.2.m2.1.1" xref="S5.SS3.p5.2.m2.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.2.m2.1b"><cn type="integer" id="S5.SS3.p5.2.m2.1.1.cmml" xref="S5.SS3.p5.2.m2.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.2.m2.1c">14</annotation></semantics></math> different views. Eight subjects (4 females and 4 males) are recorded performing <math id="S5.SS3.p5.3.m3.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S5.SS3.p5.3.m3.1a"><mn id="S5.SS3.p5.3.m3.1.1" xref="S5.SS3.p5.3.m3.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.3.m3.1b"><cn type="integer" id="S5.SS3.p5.3.m3.1.1.cmml" xref="S5.SS3.p5.3.m3.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.3.m3.1c">8</annotation></semantics></math> activities (e.g. walking/standing, exercise, sitting, crouch/reach, on the floor, sports, miscellaneous.)</p>
</div>
<div id="S5.SS3.p6" class="ltx_para">
<p id="S5.SS3.p6.4" class="ltx_p"><span id="S5.SS3.p6.4.1" class="ltx_text ltx_font_bold">TotalCapture Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Trumble et al., <a href="#bib.bib166" title="" class="ltx_ref">2017</a>)</cite> was captured in indoors with space measuring roughly 8m x 4m with <math id="S5.SS3.p6.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S5.SS3.p6.1.m1.1a"><mn id="S5.SS3.p6.1.m1.1.1" xref="S5.SS3.p6.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.1.m1.1b"><cn type="integer" id="S5.SS3.p6.1.m1.1.1.cmml" xref="S5.SS3.p6.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.1.m1.1c">8</annotation></semantics></math> calibrated HD video cameras at a frame rate of 60Hz. There are <math id="S5.SS3.p6.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S5.SS3.p6.2.m2.1a"><mn id="S5.SS3.p6.2.m2.1.1" xref="S5.SS3.p6.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.2.m2.1b"><cn type="integer" id="S5.SS3.p6.2.m2.1.1.cmml" xref="S5.SS3.p6.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.2.m2.1c">4</annotation></semantics></math> male and <math id="S5.SS3.p6.3.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S5.SS3.p6.3.m3.1a"><mn id="S5.SS3.p6.3.m3.1.1" xref="S5.SS3.p6.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.3.m3.1b"><cn type="integer" id="S5.SS3.p6.3.m3.1.1.cmml" xref="S5.SS3.p6.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.3.m3.1c">1</annotation></semantics></math> female subjects each performing four diverse performances, repeated 3 times: Range Of Motion (ROM), Walking, Acting, and Freestyle. There is a total of <math id="S5.SS3.p6.4.m4.3" class="ltx_Math" alttext="1,892,176" display="inline"><semantics id="S5.SS3.p6.4.m4.3a"><mrow id="S5.SS3.p6.4.m4.3.4.2" xref="S5.SS3.p6.4.m4.3.4.1.cmml"><mn id="S5.SS3.p6.4.m4.1.1" xref="S5.SS3.p6.4.m4.1.1.cmml">1</mn><mo id="S5.SS3.p6.4.m4.3.4.2.1" xref="S5.SS3.p6.4.m4.3.4.1.cmml">,</mo><mn id="S5.SS3.p6.4.m4.2.2" xref="S5.SS3.p6.4.m4.2.2.cmml">892</mn><mo id="S5.SS3.p6.4.m4.3.4.2.2" xref="S5.SS3.p6.4.m4.3.4.1.cmml">,</mo><mn id="S5.SS3.p6.4.m4.3.3" xref="S5.SS3.p6.4.m4.3.3.cmml">176</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.4.m4.3b"><list id="S5.SS3.p6.4.m4.3.4.1.cmml" xref="S5.SS3.p6.4.m4.3.4.2"><cn type="integer" id="S5.SS3.p6.4.m4.1.1.cmml" xref="S5.SS3.p6.4.m4.1.1">1</cn><cn type="integer" id="S5.SS3.p6.4.m4.2.2.cmml" xref="S5.SS3.p6.4.m4.2.2">892</cn><cn type="integer" id="S5.SS3.p6.4.m4.3.3.cmml" xref="S5.SS3.p6.4.m4.3.3">176</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.4.m4.3c">1,892,176</annotation></semantics></math> frames of synchronized video, IMU and Vicon data. The variation and body motions contained in particular within the acting and freestyle sequences are very challenging with actions such as yoga, giving directions, bending over and crawling performed in both the train and test data.</p>
</div>
<div id="S5.SS3.p7" class="ltx_para">
<p id="S5.SS3.p7.1" class="ltx_p"><span id="S5.SS3.p7.1.1" class="ltx_text ltx_font_bold">MARCOnI Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Elhayek et al., <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> is a test dataset containing sequences in a variety of uncontrolled indoor and outdoor scenarios. The sequences vary according to different data modalities captured (multiple videos, video + marker positions), in the numbers and identities of actors to track, the complexity of the motions, the number of cameras used, the existence and number of moving objects in the background, and the lighting conditions (i.e. some body parts lit and some in shadow). Cameras differ in the types (from cell phones to vision cameras), the frame resolutions, and the frame rates.</p>
</div>
<div id="S5.SS3.p8" class="ltx_para">
<p id="S5.SS3.p8.5" class="ltx_p"><span id="S5.SS3.p8.5.1" class="ltx_text ltx_font_bold">Panoptic Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Joo et al., <a href="#bib.bib70" title="" class="ltx_ref">2017</a>)</cite> was captured with a markerless motion capturing using multiple view systems which contains <math id="S5.SS3.p8.1.m1.1" class="ltx_Math" alttext="480" display="inline"><semantics id="S5.SS3.p8.1.m1.1a"><mn id="S5.SS3.p8.1.m1.1.1" xref="S5.SS3.p8.1.m1.1.1.cmml">480</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p8.1.m1.1b"><cn type="integer" id="S5.SS3.p8.1.m1.1.1.cmml" xref="S5.SS3.p8.1.m1.1.1">480</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p8.1.m1.1c">480</annotation></semantics></math> VGA camera views, <math id="S5.SS3.p8.2.m2.1" class="ltx_Math" alttext="31" display="inline"><semantics id="S5.SS3.p8.2.m2.1a"><mn id="S5.SS3.p8.2.m2.1.1" xref="S5.SS3.p8.2.m2.1.1.cmml">31</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p8.2.m2.1b"><cn type="integer" id="S5.SS3.p8.2.m2.1.1.cmml" xref="S5.SS3.p8.2.m2.1.1">31</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p8.2.m2.1c">31</annotation></semantics></math> HD views, <math id="S5.SS3.p8.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.SS3.p8.3.m3.1a"><mn id="S5.SS3.p8.3.m3.1.1" xref="S5.SS3.p8.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p8.3.m3.1b"><cn type="integer" id="S5.SS3.p8.3.m3.1.1.cmml" xref="S5.SS3.p8.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p8.3.m3.1c">10</annotation></semantics></math> RGB-D sensors and hardware-based synchronized system. It contains <math id="S5.SS3.p8.4.m4.1" class="ltx_Math" alttext="65" display="inline"><semantics id="S5.SS3.p8.4.m4.1a"><mn id="S5.SS3.p8.4.m4.1.1" xref="S5.SS3.p8.4.m4.1.1.cmml">65</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p8.4.m4.1b"><cn type="integer" id="S5.SS3.p8.4.m4.1.1.cmml" xref="S5.SS3.p8.4.m4.1.1">65</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p8.4.m4.1c">65</annotation></semantics></math> sequences (5.5 hours) of social interaction with <math id="S5.SS3.p8.5.m5.1" class="ltx_Math" alttext="1.5" display="inline"><semantics id="S5.SS3.p8.5.m5.1a"><mn id="S5.SS3.p8.5.m5.1.1" xref="S5.SS3.p8.5.m5.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p8.5.m5.1b"><cn type="float" id="S5.SS3.p8.5.m5.1.1.cmml" xref="S5.SS3.p8.5.m5.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p8.5.m5.1c">1.5</annotation></semantics></math> millions of 3D skeletons. The annotations include 3D keypoints, cloud points, optical flow, etc.</p>
</div>
<div id="S5.SS3.p9" class="ltx_para">
<p id="S5.SS3.p9.2" class="ltx_p"><span id="S5.SS3.p9.2.1" class="ltx_text ltx_font_bold">3DPW Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(von Marcard et al., <a href="#bib.bib101" title="" class="ltx_ref">2018</a>)</cite> was captured with a single hand-held camera in natural environments. 3D annotations are estimated from IMUs attached to subjects’ limbs with proposed method Video Inertial Poser. All subjects are provided with 3D scans. The dataset consists of <math id="S5.SS3.p9.1.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S5.SS3.p9.1.m1.1a"><mn id="S5.SS3.p9.1.m1.1.1" xref="S5.SS3.p9.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p9.1.m1.1b"><cn type="integer" id="S5.SS3.p9.1.m1.1.1.cmml" xref="S5.SS3.p9.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p9.1.m1.1c">60</annotation></semantics></math> video sequences (more than <math id="S5.SS3.p9.2.m2.2" class="ltx_Math" alttext="51,000" display="inline"><semantics id="S5.SS3.p9.2.m2.2a"><mrow id="S5.SS3.p9.2.m2.2.3.2" xref="S5.SS3.p9.2.m2.2.3.1.cmml"><mn id="S5.SS3.p9.2.m2.1.1" xref="S5.SS3.p9.2.m2.1.1.cmml">51</mn><mo id="S5.SS3.p9.2.m2.2.3.2.1" xref="S5.SS3.p9.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS3.p9.2.m2.2.2" xref="S5.SS3.p9.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p9.2.m2.2b"><list id="S5.SS3.p9.2.m2.2.3.1.cmml" xref="S5.SS3.p9.2.m2.2.3.2"><cn type="integer" id="S5.SS3.p9.2.m2.1.1.cmml" xref="S5.SS3.p9.2.m2.1.1">51</cn><cn type="integer" id="S5.SS3.p9.2.m2.2.2.cmml" xref="S5.SS3.p9.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p9.2.m2.2c">51,000</annotation></semantics></math> frames) with daily actions including walking in the city, going up-stairs, having coffee or taking the bus.</p>
</div>
<div id="S5.SS3.p10" class="ltx_para">
<p id="S5.SS3.p10.2" class="ltx_p">In addition to the datasets collected with MoCap systems, there are other approaches to create a dataset for 3D human pose estimation.
<span id="S5.SS3.p10.2.1" class="ltx_text ltx_font_bold">JTA</span> (Joint Track Auto) <cite class="ltx_cite ltx_citemacro_citep">(Fabbri et al., <a href="#bib.bib35" title="" class="ltx_ref">2018</a>)</cite> is a fully synthetic dataset generated from highly photorealistic video game Grand Theft Auto V. It contains almost <math id="S5.SS3.p10.1.m1.1" class="ltx_Math" alttext="10M" display="inline"><semantics id="S5.SS3.p10.1.m1.1a"><mrow id="S5.SS3.p10.1.m1.1.1" xref="S5.SS3.p10.1.m1.1.1.cmml"><mn id="S5.SS3.p10.1.m1.1.1.2" xref="S5.SS3.p10.1.m1.1.1.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="S5.SS3.p10.1.m1.1.1.1" xref="S5.SS3.p10.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS3.p10.1.m1.1.1.3" xref="S5.SS3.p10.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p10.1.m1.1b"><apply id="S5.SS3.p10.1.m1.1.1.cmml" xref="S5.SS3.p10.1.m1.1.1"><times id="S5.SS3.p10.1.m1.1.1.1.cmml" xref="S5.SS3.p10.1.m1.1.1.1"></times><cn type="integer" id="S5.SS3.p10.1.m1.1.1.2.cmml" xref="S5.SS3.p10.1.m1.1.1.2">10</cn><ci id="S5.SS3.p10.1.m1.1.1.3.cmml" xref="S5.SS3.p10.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p10.1.m1.1c">10M</annotation></semantics></math> annotated body poses and over <math id="S5.SS3.p10.2.m2.2" class="ltx_Math" alttext="460,800" display="inline"><semantics id="S5.SS3.p10.2.m2.2a"><mrow id="S5.SS3.p10.2.m2.2.3.2" xref="S5.SS3.p10.2.m2.2.3.1.cmml"><mn id="S5.SS3.p10.2.m2.1.1" xref="S5.SS3.p10.2.m2.1.1.cmml">460</mn><mo id="S5.SS3.p10.2.m2.2.3.2.1" xref="S5.SS3.p10.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS3.p10.2.m2.2.2" xref="S5.SS3.p10.2.m2.2.2.cmml">800</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p10.2.m2.2b"><list id="S5.SS3.p10.2.m2.2.3.1.cmml" xref="S5.SS3.p10.2.m2.2.3.2"><cn type="integer" id="S5.SS3.p10.2.m2.1.1.cmml" xref="S5.SS3.p10.2.m2.1.1">460</cn><cn type="integer" id="S5.SS3.p10.2.m2.2.2.cmml" xref="S5.SS3.p10.2.m2.2.2">800</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p10.2.m2.2c">460,800</annotation></semantics></math> densely annotated frames.
In <span id="S5.SS3.p10.2.2" class="ltx_text ltx_font_bold">Human3D+</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite>, the training images are obtained by integrating real background images and 3D textured models which generated from SCAPE model <cite class="ltx_cite ltx_citemacro_citep">(Anguelov et al., <a href="#bib.bib5" title="" class="ltx_ref">2005</a>)</cite> with different texture deformation. The parameters for generating basic SCAPE models are captured from a MoCap system, or inferred from human-annotated 2D poses.
<span id="S5.SS3.p10.2.3" class="ltx_text ltx_font_bold">SURREAL</span> (Synthetic hUmans foR REAL) <cite class="ltx_cite ltx_citemacro_citep">(Varol et al., <a href="#bib.bib168" title="" class="ltx_ref">2017</a>)</cite> contains videos of single synthetic people with real unchanged background. It contains annotations of body parts segmentation, depth, optical flow, and surface normals. The dataset employs the SMPL body model for generating body poses and shapes.
<span id="S5.SS3.p10.2.4" class="ltx_text ltx_font_bold">LSP-MPII-Ordinal</span> <cite class="ltx_cite ltx_citemacro_citep">(Pavlakos et al., <a href="#bib.bib124" title="" class="ltx_ref">2018a</a>)</cite> is an extension of two 2D human pose datasets (LSP <cite class="ltx_cite ltx_citemacro_citep">(Johnson and Everingham, <a href="#bib.bib68" title="" class="ltx_ref">2010</a>)</cite> and MPII <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite>) by adding the ordinal depth relation for each pair of joints.
<span id="S5.SS3.p10.2.5" class="ltx_text ltx_font_bold">UP-3D</span> <cite class="ltx_cite ltx_citemacro_citep">(Lassner et al., <a href="#bib.bib79" title="" class="ltx_ref">2017</a>)</cite> is a combination of color images from 2D human pose benchmarks like LSP <cite class="ltx_cite ltx_citemacro_citep">(Johnson and Everingham, <a href="#bib.bib68" title="" class="ltx_ref">2010</a>)</cite> and MPII <cite class="ltx_cite ltx_citemacro_citep">(Andriluka et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite> and human body model SMPL <cite class="ltx_cite ltx_citemacro_citet">Bogo et al. (<a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite>. The 3D human shape candidates are fit to color images by human annotators.
<span id="S5.SS3.p10.2.6" class="ltx_text ltx_font_bold">DensePose</span> <cite class="ltx_cite ltx_citemacro_citep">(Güler et al., <a href="#bib.bib49" title="" class="ltx_ref">2018</a>)</cite> is an extension on 50K COCO images with people. All RGB images are manually annotated with surface-based representations of the human body.
<span id="S5.SS3.p10.2.7" class="ltx_text ltx_font_bold">AMASS Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Mahmood et al., <a href="#bib.bib100" title="" class="ltx_ref">2019</a>)</cite> unifies 15 different optical marker-based human motion capture datasets with SMPL <cite class="ltx_cite ltx_citemacro_citet">Loper et al. (<a href="#bib.bib96" title="" class="ltx_ref">2015</a>)</cite> body model as a standard fitting representation for human skeleton and surface mesh. Each body joint in this rich dataset has 3 rotational Degrees of Freedom (DoF) which are parametrized with exponential coordinates.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Evaluation Metrics of 3D human pose estimation</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">There are several evaluation metrics for 3D human pose estimation with different limitation factors. Note that we only list widely used evaluation metrics as below.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_bold">Mean Per Joint Position Error (MPJPE)</span> is the most widely used measures to evaluate the performance of 3D pose estimation. It calculates the Euclidean distance from the estimated 3D joints to the ground truth in millimeters, averaged over all joints in one image. In the case of a set of frames, the mean error is averaged over all frames. For different datasets and different protocols, there are different data post-processing of estimated joints before computing the MPJPE. For example, in the protocol #1 of Human3.6M, the MPJPE is calculated after aligning the depths of the root joints (generally pelvis joint) <cite class="ltx_cite ltx_citemacro_citep">(Tome et al., <a href="#bib.bib162" title="" class="ltx_ref">2017</a>; Yang et al., <a href="#bib.bib179" title="" class="ltx_ref">2018</a>)</cite>, which is also called N-MPJPE <cite class="ltx_cite ltx_citemacro_citep">(Rhodin et al., <a href="#bib.bib139" title="" class="ltx_ref">2018a</a>)</cite>. The MPJPE in HumanEva-I and the protocol #2 &amp; #3 of Human3.6M is calculated after the alignment of predictions and ground truth with a rigid transformation using Procrustes Analysis <cite class="ltx_cite ltx_citemacro_citep">(Gower, <a href="#bib.bib48" title="" class="ltx_ref">1975</a>)</cite>, which is also called reconstruction error <cite class="ltx_cite ltx_citemacro_citep">(Kanazawa et al., <a href="#bib.bib73" title="" class="ltx_ref">2018</a>; Pavlakos et al., <a href="#bib.bib126" title="" class="ltx_ref">2018b</a>)</cite>, P-MPJPE <cite class="ltx_cite ltx_citemacro_citep">(Rhodin et al., <a href="#bib.bib139" title="" class="ltx_ref">2018a</a>)</cite> or PA-MPJPE <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a href="#bib.bib153" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p"><span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_bold">Percentage of Correct Keypoints (PCK)</span> and <span id="S5.SS4.p3.1.2" class="ltx_text ltx_font_bold">Area Under the Curve
(AUC)</span> are suggested by <cite class="ltx_cite ltx_citemacro_citep">(Mehta et al., <a href="#bib.bib104" title="" class="ltx_ref">2017a</a>)</cite> for 3D pose evaluation similar to PCK and AUC in MPII for 2D pose evaluation. PCK counts the percentage of points that fall in a threshold also called <span id="S5.SS4.p3.1.3" class="ltx_text ltx_font_bold">3DPCK</span>, and AUC is computed by a range of PCK thresholds. The general threshold in 3D space is 150mm, corresponding to roughly half of the head size.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">In addition to the evaluation metrics for 3D joint coordinates, there is another evaluation measurement <span id="S5.SS4.p4.1.1" class="ltx_text ltx_font_bold">Mean Per-vertex Error</span> to report the results of 3D body shape which report the error between predicted and ground truth meshes <cite class="ltx_cite ltx_citemacro_citep">(Varol et al., <a href="#bib.bib167" title="" class="ltx_ref">2018</a>; Pavlakos et al., <a href="#bib.bib126" title="" class="ltx_ref">2018b</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Research Directions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Human pose estimation is a hot research area in computer vision that evolved recently along with the blooming of deep learning. Due to limitations in hardware device capability and the quantity and quality of training data, early networks are relatively shallow, used in a very straightforward way and can only handle small images or patches <cite class="ltx_cite ltx_citemacro_citep">(Toshev and Szegedy, <a href="#bib.bib165" title="" class="ltx_ref">2014</a>; Tompson et al., <a href="#bib.bib163" title="" class="ltx_ref">2015</a>; Li and Chan, <a href="#bib.bib87" title="" class="ltx_ref">2014</a>)</cite>. More recent networks are more powerful, deeper and efficient <cite class="ltx_cite ltx_citemacro_citep">(Newell et al., <a href="#bib.bib115" title="" class="ltx_ref">2016</a>; Cao et al., <a href="#bib.bib13" title="" class="ltx_ref">2016</a>; He et al., <a href="#bib.bib51" title="" class="ltx_ref">2017</a>; Sun et al., <a href="#bib.bib151" title="" class="ltx_ref">2019</a>)</cite>. In this paper, we have reviewed the recent deep learning-based research addressing the 2D/3D human pose estimation problem from monocular images or video footage and organize approaches into four categories based on specific tasks: (1) 2D single person pose estimation, (2) 2D multi-person pose estimation, (3) 3D single person pose estimation, and (4) 3D multi-person pose estimation. Further, we have summarized the popular human pose datasets and evaluation protocols.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Despite the great development of monocular human pose estimation with deep learning, there still remain some unresolved challenges and gap between research and practical applications, such as the influence of body part occlusion and crowded people. Efficient networks and adequate training data are the most important requirements for deep learning-based approaches.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Future networks should explore both global and local contexts for more discriminative features of the human body while exploiting human body structures into the network for prior constraints. Current networks have validated some effective network design tricks such as multi-stage structure, intermediate supervision, multi-scale feature fusion, multi-task learning, body structure constrains. Network efficiency is also a very important factor to apply algorithms in real-life applications.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Diversity data can improve the robustness of networks to handle complex scenes with irregular poses, occluded body limbs and crowded people. Data collection for specific complex scenarios is an option and there are other ways to extend existing datasets. Synthetic technology can theoretically generate unlimited data while there is a domain gap between synthetic data and real data. Cross-dataset supplementation, especially to supplement 3D datasets with 2D datasets can mitigate the problem of insufficient diversity of training data.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This manuscript is based upon the work supported by National Science Foundation (NSF) under award number IIS-1400802 and National Natural Science Foundation of China (61420106007, 61671387). Yucheng Chen’s contribution was made when he was a visiting student at the City University of New York, sponsored by the Chinese Scholarship Council.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aggarwal and Cai (1999)</span>
<span class="ltx_bibblock">
Aggarwal, J.K., Cai, Q.,
1999.

</span>
<span class="ltx_bibblock">Human motion analysis: A review.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
73, 428–440.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ainsworth et al. (2011)</span>
<span class="ltx_bibblock">
Ainsworth, B.E., Haskell, W.L.,
Herrmann, S.D., Meckes, N.,
Bassett Jr, D.R., Tudor-Locke, C.,
Greer, J.L., Vezina, J.,
Whitt-Glover, M.C., Leon, A.S.,
2011.

</span>
<span class="ltx_bibblock">2011 compendium of physical activities: a second
update of codes and met values.

</span>
<span class="ltx_bibblock">Medicine &amp; science in sports &amp; exercise
43, 1575–1581.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andriluka et al. (2018)</span>
<span class="ltx_bibblock">
Andriluka, M., Iqbal, U.,
Milan, A., Insafutdinov, E.,
Pishchulin, L., Gall, J.,
Schiele, B., 2018.

</span>
<span class="ltx_bibblock">Posetrack: A benchmark for human pose estimation and
tracking, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 5167–5176.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andriluka et al. (2014)</span>
<span class="ltx_bibblock">
Andriluka, M., Pishchulin, L.,
Gehler, P., Schiele, B.,
2014.

</span>
<span class="ltx_bibblock">2d human pose estimation: New benchmark and state of
the art analysis, in: Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3686–3693.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anguelov et al. (2005)</span>
<span class="ltx_bibblock">
Anguelov, D., Srinivasan, P.,
Koller, D., Thrun, S.,
Rodgers, J., Davis, J.,
2005.

</span>
<span class="ltx_bibblock">Scape: shape completion and animation of people, in:
ACM transactions on graphics,
ACM. pp. 408–416.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arnab et al. (2019)</span>
<span class="ltx_bibblock">
Arnab, A., Doersch, C.,
Zisserman, A., 2019.

</span>
<span class="ltx_bibblock">Exploiting temporal context for 3d human pose
estimation in the wild, in: Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pp. 3395–3404.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belagiannis and Zisserman (2017)</span>
<span class="ltx_bibblock">
Belagiannis, V., Zisserman, A.,
2017.

</span>
<span class="ltx_bibblock">Recurrent human pose estimation, in:
Proc. IEEE Conference on Automatic Face and Gesture
Recognition, IEEE. pp. 468–475.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bogo et al. (2016)</span>
<span class="ltx_bibblock">
Bogo, F., Kanazawa, A.,
Lassner, C., Gehler, P.,
Romero, J., Black, M.J.,
2016.

</span>
<span class="ltx_bibblock">Keep it smpl: Automatic estimation of 3d human pose
and shape from a single image, in: Proc. European
Conference on Computer Vision, Springer. pp.
561–578.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bogo et al. (2014)</span>
<span class="ltx_bibblock">
Bogo, F., Romero, J.,
Loper, M., Black, M.J.,
2014.

</span>
<span class="ltx_bibblock">FAUST: Dataset and evaluation for 3D mesh
registration, in: Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3794–3801.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bogo et al. (2017)</span>
<span class="ltx_bibblock">
Bogo, F., Romero, J.,
Pons-Moll, G., Black, M.J.,
2017.

</span>
<span class="ltx_bibblock">Dynamic FAUST: Registering human bodies in
motion, in: Proc. IEEE Conference on Computer Vision and
Pattern Recognition, pp. 6233–6242.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bourdev and Malik (2009)</span>
<span class="ltx_bibblock">
Bourdev, L., Malik, J.,
2009.

</span>
<span class="ltx_bibblock">Poselets: Body part detectors trained using 3d human
pose annotations, in: Proc. IEEE International
Conference on Computer Vision, IEEE. pp.
1365–1372.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulat and Tzimiropoulos (2016)</span>
<span class="ltx_bibblock">
Bulat, A., Tzimiropoulos, G.,
2016.

</span>
<span class="ltx_bibblock">Human pose estimation via convolutional part heatmap
regression, in: Proc. European Conference on Computer
Vision, Springer. pp. 717–732.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2016)</span>
<span class="ltx_bibblock">
Cao, Z., Simon, T., Wei,
S.E., Sheikh, Y., 2016.

</span>
<span class="ltx_bibblock">Realtime multi-person 2d pose estimation using part
affinity fields.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1611.08050 .

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira et al. (2016)</span>
<span class="ltx_bibblock">
Carreira, J., Agrawal, P.,
Fragkiadaki, K., Malik, J.,
2016.

</span>
<span class="ltx_bibblock">Human pose estimation with iterative error feedback,
in: Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4733–4742.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Charles et al. (2014)</span>
<span class="ltx_bibblock">
Charles, J., Pfister, T.,
Everingham, M., Zisserman, A.,
2014.

</span>
<span class="ltx_bibblock">Automatic and efficient human pose estimation for
sign language videos.

</span>
<span class="ltx_bibblock">International Journal of Computer Vision
110, 70–90.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Charles et al. (2016)</span>
<span class="ltx_bibblock">
Charles, J., Pfister, T.,
Magee, D., Hogg, D.,
Zisserman, A., 2016.

</span>
<span class="ltx_bibblock">Personalizing human video pose estimation, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3063–3072.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Ramanan (2017)</span>
<span class="ltx_bibblock">
Chen, C.H., Ramanan, D.,
2017.

</span>
<span class="ltx_bibblock">3d human pose estimation= 2d pose estimation+
matching, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 7035–7043.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2013)</span>
<span class="ltx_bibblock">
Chen, L., Wei, H.,
Ferryman, J., 2013.

</span>
<span class="ltx_bibblock">A survey of human motion analysis using depth
imagery.

</span>
<span class="ltx_bibblock">Pattern Recognition Letters 34,
1995–2006.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2016)</span>
<span class="ltx_bibblock">
Chen, W., Wang, H., Li,
Y., Su, H., Wang, Z.,
Tu, C., Lischinski, D.,
Cohen-Or, D., Chen, B.,
2016.

</span>
<span class="ltx_bibblock">Synthesizing training images for boosting human 3d
pose estimation, in: Proc. IEEE International Conference
on 3D Vision, IEEE. pp. 479–488.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Yuille (2014)</span>
<span class="ltx_bibblock">
Chen, X., Yuille, A.L.,
2014.

</span>
<span class="ltx_bibblock">Articulated pose estimation by a graphical model with
image dependent pairwise relations, in: Advances in
neural information processing systems, pp. 1736–1744.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2017)</span>
<span class="ltx_bibblock">
Chen, Y., Shen, C., Wei,
X.S., Liu, L., Yang, J.,
2017.

</span>
<span class="ltx_bibblock">Adversarial posenet: A structure-aware convolutional
network for human pose estimation.

</span>
<span class="ltx_bibblock">CoRR, abs/1705.00389 2.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2018)</span>
<span class="ltx_bibblock">
Chen, Y., Wang, Z., Peng,
Y., Zhang, Z., Yu, G.,
Sun, J., 2018.

</span>
<span class="ltx_bibblock">Cascaded pyramid network for multi-person pose
estimation, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 7103–7112.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chou et al. (2017)</span>
<span class="ltx_bibblock">
Chou, C.J., Chien, J.T.,
Chen, H.T., 2017.

</span>
<span class="ltx_bibblock">Self adversarial training for human pose estimation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1707.02439 .

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. (2016)</span>
<span class="ltx_bibblock">
Chu, X., Ouyang, W., Li,
H., Wang, X., 2016.

</span>
<span class="ltx_bibblock">Structured feature learning for pose estimation, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4715–4723.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. (2017)</span>
<span class="ltx_bibblock">
Chu, X., Yang, W., Ouyang,
W., Ma, C., Yuille, A.L.,
Wang, X., 2017.

</span>
<span class="ltx_bibblock">Multi-context attention for human pose estimation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1702.07432
1.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cootes et al. (1995)</span>
<span class="ltx_bibblock">
Cootes, T.F., Taylor, C.J.,
Cooper, D.H., Graham, J.,
1995.

</span>
<span class="ltx_bibblock">Active shape models-their training and application.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
61, 38–59.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dantone et al. (2013)</span>
<span class="ltx_bibblock">
Dantone, M., Gall, J.,
Leistner, C., Van Gool, L.,
2013.

</span>
<span class="ltx_bibblock">Human pose estimation using body parts dependent
joint regressors, in: Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3041–3048.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Debnath et al. (2018)</span>
<span class="ltx_bibblock">
Debnath, B., O’Brien, M.,
Yamaguchi, M., Behera, A.,
2018.

</span>
<span class="ltx_bibblock">Adapting mobilenets for mobile based upper body pose
estimation, in: Proc. IEEE Conference on Advanced Video
and Signal Based Surveillance, pp. 1–6.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eichner and Ferrari (2010)</span>
<span class="ltx_bibblock">
Eichner, M., Ferrari, V.,
2010.

</span>
<span class="ltx_bibblock">We are family: Joint pose estimation of multiple
persons, in: Proc. European Conference on Computer
Vision, Springer. pp. 228–242.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eichner and Ferrari (2012a)</span>
<span class="ltx_bibblock">
Eichner, M., Ferrari, V.,
2012a.

</span>
<span class="ltx_bibblock">Calvin upper-body detector v1.04.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://groups.inf.ed.ac.uk/calvin/calvin_upperbody_detector/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://groups.inf.ed.ac.uk/calvin/calvin_upperbody_detector/</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eichner and Ferrari (2012b)</span>
<span class="ltx_bibblock">
Eichner, M., Ferrari, V.,
2012b.

</span>
<span class="ltx_bibblock">Human pose co-estimation and applications.

</span>
<span class="ltx_bibblock">IEEE transactions on pattern analysis and machine
intelligence 34, 2282–2288.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eichner et al. (2009)</span>
<span class="ltx_bibblock">
Eichner, M., Ferrari, V.,
Zurich, S., 2009.

</span>
<span class="ltx_bibblock">Better appearance models for pictorial structures,
in: Proc. British Machine Vision Conference,
p. 5.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elhayek et al. (2017)</span>
<span class="ltx_bibblock">
Elhayek, A., de Aguiar, E.,
Jain, A., Thompson, J.,
Pishchulin, L., Andriluka, M.,
Bregler, C., Schiele, B.,
Theobalt, C., 2017.

</span>
<span class="ltx_bibblock">Marconi—convnet-based marker-less motion capture in
outdoor and indoor scenes.

</span>
<span class="ltx_bibblock">IEEE transactions on pattern analysis and machine
intelligence 39, 501–514.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everingham et al. (2010)</span>
<span class="ltx_bibblock">
Everingham, M., Van Gool, L.,
Williams, C.K., Winn, J.,
Zisserman, A., 2010.

</span>
<span class="ltx_bibblock">The pascal visual object classes (voc) challenge.

</span>
<span class="ltx_bibblock">International journal of computer vision
88, 303–338.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabbri et al. (2018)</span>
<span class="ltx_bibblock">
Fabbri, M., Lanzi, F.,
Calderara, S., Palazzi, A.,
Vezzani, R., Cucchiara, R.,
2018.

</span>
<span class="ltx_bibblock">Learning to detect and track visible and occluded
body joints in a virtual world, in: Proc. European
Conference on Computer Vision, pp. 430–446.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Faessler et al. (2014)</span>
<span class="ltx_bibblock">
Faessler, M., Mueggler, E.,
Schwabe, K., Scaramuzza, D.,
2014.

</span>
<span class="ltx_bibblock">A monocular pose estimation system based on infrared
leds, in: Proc. IEEE International Conference on
Robotics and Automation, IEEE. pp.
907–913.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2015)</span>
<span class="ltx_bibblock">
Fan, X., Zheng, K., Lin,
Y., Wang, S., 2015.

</span>
<span class="ltx_bibblock">Combining local appearance and holistic view:
Dual-source deep neural networks for human pose estimation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1504.07159 .

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2017)</span>
<span class="ltx_bibblock">
Fang, H., Xie, S., Tai,
Y.W., Lu, C., 2017.

</span>
<span class="ltx_bibblock">Rmpe: Regional multi-person pose estimation, in:
Proc. IEEE International Conference on Computer Vision,
pp. 2334–2343.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Felzenszwalb and Huttenlocher (2005)</span>
<span class="ltx_bibblock">
Felzenszwalb, P.F., Huttenlocher, D.P.,
2005.

</span>
<span class="ltx_bibblock">Pictorial structures for object recognition.

</span>
<span class="ltx_bibblock">International journal of computer vision
61, 55–79.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2019)</span>
<span class="ltx_bibblock">
Feng, Z., Xiatian, Z.,
Mao, Y., 2019.

</span>
<span class="ltx_bibblock">Fast human pose estimation, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1–8.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferrari et al. (2008)</span>
<span class="ltx_bibblock">
Ferrari, V., Marin-Jimenez, M.,
Zisserman, A., 2008.

</span>
<span class="ltx_bibblock">Progressive search space reduction for human pose
estimation, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1–8.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gavrila (1999)</span>
<span class="ltx_bibblock">
Gavrila, D.M., 1999.

</span>
<span class="ltx_bibblock">The visual analysis of human movement: A survey.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
73, 82–98.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gkioxari et al. (2013)</span>
<span class="ltx_bibblock">
Gkioxari, G., Arbelaez, P.,
Bourdev, L., Malik, J.,
2013.

</span>
<span class="ltx_bibblock">Articulated pose estimation using discriminative
armlet classifiers, in: Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pp. 3342–3349.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gkioxari et al. (2014a)</span>
<span class="ltx_bibblock">
Gkioxari, G., Hariharan, B.,
Girshick, R., Malik, J.,
2014a.

</span>
<span class="ltx_bibblock">R-cnns for pose estimation and action detection.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1406.5212 .

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gkioxari et al. (2014b)</span>
<span class="ltx_bibblock">
Gkioxari, G., Hariharan, B.,
Girshick, R., Malik, J.,
2014b.

</span>
<span class="ltx_bibblock">Using k-poselets for detecting people and localizing
their keypoints, in: Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3582–3589.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gkioxari et al. (2016)</span>
<span class="ltx_bibblock">
Gkioxari, G., Toshev, A.,
Jaitly, N., 2016.

</span>
<span class="ltx_bibblock">Chained predictions using convolutional neural
networks, in: Proc. European Conference on Computer
Vision, Springer. pp. 728–743.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. (2016)</span>
<span class="ltx_bibblock">
Gong, W., Zhang, X.,
Gonzàlez, J., Sobral, A.,
Bouwmans, T., Tu, C.,
Zahzah, E.h., 2016.

</span>
<span class="ltx_bibblock">Human pose estimation from monocular images: A
comprehensive survey.

</span>
<span class="ltx_bibblock">Sensors 16,
1966.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gower (1975)</span>
<span class="ltx_bibblock">
Gower, J.C., 1975.

</span>
<span class="ltx_bibblock">Generalized procrustes analysis.

</span>
<span class="ltx_bibblock">Psychometrika 40,
33–51.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Güler et al. (2018)</span>
<span class="ltx_bibblock">
Güler, R.A., Neverova, N.,
Kokkinos, I., 2018.

</span>
<span class="ltx_bibblock">Densepose: Dense human pose estimation in the wild,
in: Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 7297–7306.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasler et al. (2009)</span>
<span class="ltx_bibblock">
Hasler, N., Stoll, C.,
Sunkel, M., Rosenhahn, B.,
Seidel, H.P., 2009.

</span>
<span class="ltx_bibblock">A statistical model of human pose and body shape,
in: Computer Graphics Forum,
Wiley Online Library. pp. 337–346.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2017)</span>
<span class="ltx_bibblock">
He, K., Gkioxari, G.,
Dollár, P., Girshick, R.,
2017.

</span>
<span class="ltx_bibblock">Mask r-cnn, in: Proc. IEEE
International Conference on Computer Vision, IEEE.
pp. 2980–2988.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holte et al. (2012)</span>
<span class="ltx_bibblock">
Holte, M.B., Tran, C.,
Trivedi, M.M., Moeslund, T.B.,
2012.

</span>
<span class="ltx_bibblock">Human pose estimation and activity recognition from
multi-view videos: Comparative explorations of recent developments.

</span>
<span class="ltx_bibblock">IEEE Journal of selected topics in signal
processing 6, 538–552.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard et al. (2017)</span>
<span class="ltx_bibblock">
Howard, A.G., Zhu, M.,
Chen, B., Kalenichenko, D.,
Wang, W., Weyand, T.,
Andreetto, M., Adam, H.,
2017.

</span>
<span class="ltx_bibblock">Mobilenets: Efficient convolutional neural networks
for mobile vision applications.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1704.04861 .

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2004)</span>
<span class="ltx_bibblock">
Hu, W., Tan, T., Wang,
L., Maybank, S., 2004.

</span>
<span class="ltx_bibblock">A survey on visual surveillance of object motion and
behaviors.

</span>
<span class="ltx_bibblock">IEEE Transactions on Systems, Man, and Cybernetics,
Part C (Applications and Reviews) 34,
334–352.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2017)</span>
<span class="ltx_bibblock">
Huang, S., Gong, M., Tao,
D., 2017.

</span>
<span class="ltx_bibblock">A coarse-fine network for keypoint localization, in:
Proc. IEEE International Conference on Computer Vision,
pp. 3028–3037.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">INRIA4D (accessed on 2019)</span>
<span class="ltx_bibblock">
INRIA4D, accessed on 2019.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://4drepository.inrialpes.fr" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://4drepository.inrialpes.fr</a>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Insafutdinov et al. (2017)</span>
<span class="ltx_bibblock">
Insafutdinov, E., Andriluka, M.,
Pishchulin, L., Tang, S.,
Levinkov, E., Andres, B.,
Schiele, B., 2017.

</span>
<span class="ltx_bibblock">Arttrack: Articulated multi-person tracking in the
wild, in: Proc. IEEE Conference on Computer Vision and
Pattern Recognition, pp. 6457–6465.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Insafutdinov et al. (2016)</span>
<span class="ltx_bibblock">
Insafutdinov, E., Pishchulin, L.,
Andres, B., Andriluka, M.,
Schiele, B., 2016.

</span>
<span class="ltx_bibblock">Deepercut: A deeper, stronger, and faster
multi-person pose estimation model, in: Proc. European
Conference on Computer Vision, Springer. pp.
34–50.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ionescu et al. (2014)</span>
<span class="ltx_bibblock">
Ionescu, C., Papava, D.,
Olaru, V., Sminchisescu, C.,
2014.

</span>
<span class="ltx_bibblock">Human3.6m: Large scale datasets and predictive
methods for 3d human sensing in natural environments.

</span>
<span class="ltx_bibblock">IEEE Transactions on Pattern Analysis and Machine
Intelligence 36, 1325–1339.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iqbal and Gall (2016)</span>
<span class="ltx_bibblock">
Iqbal, U., Gall, J., 2016.

</span>
<span class="ltx_bibblock">Multi-person pose estimation with local
joint-to-person associations, in: Proc. European
Conference on Computer Vision, Springer. pp.
627–642.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iqbal et al. (2016)</span>
<span class="ltx_bibblock">
Iqbal, U., Milan, A.,
Gall, J., 2016.

</span>
<span class="ltx_bibblock">Posetrack: Joint multi-person pose estimation and
tracking.

</span>
<span class="ltx_bibblock">arXiv:1611.07727 .

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaderberg et al. (2015)</span>
<span class="ltx_bibblock">
Jaderberg, M., Simonyan, K.,
Zisserman, A., et al., 2015.

</span>
<span class="ltx_bibblock">Spatial transformer networks, in:
Advances in neural information processing systems, pp.
2017–2025.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. (2013)</span>
<span class="ltx_bibblock">
Jain, A., Tompson, J.,
Andriluka, M., Taylor, G.W.,
Bregler, C., 2013.

</span>
<span class="ltx_bibblock">Learning human pose estimation features with
convolutional networks.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1312.7302 .

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. (2014)</span>
<span class="ltx_bibblock">
Jain, A., Tompson, J.,
LeCun, Y., Bregler, C.,
2014.

</span>
<span class="ltx_bibblock">Modeep: A deep learning framework using motion
features for human pose estimation, in: Proc. Asian
conference on computer vision, Springer. pp.
302–315.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jhuang et al. (2013)</span>
<span class="ltx_bibblock">
Jhuang, H., Gall, J.,
Zuffi, S., Schmid, C.,
Black, M.J., 2013.

</span>
<span class="ltx_bibblock">Towards understanding action recognition, in:
Proc. IEEE International Conference on Computer Vision,
pp. 3192–3199.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jhuang et al. (2011)</span>
<span class="ltx_bibblock">
Jhuang, H., Garrote, H.,
Poggio, E., Serre, T.,
Hmdb, T., 2011.

</span>
<span class="ltx_bibblock">A large video database for human motion recognition,
in: Proc. IEEE International Conference on Computer
Vision, p. 6.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji and Liu (2010)</span>
<span class="ltx_bibblock">
Ji, X., Liu, H., 2010.

</span>
<span class="ltx_bibblock">Advances in view-invariant human motion analysis: A
review.

</span>
<span class="ltx_bibblock">IEEE Transactions on Systems, Man, and Cybernetics,
Part C (Applications and Reviews) 40,
13–24.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson and Everingham (2010)</span>
<span class="ltx_bibblock">
Johnson, S., Everingham, M.,
2010.

</span>
<span class="ltx_bibblock">Clustered pose and nonlinear appearance models for
human pose estimation, in: Proc. British Machine Vision
Conference, p. 5.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson and Everingham (2011)</span>
<span class="ltx_bibblock">
Johnson, S., Everingham, M.,
2011.

</span>
<span class="ltx_bibblock">Learning effective human pose estimation from
inaccurate annotation, in: Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1465–1472.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joo et al. (2017)</span>
<span class="ltx_bibblock">
Joo, H., Simon, T., Li,
X., Liu, H., Tan, L.,
Gui, L., Banerjee, S.,
Godisart, T.S., Nabbe, B.,
Matthews, I., Kanade, T.,
Nobuhara, S., Sheikh, Y.,
2017.

</span>
<span class="ltx_bibblock">Panoptic studio: A massively multiview system for
social interaction capture.

</span>
<span class="ltx_bibblock">IEEE Transactions on Pattern Analysis and Machine
Intelligence 41, 190–204.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joo et al. (2018)</span>
<span class="ltx_bibblock">
Joo, H., Simon, T.,
Sheikh, Y., 2018.

</span>
<span class="ltx_bibblock">Total capture: A 3d deformation model for tracking
faces, hands, and bodies, in: Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pp. 8320–8329.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ju et al. (1996)</span>
<span class="ltx_bibblock">
Ju, S.X., Black, M.J.,
Yacoob, Y., 1996.

</span>
<span class="ltx_bibblock">Cardboard people: A parameterized model of
articulated image motion, in: Proc. IEEE Conference on
Automatic Face and Gesture Recognition, pp. 38–44.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kanazawa et al. (2018)</span>
<span class="ltx_bibblock">
Kanazawa, A., Black, M.J.,
Jacobs, D.W., Malik, J.,
2018.

</span>
<span class="ltx_bibblock">End-to-end recovery of human shape and pose, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 7122–7131.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al. (2018)</span>
<span class="ltx_bibblock">
Ke, L., Chang, M.C., Qi,
H., Lyu, S., 2018.

</span>
<span class="ltx_bibblock">Multi-scale structure-aware network for human pose
estimation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1803.09894 .

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kinect (accessed on 2019)</span>
<span class="ltx_bibblock">
Kinect, accessed on 2019.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://developer.microsoft.com/en-us/windows/kinect" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.microsoft.com/en-us/windows/kinect</a>.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocabas et al. (2018)</span>
<span class="ltx_bibblock">
Kocabas, M., Karagoz, S.,
Akbas, E., 2018.

</span>
<span class="ltx_bibblock">Multiposenet: Fast multi-person pose estimation using
pose residual network, in: Proc. European Conference on
Computer Vision, Springer. pp.
437–453.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreiss et al. (2019)</span>
<span class="ltx_bibblock">
Kreiss, S., Bertoni, L.,
Alahi, A., 2019.

</span>
<span class="ltx_bibblock">Pifpaf: Composite fields for human pose estimation,
in: Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 11977–11986.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. (2012)</span>
<span class="ltx_bibblock">
Krizhevsky, A., Sutskever, I.,
Hinton, G.E., 2012.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional
neural networks, in: Advances in neural information
processing systems, pp. 1097–1105.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lassner et al. (2017)</span>
<span class="ltx_bibblock">
Lassner, C., Romero, J.,
Kiefel, M., Bogo, F.,
Black, M.J., Gehler, P.V.,
2017.

</span>
<span class="ltx_bibblock">Unite the people: Closing the loop between 3d and 2d
human representations, in: Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4704–4713.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2017a)</span>
<span class="ltx_bibblock">
Li, B., Chen, H., Chen,
Y., Dai, Y., He, M.,
2017a.

</span>
<span class="ltx_bibblock">Skeleton boxes: Solving skeleton based action
detection with a single deep convolutional neural network, in:
Proc. IEEE International Conference on Multimedia and
Expo Workshops, pp. 613–616.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2017b)</span>
<span class="ltx_bibblock">
Li, B., Dai, Y., Cheng,
X., Chen, H., Lin, Y.,
He, M., 2017b.

</span>
<span class="ltx_bibblock">Skeleton based action recognition using
translation-scale invariant image mapping and multi-scale deep cnn, in:
Proc. IEEE International Conference on Multimedia and
Expo Workshops, pp. 601–604.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2018a)</span>
<span class="ltx_bibblock">
Li, B., Dai, Y., He, M.,
2018a.

</span>
<span class="ltx_bibblock">Monocular depth estimation with hierarchical fusion
of dilated cnns and soft-weighted-sum inference.

</span>
<span class="ltx_bibblock">Pattern Recognition 83,
328–339.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2018b)</span>
<span class="ltx_bibblock">
Li, B., He, M., Dai, Y.,
Cheng, X., Chen, Y.,
2018b.

</span>
<span class="ltx_bibblock">3d skeleton based action recognition by video-domain
translation-scale invariant mapping and multi-scale dilated cnn.

</span>
<span class="ltx_bibblock">Multimedia Tools and Applications
77, 22901–22921.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2015a)</span>
<span class="ltx_bibblock">
Li, B., Shen, C., Dai,
Y., Hengel, A., He, M.,
2015a.

</span>
<span class="ltx_bibblock">Depth and surface normal estimation from monocular
images using regression on deep features and hierarchical crfs, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1119–1127.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Lee (2019)</span>
<span class="ltx_bibblock">
Li, C., Lee, G.H., 2019.

</span>
<span class="ltx_bibblock">Generating multiple hypotheses for 3d human pose
estimation with mixture density network, in: Proc. IEEE
Conference on Computer Vision and Pattern Recognition, pp.
9887–9895.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Fei-fei (2007)</span>
<span class="ltx_bibblock">
Li, L., Fei-fei, L., 2007.

</span>
<span class="ltx_bibblock">What, where and who? classifying events by scene and
object recognition, in: Proc. IEEE International
Conference on Computer Vision, p. 6.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Chan (2014)</span>
<span class="ltx_bibblock">
Li, S., Chan, A.B., 2014.

</span>
<span class="ltx_bibblock">3d human pose estimation from monocular images with
deep convolutional neural network, in: Proc. Asian
Conference on Computer Vision, Springer. pp.
332–347.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2014)</span>
<span class="ltx_bibblock">
Li, S., Liu, Z.Q., Chan,
A.B., 2014.

</span>
<span class="ltx_bibblock">Heterogeneous multi-task learning for human pose
estimation with deep convolutional neural network, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pp. 482–489.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2015b)</span>
<span class="ltx_bibblock">
Li, S., Zhang, W., Chan,
A.B., 2015b.

</span>
<span class="ltx_bibblock">Maximum-margin structured learning with deep networks
for 3d human pose estimation, in: Proc. IEEE
International Conference on Computer Vision, pp.
2848–2856.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Li, Z., Dekel, T., Cole,
F., Tucker, R., Snavely, N.,
Liu, C., Freeman, W.,
2019.

</span>
<span class="ltx_bibblock">Learning the depths of moving people by watching
frozen , 4521–4530.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lifshitz et al. (2016)</span>
<span class="ltx_bibblock">
Lifshitz, I., Fetaya, E.,
Ullman, S., 2016.

</span>
<span class="ltx_bibblock">Human pose estimation using deep consensus voting,
in: Proc. European Conference on Computer Vision,
Springer. pp. 246–260.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2017)</span>
<span class="ltx_bibblock">
Lin, T.Y., Dollár, P.,
Girshick, R., He, K.,
Hariharan, B., Belongie, S.,
2017.

</span>
<span class="ltx_bibblock">Feature pyramid networks for object detection, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2117–2125.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M.,
Belongie, S., Hays, J.,
Perona, P., Ramanan, D.,
Dollár, P., Zitnick, C.L.,
2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context, in:
Proc. European Conference on Computer Vision,
Springer. pp. 740–755.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2015)</span>
<span class="ltx_bibblock">
Liu, Z., Zhu, J., Bu, J.,
Chen, C., 2015.

</span>
<span class="ltx_bibblock">A survey of human pose estimation: the body parts
parsing based methods.

</span>
<span class="ltx_bibblock">Journal of Visual Communication and Image
Representation 32, 10–19.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al. (2015)</span>
<span class="ltx_bibblock">
Long, J., Shelhamer, E.,
Darrell, T., 2015.

</span>
<span class="ltx_bibblock">Fully convolutional networks for semantic
segmentation, in: Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3431–3440.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loper et al. (2015)</span>
<span class="ltx_bibblock">
Loper, M., Mahmood, N.,
Romero, J., Pons-Moll, G.,
Black, M.J., 2015.

</span>
<span class="ltx_bibblock">Smpl: A skinned multi-person linear model.

</span>
<span class="ltx_bibblock">ACM Transactions on Graphics 34,
248.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2018)</span>
<span class="ltx_bibblock">
Luo, Y., Ren, J., Wang,
Z., Sun, W., Pan, J.,
Liu, J., Pang, J., Lin,
L., 2018.

</span>
<span class="ltx_bibblock">Lstm pose machines, in: Proc.
IEEE Conference on Computer Vision and Pattern Recognition, pp.
5207–5215.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luvizon et al. (2018)</span>
<span class="ltx_bibblock">
Luvizon, D.C., Picard, D.,
Tabia, H., 2018.

</span>
<span class="ltx_bibblock">2d/3d pose estimation and action recognition using
multitask deep learning, in: Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5137–5146.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luvizon et al. (2017)</span>
<span class="ltx_bibblock">
Luvizon, D.C., Tabia, H.,
Picard, D., 2017.

</span>
<span class="ltx_bibblock">Human pose regression by combining indirect part
detection and contextual information.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1710.02322 .

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahmood et al. (2019)</span>
<span class="ltx_bibblock">
Mahmood, N., Ghorbani, N.,
Troje, N.F., Pons-Moll, G.,
Black, M.J., 2019.

</span>
<span class="ltx_bibblock">Amass: Archive of motion capture as surface shapes.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1904.03278 .

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">von Marcard et al. (2018)</span>
<span class="ltx_bibblock">
von Marcard, T., Henschel, R.,
Black, M.J., Rosenhahn, B.,
Pons-Moll, G., 2018.

</span>
<span class="ltx_bibblock">Recovering accurate 3d human pose in the wild using
imus and a moving camera, in: Proc. European Conference
on Computer Vision, pp. 601–617.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">von Marcard et al. (2016)</span>
<span class="ltx_bibblock">
von Marcard, T., Pons-Moll, G.,
Rosenhahn, B., 2016.

</span>
<span class="ltx_bibblock">Human pose estimation from video and imus.

</span>
<span class="ltx_bibblock">IEEE transactions on pattern analysis and machine
intelligence 38, 1533–1547.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinez et al. (2017)</span>
<span class="ltx_bibblock">
Martinez, J., Hossain, R.,
Romero, J., Little, J.J.,
2017.

</span>
<span class="ltx_bibblock">A simple yet effective baseline for 3d human pose
estimation, in: Proc. IEEE International Conference on
Computer Vision, pp. 2640–2649.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. (2017a)</span>
<span class="ltx_bibblock">
Mehta, D., Rhodin, H.,
Casas, D., Fua, P.,
Sotnychenko, O., Xu, W.,
Theobalt, C., 2017a.

</span>
<span class="ltx_bibblock">Monocular 3d human pose estimation in the wild using
improved cnn supervision, in: Proc. IEEE International
Conference on 3D Vision, IEEE. pp.
506–516.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. (2019)</span>
<span class="ltx_bibblock">
Mehta, D., Sotnychenko, O.,
Mueller, F., Xu, W.,
Elgharib, M., Fua, P.,
Seidel, H.P., Rhodin, H.,
Pons-Moll, G., Theobalt, C.,
2019.

</span>
<span class="ltx_bibblock">Xnect: Real-time multi-person 3d human pose
estimation with a single rgb camera.

</span>
<span class="ltx_bibblock">arXiv:1907.00837 .

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. (2017b)</span>
<span class="ltx_bibblock">
Mehta, D., Sotnychenko, O.,
Mueller, F., Xu, W.,
Sridhar, S., Pons-Moll, G.,
Theobalt, C., 2017b.

</span>
<span class="ltx_bibblock">Single-shot multi-person 3d body pose estimation from
monocular rgb input.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1712.03453 .

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. (2017c)</span>
<span class="ltx_bibblock">
Mehta, D., Sridhar, S.,
Sotnychenko, O., Rhodin, H.,
Shafiei, M., Seidel, H.P.,
Xu, W., Casas, D.,
Theobalt, C., 2017c.

</span>
<span class="ltx_bibblock">Vnect: Real-time 3d human pose estimation with a
single rgb camera.

</span>
<span class="ltx_bibblock">ACM Transactions on Graphics 36,
44.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meredith et al. (2001)</span>
<span class="ltx_bibblock">
Meredith, M., Maddock, S., et al.,
2001.

</span>
<span class="ltx_bibblock">Motion capture file formats explained.

</span>
<span class="ltx_bibblock">Department of Computer Science, University of
Sheffield 211, 241–244.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moeslund and Granum (2001)</span>
<span class="ltx_bibblock">
Moeslund, T.B., Granum, E.,
2001.

</span>
<span class="ltx_bibblock">A survey of computer vision-based human motion
capture.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
81, 231–268.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moeslund et al. (2006)</span>
<span class="ltx_bibblock">
Moeslund, T.B., Hilton, A.,
Krüger, V., 2006.

</span>
<span class="ltx_bibblock">A survey of advances in vision-based human motion
capture and analysis.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
104, 90–126.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moeslund et al. (2011)</span>
<span class="ltx_bibblock">
Moeslund, T.B., Hilton, A.,
Krüger, V., Sigal, L.,
2011.

</span>
<span class="ltx_bibblock">Visual analysis of humans.

</span>
<span class="ltx_bibblock">Springer.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al. (2019)</span>
<span class="ltx_bibblock">
Moon, G., Chang, J.Y.,
Lee, K.M., 2019.

</span>
<span class="ltx_bibblock">Posefix: Model-agnostic general human pose refinement
network, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 7773–7781.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moreno-Noguer (2017)</span>
<span class="ltx_bibblock">
Moreno-Noguer, F., 2017.

</span>
<span class="ltx_bibblock">3d human pose estimation from a single image via
distance matrix regression, in: Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1561–1570.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newell et al. (2017)</span>
<span class="ltx_bibblock">
Newell, A., Huang, Z.,
Deng, J., 2017.

</span>
<span class="ltx_bibblock">Associative embedding: End-to-end learning for joint
detection and grouping, in: Advances in Neural
Information Processing Systems, pp. 2277–2287.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newell et al. (2016)</span>
<span class="ltx_bibblock">
Newell, A., Yang, K.,
Deng, J., 2016.

</span>
<span class="ltx_bibblock">Stacked hourglass networks for human pose
estimation, in: Proc. European Conference on Computer
Vision, Springer. pp. 483–499.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nibali et al. (2018)</span>
<span class="ltx_bibblock">
Nibali, A., He, Z.,
Morgan, S., Prendergast, L.,
2018.

</span>
<span class="ltx_bibblock">Numerical coordinate regression with convolutional
neural networks.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1801.07372 .

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al. (2017)</span>
<span class="ltx_bibblock">
Nie, B.X., Wei, P., Zhu,
S.C., 2017.

</span>
<span class="ltx_bibblock">Monocular 3d human pose estimation by predicting
depth on joints, in: Proc. IEEE International Conference
on Computer Vision, pp. 3447–3455.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al. (2018)</span>
<span class="ltx_bibblock">
Nie, X., Feng, J., Xing,
J., Yan, S., 2018.

</span>
<span class="ltx_bibblock">Pose partition networks for multi-person pose
estimation, in: Proc. European Conference on Computer
Vision, pp. 684–699.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ning et al. (2018)</span>
<span class="ltx_bibblock">
Ning, G., Zhang, Z., He,
Z., 2018.

</span>
<span class="ltx_bibblock">Knowledge-guided deep fractal neural networks for
human pose estimation.

</span>
<span class="ltx_bibblock">IEEE Transactions on Multimedia
20, 1246–1259.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Omran et al. (2018)</span>
<span class="ltx_bibblock">
Omran, M., Lassner, C.,
Pons-Moll, G., Gehler, P.,
Schiele, B., 2018.

</span>
<span class="ltx_bibblock">Neural body fitting: Unifying deep learning and model
based human pose and shape estimation, in: Proc. IEEE
International Conference on 3D Vision, IEEE. pp.
484–494.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2014)</span>
<span class="ltx_bibblock">
Ouyang, W., Chu, X., Wang,
X., 2014.

</span>
<span class="ltx_bibblock">Multi-source deep learning for human pose
estimation, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 2329–2336.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papandreou et al. (2018)</span>
<span class="ltx_bibblock">
Papandreou, G., Zhu, T.,
Chen, L.C., Gidaris, S.,
Tompson, J., Murphy, K.,
2018.

</span>
<span class="ltx_bibblock">Personlab: Person pose estimation and instance
segmentation with a bottom-up, part-based, geometric embedding model.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1803.08225 .

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papandreou et al. (2017)</span>
<span class="ltx_bibblock">
Papandreou, G., Zhu, T.,
Kanazawa, N., Toshev, A.,
Tompson, J., Bregler, C.,
Murphy, K., 2017.

</span>
<span class="ltx_bibblock">Towards accurate multi-person pose estimation in the
wild, in: Proc. IEEE Conference on Computer Vision and
Pattern Recognition, pp. 4903–4911.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlakos et al. (2018a)</span>
<span class="ltx_bibblock">
Pavlakos, G., Zhou, X.,
Daniilidis, K., 2018a.

</span>
<span class="ltx_bibblock">Ordinal depth supervision for 3d human pose
estimation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1805.04095 .

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlakos et al. (2017)</span>
<span class="ltx_bibblock">
Pavlakos, G., Zhou, X.,
Derpanis, K.G., Daniilidis, K.,
2017.

</span>
<span class="ltx_bibblock">Coarse-to-fine volumetric prediction for single-image
3d human pose, in: Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pp. 1263–1272.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlakos et al. (2018b)</span>
<span class="ltx_bibblock">
Pavlakos, G., Zhu, L.,
Zhou, X., Daniilidis, K.,
2018b.

</span>
<span class="ltx_bibblock">Learning to estimate 3d human pose and shape from a
single color image.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1805.04092 .

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2018)</span>
<span class="ltx_bibblock">
Peng, X., Tang, Z., Yang,
F., Feris, R.S., Metaxas, D.,
2018.

</span>
<span class="ltx_bibblock">Jointly optimize data augmentation and network
training: Adversarial data augmentation in human pose estimation, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2226–2234.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez-Sala et al. (2014)</span>
<span class="ltx_bibblock">
Perez-Sala, X., Escalera, S.,
Angulo, C., Gonzalez, J.,
2014.

</span>
<span class="ltx_bibblock">A survey on model based approaches for 2d and 3d
visual human pose recovery.

</span>
<span class="ltx_bibblock">Sensors 14,
4189–4210.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfister et al. (2015)</span>
<span class="ltx_bibblock">
Pfister, T., Charles, J.,
Zisserman, A., 2015.

</span>
<span class="ltx_bibblock">Flowing convnets for human pose estimation in
videos, in: Proc. IEEE International Conference on
Computer Vision, pp. 1913–1921.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfister et al. (2014)</span>
<span class="ltx_bibblock">
Pfister, T., Simonyan, K.,
Charles, J., Zisserman, A.,
2014.

</span>
<span class="ltx_bibblock">Deep convolutional neural networks for efficient pose
estimation in gesture videos, in: Proc. Asian Conference
on Computer Vision, Springer. pp.
538–552.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pishchulin et al. (2016)</span>
<span class="ltx_bibblock">
Pishchulin, L., Insafutdinov, E.,
Tang, S., Andres, B.,
Andriluka, M., Gehler, P.V.,
Schiele, B., 2016.

</span>
<span class="ltx_bibblock">Deepcut: Joint subset partition and labeling for
multi person pose estimation, in: Proc. IEEE Conference
on Computer Vision and Pattern Recognition, pp. 4929–4937.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pons-Moll et al. (2015)</span>
<span class="ltx_bibblock">
Pons-Moll, G., Romero, J.,
Mahmood, N., Black, M.J.,
2015.

</span>
<span class="ltx_bibblock">Dyna: A model of dynamic human shape in motion.

</span>
<span class="ltx_bibblock">ACM Transactions on Graphics 34,
120.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popa et al. (2017)</span>
<span class="ltx_bibblock">
Popa, A.I., Zanfir, M.,
Sminchisescu, C., 2017.

</span>
<span class="ltx_bibblock">Deep multitask architecture for integrated 2d and 3d
human sensing, in: Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4714–4723.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poppe (2007)</span>
<span class="ltx_bibblock">
Poppe, R., 2007.

</span>
<span class="ltx_bibblock">Vision-based human motion analysis: An overview.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
108, 4–18.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qammaz and Argyros (2019)</span>
<span class="ltx_bibblock">
Qammaz, A., Argyros, A.,
2019.

</span>
<span class="ltx_bibblock">Mocapnet: Ensemble of snn encoders for 3d human pose
estimation in rgb images, in: Proc. British Machine
VIsion Conference.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafi et al. (2016)</span>
<span class="ltx_bibblock">
Rafi, U., Leibe, B., Gall,
J., Kostrikov, I., 2016.

</span>
<span class="ltx_bibblock">An efficient convolutional network for human pose
estimation, in: Proc. British Machine Vision
Conference, p. 2.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramakrishna et al. (2014)</span>
<span class="ltx_bibblock">
Ramakrishna, V., Munoz, D.,
Hebert, M., Bagnell, J.A.,
Sheikh, Y., 2014.

</span>
<span class="ltx_bibblock">Pose machines: Articulated pose estimation via
inference machines, in: Proc. European Conference on
Computer Vision, Springer. pp.
33–47.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick,
R., Sun, J., 2015.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with
region proposal networks, in: Advances in neural
information processing systems, pp. 91–99.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rhodin et al. (2018a)</span>
<span class="ltx_bibblock">
Rhodin, H., Salzmann, M.,
Fua, P., 2018a.

</span>
<span class="ltx_bibblock">Unsupervised geometry-aware representation for 3d
human pose estimation.

</span>
<span class="ltx_bibblock">arXiv:1804.01110 .

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rhodin et al. (2018b)</span>
<span class="ltx_bibblock">
Rhodin, H., Spörri, J.,
Katircioglu, I., Constantin, V.,
Meyer, F., Müller, E.,
Salzmann, M., Fua, P.,
2018b.

</span>
<span class="ltx_bibblock">Learning monocular 3d human pose estimation from
multi-view images, in: Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pp. 8437–8446.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rogez et al. (2017)</span>
<span class="ltx_bibblock">
Rogez, G., Weinzaepfel, P.,
Schmid, C., 2017.

</span>
<span class="ltx_bibblock">Lcr-net: Localization-classification-regression for
human pose, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3433–3441.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohrbach et al. (2012)</span>
<span class="ltx_bibblock">
Rohrbach, M., Amin, S.,
Andriluka, M., Schiele, B.,
2012.

</span>
<span class="ltx_bibblock">A database for fine grained activity detection of
cooking activities, in: Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1194–1201.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sapp and Taskar (2013)</span>
<span class="ltx_bibblock">
Sapp, B., Taskar, B., 2013.

</span>
<span class="ltx_bibblock">Modec: Multimodal decomposable models for human pose
estimation, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3674–3681.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sapp et al. (2011)</span>
<span class="ltx_bibblock">
Sapp, B., Weiss, D.,
Taskar, B., 2011.

</span>
<span class="ltx_bibblock">Parsing human motion with stretchable models, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1281–1288.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarafianos et al. (2016)</span>
<span class="ltx_bibblock">
Sarafianos, N., Boteanu, B.,
Ionescu, B., Kakadiaris, I.A.,
2016.

</span>
<span class="ltx_bibblock">3d human pose estimation: A review of the literature
and analysis of covariates.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
152, 1–20.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shahroudy et al. (2016)</span>
<span class="ltx_bibblock">
Shahroudy, A., Liu, J.,
Ng, T.T., Wang, G., 2016.

</span>
<span class="ltx_bibblock">Ntu rgb+ d: A large scale dataset for 3d human
activity analysis, in: Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pp. 1010–1019.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shotton et al. (2012)</span>
<span class="ltx_bibblock">
Shotton, J., Girshick, R.,
Fitzgibbon, A., Sharp, T.,
Cook, M., Finocchio, M.,
Moore, R., Kohli, P.,
Criminisi, A., Kipman, A., et al.,
2012.

</span>
<span class="ltx_bibblock">Efficient human pose estimation from single depth
images.

</span>
<span class="ltx_bibblock">IEEE transactions on pattern analysis and machine
intelligence 35, 2821–2840.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidenbladh et al. (2000)</span>
<span class="ltx_bibblock">
Sidenbladh, H., De la Torre, F.,
Black, M.J., 2000.

</span>
<span class="ltx_bibblock">A framework for modeling the appearance of 3d
articulated figures, in: Proc. IEEE Conference on
Automatic Face and Gesture Recognition, IEEE. pp.
368–375.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sigal et al. (2010)</span>
<span class="ltx_bibblock">
Sigal, L., Balan, A.O.,
Black, M.J., 2010.

</span>
<span class="ltx_bibblock">Humaneva: Synchronized video and motion capture
dataset and baseline algorithm for evaluation of articulated human motion.

</span>
<span class="ltx_bibblock">International journal of computer vision
87, 4.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sminchisescu (2008)</span>
<span class="ltx_bibblock">
Sminchisescu, C., 2008.

</span>
<span class="ltx_bibblock">3d human motion analysis in monocular video:
techniques and challenges, in: Human Motion.
Springer, pp. 185–211.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2019)</span>
<span class="ltx_bibblock">
Sun, K., Xiao, B., Liu,
D., Wang, J., 2019.

</span>
<span class="ltx_bibblock">Deep high-resolution representation learning for
human pose estimation, in: Proc. IEEE Conference on
Computer Vision and Pattern Recognition.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2017)</span>
<span class="ltx_bibblock">
Sun, X., Shang, J., Liang,
S., Wei, Y., 2017.

</span>
<span class="ltx_bibblock">Compositional human pose regression, in:
Proc. IEEE International Conference on Computer Vision,
p. 7.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2018)</span>
<span class="ltx_bibblock">
Sun, X., Xiao, B., Wei,
F., Liang, S., Wei, Y.,
2018.

</span>
<span class="ltx_bibblock">Integral human pose regression, in:
Proc. European Conference on Computer Vision, pp.
529–545.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy et al. (2016)</span>
<span class="ltx_bibblock">
Szegedy, C., Vanhoucke, V.,
Ioffe, S., Shlens, J.,
Wojna, Z., 2016.

</span>
<span class="ltx_bibblock">Rethinking the inception architecture for computer
vision, in: Proc. IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2818–2826.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. (2017)</span>
<span class="ltx_bibblock">
Tan, J., Budvytis, I.,
Cipolla, R., 2017.

</span>
<span class="ltx_bibblock">Indirect deep structured learning for 3d human body
shape and pose prediction, in: Proc. British Machine
Vision Conference.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang and Wu (2019)</span>
<span class="ltx_bibblock">
Tang, W., Wu, Y., 2019.

</span>
<span class="ltx_bibblock">Does learning specific features for related parts
help human pose estimation?, in: Proc. IEEE Conference
on Computer Vision and Pattern Recognition, pp. 1107–1116.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2018a)</span>
<span class="ltx_bibblock">
Tang, W., Yu, P., Wu, Y.,
2018a.

</span>
<span class="ltx_bibblock">Deeply learned compositional models for human pose
estimation, in: Proc. European Conference on Computer
Vision, pp. 190–206.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2018b)</span>
<span class="ltx_bibblock">
Tang, Z., Peng, X., Geng,
S., Wu, L., Zhang, S.,
Metaxas, D., 2018b.

</span>
<span class="ltx_bibblock">Quantized densely connected u-nets for efficient
landmark localization, in: Proc. European Conference on
Computer Vision, pp. 339–354.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tekin et al. (2016)</span>
<span class="ltx_bibblock">
Tekin, B., Katircioglu, I.,
Salzmann, M., Lepetit, V.,
Fua, P., 2016.

</span>
<span class="ltx_bibblock">Structured prediction of 3d human pose with deep
neural networks.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1605.05180 .

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tekin et al. (2017)</span>
<span class="ltx_bibblock">
Tekin, B., Marquez Neila, P.,
Salzmann, M., Fua, P.,
2017.

</span>
<span class="ltx_bibblock">Learning to fuse 2d and 3d image cues for monocular
body pose estimation, in: Proc. IEEE International
Conference on Computer Vision, pp. 3941–3950.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TheCaptury (accessed on 2019)</span>
<span class="ltx_bibblock">
TheCaptury, accessed on 2019.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://thecaptury.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://thecaptury.com/</a>.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tome et al. (2017)</span>
<span class="ltx_bibblock">
Tome, D., Russell, C.,
Agapito, L., 2017.

</span>
<span class="ltx_bibblock">Lifting from the deep: Convolutional 3d pose
estimation from a single image, pp. 2500–2509.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tompson et al. (2015)</span>
<span class="ltx_bibblock">
Tompson, J., Goroshin, R.,
Jain, A., LeCun, Y.,
Bregler, C., 2015.

</span>
<span class="ltx_bibblock">Efficient object localization using convolutional
networks, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 648–656.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tompson et al. (2014)</span>
<span class="ltx_bibblock">
Tompson, J.J., Jain, A.,
LeCun, Y., Bregler, C.,
2014.

</span>
<span class="ltx_bibblock">Joint training of a convolutional network and a
graphical model for human pose estimation, in: Advances
in neural information processing systems, pp. 1799–1807.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toshev and Szegedy (2014)</span>
<span class="ltx_bibblock">
Toshev, A., Szegedy, C.,
2014.

</span>
<span class="ltx_bibblock">Deeppose: Human pose estimation via deep neural
networks, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1653–1660.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trumble et al. (2017)</span>
<span class="ltx_bibblock">
Trumble, M., Gilbert, A.,
Malleson, C., Hilton, A.,
Collomosse, J., 2017.

</span>
<span class="ltx_bibblock">Total capture: 3d human pose estimation fusing video
and inertial sensors, in: Proc. British Machine Vision
Conference, pp. 1–13.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varol et al. (2018)</span>
<span class="ltx_bibblock">
Varol, G., Ceylan, D.,
Russell, B., Yang, J.,
Yumer, E., Laptev, I.,
Schmid, C., 2018.

</span>
<span class="ltx_bibblock">Bodynet: Volumetric inference of 3d human body
shapes.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1804.04875 .

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varol et al. (2017)</span>
<span class="ltx_bibblock">
Varol, G., Romero, J.,
Martin, X., Mahmood, N.,
Black, M.J., Laptev, I.,
Schmid, C., 2017.

</span>
<span class="ltx_bibblock">Learning from synthetic humans, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4627–4635.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vicon (accessed on 2019)</span>
<span class="ltx_bibblock">
Vicon, accessed on 2019.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://www.vicon.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.vicon.com/</a>.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vondrick et al. (2013)</span>
<span class="ltx_bibblock">
Vondrick, C., Patterson, D.,
Ramanan, D., 2013.

</span>
<span class="ltx_bibblock">Efficiently scaling up crowdsourced video
annotation.

</span>
<span class="ltx_bibblock">International Journal of Computer Vision
101, 184–204.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2018a)</span>
<span class="ltx_bibblock">
Wang, M., Chen, X., Liu,
W., Qian, C., Lin, L.,
Ma, L., 2018a.

</span>
<span class="ltx_bibblock">Drpose3d: Depth ranking in 3d human pose estimation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1805.08973 .

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2018b)</span>
<span class="ltx_bibblock">
Wang, P., Li, W.,
Ogunbona, P., Wan, J.,
Escalera, S., 2018b.

</span>
<span class="ltx_bibblock">Rgb-d-based human motion recognition with deep
learning: A survey.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
171, 118–139.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2011)</span>
<span class="ltx_bibblock">
Wang, Y., Tran, D., Liao,
Z., 2011.

</span>
<span class="ltx_bibblock">Learning hierarchical poselets for human parsing,
in: Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1705–1712.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2016)</span>
<span class="ltx_bibblock">
Wei, S.E., Ramakrishna, V.,
Kanade, T., Sheikh, Y.,
2016.

</span>
<span class="ltx_bibblock">Convolutional pose machines, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4724–4732.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2017)</span>
<span class="ltx_bibblock">
Wu, J., Zheng, H., Zhao,
B., Li, Y., Yan, B.,
Liang, R., Wang, W.,
Zhou, S., Lin, G., Fu,
Y., et al., 2017.

</span>
<span class="ltx_bibblock">Ai challenger: A large-scale dataset for going deeper
in image understanding.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1711.06475 .

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2018)</span>
<span class="ltx_bibblock">
Xiao, B., Wu, H., Wei,
Y., 2018.

</span>
<span class="ltx_bibblock">Simple baselines for human pose estimation and
tracking, in: Proc. European Conference on Computer
Vision, pp. 466–481.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2017)</span>
<span class="ltx_bibblock">
Yang, W., Li, S., Ouyang,
W., Li, H., Wang, X.,
2017.

</span>
<span class="ltx_bibblock">Learning feature pyramids for human pose estimation,
in: Proc. IEEE International Conference on Computer
Vision, pp. 1281–1290.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2016)</span>
<span class="ltx_bibblock">
Yang, W., Ouyang, W., Li,
H., Wang, X., 2016.

</span>
<span class="ltx_bibblock">End-to-end learning of deformable mixture of parts
and deep convolutional neural networks for human pose estimation, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3073–3082.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Yang, W., Ouyang, W.,
Wang, X., Ren, J., Li,
H., Wang, X., 2018.

</span>
<span class="ltx_bibblock">3d human pose estimation in the wild by adversarial
learning, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 5255–5264.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Ramanan (2013)</span>
<span class="ltx_bibblock">
Yang, Y., Ramanan, D.,
2013.

</span>
<span class="ltx_bibblock">Articulated human detection with flexible mixtures of
parts.

</span>
<span class="ltx_bibblock">IEEE transactions on pattern analysis and machine
intelligence 35, 2878–2890.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zanfir et al. (2018)</span>
<span class="ltx_bibblock">
Zanfir, A., Marinoiu, E.,
Sminchisescu, C., 2018.

</span>
<span class="ltx_bibblock">Monocular 3d pose and shape estimation of multiple
people in natural scenes-the importance of multiple scene constraints, in:
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2148–2157.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2013)</span>
<span class="ltx_bibblock">
Zhang, W., Zhu, M.,
Derpanis, K.G., 2013.

</span>
<span class="ltx_bibblock">From actemes to action: A strongly-supervised
representation for detailed action understanding, in:
Proc. IEEE International Conference on Computer Vision,
pp. 2248–2255.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2018)</span>
<span class="ltx_bibblock">
Zhao, M., Li, T.,
Abu Alsheikh, M., Tian, Y.,
Zhao, H., Torralba, A.,
Katabi, D., 2018.

</span>
<span class="ltx_bibblock">Through-wall human pose estimation using radio
signals, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 7356–7365.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2017)</span>
<span class="ltx_bibblock">
Zhou, X., Huang, Q., Sun,
X., Xue, X., Wei, Y.,
2017.

</span>
<span class="ltx_bibblock">Towards 3d human pose estimation in the wild: a
weakly-supervised approach, in: Proc. IEEE International
Conference on Computer Vision, pp. 398–407.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2016)</span>
<span class="ltx_bibblock">
Zhou, X., Sun, X., Zhang,
W., Liang, S., Wei, Y.,
2016.

</span>
<span class="ltx_bibblock">Deep kinematic pose regression, in:
Proc. European Conference on Computer Vision,
Springer. pp. 186–201.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuffi and Black (2015)</span>
<span class="ltx_bibblock">
Zuffi, S., Black, M.J.,
2015.

</span>
<span class="ltx_bibblock">The stitched puppet: A graphical model of 3d human
shape and pose, in: Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3537–3546.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuffi et al. (2012)</span>
<span class="ltx_bibblock">
Zuffi, S., Freifeld, O.,
Black, M.J., 2012.

</span>
<span class="ltx_bibblock">From pictorial structures to deformable structures,
in: Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3546–3553.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2006.01421" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2006.01423" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2006.01423">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2006.01423" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2006.01424" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 09:44:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
