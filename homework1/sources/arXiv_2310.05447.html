<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.05447] Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection</title><meta property="og:description" content="In this work, we build a modular-designed codebase, formulate strong training recipes, design an error diagnosis toolbox, and discuss current methods for image-based 3D object detection.
In particular, different from o…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.05447">

<!--Generated on Wed Feb 28 01:48:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Fair and Comprehensive Comparisons for Image-Based 
<br class="ltx_break">3D Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinzhu Ma<sup id="id13.13.id1" class="ltx_sup"><span id="id13.13.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>  Yongtao Wang<sup id="id14.14.id2" class="ltx_sup"><span id="id14.14.id2.1" class="ltx_text ltx_font_italic">3</span></sup>  Yinmin Zhang<sup id="id15.15.id3" class="ltx_sup"><span id="id15.15.id3.1" class="ltx_text ltx_font_italic">1,2</span></sup>  Zhiyi Xia<sup id="id16.16.id4" class="ltx_sup">4</sup>  Yuan Meng<sup id="id17.17.id5" class="ltx_sup"><span id="id17.17.id5.1" class="ltx_text ltx_font_italic">4,∗</span></sup> 
<br class="ltx_break">Zhihui Wang<sup id="id18.18.id6" class="ltx_sup"><span id="id18.18.id6.1" class="ltx_text ltx_font_italic">3,∗</span></sup>   Haojie Li<sup id="id19.19.id7" class="ltx_sup">3</sup>   Wanli Ouyang<sup id="id20.20.id8" class="ltx_sup">1</sup> 
<br class="ltx_break"><sup id="id21.21.id9" class="ltx_sup">1</sup>Shanghai AI Lab <sup id="id22.22.id10" class="ltx_sup">2</sup>University of Sydney <sup id="id23.23.id11" class="ltx_sup">3</sup>Dalian University of Technology <sup id="id24.24.id12" class="ltx_sup">4</sup>Tsinghua University 
<br class="ltx_break"><span id="id25.25.id13" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{maxinzhu, zhangyinmin, ouyangwanli}@pjlab.org.cn</span> 
<br class="ltx_break"><span id="id26.26.id14" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{yongtaowang@mail., zhwang@, hjli@}dlut.edu.cn</span>  <span id="id27.27.id15" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{yuanmeng, xiazy21}@mail.tsinghua.edu.cn</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id28.id1" class="ltx_p">In this work, we build a modular-designed codebase, formulate strong training recipes, design an error diagnosis toolbox, and discuss current methods for image-based 3D object detection.
In particular, different from other highly mature tasks, e.g., 2D object detection, the community of image-based 3D object detection is still evolving, where methods often adopt different training recipes and tricks resulting in unfair evaluations and comparisons.
What is worse, these tricks may overwhelm their proposed designs in performance, even leading to wrong conclusions.
To address this issue, we build a module-designed codebase and formulate unified training standards for the community.
Furthermore, we also design an error diagnosis toolbox to measure the detailed characterization of detection models.
Using these tools, we analyze current methods in-depth under varying settings and provide discussions for some open questions, e.g., discrepancies in conclusions on KITTI-3D and nuScenes datasets, which have led to different dominant methods for these datasets.
We hope that this work will facilitate future research in image-based 3D object detection.
Our codes will be released at <a target="_blank" href="https://github.com/OpenGVLab/3dodi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenGVLab/3dodi</a>.</p>
</div>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>Corresponding author</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As a new and rapidly developing research field, vision-based 3D object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> shows promising potential in autonomous driving and attracts lots of attention from both academia and industry. Thanks to the unremitting efforts of numerous researchers, lots of advanced technologies, such as model designs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, detection pipelines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">95</span></a>, <a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">83</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, and challenging datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">75</span></a>]</cite>, are continuously proposed, which significantly promotes the development of this research field.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, although lots of breakthroughs in detection accuracy and inference speed have been achieved, there are still some critical problems to be solved, especially the standardization of evaluation protocols. Specifically, compared with the encouraging developments at the technique level, the conventional rules in model building, training recipes, and evaluation are not well-defined. As shown in Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, existing methods generally adopt different settings, <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.2" class="ltx_text"></span> backbones, training strategies, augmentations, <em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">etc</em>.<span id="S1.p2.1.4" class="ltx_text"></span>, to build and evaluate their models, and the commonly used benchmarks (and most of the papers) only record the final accuracy.
This makes the comparison of the detectors unfair and may lead to misleading conclusions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we aim to provide a unified platform for image-based 3D object detectors and standardize the protocols in model building and evaluation. Specifically, similar to the advanced codebases in 2D detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib86" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">86</span></a>]</cite>, we decompose the image-based 3D detection frameworks into several separate components, <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p3.1.2" class="ltx_text"></span> backbones, necks, <em id="S1.p3.1.3" class="ltx_emph ltx_font_italic">etc</em>.<span id="S1.p3.1.4" class="ltx_text"></span>, and provide a unified implementation for current methods.
Furthermore, we fully investigate existing algorithms and formulate several efficient training protocols, <em id="S1.p3.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p3.1.6" class="ltx_text"></span> training schedules, data augmentation, <em id="S1.p3.1.7" class="ltx_emph ltx_font_italic">etc</em>.
Our summarized training recipes not only provide a fair environment for evaluation but also significantly improve the performance of current methods, particularly in the KITTI-3D dataset.
For example, the methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> published two years ago trained with our recipes can achieve better (or similar) performance than the recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>, emphasizing the importance of establishing a standard training recipe.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Furthermore, to systematically analyze the bottlenecks and issues in image-based 3D object detection, we urgently require a tool to thoroughly examine detection results.
Inspired by TIDE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, we propose an error diagnosis toolbox, named TIDE3D, to measure the detailed characterization of detection algorithms. Specifically, we decouple the detection errors into seven types, and then independently quantify the impact of each error type by calculating the overall performance improvement of the model after fixing the specified errors. In this way, we can analyze a specific aspect of given algorithms while isolating other factors.
In addition to characterizing the detailed features of the models, the proposed TIDE3D also has other useful applications. For example, the effectiveness and action mechanism of a specific design/module can be explored by analyzing the change of error distribution of the baseline model with or without the target design/module.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Besides, we also find the most concerned two datasets, <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p5.1.2" class="ltx_text"></span> KITTI-3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>
and nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, are dominated by different detection pipelines, and even gradually become separate research communities. For example, none of the popular Bird’s Eye View (BEV) detection methods provide the KITTI-3D results, and the top-performing methods in the KITTI-3D leaderboard also hard to achieve good results in nuScenes. We apply the cross-metric evaluation (<em id="S1.p5.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p5.1.4" class="ltx_text"></span> applying the nuScenes-style metrics on the KITTI-3D dataset and vice versa) on the representative models and report the adopted metric is the main factor causing this phenomenon. We also provide TIDE3D analyses for this issue.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To summarize, the contributions of this work are as follows:
First, we build a modular-designed codebase for the community of image-based 3D object detection, which can serve as a foundation for future research and algorithm implementation.
Second, we investigate the training settings and formulate standard training recipes for this task.
Third, we provide an error diagnosis toolbox that can quantitatively analyze the detection models at a fine-grained level.
Last, we discuss some open problems in this field, which may provide insights for future research.
We hope our codebase, training recipes, error diagnosis toolbox, and discussions will promote better and more standardized research practices within the image-based 3D object detection community.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<div id="S1.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:72.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-96.2pt,17.8pt) scale(0.669885354195582,0.669885354195582) ;">
<table id="S1.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;" rowspan="2"></th>
<th id="S1.T1.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;"></th>
<th id="S1.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;" colspan="3">KITTI</th>
<th id="S1.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:7.0pt;padding-right:7.0pt;" colspan="3">nuScenes</th>
</tr>
<tr id="S1.T1.1.1.2.2" class="ltx_tr">
<th id="S1.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">backbone</th>
<th id="S1.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:7.0pt;padding-right:7.0pt;"># epochs</th>
<th id="S1.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:7.0pt;padding-right:7.0pt;">data aug.</th>
<th id="S1.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">others</th>
<th id="S1.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:7.0pt;padding-right:7.0pt;"># epochs</th>
<th id="S1.T1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:7.0pt;padding-right:7.0pt;">data aug.</th>
<th id="S1.T1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:7.0pt;padding-right:7.0pt;">others</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.3.1" class="ltx_tr">
<th id="S1.T1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">GUPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>
</th>
<th id="S1.T1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">DLA34-DLAUp</th>
<td id="S1.T1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">140</td>
<td id="S1.T1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">flip, crop</td>
<td id="S1.T1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S1.T1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S1.T1.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S1.T1.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
</tr>
<tr id="S1.T1.1.1.4.2" class="ltx_tr">
<th id="S1.T1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">FCOS3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>
</th>
<th id="S1.T1.1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">ResNet101-DCN-FPN</th>
<td id="S1.T1.1.1.4.2.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S1.T1.1.1.4.2.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S1.T1.1.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S1.T1.1.1.4.2.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">12</td>
<td id="S1.T1.1.1.4.2.7" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">flip</td>
<td id="S1.T1.1.1.4.2.8" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">TTA</td>
</tr>
<tr id="S1.T1.1.1.5.3" class="ltx_tr">
<th id="S1.T1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">PGD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>
</th>
<th id="S1.T1.1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">ResNet101-DCN-FPN</th>
<td id="S1.T1.1.1.5.3.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">48</td>
<td id="S1.T1.1.1.5.3.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">flip</td>
<td id="S1.T1.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">TTA</td>
<td id="S1.T1.1.1.5.3.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">24</td>
<td id="S1.T1.1.1.5.3.7" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">flip</td>
<td id="S1.T1.1.1.5.3.8" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">TTA</td>
</tr>
<tr id="S1.T1.1.1.6.4" class="ltx_tr">
<th id="S1.T1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">BEVDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>
</th>
<th id="S1.T1.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">ResNet50-FPN-LSS</th>
<td id="S1.T1.1.1.6.4.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S1.T1.1.1.6.4.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S1.T1.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S1.T1.1.1.6.4.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">24</td>
<td id="S1.T1.1.1.6.4.7" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">flip, BEV aug.</td>
<td id="S1.T1.1.1.6.4.8" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">TTA</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S1.T1.7.1" class="ltx_text ltx_font_bold">Overview of example methods on KITTI-3D and nuScenes benchmarks.</span> The existing methods generally adopt different settings, <em id="S1.T1.8.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.T1.9.3" class="ltx_text"></span> backbones, epochs, augmentations, <em id="S1.T1.10.4" class="ltx_emph ltx_font_italic">etc</em>.<span id="S1.T1.11.5" class="ltx_text"></span>, in model training. Besides, KITTI-3D and nuScenes apply different evaluation metrics and are dominated by different detection pipelines. TTA denotes the test-time augmentation.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Image-based 3D detection.</span> Image-based 3D detection is a rapidly developing research direction, and there are lots of detectors are proposed. Here we review these works based on the taxonomy proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite>: methods based on result-lifting, methods based on feature-lifting, and methods based on data-lifting. Refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> for the detailed and comprehensive literature review.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_italic">Result-lifting.</span>
The methods in this branch first estimate the 2D projections and other items (<em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p2.1.3" class="ltx_text"></span> depth, orientation, <em id="S2.p2.1.4" class="ltx_emph ltx_font_italic">etc</em>.<span id="S2.p2.1.5" class="ltx_text"></span>) of the 3D bounding boxes and then lift the results from the 2D image plane to the 3D world space.
In particular, pioneering works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib95" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">95</span></a>, <a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib84" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">84</span></a>]</cite> introduce the popular 2D detection paradigms, <em id="S2.p2.1.6" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p2.1.7" class="ltx_text"></span> faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> and FCOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>, into this research field, and lots of following works improve these baseline models in several aspects, including backbone designs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, loss functions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>, depth estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib93" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">93</span></a>, <a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">71</span></a>, <a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>, geometric constraints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite>, feature embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>, <a href="#bib.bib94" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">94</span></a>]</cite>, key-point constrains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>, depth augmented learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">87</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">78</span></a>]</cite>, temporal sequences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>, semi-supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>, <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, NMS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">68</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, data integration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, <em id="S2.p2.1.8" class="ltx_emph ltx_font_italic">etc</em>.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_italic">Feature-lifting.</span> OFT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite> and DSGN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> are the pioneering works in this research line, which lift the 2D features into 3D features (generally represented by BEV map) and then directly estimates the 3D bounding boxes with the resulting features.
CaDDN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite> uses a better transformation strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> and achieves promising results. BEVDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> introduces this paradigm into the multi-camera setting, and this BEV pipeline gradually dominates the nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> benchmark with effects of lots of follow-up works, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_italic">Data-lifting.</span> The data-lifting-based methods first estimate dense depth maps for the input images and then lift the pixels into 3D points using the estimated depth and camera parameters. After that, they generally leverage the LiDAR-based 3D detectors to predict the results from these ‘pseudo-LiDAR’ signals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">83</span></a>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>. The followers in this group are involved in the following aspects: improving the quality of pseudo-LiDAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">89</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>, focusing on foreground objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">82</span></a>, <a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">74</span></a>, <a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">88</span></a>]</cite>, end-to-ending training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>, feature representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>, geometric constraints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">85</span></a>]</cite> or confidence refinement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Codebases.</span> At present, there are two public codebases that support vision-based 3D object detection: MMDetection3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> and OpenPCDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite>. However, these two codebases are initially designed for LiDAR-based 3D object detection, and the supported vision-based methods are limited. In particular, MMDetection3D supports four vision-based detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">66</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>, <a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>, and OpenPCDet only support CaDDN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite><span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>data collected at 24-02-2023</span></span></span>(also note some methods are developed based on these two codebases although they are not included in the official ones). In contrast, our codebase is designed for vision-based methods, excluding redundant codes and dependencies for LiDAR-based methods.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.2" class="ltx_p"><span id="S2.p6.2.1" class="ltx_text ltx_font_bold">Error diagnosis for object detection.</span> To obtain the detailed characterization under the overall evaluation metric and show the strengths and weaknesses of the given models, some works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> exist to analyze the errors in 2D detectors. In particular, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> divides the errors of false positives into several categories and selects the top <math id="S2.p6.1.m1.1" class="ltx_Math" alttext="{\rm N}" display="inline"><semantics id="S2.p6.1.m1.1a"><mi mathvariant="normal" id="S2.p6.1.m1.1.1" xref="S2.p6.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.1b"><ci id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1">N</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.1c">{\rm N}</annotation></semantics></math> most confident detections for errors analysis. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> further adds statistics for false negatives. More specifically, it progressively replaces the predicted items with corresponding ground-truth values and uses the <math id="S2.p6.2.m2.1" class="ltx_Math" alttext="\Delta{\rm AP}" display="inline"><semantics id="S2.p6.2.m2.1a"><mrow id="S2.p6.2.m2.1.1" xref="S2.p6.2.m2.1.1.cmml"><mi mathvariant="normal" id="S2.p6.2.m2.1.1.2" xref="S2.p6.2.m2.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S2.p6.2.m2.1.1.1" xref="S2.p6.2.m2.1.1.1.cmml">​</mo><mi id="S2.p6.2.m2.1.1.3" xref="S2.p6.2.m2.1.1.3.cmml">AP</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.2.m2.1b"><apply id="S2.p6.2.m2.1.1.cmml" xref="S2.p6.2.m2.1.1"><times id="S2.p6.2.m2.1.1.1.cmml" xref="S2.p6.2.m2.1.1.1"></times><ci id="S2.p6.2.m2.1.1.2.cmml" xref="S2.p6.2.m2.1.1.2">Δ</ci><ci id="S2.p6.2.m2.1.1.3.cmml" xref="S2.p6.2.m2.1.1.3">AP</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.2.m2.1c">\Delta{\rm AP}</annotation></semantics></math> metric to quantify the importance of each item.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> points out that this iterative approach incorrectly amplifies the impact of later error types, thus proposing a strategy where each error type is fixed independently. These works contribute to model analysis in the field of 2D detection, and to our best knowledge, there is no previous attempt to provide such a generic toolbox for 3D object detection. In this work, we provide a costumed error diagnosis toolbox to fill this gap.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Codebase</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The objective of this work is to build a fair platform and provide experimental analysis for image-based 3D detection algorithms. For this purpose, we first build the general codebase which can support this work and facilitate future research.
Specifically, we decompose the image-based 3D detection frameworks into different components, <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p1.1.2" class="ltx_text"></span> backbones, necks, heads, <em id="S3.p1.1.3" class="ltx_emph ltx_font_italic">etc</em>.<span id="S3.p1.1.4" class="ltx_text"></span>, and we provide common choices for each component, <em id="S3.p1.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p1.1.6" class="ltx_text"></span> ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, DLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite>, or DCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">96</span></a>]</cite> for the backbones.
In addition to the CNN modules, we also provide the implementation of other parts, such as augmentations, post-processing, <em id="S3.p1.1.7" class="ltx_emph ltx_font_italic">etc</em>.
Currently, the proposed codebase supports more than ten methods such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib91" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">91</span></a>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib95" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">95</span></a>, <a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>, <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> and two commonly used datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. Our codebase will be publicly available and continuously maintained.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Comparison with official implementation.</span>
Here we provide a comparison of our implementation and the official implementation for some representative works.
In particular, here we take MonoDLE and GUPNet as examples for KITTI-3D dataset, and FCOS3D as example for nuScenes dataset.
Based on the results shwn in Table <a href="#S3.T2" title="Table 2 ‣ 3 Codebase ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we can find our implementation gets consistent or better results than the official numbers. More details and the results for other models can be found in our public codebase.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:403.3pt;height:176.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(37.1pt,-16.2pt) scale(1.22530849959341,1.22530849959341) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" rowspan="2"></th>
<th id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="3">KITTI</th>
<th id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2">nuScenes</th>
</tr>
<tr id="S3.T2.1.1.2.2" class="ltx_tr">
<th id="S3.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Easy</th>
<th id="S3.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Mod.</th>
<th id="S3.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Hard</th>
<th id="S3.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">mAP</th>
<th id="S3.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">NDS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.3.1" class="ltx_tr">
<th id="S3.T2.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MonoDLE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>
</th>
<td id="S3.T2.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">17.89</td>
<td id="S3.T2.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">13.87</td>
<td id="S3.T2.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.03</td>
<td id="S3.T2.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T2.1.1.4.2" class="ltx_tr">
<th id="S3.T2.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MonoDLE - ours</th>
<td id="S3.T2.1.1.4.2.2" class="ltx_td ltx_align_center">18.37</td>
<td id="S3.T2.1.1.4.2.3" class="ltx_td ltx_align_center">13.79</td>
<td id="S3.T2.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">12.24</td>
<td id="S3.T2.1.1.4.2.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.1.4.2.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.1.1.5.3" class="ltx_tr">
<th id="S3.T2.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GUPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>
</th>
<td id="S3.T2.1.1.5.3.2" class="ltx_td ltx_align_center">21.48</td>
<td id="S3.T2.1.1.5.3.3" class="ltx_td ltx_align_center">15.22</td>
<td id="S3.T2.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">12.79</td>
<td id="S3.T2.1.1.5.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.1.5.3.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.1.1.6.4" class="ltx_tr">
<th id="S3.T2.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GUPNet - ours</th>
<td id="S3.T2.1.1.6.4.2" class="ltx_td ltx_align_center">21.66</td>
<td id="S3.T2.1.1.6.4.3" class="ltx_td ltx_align_center">15.45</td>
<td id="S3.T2.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">12.51</td>
<td id="S3.T2.1.1.6.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.1.6.4.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.1.1.7.5" class="ltx_tr">
<th id="S3.T2.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">FCOS3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>
</th>
<td id="S3.T2.1.1.7.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.1.7.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.1.1.7.5.5" class="ltx_td ltx_align_center">30.6</td>
<td id="S3.T2.1.1.7.5.6" class="ltx_td ltx_align_center">38.1</td>
</tr>
<tr id="S3.T2.1.1.8.6" class="ltx_tr">
<th id="S3.T2.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">FCOS3D - ours</th>
<td id="S3.T2.1.1.8.6.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.1.8.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.1.1.8.6.5" class="ltx_td ltx_align_center">30.4</td>
<td id="S3.T2.1.1.8.6.6" class="ltx_td ltx_align_center">38.5</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S3.T2.4.1" class="ltx_text ltx_font_bold">Comparison of the implementations</span> on KITTI and nuScenes <em id="S3.T2.5.2" class="ltx_emph ltx_font_italic">validation</em> sets. We report the average performance of the last epoch for both the official codes and our implementations over five runs. </figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Diagnosis Toolkit</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We propose a general toolkit, named TIDE3D, to diagnose the cause of errors for 3D object detection models. We first briefly review the computing process of mean Average Precision (mAP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> and then introduce how to divide the detection errors into fine-grained types. Finally, we present how to weigh these errors and the implementation details.
We also recommend readers refer to TIDE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> for preliminary knowledge.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.3" class="ltx_p"><span id="S4.p2.3.1" class="ltx_text ltx_font_bold">Review of mAP.</span>
As the most commonly used metric for evaluating object detection methods, mAP provides a comprehensive overview of a detector’s performance.
Given the predictions and corresponding ground truths, each ground truth is matched to at most one prediction according to a specified metric, <em id="S4.p2.3.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.p2.3.3" class="ltx_text"></span> KITTI-3D uses 3D Intersection over Union (IoU) and nuScenes adopts center distance as the metric.
If multiple predictions meet the constraint, the ground truth only matches the prediction which has the maximum confidence score.
The matched predictions are true positives (TP), and the remaining ones are false positives (FP).
After sorting the predictions by descending confidence, the number of true positives and false positives in the subset of predictions with confidence scores greater than <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">c</annotation></semantics></math> is counted as <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="{\rm N}_{{\rm TP}}" display="inline"><semantics id="S4.p2.2.m2.1a"><msub id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">N</mi><mi id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">TP</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">N</ci><ci id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">TP</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">{\rm N}_{{\rm TP}}</annotation></semantics></math> and <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="{\rm N}_{{\rm FP}}" display="inline"><semantics id="S4.p2.3.m3.1a"><msub id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi mathvariant="normal" id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">N</mi><mi id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">FP</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1">subscript</csymbol><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">N</ci><ci id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">FP</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">{\rm N}_{{\rm FP}}</annotation></semantics></math>.
Then, the precision and recall of the predictions subset are obtained by:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.2" class="ltx_Math" alttext="P_{c}=\frac{{\rm N}_{{\rm TP}}}{{\rm N}_{{\rm TP}}+{\rm N}_{{\rm FP}}}~{}~{}~{}~{}~{}~{}~{}R_{c}=\frac{{\rm N}_{{\rm TP}}}{{\rm N}_{{\rm GT}}}" display="block"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2.2" xref="S4.E1.m1.2.2.3.cmml"><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><msub id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml"><mi id="S4.E1.m1.1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.2.2.cmml">P</mi><mi id="S4.E1.m1.1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.2.3.cmml">c</mi></msub><mo id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml"><msub id="S4.E1.m1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.3.2.cmml"><mi mathvariant="normal" id="S4.E1.m1.1.1.1.1.3.2.2" xref="S4.E1.m1.1.1.1.1.3.2.2.cmml">N</mi><mi id="S4.E1.m1.1.1.1.1.3.2.3" xref="S4.E1.m1.1.1.1.1.3.2.3.cmml">TP</mi></msub><mrow id="S4.E1.m1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.3.3.cmml"><msub id="S4.E1.m1.1.1.1.1.3.3.2" xref="S4.E1.m1.1.1.1.1.3.3.2.cmml"><mi mathvariant="normal" id="S4.E1.m1.1.1.1.1.3.3.2.2" xref="S4.E1.m1.1.1.1.1.3.3.2.2.cmml">N</mi><mi id="S4.E1.m1.1.1.1.1.3.3.2.3" xref="S4.E1.m1.1.1.1.1.3.3.2.3.cmml">TP</mi></msub><mo id="S4.E1.m1.1.1.1.1.3.3.1" xref="S4.E1.m1.1.1.1.1.3.3.1.cmml">+</mo><msub id="S4.E1.m1.1.1.1.1.3.3.3" xref="S4.E1.m1.1.1.1.1.3.3.3.cmml"><mi mathvariant="normal" id="S4.E1.m1.1.1.1.1.3.3.3.2" xref="S4.E1.m1.1.1.1.1.3.3.3.2.cmml">N</mi><mi id="S4.E1.m1.1.1.1.1.3.3.3.3" xref="S4.E1.m1.1.1.1.1.3.3.3.3.cmml">FP</mi></msub></mrow></mfrac></mrow><mspace width="2.31em" id="S4.E1.m1.2.2.2.3" xref="S4.E1.m1.2.2.3a.cmml"></mspace><mrow id="S4.E1.m1.2.2.2.2" xref="S4.E1.m1.2.2.2.2.cmml"><msub id="S4.E1.m1.2.2.2.2.2" xref="S4.E1.m1.2.2.2.2.2.cmml"><mi id="S4.E1.m1.2.2.2.2.2.2" xref="S4.E1.m1.2.2.2.2.2.2.cmml">R</mi><mi id="S4.E1.m1.2.2.2.2.2.3" xref="S4.E1.m1.2.2.2.2.2.3.cmml">c</mi></msub><mo id="S4.E1.m1.2.2.2.2.1" xref="S4.E1.m1.2.2.2.2.1.cmml">=</mo><mfrac id="S4.E1.m1.2.2.2.2.3" xref="S4.E1.m1.2.2.2.2.3.cmml"><msub id="S4.E1.m1.2.2.2.2.3.2" xref="S4.E1.m1.2.2.2.2.3.2.cmml"><mi mathvariant="normal" id="S4.E1.m1.2.2.2.2.3.2.2" xref="S4.E1.m1.2.2.2.2.3.2.2.cmml">N</mi><mi id="S4.E1.m1.2.2.2.2.3.2.3" xref="S4.E1.m1.2.2.2.2.3.2.3.cmml">TP</mi></msub><msub id="S4.E1.m1.2.2.2.2.3.3" xref="S4.E1.m1.2.2.2.2.3.3.cmml"><mi mathvariant="normal" id="S4.E1.m1.2.2.2.2.3.3.2" xref="S4.E1.m1.2.2.2.2.3.3.2.cmml">N</mi><mi id="S4.E1.m1.2.2.2.2.3.3.3" xref="S4.E1.m1.2.2.2.2.3.3.3.cmml">GT</mi></msub></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.3.cmml" xref="S4.E1.m1.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.3a.cmml" xref="S4.E1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1"><eq id="S4.E1.m1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1"></eq><apply id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2">𝑃</ci><ci id="S4.E1.m1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.2.3">𝑐</ci></apply><apply id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3"><divide id="S4.E1.m1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3"></divide><apply id="S4.E1.m1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2.2">N</ci><ci id="S4.E1.m1.1.1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.1.1.3.2.3">TP</ci></apply><apply id="S4.E1.m1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3"><plus id="S4.E1.m1.1.1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.3.1"></plus><apply id="S4.E1.m1.1.1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.3.3.2.1.cmml" xref="S4.E1.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.3.3.2.2.cmml" xref="S4.E1.m1.1.1.1.1.3.3.2.2">N</ci><ci id="S4.E1.m1.1.1.1.1.3.3.2.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3.2.3">TP</ci></apply><apply id="S4.E1.m1.1.1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3.2">N</ci><ci id="S4.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3.3">FP</ci></apply></apply></apply></apply><apply id="S4.E1.m1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2"><eq id="S4.E1.m1.2.2.2.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1"></eq><apply id="S4.E1.m1.2.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.2.1.cmml" xref="S4.E1.m1.2.2.2.2.2">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.2.2">𝑅</ci><ci id="S4.E1.m1.2.2.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.2.2.3">𝑐</ci></apply><apply id="S4.E1.m1.2.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.2.3"><divide id="S4.E1.m1.2.2.2.2.3.1.cmml" xref="S4.E1.m1.2.2.2.2.3"></divide><apply id="S4.E1.m1.2.2.2.2.3.2.cmml" xref="S4.E1.m1.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.3.2.1.cmml" xref="S4.E1.m1.2.2.2.2.3.2">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.3.2.2.cmml" xref="S4.E1.m1.2.2.2.2.3.2.2">N</ci><ci id="S4.E1.m1.2.2.2.2.3.2.3.cmml" xref="S4.E1.m1.2.2.2.2.3.2.3">TP</ci></apply><apply id="S4.E1.m1.2.2.2.2.3.3.cmml" xref="S4.E1.m1.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.3.3.1.cmml" xref="S4.E1.m1.2.2.2.2.3.3">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.3.3.2.cmml" xref="S4.E1.m1.2.2.2.2.3.3.2">N</ci><ci id="S4.E1.m1.2.2.2.2.3.3.3.cmml" xref="S4.E1.m1.2.2.2.2.3.3.3">GT</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">P_{c}=\frac{{\rm N}_{{\rm TP}}}{{\rm N}_{{\rm TP}}+{\rm N}_{{\rm FP}}}~{}~{}~{}~{}~{}~{}~{}R_{c}=\frac{{\rm N}_{{\rm TP}}}{{\rm N}_{{\rm GT}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.p2.5" class="ltx_p">where <math id="S4.p2.4.m1.1" class="ltx_Math" alttext="{\rm N}_{{\rm GT}}" display="inline"><semantics id="S4.p2.4.m1.1a"><msub id="S4.p2.4.m1.1.1" xref="S4.p2.4.m1.1.1.cmml"><mi mathvariant="normal" id="S4.p2.4.m1.1.1.2" xref="S4.p2.4.m1.1.1.2.cmml">N</mi><mi id="S4.p2.4.m1.1.1.3" xref="S4.p2.4.m1.1.1.3.cmml">GT</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.4.m1.1b"><apply id="S4.p2.4.m1.1.1.cmml" xref="S4.p2.4.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.4.m1.1.1.1.cmml" xref="S4.p2.4.m1.1.1">subscript</csymbol><ci id="S4.p2.4.m1.1.1.2.cmml" xref="S4.p2.4.m1.1.1.2">N</ci><ci id="S4.p2.4.m1.1.1.3.cmml" xref="S4.p2.4.m1.1.1.3">GT</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m1.1c">{\rm N}_{{\rm GT}}</annotation></semantics></math> is the number of ground-truth objects.
As the confidence threshold <math id="S4.p2.5.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S4.p2.5.m2.1a"><mi id="S4.p2.5.m2.1.1" xref="S4.p2.5.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.p2.5.m2.1b"><ci id="S4.p2.5.m2.1.1.cmml" xref="S4.p2.5.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m2.1c">c</annotation></semantics></math> changes, the precision-recall curve is plotted and the Average Precision (AP) is obtained by calculating the area under the curve. Finally, mAP is obtained by averaging the AP values of all categories.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Error definitions.</span>
Due to the complexity of calculating mAP, it prevents researchers from further analyzing the performance, and two detectors with the same mAP value may have different strengths and weaknesses. To address this issue, we segment the detection errors into several separate groups and measure the contribution of each error type.
Specifically, based on the taxonomy of 2D detection error proposed in TIDE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> (including classification error, localization error, both classification and localization error, duplication error, background error, and missing error. See Table <a href="#S4.T3" title="Table 3 ‣ 4 Diagnosis Toolkit ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for details), we further propose two modifications for 3D object detection, including the sub-error types of localization and the ranking error.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Sub-error types of localization.</span>
Since the localization of the 3D bounding box is determined by many factors, we further divide the localization error into three sub-errors to explore the effect of predicted location, dimension, and orientation on inaccurate localization.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Ranking error.</span> To describe each object in the scene, the detection model needs to output the 3D bounding box and its confidence. However, some detections may have higher confidence but lower accuracy. In the process of calculating AP, the detected boxes are sorted by <em id="S4.p5.1.2" class="ltx_emph ltx_font_italic">descending confidence</em> and the precision of the high-confidence detections is calculated first. Therefore, the misalignment between confidence and the quality of the box naturally leads to the decline of AP and brings about possible room for improvement.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Weighting the errors.</span> We use <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="\Delta{\rm AP}" display="inline"><semantics id="S4.p6.1.m1.1a"><mrow id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.p6.1.m1.1.1.2" xref="S4.p6.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.1" xref="S4.p6.1.m1.1.1.1.cmml">​</mo><mi id="S4.p6.1.m1.1.1.3" xref="S4.p6.1.m1.1.1.3.cmml">AP</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><apply id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"><times id="S4.p6.1.m1.1.1.1.cmml" xref="S4.p6.1.m1.1.1.1"></times><ci id="S4.p6.1.m1.1.1.2.cmml" xref="S4.p6.1.m1.1.1.2">Δ</ci><ci id="S4.p6.1.m1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.3">AP</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">\Delta{\rm AP}</annotation></semantics></math> to quantify the impact of each error type. In particular, similar to TIDE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, we independently fix the errors (called “oracle”, see Table <a href="#S4.T3" title="Table 3 ‣ 4 Diagnosis Toolkit ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for the details), and compute the change of AP to measure the impact of this error type by:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\Delta{\rm AP}_{o}={\rm AP}_{o}-{\rm AP}," display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml"><mi mathvariant="normal" id="S4.E2.m1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.2.1" xref="S4.E2.m1.1.1.1.1.2.1.cmml">​</mo><msub id="S4.E2.m1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.3.2" xref="S4.E2.m1.1.1.1.1.2.3.2.cmml">AP</mi><mi id="S4.E2.m1.1.1.1.1.2.3.3" xref="S4.E2.m1.1.1.1.1.2.3.3.cmml">o</mi></msub></mrow><mo id="S4.E2.m1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.cmml"><msub id="S4.E2.m1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.3.2.cmml"><mi id="S4.E2.m1.1.1.1.1.3.2.2" xref="S4.E2.m1.1.1.1.1.3.2.2.cmml">AP</mi><mi id="S4.E2.m1.1.1.1.1.3.2.3" xref="S4.E2.m1.1.1.1.1.3.2.3.cmml">o</mi></msub><mo id="S4.E2.m1.1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.1.3.1.cmml">−</mo><mi id="S4.E2.m1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.3.3.cmml">AP</mi></mrow></mrow><mo id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"></eq><apply id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2"><times id="S4.E2.m1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1"></times><ci id="S4.E2.m1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2">Δ</ci><apply id="S4.E2.m1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.3.2">AP</ci><ci id="S4.E2.m1.1.1.1.1.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3">𝑜</ci></apply></apply><apply id="S4.E2.m1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3"><minus id="S4.E2.m1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1"></minus><apply id="S4.E2.m1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2">AP</ci><ci id="S4.E2.m1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3">𝑜</ci></apply><ci id="S4.E2.m1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3">AP</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\Delta{\rm AP}_{o}={\rm AP}_{o}-{\rm AP},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.p6.5" class="ltx_p">where <math id="S4.p6.2.m1.1" class="ltx_Math" alttext="{\rm AP}_{o}" display="inline"><semantics id="S4.p6.2.m1.1a"><msub id="S4.p6.2.m1.1.1" xref="S4.p6.2.m1.1.1.cmml"><mi id="S4.p6.2.m1.1.1.2" xref="S4.p6.2.m1.1.1.2.cmml">AP</mi><mi id="S4.p6.2.m1.1.1.3" xref="S4.p6.2.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p6.2.m1.1b"><apply id="S4.p6.2.m1.1.1.cmml" xref="S4.p6.2.m1.1.1"><csymbol cd="ambiguous" id="S4.p6.2.m1.1.1.1.cmml" xref="S4.p6.2.m1.1.1">subscript</csymbol><ci id="S4.p6.2.m1.1.1.2.cmml" xref="S4.p6.2.m1.1.1.2">AP</ci><ci id="S4.p6.2.m1.1.1.3.cmml" xref="S4.p6.2.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.2.m1.1c">{\rm AP}_{o}</annotation></semantics></math> is the <math id="S4.p6.3.m2.1" class="ltx_Math" alttext="{\rm AP}" display="inline"><semantics id="S4.p6.3.m2.1a"><mi id="S4.p6.3.m2.1.1" xref="S4.p6.3.m2.1.1.cmml">AP</mi><annotation-xml encoding="MathML-Content" id="S4.p6.3.m2.1b"><ci id="S4.p6.3.m2.1.1.cmml" xref="S4.p6.3.m2.1.1">AP</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.3.m2.1c">{\rm AP}</annotation></semantics></math> after applying the oracle <math id="S4.p6.4.m3.1" class="ltx_Math" alttext="o" display="inline"><semantics id="S4.p6.4.m3.1a"><mi id="S4.p6.4.m3.1.1" xref="S4.p6.4.m3.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S4.p6.4.m3.1b"><ci id="S4.p6.4.m3.1.1.cmml" xref="S4.p6.4.m3.1.1">𝑜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.4.m3.1c">o</annotation></semantics></math>. In this way, we can capture the effect of each error type on the final metric, and a lower <math id="S4.p6.5.m4.1" class="ltx_Math" alttext="\Delta{\rm AP}" display="inline"><semantics id="S4.p6.5.m4.1a"><mrow id="S4.p6.5.m4.1.1" xref="S4.p6.5.m4.1.1.cmml"><mi mathvariant="normal" id="S4.p6.5.m4.1.1.2" xref="S4.p6.5.m4.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.p6.5.m4.1.1.1" xref="S4.p6.5.m4.1.1.1.cmml">​</mo><mi id="S4.p6.5.m4.1.1.3" xref="S4.p6.5.m4.1.1.3.cmml">AP</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.5.m4.1b"><apply id="S4.p6.5.m4.1.1.cmml" xref="S4.p6.5.m4.1.1"><times id="S4.p6.5.m4.1.1.1.cmml" xref="S4.p6.5.m4.1.1.1"></times><ci id="S4.p6.5.m4.1.1.2.cmml" xref="S4.p6.5.m4.1.1.2">Δ</ci><ci id="S4.p6.5.m4.1.1.3.cmml" xref="S4.p6.5.m4.1.1.3">AP</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.5.m4.1c">\Delta{\rm AP}</annotation></semantics></math> of the given error type indicates the model performs better at this part.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">Due to current datasets generally adopting different evaluation metrics, the detailed implementations of TIDE3D are also slightly different for these datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>. More details of the specific implementations are provided in Appendix <a href="#S1.SS3" title="A.3 Implementation of TIDE3D ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:5085.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-50.6pt,594.0pt) scale(0.810629604514514,0.810629604514514) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S4.T3.1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.2.1.1.1.1" class="ltx_p">Error Type</span>
</span>
</th>
<th id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:199.2pt;">
<span id="S4.T3.1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.2.1.2.1.1" class="ltx_p">Definition</span>
</span>
</th>
<th id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:227.6pt;">
<span id="S4.T3.1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.2.1.3.1.1" class="ltx_p">Oracle</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.3.1" class="ltx_tr">
<td id="S4.T3.1.1.3.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S4.T3.1.1.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.3.1.1.1.1" class="ltx_p">Classification</span>
</span>
</td>
<td id="S4.T3.1.1.3.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:199.2pt;">
<span id="S4.T3.1.1.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.3.1.2.1.1" class="ltx_p">Incorrect classification and correct localization.</span>
</span>
</td>
<td id="S4.T3.1.1.3.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:227.6pt;" rowspan="2">
<span id="S4.T3.1.1.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.3.1.3.1.1" class="ltx_p">Correct the category classification / location of detection, and delete this detection if it becomes duplicated.</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.1.4.2" class="ltx_tr">
<td id="S4.T3.1.1.4.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S4.T3.1.1.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.4.2.1.1.1" class="ltx_p">Localization</span>
</span>
</td>
<td id="S4.T3.1.1.4.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:199.2pt;">
<span id="S4.T3.1.1.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.4.2.2.1.1" class="ltx_p">Correct classification and incorrect localization.</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.1.5.3" class="ltx_tr">
<td id="S4.T3.1.1.5.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S4.T3.1.1.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.5.3.1.1.1" class="ltx_p">Both Cls. and Loc.</span>
</span>
</td>
<td id="S4.T3.1.1.5.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:199.2pt;">
<span id="S4.T3.1.1.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.5.3.2.1.1" class="ltx_p">Incorrect classification and incorrect localization.</span>
</span>
</td>
<td id="S4.T3.1.1.5.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:227.6pt;" rowspan="3">
<span id="S4.T3.1.1.5.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.5.3.3.1.1" class="ltx_p">Delete the inaccurate / duplicate detection.</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.1.6.4" class="ltx_tr">
<td id="S4.T3.1.1.6.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S4.T3.1.1.6.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.6.4.1.1.1" class="ltx_p">Duplication</span>
</span>
</td>
<td id="S4.T3.1.1.6.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:199.2pt;">
<span id="S4.T3.1.1.6.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.6.4.2.1.1" class="ltx_p">Correct classification and correct localization, but the corresponding GT has been matched by another higher-scoring detection.</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.1.7.5" class="ltx_tr">
<td id="S4.T3.1.1.7.5.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S4.T3.1.1.7.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.7.5.1.1.1" class="ltx_p">Background</span>
</span>
</td>
<td id="S4.T3.1.1.7.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:199.2pt;">
<span id="S4.T3.1.1.7.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.7.5.2.1.1" class="ltx_p">Detection from the background area.</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.1.8.6" class="ltx_tr">
<td id="S4.T3.1.1.8.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S4.T3.1.1.8.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.8.6.1.1.1" class="ltx_p">Missing</span>
</span>
</td>
<td id="S4.T3.1.1.8.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:199.2pt;">
<span id="S4.T3.1.1.8.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.8.6.2.1.1" class="ltx_p">Undetected GT not covered by classification and localization error.</span>
</span>
</td>
<td id="S4.T3.1.1.8.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:227.6pt;">
<span id="S4.T3.1.1.8.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.8.6.3.1.1" class="ltx_p">Delete the missed ground truth.</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S4.T3.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.2.1.1" class="ltx_p">Ranking</span>
</span>
</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:199.2pt;">
<span id="S4.T3.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.3.1.1" class="ltx_p">Inconsistency between confidence and localization quality of detections.</span>
</span>
</td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" style="width:227.6pt;">
<span id="S4.T3.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.1.1.1" class="ltx_p">Sort the detections by descending <math id="S4.T3.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="{\rm IoU}" display="inline"><semantics id="S4.T3.1.1.1.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.1.m1.1.1.cmml">IoU</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.1.m1.1.1">IoU</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.1.m1.1c">{\rm IoU}</annotation></semantics></math> score and calculate the precision of accurate detections first.</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S4.T3.3.1" class="ltx_text ltx_font_bold">Definition and oracle of detection errors.</span></figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Approach</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We first discuss the effects of common training settings on the final accuracy of current methods and then provide efficient and fair training recipes for further work. Furthermore, we also show the state-of-the-art (SOTA)
methods can be further promoted in performance with the summarized training recipes and some existing techniques in the KITTI-3D benchmark.
Besides, the detailed features of the detection algorithms are evaluated with the proposed TIDE3D, and we also show our diagnosis tool can be used to explore some open issues.
Finally, we discuss the conflicts between KITTI-3D and nuScenes datasets.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.4" class="ltx_p"><span id="S5.p2.4.1" class="ltx_text ltx_font_bold">Training recipes.</span> We observe that current methods generally adopt different training recipes, <em id="S5.p2.4.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.p2.4.3" class="ltx_text"></span> the training schedule in KITTI-3D ranges from 60 epochs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> to 200 epochs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>. We show that training differences like this greatly affect the performance of the algorithms and may mislead the latecomers to some extent, and it is necessary to formulate standard training recipes. Specifically, in KITTI-3D, we take the 70-epoch schedule as baseline (1<math id="S5.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.p2.1.m1.1a"><mo id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><times id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\times</annotation></semantics></math>) and also evaluate the models under 2<math id="S5.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.p2.2.m2.1a"><mo id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><times id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">\times</annotation></semantics></math> and 3<math id="S5.p2.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.p2.3.m3.1a"><mo id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><times id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">\times</annotation></semantics></math> schedules. Similarly, the corresponding settings in nuScenes are 12, 24, and 36 epochs. We recommend the 2<math id="S5.p2.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.p2.4.m4.1a"><mo id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><times id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.1c">\times</annotation></semantics></math> schedule because this is the closest one to the common choice of other algorithms, and use other schedulers for further evaluation.
Furthermore, we also ablate other choices in model training and provide a general training recipe for image-based 3D detectors. We report, in addition to some basic choices such as random crop or horizontal flip, the photometric distortion and one-cycle learning rate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>]</cite> are also highly effective for our task and should be included in the conventional setting.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Promoted baselines in KITTI-3D.</span>
We also find, although KITTI-3D is released for a longer time and more methods are proposed on KITTI-3D benchmark, the baseline models in KITTI-3D are still <em id="S5.p3.1.2" class="ltx_emph ltx_font_italic">under-tuned</em> compared to these on nuScenes.
Here we show current baseline models can be greatly promoted. We report that a two-year-ago model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> (re-trained with pre-trained backbone, pseudo-labels, and the summarized training recipe) can achieve 17.65 <math id="S5.p3.1.m1.1" class="ltx_Math" alttext="\rm{AP}_{40}" display="inline"><semantics id="S5.p3.1.m1.1a"><msub id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml"><mi id="S5.p3.1.m1.1.1.2" xref="S5.p3.1.m1.1.1.2.cmml">AP</mi><mn id="S5.p3.1.m1.1.1.3" xref="S5.p3.1.m1.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><apply id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p3.1.m1.1.1.1.cmml" xref="S5.p3.1.m1.1.1">subscript</csymbol><ci id="S5.p3.1.m1.1.1.2.cmml" xref="S5.p3.1.m1.1.1.2">AP</ci><cn type="integer" id="S5.p3.1.m1.1.1.3.cmml" xref="S5.p3.1.m1.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">\rm{AP}_{40}</annotation></semantics></math> on the KITTI-3D validation set, surpassing the SOTA by 1.11 points. Even so, we believe this number can be improved with further tuning or other tricks (<em id="S5.p3.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.p3.1.4" class="ltx_text"></span> test-time augmentation and model-ensemble <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>).</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Dataset and metrics.</span> KITTI-3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> and nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> are the most concerned datasets in the image-based 3D detection field, however, they are dominated by different detection pipelines. We find that they are different in many aspects, and nuScene is generally considered to be more friendly to image-based algorithms. We adopt nuScenes-style evaluation on the KITTI-3D (and vice versa) and provide our discussion based on the cross-metric evaluation. More details are given in Section <a href="#S6.SS4" title="6.4 Metric Analysis for KITTI-3D and nuScenes ‣ 6 Experiments and Analysis ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Detailed analysis with TIDE3D.</span>
With the TIDE3D introduced in Section <a href="#S4" title="4 Diagnosis Toolkit ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we can: (i) Analyze the detailed characteristics of 3D object detection models, whether it is an image-based model or LiDAR-based model, evaluated with KITTI-3D-style or nuScene-style metrics, and designed for outdoor or indoor scenes (see Appendix <a href="#S1.SS5" title="A.5 More TIDE3D Analysis ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.5</span></a> for the analyses for indoor dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> and LiDAR-based models).
(ii) Conduct solid ablation studies and explore the action mechanisms for specific designs/modules.</p>
</div>
<figure id="S5.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.05447/assets/x1.png" id="S5.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="218" height="131" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.05447/assets/x2.png" id="S5.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="218" height="131" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Learning curves on nuScenes (<span id="S5.F1.8.1" class="ltx_text ltx_font_bold">Left</span>) and KITTI-3D (<span id="S5.F1.9.2" class="ltx_text ltx_font_bold">Right</span>) with different detection models. With the 3<math id="S5.F1.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.F1.3.m1.1b"><mo id="S5.F1.3.m1.1.1" xref="S5.F1.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.F1.3.m1.1c"><times id="S5.F1.3.m1.1.1.cmml" xref="S5.F1.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.F1.3.m1.1d">\times</annotation></semantics></math> scheduler, BEVDet with R50-FPN optimized by AdamW and FCOS3D with R101-DCN-FPN optimized by SGD achieve 28.56 and 31.67 mAP on nuScenes, respectively, while GUPNet with DLA-34 optimized by Adam achieves 15.64 <math id="S5.F1.4.m2.1" class="ltx_Math" alttext="{\rm AP}_{40}" display="inline"><semantics id="S5.F1.4.m2.1b"><msub id="S5.F1.4.m2.1.1" xref="S5.F1.4.m2.1.1.cmml"><mi id="S5.F1.4.m2.1.1.2" xref="S5.F1.4.m2.1.1.2.cmml">AP</mi><mn id="S5.F1.4.m2.1.1.3" xref="S5.F1.4.m2.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S5.F1.4.m2.1c"><apply id="S5.F1.4.m2.1.1.cmml" xref="S5.F1.4.m2.1.1"><csymbol cd="ambiguous" id="S5.F1.4.m2.1.1.1.cmml" xref="S5.F1.4.m2.1.1">subscript</csymbol><ci id="S5.F1.4.m2.1.1.2.cmml" xref="S5.F1.4.m2.1.1.2">AP</ci><cn type="integer" id="S5.F1.4.m2.1.1.3.cmml" xref="S5.F1.4.m2.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F1.4.m2.1d">{\rm AP}_{40}</annotation></semantics></math>@IoU=0.7 on KITTI-3D (<em id="S5.F1.10.3" class="ltx_emph ltx_font_italic">validation</em> set).
For better presentation, we present the curves by moving the average with windows size = 5. The original curves are shown in Appendix <a href="#S1.SS4" title="A.4 Training Curve ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a>.</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments and Analysis</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text ltx_font_bold">Dataset and metrics.</span> We conduct experiments on KITTI-3D and nuScenes datasets, and
the detailed introduction for these two datasets is provided in Appendix <a href="#S1.SS1" title="A.1 Datasets and Metrics ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>.
Due to access restrictions of testing servers, the evaluation is conducted on <em id="S6.p1.1.2" class="ltx_emph ltx_font_italic">validation</em> set, and we further apply cross-dataset metrics for better evaluation, <em id="S6.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S6.p1.1.4" class="ltx_text"></span> applying nuScenes-style metrics on the KITTI-3D dataset.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Baseline models.</span> In this work, we mainly focus on the baselines based on the popular detection pipeline, including the CenterNet pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, the BEV pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, and the FCOS3D pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>, <a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Other settings.</span> To replicate the reported performance and minimize the impact of complex architectures and additional hyper-parameters, we adopt the simplest default configurations provided in the official code for most methods. Specifically, we use basic training schedules, simple architectures, and standard data augmentation techniques as the default settings. Further details beyond to these settings are provided in according sections.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Training Recipes</h3>

<div id="S6.SS1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS1.p1.2" class="ltx_p"><span id="S6.SS1.p1.2.1" class="ltx_text ltx_font_bold">Training schedules.</span> We first present the effects of training schedules. Specifically, we experiment with three different schedules, ranging from 1<math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mo id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><times id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">\times</annotation></semantics></math> to 3<math id="S6.SS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p1.2.m2.1a"><mo id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><times id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">\times</annotation></semantics></math>, for both KITTI-3D and nuScenes.
Notably, we plot the training curves of all three schedules in the same figure to facilitate comparison. The AP curves shown in Figure <a href="#S5.F1" title="Figure 1 ‣ 5 Approach ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> indicate that schedules have a significant impact on the final performance, highlighting the importance of a standard scheduler for a fair comparison. Furthermore, the following observations can be summarized from these curves:</p>
</div>
<div id="S6.SS1.p2" class="ltx_para ltx_noindent">
<p id="S6.SS1.p2.1" class="ltx_p"><span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_bold">i.</span> Different detection models require varying numbers of epochs as the optimal settings.
For example, BEVDet exhibits significant and smooth performance improvements as the number of training epochs increases, while FCOS3D achieves near-optimal performance faster with perturbation after learning rate decay. This discrepancy may stem from the difference in optimizers, iterations, and detection pipelines. Intuitively, required training time can substantially vary across different settings, which may reduce the fairness of comparisons between them.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para ltx_noindent">
<p id="S6.SS1.p3.4" class="ltx_p"><span id="S6.SS1.p3.4.1" class="ltx_text ltx_font_bold">ii.</span>
Increasing the training procedure enhances the stability and precision of the fine-tuned model. Even after completing the 3<math id="S6.SS1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p3.1.m1.1a"><mo id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><times id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">\times</annotation></semantics></math> training, the trend of mAP/<math id="S6.SS1.p3.2.m2.1" class="ltx_Math" alttext="\rm{AP}_{40}" display="inline"><semantics id="S6.SS1.p3.2.m2.1a"><msub id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml"><mi id="S6.SS1.p3.2.m2.1.1.2" xref="S6.SS1.p3.2.m2.1.1.2.cmml">AP</mi><mn id="S6.SS1.p3.2.m2.1.1.3" xref="S6.SS1.p3.2.m2.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><apply id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS1.p3.2.m2.1.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S6.SS1.p3.2.m2.1.1.2.cmml" xref="S6.SS1.p3.2.m2.1.1.2">AP</ci><cn type="integer" id="S6.SS1.p3.2.m2.1.1.3.cmml" xref="S6.SS1.p3.2.m2.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">\rm{AP}_{40}</annotation></semantics></math> improvement continues to exist. The commonly accepted training schedule (mostly 1<math id="S6.SS1.p3.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p3.3.m3.1a"><mo id="S6.SS1.p3.3.m3.1.1" xref="S6.SS1.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.3.m3.1b"><times id="S6.SS1.p3.3.m3.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.3.m3.1c">\times</annotation></semantics></math> or 2<math id="S6.SS1.p3.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p3.4.m4.1a"><mo id="S6.SS1.p3.4.m4.1.1" xref="S6.SS1.p3.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.4.m4.1b"><times id="S6.SS1.p3.4.m4.1.1.cmml" xref="S6.SS1.p3.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.4.m4.1c">\times</annotation></semantics></math> on nuScenes) may be insufficient to unlock the full potential of a model. Therefore, the standard training procedure remains further discussed.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para ltx_noindent">
<p id="S6.SS1.p4.3" class="ltx_p"><span id="S6.SS1.p4.3.1" class="ltx_text ltx_font_bold">iii.</span>
We observe a significant drop and a recovery at the beginning of the finetuning stage of 1<math id="S6.SS1.p4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p4.1.m1.1a"><mo id="S6.SS1.p4.1.m1.1.1" xref="S6.SS1.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.1.m1.1b"><times id="S6.SS1.p4.1.m1.1.1.cmml" xref="S6.SS1.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.1.m1.1c">\times</annotation></semantics></math> training in the experiments of BEVDet and GUPNet. With training time increasing from 2<math id="S6.SS1.p4.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p4.2.m2.1a"><mo id="S6.SS1.p4.2.m2.1.1" xref="S6.SS1.p4.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.2.m2.1b"><times id="S6.SS1.p4.2.m2.1.1.cmml" xref="S6.SS1.p4.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.2.m2.1c">\times</annotation></semantics></math> to 3<math id="S6.SS1.p4.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p4.3.m3.1a"><mo id="S6.SS1.p4.3.m3.1.1" xref="S6.SS1.p4.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.3.m3.1b"><times id="S6.SS1.p4.3.m3.1.1.cmml" xref="S6.SS1.p4.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.3.m3.1c">\times</annotation></semantics></math>, this phenomenon alleviates and even disappears. We suppose that longer training time helps the network learn more robust features, which prevents the overfitting brought by the sudden decay of the learning rate. More training steps at a small learning rate also help counter this issue.</p>
</div>
<div id="S6.SS1.p5" class="ltx_para ltx_noindent">
<p id="S6.SS1.p5.1" class="ltx_p"><span id="S6.SS1.p5.1.1" class="ltx_text ltx_font_bold">iv.</span> Even when the training is about to be completed, we still observe performances are unstable (see Appendix <a href="#S1.SS4" title="A.4 Training Curve ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a> for the un-smoothed curves). This phenomenon is particularly evident for the KITTI-3D-style metrics. Based on this, the comparison based on KITTI-3D metrics should be carefully conducted.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<div id="S6.T4.10" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:118.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-64.5pt,22.0pt) scale(0.728902173888649,0.728902173888649) ;">
<table id="S6.T4.10.10" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T4.2.2.2" class="ltx_tr">
<td id="S6.T4.2.2.2.3" class="ltx_td ltx_border_r" rowspan="2"></td>
<td id="S6.T4.2.2.2.4" class="ltx_td"></td>
<td id="S6.T4.2.2.2.5" class="ltx_td ltx_border_r"></td>
<td id="S6.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r" colspan="3">
<math id="S6.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="{\rm AP}_{40}" display="inline"><semantics id="S6.T4.1.1.1.1.m1.1a"><msub id="S6.T4.1.1.1.1.m1.1.1" xref="S6.T4.1.1.1.1.m1.1.1.cmml"><mi id="S6.T4.1.1.1.1.m1.1.1.2" xref="S6.T4.1.1.1.1.m1.1.1.2.cmml">AP</mi><mn id="S6.T4.1.1.1.1.m1.1.1.3" xref="S6.T4.1.1.1.1.m1.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.m1.1b"><apply id="S6.T4.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.1.1.1.1.m1.1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S6.T4.1.1.1.1.m1.1.1.2.cmml" xref="S6.T4.1.1.1.1.m1.1.1.2">AP</ci><cn type="integer" id="S6.T4.1.1.1.1.m1.1.1.3.cmml" xref="S6.T4.1.1.1.1.m1.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.m1.1c">{\rm AP}_{40}</annotation></semantics></math>@IoU=0.7</td>
<td id="S6.T4.2.2.2.2" class="ltx_td ltx_align_center" colspan="3">
<math id="S6.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="{\rm AP}_{40}" display="inline"><semantics id="S6.T4.2.2.2.2.m1.1a"><msub id="S6.T4.2.2.2.2.m1.1.1" xref="S6.T4.2.2.2.2.m1.1.1.cmml"><mi id="S6.T4.2.2.2.2.m1.1.1.2" xref="S6.T4.2.2.2.2.m1.1.1.2.cmml">AP</mi><mn id="S6.T4.2.2.2.2.m1.1.1.3" xref="S6.T4.2.2.2.2.m1.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.2.m1.1b"><apply id="S6.T4.2.2.2.2.m1.1.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.2.2.2.2.m1.1.1.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S6.T4.2.2.2.2.m1.1.1.2.cmml" xref="S6.T4.2.2.2.2.m1.1.1.2">AP</ci><cn type="integer" id="S6.T4.2.2.2.2.m1.1.1.3.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.2.m1.1c">{\rm AP}_{40}</annotation></semantics></math>@IoU=0.5</td>
</tr>
<tr id="S6.T4.10.10.11.1" class="ltx_tr">
<td id="S6.T4.10.10.11.1.1" class="ltx_td ltx_align_center">schedule</td>
<td id="S6.T4.10.10.11.1.2" class="ltx_td ltx_align_center ltx_border_r">pre-training</td>
<td id="S6.T4.10.10.11.1.3" class="ltx_td ltx_align_center">easy</td>
<td id="S6.T4.10.10.11.1.4" class="ltx_td ltx_align_center">mod.</td>
<td id="S6.T4.10.10.11.1.5" class="ltx_td ltx_align_center ltx_border_r">hard</td>
<td id="S6.T4.10.10.11.1.6" class="ltx_td ltx_align_center">easy</td>
<td id="S6.T4.10.10.11.1.7" class="ltx_td ltx_align_center">mod.</td>
<td id="S6.T4.10.10.11.1.8" class="ltx_td ltx_align_center">hard</td>
</tr>
<tr id="S6.T4.3.3.3" class="ltx_tr">
<td id="S6.T4.3.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">GUPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>
</td>
<td id="S6.T4.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t">2<math id="S6.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T4.3.3.3.1.m1.1a"><mo id="S6.T4.3.3.3.1.m1.1.1" xref="S6.T4.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T4.3.3.3.1.m1.1b"><times id="S6.T4.3.3.3.1.m1.1.1.cmml" xref="S6.T4.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.3.3.3.1.m1.1c">\times</annotation></semantics></math>
</td>
<td id="S6.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ImageNet</td>
<td id="S6.T4.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">21.48</td>
<td id="S6.T4.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">15.22</td>
<td id="S6.T4.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.79</td>
<td id="S6.T4.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t">59.30</td>
<td id="S6.T4.3.3.3.8" class="ltx_td ltx_align_center ltx_border_t">43.83</td>
<td id="S6.T4.3.3.3.9" class="ltx_td ltx_align_center ltx_border_t">38.40</td>
</tr>
<tr id="S6.T4.5.5.5" class="ltx_tr">
<td id="S6.T4.5.5.5.3" class="ltx_td ltx_align_left ltx_border_r">DID-M3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>
</td>
<td id="S6.T4.5.5.5.2" class="ltx_td ltx_align_center">
<math id="S6.T4.4.4.4.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S6.T4.4.4.4.1.m1.1a"><mo id="S6.T4.4.4.4.1.m1.1.1" xref="S6.T4.4.4.4.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S6.T4.4.4.4.1.m1.1b"><approx id="S6.T4.4.4.4.1.m1.1.1.cmml" xref="S6.T4.4.4.4.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.4.4.4.1.m1.1c">\approx</annotation></semantics></math> 3<math id="S6.T4.5.5.5.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T4.5.5.5.2.m2.1a"><mo id="S6.T4.5.5.5.2.m2.1.1" xref="S6.T4.5.5.5.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T4.5.5.5.2.m2.1b"><times id="S6.T4.5.5.5.2.m2.1.1.cmml" xref="S6.T4.5.5.5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.5.5.5.2.m2.1c">\times</annotation></semantics></math>
</td>
<td id="S6.T4.5.5.5.4" class="ltx_td ltx_align_center ltx_border_r">ImageNet</td>
<td id="S6.T4.5.5.5.5" class="ltx_td ltx_align_center">22.98</td>
<td id="S6.T4.5.5.5.6" class="ltx_td ltx_align_center">16.12</td>
<td id="S6.T4.5.5.5.7" class="ltx_td ltx_align_center ltx_border_r">14.03</td>
<td id="S6.T4.5.5.5.8" class="ltx_td ltx_align_center">-</td>
<td id="S6.T4.5.5.5.9" class="ltx_td ltx_align_center">-</td>
<td id="S6.T4.5.5.5.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T4.6.6.6" class="ltx_tr">
<td id="S6.T4.6.6.6.2" class="ltx_td ltx_align_left ltx_border_r">DEVIANT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>
</td>
<td id="S6.T4.6.6.6.1" class="ltx_td ltx_align_center">2<math id="S6.T4.6.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T4.6.6.6.1.m1.1a"><mo id="S6.T4.6.6.6.1.m1.1.1" xref="S6.T4.6.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T4.6.6.6.1.m1.1b"><times id="S6.T4.6.6.6.1.m1.1.1.cmml" xref="S6.T4.6.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.6.6.6.1.m1.1c">\times</annotation></semantics></math>
</td>
<td id="S6.T4.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r">ImageNet</td>
<td id="S6.T4.6.6.6.4" class="ltx_td ltx_align_center">24.63</td>
<td id="S6.T4.6.6.6.5" class="ltx_td ltx_align_center">16.54</td>
<td id="S6.T4.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r">14.52</td>
<td id="S6.T4.6.6.6.7" class="ltx_td ltx_align_center">61.00</td>
<td id="S6.T4.6.6.6.8" class="ltx_td ltx_align_center">46.00</td>
<td id="S6.T4.6.6.6.9" class="ltx_td ltx_align_center">40.18</td>
</tr>
<tr id="S6.T4.7.7.7" class="ltx_tr">
<td id="S6.T4.7.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">GUPNet</td>
<td id="S6.T4.7.7.7.1" class="ltx_td ltx_align_center ltx_border_t">2<math id="S6.T4.7.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T4.7.7.7.1.m1.1a"><mo id="S6.T4.7.7.7.1.m1.1.1" xref="S6.T4.7.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T4.7.7.7.1.m1.1b"><times id="S6.T4.7.7.7.1.m1.1.1.cmml" xref="S6.T4.7.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.7.7.7.1.m1.1c">\times</annotation></semantics></math>
</td>
<td id="S6.T4.7.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">None</td>
<td id="S6.T4.7.7.7.4" class="ltx_td ltx_align_center ltx_border_t">18.74</td>
<td id="S6.T4.7.7.7.5" class="ltx_td ltx_align_center ltx_border_t">13.40</td>
<td id="S6.T4.7.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.62</td>
<td id="S6.T4.7.7.7.7" class="ltx_td ltx_align_center ltx_border_t">53.80</td>
<td id="S6.T4.7.7.7.8" class="ltx_td ltx_align_center ltx_border_t">40.69</td>
<td id="S6.T4.7.7.7.9" class="ltx_td ltx_align_center ltx_border_t">35.73</td>
</tr>
<tr id="S6.T4.8.8.8" class="ltx_tr">
<td id="S6.T4.8.8.8.2" class="ltx_td ltx_align_left ltx_border_r">GUPNet</td>
<td id="S6.T4.8.8.8.1" class="ltx_td ltx_align_center">2<math id="S6.T4.8.8.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T4.8.8.8.1.m1.1a"><mo id="S6.T4.8.8.8.1.m1.1.1" xref="S6.T4.8.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T4.8.8.8.1.m1.1b"><times id="S6.T4.8.8.8.1.m1.1.1.cmml" xref="S6.T4.8.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.8.8.8.1.m1.1c">\times</annotation></semantics></math>
</td>
<td id="S6.T4.8.8.8.3" class="ltx_td ltx_align_center ltx_border_r">ImageNet</td>
<td id="S6.T4.8.8.8.4" class="ltx_td ltx_align_center">21.51</td>
<td id="S6.T4.8.8.8.5" class="ltx_td ltx_align_center">15.82</td>
<td id="S6.T4.8.8.8.6" class="ltx_td ltx_align_center ltx_border_r">13.30</td>
<td id="S6.T4.8.8.8.7" class="ltx_td ltx_align_center">60.18</td>
<td id="S6.T4.8.8.8.8" class="ltx_td ltx_align_center">44.35</td>
<td id="S6.T4.8.8.8.9" class="ltx_td ltx_align_center">39.27</td>
</tr>
<tr id="S6.T4.9.9.9" class="ltx_tr">
<td id="S6.T4.9.9.9.2" class="ltx_td ltx_align_left ltx_border_r">GUPNet</td>
<td id="S6.T4.9.9.9.1" class="ltx_td ltx_align_center">2<math id="S6.T4.9.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T4.9.9.9.1.m1.1a"><mo id="S6.T4.9.9.9.1.m1.1.1" xref="S6.T4.9.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T4.9.9.9.1.m1.1b"><times id="S6.T4.9.9.9.1.m1.1.1.cmml" xref="S6.T4.9.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.9.9.9.1.m1.1c">\times</annotation></semantics></math>
</td>
<td id="S6.T4.9.9.9.3" class="ltx_td ltx_align_center ltx_border_r">DD3D</td>
<td id="S6.T4.9.9.9.4" class="ltx_td ltx_align_center">24.25</td>
<td id="S6.T4.9.9.9.5" class="ltx_td ltx_align_center">15.82</td>
<td id="S6.T4.9.9.9.6" class="ltx_td ltx_align_center ltx_border_r">13.26</td>
<td id="S6.T4.9.9.9.7" class="ltx_td ltx_align_center">63.16</td>
<td id="S6.T4.9.9.9.8" class="ltx_td ltx_align_center">44.60</td>
<td id="S6.T4.9.9.9.9" class="ltx_td ltx_align_center">40.08</td>
</tr>
<tr id="S6.T4.10.10.10" class="ltx_tr">
<td id="S6.T4.10.10.10.2" class="ltx_td ltx_align_left ltx_border_r">GUPNet + PL</td>
<td id="S6.T4.10.10.10.1" class="ltx_td ltx_align_center">2<math id="S6.T4.10.10.10.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T4.10.10.10.1.m1.1a"><mo id="S6.T4.10.10.10.1.m1.1.1" xref="S6.T4.10.10.10.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T4.10.10.10.1.m1.1b"><times id="S6.T4.10.10.10.1.m1.1.1.cmml" xref="S6.T4.10.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.10.10.10.1.m1.1c">\times</annotation></semantics></math>
</td>
<td id="S6.T4.10.10.10.3" class="ltx_td ltx_align_center ltx_border_r">DD3D</td>
<td id="S6.T4.10.10.10.4" class="ltx_td ltx_align_center">24.71</td>
<td id="S6.T4.10.10.10.5" class="ltx_td ltx_align_center">17.25</td>
<td id="S6.T4.10.10.10.6" class="ltx_td ltx_align_center ltx_border_r">15.21</td>
<td id="S6.T4.10.10.10.7" class="ltx_td ltx_align_center">63.73</td>
<td id="S6.T4.10.10.10.8" class="ltx_td ltx_align_center">46.56</td>
<td id="S6.T4.10.10.10.9" class="ltx_td ltx_align_center">42.45</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
<span id="S6.T4.14.1" class="ltx_text ltx_font_bold">Promoted baselines on KITTI-3D <em id="S6.T4.14.1.1" class="ltx_emph ltx_font_italic">validation</em> set</span>. We report the 3D object detection performance of GUPNet with various pretrained backbones, evaluated by <math id="S6.T4.12.m1.1" class="ltx_Math" alttext="\rm{AP}_{40}" display="inline"><semantics id="S6.T4.12.m1.1b"><msub id="S6.T4.12.m1.1.1" xref="S6.T4.12.m1.1.1.cmml"><mi id="S6.T4.12.m1.1.1.2" xref="S6.T4.12.m1.1.1.2.cmml">AP</mi><mn id="S6.T4.12.m1.1.1.3" xref="S6.T4.12.m1.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T4.12.m1.1c"><apply id="S6.T4.12.m1.1.1.cmml" xref="S6.T4.12.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.12.m1.1.1.1.cmml" xref="S6.T4.12.m1.1.1">subscript</csymbol><ci id="S6.T4.12.m1.1.1.2.cmml" xref="S6.T4.12.m1.1.1.2">AP</ci><cn type="integer" id="S6.T4.12.m1.1.1.3.cmml" xref="S6.T4.12.m1.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.12.m1.1d">\rm{AP}_{40}</annotation></semantics></math>@IoU=0.7/0.5. We also present SOTA methods for reference.
Note that both DID-M3D and DEVIANT are built based on the GUPNet baseline, and our modifications are only involved in the training phase, which shows the importance of building fair training recipes again.</figcaption>
</figure>
<div id="S6.SS1.p6" class="ltx_para ltx_noindent">
<p id="S6.SS1.p6.1" class="ltx_p"><span id="S6.SS1.p6.1.1" class="ltx_text ltx_font_bold">Other training recipes.</span> Due to strict geometric relations between the 2D image plane and the 3D world space, some complicated data augmentations are hard to be applied, and the horizontal flip is the common choice for existing methods. Note some simple geometric augmentations, like center crop and resize, are also commonly used in the KITTI-3D dataset, and see Appendix <a href="#S1.SS6" title="A.6 Discussion about Training Recipes ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.6</span></a> for more discussions on such data augmentations. Here we show the photometric distortion and the one-cycle learning rate schedule are two cost-free training tricks to improve the detection accuracy (especially in KITTI-3D) and show their effectiveness with two representative baselines in Table <a href="#S6.T5" title="Table 5 ‣ 6.1 Training Recipes ‣ 6 Experiments and Analysis ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
We can find that both of them improve the GUPNet by a significant margin and can work together. Although nuScenes is a large-scale dataset, applying photometric distortion still brings a modest mAP improvement. Besides, we find the effects of one-cycle learning rate on nuScenes baselines are unstable, so we omit the corresponding cells to avoid misleading.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<div id="S6.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:79.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(68.8pt,-12.6pt) scale(1.46493007490662,1.46493007490662) ;">
<table id="S6.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.1.1.2.1" class="ltx_tr">
<th id="S6.T5.1.1.2.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S6.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;">baseline</th>
<th id="S6.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;">w/ distortion</th>
<th id="S6.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;">w/ one-cycle lr</th>
<th id="S6.T5.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;">full</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.1.1.1" class="ltx_tr">
<th id="S6.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">GUPNet [<math id="S6.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\rm{AP}_{40}" display="inline"><semantics id="S6.T5.1.1.1.1.m1.1a"><msub id="S6.T5.1.1.1.1.m1.1.1" xref="S6.T5.1.1.1.1.m1.1.1.cmml"><mi id="S6.T5.1.1.1.1.m1.1.1.2" xref="S6.T5.1.1.1.1.m1.1.1.2.cmml">AP</mi><mn id="S6.T5.1.1.1.1.m1.1.1.3" xref="S6.T5.1.1.1.1.m1.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.1.m1.1b"><apply id="S6.T5.1.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T5.1.1.1.1.m1.1.1.1.cmml" xref="S6.T5.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S6.T5.1.1.1.1.m1.1.1.2.cmml" xref="S6.T5.1.1.1.1.m1.1.1.2">AP</ci><cn type="integer" id="S6.T5.1.1.1.1.m1.1.1.3.cmml" xref="S6.T5.1.1.1.1.m1.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.1.m1.1c">\rm{AP}_{40}</annotation></semantics></math>]</th>
<td id="S6.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">15.26</td>
<td id="S6.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">16.54</td>
<td id="S6.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">15.87</td>
<td id="S6.T5.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">16.83</td>
</tr>
<tr id="S6.T5.1.1.3.1" class="ltx_tr">
<th id="S6.T5.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">BEVDet [mAP]</th>
<td id="S6.T5.1.1.3.1.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">25.12</td>
<td id="S6.T5.1.1.3.1.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">25.20</td>
<td id="S6.T5.1.1.3.1.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S6.T5.1.1.3.1.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>The photometric distortion and one-cycle learning rate are also effective training choices.</figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Promoted KITTI-3D Baselines.</h3>

<div id="S6.SS2.p1" class="ltx_para ltx_noindent">
<p id="S6.SS2.p1.1" class="ltx_p"><span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_bold">Leveraging pre-training weights.</span>
We show the impact of weight initialization on detection models in Table <a href="#S6.T4" title="Table 4 ‣ 6.1 Training Recipes ‣ 6 Experiments and Analysis ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In particular, we compare three initial weights, including random initialization, ImageNet pretraining, and DD3D pertaining <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>. We can find that: <span id="S6.SS2.p1.1.2" class="ltx_text ltx_font_bold">i.</span> Similar to other computer vision tasks, model pre-training provides useful priors to the models and improve the performance significantly. <span id="S6.SS2.p1.1.3" class="ltx_text ltx_font_bold">ii.</span> As the data domains and proxy tasks become more similar (see Table <a href="#S6.T6" title="Table 6 ‣ 6.2 Promoted KITTI-3D Baselines. ‣ 6 Experiments and Analysis ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), the pre-training weights also become more effective. Based on the above observations, designing custom pre-training algorithms, especially in an unsupervised manner, should be a promising research direction for future work.</p>
</div>
<figure id="S6.T6" class="ltx_table">
<div id="S6.T6.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:75.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(62.2pt,-10.9pt) scale(1.40216437142101,1.40216437142101) ;">
<table id="S6.T6.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T6.1.1.1" class="ltx_tr">
<th id="S6.T6.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;"></th>
<th id="S6.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:7.0pt;padding-right:7.0pt;">domain</th>
<th id="S6.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:7.0pt;padding-right:7.0pt;">proxy task</th>
<th id="S6.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:7.0pt;padding-right:7.0pt;"># img. <math id="S6.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T6.1.1.1.1.m1.1a"><mo id="S6.T6.1.1.1.1.m1.1.1" xref="S6.T6.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T6.1.1.1.1.m1.1b"><times id="S6.T6.1.1.1.1.m1.1.1.cmml" xref="S6.T6.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.1.1.1.1.m1.1c">\times</annotation></semantics></math> # ep.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T6.2.2.2" class="ltx_tr">
<th id="S6.T6.2.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">ImageNet</th>
<td id="S6.T6.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">daily scenes</td>
<td id="S6.T6.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">classification</td>
<td id="S6.T6.2.2.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">1.3M <math id="S6.T6.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T6.2.2.2.1.m1.1a"><mo id="S6.T6.2.2.2.1.m1.1.1" xref="S6.T6.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T6.2.2.2.1.m1.1b"><times id="S6.T6.2.2.2.1.m1.1.1.cmml" xref="S6.T6.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.2.2.2.1.m1.1c">\times</annotation></semantics></math> 100</td>
</tr>
<tr id="S6.T6.3.3.3" class="ltx_tr">
<th id="S6.T6.3.3.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">DD3D</th>
<td id="S6.T6.3.3.3.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">driving scenes</td>
<td id="S6.T6.3.3.3.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">depth estimation</td>
<td id="S6.T6.3.3.3.1" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">15M <math id="S6.T6.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T6.3.3.3.1.m1.1a"><mo id="S6.T6.3.3.3.1.m1.1.1" xref="S6.T6.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T6.3.3.3.1.m1.1b"><times id="S6.T6.3.3.3.1.m1.1.1.cmml" xref="S6.T6.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.3.3.3.1.m1.1c">\times</annotation></semantics></math> 12.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Information of different pre-training models. Ep. denotes the pretraining epochs.</figcaption>
</figure>
<div id="S6.SS2.p2" class="ltx_para ltx_noindent">
<p id="S6.SS2.p2.1" class="ltx_p"><span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_bold">Pseudo-labeling.</span> According to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>, <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, we generate the pseudo-labels (only for the 3,712 training images) from a LiDAR-based model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite>. Then we use them to train GUPNet and report the performance in Table <a href="#S6.T4" title="Table 4 ‣ 6.1 Training Recipes ‣ 6 Experiments and Analysis ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. According to the results, the final result of our GUPNet can achieve 17.65 <math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="\rm{AP}_{40}" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><msub id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml"><mi id="S6.SS2.p2.1.m1.1.1.2" xref="S6.SS2.p2.1.m1.1.1.2.cmml">AP</mi><mn id="S6.SS2.p2.1.m1.1.1.3" xref="S6.SS2.p2.1.m1.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><apply id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p2.1.m1.1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S6.SS2.p2.1.m1.1.1.2.cmml" xref="S6.SS2.p2.1.m1.1.1.2">AP</ci><cn type="integer" id="S6.SS2.p2.1.m1.1.1.3.cmml" xref="S6.SS2.p2.1.m1.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">\rm{AP}_{40}</annotation></semantics></math> under the moderate setting, surpassing the previous SOTA (DEVIANT) by 0.71 points. We emphasize that we do not make any changes to the detection model, and the modifications are only involved in the training phase. This result shows the urgency and importance of establishing a fair and efficient training recipe.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>TIDE3D Analysis</h3>

<figure id="S6.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05447/assets/figures/kitti_tide.png" id="S6.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="329" height="177" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>KITTI dataset</figcaption>
<figure id="S6.F2.sf2" class="ltx_figure"><img src="/html/2310.05447/assets/figures/nusc_tide.png" id="S6.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="329" height="177" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>nuScenes dataset</figcaption>
</figure>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S6.F2.sf1.4.1" class="ltx_text ltx_font_bold">Example error diagnosis results.</span>
(a) Error analyses on KITTI-3D <em id="S6.F2.sf1.5.2" class="ltx_emph ltx_font_italic">validation</em> set under the moderate setting.
From left to right, we show the results of GUPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> for the Car category with 0.7 IoU threshold, 0.5 IoU threshold, and the results of a LiDAR-based method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite> for the Car category with 0.7 IoU threshold. (b) Error diagnosis results of FCOS3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite> on nuScenes <em id="S6.F2.sf1.6.3" class="ltx_emph ltx_font_italic">validation</em> set. From left to right, we show error diagnosis results based on KITTI-3D-style metrics with IoU@0.5 for all categories, the Car category, and the results based on nuScenes-style metrics for all categories.</figcaption>
<br class="ltx_break ltx_break">
<p id="S6.F2.sf1.7" class="ltx_p ltx_figure_panel ltx_align_center"><span class="ltx_rule" style="color:#000000;background:#000000;display:inline-block;"> </span></p>
</figure>
<div id="S6.SS3.p1" class="ltx_para ltx_noindent">
<p id="S6.SS3.p1.1" class="ltx_p"><span id="S6.SS3.p1.1.1" class="ltx_text ltx_font_bold">Error diagnosis.</span>
Here we present the error diagnosis results of some representative methods, settings, and evaluation metrics in Figure <a href="#S6.F2.sf1" title="Figure 2a ‣ 6.3 TIDE3D Analysis ‣ 6 Experiments and Analysis ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2a</span></a>. According to these error distributions, we can get the following observations:</p>
</div>
<div id="S6.SS3.p2" class="ltx_para ltx_noindent">
<p id="S6.SS3.p2.1" class="ltx_p"><span id="S6.SS3.p2.1.1" class="ltx_text ltx_font_bold">i.</span> Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>, <a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>, we find localization error is the bottleneck of current methods and it is mainly caused by the inaccurate location (center). Even for the LiDAR-based method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite> which estimates the results from the data with accurate spatial information, localization error still ranks first among all error types. Differently, for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite>, the dimension and the orientation also have a significant impact on the quality of localization.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para ltx_noindent">
<p id="S6.SS3.p3.1" class="ltx_p"><span id="S6.SS3.p3.1.1" class="ltx_text ltx_font_bold">ii.</span> We find the misalignment between the confidence and the bounding box’s quality is a serious problem in this field, suggesting that giving more accurate confidence is an effective way to improve detection accuracy. An empirical study of this problem is given in the following part.</p>
</div>
<div id="S6.SS3.p4" class="ltx_para ltx_noindent">
<p id="S6.SS3.p4.1" class="ltx_p"><span id="S6.SS3.p4.1.1" class="ltx_text ltx_font_bold">iii.</span> Under a lower IoU threshold, the KITTI-3D-style metric is still dominated by the localization error. Meanwhile, this issue has been alleviated on nuScenes-style metric. This indicates 3D IoU-based AP is a very sensitive metric, and center distance-based AP is more friendly to image-based methods. More TIDE3D results and analyses, <em id="S6.SS3.p4.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S6.SS3.p4.1.3" class="ltx_text"></span> indoor scenes, are provided in Appendix <a href="#S1.SS5" title="A.5 More TIDE3D Analysis ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.5</span></a>.</p>
</div>
<div id="S6.SS3.p5" class="ltx_para ltx_noindent">
<p id="S6.SS3.p5.6" class="ltx_p"><span id="S6.SS3.p5.6.1" class="ltx_text ltx_font_bold">Validating design choices.</span>
We show the proposed TIDE3D can validate whether a given design choice supports its motivation/claim. For instance, some works, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> proposed their designs to get better confidence scores of detection results.
Here we use TIDE3D to validate the design of GUPNet.
Particularly, GUPNet models the uncertainty of estimated depth (<math id="S6.SS3.p5.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S6.SS3.p5.1.m1.1a"><mi id="S6.SS3.p5.1.m1.1.1" xref="S6.SS3.p5.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p5.1.m1.1b"><ci id="S6.SS3.p5.1.m1.1.1.cmml" xref="S6.SS3.p5.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p5.1.m1.1c">\sigma</annotation></semantics></math>) and further gets the confidence of depth with this uncertainty by: <math id="S6.SS3.p5.2.m2.1" class="ltx_Math" alttext="p_{depth}=exp(-\sigma)" display="inline"><semantics id="S6.SS3.p5.2.m2.1a"><mrow id="S6.SS3.p5.2.m2.1.1" xref="S6.SS3.p5.2.m2.1.1.cmml"><msub id="S6.SS3.p5.2.m2.1.1.3" xref="S6.SS3.p5.2.m2.1.1.3.cmml"><mi id="S6.SS3.p5.2.m2.1.1.3.2" xref="S6.SS3.p5.2.m2.1.1.3.2.cmml">p</mi><mrow id="S6.SS3.p5.2.m2.1.1.3.3" xref="S6.SS3.p5.2.m2.1.1.3.3.cmml"><mi id="S6.SS3.p5.2.m2.1.1.3.3.2" xref="S6.SS3.p5.2.m2.1.1.3.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p5.2.m2.1.1.3.3.1" xref="S6.SS3.p5.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S6.SS3.p5.2.m2.1.1.3.3.3" xref="S6.SS3.p5.2.m2.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p5.2.m2.1.1.3.3.1a" xref="S6.SS3.p5.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S6.SS3.p5.2.m2.1.1.3.3.4" xref="S6.SS3.p5.2.m2.1.1.3.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p5.2.m2.1.1.3.3.1b" xref="S6.SS3.p5.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S6.SS3.p5.2.m2.1.1.3.3.5" xref="S6.SS3.p5.2.m2.1.1.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p5.2.m2.1.1.3.3.1c" xref="S6.SS3.p5.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S6.SS3.p5.2.m2.1.1.3.3.6" xref="S6.SS3.p5.2.m2.1.1.3.3.6.cmml">h</mi></mrow></msub><mo id="S6.SS3.p5.2.m2.1.1.2" xref="S6.SS3.p5.2.m2.1.1.2.cmml">=</mo><mrow id="S6.SS3.p5.2.m2.1.1.1" xref="S6.SS3.p5.2.m2.1.1.1.cmml"><mi id="S6.SS3.p5.2.m2.1.1.1.3" xref="S6.SS3.p5.2.m2.1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p5.2.m2.1.1.1.2" xref="S6.SS3.p5.2.m2.1.1.1.2.cmml">​</mo><mi id="S6.SS3.p5.2.m2.1.1.1.4" xref="S6.SS3.p5.2.m2.1.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p5.2.m2.1.1.1.2a" xref="S6.SS3.p5.2.m2.1.1.1.2.cmml">​</mo><mi id="S6.SS3.p5.2.m2.1.1.1.5" xref="S6.SS3.p5.2.m2.1.1.1.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p5.2.m2.1.1.1.2b" xref="S6.SS3.p5.2.m2.1.1.1.2.cmml">​</mo><mrow id="S6.SS3.p5.2.m2.1.1.1.1.1" xref="S6.SS3.p5.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S6.SS3.p5.2.m2.1.1.1.1.1.2" xref="S6.SS3.p5.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.SS3.p5.2.m2.1.1.1.1.1.1" xref="S6.SS3.p5.2.m2.1.1.1.1.1.1.cmml"><mo id="S6.SS3.p5.2.m2.1.1.1.1.1.1a" xref="S6.SS3.p5.2.m2.1.1.1.1.1.1.cmml">−</mo><mi id="S6.SS3.p5.2.m2.1.1.1.1.1.1.2" xref="S6.SS3.p5.2.m2.1.1.1.1.1.1.2.cmml">σ</mi></mrow><mo stretchy="false" id="S6.SS3.p5.2.m2.1.1.1.1.1.3" xref="S6.SS3.p5.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p5.2.m2.1b"><apply id="S6.SS3.p5.2.m2.1.1.cmml" xref="S6.SS3.p5.2.m2.1.1"><eq id="S6.SS3.p5.2.m2.1.1.2.cmml" xref="S6.SS3.p5.2.m2.1.1.2"></eq><apply id="S6.SS3.p5.2.m2.1.1.3.cmml" xref="S6.SS3.p5.2.m2.1.1.3"><csymbol cd="ambiguous" id="S6.SS3.p5.2.m2.1.1.3.1.cmml" xref="S6.SS3.p5.2.m2.1.1.3">subscript</csymbol><ci id="S6.SS3.p5.2.m2.1.1.3.2.cmml" xref="S6.SS3.p5.2.m2.1.1.3.2">𝑝</ci><apply id="S6.SS3.p5.2.m2.1.1.3.3.cmml" xref="S6.SS3.p5.2.m2.1.1.3.3"><times id="S6.SS3.p5.2.m2.1.1.3.3.1.cmml" xref="S6.SS3.p5.2.m2.1.1.3.3.1"></times><ci id="S6.SS3.p5.2.m2.1.1.3.3.2.cmml" xref="S6.SS3.p5.2.m2.1.1.3.3.2">𝑑</ci><ci id="S6.SS3.p5.2.m2.1.1.3.3.3.cmml" xref="S6.SS3.p5.2.m2.1.1.3.3.3">𝑒</ci><ci id="S6.SS3.p5.2.m2.1.1.3.3.4.cmml" xref="S6.SS3.p5.2.m2.1.1.3.3.4">𝑝</ci><ci id="S6.SS3.p5.2.m2.1.1.3.3.5.cmml" xref="S6.SS3.p5.2.m2.1.1.3.3.5">𝑡</ci><ci id="S6.SS3.p5.2.m2.1.1.3.3.6.cmml" xref="S6.SS3.p5.2.m2.1.1.3.3.6">ℎ</ci></apply></apply><apply id="S6.SS3.p5.2.m2.1.1.1.cmml" xref="S6.SS3.p5.2.m2.1.1.1"><times id="S6.SS3.p5.2.m2.1.1.1.2.cmml" xref="S6.SS3.p5.2.m2.1.1.1.2"></times><ci id="S6.SS3.p5.2.m2.1.1.1.3.cmml" xref="S6.SS3.p5.2.m2.1.1.1.3">𝑒</ci><ci id="S6.SS3.p5.2.m2.1.1.1.4.cmml" xref="S6.SS3.p5.2.m2.1.1.1.4">𝑥</ci><ci id="S6.SS3.p5.2.m2.1.1.1.5.cmml" xref="S6.SS3.p5.2.m2.1.1.1.5">𝑝</ci><apply id="S6.SS3.p5.2.m2.1.1.1.1.1.1.cmml" xref="S6.SS3.p5.2.m2.1.1.1.1.1"><minus id="S6.SS3.p5.2.m2.1.1.1.1.1.1.1.cmml" xref="S6.SS3.p5.2.m2.1.1.1.1.1"></minus><ci id="S6.SS3.p5.2.m2.1.1.1.1.1.1.2.cmml" xref="S6.SS3.p5.2.m2.1.1.1.1.1.1.2">𝜎</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p5.2.m2.1c">p_{depth}=exp(-\sigma)</annotation></semantics></math>. Finally, the 3D confidence (<math id="S6.SS3.p5.3.m3.1" class="ltx_Math" alttext="p_{3D}" display="inline"><semantics id="S6.SS3.p5.3.m3.1a"><msub id="S6.SS3.p5.3.m3.1.1" xref="S6.SS3.p5.3.m3.1.1.cmml"><mi id="S6.SS3.p5.3.m3.1.1.2" xref="S6.SS3.p5.3.m3.1.1.2.cmml">p</mi><mrow id="S6.SS3.p5.3.m3.1.1.3" xref="S6.SS3.p5.3.m3.1.1.3.cmml"><mn id="S6.SS3.p5.3.m3.1.1.3.2" xref="S6.SS3.p5.3.m3.1.1.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S6.SS3.p5.3.m3.1.1.3.1" xref="S6.SS3.p5.3.m3.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p5.3.m3.1.1.3.3" xref="S6.SS3.p5.3.m3.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p5.3.m3.1b"><apply id="S6.SS3.p5.3.m3.1.1.cmml" xref="S6.SS3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS3.p5.3.m3.1.1.1.cmml" xref="S6.SS3.p5.3.m3.1.1">subscript</csymbol><ci id="S6.SS3.p5.3.m3.1.1.2.cmml" xref="S6.SS3.p5.3.m3.1.1.2">𝑝</ci><apply id="S6.SS3.p5.3.m3.1.1.3.cmml" xref="S6.SS3.p5.3.m3.1.1.3"><times id="S6.SS3.p5.3.m3.1.1.3.1.cmml" xref="S6.SS3.p5.3.m3.1.1.3.1"></times><cn type="integer" id="S6.SS3.p5.3.m3.1.1.3.2.cmml" xref="S6.SS3.p5.3.m3.1.1.3.2">3</cn><ci id="S6.SS3.p5.3.m3.1.1.3.3.cmml" xref="S6.SS3.p5.3.m3.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p5.3.m3.1c">p_{3D}</annotation></semantics></math>) can be obtained by correcting 2D confidence (<math id="S6.SS3.p5.4.m4.1" class="ltx_Math" alttext="p_{2D}" display="inline"><semantics id="S6.SS3.p5.4.m4.1a"><msub id="S6.SS3.p5.4.m4.1.1" xref="S6.SS3.p5.4.m4.1.1.cmml"><mi id="S6.SS3.p5.4.m4.1.1.2" xref="S6.SS3.p5.4.m4.1.1.2.cmml">p</mi><mrow id="S6.SS3.p5.4.m4.1.1.3" xref="S6.SS3.p5.4.m4.1.1.3.cmml"><mn id="S6.SS3.p5.4.m4.1.1.3.2" xref="S6.SS3.p5.4.m4.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S6.SS3.p5.4.m4.1.1.3.1" xref="S6.SS3.p5.4.m4.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p5.4.m4.1.1.3.3" xref="S6.SS3.p5.4.m4.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p5.4.m4.1b"><apply id="S6.SS3.p5.4.m4.1.1.cmml" xref="S6.SS3.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S6.SS3.p5.4.m4.1.1.1.cmml" xref="S6.SS3.p5.4.m4.1.1">subscript</csymbol><ci id="S6.SS3.p5.4.m4.1.1.2.cmml" xref="S6.SS3.p5.4.m4.1.1.2">𝑝</ci><apply id="S6.SS3.p5.4.m4.1.1.3.cmml" xref="S6.SS3.p5.4.m4.1.1.3"><times id="S6.SS3.p5.4.m4.1.1.3.1.cmml" xref="S6.SS3.p5.4.m4.1.1.3.1"></times><cn type="integer" id="S6.SS3.p5.4.m4.1.1.3.2.cmml" xref="S6.SS3.p5.4.m4.1.1.3.2">2</cn><ci id="S6.SS3.p5.4.m4.1.1.3.3.cmml" xref="S6.SS3.p5.4.m4.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p5.4.m4.1c">p_{2D}</annotation></semantics></math>) with depth confidence: <math id="S6.SS3.p5.5.m5.1" class="ltx_Math" alttext="p_{3D}=p_{depth}\cdot p_{2D}" display="inline"><semantics id="S6.SS3.p5.5.m5.1a"><mrow id="S6.SS3.p5.5.m5.1.1" xref="S6.SS3.p5.5.m5.1.1.cmml"><msub id="S6.SS3.p5.5.m5.1.1.2" xref="S6.SS3.p5.5.m5.1.1.2.cmml"><mi id="S6.SS3.p5.5.m5.1.1.2.2" xref="S6.SS3.p5.5.m5.1.1.2.2.cmml">p</mi><mrow id="S6.SS3.p5.5.m5.1.1.2.3" xref="S6.SS3.p5.5.m5.1.1.2.3.cmml"><mn id="S6.SS3.p5.5.m5.1.1.2.3.2" xref="S6.SS3.p5.5.m5.1.1.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S6.SS3.p5.5.m5.1.1.2.3.1" xref="S6.SS3.p5.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S6.SS3.p5.5.m5.1.1.2.3.3" xref="S6.SS3.p5.5.m5.1.1.2.3.3.cmml">D</mi></mrow></msub><mo id="S6.SS3.p5.5.m5.1.1.1" xref="S6.SS3.p5.5.m5.1.1.1.cmml">=</mo><mrow id="S6.SS3.p5.5.m5.1.1.3" xref="S6.SS3.p5.5.m5.1.1.3.cmml"><msub id="S6.SS3.p5.5.m5.1.1.3.2" xref="S6.SS3.p5.5.m5.1.1.3.2.cmml"><mi id="S6.SS3.p5.5.m5.1.1.3.2.2" xref="S6.SS3.p5.5.m5.1.1.3.2.2.cmml">p</mi><mrow id="S6.SS3.p5.5.m5.1.1.3.2.3" xref="S6.SS3.p5.5.m5.1.1.3.2.3.cmml"><mi id="S6.SS3.p5.5.m5.1.1.3.2.3.2" xref="S6.SS3.p5.5.m5.1.1.3.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p5.5.m5.1.1.3.2.3.1" xref="S6.SS3.p5.5.m5.1.1.3.2.3.1.cmml">​</mo><mi id="S6.SS3.p5.5.m5.1.1.3.2.3.3" xref="S6.SS3.p5.5.m5.1.1.3.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p5.5.m5.1.1.3.2.3.1a" xref="S6.SS3.p5.5.m5.1.1.3.2.3.1.cmml">​</mo><mi id="S6.SS3.p5.5.m5.1.1.3.2.3.4" xref="S6.SS3.p5.5.m5.1.1.3.2.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p5.5.m5.1.1.3.2.3.1b" xref="S6.SS3.p5.5.m5.1.1.3.2.3.1.cmml">​</mo><mi id="S6.SS3.p5.5.m5.1.1.3.2.3.5" xref="S6.SS3.p5.5.m5.1.1.3.2.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p5.5.m5.1.1.3.2.3.1c" xref="S6.SS3.p5.5.m5.1.1.3.2.3.1.cmml">​</mo><mi id="S6.SS3.p5.5.m5.1.1.3.2.3.6" xref="S6.SS3.p5.5.m5.1.1.3.2.3.6.cmml">h</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S6.SS3.p5.5.m5.1.1.3.1" xref="S6.SS3.p5.5.m5.1.1.3.1.cmml">⋅</mo><msub id="S6.SS3.p5.5.m5.1.1.3.3" xref="S6.SS3.p5.5.m5.1.1.3.3.cmml"><mi id="S6.SS3.p5.5.m5.1.1.3.3.2" xref="S6.SS3.p5.5.m5.1.1.3.3.2.cmml">p</mi><mrow id="S6.SS3.p5.5.m5.1.1.3.3.3" xref="S6.SS3.p5.5.m5.1.1.3.3.3.cmml"><mn id="S6.SS3.p5.5.m5.1.1.3.3.3.2" xref="S6.SS3.p5.5.m5.1.1.3.3.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S6.SS3.p5.5.m5.1.1.3.3.3.1" xref="S6.SS3.p5.5.m5.1.1.3.3.3.1.cmml">​</mo><mi id="S6.SS3.p5.5.m5.1.1.3.3.3.3" xref="S6.SS3.p5.5.m5.1.1.3.3.3.3.cmml">D</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p5.5.m5.1b"><apply id="S6.SS3.p5.5.m5.1.1.cmml" xref="S6.SS3.p5.5.m5.1.1"><eq id="S6.SS3.p5.5.m5.1.1.1.cmml" xref="S6.SS3.p5.5.m5.1.1.1"></eq><apply id="S6.SS3.p5.5.m5.1.1.2.cmml" xref="S6.SS3.p5.5.m5.1.1.2"><csymbol cd="ambiguous" id="S6.SS3.p5.5.m5.1.1.2.1.cmml" xref="S6.SS3.p5.5.m5.1.1.2">subscript</csymbol><ci id="S6.SS3.p5.5.m5.1.1.2.2.cmml" xref="S6.SS3.p5.5.m5.1.1.2.2">𝑝</ci><apply id="S6.SS3.p5.5.m5.1.1.2.3.cmml" xref="S6.SS3.p5.5.m5.1.1.2.3"><times id="S6.SS3.p5.5.m5.1.1.2.3.1.cmml" xref="S6.SS3.p5.5.m5.1.1.2.3.1"></times><cn type="integer" id="S6.SS3.p5.5.m5.1.1.2.3.2.cmml" xref="S6.SS3.p5.5.m5.1.1.2.3.2">3</cn><ci id="S6.SS3.p5.5.m5.1.1.2.3.3.cmml" xref="S6.SS3.p5.5.m5.1.1.2.3.3">𝐷</ci></apply></apply><apply id="S6.SS3.p5.5.m5.1.1.3.cmml" xref="S6.SS3.p5.5.m5.1.1.3"><ci id="S6.SS3.p5.5.m5.1.1.3.1.cmml" xref="S6.SS3.p5.5.m5.1.1.3.1">⋅</ci><apply id="S6.SS3.p5.5.m5.1.1.3.2.cmml" xref="S6.SS3.p5.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="S6.SS3.p5.5.m5.1.1.3.2.1.cmml" xref="S6.SS3.p5.5.m5.1.1.3.2">subscript</csymbol><ci id="S6.SS3.p5.5.m5.1.1.3.2.2.cmml" xref="S6.SS3.p5.5.m5.1.1.3.2.2">𝑝</ci><apply id="S6.SS3.p5.5.m5.1.1.3.2.3.cmml" xref="S6.SS3.p5.5.m5.1.1.3.2.3"><times id="S6.SS3.p5.5.m5.1.1.3.2.3.1.cmml" xref="S6.SS3.p5.5.m5.1.1.3.2.3.1"></times><ci id="S6.SS3.p5.5.m5.1.1.3.2.3.2.cmml" xref="S6.SS3.p5.5.m5.1.1.3.2.3.2">𝑑</ci><ci id="S6.SS3.p5.5.m5.1.1.3.2.3.3.cmml" xref="S6.SS3.p5.5.m5.1.1.3.2.3.3">𝑒</ci><ci id="S6.SS3.p5.5.m5.1.1.3.2.3.4.cmml" xref="S6.SS3.p5.5.m5.1.1.3.2.3.4">𝑝</ci><ci id="S6.SS3.p5.5.m5.1.1.3.2.3.5.cmml" xref="S6.SS3.p5.5.m5.1.1.3.2.3.5">𝑡</ci><ci id="S6.SS3.p5.5.m5.1.1.3.2.3.6.cmml" xref="S6.SS3.p5.5.m5.1.1.3.2.3.6">ℎ</ci></apply></apply><apply id="S6.SS3.p5.5.m5.1.1.3.3.cmml" xref="S6.SS3.p5.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S6.SS3.p5.5.m5.1.1.3.3.1.cmml" xref="S6.SS3.p5.5.m5.1.1.3.3">subscript</csymbol><ci id="S6.SS3.p5.5.m5.1.1.3.3.2.cmml" xref="S6.SS3.p5.5.m5.1.1.3.3.2">𝑝</ci><apply id="S6.SS3.p5.5.m5.1.1.3.3.3.cmml" xref="S6.SS3.p5.5.m5.1.1.3.3.3"><times id="S6.SS3.p5.5.m5.1.1.3.3.3.1.cmml" xref="S6.SS3.p5.5.m5.1.1.3.3.3.1"></times><cn type="integer" id="S6.SS3.p5.5.m5.1.1.3.3.3.2.cmml" xref="S6.SS3.p5.5.m5.1.1.3.3.3.2">2</cn><ci id="S6.SS3.p5.5.m5.1.1.3.3.3.3.cmml" xref="S6.SS3.p5.5.m5.1.1.3.3.3.3">𝐷</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p5.5.m5.1c">p_{3D}=p_{depth}\cdot p_{2D}</annotation></semantics></math>. Table <a href="#S6.T7" title="Table 7 ‣ 6.3 TIDE3D Analysis ‣ 6 Experiments and Analysis ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> gives the results of GUPNet equipped with 2D confidence and 3D confidence respectively. We observe that the <math id="S6.SS3.p5.6.m6.1" class="ltx_Math" alttext="\rm{AP}_{40}" display="inline"><semantics id="S6.SS3.p5.6.m6.1a"><msub id="S6.SS3.p5.6.m6.1.1" xref="S6.SS3.p5.6.m6.1.1.cmml"><mi id="S6.SS3.p5.6.m6.1.1.2" xref="S6.SS3.p5.6.m6.1.1.2.cmml">AP</mi><mn id="S6.SS3.p5.6.m6.1.1.3" xref="S6.SS3.p5.6.m6.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p5.6.m6.1b"><apply id="S6.SS3.p5.6.m6.1.1.cmml" xref="S6.SS3.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S6.SS3.p5.6.m6.1.1.1.cmml" xref="S6.SS3.p5.6.m6.1.1">subscript</csymbol><ci id="S6.SS3.p5.6.m6.1.1.2.cmml" xref="S6.SS3.p5.6.m6.1.1.2">AP</ci><cn type="integer" id="S6.SS3.p5.6.m6.1.1.3.cmml" xref="S6.SS3.p5.6.m6.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p5.6.m6.1c">\rm{AP}_{40}</annotation></semantics></math> improved by 2.21 and the ranking error reduces to 10.81 from 15.10, which confirms the effectiveness of this design. Meanwhile, we find the localization error also reduced, which is caused by the high-quality results being improved and corresponding results having a higher priority in AP computing. See Appendix <a href="#S1.SS7" title="A.7 Ranking Error in TIDE3D ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.7</span></a> for more analysis about the ranking error and localization error in TIDE3D.</p>
</div>
<figure id="S6.T7" class="ltx_table">
<div id="S6.T7.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:108.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(73.4pt,-18.4pt) scale(1.51150342671277,1.51150342671277) ;">
<table id="S6.T7.5.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T7.5.5.5" class="ltx_tr">
<th id="S6.T7.5.5.5.6" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S6.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><math id="S6.T7.1.1.1.1.m1.1" class="ltx_Math" alttext="{\rm AP}_{40}\uparrow" display="inline"><semantics id="S6.T7.1.1.1.1.m1.1a"><mrow id="S6.T7.1.1.1.1.m1.1.1" xref="S6.T7.1.1.1.1.m1.1.1.cmml"><msub id="S6.T7.1.1.1.1.m1.1.1.2" xref="S6.T7.1.1.1.1.m1.1.1.2.cmml"><mi id="S6.T7.1.1.1.1.m1.1.1.2.2" xref="S6.T7.1.1.1.1.m1.1.1.2.2.cmml">AP</mi><mn id="S6.T7.1.1.1.1.m1.1.1.2.3" xref="S6.T7.1.1.1.1.m1.1.1.2.3.cmml">40</mn></msub><mo stretchy="false" id="S6.T7.1.1.1.1.m1.1.1.1" xref="S6.T7.1.1.1.1.m1.1.1.1.cmml">↑</mo><mi id="S6.T7.1.1.1.1.m1.1.1.3" xref="S6.T7.1.1.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T7.1.1.1.1.m1.1b"><apply id="S6.T7.1.1.1.1.m1.1.1.cmml" xref="S6.T7.1.1.1.1.m1.1.1"><ci id="S6.T7.1.1.1.1.m1.1.1.1.cmml" xref="S6.T7.1.1.1.1.m1.1.1.1">↑</ci><apply id="S6.T7.1.1.1.1.m1.1.1.2.cmml" xref="S6.T7.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S6.T7.1.1.1.1.m1.1.1.2.1.cmml" xref="S6.T7.1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S6.T7.1.1.1.1.m1.1.1.2.2.cmml" xref="S6.T7.1.1.1.1.m1.1.1.2.2">AP</ci><cn type="integer" id="S6.T7.1.1.1.1.m1.1.1.2.3.cmml" xref="S6.T7.1.1.1.1.m1.1.1.2.3">40</cn></apply><csymbol cd="latexml" id="S6.T7.1.1.1.1.m1.1.1.3.cmml" xref="S6.T7.1.1.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.1.1.1.1.m1.1c">{\rm AP}_{40}\uparrow</annotation></semantics></math></th>
<th id="S6.T7.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><math id="S6.T7.2.2.2.2.m1.1" class="ltx_Math" alttext="\text{E}_{\texttt{loc}}\downarrow" display="inline"><semantics id="S6.T7.2.2.2.2.m1.1a"><mrow id="S6.T7.2.2.2.2.m1.1.1" xref="S6.T7.2.2.2.2.m1.1.1.cmml"><msub id="S6.T7.2.2.2.2.m1.1.1.2" xref="S6.T7.2.2.2.2.m1.1.1.2.cmml"><mtext id="S6.T7.2.2.2.2.m1.1.1.2.2" xref="S6.T7.2.2.2.2.m1.1.1.2.2a.cmml">E</mtext><mtext class="ltx_mathvariant_monospace" id="S6.T7.2.2.2.2.m1.1.1.2.3" xref="S6.T7.2.2.2.2.m1.1.1.2.3a.cmml">loc</mtext></msub><mo stretchy="false" id="S6.T7.2.2.2.2.m1.1.1.1" xref="S6.T7.2.2.2.2.m1.1.1.1.cmml">↓</mo><mi id="S6.T7.2.2.2.2.m1.1.1.3" xref="S6.T7.2.2.2.2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T7.2.2.2.2.m1.1b"><apply id="S6.T7.2.2.2.2.m1.1.1.cmml" xref="S6.T7.2.2.2.2.m1.1.1"><ci id="S6.T7.2.2.2.2.m1.1.1.1.cmml" xref="S6.T7.2.2.2.2.m1.1.1.1">↓</ci><apply id="S6.T7.2.2.2.2.m1.1.1.2.cmml" xref="S6.T7.2.2.2.2.m1.1.1.2"><csymbol cd="ambiguous" id="S6.T7.2.2.2.2.m1.1.1.2.1.cmml" xref="S6.T7.2.2.2.2.m1.1.1.2">subscript</csymbol><ci id="S6.T7.2.2.2.2.m1.1.1.2.2a.cmml" xref="S6.T7.2.2.2.2.m1.1.1.2.2"><mtext id="S6.T7.2.2.2.2.m1.1.1.2.2.cmml" xref="S6.T7.2.2.2.2.m1.1.1.2.2">E</mtext></ci><ci id="S6.T7.2.2.2.2.m1.1.1.2.3a.cmml" xref="S6.T7.2.2.2.2.m1.1.1.2.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S6.T7.2.2.2.2.m1.1.1.2.3.cmml" xref="S6.T7.2.2.2.2.m1.1.1.2.3">loc</mtext></ci></apply><csymbol cd="latexml" id="S6.T7.2.2.2.2.m1.1.1.3.cmml" xref="S6.T7.2.2.2.2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.2.2.2.2.m1.1c">\text{E}_{\texttt{loc}}\downarrow</annotation></semantics></math></th>
<th id="S6.T7.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><math id="S6.T7.3.3.3.3.m1.1" class="ltx_Math" alttext="\text{E}_{\texttt{bkg}}\downarrow" display="inline"><semantics id="S6.T7.3.3.3.3.m1.1a"><mrow id="S6.T7.3.3.3.3.m1.1.1" xref="S6.T7.3.3.3.3.m1.1.1.cmml"><msub id="S6.T7.3.3.3.3.m1.1.1.2" xref="S6.T7.3.3.3.3.m1.1.1.2.cmml"><mtext id="S6.T7.3.3.3.3.m1.1.1.2.2" xref="S6.T7.3.3.3.3.m1.1.1.2.2a.cmml">E</mtext><mtext class="ltx_mathvariant_monospace" id="S6.T7.3.3.3.3.m1.1.1.2.3" xref="S6.T7.3.3.3.3.m1.1.1.2.3a.cmml">bkg</mtext></msub><mo stretchy="false" id="S6.T7.3.3.3.3.m1.1.1.1" xref="S6.T7.3.3.3.3.m1.1.1.1.cmml">↓</mo><mi id="S6.T7.3.3.3.3.m1.1.1.3" xref="S6.T7.3.3.3.3.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T7.3.3.3.3.m1.1b"><apply id="S6.T7.3.3.3.3.m1.1.1.cmml" xref="S6.T7.3.3.3.3.m1.1.1"><ci id="S6.T7.3.3.3.3.m1.1.1.1.cmml" xref="S6.T7.3.3.3.3.m1.1.1.1">↓</ci><apply id="S6.T7.3.3.3.3.m1.1.1.2.cmml" xref="S6.T7.3.3.3.3.m1.1.1.2"><csymbol cd="ambiguous" id="S6.T7.3.3.3.3.m1.1.1.2.1.cmml" xref="S6.T7.3.3.3.3.m1.1.1.2">subscript</csymbol><ci id="S6.T7.3.3.3.3.m1.1.1.2.2a.cmml" xref="S6.T7.3.3.3.3.m1.1.1.2.2"><mtext id="S6.T7.3.3.3.3.m1.1.1.2.2.cmml" xref="S6.T7.3.3.3.3.m1.1.1.2.2">E</mtext></ci><ci id="S6.T7.3.3.3.3.m1.1.1.2.3a.cmml" xref="S6.T7.3.3.3.3.m1.1.1.2.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S6.T7.3.3.3.3.m1.1.1.2.3.cmml" xref="S6.T7.3.3.3.3.m1.1.1.2.3">bkg</mtext></ci></apply><csymbol cd="latexml" id="S6.T7.3.3.3.3.m1.1.1.3.cmml" xref="S6.T7.3.3.3.3.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.3.3.3.3.m1.1c">\text{E}_{\texttt{bkg}}\downarrow</annotation></semantics></math></th>
<th id="S6.T7.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><math id="S6.T7.4.4.4.4.m1.1" class="ltx_Math" alttext="\text{E}_{\texttt{miss}}\downarrow" display="inline"><semantics id="S6.T7.4.4.4.4.m1.1a"><mrow id="S6.T7.4.4.4.4.m1.1.1" xref="S6.T7.4.4.4.4.m1.1.1.cmml"><msub id="S6.T7.4.4.4.4.m1.1.1.2" xref="S6.T7.4.4.4.4.m1.1.1.2.cmml"><mtext id="S6.T7.4.4.4.4.m1.1.1.2.2" xref="S6.T7.4.4.4.4.m1.1.1.2.2a.cmml">E</mtext><mtext class="ltx_mathvariant_monospace" id="S6.T7.4.4.4.4.m1.1.1.2.3" xref="S6.T7.4.4.4.4.m1.1.1.2.3a.cmml">miss</mtext></msub><mo stretchy="false" id="S6.T7.4.4.4.4.m1.1.1.1" xref="S6.T7.4.4.4.4.m1.1.1.1.cmml">↓</mo><mi id="S6.T7.4.4.4.4.m1.1.1.3" xref="S6.T7.4.4.4.4.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T7.4.4.4.4.m1.1b"><apply id="S6.T7.4.4.4.4.m1.1.1.cmml" xref="S6.T7.4.4.4.4.m1.1.1"><ci id="S6.T7.4.4.4.4.m1.1.1.1.cmml" xref="S6.T7.4.4.4.4.m1.1.1.1">↓</ci><apply id="S6.T7.4.4.4.4.m1.1.1.2.cmml" xref="S6.T7.4.4.4.4.m1.1.1.2"><csymbol cd="ambiguous" id="S6.T7.4.4.4.4.m1.1.1.2.1.cmml" xref="S6.T7.4.4.4.4.m1.1.1.2">subscript</csymbol><ci id="S6.T7.4.4.4.4.m1.1.1.2.2a.cmml" xref="S6.T7.4.4.4.4.m1.1.1.2.2"><mtext id="S6.T7.4.4.4.4.m1.1.1.2.2.cmml" xref="S6.T7.4.4.4.4.m1.1.1.2.2">E</mtext></ci><ci id="S6.T7.4.4.4.4.m1.1.1.2.3a.cmml" xref="S6.T7.4.4.4.4.m1.1.1.2.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S6.T7.4.4.4.4.m1.1.1.2.3.cmml" xref="S6.T7.4.4.4.4.m1.1.1.2.3">miss</mtext></ci></apply><csymbol cd="latexml" id="S6.T7.4.4.4.4.m1.1.1.3.cmml" xref="S6.T7.4.4.4.4.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.4.4.4.4.m1.1c">\text{E}_{\texttt{miss}}\downarrow</annotation></semantics></math></th>
<th id="S6.T7.5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><math id="S6.T7.5.5.5.5.m1.1" class="ltx_Math" alttext="\text{E}_{\texttt{rank}}\downarrow" display="inline"><semantics id="S6.T7.5.5.5.5.m1.1a"><mrow id="S6.T7.5.5.5.5.m1.1.1" xref="S6.T7.5.5.5.5.m1.1.1.cmml"><msub id="S6.T7.5.5.5.5.m1.1.1.2" xref="S6.T7.5.5.5.5.m1.1.1.2.cmml"><mtext id="S6.T7.5.5.5.5.m1.1.1.2.2" xref="S6.T7.5.5.5.5.m1.1.1.2.2a.cmml">E</mtext><mtext class="ltx_mathvariant_monospace" id="S6.T7.5.5.5.5.m1.1.1.2.3" xref="S6.T7.5.5.5.5.m1.1.1.2.3a.cmml">rank</mtext></msub><mo stretchy="false" id="S6.T7.5.5.5.5.m1.1.1.1" xref="S6.T7.5.5.5.5.m1.1.1.1.cmml">↓</mo><mi id="S6.T7.5.5.5.5.m1.1.1.3" xref="S6.T7.5.5.5.5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T7.5.5.5.5.m1.1b"><apply id="S6.T7.5.5.5.5.m1.1.1.cmml" xref="S6.T7.5.5.5.5.m1.1.1"><ci id="S6.T7.5.5.5.5.m1.1.1.1.cmml" xref="S6.T7.5.5.5.5.m1.1.1.1">↓</ci><apply id="S6.T7.5.5.5.5.m1.1.1.2.cmml" xref="S6.T7.5.5.5.5.m1.1.1.2"><csymbol cd="ambiguous" id="S6.T7.5.5.5.5.m1.1.1.2.1.cmml" xref="S6.T7.5.5.5.5.m1.1.1.2">subscript</csymbol><ci id="S6.T7.5.5.5.5.m1.1.1.2.2a.cmml" xref="S6.T7.5.5.5.5.m1.1.1.2.2"><mtext id="S6.T7.5.5.5.5.m1.1.1.2.2.cmml" xref="S6.T7.5.5.5.5.m1.1.1.2.2">E</mtext></ci><ci id="S6.T7.5.5.5.5.m1.1.1.2.3a.cmml" xref="S6.T7.5.5.5.5.m1.1.1.2.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S6.T7.5.5.5.5.m1.1.1.2.3.cmml" xref="S6.T7.5.5.5.5.m1.1.1.2.3">rank</mtext></ci></apply><csymbol cd="latexml" id="S6.T7.5.5.5.5.m1.1.1.3.cmml" xref="S6.T7.5.5.5.5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.5.5.5.5.m1.1c">\text{E}_{\texttt{rank}}\downarrow</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T7.5.5.6.1" class="ltx_tr">
<th id="S6.T7.5.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">GUPNet w/ 2D conf.</th>
<td id="S6.T7.5.5.6.1.2" class="ltx_td ltx_align_center ltx_border_t">14.81</td>
<td id="S6.T7.5.5.6.1.3" class="ltx_td ltx_align_center ltx_border_t">69.87</td>
<td id="S6.T7.5.5.6.1.4" class="ltx_td ltx_align_center ltx_border_t">0.46</td>
<td id="S6.T7.5.5.6.1.5" class="ltx_td ltx_align_center ltx_border_t">1.73</td>
<td id="S6.T7.5.5.6.1.6" class="ltx_td ltx_align_center ltx_border_t">15.10</td>
</tr>
<tr id="S6.T7.5.5.7.2" class="ltx_tr">
<th id="S6.T7.5.5.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GUPNet w/ 3D conf.</th>
<td id="S6.T7.5.5.7.2.2" class="ltx_td ltx_align_center">17.02</td>
<td id="S6.T7.5.5.7.2.3" class="ltx_td ltx_align_center">62.84</td>
<td id="S6.T7.5.5.7.2.4" class="ltx_td ltx_align_center">0.15</td>
<td id="S6.T7.5.5.7.2.5" class="ltx_td ltx_align_center">4.41</td>
<td id="S6.T7.5.5.7.2.6" class="ltx_td ltx_align_center">10.81</td>
</tr>
<tr id="S6.T7.5.5.8.3" class="ltx_tr">
<th id="S6.T7.5.5.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Improvement</th>
<td id="S6.T7.5.5.8.3.2" class="ltx_td ltx_align_center">+2.21</td>
<td id="S6.T7.5.5.8.3.3" class="ltx_td ltx_align_center">-7.03</td>
<td id="S6.T7.5.5.8.3.4" class="ltx_td ltx_align_center">-0.31</td>
<td id="S6.T7.5.5.8.3.5" class="ltx_td ltx_align_center">+2.68</td>
<td id="S6.T7.5.5.8.3.6" class="ltx_td ltx_align_center">-4.29</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span><span id="S6.T7.10.1" class="ltx_text ltx_font_bold">Effects of 3D confidence.</span> Experiments are conducted on KITTI-3D <em id="S6.T7.11.2" class="ltx_emph ltx_font_italic">validation</em> set. <math id="S6.T7.7.m1.1" class="ltx_Math" alttext="\Delta{\rm AP}_{40}" display="inline"><semantics id="S6.T7.7.m1.1b"><mrow id="S6.T7.7.m1.1.1" xref="S6.T7.7.m1.1.1.cmml"><mi mathvariant="normal" id="S6.T7.7.m1.1.1.2" xref="S6.T7.7.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S6.T7.7.m1.1.1.1" xref="S6.T7.7.m1.1.1.1.cmml">​</mo><msub id="S6.T7.7.m1.1.1.3" xref="S6.T7.7.m1.1.1.3.cmml"><mi id="S6.T7.7.m1.1.1.3.2" xref="S6.T7.7.m1.1.1.3.2.cmml">AP</mi><mn id="S6.T7.7.m1.1.1.3.3" xref="S6.T7.7.m1.1.1.3.3.cmml">40</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.T7.7.m1.1c"><apply id="S6.T7.7.m1.1.1.cmml" xref="S6.T7.7.m1.1.1"><times id="S6.T7.7.m1.1.1.1.cmml" xref="S6.T7.7.m1.1.1.1"></times><ci id="S6.T7.7.m1.1.1.2.cmml" xref="S6.T7.7.m1.1.1.2">Δ</ci><apply id="S6.T7.7.m1.1.1.3.cmml" xref="S6.T7.7.m1.1.1.3"><csymbol cd="ambiguous" id="S6.T7.7.m1.1.1.3.1.cmml" xref="S6.T7.7.m1.1.1.3">subscript</csymbol><ci id="S6.T7.7.m1.1.1.3.2.cmml" xref="S6.T7.7.m1.1.1.3.2">AP</ci><cn type="integer" id="S6.T7.7.m1.1.1.3.3.cmml" xref="S6.T7.7.m1.1.1.3.3">40</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.7.m1.1d">\Delta{\rm AP}_{40}</annotation></semantics></math> is denoted as E for brevity.</figcaption>
</figure>
<figure id="S6.T8" class="ltx_table">
<div id="S6.T8.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:307.9pt;height:106.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.7pt,9.6pt) scale(0.847550353409027,0.847550353409027) ;">
<table id="S6.T8.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T8.2.2.3.1" class="ltx_tr">
<th id="S6.T8.2.2.3.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;" rowspan="2"></th>
<th id="S6.T8.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;" rowspan="2"><span id="S6.T8.2.2.3.1.2.1" class="ltx_text">Dataset</span></th>
<th id="S6.T8.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;" rowspan="2"><span id="S6.T8.2.2.3.1.3.1" class="ltx_text">Class</span></th>
<th id="S6.T8.2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="3">KITTI Metric</th>
<th id="S6.T8.2.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="2">nuScenes Metric</th>
</tr>
<tr id="S6.T8.2.2.2" class="ltx_tr">
<th id="S6.T8.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">@0.7</th>
<th id="S6.T8.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">@0.5</th>
<th id="S6.T8.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">@0.25</th>
<th id="S6.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><math id="S6.T8.1.1.1.1.m1.1" class="ltx_Math" alttext="{\rm mAP}" display="inline"><semantics id="S6.T8.1.1.1.1.m1.1a"><mi id="S6.T8.1.1.1.1.m1.1.1" xref="S6.T8.1.1.1.1.m1.1.1.cmml">mAP</mi><annotation-xml encoding="MathML-Content" id="S6.T8.1.1.1.1.m1.1b"><ci id="S6.T8.1.1.1.1.m1.1.1.cmml" xref="S6.T8.1.1.1.1.m1.1.1">mAP</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T8.1.1.1.1.m1.1c">{\rm mAP}</annotation></semantics></math></th>
<th id="S6.T8.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><math id="S6.T8.2.2.2.2.m1.1" class="ltx_Math" alttext="{\rm NDS}" display="inline"><semantics id="S6.T8.2.2.2.2.m1.1a"><mi id="S6.T8.2.2.2.2.m1.1.1" xref="S6.T8.2.2.2.2.m1.1.1.cmml">NDS</mi><annotation-xml encoding="MathML-Content" id="S6.T8.2.2.2.2.m1.1b"><ci id="S6.T8.2.2.2.2.m1.1.1.cmml" xref="S6.T8.2.2.2.2.m1.1.1">NDS</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T8.2.2.2.2.m1.1c">{\rm NDS}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T8.2.2.4.1" class="ltx_tr">
<td id="S6.T8.2.2.4.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">GUPNet</td>
<td id="S6.T8.2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">KITTI-3D</td>
<td id="S6.T8.2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">Car</td>
<td id="S6.T8.2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">14.85</td>
<td id="S6.T8.2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">40.85</td>
<td id="S6.T8.2.2.4.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">62.87</td>
<td id="S6.T8.2.2.4.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">47.57</td>
<td id="S6.T8.2.2.4.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">61.85</td>
</tr>
<tr id="S6.T8.2.2.5.2" class="ltx_tr">
<td id="S6.T8.2.2.5.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">PV-RCNN</td>
<td id="S6.T8.2.2.5.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">KITTI-3D</td>
<td id="S6.T8.2.2.5.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">Car</td>
<td id="S6.T8.2.2.5.2.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">82.41</td>
<td id="S6.T8.2.2.5.2.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">94.31</td>
<td id="S6.T8.2.2.5.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">94.47</td>
<td id="S6.T8.2.2.5.2.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">92.74</td>
<td id="S6.T8.2.2.5.2.8" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">92.61</td>
</tr>
<tr id="S6.T8.2.2.6.3" class="ltx_tr">
<td id="S6.T8.2.2.6.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">FCOS3D</td>
<td id="S6.T8.2.2.6.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">nuScenes</td>
<td id="S6.T8.2.2.6.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">Car</td>
<td id="S6.T8.2.2.6.3.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">1.83</td>
<td id="S6.T8.2.2.6.3.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">20.87</td>
<td id="S6.T8.2.2.6.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">50.74</td>
<td id="S6.T8.2.2.6.3.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">47.96</td>
<td id="S6.T8.2.2.6.3.8" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">54.32</td>
</tr>
<tr id="S6.T8.2.2.7.4" class="ltx_tr">
<td id="S6.T8.2.2.7.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">FCOS3D</td>
<td id="S6.T8.2.2.7.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">nuScenes</td>
<td id="S6.T8.2.2.7.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">All</td>
<td id="S6.T8.2.2.7.4.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.26</td>
<td id="S6.T8.2.2.7.4.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">6.06</td>
<td id="S6.T8.2.2.7.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">17.57</td>
<td id="S6.T8.2.2.7.4.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">31.05</td>
<td id="S6.T8.2.2.7.4.8" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">40.31</td>
</tr>
<tr id="S6.T8.2.2.8.5" class="ltx_tr">
<td id="S6.T8.2.2.8.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">BEVDet</td>
<td id="S6.T8.2.2.8.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">nuScenes</td>
<td id="S6.T8.2.2.8.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">All</td>
<td id="S6.T8.2.2.8.5.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.10</td>
<td id="S6.T8.2.2.8.5.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">6.55</td>
<td id="S6.T8.2.2.8.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">19.25</td>
<td id="S6.T8.2.2.8.5.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">30.75</td>
<td id="S6.T8.2.2.8.5.8" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">38.22</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>
<span id="S6.T8.4.1" class="ltx_text ltx_font_bold">Evaluation Metric.</span>
We exchange the metrics between KITTI-3D and nuScenes for the evaluation of 3D object detectors.
This provides a standardized way to compare the performance of 3D object detectors across the two datasets.
Dataset represents the source of training data for the detector. For detectors trained on KITTI-3D, we only compute translation, scale, and orientation for NDS under nuScenes metric. For KITTI metric, we evaluate the results of cars under the hard strict with varying IoU thresholds.
</figcaption>
</figure>
<figure id="S6.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05447/assets/figures/TIDE/BEVDet_iou0.25_wholeclass.png" id="S6.F3.sf1.g1" class="ltx_graphics ltx_img_portrait" width="145" height="232" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05447/assets/figures/TIDE/FCOS3D_iou0.25_wholeclass.png" id="S6.F3.sf2.g1" class="ltx_graphics ltx_img_portrait" width="145" height="230" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>

<span id="S6.F3.3.1" class="ltx_text ltx_font_bold">TIDE3D analyses</span> of BEVDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> and FCOS3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite> for all categories on nuScenes <em id="S6.F3.4.2" class="ltx_emph ltx_font_italic">validation</em> set based on the KITTI-style metric with 0.25 IoU threshold. Our results indicate that FCOS3D is impacted by various errors, while BEVDet is mainly dominated by localization errors.</figcaption>
</figure>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Metric Analysis for KITTI-3D and nuScenes</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">Additionally, we have observed interesting discrepancies in evaluation metrics between KITTI-3D and nuScenes.
Specifically, as shown in Table <a href="#S6.T8" title="Table 8 ‣ 6.3 TIDE3D Analysis ‣ 6 Experiments and Analysis ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, GUPNet and FCOS3D get similar AP of Car category under the nuScenes-style metric, but they show a significant performance gap under the KITTI-style metric. As we know, KITTI-3D primarily uses the strict 3D IoU metric with a certain threshold as the metric, while nuScenes measures the center distance and a series of decoupled error attributes such as translation, scale, orientation, <em id="S6.SS4.p1.1.1" class="ltx_emph ltx_font_italic">etc</em>.<span id="S6.SS4.p1.1.2" class="ltx_text"></span>, separately (Appendix <a href="#S1.SS1" title="A.1 Datasets and Metrics ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a> provides the details of metrics).
We find that translation, scale, and orientation jointly affect the result of IoU, which means that methods achieving competitive performance on the KITTI-3D metric require comprehensive development, while methods with special strengths in nuScenes can also get a good performance. According to our experience, a model with good performance under the KITTI-style metric generally performs well under the nuScenes-style metric, but the opposite is not always true.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">Furthermore, we present TIDE3D analyses of BEVDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> and FCOS3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite> for all categories on nuScenes <em id="S6.SS4.p2.1.1" class="ltx_emph ltx_font_italic">validation</em> set based on the KITTI-style metric with 0.25 IoU threshold in Figure <a href="#S6.F3" title="Figure 3 ‣ 6.3 TIDE3D Analysis ‣ 6 Experiments and Analysis ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Based on the distribution of localization sub-errors (6.5-1.6-1.7 <span id="S6.SS4.p2.1.2" class="ltx_text ltx_font_italic">v.s.</span> 10.1-1.7-1.2), we can find that FCOS3D makes more accurate predictions for orientation and dimension than BEVDet.
One of the reasons for this is the loss of height information of BEVDet in the LSS (lift, splat, shoot) process, which serves to transform the image feature into BEV space. Consequently, the model may struggle to estimate the height of objects accurately and the center height of objects.
See Appendix <a href="#S1.SS2" title="A.2 Average Height Error ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a> for more results and analyses.</p>
</div>
<div id="S6.SS4.p3" class="ltx_para">
<p id="S6.SS4.p3.1" class="ltx_p">The above two reasons explain why the mainstream methods for KITTI and nuScenes are distinct, and it is reasonable and necessary to separately design models to match the preferences of these two kinds of metrics.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we aim to standardize the evaluation protocols for image-based 3D object detection, an ever-evolving research field. We build a modular-designed codebase and provide efficient training recipes for this task, leading to significant improvements in the performance of current methods. Besides, we offer an error diagnosis toolbox to measure the detailed characterization of detection algorithms and highlight some open problems in the field. We hope our codebase, training recipes, error diagnosis toolbox, and discussions will foster better and more standardized research practices within the image-based 3D object detection community.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported by the Australian Medical Research Future Fund MRFAI000085, CRC-P Smart Material Recovery Facility (SMRF) – Curby Soft Plastics, CRC-P ARIA - Bionic Visual-Spatial Prosthesis for the Blind, and National Natural Science Foundation of
China (NSFC Grants NO. 61976038).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.4.1" class="ltx_text" style="font-size:90%;">
Coco analysis toolkit.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://cocodataset.org/##detection-eval" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://cocodataset.org/##detection-eval</a><span id="bib.bib1.5.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text" style="font-size:90%;">Accessed: 2020-03-01.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.2.2.1" class="ltx_text" style="font-size:90%;">[2]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="font-size:90%;">
Daniel Bolya, Sean Foley, James Hays, and Judy Hoffman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.5.1" class="ltx_text" style="font-size:90%;">Tide: A general toolbox for identifying object detection errors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib2.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.2.2.1" class="ltx_text" style="font-size:90%;">[3]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="font-size:90%;">
Garrick Brazil and Xiaoming Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.5.1" class="ltx_text" style="font-size:90%;">M3d-rpn: Monocular 3d region proposal network for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib3.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.2.2.1" class="ltx_text" style="font-size:90%;">[4]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="font-size:90%;">
Garrick Brazil, Gerard Pons-Moll, Xiaoming Liu, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.5.1" class="ltx_text" style="font-size:90%;">Kinematic 3d object detection in monocular video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib4.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.2.2.1" class="ltx_text" style="font-size:90%;">[5]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="font-size:90%;">
Garrick Brazil, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia
Gkioxari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.5.1" class="ltx_text" style="font-size:90%;">Omni3D: A large benchmark and model for 3D object detection in
the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:2207.10660</span><span id="bib.bib5.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.2.2.1" class="ltx_text" style="font-size:90%;">[6]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.5.1" class="ltx_text" style="font-size:90%;">nuscenes: A multimodal dataset for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib6.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.2.2.1" class="ltx_text" style="font-size:90%;">[7]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="font-size:90%;">
Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa, Céline Teuliere, and
Thierry Chateau.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.5.1" class="ltx_text" style="font-size:90%;">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d
vehicle analysis from monocular image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib7.8.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.2.2.1" class="ltx_text" style="font-size:90%;">[8]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="font-size:90%;">
Hansheng Chen, Yuyao Huang, Wei Tian, Zhong Gao, and Lu Xiong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.5.1" class="ltx_text" style="font-size:90%;">Monorun: Monocular 3d object detection by reconstruction and
uncertainty propagation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.2.2.1" class="ltx_text" style="font-size:90%;">[9]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.4.1" class="ltx_text" style="font-size:90%;">
Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li,
Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng,
Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,
Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and
Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.5.1" class="ltx_text" style="font-size:90%;">MMDetection: Open mmlab detection toolbox and benchmark.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.07155</span><span id="bib.bib9.7.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.2.2.1" class="ltx_text" style="font-size:90%;">[10]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.4.1" class="ltx_text" style="font-size:90%;">
Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and Raquel
Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.5.1" class="ltx_text" style="font-size:90%;">Monocular 3d object detection for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib10.8.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.2.2.1" class="ltx_text" style="font-size:90%;">[11]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.4.1" class="ltx_text" style="font-size:90%;">
Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Andrew G Berneshawi, Huimin Ma, Sanja
Fidler, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.5.1" class="ltx_text" style="font-size:90%;">3d object proposals for accurate object class detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib11.8.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.2.2.1" class="ltx_text" style="font-size:90%;">[12]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="font-size:90%;">
Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.5.1" class="ltx_text" style="font-size:90%;">Multi-view 3d object detection network for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib12.8.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.2.2.1" class="ltx_text" style="font-size:90%;">[13]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="font-size:90%;">
Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.5.1" class="ltx_text" style="font-size:90%;">Dsgn: Deep stereo geometry network for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib13.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.2.2.1" class="ltx_text" style="font-size:90%;">[14]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="font-size:90%;">
Yi-Nan Chen, Hang Dai, and Yong Ding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.5.1" class="ltx_text" style="font-size:90%;">Pseudo-stereo for monocular 3d object detection in autonomous
driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib14.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.2.2.1" class="ltx_text" style="font-size:90%;">[15]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.4.1" class="ltx_text" style="font-size:90%;">
Zhiyu Chong, Xinzhu Ma, Hong Zhang, Yuxin Yue, Haojie Li, Zhihui Wang, and
Wanli Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.5.1" class="ltx_text" style="font-size:90%;">Monodistill: Learning spatial features for monocular 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib15.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.2.2.1" class="ltx_text" style="font-size:90%;">[16]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="font-size:90%;">
MMDetection3D Contributors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.5.1" class="ltx_text" style="font-size:90%;">MMDetection3D: OpenMMLab next-generation platform for general 3D
object detection.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/open-mmlab/mmdetection3d" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/open-mmlab/mmdetection3d</a><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.2.2.1" class="ltx_text" style="font-size:90%;">[17]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.4.1" class="ltx_text" style="font-size:90%;">
Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias Nießner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.5.1" class="ltx_text" style="font-size:90%;">Scannet: Richly-annotated 3d reconstructions of indoor scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.8.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.2.2.1" class="ltx_text" style="font-size:90%;">[18]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="font-size:90%;">
Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu, and Ping
Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.5.1" class="ltx_text" style="font-size:90%;">Learning depth-guided convolutions for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib18.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.2.2.1" class="ltx_text" style="font-size:90%;">[19]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew
Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.5.1" class="ltx_text" style="font-size:90%;">The pascal visual object classes (voc) challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib19.7.2" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.2.2.1" class="ltx_text" style="font-size:90%;">[20]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="font-size:90%;">
Divyansh Garg, Yan Wang, Bharath Hariharan, Mark Campbell, Kilian Q Weinberger,
and Wei-Lun Chao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.5.1" class="ltx_text" style="font-size:90%;">Wasserstein distances for stereo disparity estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text" style="font-size:90%;">2020.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.2.2.1" class="ltx_text" style="font-size:90%;">[21]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.5.1" class="ltx_text" style="font-size:90%;">Are we ready for autonomous driving? the kitti vision benchmark
suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib21.8.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.2.2.1" class="ltx_text" style="font-size:90%;">[22]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:90%;">
Jiaqi Gu, Bojian Wu, Lubin Fan, Jianqiang Huang, Shen Cao, Zhiyu Xiang, and
Xian-Sheng Hua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.5.1" class="ltx_text" style="font-size:90%;">Homography loss for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib22.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.2.2.1" class="ltx_text" style="font-size:90%;">[23]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.4.1" class="ltx_text" style="font-size:90%;">
Xiaoyang Guo, Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.5.1" class="ltx_text" style="font-size:90%;">Liga-stereo: Learning lidar geometry aware representations for
stereo-based 3d detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib23.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.2.2.1" class="ltx_text" style="font-size:90%;">[24]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.5.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib24.8.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.2.2.1" class="ltx_text" style="font-size:90%;">[25]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="font-size:90%;">
Derek Hoiem, Yodsawalai Chodpathumwan, and Qieyun Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.5.1" class="ltx_text" style="font-size:90%;">Diagnosing error in object detectors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib25.8.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.2.2.1" class="ltx_text" style="font-size:90%;">[26]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="font-size:90%;">
Yu Hong, Hang Dai, and Yong Ding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.5.1" class="ltx_text" style="font-size:90%;">Cross-modality knowledge distillation network for monocular 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib26.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.2.2.1" class="ltx_text" style="font-size:90%;">[27]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.4.1" class="ltx_text" style="font-size:90%;">
Junjie Huang, Guan Huang, Zheng Zhu, and Dalong Du.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.5.1" class="ltx_text" style="font-size:90%;">Bevdet: High-performance multi-camera 3d object detection in
bird-eye-view.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.11790</span><span id="bib.bib27.7.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.2.2.1" class="ltx_text" style="font-size:90%;">[28]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text" style="font-size:90%;">
Jason Ku, Alex D Pon, and Steven L Waslander.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.5.1" class="ltx_text" style="font-size:90%;">Monocular 3d object detection leveraging accurate proposals and shape
reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib28.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.2.2.1" class="ltx_text" style="font-size:90%;">[29]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="font-size:90%;">
Abhinav Kumar, Garrick Brazil, Enrique Corona, Armin Parchami, and Xiaoming
Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.5.1" class="ltx_text" style="font-size:90%;">Deviant: Depth equivariant network for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib29.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.2.2.1" class="ltx_text" style="font-size:90%;">[30]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.4.1" class="ltx_text" style="font-size:90%;">
Abhinav Kumar, Garrick Brazil, and Xiaoming Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.5.1" class="ltx_text" style="font-size:90%;">Groomed-nms: Grouped mathematically differentiable nms for monocular
3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib30.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.2.2.1" class="ltx_text" style="font-size:90%;">[31]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="font-size:90%;">
Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.5.1" class="ltx_text" style="font-size:90%;">Gs3d: An efficient 3d object detection framework for autonomous
driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib31.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.2.2.1" class="ltx_text" style="font-size:90%;">[32]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="font-size:90%;">
Peiliang Li, Xiaozhi Chen, and Shaojie Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.5.1" class="ltx_text" style="font-size:90%;">Stereo r-cnn based 3d object detection for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib32.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.2.2.1" class="ltx_text" style="font-size:90%;">[33]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.4.1" class="ltx_text" style="font-size:90%;">
Peixuan Li, Huaici Zhao, Pengfei Liu, and Feidao Cao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.5.1" class="ltx_text" style="font-size:90%;">Rtm3d: Real-time monocular 3d detection from object keypoints for
autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib33.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.2.2.1" class="ltx_text" style="font-size:90%;">[34]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="font-size:90%;">
Yinhao Li, Han Bao, Zheng Ge, Jinrong Yang, Jianjian Sun, and Zeming Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.5.1" class="ltx_text" style="font-size:90%;">Bevstereo: Enhancing depth estimation in multi-view 3d object
detection with dynamic temporal stereo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib34.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.2.2.1" class="ltx_text" style="font-size:90%;">[35]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="font-size:90%;">
Yingyan Li, Yuntao Chen, Jiawei He, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.5.1" class="ltx_text" style="font-size:90%;">Densely constrained depth estimator for monocular 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib35.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.2.2.1" class="ltx_text" style="font-size:90%;">[36]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.4.1" class="ltx_text" style="font-size:90%;">
Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran Wang, Yukang Shi,
Jianjian Sun, and Zeming Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.5.1" class="ltx_text" style="font-size:90%;">Bevdepth: Acquisition of reliable depth for multi-view 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib36.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.2.2.1" class="ltx_text" style="font-size:90%;">[37]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.4.1" class="ltx_text" style="font-size:90%;">
Zhuoling Li, Zhan Qu, Yang Zhou, Jianzhuang Liu, Haoqian Wang, and Lihui Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.5.1" class="ltx_text" style="font-size:90%;">Diversity matters: Fully exploiting depth clues for reliable
monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib37.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.2.2.1" class="ltx_text" style="font-size:90%;">[38]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.4.1" class="ltx_text" style="font-size:90%;">
Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao,
and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.5.1" class="ltx_text" style="font-size:90%;">Bevformer: Learning bird’s-eye-view representation from
multi-camera images via spatiotemporal transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib38.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.2.2.1" class="ltx_text" style="font-size:90%;">[39]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.4.1" class="ltx_text" style="font-size:90%;">
Qing Lian, Peiliang Li, and Xiaozhi Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.5.1" class="ltx_text" style="font-size:90%;">Monojsg: Joint semantic and geometric cost volume for monocular 3d
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib39.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.2.2.1" class="ltx_text" style="font-size:90%;">[40]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.4.1" class="ltx_text" style="font-size:90%;">
Qing Lian, Yanbo Xu, Weilong Yao, Yingcong Chen, and Tong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.5.1" class="ltx_text" style="font-size:90%;">Semi-supervised monocular 3d object detection by multi-view
consistency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib40.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.2.2.1" class="ltx_text" style="font-size:90%;">[41]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.4.1" class="ltx_text" style="font-size:90%;">
Qing Lian, Botao Ye, Ruijia Xu, Weilong Yao, and Tong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.5.1" class="ltx_text" style="font-size:90%;">Exploring geometric consistency for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib41.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.2.2.1" class="ltx_text" style="font-size:90%;">[42]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.4.1" class="ltx_text" style="font-size:90%;">
Ce Liu, Shuhang Gu, Luc Van Gool, and Radu Timofte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.5.1" class="ltx_text" style="font-size:90%;">Deep line encoding for monocular 3d object detection and depth
prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">BMVC</span><span id="bib.bib42.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.2.2.1" class="ltx_text" style="font-size:90%;">[43]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.4.1" class="ltx_text" style="font-size:90%;">
Xianpeng Liu, Nan Xue, and Tianfu Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.5.1" class="ltx_text" style="font-size:90%;">Learning auxiliary monocular contexts helps monocular 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib43.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.2.2.1" class="ltx_text" style="font-size:90%;">[44]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.4.1" class="ltx_text" style="font-size:90%;">
Zechen Liu, Zizhang Wu, and Roland Tóth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.5.1" class="ltx_text" style="font-size:90%;">Smoke: Single-stage monocular 3d object detection via keypoint
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR Workshops</span><span id="bib.bib44.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.2.2.1" class="ltx_text" style="font-size:90%;">[45]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.4.1" class="ltx_text" style="font-size:90%;">
Zongdai Liu, Dingfu Zhou, Feixiang Lu, Jin Fang, and Liangjun Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.5.1" class="ltx_text" style="font-size:90%;">Autoshape: Real-time shape-aware monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib45.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.2.2.1" class="ltx_text" style="font-size:90%;">[46]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.4.1" class="ltx_text" style="font-size:90%;">
Yan Lu, Xinzhu Ma, Lei Yang, Tianzhu Zhang, Yating Liu, Qi Chu, Junjie Yan, and
Wanli Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.5.1" class="ltx_text" style="font-size:90%;">Geometry uncertainty projection network for monocular 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib46.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.2.2.1" class="ltx_text" style="font-size:90%;">[47]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.4.1" class="ltx_text" style="font-size:90%;">
Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu Zeng, and Wanli Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.5.1" class="ltx_text" style="font-size:90%;">Rethinking pseudo-lidar representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib47.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.2.2.1" class="ltx_text" style="font-size:90%;">[48]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.4.1" class="ltx_text" style="font-size:90%;">
Xinzhu Ma, Yuan Meng, Yinmin Zhang, Lei Bai, Jun Hou, Shuai Yi, and Wanli
Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.5.1" class="ltx_text" style="font-size:90%;">An empirical study of pseudo-labeling for image-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2208.07137</span><span id="bib.bib48.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.2.2.1" class="ltx_text" style="font-size:90%;">[49]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.4.1" class="ltx_text" style="font-size:90%;">
Xinzhu Ma, Wanli Ouyang, Andrea Simonelli, and Elisa Ricci.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.5.1" class="ltx_text" style="font-size:90%;">3d object detection from images for autonomous driving: a survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2202.02980</span><span id="bib.bib49.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.2.2.1" class="ltx_text" style="font-size:90%;">[50]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.4.1" class="ltx_text" style="font-size:90%;">
Xinzhu Ma, Zhihui Wang, Haojie Li, Pengbo Zhang, Wanli Ouyang, and Xin Fan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.5.1" class="ltx_text" style="font-size:90%;">Accurate monocular 3d object detection via color-embedded 3d
reconstruction for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib50.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.2.2.1" class="ltx_text" style="font-size:90%;">[51]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.4.1" class="ltx_text" style="font-size:90%;">
Xinzhu Ma, Yinmin Zhang, Dan Xu, Dongzhan Zhou, Shuai Yi, Haojie Li, and Wanli
Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.5.1" class="ltx_text" style="font-size:90%;">Delving into localization errors for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib51.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.2.2.1" class="ltx_text" style="font-size:90%;">[52]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.4.1" class="ltx_text" style="font-size:90%;">
Fabian Manhardt, Wadim Kehl, and Adrien Gaidon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.5.1" class="ltx_text" style="font-size:90%;">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric
shape.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib52.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.2.2.1" class="ltx_text" style="font-size:90%;">[53]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.4.1" class="ltx_text" style="font-size:90%;">
Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, and Adrien Gaidon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.5.1" class="ltx_text" style="font-size:90%;">Is pseudo-lidar needed for monocular 3d object detection?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib53.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.2.2.1" class="ltx_text" style="font-size:90%;">[54]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.4.1" class="ltx_text" style="font-size:90%;">
Liang Peng, Fei Liu, Zhengxu Yu, Senbo Yan, Dan Deng, Zheng Yang, Haifeng Liu,
and Deng Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.5.1" class="ltx_text" style="font-size:90%;">Lidar point cloud guided monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib54.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.2.2.1" class="ltx_text" style="font-size:90%;">[55]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.4.1" class="ltx_text" style="font-size:90%;">
Liang Peng, Xiaopei Wu, Zheng Yang, Haifeng Liu, and Deng Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.5.1" class="ltx_text" style="font-size:90%;">Did-m3d: Decoupling instance depth for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib55.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.2.2.1" class="ltx_text" style="font-size:90%;">[56]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.4.1" class="ltx_text" style="font-size:90%;">
Wanli Peng, Hao Pan, He Liu, and Yi Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.5.1" class="ltx_text" style="font-size:90%;">Ida-3d: Instance-depth-aware 3d object detection from stereo vision
for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib56.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.2.2.1" class="ltx_text" style="font-size:90%;">[57]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.4.1" class="ltx_text" style="font-size:90%;">
Jonah Philion and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.5.1" class="ltx_text" style="font-size:90%;">Lift, splat, shoot: Encoding images from arbitrary camera rigs by
implicitly unprojecting to 3d.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib57.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.2.2.1" class="ltx_text" style="font-size:90%;">[58]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.4.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.5.1" class="ltx_text" style="font-size:90%;">Deep hough voting for 3d object detection in point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib58.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.2.2.1" class="ltx_text" style="font-size:90%;">[59]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.4.1" class="ltx_text" style="font-size:90%;">
Rui Qian, Divyansh Garg, Yan Wang, Yurong You, Serge Belongie, Bharath
Hariharan, Mark Campbell, Kilian Q Weinberger, and Wei-Lun Chao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.5.1" class="ltx_text" style="font-size:90%;">End-to-end pseudo-lidar for image-based 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib59.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.2.2.1" class="ltx_text" style="font-size:90%;">[60]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.4.1" class="ltx_text" style="font-size:90%;">
Zequn Qin and Xi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.5.1" class="ltx_text" style="font-size:90%;">Monoground: Detecting monocular 3d objects from the ground.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib60.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.2.2.1" class="ltx_text" style="font-size:90%;">[61]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.4.1" class="ltx_text" style="font-size:90%;">
Zengyi Qin, Jinglu Wang, and Yan Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.5.1" class="ltx_text" style="font-size:90%;">Monogrnet: A geometric reasoning network for monocular 3d object
localization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib61.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.2.2.1" class="ltx_text" style="font-size:90%;">[62]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.4.1" class="ltx_text" style="font-size:90%;">
Zengyi Qin, Jinglu Wang, and Yan Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.5.1" class="ltx_text" style="font-size:90%;">Triangulation learning network: from monocular to stereo 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib62.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.2.2.1" class="ltx_text" style="font-size:90%;">[63]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.4.1" class="ltx_text" style="font-size:90%;">
Cody Reading, Ali Harakeh, Julia Chae, and Steven L Waslander.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.5.1" class="ltx_text" style="font-size:90%;">Categorical depth distribution network for monocular 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib63.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.2.2.1" class="ltx_text" style="font-size:90%;">[64]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.4.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.5.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib64.7.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.2.2.1" class="ltx_text" style="font-size:90%;">[65]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.4.1" class="ltx_text" style="font-size:90%;">
Thomas Roddick, Alex Kendall, and Roberto Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.5.1" class="ltx_text" style="font-size:90%;">Orthographic feature transform for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">BMVC</span><span id="bib.bib65.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.2.2.1" class="ltx_text" style="font-size:90%;">[66]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.4.1" class="ltx_text" style="font-size:90%;">
Danila Rukhovich, Anna Vorontsova, and Anton Konushin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.5.1" class="ltx_text" style="font-size:90%;">Imvoxelnet: Image to voxels projection for monocular and multi-view
general-purpose 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">WACV</span><span id="bib.bib66.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.2.2.1" class="ltx_text" style="font-size:90%;">[67]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.4.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and
Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.5.1" class="ltx_text" style="font-size:90%;">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib67.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.2.2.1" class="ltx_text" style="font-size:90%;">[68]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.4.1" class="ltx_text" style="font-size:90%;">
Xuepeng Shi, Zhixiang Chen, and Tae-Kyun Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.5.1" class="ltx_text" style="font-size:90%;">Distance-normalized unified representation for monocular 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib68.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib68.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.2.2.1" class="ltx_text" style="font-size:90%;">[69]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.4.1" class="ltx_text" style="font-size:90%;">
Andrea Simonelli, Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder, and
Elisa Ricci.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.5.1" class="ltx_text" style="font-size:90%;">Are we missing confidence in pseudo-lidar methods for monocular 3d
object detection?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib69.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib69.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.2.2.1" class="ltx_text" style="font-size:90%;">[70]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.4.1" class="ltx_text" style="font-size:90%;">
Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi, Manuel López-Antequera,
and Peter Kontschieder.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.5.1" class="ltx_text" style="font-size:90%;">Disentangling monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib70.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib70.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.2.2.1" class="ltx_text" style="font-size:90%;">[71]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.4.1" class="ltx_text" style="font-size:90%;">
Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi, Elisa Ricci, and Peter
Kontschieder.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.5.1" class="ltx_text" style="font-size:90%;">Towards generalization across depth for monocular 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib71.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib71.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.2.2.1" class="ltx_text" style="font-size:90%;">[72]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.4.1" class="ltx_text" style="font-size:90%;">
Vishwanath A Sindagi, Yin Zhou, and Oncel Tuzel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.5.1" class="ltx_text" style="font-size:90%;">Mvx-net: Multimodal voxelnet for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib72.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib72.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.2.2.1" class="ltx_text" style="font-size:90%;">[73]</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.4.1" class="ltx_text" style="font-size:90%;">
Leslie N Smith and Nicholay Topin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.5.1" class="ltx_text" style="font-size:90%;">Super-convergence: Very fast training of neural networks using large
learning rates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1708.07120</span><span id="bib.bib73.7.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.2.2.1" class="ltx_text" style="font-size:90%;">[74]</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.4.1" class="ltx_text" style="font-size:90%;">
Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou,
and Hujun Bao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.5.1" class="ltx_text" style="font-size:90%;">Disp r-cnn: Stereo 3d object detection via shape prior guided
instance disparity estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib74.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib74.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.2.2.1" class="ltx_text" style="font-size:90%;">[75]</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.4.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.5.1" class="ltx_text" style="font-size:90%;">Scalability in perception for autonomous driving: Waymo open dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib75.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib75.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.2.2.1" class="ltx_text" style="font-size:90%;">[76]</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.4.1" class="ltx_text" style="font-size:90%;">
OpenPCDet Development Team.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.5.1" class="ltx_text" style="font-size:90%;">Openpcdet: An open-source toolbox for 3d object detection from point
clouds.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/open-mmlab/OpenPCDet" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/open-mmlab/OpenPCDet</a><span id="bib.bib76.6.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.2.2.1" class="ltx_text" style="font-size:90%;">[77]</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.4.1" class="ltx_text" style="font-size:90%;">
Zhi Tian, Chunhua Shen, Hao Chen, and Tong He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.5.1" class="ltx_text" style="font-size:90%;">Fcos: Fully convolutional one-stage object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib77.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib77.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.2.2.1" class="ltx_text" style="font-size:90%;">[78]</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.4.1" class="ltx_text" style="font-size:90%;">
Li Wang, Liang Du, Xiaoqing Ye, Yanwei Fu, Guodong Guo, Xiangyang Xue, Jianfeng
Feng, and Li Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.5.1" class="ltx_text" style="font-size:90%;">Depth-conditioned dynamic message propagation for monocular 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib78.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib78.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.2.2.1" class="ltx_text" style="font-size:90%;">[79]</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.4.1" class="ltx_text" style="font-size:90%;">
Tai Wang, Jiangmiao Pang, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.5.1" class="ltx_text" style="font-size:90%;">Monocular 3d object detection with depth from motion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib79.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib79.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib80.2.2.1" class="ltx_text" style="font-size:90%;">[80]</span></span>
<span class="ltx_bibblock"><span id="bib.bib80.4.1" class="ltx_text" style="font-size:90%;">
Tai Wang, ZHU Xinge, Jiangmiao Pang, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.5.1" class="ltx_text" style="font-size:90%;">Probabilistic and geometric depth: Detecting objects in perspective.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib80.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRL</span><span id="bib.bib80.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib81.2.2.1" class="ltx_text" style="font-size:90%;">[81]</span></span>
<span class="ltx_bibblock"><span id="bib.bib81.4.1" class="ltx_text" style="font-size:90%;">
Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.5.1" class="ltx_text" style="font-size:90%;">Fcos3d: Fully convolutional one-stage monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib81.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV Workshop</span><span id="bib.bib81.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib82.2.2.1" class="ltx_text" style="font-size:90%;">[82]</span></span>
<span class="ltx_bibblock"><span id="bib.bib82.4.1" class="ltx_text" style="font-size:90%;">
Xinlong Wang, Wei Yin, Tao Kong, Yuning Jiang, Lei Li, and Chunhua Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.5.1" class="ltx_text" style="font-size:90%;">Task-aware monocular depth estimation for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib82.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib82.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib83.2.2.1" class="ltx_text" style="font-size:90%;">[83]</span></span>
<span class="ltx_bibblock"><span id="bib.bib83.4.1" class="ltx_text" style="font-size:90%;">
Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and
Kilian Q Weinberger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.5.1" class="ltx_text" style="font-size:90%;">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d
object detection for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib83.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib83.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib84.2.2.1" class="ltx_text" style="font-size:90%;">[84]</span></span>
<span class="ltx_bibblock"><span id="bib.bib84.4.1" class="ltx_text" style="font-size:90%;">
Yue Wang, Vitor Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, and Justin
Solomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.5.1" class="ltx_text" style="font-size:90%;">Detr3d: 3d object detection from multi-view images via 3d-to-2d
queries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib84.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRL</span><span id="bib.bib84.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib85.2.2.1" class="ltx_text" style="font-size:90%;">[85]</span></span>
<span class="ltx_bibblock"><span id="bib.bib85.4.1" class="ltx_text" style="font-size:90%;">
Xinshuo Weng and Kris Kitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.5.1" class="ltx_text" style="font-size:90%;">Monocular 3d object detection with pseudo-lidar point cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib85.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV Workshops</span><span id="bib.bib85.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib86.2.2.1" class="ltx_text" style="font-size:90%;">[86]</span></span>
<span class="ltx_bibblock"><span id="bib.bib86.4.1" class="ltx_text" style="font-size:90%;">
Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.5.1" class="ltx_text" style="font-size:90%;">Detectron2.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/facebookresearch/detectron2" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/facebookresearch/detectron2</a><span id="bib.bib86.6.1" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib87.2.2.1" class="ltx_text" style="font-size:90%;">[87]</span></span>
<span class="ltx_bibblock"><span id="bib.bib87.4.1" class="ltx_text" style="font-size:90%;">
Bin Xu and Zhenzhong Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.5.1" class="ltx_text" style="font-size:90%;">Multi-level fusion based 3d object detection from monocular images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib87.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib87.8.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib88.2.2.1" class="ltx_text" style="font-size:90%;">[88]</span></span>
<span class="ltx_bibblock"><span id="bib.bib88.4.1" class="ltx_text" style="font-size:90%;">
Zhenbo Xu, Wei Zhang, Xiaoqing Ye, Xiao Tan, Wei Yang, Shilei Wen, Errui Ding,
Ajin Meng, and Liusheng Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.5.1" class="ltx_text" style="font-size:90%;">Zoomnet: Part-aware adaptive zooming neural network for 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib88.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib88.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib89.2.2.1" class="ltx_text" style="font-size:90%;">[89]</span></span>
<span class="ltx_bibblock"><span id="bib.bib89.4.1" class="ltx_text" style="font-size:90%;">
Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Geoff Pleiss, Bharath
Hariharan, Mark Campbell, and Kilian Q Weinberger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.5.1" class="ltx_text" style="font-size:90%;">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous
driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib89.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib89.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib90.2.2.1" class="ltx_text" style="font-size:90%;">[90]</span></span>
<span class="ltx_bibblock"><span id="bib.bib90.4.1" class="ltx_text" style="font-size:90%;">
Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.5.1" class="ltx_text" style="font-size:90%;">Deep layer aggregation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib90.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib90.8.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib91.2.2.1" class="ltx_text" style="font-size:90%;">[91]</span></span>
<span class="ltx_bibblock"><span id="bib.bib91.4.1" class="ltx_text" style="font-size:90%;">
Renrui Zhang, Han Qiu, Tai Wang, Xuanzhuo Xu, Ziyu Guo, Yu Qiao, Peng Gao, and
Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.5.1" class="ltx_text" style="font-size:90%;">Monodetr: Depth-aware transformer for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.13310</span><span id="bib.bib91.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib92.2.2.1" class="ltx_text" style="font-size:90%;">[92]</span></span>
<span class="ltx_bibblock"><span id="bib.bib92.4.1" class="ltx_text" style="font-size:90%;">
Yunpeng Zhang, Jiwen Lu, and Jie Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.5.1" class="ltx_text" style="font-size:90%;">Objects are different: Flexible monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib92.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib92.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib93.2.2.1" class="ltx_text" style="font-size:90%;">[93]</span></span>
<span class="ltx_bibblock"><span id="bib.bib93.4.1" class="ltx_text" style="font-size:90%;">
Yinmin Zhang, Xinzhu Ma, Shuai Yi, Jun Hou, Zhihui Wang, Wanli Ouyang, and Dan
Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.5.1" class="ltx_text" style="font-size:90%;">Learning geometry-guided depth via projective modeling for monocular
3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.13931</span><span id="bib.bib93.7.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib94.2.2.1" class="ltx_text" style="font-size:90%;">[94]</span></span>
<span class="ltx_bibblock"><span id="bib.bib94.4.1" class="ltx_text" style="font-size:90%;">
Yunpeng Zhang, Wenzhao Zheng, Zheng Zhu, Guan Huang, Dalong Du, Jie Zhou, and
Jiwen Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.5.1" class="ltx_text" style="font-size:90%;">Dimension embeddings for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib94.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib94.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib95.2.2.1" class="ltx_text" style="font-size:90%;">[95]</span></span>
<span class="ltx_bibblock"><span id="bib.bib95.4.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.5.1" class="ltx_text" style="font-size:90%;">Objects as points.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1904.07850</span><span id="bib.bib95.7.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib96.2.2.1" class="ltx_text" style="font-size:90%;">[96]</span></span>
<span class="ltx_bibblock"><span id="bib.bib96.4.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib96.5.1" class="ltx_text" style="font-size:90%;">Deformable convnets v2: More deformable, better results.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib96.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib96.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib96.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">A </span>Appendix</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Datasets and Metrics</h3>

<div id="S1.SS1.p1" class="ltx_para ltx_noindent">
<p id="S1.SS1.p1.1" class="ltx_p"><span id="S1.SS1.p1.1.1" class="ltx_text ltx_font_bold">KITTI-3D.</span>
KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> is the most popular dataset in autonomous driving scenario in the past decade. KITTI-3D is a subset of KITTI dataset, which contains 7,481 and 7,518 frames for training and testing respectively. In each frame, the stereo images, synchronized LiDAR sweep, calibration files, and 3D bounding box annotations are provided. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>, we split the training frames into a training set (3,712 frames) and a validation set (3,769 frames), and conduct the experiments in this split.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite>, we adopt the <math id="S1.SS1.p1.1.m1.1" class="ltx_Math" alttext="{\rm AP}_{40}" display="inline"><semantics id="S1.SS1.p1.1.m1.1a"><msub id="S1.SS1.p1.1.m1.1.1" xref="S1.SS1.p1.1.m1.1.1.cmml"><mi id="S1.SS1.p1.1.m1.1.1.2" xref="S1.SS1.p1.1.m1.1.1.2.cmml">AP</mi><mn id="S1.SS1.p1.1.m1.1.1.3" xref="S1.SS1.p1.1.m1.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.1.m1.1b"><apply id="S1.SS1.p1.1.m1.1.1.cmml" xref="S1.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.SS1.p1.1.m1.1.1.1.cmml" xref="S1.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S1.SS1.p1.1.m1.1.1.2.cmml" xref="S1.SS1.p1.1.m1.1.1.2">AP</ci><cn type="integer" id="S1.SS1.p1.1.m1.1.1.3.cmml" xref="S1.SS1.p1.1.m1.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.1.m1.1c">{\rm AP}_{40}</annotation></semantics></math> of 3D detection and Bird’s Eye View (BEV) detection as metrics. We mainly focus on the Car category on KITTI-3D in the main paper, and both 0.7 and 0.5
IoU thresholds are considered. In this supplementary, we also discuss the Pedestrian and Cyclist categories with 0.5 IoU threshold.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para ltx_noindent">
<p id="S1.SS1.p2.1" class="ltx_p"><span id="S1.SS1.p2.1.1" class="ltx_text ltx_font_bold">nuScenes.</span> nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> is a large-scale autonomous driving dataset proposed in 2020.
It provides about 40K annotated frames of 10 categories for the panoramic view in the autonomous driving scenario. In particular, there are 28,130 frames, 6,019 frames, and 6,008 frames for training, validation, and testing respectively (six images per frame). As for evaluation metrics, nuScenes uses a 2D center error (in the ground plane) based AP metric. Specifically, they consider four thresholds, <math id="S1.SS1.p2.1.m1.4" class="ltx_Math" alttext="\mathbb{D}=\{0.5,1,2,4\}" display="inline"><semantics id="S1.SS1.p2.1.m1.4a"><mrow id="S1.SS1.p2.1.m1.4.5" xref="S1.SS1.p2.1.m1.4.5.cmml"><mi id="S1.SS1.p2.1.m1.4.5.2" xref="S1.SS1.p2.1.m1.4.5.2.cmml">𝔻</mi><mo id="S1.SS1.p2.1.m1.4.5.1" xref="S1.SS1.p2.1.m1.4.5.1.cmml">=</mo><mrow id="S1.SS1.p2.1.m1.4.5.3.2" xref="S1.SS1.p2.1.m1.4.5.3.1.cmml"><mo stretchy="false" id="S1.SS1.p2.1.m1.4.5.3.2.1" xref="S1.SS1.p2.1.m1.4.5.3.1.cmml">{</mo><mn id="S1.SS1.p2.1.m1.1.1" xref="S1.SS1.p2.1.m1.1.1.cmml">0.5</mn><mo id="S1.SS1.p2.1.m1.4.5.3.2.2" xref="S1.SS1.p2.1.m1.4.5.3.1.cmml">,</mo><mn id="S1.SS1.p2.1.m1.2.2" xref="S1.SS1.p2.1.m1.2.2.cmml">1</mn><mo id="S1.SS1.p2.1.m1.4.5.3.2.3" xref="S1.SS1.p2.1.m1.4.5.3.1.cmml">,</mo><mn id="S1.SS1.p2.1.m1.3.3" xref="S1.SS1.p2.1.m1.3.3.cmml">2</mn><mo id="S1.SS1.p2.1.m1.4.5.3.2.4" xref="S1.SS1.p2.1.m1.4.5.3.1.cmml">,</mo><mn id="S1.SS1.p2.1.m1.4.4" xref="S1.SS1.p2.1.m1.4.4.cmml">4</mn><mo stretchy="false" id="S1.SS1.p2.1.m1.4.5.3.2.5" xref="S1.SS1.p2.1.m1.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p2.1.m1.4b"><apply id="S1.SS1.p2.1.m1.4.5.cmml" xref="S1.SS1.p2.1.m1.4.5"><eq id="S1.SS1.p2.1.m1.4.5.1.cmml" xref="S1.SS1.p2.1.m1.4.5.1"></eq><ci id="S1.SS1.p2.1.m1.4.5.2.cmml" xref="S1.SS1.p2.1.m1.4.5.2">𝔻</ci><set id="S1.SS1.p2.1.m1.4.5.3.1.cmml" xref="S1.SS1.p2.1.m1.4.5.3.2"><cn type="float" id="S1.SS1.p2.1.m1.1.1.cmml" xref="S1.SS1.p2.1.m1.1.1">0.5</cn><cn type="integer" id="S1.SS1.p2.1.m1.2.2.cmml" xref="S1.SS1.p2.1.m1.2.2">1</cn><cn type="integer" id="S1.SS1.p2.1.m1.3.3.cmml" xref="S1.SS1.p2.1.m1.3.3">2</cn><cn type="integer" id="S1.SS1.p2.1.m1.4.4.cmml" xref="S1.SS1.p2.1.m1.4.4">4</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p2.1.m1.4c">\mathbb{D}=\{0.5,1,2,4\}</annotation></semantics></math>, for AP computing, and average them over 10 categories to get the final mAP:</p>
<table id="S1.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.E3.m1.5" class="ltx_Math" alttext="{\rm mAP}=\frac{1}{|\mathbb{C}||\mathbb{D}|}\sum_{c\in\mathbb{C}}\sum_{d\in\mathbb{D}}{\rm AP}_{c,d}," display="block"><semantics id="S1.E3.m1.5a"><mrow id="S1.E3.m1.5.5.1" xref="S1.E3.m1.5.5.1.1.cmml"><mrow id="S1.E3.m1.5.5.1.1" xref="S1.E3.m1.5.5.1.1.cmml"><mi id="S1.E3.m1.5.5.1.1.2" xref="S1.E3.m1.5.5.1.1.2.cmml">mAP</mi><mo id="S1.E3.m1.5.5.1.1.1" xref="S1.E3.m1.5.5.1.1.1.cmml">=</mo><mrow id="S1.E3.m1.5.5.1.1.3" xref="S1.E3.m1.5.5.1.1.3.cmml"><mfrac id="S1.E3.m1.2.2" xref="S1.E3.m1.2.2.cmml"><mn id="S1.E3.m1.2.2.4" xref="S1.E3.m1.2.2.4.cmml">1</mn><mrow id="S1.E3.m1.2.2.2" xref="S1.E3.m1.2.2.2.cmml"><mrow id="S1.E3.m1.2.2.2.4.2" xref="S1.E3.m1.2.2.2.4.1.cmml"><mo stretchy="false" id="S1.E3.m1.2.2.2.4.2.1" xref="S1.E3.m1.2.2.2.4.1.1.cmml">|</mo><mi id="S1.E3.m1.1.1.1.1" xref="S1.E3.m1.1.1.1.1.cmml">ℂ</mi><mo stretchy="false" id="S1.E3.m1.2.2.2.4.2.2" xref="S1.E3.m1.2.2.2.4.1.1.cmml">|</mo></mrow><mo lspace="0em" rspace="0em" id="S1.E3.m1.2.2.2.3" xref="S1.E3.m1.2.2.2.3.cmml">​</mo><mrow id="S1.E3.m1.2.2.2.5.2" xref="S1.E3.m1.2.2.2.5.1.cmml"><mo stretchy="false" id="S1.E3.m1.2.2.2.5.2.1" xref="S1.E3.m1.2.2.2.5.1.1.cmml">|</mo><mi id="S1.E3.m1.2.2.2.2" xref="S1.E3.m1.2.2.2.2.cmml">𝔻</mi><mo stretchy="false" id="S1.E3.m1.2.2.2.5.2.2" xref="S1.E3.m1.2.2.2.5.1.1.cmml">|</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em" id="S1.E3.m1.5.5.1.1.3.1" xref="S1.E3.m1.5.5.1.1.3.1.cmml">​</mo><mrow id="S1.E3.m1.5.5.1.1.3.2" xref="S1.E3.m1.5.5.1.1.3.2.cmml"><munder id="S1.E3.m1.5.5.1.1.3.2.1" xref="S1.E3.m1.5.5.1.1.3.2.1.cmml"><mo movablelimits="false" rspace="0em" id="S1.E3.m1.5.5.1.1.3.2.1.2" xref="S1.E3.m1.5.5.1.1.3.2.1.2.cmml">∑</mo><mrow id="S1.E3.m1.5.5.1.1.3.2.1.3" xref="S1.E3.m1.5.5.1.1.3.2.1.3.cmml"><mi id="S1.E3.m1.5.5.1.1.3.2.1.3.2" xref="S1.E3.m1.5.5.1.1.3.2.1.3.2.cmml">c</mi><mo id="S1.E3.m1.5.5.1.1.3.2.1.3.1" xref="S1.E3.m1.5.5.1.1.3.2.1.3.1.cmml">∈</mo><mi id="S1.E3.m1.5.5.1.1.3.2.1.3.3" xref="S1.E3.m1.5.5.1.1.3.2.1.3.3.cmml">ℂ</mi></mrow></munder><mrow id="S1.E3.m1.5.5.1.1.3.2.2" xref="S1.E3.m1.5.5.1.1.3.2.2.cmml"><munder id="S1.E3.m1.5.5.1.1.3.2.2.1" xref="S1.E3.m1.5.5.1.1.3.2.2.1.cmml"><mo movablelimits="false" id="S1.E3.m1.5.5.1.1.3.2.2.1.2" xref="S1.E3.m1.5.5.1.1.3.2.2.1.2.cmml">∑</mo><mrow id="S1.E3.m1.5.5.1.1.3.2.2.1.3" xref="S1.E3.m1.5.5.1.1.3.2.2.1.3.cmml"><mi id="S1.E3.m1.5.5.1.1.3.2.2.1.3.2" xref="S1.E3.m1.5.5.1.1.3.2.2.1.3.2.cmml">d</mi><mo id="S1.E3.m1.5.5.1.1.3.2.2.1.3.1" xref="S1.E3.m1.5.5.1.1.3.2.2.1.3.1.cmml">∈</mo><mi id="S1.E3.m1.5.5.1.1.3.2.2.1.3.3" xref="S1.E3.m1.5.5.1.1.3.2.2.1.3.3.cmml">𝔻</mi></mrow></munder><msub id="S1.E3.m1.5.5.1.1.3.2.2.2" xref="S1.E3.m1.5.5.1.1.3.2.2.2.cmml"><mi id="S1.E3.m1.5.5.1.1.3.2.2.2.2" xref="S1.E3.m1.5.5.1.1.3.2.2.2.2.cmml">AP</mi><mrow id="S1.E3.m1.4.4.2.4" xref="S1.E3.m1.4.4.2.3.cmml"><mi id="S1.E3.m1.3.3.1.1" xref="S1.E3.m1.3.3.1.1.cmml">c</mi><mo id="S1.E3.m1.4.4.2.4.1" xref="S1.E3.m1.4.4.2.3.cmml">,</mo><mi id="S1.E3.m1.4.4.2.2" xref="S1.E3.m1.4.4.2.2.cmml">d</mi></mrow></msub></mrow></mrow></mrow></mrow><mo id="S1.E3.m1.5.5.1.2" xref="S1.E3.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.E3.m1.5b"><apply id="S1.E3.m1.5.5.1.1.cmml" xref="S1.E3.m1.5.5.1"><eq id="S1.E3.m1.5.5.1.1.1.cmml" xref="S1.E3.m1.5.5.1.1.1"></eq><ci id="S1.E3.m1.5.5.1.1.2.cmml" xref="S1.E3.m1.5.5.1.1.2">mAP</ci><apply id="S1.E3.m1.5.5.1.1.3.cmml" xref="S1.E3.m1.5.5.1.1.3"><times id="S1.E3.m1.5.5.1.1.3.1.cmml" xref="S1.E3.m1.5.5.1.1.3.1"></times><apply id="S1.E3.m1.2.2.cmml" xref="S1.E3.m1.2.2"><divide id="S1.E3.m1.2.2.3.cmml" xref="S1.E3.m1.2.2"></divide><cn type="integer" id="S1.E3.m1.2.2.4.cmml" xref="S1.E3.m1.2.2.4">1</cn><apply id="S1.E3.m1.2.2.2.cmml" xref="S1.E3.m1.2.2.2"><times id="S1.E3.m1.2.2.2.3.cmml" xref="S1.E3.m1.2.2.2.3"></times><apply id="S1.E3.m1.2.2.2.4.1.cmml" xref="S1.E3.m1.2.2.2.4.2"><abs id="S1.E3.m1.2.2.2.4.1.1.cmml" xref="S1.E3.m1.2.2.2.4.2.1"></abs><ci id="S1.E3.m1.1.1.1.1.cmml" xref="S1.E3.m1.1.1.1.1">ℂ</ci></apply><apply id="S1.E3.m1.2.2.2.5.1.cmml" xref="S1.E3.m1.2.2.2.5.2"><abs id="S1.E3.m1.2.2.2.5.1.1.cmml" xref="S1.E3.m1.2.2.2.5.2.1"></abs><ci id="S1.E3.m1.2.2.2.2.cmml" xref="S1.E3.m1.2.2.2.2">𝔻</ci></apply></apply></apply><apply id="S1.E3.m1.5.5.1.1.3.2.cmml" xref="S1.E3.m1.5.5.1.1.3.2"><apply id="S1.E3.m1.5.5.1.1.3.2.1.cmml" xref="S1.E3.m1.5.5.1.1.3.2.1"><csymbol cd="ambiguous" id="S1.E3.m1.5.5.1.1.3.2.1.1.cmml" xref="S1.E3.m1.5.5.1.1.3.2.1">subscript</csymbol><sum id="S1.E3.m1.5.5.1.1.3.2.1.2.cmml" xref="S1.E3.m1.5.5.1.1.3.2.1.2"></sum><apply id="S1.E3.m1.5.5.1.1.3.2.1.3.cmml" xref="S1.E3.m1.5.5.1.1.3.2.1.3"><in id="S1.E3.m1.5.5.1.1.3.2.1.3.1.cmml" xref="S1.E3.m1.5.5.1.1.3.2.1.3.1"></in><ci id="S1.E3.m1.5.5.1.1.3.2.1.3.2.cmml" xref="S1.E3.m1.5.5.1.1.3.2.1.3.2">𝑐</ci><ci id="S1.E3.m1.5.5.1.1.3.2.1.3.3.cmml" xref="S1.E3.m1.5.5.1.1.3.2.1.3.3">ℂ</ci></apply></apply><apply id="S1.E3.m1.5.5.1.1.3.2.2.cmml" xref="S1.E3.m1.5.5.1.1.3.2.2"><apply id="S1.E3.m1.5.5.1.1.3.2.2.1.cmml" xref="S1.E3.m1.5.5.1.1.3.2.2.1"><csymbol cd="ambiguous" id="S1.E3.m1.5.5.1.1.3.2.2.1.1.cmml" xref="S1.E3.m1.5.5.1.1.3.2.2.1">subscript</csymbol><sum id="S1.E3.m1.5.5.1.1.3.2.2.1.2.cmml" xref="S1.E3.m1.5.5.1.1.3.2.2.1.2"></sum><apply id="S1.E3.m1.5.5.1.1.3.2.2.1.3.cmml" xref="S1.E3.m1.5.5.1.1.3.2.2.1.3"><in id="S1.E3.m1.5.5.1.1.3.2.2.1.3.1.cmml" xref="S1.E3.m1.5.5.1.1.3.2.2.1.3.1"></in><ci id="S1.E3.m1.5.5.1.1.3.2.2.1.3.2.cmml" xref="S1.E3.m1.5.5.1.1.3.2.2.1.3.2">𝑑</ci><ci id="S1.E3.m1.5.5.1.1.3.2.2.1.3.3.cmml" xref="S1.E3.m1.5.5.1.1.3.2.2.1.3.3">𝔻</ci></apply></apply><apply id="S1.E3.m1.5.5.1.1.3.2.2.2.cmml" xref="S1.E3.m1.5.5.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S1.E3.m1.5.5.1.1.3.2.2.2.1.cmml" xref="S1.E3.m1.5.5.1.1.3.2.2.2">subscript</csymbol><ci id="S1.E3.m1.5.5.1.1.3.2.2.2.2.cmml" xref="S1.E3.m1.5.5.1.1.3.2.2.2.2">AP</ci><list id="S1.E3.m1.4.4.2.3.cmml" xref="S1.E3.m1.4.4.2.4"><ci id="S1.E3.m1.3.3.1.1.cmml" xref="S1.E3.m1.3.3.1.1">𝑐</ci><ci id="S1.E3.m1.4.4.2.2.cmml" xref="S1.E3.m1.4.4.2.2">𝑑</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.E3.m1.5c">{\rm mAP}=\frac{1}{|\mathbb{C}||\mathbb{D}|}\sum_{c\in\mathbb{C}}\sum_{d\in\mathbb{D}}{\rm AP}_{c,d},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S1.SS1.p2.3" class="ltx_p">where <math id="S1.SS1.p2.2.m1.1" class="ltx_Math" alttext="|\mathbb{C}|" display="inline"><semantics id="S1.SS1.p2.2.m1.1a"><mrow id="S1.SS1.p2.2.m1.1.2.2" xref="S1.SS1.p2.2.m1.1.2.1.cmml"><mo stretchy="false" id="S1.SS1.p2.2.m1.1.2.2.1" xref="S1.SS1.p2.2.m1.1.2.1.1.cmml">|</mo><mi id="S1.SS1.p2.2.m1.1.1" xref="S1.SS1.p2.2.m1.1.1.cmml">ℂ</mi><mo stretchy="false" id="S1.SS1.p2.2.m1.1.2.2.2" xref="S1.SS1.p2.2.m1.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p2.2.m1.1b"><apply id="S1.SS1.p2.2.m1.1.2.1.cmml" xref="S1.SS1.p2.2.m1.1.2.2"><abs id="S1.SS1.p2.2.m1.1.2.1.1.cmml" xref="S1.SS1.p2.2.m1.1.2.2.1"></abs><ci id="S1.SS1.p2.2.m1.1.1.cmml" xref="S1.SS1.p2.2.m1.1.1">ℂ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p2.2.m1.1c">|\mathbb{C}|</annotation></semantics></math> and <math id="S1.SS1.p2.3.m2.1" class="ltx_Math" alttext="|\mathbb{D}|" display="inline"><semantics id="S1.SS1.p2.3.m2.1a"><mrow id="S1.SS1.p2.3.m2.1.2.2" xref="S1.SS1.p2.3.m2.1.2.1.cmml"><mo stretchy="false" id="S1.SS1.p2.3.m2.1.2.2.1" xref="S1.SS1.p2.3.m2.1.2.1.1.cmml">|</mo><mi id="S1.SS1.p2.3.m2.1.1" xref="S1.SS1.p2.3.m2.1.1.cmml">𝔻</mi><mo stretchy="false" id="S1.SS1.p2.3.m2.1.2.2.2" xref="S1.SS1.p2.3.m2.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p2.3.m2.1b"><apply id="S1.SS1.p2.3.m2.1.2.1.cmml" xref="S1.SS1.p2.3.m2.1.2.2"><abs id="S1.SS1.p2.3.m2.1.2.1.1.cmml" xref="S1.SS1.p2.3.m2.1.2.2.1"></abs><ci id="S1.SS1.p2.3.m2.1.1.cmml" xref="S1.SS1.p2.3.m2.1.1">𝔻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p2.3.m2.1c">|\mathbb{D}|</annotation></semantics></math> are the category set and threshold set respectively.
Furthermore, five true positive metrics (TP metrics) are also considered, including Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE), and Average Attribute Error (AAE). For each TP metric, the mean TP metric (mTP metric) are computed by:</p>
<table id="S1.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.E4.m1.4" class="ltx_Math" alttext="{\rm mTP}_{k}=\frac{1}{|\mathbb{C}|}\sum_{c\in\mathbb{C}}\mathrm{TP}_{k,c}," display="block"><semantics id="S1.E4.m1.4a"><mrow id="S1.E4.m1.4.4.1" xref="S1.E4.m1.4.4.1.1.cmml"><mrow id="S1.E4.m1.4.4.1.1" xref="S1.E4.m1.4.4.1.1.cmml"><msub id="S1.E4.m1.4.4.1.1.2" xref="S1.E4.m1.4.4.1.1.2.cmml"><mi id="S1.E4.m1.4.4.1.1.2.2" xref="S1.E4.m1.4.4.1.1.2.2.cmml">mTP</mi><mi id="S1.E4.m1.4.4.1.1.2.3" xref="S1.E4.m1.4.4.1.1.2.3.cmml">k</mi></msub><mo id="S1.E4.m1.4.4.1.1.1" xref="S1.E4.m1.4.4.1.1.1.cmml">=</mo><mrow id="S1.E4.m1.4.4.1.1.3" xref="S1.E4.m1.4.4.1.1.3.cmml"><mfrac id="S1.E4.m1.1.1" xref="S1.E4.m1.1.1.cmml"><mn id="S1.E4.m1.1.1.3" xref="S1.E4.m1.1.1.3.cmml">1</mn><mrow id="S1.E4.m1.1.1.1.3" xref="S1.E4.m1.1.1.1.2.cmml"><mo stretchy="false" id="S1.E4.m1.1.1.1.3.1" xref="S1.E4.m1.1.1.1.2.1.cmml">|</mo><mi id="S1.E4.m1.1.1.1.1" xref="S1.E4.m1.1.1.1.1.cmml">ℂ</mi><mo stretchy="false" id="S1.E4.m1.1.1.1.3.2" xref="S1.E4.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S1.E4.m1.4.4.1.1.3.1" xref="S1.E4.m1.4.4.1.1.3.1.cmml">​</mo><mrow id="S1.E4.m1.4.4.1.1.3.2" xref="S1.E4.m1.4.4.1.1.3.2.cmml"><munder id="S1.E4.m1.4.4.1.1.3.2.1" xref="S1.E4.m1.4.4.1.1.3.2.1.cmml"><mo movablelimits="false" id="S1.E4.m1.4.4.1.1.3.2.1.2" xref="S1.E4.m1.4.4.1.1.3.2.1.2.cmml">∑</mo><mrow id="S1.E4.m1.4.4.1.1.3.2.1.3" xref="S1.E4.m1.4.4.1.1.3.2.1.3.cmml"><mi id="S1.E4.m1.4.4.1.1.3.2.1.3.2" xref="S1.E4.m1.4.4.1.1.3.2.1.3.2.cmml">c</mi><mo id="S1.E4.m1.4.4.1.1.3.2.1.3.1" xref="S1.E4.m1.4.4.1.1.3.2.1.3.1.cmml">∈</mo><mi id="S1.E4.m1.4.4.1.1.3.2.1.3.3" xref="S1.E4.m1.4.4.1.1.3.2.1.3.3.cmml">ℂ</mi></mrow></munder><msub id="S1.E4.m1.4.4.1.1.3.2.2" xref="S1.E4.m1.4.4.1.1.3.2.2.cmml"><mi id="S1.E4.m1.4.4.1.1.3.2.2.2" xref="S1.E4.m1.4.4.1.1.3.2.2.2.cmml">TP</mi><mrow id="S1.E4.m1.3.3.2.4" xref="S1.E4.m1.3.3.2.3.cmml"><mi id="S1.E4.m1.2.2.1.1" xref="S1.E4.m1.2.2.1.1.cmml">k</mi><mo id="S1.E4.m1.3.3.2.4.1" xref="S1.E4.m1.3.3.2.3.cmml">,</mo><mi id="S1.E4.m1.3.3.2.2" xref="S1.E4.m1.3.3.2.2.cmml">c</mi></mrow></msub></mrow></mrow></mrow><mo id="S1.E4.m1.4.4.1.2" xref="S1.E4.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.E4.m1.4b"><apply id="S1.E4.m1.4.4.1.1.cmml" xref="S1.E4.m1.4.4.1"><eq id="S1.E4.m1.4.4.1.1.1.cmml" xref="S1.E4.m1.4.4.1.1.1"></eq><apply id="S1.E4.m1.4.4.1.1.2.cmml" xref="S1.E4.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S1.E4.m1.4.4.1.1.2.1.cmml" xref="S1.E4.m1.4.4.1.1.2">subscript</csymbol><ci id="S1.E4.m1.4.4.1.1.2.2.cmml" xref="S1.E4.m1.4.4.1.1.2.2">mTP</ci><ci id="S1.E4.m1.4.4.1.1.2.3.cmml" xref="S1.E4.m1.4.4.1.1.2.3">𝑘</ci></apply><apply id="S1.E4.m1.4.4.1.1.3.cmml" xref="S1.E4.m1.4.4.1.1.3"><times id="S1.E4.m1.4.4.1.1.3.1.cmml" xref="S1.E4.m1.4.4.1.1.3.1"></times><apply id="S1.E4.m1.1.1.cmml" xref="S1.E4.m1.1.1"><divide id="S1.E4.m1.1.1.2.cmml" xref="S1.E4.m1.1.1"></divide><cn type="integer" id="S1.E4.m1.1.1.3.cmml" xref="S1.E4.m1.1.1.3">1</cn><apply id="S1.E4.m1.1.1.1.2.cmml" xref="S1.E4.m1.1.1.1.3"><abs id="S1.E4.m1.1.1.1.2.1.cmml" xref="S1.E4.m1.1.1.1.3.1"></abs><ci id="S1.E4.m1.1.1.1.1.cmml" xref="S1.E4.m1.1.1.1.1">ℂ</ci></apply></apply><apply id="S1.E4.m1.4.4.1.1.3.2.cmml" xref="S1.E4.m1.4.4.1.1.3.2"><apply id="S1.E4.m1.4.4.1.1.3.2.1.cmml" xref="S1.E4.m1.4.4.1.1.3.2.1"><csymbol cd="ambiguous" id="S1.E4.m1.4.4.1.1.3.2.1.1.cmml" xref="S1.E4.m1.4.4.1.1.3.2.1">subscript</csymbol><sum id="S1.E4.m1.4.4.1.1.3.2.1.2.cmml" xref="S1.E4.m1.4.4.1.1.3.2.1.2"></sum><apply id="S1.E4.m1.4.4.1.1.3.2.1.3.cmml" xref="S1.E4.m1.4.4.1.1.3.2.1.3"><in id="S1.E4.m1.4.4.1.1.3.2.1.3.1.cmml" xref="S1.E4.m1.4.4.1.1.3.2.1.3.1"></in><ci id="S1.E4.m1.4.4.1.1.3.2.1.3.2.cmml" xref="S1.E4.m1.4.4.1.1.3.2.1.3.2">𝑐</ci><ci id="S1.E4.m1.4.4.1.1.3.2.1.3.3.cmml" xref="S1.E4.m1.4.4.1.1.3.2.1.3.3">ℂ</ci></apply></apply><apply id="S1.E4.m1.4.4.1.1.3.2.2.cmml" xref="S1.E4.m1.4.4.1.1.3.2.2"><csymbol cd="ambiguous" id="S1.E4.m1.4.4.1.1.3.2.2.1.cmml" xref="S1.E4.m1.4.4.1.1.3.2.2">subscript</csymbol><ci id="S1.E4.m1.4.4.1.1.3.2.2.2.cmml" xref="S1.E4.m1.4.4.1.1.3.2.2.2">TP</ci><list id="S1.E4.m1.3.3.2.3.cmml" xref="S1.E4.m1.3.3.2.4"><ci id="S1.E4.m1.2.2.1.1.cmml" xref="S1.E4.m1.2.2.1.1">𝑘</ci><ci id="S1.E4.m1.3.3.2.2.cmml" xref="S1.E4.m1.3.3.2.2">𝑐</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.E4.m1.4c">{\rm mTP}_{k}=\frac{1}{|\mathbb{C}|}\sum_{c\in\mathbb{C}}\mathrm{TP}_{k,c},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S1.SS1.p2.5" class="ltx_p">where <math id="S1.SS1.p2.4.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S1.SS1.p2.4.m1.1a"><mi id="S1.SS1.p2.4.m1.1.1" xref="S1.SS1.p2.4.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.SS1.p2.4.m1.1b"><ci id="S1.SS1.p2.4.m1.1.1.cmml" xref="S1.SS1.p2.4.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p2.4.m1.1c">k</annotation></semantics></math> is the index of TP metrics, <em id="S1.SS1.p2.5.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.SS1.p2.5.2" class="ltx_text"></span> <math id="S1.SS1.p2.5.m2.1" class="ltx_Math" alttext="\mathrm{TP}_{1}" display="inline"><semantics id="S1.SS1.p2.5.m2.1a"><msub id="S1.SS1.p2.5.m2.1.1" xref="S1.SS1.p2.5.m2.1.1.cmml"><mi id="S1.SS1.p2.5.m2.1.1.2" xref="S1.SS1.p2.5.m2.1.1.2.cmml">TP</mi><mn id="S1.SS1.p2.5.m2.1.1.3" xref="S1.SS1.p2.5.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S1.SS1.p2.5.m2.1b"><apply id="S1.SS1.p2.5.m2.1.1.cmml" xref="S1.SS1.p2.5.m2.1.1"><csymbol cd="ambiguous" id="S1.SS1.p2.5.m2.1.1.1.cmml" xref="S1.SS1.p2.5.m2.1.1">subscript</csymbol><ci id="S1.SS1.p2.5.m2.1.1.2.cmml" xref="S1.SS1.p2.5.m2.1.1.2">TP</ci><cn type="integer" id="S1.SS1.p2.5.m2.1.1.3.cmml" xref="S1.SS1.p2.5.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p2.5.m2.1c">\mathrm{TP}_{1}</annotation></semantics></math> is ATE.
Finally, the nuScenes detection score (NDS) is computed by weighted averaging the above metrics:</p>
<table id="S1.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.E5.m1.3" class="ltx_Math" alttext="{\rm NDS}=\frac{1}{10}[5\cdot{\rm mAP}+\sum_{k=1}^{5}(1-\min(1,{\rm mTP}_{k}))]." display="block"><semantics id="S1.E5.m1.3a"><mrow id="S1.E5.m1.3.3.1" xref="S1.E5.m1.3.3.1.1.cmml"><mrow id="S1.E5.m1.3.3.1.1" xref="S1.E5.m1.3.3.1.1.cmml"><mi id="S1.E5.m1.3.3.1.1.3" xref="S1.E5.m1.3.3.1.1.3.cmml">NDS</mi><mo id="S1.E5.m1.3.3.1.1.2" xref="S1.E5.m1.3.3.1.1.2.cmml">=</mo><mrow id="S1.E5.m1.3.3.1.1.1" xref="S1.E5.m1.3.3.1.1.1.cmml"><mfrac id="S1.E5.m1.3.3.1.1.1.3" xref="S1.E5.m1.3.3.1.1.1.3.cmml"><mn id="S1.E5.m1.3.3.1.1.1.3.2" xref="S1.E5.m1.3.3.1.1.1.3.2.cmml">1</mn><mn id="S1.E5.m1.3.3.1.1.1.3.3" xref="S1.E5.m1.3.3.1.1.1.3.3.cmml">10</mn></mfrac><mo lspace="0em" rspace="0em" id="S1.E5.m1.3.3.1.1.1.2" xref="S1.E5.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S1.E5.m1.3.3.1.1.1.1.1" xref="S1.E5.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S1.E5.m1.3.3.1.1.1.1.1.2" xref="S1.E5.m1.3.3.1.1.1.1.2.1.cmml">[</mo><mrow id="S1.E5.m1.3.3.1.1.1.1.1.1" xref="S1.E5.m1.3.3.1.1.1.1.1.1.cmml"><mrow id="S1.E5.m1.3.3.1.1.1.1.1.1.3" xref="S1.E5.m1.3.3.1.1.1.1.1.1.3.cmml"><mn id="S1.E5.m1.3.3.1.1.1.1.1.1.3.2" xref="S1.E5.m1.3.3.1.1.1.1.1.1.3.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S1.E5.m1.3.3.1.1.1.1.1.1.3.1" xref="S1.E5.m1.3.3.1.1.1.1.1.1.3.1.cmml">⋅</mo><mi id="S1.E5.m1.3.3.1.1.1.1.1.1.3.3" xref="S1.E5.m1.3.3.1.1.1.1.1.1.3.3.cmml">mAP</mi></mrow><mo rspace="0.055em" id="S1.E5.m1.3.3.1.1.1.1.1.1.2" xref="S1.E5.m1.3.3.1.1.1.1.1.1.2.cmml">+</mo><mrow id="S1.E5.m1.3.3.1.1.1.1.1.1.1" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.cmml"><munderover id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.2" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.2" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.2.cmml">k</mi><mo id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.1" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.3" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mn id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.3" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.3.cmml">5</mn></munderover><mrow id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mn id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn><mo id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S1.E5.m1.1.1" xref="S1.E5.m1.1.1.cmml">min</mi><mo id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1a" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><mn id="S1.E5.m1.2.2" xref="S1.E5.m1.2.2.cmml">1</mn><mo id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">mTP</mi><mi id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S1.E5.m1.3.3.1.1.1.1.1.3" xref="S1.E5.m1.3.3.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo lspace="0em" id="S1.E5.m1.3.3.1.2" xref="S1.E5.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.E5.m1.3b"><apply id="S1.E5.m1.3.3.1.1.cmml" xref="S1.E5.m1.3.3.1"><eq id="S1.E5.m1.3.3.1.1.2.cmml" xref="S1.E5.m1.3.3.1.1.2"></eq><ci id="S1.E5.m1.3.3.1.1.3.cmml" xref="S1.E5.m1.3.3.1.1.3">NDS</ci><apply id="S1.E5.m1.3.3.1.1.1.cmml" xref="S1.E5.m1.3.3.1.1.1"><times id="S1.E5.m1.3.3.1.1.1.2.cmml" xref="S1.E5.m1.3.3.1.1.1.2"></times><apply id="S1.E5.m1.3.3.1.1.1.3.cmml" xref="S1.E5.m1.3.3.1.1.1.3"><divide id="S1.E5.m1.3.3.1.1.1.3.1.cmml" xref="S1.E5.m1.3.3.1.1.1.3"></divide><cn type="integer" id="S1.E5.m1.3.3.1.1.1.3.2.cmml" xref="S1.E5.m1.3.3.1.1.1.3.2">1</cn><cn type="integer" id="S1.E5.m1.3.3.1.1.1.3.3.cmml" xref="S1.E5.m1.3.3.1.1.1.3.3">10</cn></apply><apply id="S1.E5.m1.3.3.1.1.1.1.2.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S1.E5.m1.3.3.1.1.1.1.2.1.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S1.E5.m1.3.3.1.1.1.1.1.1.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1"><plus id="S1.E5.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.2"></plus><apply id="S1.E5.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.3"><ci id="S1.E5.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.3.1">⋅</ci><cn type="integer" id="S1.E5.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.3.2">5</cn><ci id="S1.E5.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.3.3">mAP</ci></apply><apply id="S1.E5.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1"><apply id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.1.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.2.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3"><eq id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.2">𝑘</ci><cn type="integer" id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><cn type="integer" id="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.2.3">5</cn></apply><apply id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1"><minus id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.2"></minus><cn type="integer" id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.3">1</cn><apply id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"><min id="S1.E5.m1.1.1.cmml" xref="S1.E5.m1.1.1"></min><cn type="integer" id="S1.E5.m1.2.2.cmml" xref="S1.E5.m1.2.2">1</cn><apply id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">mTP</ci><ci id="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S1.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑘</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.E5.m1.3c">{\rm NDS}=\frac{1}{10}[5\cdot{\rm mAP}+\sum_{k=1}^{5}(1-\min(1,{\rm mTP}_{k}))].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S1.SS1.p3" class="ltx_para ltx_noindent">
<p id="S1.SS1.p3.1" class="ltx_p"><span id="S1.SS1.p3.1.1" class="ltx_text ltx_font_bold">ScanNet V2.</span> ScanNet V2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> is a richly annotated dataset of 3D reconstructed of indoor scenes, and 3D box annotations can be derived from the annotated meshes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>. There are about 1.2K training samples with 18 object different categories for hundreds of rooms. In this dataset, we adopt the 3D IoU based mAP as metric, and 0.25 and 0.5 IoU thresholds are considered in model evaluation and error analysis.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Average Height Error</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">In the main paper, Table <a href="#S6.T8" title="Table 8 ‣ 6.3 TIDE3D Analysis ‣ 6 Experiments and Analysis ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows that different evaluation metrics have their preferences for models. Specifically, on the nuScenes dataset, FCOS3D and BEVDet show similar performances under the nuScenes metric, but there is a large gap under the KITTI metric.
To further explore this phenomenon, we define Average Height Error (AHE) as the 1D Euclidean distance of the center height of the bounding box, in order to bridge the gap between 2D IoU and 3D IoU.
Table <a href="#S1.T9" title="Table 9 ‣ A.2 Average Height Error ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows that the predicted center height of BEVDet is significantly worse than that of FCOS3D.
It reflects that the nuScenes metric is more friendly to BEV-based methods.</p>
</div>
<figure id="S1.T9" class="ltx_table">
<div id="S1.T9.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:77.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(33.1pt,-11.9pt) scale(1.43953328227865,1.43953328227865) ;">
<table id="S1.T9.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T9.1.1.1.1" class="ltx_tr">
<th id="S1.T9.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;"></th>
<th id="S1.T9.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:7.0pt;padding-right:7.0pt;">mATE</th>
<th id="S1.T9.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:7.0pt;padding-right:7.0pt;">mAHE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T9.1.1.2.1" class="ltx_tr">
<th id="S1.T9.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">FCOS3D</th>
<td id="S1.T9.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">0.78</td>
<td id="S1.T9.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">0.11</td>
</tr>
<tr id="S1.T9.1.1.3.2" class="ltx_tr">
<th id="S1.T9.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:7.0pt;padding-right:7.0pt;">BEVDet</th>
<td id="S1.T9.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.72</td>
<td id="S1.T9.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.15</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Average Translation Error (ATE) and Average Height Error (AHE) of BEVDet and FCOS3D for all categories on nuScenes <em id="S1.T9.3.1" class="ltx_emph ltx_font_italic">validation</em> set.</figcaption>
</figure>
<figure id="S1.F4" class="ltx_figure"><img src="/html/2310.05447/assets/x3.png" id="S1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="276" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Learning curves on KITTI-3D without smoothing. </figcaption>
</figure>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Implementation of TIDE3D</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.6" class="ltx_p">In the main paper, we introduce the design principles of TIDE3D, and there are some minor implementation differences on different datasets.
Specifically, in KITTI-3D, TIDE3D conduct error diagnosis for three categories with three different settings separately. We directly compute the 3D IoU of detections and ground truths in the 3D space, instead of computing the BEV IoU and multiplying it by the 1D IoU at the height dimension. This design allows us to get more accurate 3D IoU for the objects whose roll angles and pitch angles are not zero (the roll angles and pitch angles of objects are always zero in KITTI-3D, but not necessarily for other datasets, such as nuScenes). In ScanNet V2, the error distribution is collected over all categories, <em id="S1.SS3.p1.6.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.SS3.p1.6.2" class="ltx_text"></span> based on mAP. For nuScene, we compute both <math id="S1.SS3.p1.1.m1.1" class="ltx_Math" alttext="\Delta\rm{mAP}" display="inline"><semantics id="S1.SS3.p1.1.m1.1a"><mrow id="S1.SS3.p1.1.m1.1.1" xref="S1.SS3.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S1.SS3.p1.1.m1.1.1.2" xref="S1.SS3.p1.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S1.SS3.p1.1.m1.1.1.1" xref="S1.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S1.SS3.p1.1.m1.1.1.3" xref="S1.SS3.p1.1.m1.1.1.3.cmml">mAP</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.SS3.p1.1.m1.1b"><apply id="S1.SS3.p1.1.m1.1.1.cmml" xref="S1.SS3.p1.1.m1.1.1"><times id="S1.SS3.p1.1.m1.1.1.1.cmml" xref="S1.SS3.p1.1.m1.1.1.1"></times><ci id="S1.SS3.p1.1.m1.1.1.2.cmml" xref="S1.SS3.p1.1.m1.1.1.2">Δ</ci><ci id="S1.SS3.p1.1.m1.1.1.3.cmml" xref="S1.SS3.p1.1.m1.1.1.3">mAP</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS3.p1.1.m1.1c">\Delta\rm{mAP}</annotation></semantics></math> and <math id="S1.SS3.p1.2.m2.1" class="ltx_Math" alttext="\Delta\rm{NDS}" display="inline"><semantics id="S1.SS3.p1.2.m2.1a"><mrow id="S1.SS3.p1.2.m2.1.1" xref="S1.SS3.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S1.SS3.p1.2.m2.1.1.2" xref="S1.SS3.p1.2.m2.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S1.SS3.p1.2.m2.1.1.1" xref="S1.SS3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S1.SS3.p1.2.m2.1.1.3" xref="S1.SS3.p1.2.m2.1.1.3.cmml">NDS</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.SS3.p1.2.m2.1b"><apply id="S1.SS3.p1.2.m2.1.1.cmml" xref="S1.SS3.p1.2.m2.1.1"><times id="S1.SS3.p1.2.m2.1.1.1.cmml" xref="S1.SS3.p1.2.m2.1.1.1"></times><ci id="S1.SS3.p1.2.m2.1.1.2.cmml" xref="S1.SS3.p1.2.m2.1.1.2">Δ</ci><ci id="S1.SS3.p1.2.m2.1.1.3.cmml" xref="S1.SS3.p1.2.m2.1.1.3">NDS</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS3.p1.2.m2.1c">\Delta\rm{NDS}</annotation></semantics></math> for error diagnosis (we mainly focus on <math id="S1.SS3.p1.3.m3.1" class="ltx_Math" alttext="\Delta\rm{mAP}" display="inline"><semantics id="S1.SS3.p1.3.m3.1a"><mrow id="S1.SS3.p1.3.m3.1.1" xref="S1.SS3.p1.3.m3.1.1.cmml"><mi mathvariant="normal" id="S1.SS3.p1.3.m3.1.1.2" xref="S1.SS3.p1.3.m3.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S1.SS3.p1.3.m3.1.1.1" xref="S1.SS3.p1.3.m3.1.1.1.cmml">​</mo><mi id="S1.SS3.p1.3.m3.1.1.3" xref="S1.SS3.p1.3.m3.1.1.3.cmml">mAP</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.SS3.p1.3.m3.1b"><apply id="S1.SS3.p1.3.m3.1.1.cmml" xref="S1.SS3.p1.3.m3.1.1"><times id="S1.SS3.p1.3.m3.1.1.1.cmml" xref="S1.SS3.p1.3.m3.1.1.1"></times><ci id="S1.SS3.p1.3.m3.1.1.2.cmml" xref="S1.SS3.p1.3.m3.1.1.2">Δ</ci><ci id="S1.SS3.p1.3.m3.1.1.3.cmml" xref="S1.SS3.p1.3.m3.1.1.3">mAP</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS3.p1.3.m3.1c">\Delta\rm{mAP}</annotation></semantics></math>).
Besides, there is a hyper-parameter <math id="S1.SS3.p1.4.m4.1" class="ltx_Math" alttext="t_{f}" display="inline"><semantics id="S1.SS3.p1.4.m4.1a"><msub id="S1.SS3.p1.4.m4.1.1" xref="S1.SS3.p1.4.m4.1.1.cmml"><mi id="S1.SS3.p1.4.m4.1.1.2" xref="S1.SS3.p1.4.m4.1.1.2.cmml">t</mi><mi id="S1.SS3.p1.4.m4.1.1.3" xref="S1.SS3.p1.4.m4.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S1.SS3.p1.4.m4.1b"><apply id="S1.SS3.p1.4.m4.1.1.cmml" xref="S1.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S1.SS3.p1.4.m4.1.1.1.cmml" xref="S1.SS3.p1.4.m4.1.1">subscript</csymbol><ci id="S1.SS3.p1.4.m4.1.1.2.cmml" xref="S1.SS3.p1.4.m4.1.1.2">𝑡</ci><ci id="S1.SS3.p1.4.m4.1.1.3.cmml" xref="S1.SS3.p1.4.m4.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS3.p1.4.m4.1c">t_{f}</annotation></semantics></math> in TIDE3D, which is used to identify whether a false positive belongs to ’localization error’ or ’background error’. We set <math id="S1.SS3.p1.5.m5.1" class="ltx_Math" alttext="t_{f}=0.1" display="inline"><semantics id="S1.SS3.p1.5.m5.1a"><mrow id="S1.SS3.p1.5.m5.1.1" xref="S1.SS3.p1.5.m5.1.1.cmml"><msub id="S1.SS3.p1.5.m5.1.1.2" xref="S1.SS3.p1.5.m5.1.1.2.cmml"><mi id="S1.SS3.p1.5.m5.1.1.2.2" xref="S1.SS3.p1.5.m5.1.1.2.2.cmml">t</mi><mi id="S1.SS3.p1.5.m5.1.1.2.3" xref="S1.SS3.p1.5.m5.1.1.2.3.cmml">f</mi></msub><mo id="S1.SS3.p1.5.m5.1.1.1" xref="S1.SS3.p1.5.m5.1.1.1.cmml">=</mo><mn id="S1.SS3.p1.5.m5.1.1.3" xref="S1.SS3.p1.5.m5.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.SS3.p1.5.m5.1b"><apply id="S1.SS3.p1.5.m5.1.1.cmml" xref="S1.SS3.p1.5.m5.1.1"><eq id="S1.SS3.p1.5.m5.1.1.1.cmml" xref="S1.SS3.p1.5.m5.1.1.1"></eq><apply id="S1.SS3.p1.5.m5.1.1.2.cmml" xref="S1.SS3.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S1.SS3.p1.5.m5.1.1.2.1.cmml" xref="S1.SS3.p1.5.m5.1.1.2">subscript</csymbol><ci id="S1.SS3.p1.5.m5.1.1.2.2.cmml" xref="S1.SS3.p1.5.m5.1.1.2.2">𝑡</ci><ci id="S1.SS3.p1.5.m5.1.1.2.3.cmml" xref="S1.SS3.p1.5.m5.1.1.2.3">𝑓</ci></apply><cn type="float" id="S1.SS3.p1.5.m5.1.1.3.cmml" xref="S1.SS3.p1.5.m5.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS3.p1.5.m5.1c">t_{f}=0.1</annotation></semantics></math> for 3D IoU based metrics and <math id="S1.SS3.p1.6.m6.1" class="ltx_Math" alttext="t_{f}=5" display="inline"><semantics id="S1.SS3.p1.6.m6.1a"><mrow id="S1.SS3.p1.6.m6.1.1" xref="S1.SS3.p1.6.m6.1.1.cmml"><msub id="S1.SS3.p1.6.m6.1.1.2" xref="S1.SS3.p1.6.m6.1.1.2.cmml"><mi id="S1.SS3.p1.6.m6.1.1.2.2" xref="S1.SS3.p1.6.m6.1.1.2.2.cmml">t</mi><mi id="S1.SS3.p1.6.m6.1.1.2.3" xref="S1.SS3.p1.6.m6.1.1.2.3.cmml">f</mi></msub><mo id="S1.SS3.p1.6.m6.1.1.1" xref="S1.SS3.p1.6.m6.1.1.1.cmml">=</mo><mn id="S1.SS3.p1.6.m6.1.1.3" xref="S1.SS3.p1.6.m6.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.SS3.p1.6.m6.1b"><apply id="S1.SS3.p1.6.m6.1.1.cmml" xref="S1.SS3.p1.6.m6.1.1"><eq id="S1.SS3.p1.6.m6.1.1.1.cmml" xref="S1.SS3.p1.6.m6.1.1.1"></eq><apply id="S1.SS3.p1.6.m6.1.1.2.cmml" xref="S1.SS3.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S1.SS3.p1.6.m6.1.1.2.1.cmml" xref="S1.SS3.p1.6.m6.1.1.2">subscript</csymbol><ci id="S1.SS3.p1.6.m6.1.1.2.2.cmml" xref="S1.SS3.p1.6.m6.1.1.2.2">𝑡</ci><ci id="S1.SS3.p1.6.m6.1.1.2.3.cmml" xref="S1.SS3.p1.6.m6.1.1.2.3">𝑓</ci></apply><cn type="integer" id="S1.SS3.p1.6.m6.1.1.3.cmml" xref="S1.SS3.p1.6.m6.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS3.p1.6.m6.1c">t_{f}=5</annotation></semantics></math> for center-distance based metrics.</p>
</div>
<figure id="S1.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05447/assets/figures/TIDE/kitti_ped_iou0.5_mod.png" id="S1.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="329" height="177" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Pedestrian</figcaption>
<figure id="S1.F5.sf2" class="ltx_figure"><img src="/html/2310.05447/assets/figures/TIDE/kitti_cyc_iou0.5_mod.png" id="S1.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="329" height="178" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Cyclist</figcaption>
</figure>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S1.F5.sf1.3.1" class="ltx_text ltx_font_bold">Example error diagnosis results.</span>
We show the results of various models for the Pedestrian category and Cyclist category on KITTI-3D <em id="S1.F5.sf1.4.2" class="ltx_emph ltx_font_italic">validation</em> set under the moderate setting with 0.5 IoU threshold.
Specifically, we select CaDDn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite> (image-based), PV-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite> (LiDAR-based) and MVX-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite> (fusion-based) to explore the bottleneck of detectors with different modalities.
</figcaption>
<br class="ltx_break ltx_break">
<p id="S1.F5.sf1.5" class="ltx_p ltx_figure_panel ltx_align_center"><span class="ltx_rule" style="color:#000000;background:#000000;display:inline-block;"> </span></p>
</figure>
</section>
<section id="S1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Training Curve</h3>

<div id="S1.SS4.p1" class="ltx_para">
<p id="S1.SS4.p1.2" class="ltx_p">Figure <a href="#S1.F4" title="Figure 4 ‣ A.2 Average Height Error ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the un-smoothed learning curve of GUPNet in KITTI-3D (Figure <a href="#S5.F1" title="Figure 1 ‣ 5 Approach ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, right), and we can see that the performance fluctuates violently during training. Even in the later stage of training, the <math id="S1.SS4.p1.1.m1.1" class="ltx_Math" alttext="\rm{AP}_{40}" display="inline"><semantics id="S1.SS4.p1.1.m1.1a"><msub id="S1.SS4.p1.1.m1.1.1" xref="S1.SS4.p1.1.m1.1.1.cmml"><mi id="S1.SS4.p1.1.m1.1.1.2" xref="S1.SS4.p1.1.m1.1.1.2.cmml">AP</mi><mn id="S1.SS4.p1.1.m1.1.1.3" xref="S1.SS4.p1.1.m1.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S1.SS4.p1.1.m1.1b"><apply id="S1.SS4.p1.1.m1.1.1.cmml" xref="S1.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.SS4.p1.1.m1.1.1.1.cmml" xref="S1.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S1.SS4.p1.1.m1.1.1.2.cmml" xref="S1.SS4.p1.1.m1.1.1.2">AP</ci><cn type="integer" id="S1.SS4.p1.1.m1.1.1.3.cmml" xref="S1.SS4.p1.1.m1.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS4.p1.1.m1.1c">\rm{AP}_{40}</annotation></semantics></math> is still unstable (especially for the <math id="S1.SS4.p1.2.m2.1" class="ltx_math_unparsed" alttext="1\times" display="inline"><semantics id="S1.SS4.p1.2.m2.1a"><mrow id="S1.SS4.p1.2.m2.1b"><mn id="S1.SS4.p1.2.m2.1.1">1</mn><mo lspace="0.222em" id="S1.SS4.p1.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S1.SS4.p1.2.m2.1c">1\times</annotation></semantics></math> schedule). Meanwhile, we also observe the final performance fluctuates over multiple runs (similar to Table 20 in DEVIANT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>), and reporting mean values of multiple runs is recommended for future work.</p>
</div>
</section>
<section id="S1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>More TIDE3D Analysis</h3>

<div id="S1.SS5.p1" class="ltx_para ltx_noindent">
<p id="S1.SS5.p1.1" class="ltx_p"><span id="S1.SS5.p1.1.1" class="ltx_text ltx_font_bold">Pedestrian and Cyclist.</span>
Here we show the error diagnosis result and discussion for the Pedestrian and Cyclist categories in Figure <a href="#S1.F5.sf1" title="Figure 5a ‣ A.3 Implementation of TIDE3D ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5a</span></a>. We can find that the major issue of the image-based model (CaDDn) is still the inaccurate localization. For the LiDAR-based model (PV-RCNN), the localization error is relatively small and mainly caused by the inaccurate estimation of dimension (instead of location). This is probably because LiDAR provides accurate location information but can only capture the surface of the objects. Besides, different from image-based methods, we can find that background error accounts for a large part of the whole error distribution. This is because LiDAR points lack color information, and the models base on them easily generate false positives when they meet objects with similar structures to the objects of interest. See Figure <a href="#S1.F6" title="Figure 6 ‣ A.5 More TIDE3D Analysis ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> for a typical detection result.</p>
</div>
<figure id="S1.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2310.05447/assets/figures/errorcase_2d.png" id="S1.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="592" height="179" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2310.05447/assets/figures/errorcase_3d.png" id="S1.F6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="592" height="373" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>An typical error case for LiDAR-based methods. A tree trunk is wrongly recognized as a cyclist due to the lack of color information. Cars, cyclists, and pedestrians are visualized with yellow, purple, and green boxes. Figures are copied from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>.</figcaption>
</figure>
<div id="S1.SS5.p2" class="ltx_para ltx_noindent">
<p id="S1.SS5.p2.1" class="ltx_p"><span id="S1.SS5.p2.1.1" class="ltx_text ltx_font_bold">Error diagnosis in ScanNet V2.</span>
In addition to autonomous driving scenario, TIDE3D can also be used in indoor scenes. Here we use ScanNet v2 and VoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>, which is a powerful detection model based on point cloud, as an example.
Figure <a href="#S1.F7" title="Figure 7 ‣ A.5 More TIDE3D Analysis ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that VoteNet has different error distributions under different IoU thresholds.
Specifically, under a strict IoU threshold of 0.5, the model error is dominated by localization error which is caused by the inaccurate estimation of center and dimension (note the bounding box annotation in ScanNet V2 is axis-aligned and the model does not need to predict the rotation angle).
For a loose threshold of 0.25, the localization error is mitigated and the overall performance is limited by more factors, including ranking error, missing error, or background error.</p>
</div>
<div id="S1.SS5.p3" class="ltx_para">
<p id="S1.SS5.p3.1" class="ltx_p">In order to analyze the diverse problems encountered in detecting different categories of objects, we can further analyze the AP of a specified single category.
As an example, we look at the <span id="S1.SS5.p3.1.1" class="ltx_text ltx_font_bold">‘picture’</span> which is the worst performing category as shown in Figure <a href="#S1.F8" title="Figure 8 ‣ A.5 More TIDE3D Analysis ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.
Since the pictures usually have small sizes and easily blend into the background point cloud of the wall, they are quite unrecognizable especially when there is no color information provided by the image data.
Besides, incomplete annotations also confuse the detector’s recognition of the pictures. Therefore, the main problems besides ranking error are missing error and background error for this category.</p>
</div>
<figure id="S1.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05447/assets/figures/TIDE/VoteNet_iou0.5_wholeclass.png" id="S1.F7.sf1.g1" class="ltx_graphics ltx_img_portrait" width="145" height="232" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05447/assets/figures/TIDE/VoteNet_iou0.25_wholeclass.png" id="S1.F7.sf2.g1" class="ltx_graphics ltx_img_portrait" width="145" height="232" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="S1.F7.4.1" class="ltx_text ltx_font_bold">Example error diagnosis results of VoteNet on ScanNet V2 <em id="S1.F7.4.1.1" class="ltx_emph ltx_font_italic">validation</em> set.</span>
We show the results with 0.5 IoU threshold (<em id="S1.F7.5.2" class="ltx_emph ltx_font_italic">left</em>) and 0.25 IoU threshold (<em id="S1.F7.6.3" class="ltx_emph ltx_font_italic">right</em>) for all categories.</figcaption>
</figure>
<figure id="S1.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05447/assets/figures/TIDE/scannet_picture_example2.png" id="S1.F8.sf1.g1" class="ltx_graphics ltx_img_portrait" width="145" height="232" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05447/assets/figures/TIDE/VoteNet_iou0.25_picture.png" id="S1.F8.sf2.g1" class="ltx_graphics ltx_img_portrait" width="145" height="232" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>
<span id="S1.F8.2.1" class="ltx_text ltx_font_bold">Error diagnosis of VoteNet for ‘picture’ category.</span> The error distribution is collected with the IoU threshold of 0.25 on ScanNet V2. A training sample with several ‘picture’ items is provided to show the low quality of annotation, which is a major reason leading to the high background and missing errors.
We use blue boxes to denote the ground truth items. Best viewed in color with zoom-in.</figcaption>
</figure>
</section>
<section id="S1.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Discussion about Training Recipes</h3>

<div id="S1.SS6.p1" class="ltx_para">
<p id="S1.SS6.p1.1" class="ltx_p">We observe the common choices of training recipes show varying impacts on different models. Here we give some notes on model training:</p>
</div>
<div id="S1.SS6.p2" class="ltx_para ltx_noindent">
<p id="S1.SS6.p2.2" class="ltx_p"><span id="S1.SS6.p2.2.1" class="ltx_text ltx_font_bold">i.</span> The optimal training recipes are different for different models.
For example, the detection models with deformable convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">96</span></a>]</cite>, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">92</span></a>]</cite>, require a lower learning rate (<em id="S1.SS6.p2.2.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.SS6.p2.2.3" class="ltx_text"></span> <math id="S1.SS6.p2.1.m1.1" class="ltx_Math" alttext="1.25e^{-4}" display="inline"><semantics id="S1.SS6.p2.1.m1.1a"><mrow id="S1.SS6.p2.1.m1.1.1" xref="S1.SS6.p2.1.m1.1.1.cmml"><mn id="S1.SS6.p2.1.m1.1.1.2" xref="S1.SS6.p2.1.m1.1.1.2.cmml">1.25</mn><mo lspace="0em" rspace="0em" id="S1.SS6.p2.1.m1.1.1.1" xref="S1.SS6.p2.1.m1.1.1.1.cmml">​</mo><msup id="S1.SS6.p2.1.m1.1.1.3" xref="S1.SS6.p2.1.m1.1.1.3.cmml"><mi id="S1.SS6.p2.1.m1.1.1.3.2" xref="S1.SS6.p2.1.m1.1.1.3.2.cmml">e</mi><mrow id="S1.SS6.p2.1.m1.1.1.3.3" xref="S1.SS6.p2.1.m1.1.1.3.3.cmml"><mo id="S1.SS6.p2.1.m1.1.1.3.3a" xref="S1.SS6.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S1.SS6.p2.1.m1.1.1.3.3.2" xref="S1.SS6.p2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.SS6.p2.1.m1.1b"><apply id="S1.SS6.p2.1.m1.1.1.cmml" xref="S1.SS6.p2.1.m1.1.1"><times id="S1.SS6.p2.1.m1.1.1.1.cmml" xref="S1.SS6.p2.1.m1.1.1.1"></times><cn type="float" id="S1.SS6.p2.1.m1.1.1.2.cmml" xref="S1.SS6.p2.1.m1.1.1.2">1.25</cn><apply id="S1.SS6.p2.1.m1.1.1.3.cmml" xref="S1.SS6.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S1.SS6.p2.1.m1.1.1.3.1.cmml" xref="S1.SS6.p2.1.m1.1.1.3">superscript</csymbol><ci id="S1.SS6.p2.1.m1.1.1.3.2.cmml" xref="S1.SS6.p2.1.m1.1.1.3.2">𝑒</ci><apply id="S1.SS6.p2.1.m1.1.1.3.3.cmml" xref="S1.SS6.p2.1.m1.1.1.3.3"><minus id="S1.SS6.p2.1.m1.1.1.3.3.1.cmml" xref="S1.SS6.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S1.SS6.p2.1.m1.1.1.3.3.2.cmml" xref="S1.SS6.p2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS6.p2.1.m1.1c">1.25e^{-4}</annotation></semantics></math>) for stable training, while other models with similar network architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> perform better at a larger one (<em id="S1.SS6.p2.2.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.SS6.p2.2.5" class="ltx_text"></span> <math id="S1.SS6.p2.2.m2.1" class="ltx_Math" alttext="1.25e^{-3}" display="inline"><semantics id="S1.SS6.p2.2.m2.1a"><mrow id="S1.SS6.p2.2.m2.1.1" xref="S1.SS6.p2.2.m2.1.1.cmml"><mn id="S1.SS6.p2.2.m2.1.1.2" xref="S1.SS6.p2.2.m2.1.1.2.cmml">1.25</mn><mo lspace="0em" rspace="0em" id="S1.SS6.p2.2.m2.1.1.1" xref="S1.SS6.p2.2.m2.1.1.1.cmml">​</mo><msup id="S1.SS6.p2.2.m2.1.1.3" xref="S1.SS6.p2.2.m2.1.1.3.cmml"><mi id="S1.SS6.p2.2.m2.1.1.3.2" xref="S1.SS6.p2.2.m2.1.1.3.2.cmml">e</mi><mrow id="S1.SS6.p2.2.m2.1.1.3.3" xref="S1.SS6.p2.2.m2.1.1.3.3.cmml"><mo id="S1.SS6.p2.2.m2.1.1.3.3a" xref="S1.SS6.p2.2.m2.1.1.3.3.cmml">−</mo><mn id="S1.SS6.p2.2.m2.1.1.3.3.2" xref="S1.SS6.p2.2.m2.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.SS6.p2.2.m2.1b"><apply id="S1.SS6.p2.2.m2.1.1.cmml" xref="S1.SS6.p2.2.m2.1.1"><times id="S1.SS6.p2.2.m2.1.1.1.cmml" xref="S1.SS6.p2.2.m2.1.1.1"></times><cn type="float" id="S1.SS6.p2.2.m2.1.1.2.cmml" xref="S1.SS6.p2.2.m2.1.1.2">1.25</cn><apply id="S1.SS6.p2.2.m2.1.1.3.cmml" xref="S1.SS6.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S1.SS6.p2.2.m2.1.1.3.1.cmml" xref="S1.SS6.p2.2.m2.1.1.3">superscript</csymbol><ci id="S1.SS6.p2.2.m2.1.1.3.2.cmml" xref="S1.SS6.p2.2.m2.1.1.3.2">𝑒</ci><apply id="S1.SS6.p2.2.m2.1.1.3.3.cmml" xref="S1.SS6.p2.2.m2.1.1.3.3"><minus id="S1.SS6.p2.2.m2.1.1.3.3.1.cmml" xref="S1.SS6.p2.2.m2.1.1.3.3"></minus><cn type="integer" id="S1.SS6.p2.2.m2.1.1.3.3.2.cmml" xref="S1.SS6.p2.2.m2.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS6.p2.2.m2.1c">1.25e^{-3}</annotation></semantics></math>).</p>
</div>
<div id="S1.SS6.p3" class="ltx_para ltx_noindent">
<p id="S1.SS6.p3.1" class="ltx_p"><span id="S1.SS6.p3.1.1" class="ltx_text ltx_font_bold">ii.</span> Some data augmentations may have similar effects. For example, applying random crop (at a small-scale) and random shift separately can improve the accuracy, while applying both of them still get similar performance. This suggests that these two operations may work in a similar way, <em id="S1.SS6.p3.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.SS6.p3.1.3" class="ltx_text"></span> applying geometric transformations to the images, thus making the models sensitive to the location of objects.</p>
</div>
<div id="S1.SS6.p4" class="ltx_para ltx_noindent">
<p id="S1.SS6.p4.1" class="ltx_p"><span id="S1.SS6.p4.1.1" class="ltx_text ltx_font_bold">iii.</span> For different datasets/metrics, the same augmentation may have different effects. For example, random crop works well in KITTI-3D dataset, while the performance change of applying this operation on nuScene is relatively inconspicuous. This is mainly caused by the difference in data size and evaluation metrics, and the custom training recipes for different application scenarios are required.</p>
</div>
<div id="S1.SS6.p5" class="ltx_para ltx_noindent">
<p id="S1.SS6.p5.1" class="ltx_p"><span id="S1.SS6.p5.1.1" class="ltx_text ltx_font_bold">iv.</span> Data augmentation is an effective way to improve the performance of detection models, even in large-scale datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">75</span></a>]</cite>. However, it is difficult to align geometric changes in the 2D image plane and the 3D world space, resulting in limited augmentation strategies. The popular BEV pipeline allow us to conduct data augmentation in the BEV space, which is a key factor for the success of such methods. Meanwhile, this also indicates that the algorithm performance can be improved by designing data augmentation strategies in the future.</p>
</div>
</section>
<section id="S1.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.7 </span>Ranking Error in TIDE3D</h3>

<div id="S1.SS7.p1" class="ltx_para">
<p id="S1.SS7.p1.1" class="ltx_p">We have shown that the ranking error, which is ignored in TIDE, is a major error type in 3D object detection systems. Different from other error types, ranking error involves multiple predictions, and fixing this type may affect other error types. For example, in Section <a href="#S6.SS3" title="6.3 TIDE3D Analysis ‣ 6 Experiments and Analysis ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>, we show the effects of 3D confidence with TIDE3D and find that both the localization error and ranking error are significantly reduced (similar phenomenon is shown in Table 2 of TIDE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>), which is caused by the complicated interactions between multiple predictions. We argue that, although improving the quality of localization and confidence may show similar numbers in localization error, they achieve this in a completely different way and can be further identified. Specifically, we present the Precision-Recall (PR) curves before/after applying localization and ranking oracles in Figure <a href="#S1.F9" title="Figure 9 ‣ A.7 Ranking Error in TIDE3D ‣ A Appendix ‣ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. We can find that the ranking (misalignment) oracle improves the AP by maximizing the precision at each recall level, while the localization oracle corrects the false positive to true positive. All the figures we presented will be given from TIDE3D.</p>
</div>
<figure id="S1.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F9.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05447/assets/figures/pr-rank2.png" id="S1.F9.sf1.g1" class="ltx_graphics ltx_img_landscape" width="349" height="262" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F9.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05447/assets/figures/pr-loc2.png" id="S1.F9.sf2.g1" class="ltx_graphics ltx_img_landscape" width="349" height="262" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="S1.F9.7.1" class="ltx_text ltx_font_bold">PR curves.</span>
We show the PR curves before/after applying the ranking oracle (<span id="S1.F9.8.2" class="ltx_text ltx_font_bold">left</span>) and the localization oracle (<span id="S1.F9.9.3" class="ltx_text ltx_font_bold">right</span>). The baseline is GUPNet and the metric is <math id="S1.F9.2.m1.1" class="ltx_Math" alttext="\rm{AP}_{40}" display="inline"><semantics id="S1.F9.2.m1.1b"><msub id="S1.F9.2.m1.1.1" xref="S1.F9.2.m1.1.1.cmml"><mi id="S1.F9.2.m1.1.1.2" xref="S1.F9.2.m1.1.1.2.cmml">AP</mi><mn id="S1.F9.2.m1.1.1.3" xref="S1.F9.2.m1.1.1.3.cmml">40</mn></msub><annotation-xml encoding="MathML-Content" id="S1.F9.2.m1.1c"><apply id="S1.F9.2.m1.1.1.cmml" xref="S1.F9.2.m1.1.1"><csymbol cd="ambiguous" id="S1.F9.2.m1.1.1.1.cmml" xref="S1.F9.2.m1.1.1">subscript</csymbol><ci id="S1.F9.2.m1.1.1.2.cmml" xref="S1.F9.2.m1.1.1.2">AP</ci><cn type="integer" id="S1.F9.2.m1.1.1.3.cmml" xref="S1.F9.2.m1.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F9.2.m1.1d">\rm{AP}_{40}</annotation></semantics></math> on KITTI-3D <em id="S1.F9.10.4" class="ltx_emph ltx_font_italic">validation</em> set under the moderate setting.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.05446" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.05447" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.05447">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.05447" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.05448" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 01:48:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
