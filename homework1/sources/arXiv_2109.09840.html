<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.09840] Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling</title><meta property="og:description" content="Shape and pose estimation is a critical perception problem for a self-driving car to fully understand its surrounding environment. One fundamental challenge in solving this problem is the incomplete sensor signal (e.g.â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.09840">

<!--Generated on Tue Mar 19 14:45:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Josephine Monica<sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Wei-Lun Chao<sup id="id7.7.id2" class="ltx_sup"><span id="id7.7.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, and Mark Campbell<sup id="id8.8.id3" class="ltx_sup"><span id="id8.8.id3.1" class="ltx_text ltx_font_italic">3</span></sup>
</span><span class="ltx_author_notes"><math id="id4.4.m1.1" class="ltx_math_unparsed" alttext="{}^{1},^{3}" display="inline"><semantics id="id4.4.m1.1a"><mmultiscripts id="id4.4.m1.1.1"><mo id="id4.4.m1.1.1.2.2">,</mo><mrow id="id4.4.m1.1.1a"></mrow><mn id="id4.4.m1.1.1.2.3">3</mn><mprescripts id="id4.4.m1.1.1b"></mprescripts><mrow id="id4.4.m1.1.1c"></mrow><mn id="id4.4.m1.1.1.3">1</mn></mmultiscripts><annotation encoding="application/x-tex" id="id4.4.m1.1b">{}^{1},^{3}</annotation></semantics></math>Mechanical and Aerospace Engineering Department, Cornell University, NY, USA
<span id="id9.9.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{jm2684, mc288}@cornell.edu</span><sup id="id10.10.id1" class="ltx_sup"><span id="id10.10.id1.1" class="ltx_text ltx_font_italic">2</span></sup>Computer Science and Engineering Department, the Ohio State University, OH, USA <span id="id11.11.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">chao.209@osu.edu</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">Shape and pose estimation is a critical perception problem for a self-driving car to fully understand its surrounding environment. One fundamental challenge in solving this problem is the incomplete sensor signal (e.g., LiDAR scans), especially for faraway or occluded objects.
In this paper, we propose a novel algorithm to address this challenge, which explicitly leverages the sensor signal captured over consecutive time: the consecutive signals can provide more information about an object, including different viewpoints and its motion. By encoding the consecutive signals via a recurrent neural network, not only our algorithm improves the shape and pose estimates, but also produces a labeling tool that can benefit other tasks in autonomous driving research. Specifically, building upon our algorithm, we propose a novel pipeline to automatically annotate high-quality labels for amodal segmentation on images, which are hard and laborious to annotate manually. Our code and data will be made publicly available.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A self-driving car must perceive its environment, identify other traffic participants (e.g., vehicles and pedestrians), and importantly, estimate their shapes and poses in order to plan and act safely.
One of the fundamental challenges for these problems is the sensor signal: a LiDAR scan may only capture one partial view of an object, making shape and pose estimation an ill-posed problem.
Many existing approaches address this challenge by
training a neural network to encode prior knowledge of complete object shapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. While showing promising results, these approaches are still highly sensitive to the quality of input signals.
Specifically, the accuracy of shape and pose estimation drastically drops for faraway or heavily occluded objects whose signals are sparse and limited.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we propose to address this challenge by explicitly leveraging consecutive LiDAR scans: we find that existing methods process each LiDAR scan independently, even though the same object may appear consecutively over time.
We argue that consecutive LiDAR scans are crucial for high-quality pose and shape estimation.
First, while an object may be partially or sparsely observed at each time step, the observations can collectively render a more complete shape over time. Second, traffic participants like vehicles usually move in relatively constrained ways such that temporal information can help correct unreliable pose estimates, as evidenced by the improvement of video-based object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> over frame-wise detection. Third, for objects that are rarely seen in the past (i.e., in the training data), the observation over time essentially offers extra data to adapt the algorithm for improved estimation.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We propose a novel learning-based approach for joint shape and pose estimation that explicitly takes advantage of the consecutive LiDAR scans. Given a sequence of point cloud segments that coarsely captures a single object<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Point cloud segments of individual objects can be extracted from LiDAR scenes through 3D instance segmentation, 3D object detection, or clustering, followed by tracking and data association over time.</span></span></span> (specifically, vehicles in this paper), we propose to fuse the newly extracted features from the current point cloud segment with those from the past via a recurrent neural networkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Leveraging past measurements provides additional information, especially for estimating the complete shape.
In addition, the network can internally learn a motion and behavioral model that is beneficial for pose estimation. We demonstrate that our approach in using consecutive LiDAR scans improves the accuracy of shape and pose estimates through validation on both simulated and real datasets.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Besides improving the shape and pose estimation along with other downstream tasks <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">online</em>, our sequential approach can also benefit other autonomous driving tasks <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">offline</em>.
Specifically, for this paper, we propose to apply the approach to automatically annotate labels for the task of amodal instance segmentation of images, a task aiming to segment the full (amodal) masks of objects irrespective of potential occlusions Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. This is particularly important in autonomous driving, where dense traffic and adverse conditions (e.g., snow, rain, night) may obfuscate objects.
One bottleneck of this research, aside from the algorithm design, is the lack of large-scale datasets annotated with ground-truth amodal masks.
Concretely, manual annotation of amodal masks is laborious and heavily relies on the subjective intuition of the occluded objectsâ€™ shape by the annotator, making it hard to maintain consistency and reliability of the labels.
Fortunately, autonomous driving data is typically collected in sequence, and usually includes both image and LiDAR information.
Thus, we leverage the point cloud sequence and our sequential algorithm to estimate the complete 3D object shapes at each time frame, these 3D shape results are then projected onto the corresponding image to obtain the amodal masks. The occlusion ordering can be immediately obtained from depth information. Overall, our <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">automatic</span> labeling pipeline helps to resolve the ambiguity in manually annotating amodal masks from just a single image.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our main contributions are:
</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The first joint shape and pose estimation algorithm that leverages information in time frames to improve sequential estimates.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A simulated dataset of sequences of partial LiDAR measurements of vehicles along with their corresponding complete 3D shapes. The data and data generation code will also be released to enable other researchers to create their own data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">A novel automatic annotation pipeline for amodal instance segmentation that is built upon our sequential algorithm.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Shape Estimation</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Previous works have proposed different methods and representations in estimating vehiclesâ€™ shapes from LiDAR observations for fine-grained understanding beyond simple bounding boxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. However, these works obtain shape estimates only for parts of vehicles that have been previously seen, often resulting in incomplete shape estimates. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> predict the complete shape of an object from an image through a shape matching process, i.e., representing the object shape as a combination of shape basis, requiring an extensive CAD model library. While 2D image projections of the resulting shape estimates may be accurate, RGB-based methods lack the 3D depth spatial information that is crucial to produce accurate shape estimates in 3D space. As LiDAR sensor is commonly available in autonomous driving setup, 3D point cloud information can be utilized for more accurate shape and pose predictions.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Point cloud completion aims to predict the complete shape of an object given an incomplete point cloud measurement. Point Completion Network (PCN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, one of the pioneers for learning-based shape completion, proposes a simple encoder-decoder shape completion network. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> designs a new decoder architecture that generates point cloud based on a hierarchical tree structure. More recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> proposes a completion method that directly predicts the missing point cloud and appends it to the point cloud input.
However, they all assume well-aligned, canonical point cloud inputs, which are hard to obtain for actual sensor data. Thus, these methods require a pose estimation and a point cloud alignment to a canonical pose as preprocessing steps to operate on real-world data. Consequently, the shape estimation is subject to errors from the pose estimation.
To address this issue, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> proposes a network that jointly estimates the pose and complete shape of vehicles, sharing information between the two tasks. Our work builds upon these efforts. However, unlike existing works which make an independent estimate for each point cloud input, our method fuses information over time to produce more accurate shape and pose estimates.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Pose Estimation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Pose estimation in autonomous driving refers to inferring the location and orientation of an object. Many detection and tracking algorithms represent vehicles by bounding boxes (parameterized by sizes, orientations, and center locations) and perform traditional filtering, such as the Kalman filter, to fuse new measurements with prior beliefs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. However, representing vehicles by bounding boxes eliminates detailed shape information that can facilitate accurate and robust tracking.
To make full use of shape information, some trackers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> apply Iterative Closest Point (ICP) to align point cloud measurements. However, they heavily rely on good initialization and often get stuck in local minima. As opposed to ICP, Annealed Dynamic Histogram (ADH) tracker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> globally explores the state space to find the best Markovian alignment between measurements. However, ADH scores the point cloud alignment only based on the latest two measurements. Thus, ADH lacks robustness when the point cloud is sparse, or the viewpoint changes rapidly. Learning-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> benefit from curated training data and can learn to be robust against occlusions and inferior sensor data. However, most of these approaches operate separately on individual frames and do not utilize past information along tracks.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Amodal Mask Labeling</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Labeling amodal masks is commonly deemed as a subjective and ill-posed problem, as it involves the annotatorsâ€™ intuition to predict the occluded masks. Thus, it requires rigorous guidelines and control to ensure the consistency and quality of the labels.
For example, the KINS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> provides manually annotated amodal mask labels on the autonomous driving KITTI object detection data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. To obtain high-quality labels, the data is repetitively labeled six times by three different annotators. This shows the difficulty of amodal mask labeling.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Several attempts have been made to automate the amodal mask label generation. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> intentionally introduces occlusion to the image by overlaying unoccluded instances with other object masks. The new overlaid image is used as the input data, while the original unoccluded masks serve as the amodal mask labels. While this can efficiently provide accurate amodal mask labels, the generated images may not look realistic or reflect a realistic occlusion relationship in the real world. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> aligns 3D CAD models from PASCAL 3D+ dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> to the target instances in PASCAL VOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> images and projects the model onto the image frames to acquire the labels. While the generated images come from a real-world occlusion relationship, the process requires object 3D CAD models and manual alignment of the models.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2109.09840/assets/figs/overview-corrected.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1136" height="284" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview diagram of our sequential shape and pose estimation pipeline. The encoder consists of two stacked PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> layers. The fusion network is a single layer GRU. The shape and pose decoders are Multi-layer Perceptrons (MLPs). </figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Sequential Joint Shape and Pose Estimation</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.7" class="ltx_p">We formulate the problem of <em id="S3.p1.7.1" class="ltx_emph ltx_font_italic">sequential joint shape and pose estimation</em> as estimating the complete point cloud <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="X_{t}" display="inline"><semantics id="S3.p1.1.m1.1a"><msub id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">X</mi><mi id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">ğ‘‹</ci><ci id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">X_{t}</annotation></semantics></math> and homogeneous transformation <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="T_{t}" display="inline"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">T</mi><mi id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">ğ‘‡</ci><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">T_{t}</annotation></semantics></math> (from the canonical reference pose) of an object at time <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">t</annotation></semantics></math>, given unaligned partial point cloud measurements <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="Z_{1:t}" display="inline"><semantics id="S3.p1.4.m4.1a"><msub id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">Z</mi><mrow id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml"><mn id="S3.p1.4.m4.1.1.3.2" xref="S3.p1.4.m4.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.p1.4.m4.1.1.3.1" xref="S3.p1.4.m4.1.1.3.1.cmml">:</mo><mi id="S3.p1.4.m4.1.1.3.3" xref="S3.p1.4.m4.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">ğ‘</ci><apply id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3"><ci id="S3.p1.4.m4.1.1.3.1.cmml" xref="S3.p1.4.m4.1.1.3.1">:</ci><cn type="integer" id="S3.p1.4.m4.1.1.3.2.cmml" xref="S3.p1.4.m4.1.1.3.2">1</cn><ci id="S3.p1.4.m4.1.1.3.3.cmml" xref="S3.p1.4.m4.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">Z_{1:t}</annotation></semantics></math> of the same object. As vehicles commonly move on a planar ground, we only estimate the planar translation and rotation for <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="T_{t}" display="inline"><semantics id="S3.p1.5.m5.1a"><msub id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml"><mi id="S3.p1.5.m5.1.1.2" xref="S3.p1.5.m5.1.1.2.cmml">T</mi><mi id="S3.p1.5.m5.1.1.3" xref="S3.p1.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><apply id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.1.cmml" xref="S3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.p1.5.m5.1.1.2.cmml" xref="S3.p1.5.m5.1.1.2">ğ‘‡</ci><ci id="S3.p1.5.m5.1.1.3.cmml" xref="S3.p1.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">T_{t}</annotation></semantics></math>. <math id="S3.p1.6.m6.1" class="ltx_Math" alttext="Z_{1:t}" display="inline"><semantics id="S3.p1.6.m6.1a"><msub id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml"><mi id="S3.p1.6.m6.1.1.2" xref="S3.p1.6.m6.1.1.2.cmml">Z</mi><mrow id="S3.p1.6.m6.1.1.3" xref="S3.p1.6.m6.1.1.3.cmml"><mn id="S3.p1.6.m6.1.1.3.2" xref="S3.p1.6.m6.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.p1.6.m6.1.1.3.1" xref="S3.p1.6.m6.1.1.3.1.cmml">:</mo><mi id="S3.p1.6.m6.1.1.3.3" xref="S3.p1.6.m6.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><apply id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p1.6.m6.1.1.1.cmml" xref="S3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.p1.6.m6.1.1.2.cmml" xref="S3.p1.6.m6.1.1.2">ğ‘</ci><apply id="S3.p1.6.m6.1.1.3.cmml" xref="S3.p1.6.m6.1.1.3"><ci id="S3.p1.6.m6.1.1.3.1.cmml" xref="S3.p1.6.m6.1.1.3.1">:</ci><cn type="integer" id="S3.p1.6.m6.1.1.3.2.cmml" xref="S3.p1.6.m6.1.1.3.2">1</cn><ci id="S3.p1.6.m6.1.1.3.3.cmml" xref="S3.p1.6.m6.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">Z_{1:t}</annotation></semantics></math> can be obtained by a
frame-wise point cloud segmentation (e.g., via object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> or 3D instance segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>), followed by a standard data association over time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Our approach outputs the complete point cloud in the original pose of the unaligned measurement <math id="S3.p1.7.m7.1" class="ltx_Math" alttext="Z_{t}" display="inline"><semantics id="S3.p1.7.m7.1a"><msub id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml"><mi id="S3.p1.7.m7.1.1.2" xref="S3.p1.7.m7.1.1.2.cmml">Z</mi><mi id="S3.p1.7.m7.1.1.3" xref="S3.p1.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><apply id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p1.7.m7.1.1.1.cmml" xref="S3.p1.7.m7.1.1">subscript</csymbol><ci id="S3.p1.7.m7.1.1.2.cmml" xref="S3.p1.7.m7.1.1.2">ğ‘</ci><ci id="S3.p1.7.m7.1.1.3.cmml" xref="S3.p1.7.m7.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">Z_{t}</annotation></semantics></math> rather than in its canonical pose.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Pipeline</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">Our algorithm begins with estimating the complete shape and pose of a vehicle from its first observation using an encoder-decoder-style neural network inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. For the sequential time frames, however, unlike existing work, we recursively fuse the newly extracted point cloud features with those from the past through Gated Recurrent Units (GRUs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
<a href="#S2.F1" title="Figure 1 â€£ II-C Amodal Mask Labeling â€£ II Related Work â€£ Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 1</span></a> shows the overall pipeline of our sequential shape and pose estimation framework. First, we preprocess the input by shifting the partial point cloud measurement <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="Z_{t}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">Z</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">Z_{t}</annotation></semantics></math> by its mean <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\bar{Z}_{t}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mover accent="true" id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2.2" xref="S3.SS1.p1.2.m2.1.1.2.2.cmml">Z</mi><mo id="S3.SS1.p1.2.m2.1.1.2.1" xref="S3.SS1.p1.2.m2.1.1.2.1.cmml">Â¯</mo></mover><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><apply id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2"><ci id="S3.SS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1.2.1">Â¯</ci><ci id="S3.SS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2.2">ğ‘</ci></apply><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\bar{Z}_{t}</annotation></semantics></math> (demeaning in <a href="#S2.F1" title="Figure 1 â€£ II-C Amodal Mask Labeling â€£ II Related Work â€£ Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 1</span></a>):</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="Z_{t}^{\prime}=Z_{t}-\bar{Z}_{t}" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msubsup id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2.2" xref="S3.E1.m1.1.1.2.2.2.cmml">Z</mi><mi id="S3.E1.m1.1.1.2.2.3" xref="S3.E1.m1.1.1.2.2.3.cmml">t</mi><mo id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml">â€²</mo></msubsup><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">Z</mi><mi id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml">t</mi></msub><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">âˆ’</mo><msub id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mover accent="true" id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml"><mi id="S3.E1.m1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.3.3.2.2.cmml">Z</mi><mo id="S3.E1.m1.1.1.3.3.2.1" xref="S3.E1.m1.1.1.3.3.2.1.cmml">Â¯</mo></mover><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.2.2.2">ğ‘</ci><ci id="S3.E1.m1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.2.2.3">ğ‘¡</ci></apply><ci id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3">â€²</ci></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><minus id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></minus><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">ğ‘</ci><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">ğ‘¡</ci></apply><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3">subscript</csymbol><apply id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2"><ci id="S3.E1.m1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3.2.1">Â¯</ci><ci id="S3.E1.m1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.3.3.2.2">ğ‘</ci></apply><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">ğ‘¡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">Z_{t}^{\prime}=Z_{t}-\bar{Z}_{t}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.4" class="ltx_p">In our pipeline, demeaning assists the fusion process by coarsely aligning point cloud measurements from different time steps to the same origin coordinate. Additionally, this step makes training and inference easier by narrowing the network input and output range around the origin. The demeaned point cloud <math id="S3.SS1.p1.3.m1.1" class="ltx_Math" alttext="Z_{t}^{\prime}" display="inline"><semantics id="S3.SS1.p1.3.m1.1a"><msubsup id="S3.SS1.p1.3.m1.1.1" xref="S3.SS1.p1.3.m1.1.1.cmml"><mi id="S3.SS1.p1.3.m1.1.1.2.2" xref="S3.SS1.p1.3.m1.1.1.2.2.cmml">Z</mi><mi id="S3.SS1.p1.3.m1.1.1.2.3" xref="S3.SS1.p1.3.m1.1.1.2.3.cmml">t</mi><mo id="S3.SS1.p1.3.m1.1.1.3" xref="S3.SS1.p1.3.m1.1.1.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m1.1b"><apply id="S3.SS1.p1.3.m1.1.1.cmml" xref="S3.SS1.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m1.1.1.1.cmml" xref="S3.SS1.p1.3.m1.1.1">superscript</csymbol><apply id="S3.SS1.p1.3.m1.1.1.2.cmml" xref="S3.SS1.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m1.1.1.2.1.cmml" xref="S3.SS1.p1.3.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m1.1.1.2.2.cmml" xref="S3.SS1.p1.3.m1.1.1.2.2">ğ‘</ci><ci id="S3.SS1.p1.3.m1.1.1.2.3.cmml" xref="S3.SS1.p1.3.m1.1.1.2.3">ğ‘¡</ci></apply><ci id="S3.SS1.p1.3.m1.1.1.3.cmml" xref="S3.SS1.p1.3.m1.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m1.1c">Z_{t}^{\prime}</annotation></semantics></math> is then passed to the joint encoder to extract the feature vector <math id="S3.SS1.p1.4.m2.1" class="ltx_Math" alttext="f_{t}" display="inline"><semantics id="S3.SS1.p1.4.m2.1a"><msub id="S3.SS1.p1.4.m2.1.1" xref="S3.SS1.p1.4.m2.1.1.cmml"><mi id="S3.SS1.p1.4.m2.1.1.2" xref="S3.SS1.p1.4.m2.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.4.m2.1.1.3" xref="S3.SS1.p1.4.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m2.1b"><apply id="S3.SS1.p1.4.m2.1.1.cmml" xref="S3.SS1.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m2.1.1.1.cmml" xref="S3.SS1.p1.4.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m2.1.1.2.cmml" xref="S3.SS1.p1.4.m2.1.1.2">ğ‘“</ci><ci id="S3.SS1.p1.4.m2.1.1.3.cmml" xref="S3.SS1.p1.4.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m2.1c">f_{t}</annotation></semantics></math>:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="f_{t}=f(Z_{t}^{\prime})" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">f</mi><mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">t</mi></msub><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.cmml">Z</mi><mi id="S3.E2.m1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.3.cmml">t</mi><mo id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml">â€²</mo></msubsup><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">ğ‘“</ci><ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">ğ‘¡</ci></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">ğ‘“</ci><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3">ğ‘¡</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">â€²</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">f_{t}=f(Z_{t}^{\prime})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.5" class="ltx_p">which summarizes the measurement at time <math id="S3.SS1.p1.5.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS1.p1.5.m1.1a"><mi id="S3.SS1.p1.5.m1.1.1" xref="S3.SS1.p1.5.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m1.1b"><ci id="S3.SS1.p1.5.m1.1.1.cmml" xref="S3.SS1.p1.5.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m1.1c">t</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.5" class="ltx_p">Instead of using <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="f_{t}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">ğ‘“</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">f_{t}</annotation></semantics></math> alone to predict the complete shape and pose, our method leverages all measurements <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="Z_{1:t}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">Z</mi><mrow id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mn id="S3.SS1.p2.2.m2.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS1.p2.2.m2.1.1.3.1" xref="S3.SS1.p2.2.m2.1.1.3.1.cmml">:</mo><mi id="S3.SS1.p2.2.m2.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">ğ‘</ci><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><ci id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3.1">:</ci><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2">1</cn><ci id="S3.SS1.p2.2.m2.1.1.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">Z_{1:t}</annotation></semantics></math> up to the latest frame in making the prediction. In particular, we employ a GRU module which recursively updates its hidden state <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="h_{t}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">h</mi><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">â„</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">h_{t}</annotation></semantics></math> by fusing the previous hidden state <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="h_{t-1}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">h</mi><mrow id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p2.4.m4.1.1.3.1" xref="S3.SS1.p2.4.m4.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">â„</ci><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><minus id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.1"></minus><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">ğ‘¡</ci><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">h_{t-1}</annotation></semantics></math> with the current measurement feature <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="f_{t}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">f</mi><mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">ğ‘“</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">f_{t}</annotation></semantics></math>:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="h_{t}=GRU(h_{t-1},f_{t})" display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><msub id="S3.E3.m1.2.2.4" xref="S3.E3.m1.2.2.4.cmml"><mi id="S3.E3.m1.2.2.4.2" xref="S3.E3.m1.2.2.4.2.cmml">h</mi><mi id="S3.E3.m1.2.2.4.3" xref="S3.E3.m1.2.2.4.3.cmml">t</mi></msub><mo id="S3.E3.m1.2.2.3" xref="S3.E3.m1.2.2.3.cmml">=</mo><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.4.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">â€‹</mo><mi id="S3.E3.m1.2.2.2.5" xref="S3.E3.m1.2.2.2.5.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.3a" xref="S3.E3.m1.2.2.2.3.cmml">â€‹</mo><mi id="S3.E3.m1.2.2.2.6" xref="S3.E3.m1.2.2.2.6.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.3b" xref="S3.E3.m1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E3.m1.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">h</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="S3.E3.m1.1.1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">âˆ’</mo><mn id="S3.E3.m1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.E3.m1.2.2.2.2.2.4" xref="S3.E3.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.E3.m1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.2.cmml">f</mi><mi id="S3.E3.m1.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.3.cmml">t</mi></msub><mo stretchy="false" id="S3.E3.m1.2.2.2.2.2.5" xref="S3.E3.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><eq id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.3"></eq><apply id="S3.E3.m1.2.2.4.cmml" xref="S3.E3.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.4.1.cmml" xref="S3.E3.m1.2.2.4">subscript</csymbol><ci id="S3.E3.m1.2.2.4.2.cmml" xref="S3.E3.m1.2.2.4.2">â„</ci><ci id="S3.E3.m1.2.2.4.3.cmml" xref="S3.E3.m1.2.2.4.3">ğ‘¡</ci></apply><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><times id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3"></times><ci id="S3.E3.m1.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.4">ğº</ci><ci id="S3.E3.m1.2.2.2.5.cmml" xref="S3.E3.m1.2.2.2.5">ğ‘…</ci><ci id="S3.E3.m1.2.2.2.6.cmml" xref="S3.E3.m1.2.2.2.6">ğ‘ˆ</ci><interval closure="open" id="S3.E3.m1.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2"><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">â„</ci><apply id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3"><minus id="S3.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.1"></minus><ci id="S3.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.2">ğ‘¡</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.E3.m1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2">ğ‘“</ci><ci id="S3.E3.m1.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.3">ğ‘¡</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">h_{t}=GRU(h_{t-1},f_{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.8" class="ltx_p">The hidden state <math id="S3.SS1.p2.6.m1.1" class="ltx_Math" alttext="h_{t}" display="inline"><semantics id="S3.SS1.p2.6.m1.1a"><msub id="S3.SS1.p2.6.m1.1.1" xref="S3.SS1.p2.6.m1.1.1.cmml"><mi id="S3.SS1.p2.6.m1.1.1.2" xref="S3.SS1.p2.6.m1.1.1.2.cmml">h</mi><mi id="S3.SS1.p2.6.m1.1.1.3" xref="S3.SS1.p2.6.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m1.1b"><apply id="S3.SS1.p2.6.m1.1.1.cmml" xref="S3.SS1.p2.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m1.1.1.1.cmml" xref="S3.SS1.p2.6.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m1.1.1.2.cmml" xref="S3.SS1.p2.6.m1.1.1.2">â„</ci><ci id="S3.SS1.p2.6.m1.1.1.3.cmml" xref="S3.SS1.p2.6.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m1.1c">h_{t}</annotation></semantics></math>, which can be seen as a summary of all available measurements <math id="S3.SS1.p2.7.m2.1" class="ltx_Math" alttext="Z_{1:t}" display="inline"><semantics id="S3.SS1.p2.7.m2.1a"><msub id="S3.SS1.p2.7.m2.1.1" xref="S3.SS1.p2.7.m2.1.1.cmml"><mi id="S3.SS1.p2.7.m2.1.1.2" xref="S3.SS1.p2.7.m2.1.1.2.cmml">Z</mi><mrow id="S3.SS1.p2.7.m2.1.1.3" xref="S3.SS1.p2.7.m2.1.1.3.cmml"><mn id="S3.SS1.p2.7.m2.1.1.3.2" xref="S3.SS1.p2.7.m2.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS1.p2.7.m2.1.1.3.1" xref="S3.SS1.p2.7.m2.1.1.3.1.cmml">:</mo><mi id="S3.SS1.p2.7.m2.1.1.3.3" xref="S3.SS1.p2.7.m2.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m2.1b"><apply id="S3.SS1.p2.7.m2.1.1.cmml" xref="S3.SS1.p2.7.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m2.1.1.1.cmml" xref="S3.SS1.p2.7.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m2.1.1.2.cmml" xref="S3.SS1.p2.7.m2.1.1.2">ğ‘</ci><apply id="S3.SS1.p2.7.m2.1.1.3.cmml" xref="S3.SS1.p2.7.m2.1.1.3"><ci id="S3.SS1.p2.7.m2.1.1.3.1.cmml" xref="S3.SS1.p2.7.m2.1.1.3.1">:</ci><cn type="integer" id="S3.SS1.p2.7.m2.1.1.3.2.cmml" xref="S3.SS1.p2.7.m2.1.1.3.2">1</cn><ci id="S3.SS1.p2.7.m2.1.1.3.3.cmml" xref="S3.SS1.p2.7.m2.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m2.1c">Z_{1:t}</annotation></semantics></math>, is then used as the input to the pose and shape decoders.
This fusion process is executed at the feature level instead of the point-cloud level, allowing the network to learn better how to combine and extract the most useful information from <math id="S3.SS1.p2.8.m3.1" class="ltx_Math" alttext="Z_{1:t}" display="inline"><semantics id="S3.SS1.p2.8.m3.1a"><msub id="S3.SS1.p2.8.m3.1.1" xref="S3.SS1.p2.8.m3.1.1.cmml"><mi id="S3.SS1.p2.8.m3.1.1.2" xref="S3.SS1.p2.8.m3.1.1.2.cmml">Z</mi><mrow id="S3.SS1.p2.8.m3.1.1.3" xref="S3.SS1.p2.8.m3.1.1.3.cmml"><mn id="S3.SS1.p2.8.m3.1.1.3.2" xref="S3.SS1.p2.8.m3.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS1.p2.8.m3.1.1.3.1" xref="S3.SS1.p2.8.m3.1.1.3.1.cmml">:</mo><mi id="S3.SS1.p2.8.m3.1.1.3.3" xref="S3.SS1.p2.8.m3.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m3.1b"><apply id="S3.SS1.p2.8.m3.1.1.cmml" xref="S3.SS1.p2.8.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m3.1.1.1.cmml" xref="S3.SS1.p2.8.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m3.1.1.2.cmml" xref="S3.SS1.p2.8.m3.1.1.2">ğ‘</ci><apply id="S3.SS1.p2.8.m3.1.1.3.cmml" xref="S3.SS1.p2.8.m3.1.1.3"><ci id="S3.SS1.p2.8.m3.1.1.3.1.cmml" xref="S3.SS1.p2.8.m3.1.1.3.1">:</ci><cn type="integer" id="S3.SS1.p2.8.m3.1.1.3.2.cmml" xref="S3.SS1.p2.8.m3.1.1.3.2">1</cn><ci id="S3.SS1.p2.8.m3.1.1.3.3.cmml" xref="S3.SS1.p2.8.m3.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m3.1c">Z_{1:t}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.3" class="ltx_p">The updated hidden state <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="h_{t}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">h</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">â„</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">h_{t}</annotation></semantics></math> is then passed to the shape and pose decoders to estimate the complete point cloud <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="X_{t}^{\prime}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><msubsup id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2.2" xref="S3.SS1.p3.2.m2.1.1.2.2.cmml">X</mi><mi id="S3.SS1.p3.2.m2.1.1.2.3" xref="S3.SS1.p3.2.m2.1.1.2.3.cmml">t</mi><mo id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">superscript</csymbol><apply id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.2.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2.2">ğ‘‹</ci><ci id="S3.SS1.p3.2.m2.1.1.2.3.cmml" xref="S3.SS1.p3.2.m2.1.1.2.3">ğ‘¡</ci></apply><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">X_{t}^{\prime}</annotation></semantics></math> and pose <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="T_{t}^{\prime}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msubsup id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2.2" xref="S3.SS1.p3.3.m3.1.1.2.2.cmml">T</mi><mi id="S3.SS1.p3.3.m3.1.1.2.3" xref="S3.SS1.p3.3.m3.1.1.2.3.cmml">t</mi><mo id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">superscript</csymbol><apply id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.2.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2.2">ğ‘‡</ci><ci id="S3.SS1.p3.3.m3.1.1.2.3.cmml" xref="S3.SS1.p3.3.m3.1.1.2.3">ğ‘¡</ci></apply><ci id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">T_{t}^{\prime}</annotation></semantics></math>:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="X_{t}^{\prime}=g_{\text{shape}}(h_{t}),\;T_{t}^{\prime}=g_{\text{pose}}(h_{t})" display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.3.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><msubsup id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.2.2" xref="S3.E4.m1.1.1.1.1.3.2.2.cmml">X</mi><mi id="S3.E4.m1.1.1.1.1.3.2.3" xref="S3.E4.m1.1.1.1.1.3.2.3.cmml">t</mi><mo id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml">â€²</mo></msubsup><mo id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.3.2.cmml">g</mi><mtext id="S3.E4.m1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.3.3a.cmml">shape</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml">h</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.447em" id="S3.E4.m1.2.2.2.3" xref="S3.E4.m1.2.2.3a.cmml">,</mo><mrow id="S3.E4.m1.2.2.2.2" xref="S3.E4.m1.2.2.2.2.cmml"><msubsup id="S3.E4.m1.2.2.2.2.3" xref="S3.E4.m1.2.2.2.2.3.cmml"><mi id="S3.E4.m1.2.2.2.2.3.2.2" xref="S3.E4.m1.2.2.2.2.3.2.2.cmml">T</mi><mi id="S3.E4.m1.2.2.2.2.3.2.3" xref="S3.E4.m1.2.2.2.2.3.2.3.cmml">t</mi><mo id="S3.E4.m1.2.2.2.2.3.3" xref="S3.E4.m1.2.2.2.2.3.3.cmml">â€²</mo></msubsup><mo id="S3.E4.m1.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.cmml">=</mo><mrow id="S3.E4.m1.2.2.2.2.1" xref="S3.E4.m1.2.2.2.2.1.cmml"><msub id="S3.E4.m1.2.2.2.2.1.3" xref="S3.E4.m1.2.2.2.2.1.3.cmml"><mi id="S3.E4.m1.2.2.2.2.1.3.2" xref="S3.E4.m1.2.2.2.2.1.3.2.cmml">g</mi><mtext id="S3.E4.m1.2.2.2.2.1.3.3" xref="S3.E4.m1.2.2.2.2.1.3.3a.cmml">pose</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.2.2.1.2" xref="S3.E4.m1.2.2.2.2.1.2.cmml">â€‹</mo><mrow id="S3.E4.m1.2.2.2.2.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.2.2.1.1.1.2" xref="S3.E4.m1.2.2.2.2.1.1.1.1.cmml">(</mo><msub id="S3.E4.m1.2.2.2.2.1.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.2" xref="S3.E4.m1.2.2.2.2.1.1.1.1.2.cmml">h</mi><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S3.E4.m1.2.2.2.2.1.1.1.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.3a.cmml" xref="S3.E4.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"></eq><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3">superscript</csymbol><apply id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2.2">ğ‘‹</ci><ci id="S3.E4.m1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3">ğ‘¡</ci></apply><ci id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3">â€²</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2"></times><apply id="S3.E4.m1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.3.2">ğ‘”</ci><ci id="S3.E4.m1.1.1.1.1.1.3.3a.cmml" xref="S3.E4.m1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E4.m1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.3.3">shape</mtext></ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2">â„</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3">ğ‘¡</ci></apply></apply></apply><apply id="S3.E4.m1.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2"><eq id="S3.E4.m1.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2"></eq><apply id="S3.E4.m1.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.3.1.cmml" xref="S3.E4.m1.2.2.2.2.3">superscript</csymbol><apply id="S3.E4.m1.2.2.2.2.3.2.cmml" xref="S3.E4.m1.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.3.2.1.cmml" xref="S3.E4.m1.2.2.2.2.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.3.2.2.cmml" xref="S3.E4.m1.2.2.2.2.3.2.2">ğ‘‡</ci><ci id="S3.E4.m1.2.2.2.2.3.2.3.cmml" xref="S3.E4.m1.2.2.2.2.3.2.3">ğ‘¡</ci></apply><ci id="S3.E4.m1.2.2.2.2.3.3.cmml" xref="S3.E4.m1.2.2.2.2.3.3">â€²</ci></apply><apply id="S3.E4.m1.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.1"><times id="S3.E4.m1.2.2.2.2.1.2.cmml" xref="S3.E4.m1.2.2.2.2.1.2"></times><apply id="S3.E4.m1.2.2.2.2.1.3.cmml" xref="S3.E4.m1.2.2.2.2.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.3.1.cmml" xref="S3.E4.m1.2.2.2.2.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.3.2.cmml" xref="S3.E4.m1.2.2.2.2.1.3.2">ğ‘”</ci><ci id="S3.E4.m1.2.2.2.2.1.3.3a.cmml" xref="S3.E4.m1.2.2.2.2.1.3.3"><mtext mathsize="70%" id="S3.E4.m1.2.2.2.2.1.3.3.cmml" xref="S3.E4.m1.2.2.2.2.1.3.3">pose</mtext></ci></apply><apply id="S3.E4.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.2">â„</ci><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.3">ğ‘¡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">X_{t}^{\prime}=g_{\text{shape}}(h_{t}),\;T_{t}^{\prime}=g_{\text{pose}}(h_{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.5" class="ltx_p">These shape and pose estimates are still in the <math id="S3.SS1.p3.4.m1.1" class="ltx_Math" alttext="Z^{\prime}_{t}" display="inline"><semantics id="S3.SS1.p3.4.m1.1a"><msubsup id="S3.SS1.p3.4.m1.1.1" xref="S3.SS1.p3.4.m1.1.1.cmml"><mi id="S3.SS1.p3.4.m1.1.1.2.2" xref="S3.SS1.p3.4.m1.1.1.2.2.cmml">Z</mi><mi id="S3.SS1.p3.4.m1.1.1.3" xref="S3.SS1.p3.4.m1.1.1.3.cmml">t</mi><mo id="S3.SS1.p3.4.m1.1.1.2.3" xref="S3.SS1.p3.4.m1.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m1.1b"><apply id="S3.SS1.p3.4.m1.1.1.cmml" xref="S3.SS1.p3.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m1.1.1.1.cmml" xref="S3.SS1.p3.4.m1.1.1">subscript</csymbol><apply id="S3.SS1.p3.4.m1.1.1.2.cmml" xref="S3.SS1.p3.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m1.1.1.2.1.cmml" xref="S3.SS1.p3.4.m1.1.1">superscript</csymbol><ci id="S3.SS1.p3.4.m1.1.1.2.2.cmml" xref="S3.SS1.p3.4.m1.1.1.2.2">ğ‘</ci><ci id="S3.SS1.p3.4.m1.1.1.2.3.cmml" xref="S3.SS1.p3.4.m1.1.1.2.3">â€²</ci></apply><ci id="S3.SS1.p3.4.m1.1.1.3.cmml" xref="S3.SS1.p3.4.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m1.1c">Z^{\prime}_{t}</annotation></semantics></math> coordinate frame (i.e., coordinate frame whose origin lies at the measurement mean). A forward transformation is applied to bring each back to the original measurement <math id="S3.SS1.p3.5.m2.1" class="ltx_Math" alttext="Z_{t}" display="inline"><semantics id="S3.SS1.p3.5.m2.1a"><msub id="S3.SS1.p3.5.m2.1.1" xref="S3.SS1.p3.5.m2.1.1.cmml"><mi id="S3.SS1.p3.5.m2.1.1.2" xref="S3.SS1.p3.5.m2.1.1.2.cmml">Z</mi><mi id="S3.SS1.p3.5.m2.1.1.3" xref="S3.SS1.p3.5.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m2.1b"><apply id="S3.SS1.p3.5.m2.1.1.cmml" xref="S3.SS1.p3.5.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m2.1.1.1.cmml" xref="S3.SS1.p3.5.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m2.1.1.2.cmml" xref="S3.SS1.p3.5.m2.1.1.2">ğ‘</ci><ci id="S3.SS1.p3.5.m2.1.1.3.cmml" xref="S3.SS1.p3.5.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m2.1c">Z_{t}</annotation></semantics></math> frame:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.3" class="ltx_Math" alttext="X_{t}=X_{t}^{\prime}+\bar{Z}_{t},\;T_{t}=\begin{bmatrix}I&amp;\bar{Z}_{t}\\
0&amp;1\end{bmatrix}T_{t}^{\prime}" display="block"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3.2" xref="S3.E5.m1.3.3.3.cmml"><mrow id="S3.E5.m1.2.2.1.1" xref="S3.E5.m1.2.2.1.1.cmml"><msub id="S3.E5.m1.2.2.1.1.2" xref="S3.E5.m1.2.2.1.1.2.cmml"><mi id="S3.E5.m1.2.2.1.1.2.2" xref="S3.E5.m1.2.2.1.1.2.2.cmml">X</mi><mi id="S3.E5.m1.2.2.1.1.2.3" xref="S3.E5.m1.2.2.1.1.2.3.cmml">t</mi></msub><mo id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.E5.m1.2.2.1.1.3" xref="S3.E5.m1.2.2.1.1.3.cmml"><msubsup id="S3.E5.m1.2.2.1.1.3.2" xref="S3.E5.m1.2.2.1.1.3.2.cmml"><mi id="S3.E5.m1.2.2.1.1.3.2.2.2" xref="S3.E5.m1.2.2.1.1.3.2.2.2.cmml">X</mi><mi id="S3.E5.m1.2.2.1.1.3.2.2.3" xref="S3.E5.m1.2.2.1.1.3.2.2.3.cmml">t</mi><mo id="S3.E5.m1.2.2.1.1.3.2.3" xref="S3.E5.m1.2.2.1.1.3.2.3.cmml">â€²</mo></msubsup><mo id="S3.E5.m1.2.2.1.1.3.1" xref="S3.E5.m1.2.2.1.1.3.1.cmml">+</mo><msub id="S3.E5.m1.2.2.1.1.3.3" xref="S3.E5.m1.2.2.1.1.3.3.cmml"><mover accent="true" id="S3.E5.m1.2.2.1.1.3.3.2" xref="S3.E5.m1.2.2.1.1.3.3.2.cmml"><mi id="S3.E5.m1.2.2.1.1.3.3.2.2" xref="S3.E5.m1.2.2.1.1.3.3.2.2.cmml">Z</mi><mo id="S3.E5.m1.2.2.1.1.3.3.2.1" xref="S3.E5.m1.2.2.1.1.3.3.2.1.cmml">Â¯</mo></mover><mi id="S3.E5.m1.2.2.1.1.3.3.3" xref="S3.E5.m1.2.2.1.1.3.3.3.cmml">t</mi></msub></mrow></mrow><mo rspace="0.447em" id="S3.E5.m1.3.3.2.3" xref="S3.E5.m1.3.3.3a.cmml">,</mo><mrow id="S3.E5.m1.3.3.2.2" xref="S3.E5.m1.3.3.2.2.cmml"><msub id="S3.E5.m1.3.3.2.2.2" xref="S3.E5.m1.3.3.2.2.2.cmml"><mi id="S3.E5.m1.3.3.2.2.2.2" xref="S3.E5.m1.3.3.2.2.2.2.cmml">T</mi><mi id="S3.E5.m1.3.3.2.2.2.3" xref="S3.E5.m1.3.3.2.2.2.3.cmml">t</mi></msub><mo id="S3.E5.m1.3.3.2.2.1" xref="S3.E5.m1.3.3.2.2.1.cmml">=</mo><mrow id="S3.E5.m1.3.3.2.2.3" xref="S3.E5.m1.3.3.2.2.3.cmml"><mrow id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.2.cmml"><mo id="S3.E5.m1.1.1.3.1" xref="S3.E5.m1.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mtr id="S3.E5.m1.1.1.1.1a" xref="S3.E5.m1.1.1.1.1.cmml"><mtd id="S3.E5.m1.1.1.1.1b" xref="S3.E5.m1.1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.cmml">I</mi></mtd><mtd id="S3.E5.m1.1.1.1.1c" xref="S3.E5.m1.1.1.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.1.2.1" xref="S3.E5.m1.1.1.1.1.1.2.1.cmml"><mover accent="true" id="S3.E5.m1.1.1.1.1.1.2.1.2" xref="S3.E5.m1.1.1.1.1.1.2.1.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.2.1.2.2" xref="S3.E5.m1.1.1.1.1.1.2.1.2.2.cmml">Z</mi><mo id="S3.E5.m1.1.1.1.1.1.2.1.2.1" xref="S3.E5.m1.1.1.1.1.1.2.1.2.1.cmml">Â¯</mo></mover><mi id="S3.E5.m1.1.1.1.1.1.2.1.3" xref="S3.E5.m1.1.1.1.1.1.2.1.3.cmml">t</mi></msub></mtd></mtr><mtr id="S3.E5.m1.1.1.1.1d" xref="S3.E5.m1.1.1.1.1.cmml"><mtd id="S3.E5.m1.1.1.1.1e" xref="S3.E5.m1.1.1.1.1.cmml"><mn id="S3.E5.m1.1.1.1.1.2.1.1" xref="S3.E5.m1.1.1.1.1.2.1.1.cmml">0</mn></mtd><mtd id="S3.E5.m1.1.1.1.1f" xref="S3.E5.m1.1.1.1.1.cmml"><mn id="S3.E5.m1.1.1.1.1.2.2.1" xref="S3.E5.m1.1.1.1.1.2.2.1.cmml">1</mn></mtd></mtr></mtable><mo id="S3.E5.m1.1.1.3.2" xref="S3.E5.m1.1.1.2.1.cmml">]</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.2.2.3.1" xref="S3.E5.m1.3.3.2.2.3.1.cmml">â€‹</mo><msubsup id="S3.E5.m1.3.3.2.2.3.2" xref="S3.E5.m1.3.3.2.2.3.2.cmml"><mi id="S3.E5.m1.3.3.2.2.3.2.2.2" xref="S3.E5.m1.3.3.2.2.3.2.2.2.cmml">T</mi><mi id="S3.E5.m1.3.3.2.2.3.2.2.3" xref="S3.E5.m1.3.3.2.2.3.2.2.3.cmml">t</mi><mo id="S3.E5.m1.3.3.2.2.3.2.3" xref="S3.E5.m1.3.3.2.2.3.2.3.cmml">â€²</mo></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.3.cmml" xref="S3.E5.m1.3.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.3a.cmml" xref="S3.E5.m1.3.3.2.3">formulae-sequence</csymbol><apply id="S3.E5.m1.2.2.1.1.cmml" xref="S3.E5.m1.2.2.1.1"><eq id="S3.E5.m1.2.2.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1"></eq><apply id="S3.E5.m1.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.2.1.cmml" xref="S3.E5.m1.2.2.1.1.2">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.2.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2">ğ‘‹</ci><ci id="S3.E5.m1.2.2.1.1.2.3.cmml" xref="S3.E5.m1.2.2.1.1.2.3">ğ‘¡</ci></apply><apply id="S3.E5.m1.2.2.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.3"><plus id="S3.E5.m1.2.2.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.3.1"></plus><apply id="S3.E5.m1.2.2.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.3.2.1.cmml" xref="S3.E5.m1.2.2.1.1.3.2">superscript</csymbol><apply id="S3.E5.m1.2.2.1.1.3.2.2.cmml" xref="S3.E5.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.3.2.2.1.cmml" xref="S3.E5.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.3.2.2.2.cmml" xref="S3.E5.m1.2.2.1.1.3.2.2.2">ğ‘‹</ci><ci id="S3.E5.m1.2.2.1.1.3.2.2.3.cmml" xref="S3.E5.m1.2.2.1.1.3.2.2.3">ğ‘¡</ci></apply><ci id="S3.E5.m1.2.2.1.1.3.2.3.cmml" xref="S3.E5.m1.2.2.1.1.3.2.3">â€²</ci></apply><apply id="S3.E5.m1.2.2.1.1.3.3.cmml" xref="S3.E5.m1.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.3.3.1.cmml" xref="S3.E5.m1.2.2.1.1.3.3">subscript</csymbol><apply id="S3.E5.m1.2.2.1.1.3.3.2.cmml" xref="S3.E5.m1.2.2.1.1.3.3.2"><ci id="S3.E5.m1.2.2.1.1.3.3.2.1.cmml" xref="S3.E5.m1.2.2.1.1.3.3.2.1">Â¯</ci><ci id="S3.E5.m1.2.2.1.1.3.3.2.2.cmml" xref="S3.E5.m1.2.2.1.1.3.3.2.2">ğ‘</ci></apply><ci id="S3.E5.m1.2.2.1.1.3.3.3.cmml" xref="S3.E5.m1.2.2.1.1.3.3.3">ğ‘¡</ci></apply></apply></apply><apply id="S3.E5.m1.3.3.2.2.cmml" xref="S3.E5.m1.3.3.2.2"><eq id="S3.E5.m1.3.3.2.2.1.cmml" xref="S3.E5.m1.3.3.2.2.1"></eq><apply id="S3.E5.m1.3.3.2.2.2.cmml" xref="S3.E5.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.2.2.2.1.cmml" xref="S3.E5.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.E5.m1.3.3.2.2.2.2.cmml" xref="S3.E5.m1.3.3.2.2.2.2">ğ‘‡</ci><ci id="S3.E5.m1.3.3.2.2.2.3.cmml" xref="S3.E5.m1.3.3.2.2.2.3">ğ‘¡</ci></apply><apply id="S3.E5.m1.3.3.2.2.3.cmml" xref="S3.E5.m1.3.3.2.2.3"><times id="S3.E5.m1.3.3.2.2.3.1.cmml" xref="S3.E5.m1.3.3.2.2.3.1"></times><apply id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.3"><csymbol cd="latexml" id="S3.E5.m1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.3.1">matrix</csymbol><matrix id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1"><matrixrow id="S3.E5.m1.1.1.1.1a.cmml" xref="S3.E5.m1.1.1.1.1"><ci id="S3.E5.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1">ğ¼</ci><apply id="S3.E5.m1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.2.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.2.1">subscript</csymbol><apply id="S3.E5.m1.1.1.1.1.1.2.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.2.1.2"><ci id="S3.E5.m1.1.1.1.1.1.2.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.2.1.2.1">Â¯</ci><ci id="S3.E5.m1.1.1.1.1.1.2.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.2.1.2.2">ğ‘</ci></apply><ci id="S3.E5.m1.1.1.1.1.1.2.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.2.1.3">ğ‘¡</ci></apply></matrixrow><matrixrow id="S3.E5.m1.1.1.1.1b.cmml" xref="S3.E5.m1.1.1.1.1"><cn type="integer" id="S3.E5.m1.1.1.1.1.2.1.1.cmml" xref="S3.E5.m1.1.1.1.1.2.1.1">0</cn><cn type="integer" id="S3.E5.m1.1.1.1.1.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1">1</cn></matrixrow></matrix></apply><apply id="S3.E5.m1.3.3.2.2.3.2.cmml" xref="S3.E5.m1.3.3.2.2.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.2.2.3.2.1.cmml" xref="S3.E5.m1.3.3.2.2.3.2">superscript</csymbol><apply id="S3.E5.m1.3.3.2.2.3.2.2.cmml" xref="S3.E5.m1.3.3.2.2.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.2.2.3.2.2.1.cmml" xref="S3.E5.m1.3.3.2.2.3.2">subscript</csymbol><ci id="S3.E5.m1.3.3.2.2.3.2.2.2.cmml" xref="S3.E5.m1.3.3.2.2.3.2.2.2">ğ‘‡</ci><ci id="S3.E5.m1.3.3.2.2.3.2.2.3.cmml" xref="S3.E5.m1.3.3.2.2.3.2.2.3">ğ‘¡</ci></apply><ci id="S3.E5.m1.3.3.2.2.3.2.3.cmml" xref="S3.E5.m1.3.3.2.2.3.2.3">â€²</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">X_{t}=X_{t}^{\prime}+\bar{Z}_{t},\;T_{t}=\begin{bmatrix}I&amp;\bar{Z}_{t}\\
0&amp;1\end{bmatrix}T_{t}^{\prime}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.2" class="ltx_p">Overall, our pipeline shares some conceptual similarities to shape estimation of a moving object using traditional filtering methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> which consists of dynamics and measurement update steps. Dynamics update aims to transform the shape estimate to the same coordinate frame as the new measurement to assist the measurement update (e.g., point cloud fusion process). Our demeaning block is analogous to the dynamics update, but instead of transforming the shape estimate forward, it transforms the new measurement back to the previously fused state frame. We design the dynamics update in reverse to avoid having to decode <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="h_{t-1}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><msub id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">h</mi><mrow id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml"><mi id="S3.SS1.p4.1.m1.1.1.3.2" xref="S3.SS1.p4.1.m1.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p4.1.m1.1.1.3.1" xref="S3.SS1.p4.1.m1.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p4.1.m1.1.1.3.3" xref="S3.SS1.p4.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">â„</ci><apply id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3"><minus id="S3.SS1.p4.1.m1.1.1.3.1.cmml" xref="S3.SS1.p4.1.m1.1.1.3.1"></minus><ci id="S3.SS1.p4.1.m1.1.1.3.2.cmml" xref="S3.SS1.p4.1.m1.1.1.3.2">ğ‘¡</ci><cn type="integer" id="S3.SS1.p4.1.m1.1.1.3.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">h_{t-1}</annotation></semantics></math> to shape and pose before transforming then encoding them again to get the next time step hidden state <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="h_{t}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">h</mi><mi id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">â„</ci><ci id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">h_{t}</annotation></semantics></math>. This redundant decoding-encoding step will induce information loss and produce sub-optimal results. Meanwhile, our GRU fusion is analogous to the measurement update, which utilizes the measurement information to update the shape and pose estimates.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Training</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Neural networks that contain multiple components are challenging to train in a naive end-to-end manner.
We thus follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> to employ a multi-stage training procedure.</p>
</div>
<section id="S3.SS2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Stage 1</h4>

<div id="S3.SS2.SSSx1.p1" class="ltx_para">
<p id="S3.SS2.SSSx1.p1.2" class="ltx_p">We train the encoder, GRU, and shape decoder by minimizing the Chamfer Distance (CD) shape loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> between the point cloud estimate <math id="S3.SS2.SSSx1.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.SSSx1.p1.1.m1.1a"><mi id="S3.SS2.SSSx1.p1.1.m1.1.1" xref="S3.SS2.SSSx1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p1.1.m1.1b"><ci id="S3.SS2.SSSx1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSSx1.p1.1.m1.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p1.1.m1.1c">X</annotation></semantics></math> and ground truth <math id="S3.SS2.SSSx1.p1.2.m2.1" class="ltx_Math" alttext="X^{\text{gt}}" display="inline"><semantics id="S3.SS2.SSSx1.p1.2.m2.1a"><msup id="S3.SS2.SSSx1.p1.2.m2.1.1" xref="S3.SS2.SSSx1.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSSx1.p1.2.m2.1.1.2" xref="S3.SS2.SSSx1.p1.2.m2.1.1.2.cmml">X</mi><mtext id="S3.SS2.SSSx1.p1.2.m2.1.1.3" xref="S3.SS2.SSSx1.p1.2.m2.1.1.3a.cmml">gt</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p1.2.m2.1b"><apply id="S3.SS2.SSSx1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSSx1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSSx1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSSx1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.SSSx1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSSx1.p1.2.m2.1.1.2">ğ‘‹</ci><ci id="S3.SS2.SSSx1.p1.2.m2.1.1.3a.cmml" xref="S3.SS2.SSSx1.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSSx1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSSx1.p1.2.m2.1.1.3">gt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p1.2.m2.1c">X^{\text{gt}}</annotation></semantics></math>:</p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td colspan="2" class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S3.E6.m1.38" class="ltx_Math" alttext="\displaystyle\begin{split}L_{\text{CD}}(X,X^{\text{gt}})=\frac{1}{|X|}\sum_{\hat{x}\in X}\min_{x\in X^{\text{gt}}}\|\hat{x}-x\|_{2}\\
+\frac{1}{|X^{\text{gt}}|}\sum_{x\in X^{\text{gt}}}\min_{\hat{x}\in X}\|x-\hat{x}\|_{2}\end{split}" display="inline"><semantics id="S3.E6.m1.38a"><mtable rowspacing="0pt" id="S3.E6.m1.38.38.6" xref="S3.E6.m1.35.35.3.cmml"><mtr id="S3.E6.m1.38.38.6a" xref="S3.E6.m1.35.35.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E6.m1.38.38.6b" xref="S3.E6.m1.35.35.3.cmml"><mrow id="S3.E6.m1.37.37.5.34.22.22" xref="S3.E6.m1.35.35.3.cmml"><mrow id="S3.E6.m1.36.36.4.33.21.21.21" xref="S3.E6.m1.35.35.3.cmml"><msub id="S3.E6.m1.36.36.4.33.21.21.21.3" xref="S3.E6.m1.35.35.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml">L</mi><mtext id="S3.E6.m1.2.2.2.2.2.2.1" xref="S3.E6.m1.2.2.2.2.2.2.1a.cmml">CD</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.36.36.4.33.21.21.21.2" xref="S3.E6.m1.35.35.3.cmml">â€‹</mo><mrow id="S3.E6.m1.36.36.4.33.21.21.21.1.1" xref="S3.E6.m1.35.35.3.cmml"><mo stretchy="false" id="S3.E6.m1.3.3.3.3.3.3" xref="S3.E6.m1.35.35.3.cmml">(</mo><mi id="S3.E6.m1.4.4.4.4.4.4" xref="S3.E6.m1.4.4.4.4.4.4.cmml">X</mi><mo id="S3.E6.m1.5.5.5.5.5.5" xref="S3.E6.m1.35.35.3.cmml">,</mo><msup id="S3.E6.m1.36.36.4.33.21.21.21.1.1.1" xref="S3.E6.m1.35.35.3.cmml"><mi id="S3.E6.m1.6.6.6.6.6.6" xref="S3.E6.m1.6.6.6.6.6.6.cmml">X</mi><mtext id="S3.E6.m1.7.7.7.7.7.7.1" xref="S3.E6.m1.7.7.7.7.7.7.1a.cmml">gt</mtext></msup><mo stretchy="false" id="S3.E6.m1.8.8.8.8.8.8" xref="S3.E6.m1.35.35.3.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.9.9.9.9.9.9" xref="S3.E6.m1.9.9.9.9.9.9.cmml">=</mo><mrow id="S3.E6.m1.37.37.5.34.22.22.22" xref="S3.E6.m1.35.35.3.cmml"><mstyle displaystyle="true" id="S3.E6.m1.10.10.10.10.10.10" xref="S3.E6.m1.10.10.10.10.10.10.cmml"><mfrac id="S3.E6.m1.10.10.10.10.10.10a" xref="S3.E6.m1.10.10.10.10.10.10.cmml"><mn id="S3.E6.m1.10.10.10.10.10.10.3" xref="S3.E6.m1.10.10.10.10.10.10.3.cmml">1</mn><mrow id="S3.E6.m1.10.10.10.10.10.10.1.3" xref="S3.E6.m1.10.10.10.10.10.10.1.2.cmml"><mo stretchy="false" id="S3.E6.m1.10.10.10.10.10.10.1.3.1" xref="S3.E6.m1.10.10.10.10.10.10.1.2.1.cmml">|</mo><mi id="S3.E6.m1.10.10.10.10.10.10.1.1" xref="S3.E6.m1.10.10.10.10.10.10.1.1.cmml">X</mi><mo stretchy="false" id="S3.E6.m1.10.10.10.10.10.10.1.3.2" xref="S3.E6.m1.10.10.10.10.10.10.1.2.1.cmml">|</mo></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E6.m1.37.37.5.34.22.22.22.2" xref="S3.E6.m1.35.35.3.cmml">â€‹</mo><mrow id="S3.E6.m1.37.37.5.34.22.22.22.1" xref="S3.E6.m1.35.35.3.cmml"><mstyle displaystyle="true" id="S3.E6.m1.37.37.5.34.22.22.22.1.2" xref="S3.E6.m1.35.35.3.cmml"><munder id="S3.E6.m1.37.37.5.34.22.22.22.1.2a" xref="S3.E6.m1.35.35.3.cmml"><mo movablelimits="false" id="S3.E6.m1.11.11.11.11.11.11" xref="S3.E6.m1.11.11.11.11.11.11.cmml">âˆ‘</mo><mrow id="S3.E6.m1.12.12.12.12.12.12.1" xref="S3.E6.m1.12.12.12.12.12.12.1.cmml"><mover accent="true" id="S3.E6.m1.12.12.12.12.12.12.1.2" xref="S3.E6.m1.12.12.12.12.12.12.1.2.cmml"><mi id="S3.E6.m1.12.12.12.12.12.12.1.2.2" xref="S3.E6.m1.12.12.12.12.12.12.1.2.2.cmml">x</mi><mo id="S3.E6.m1.12.12.12.12.12.12.1.2.1" xref="S3.E6.m1.12.12.12.12.12.12.1.2.1.cmml">^</mo></mover><mo id="S3.E6.m1.12.12.12.12.12.12.1.1" xref="S3.E6.m1.12.12.12.12.12.12.1.1.cmml">âˆˆ</mo><mi id="S3.E6.m1.12.12.12.12.12.12.1.3" xref="S3.E6.m1.12.12.12.12.12.12.1.3.cmml">X</mi></mrow></munder></mstyle><mrow id="S3.E6.m1.37.37.5.34.22.22.22.1.1" xref="S3.E6.m1.35.35.3.cmml"><munder id="S3.E6.m1.37.37.5.34.22.22.22.1.1.2" xref="S3.E6.m1.35.35.3.cmml"><mi id="S3.E6.m1.13.13.13.13.13.13" xref="S3.E6.m1.13.13.13.13.13.13.cmml">min</mi><mrow id="S3.E6.m1.14.14.14.14.14.14.1" xref="S3.E6.m1.14.14.14.14.14.14.1.cmml"><mi id="S3.E6.m1.14.14.14.14.14.14.1.2" xref="S3.E6.m1.14.14.14.14.14.14.1.2.cmml">x</mi><mo id="S3.E6.m1.14.14.14.14.14.14.1.1" xref="S3.E6.m1.14.14.14.14.14.14.1.1.cmml">âˆˆ</mo><msup id="S3.E6.m1.14.14.14.14.14.14.1.3" xref="S3.E6.m1.14.14.14.14.14.14.1.3.cmml"><mi id="S3.E6.m1.14.14.14.14.14.14.1.3.2" xref="S3.E6.m1.14.14.14.14.14.14.1.3.2.cmml">X</mi><mtext id="S3.E6.m1.14.14.14.14.14.14.1.3.3" xref="S3.E6.m1.14.14.14.14.14.14.1.3.3a.cmml">gt</mtext></msup></mrow></munder><mo id="S3.E6.m1.37.37.5.34.22.22.22.1.1a" xref="S3.E6.m1.35.35.3.cmml">â¡</mo><msub id="S3.E6.m1.37.37.5.34.22.22.22.1.1.1" xref="S3.E6.m1.35.35.3.cmml"><mrow id="S3.E6.m1.37.37.5.34.22.22.22.1.1.1.1.1" xref="S3.E6.m1.35.35.3.cmml"><mo stretchy="false" id="S3.E6.m1.15.15.15.15.15.15" xref="S3.E6.m1.35.35.3.cmml">â€–</mo><mrow id="S3.E6.m1.37.37.5.34.22.22.22.1.1.1.1.1.1" xref="S3.E6.m1.35.35.3.cmml"><mover accent="true" id="S3.E6.m1.16.16.16.16.16.16" xref="S3.E6.m1.16.16.16.16.16.16.cmml"><mi id="S3.E6.m1.16.16.16.16.16.16.2" xref="S3.E6.m1.16.16.16.16.16.16.2.cmml">x</mi><mo id="S3.E6.m1.16.16.16.16.16.16.1" xref="S3.E6.m1.16.16.16.16.16.16.1.cmml">^</mo></mover><mo id="S3.E6.m1.17.17.17.17.17.17" xref="S3.E6.m1.17.17.17.17.17.17.cmml">âˆ’</mo><mi id="S3.E6.m1.18.18.18.18.18.18" xref="S3.E6.m1.18.18.18.18.18.18.cmml">x</mi></mrow><mo stretchy="false" id="S3.E6.m1.19.19.19.19.19.19" xref="S3.E6.m1.35.35.3.cmml">â€–</mo></mrow><mn id="S3.E6.m1.20.20.20.20.20.20.1" xref="S3.E6.m1.20.20.20.20.20.20.1.cmml">2</mn></msub></mrow></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E6.m1.38.38.6c" xref="S3.E6.m1.35.35.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E6.m1.38.38.6d" xref="S3.E6.m1.35.35.3.cmml"><mrow id="S3.E6.m1.38.38.6.35.13.13" xref="S3.E6.m1.35.35.3.cmml"><mo id="S3.E6.m1.38.38.6.35.13.13a" xref="S3.E6.m1.35.35.3.cmml">+</mo><mrow id="S3.E6.m1.38.38.6.35.13.13.13" xref="S3.E6.m1.35.35.3.cmml"><mstyle displaystyle="true" id="S3.E6.m1.22.22.22.2.2.2" xref="S3.E6.m1.22.22.22.2.2.2.cmml"><mfrac id="S3.E6.m1.22.22.22.2.2.2a" xref="S3.E6.m1.22.22.22.2.2.2.cmml"><mn id="S3.E6.m1.22.22.22.2.2.2.3" xref="S3.E6.m1.22.22.22.2.2.2.3.cmml">1</mn><mrow id="S3.E6.m1.22.22.22.2.2.2.1.1" xref="S3.E6.m1.22.22.22.2.2.2.1.2.cmml"><mo stretchy="false" id="S3.E6.m1.22.22.22.2.2.2.1.1.2" xref="S3.E6.m1.22.22.22.2.2.2.1.2.1.cmml">|</mo><msup id="S3.E6.m1.22.22.22.2.2.2.1.1.1" xref="S3.E6.m1.22.22.22.2.2.2.1.1.1.cmml"><mi id="S3.E6.m1.22.22.22.2.2.2.1.1.1.2" xref="S3.E6.m1.22.22.22.2.2.2.1.1.1.2.cmml">X</mi><mtext id="S3.E6.m1.22.22.22.2.2.2.1.1.1.3" xref="S3.E6.m1.22.22.22.2.2.2.1.1.1.3a.cmml">gt</mtext></msup><mo stretchy="false" id="S3.E6.m1.22.22.22.2.2.2.1.1.3" xref="S3.E6.m1.22.22.22.2.2.2.1.2.1.cmml">|</mo></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E6.m1.38.38.6.35.13.13.13.2" xref="S3.E6.m1.35.35.3.cmml">â€‹</mo><mrow id="S3.E6.m1.38.38.6.35.13.13.13.1" xref="S3.E6.m1.35.35.3.cmml"><mstyle displaystyle="true" id="S3.E6.m1.38.38.6.35.13.13.13.1.2" xref="S3.E6.m1.35.35.3.cmml"><munder id="S3.E6.m1.38.38.6.35.13.13.13.1.2a" xref="S3.E6.m1.35.35.3.cmml"><mo movablelimits="false" id="S3.E6.m1.23.23.23.3.3.3" xref="S3.E6.m1.23.23.23.3.3.3.cmml">âˆ‘</mo><mrow id="S3.E6.m1.24.24.24.4.4.4.1" xref="S3.E6.m1.24.24.24.4.4.4.1.cmml"><mi id="S3.E6.m1.24.24.24.4.4.4.1.2" xref="S3.E6.m1.24.24.24.4.4.4.1.2.cmml">x</mi><mo id="S3.E6.m1.24.24.24.4.4.4.1.1" xref="S3.E6.m1.24.24.24.4.4.4.1.1.cmml">âˆˆ</mo><msup id="S3.E6.m1.24.24.24.4.4.4.1.3" xref="S3.E6.m1.24.24.24.4.4.4.1.3.cmml"><mi id="S3.E6.m1.24.24.24.4.4.4.1.3.2" xref="S3.E6.m1.24.24.24.4.4.4.1.3.2.cmml">X</mi><mtext id="S3.E6.m1.24.24.24.4.4.4.1.3.3" xref="S3.E6.m1.24.24.24.4.4.4.1.3.3a.cmml">gt</mtext></msup></mrow></munder></mstyle><mrow id="S3.E6.m1.38.38.6.35.13.13.13.1.1" xref="S3.E6.m1.35.35.3.cmml"><munder id="S3.E6.m1.38.38.6.35.13.13.13.1.1.2" xref="S3.E6.m1.35.35.3.cmml"><mi id="S3.E6.m1.25.25.25.5.5.5" xref="S3.E6.m1.25.25.25.5.5.5.cmml">min</mi><mrow id="S3.E6.m1.26.26.26.6.6.6.1" xref="S3.E6.m1.26.26.26.6.6.6.1.cmml"><mover accent="true" id="S3.E6.m1.26.26.26.6.6.6.1.2" xref="S3.E6.m1.26.26.26.6.6.6.1.2.cmml"><mi id="S3.E6.m1.26.26.26.6.6.6.1.2.2" xref="S3.E6.m1.26.26.26.6.6.6.1.2.2.cmml">x</mi><mo id="S3.E6.m1.26.26.26.6.6.6.1.2.1" xref="S3.E6.m1.26.26.26.6.6.6.1.2.1.cmml">^</mo></mover><mo id="S3.E6.m1.26.26.26.6.6.6.1.1" xref="S3.E6.m1.26.26.26.6.6.6.1.1.cmml">âˆˆ</mo><mi id="S3.E6.m1.26.26.26.6.6.6.1.3" xref="S3.E6.m1.26.26.26.6.6.6.1.3.cmml">X</mi></mrow></munder><mo id="S3.E6.m1.38.38.6.35.13.13.13.1.1a" xref="S3.E6.m1.35.35.3.cmml">â¡</mo><msub id="S3.E6.m1.38.38.6.35.13.13.13.1.1.1" xref="S3.E6.m1.35.35.3.cmml"><mrow id="S3.E6.m1.38.38.6.35.13.13.13.1.1.1.1.1" xref="S3.E6.m1.35.35.3.cmml"><mo stretchy="false" id="S3.E6.m1.27.27.27.7.7.7" xref="S3.E6.m1.35.35.3.cmml">â€–</mo><mrow id="S3.E6.m1.38.38.6.35.13.13.13.1.1.1.1.1.1" xref="S3.E6.m1.35.35.3.cmml"><mi id="S3.E6.m1.28.28.28.8.8.8" xref="S3.E6.m1.28.28.28.8.8.8.cmml">x</mi><mo id="S3.E6.m1.29.29.29.9.9.9" xref="S3.E6.m1.29.29.29.9.9.9.cmml">âˆ’</mo><mover accent="true" id="S3.E6.m1.30.30.30.10.10.10" xref="S3.E6.m1.30.30.30.10.10.10.cmml"><mi id="S3.E6.m1.30.30.30.10.10.10.2" xref="S3.E6.m1.30.30.30.10.10.10.2.cmml">x</mi><mo id="S3.E6.m1.30.30.30.10.10.10.1" xref="S3.E6.m1.30.30.30.10.10.10.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S3.E6.m1.31.31.31.11.11.11" xref="S3.E6.m1.35.35.3.cmml">â€–</mo></mrow><mn id="S3.E6.m1.32.32.32.12.12.12.1" xref="S3.E6.m1.32.32.32.12.12.12.1.cmml">2</mn></msub></mrow></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E6.m1.38b"><apply id="S3.E6.m1.35.35.3.cmml" xref="S3.E6.m1.38.38.6"><eq id="S3.E6.m1.9.9.9.9.9.9.cmml" xref="S3.E6.m1.9.9.9.9.9.9"></eq><apply id="S3.E6.m1.33.33.1.1.cmml" xref="S3.E6.m1.38.38.6"><times id="S3.E6.m1.33.33.1.1.2.cmml" xref="S3.E6.m1.38.38.6"></times><apply id="S3.E6.m1.33.33.1.1.3.cmml" xref="S3.E6.m1.38.38.6"><csymbol cd="ambiguous" id="S3.E6.m1.33.33.1.1.3.1.cmml" xref="S3.E6.m1.38.38.6">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1">ğ¿</ci><ci id="S3.E6.m1.2.2.2.2.2.2.1a.cmml" xref="S3.E6.m1.2.2.2.2.2.2.1"><mtext mathsize="70%" id="S3.E6.m1.2.2.2.2.2.2.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2.1">CD</mtext></ci></apply><interval closure="open" id="S3.E6.m1.33.33.1.1.1.2.cmml" xref="S3.E6.m1.38.38.6"><ci id="S3.E6.m1.4.4.4.4.4.4.cmml" xref="S3.E6.m1.4.4.4.4.4.4">ğ‘‹</ci><apply id="S3.E6.m1.33.33.1.1.1.1.1.cmml" xref="S3.E6.m1.38.38.6"><csymbol cd="ambiguous" id="S3.E6.m1.33.33.1.1.1.1.1.1.cmml" xref="S3.E6.m1.38.38.6">superscript</csymbol><ci id="S3.E6.m1.6.6.6.6.6.6.cmml" xref="S3.E6.m1.6.6.6.6.6.6">ğ‘‹</ci><ci id="S3.E6.m1.7.7.7.7.7.7.1a.cmml" xref="S3.E6.m1.7.7.7.7.7.7.1"><mtext mathsize="70%" id="S3.E6.m1.7.7.7.7.7.7.1.cmml" xref="S3.E6.m1.7.7.7.7.7.7.1">gt</mtext></ci></apply></interval></apply><apply id="S3.E6.m1.35.35.3.3.cmml" xref="S3.E6.m1.38.38.6"><plus id="S3.E6.m1.21.21.21.1.1.1.cmml" xref="S3.E6.m1.38.38.6"></plus><apply id="S3.E6.m1.34.34.2.2.1.cmml" xref="S3.E6.m1.38.38.6"><times id="S3.E6.m1.34.34.2.2.1.2.cmml" xref="S3.E6.m1.38.38.6"></times><apply id="S3.E6.m1.10.10.10.10.10.10.cmml" xref="S3.E6.m1.10.10.10.10.10.10"><divide id="S3.E6.m1.10.10.10.10.10.10.2.cmml" xref="S3.E6.m1.10.10.10.10.10.10"></divide><cn type="integer" id="S3.E6.m1.10.10.10.10.10.10.3.cmml" xref="S3.E6.m1.10.10.10.10.10.10.3">1</cn><apply id="S3.E6.m1.10.10.10.10.10.10.1.2.cmml" xref="S3.E6.m1.10.10.10.10.10.10.1.3"><abs id="S3.E6.m1.10.10.10.10.10.10.1.2.1.cmml" xref="S3.E6.m1.10.10.10.10.10.10.1.3.1"></abs><ci id="S3.E6.m1.10.10.10.10.10.10.1.1.cmml" xref="S3.E6.m1.10.10.10.10.10.10.1.1">ğ‘‹</ci></apply></apply><apply id="S3.E6.m1.34.34.2.2.1.1.cmml" xref="S3.E6.m1.38.38.6"><apply id="S3.E6.m1.34.34.2.2.1.1.2.cmml" xref="S3.E6.m1.38.38.6"><csymbol cd="ambiguous" id="S3.E6.m1.34.34.2.2.1.1.2.1.cmml" xref="S3.E6.m1.38.38.6">subscript</csymbol><sum id="S3.E6.m1.11.11.11.11.11.11.cmml" xref="S3.E6.m1.11.11.11.11.11.11"></sum><apply id="S3.E6.m1.12.12.12.12.12.12.1.cmml" xref="S3.E6.m1.12.12.12.12.12.12.1"><in id="S3.E6.m1.12.12.12.12.12.12.1.1.cmml" xref="S3.E6.m1.12.12.12.12.12.12.1.1"></in><apply id="S3.E6.m1.12.12.12.12.12.12.1.2.cmml" xref="S3.E6.m1.12.12.12.12.12.12.1.2"><ci id="S3.E6.m1.12.12.12.12.12.12.1.2.1.cmml" xref="S3.E6.m1.12.12.12.12.12.12.1.2.1">^</ci><ci id="S3.E6.m1.12.12.12.12.12.12.1.2.2.cmml" xref="S3.E6.m1.12.12.12.12.12.12.1.2.2">ğ‘¥</ci></apply><ci id="S3.E6.m1.12.12.12.12.12.12.1.3.cmml" xref="S3.E6.m1.12.12.12.12.12.12.1.3">ğ‘‹</ci></apply></apply><apply id="S3.E6.m1.34.34.2.2.1.1.1.cmml" xref="S3.E6.m1.38.38.6"><apply id="S3.E6.m1.34.34.2.2.1.1.1.2.cmml" xref="S3.E6.m1.38.38.6"><csymbol cd="ambiguous" id="S3.E6.m1.34.34.2.2.1.1.1.2.1.cmml" xref="S3.E6.m1.38.38.6">subscript</csymbol><min id="S3.E6.m1.13.13.13.13.13.13.cmml" xref="S3.E6.m1.13.13.13.13.13.13"></min><apply id="S3.E6.m1.14.14.14.14.14.14.1.cmml" xref="S3.E6.m1.14.14.14.14.14.14.1"><in id="S3.E6.m1.14.14.14.14.14.14.1.1.cmml" xref="S3.E6.m1.14.14.14.14.14.14.1.1"></in><ci id="S3.E6.m1.14.14.14.14.14.14.1.2.cmml" xref="S3.E6.m1.14.14.14.14.14.14.1.2">ğ‘¥</ci><apply id="S3.E6.m1.14.14.14.14.14.14.1.3.cmml" xref="S3.E6.m1.14.14.14.14.14.14.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.14.14.14.14.14.14.1.3.1.cmml" xref="S3.E6.m1.14.14.14.14.14.14.1.3">superscript</csymbol><ci id="S3.E6.m1.14.14.14.14.14.14.1.3.2.cmml" xref="S3.E6.m1.14.14.14.14.14.14.1.3.2">ğ‘‹</ci><ci id="S3.E6.m1.14.14.14.14.14.14.1.3.3a.cmml" xref="S3.E6.m1.14.14.14.14.14.14.1.3.3"><mtext mathsize="50%" id="S3.E6.m1.14.14.14.14.14.14.1.3.3.cmml" xref="S3.E6.m1.14.14.14.14.14.14.1.3.3">gt</mtext></ci></apply></apply></apply><apply id="S3.E6.m1.34.34.2.2.1.1.1.1.cmml" xref="S3.E6.m1.38.38.6"><csymbol cd="ambiguous" id="S3.E6.m1.34.34.2.2.1.1.1.1.2.cmml" xref="S3.E6.m1.38.38.6">subscript</csymbol><apply id="S3.E6.m1.34.34.2.2.1.1.1.1.1.2.cmml" xref="S3.E6.m1.38.38.6"><csymbol cd="latexml" id="S3.E6.m1.34.34.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.38.38.6">delimited-âˆ¥âˆ¥</csymbol><apply id="S3.E6.m1.34.34.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.38.38.6"><minus id="S3.E6.m1.17.17.17.17.17.17.cmml" xref="S3.E6.m1.17.17.17.17.17.17"></minus><apply id="S3.E6.m1.16.16.16.16.16.16.cmml" xref="S3.E6.m1.16.16.16.16.16.16"><ci id="S3.E6.m1.16.16.16.16.16.16.1.cmml" xref="S3.E6.m1.16.16.16.16.16.16.1">^</ci><ci id="S3.E6.m1.16.16.16.16.16.16.2.cmml" xref="S3.E6.m1.16.16.16.16.16.16.2">ğ‘¥</ci></apply><ci id="S3.E6.m1.18.18.18.18.18.18.cmml" xref="S3.E6.m1.18.18.18.18.18.18">ğ‘¥</ci></apply></apply><cn type="integer" id="S3.E6.m1.20.20.20.20.20.20.1.cmml" xref="S3.E6.m1.20.20.20.20.20.20.1">2</cn></apply></apply></apply></apply><apply id="S3.E6.m1.35.35.3.3.2.cmml" xref="S3.E6.m1.38.38.6"><times id="S3.E6.m1.35.35.3.3.2.2.cmml" xref="S3.E6.m1.38.38.6"></times><apply id="S3.E6.m1.22.22.22.2.2.2.cmml" xref="S3.E6.m1.22.22.22.2.2.2"><divide id="S3.E6.m1.22.22.22.2.2.2.2.cmml" xref="S3.E6.m1.22.22.22.2.2.2"></divide><cn type="integer" id="S3.E6.m1.22.22.22.2.2.2.3.cmml" xref="S3.E6.m1.22.22.22.2.2.2.3">1</cn><apply id="S3.E6.m1.22.22.22.2.2.2.1.2.cmml" xref="S3.E6.m1.22.22.22.2.2.2.1.1"><abs id="S3.E6.m1.22.22.22.2.2.2.1.2.1.cmml" xref="S3.E6.m1.22.22.22.2.2.2.1.1.2"></abs><apply id="S3.E6.m1.22.22.22.2.2.2.1.1.1.cmml" xref="S3.E6.m1.22.22.22.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.22.22.22.2.2.2.1.1.1.1.cmml" xref="S3.E6.m1.22.22.22.2.2.2.1.1.1">superscript</csymbol><ci id="S3.E6.m1.22.22.22.2.2.2.1.1.1.2.cmml" xref="S3.E6.m1.22.22.22.2.2.2.1.1.1.2">ğ‘‹</ci><ci id="S3.E6.m1.22.22.22.2.2.2.1.1.1.3a.cmml" xref="S3.E6.m1.22.22.22.2.2.2.1.1.1.3"><mtext mathsize="70%" id="S3.E6.m1.22.22.22.2.2.2.1.1.1.3.cmml" xref="S3.E6.m1.22.22.22.2.2.2.1.1.1.3">gt</mtext></ci></apply></apply></apply><apply id="S3.E6.m1.35.35.3.3.2.1.cmml" xref="S3.E6.m1.38.38.6"><apply id="S3.E6.m1.35.35.3.3.2.1.2.cmml" xref="S3.E6.m1.38.38.6"><csymbol cd="ambiguous" id="S3.E6.m1.35.35.3.3.2.1.2.1.cmml" xref="S3.E6.m1.38.38.6">subscript</csymbol><sum id="S3.E6.m1.23.23.23.3.3.3.cmml" xref="S3.E6.m1.23.23.23.3.3.3"></sum><apply id="S3.E6.m1.24.24.24.4.4.4.1.cmml" xref="S3.E6.m1.24.24.24.4.4.4.1"><in id="S3.E6.m1.24.24.24.4.4.4.1.1.cmml" xref="S3.E6.m1.24.24.24.4.4.4.1.1"></in><ci id="S3.E6.m1.24.24.24.4.4.4.1.2.cmml" xref="S3.E6.m1.24.24.24.4.4.4.1.2">ğ‘¥</ci><apply id="S3.E6.m1.24.24.24.4.4.4.1.3.cmml" xref="S3.E6.m1.24.24.24.4.4.4.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.24.24.24.4.4.4.1.3.1.cmml" xref="S3.E6.m1.24.24.24.4.4.4.1.3">superscript</csymbol><ci id="S3.E6.m1.24.24.24.4.4.4.1.3.2.cmml" xref="S3.E6.m1.24.24.24.4.4.4.1.3.2">ğ‘‹</ci><ci id="S3.E6.m1.24.24.24.4.4.4.1.3.3a.cmml" xref="S3.E6.m1.24.24.24.4.4.4.1.3.3"><mtext mathsize="50%" id="S3.E6.m1.24.24.24.4.4.4.1.3.3.cmml" xref="S3.E6.m1.24.24.24.4.4.4.1.3.3">gt</mtext></ci></apply></apply></apply><apply id="S3.E6.m1.35.35.3.3.2.1.1.cmml" xref="S3.E6.m1.38.38.6"><apply id="S3.E6.m1.35.35.3.3.2.1.1.2.cmml" xref="S3.E6.m1.38.38.6"><csymbol cd="ambiguous" id="S3.E6.m1.35.35.3.3.2.1.1.2.1.cmml" xref="S3.E6.m1.38.38.6">subscript</csymbol><min id="S3.E6.m1.25.25.25.5.5.5.cmml" xref="S3.E6.m1.25.25.25.5.5.5"></min><apply id="S3.E6.m1.26.26.26.6.6.6.1.cmml" xref="S3.E6.m1.26.26.26.6.6.6.1"><in id="S3.E6.m1.26.26.26.6.6.6.1.1.cmml" xref="S3.E6.m1.26.26.26.6.6.6.1.1"></in><apply id="S3.E6.m1.26.26.26.6.6.6.1.2.cmml" xref="S3.E6.m1.26.26.26.6.6.6.1.2"><ci id="S3.E6.m1.26.26.26.6.6.6.1.2.1.cmml" xref="S3.E6.m1.26.26.26.6.6.6.1.2.1">^</ci><ci id="S3.E6.m1.26.26.26.6.6.6.1.2.2.cmml" xref="S3.E6.m1.26.26.26.6.6.6.1.2.2">ğ‘¥</ci></apply><ci id="S3.E6.m1.26.26.26.6.6.6.1.3.cmml" xref="S3.E6.m1.26.26.26.6.6.6.1.3">ğ‘‹</ci></apply></apply><apply id="S3.E6.m1.35.35.3.3.2.1.1.1.cmml" xref="S3.E6.m1.38.38.6"><csymbol cd="ambiguous" id="S3.E6.m1.35.35.3.3.2.1.1.1.2.cmml" xref="S3.E6.m1.38.38.6">subscript</csymbol><apply id="S3.E6.m1.35.35.3.3.2.1.1.1.1.2.cmml" xref="S3.E6.m1.38.38.6"><csymbol cd="latexml" id="S3.E6.m1.35.35.3.3.2.1.1.1.1.2.1.cmml" xref="S3.E6.m1.38.38.6">delimited-âˆ¥âˆ¥</csymbol><apply id="S3.E6.m1.35.35.3.3.2.1.1.1.1.1.1.cmml" xref="S3.E6.m1.38.38.6"><minus id="S3.E6.m1.29.29.29.9.9.9.cmml" xref="S3.E6.m1.29.29.29.9.9.9"></minus><ci id="S3.E6.m1.28.28.28.8.8.8.cmml" xref="S3.E6.m1.28.28.28.8.8.8">ğ‘¥</ci><apply id="S3.E6.m1.30.30.30.10.10.10.cmml" xref="S3.E6.m1.30.30.30.10.10.10"><ci id="S3.E6.m1.30.30.30.10.10.10.1.cmml" xref="S3.E6.m1.30.30.30.10.10.10.1">^</ci><ci id="S3.E6.m1.30.30.30.10.10.10.2.cmml" xref="S3.E6.m1.30.30.30.10.10.10.2">ğ‘¥</ci></apply></apply></apply><cn type="integer" id="S3.E6.m1.32.32.32.12.12.12.1.cmml" xref="S3.E6.m1.32.32.32.12.12.12.1">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.38c">\displaystyle\begin{split}L_{\text{CD}}(X,X^{\text{gt}})=\frac{1}{|X|}\sum_{\hat{x}\in X}\min_{x\in X^{\text{gt}}}\|\hat{x}-x\|_{2}\\
+\frac{1}{|X^{\text{gt}}|}\sum_{x\in X^{\text{gt}}}\min_{\hat{x}\in X}\|x-\hat{x}\|_{2}\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Stage 2</h4>

<div id="S3.SS2.SSSx2.p1" class="ltx_para">
<p id="S3.SS2.SSSx2.p1.3" class="ltx_p">As features from previous shape training have captured some notions about pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, we train the pose decoder on top of those features while freezing the rest of the network weights. We train the pose decoder by minimizing the pose loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>:</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.5" class="ltx_Math" alttext="L_{P}(T,T^{\text{gt}};X^{\text{gt}})=\frac{1}{|X^{\text{gt}}|}\sum_{x\in X^{\text{gt}}}\lVert T^{-1}x-({T^{\text{gt}}})^{-1}x\rVert_{2}^{2}" display="block"><semantics id="S3.E7.m1.5a"><mrow id="S3.E7.m1.5.5" xref="S3.E7.m1.5.5.cmml"><mrow id="S3.E7.m1.4.4.2" xref="S3.E7.m1.4.4.2.cmml"><msub id="S3.E7.m1.4.4.2.4" xref="S3.E7.m1.4.4.2.4.cmml"><mi id="S3.E7.m1.4.4.2.4.2" xref="S3.E7.m1.4.4.2.4.2.cmml">L</mi><mi id="S3.E7.m1.4.4.2.4.3" xref="S3.E7.m1.4.4.2.4.3.cmml">P</mi></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.4.4.2.3" xref="S3.E7.m1.4.4.2.3.cmml">â€‹</mo><mrow id="S3.E7.m1.4.4.2.2.2" xref="S3.E7.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="S3.E7.m1.4.4.2.2.2.3" xref="S3.E7.m1.4.4.2.2.3.cmml">(</mo><mi id="S3.E7.m1.2.2" xref="S3.E7.m1.2.2.cmml">T</mi><mo id="S3.E7.m1.4.4.2.2.2.4" xref="S3.E7.m1.4.4.2.2.3.cmml">,</mo><msup id="S3.E7.m1.3.3.1.1.1.1" xref="S3.E7.m1.3.3.1.1.1.1.cmml"><mi id="S3.E7.m1.3.3.1.1.1.1.2" xref="S3.E7.m1.3.3.1.1.1.1.2.cmml">T</mi><mtext id="S3.E7.m1.3.3.1.1.1.1.3" xref="S3.E7.m1.3.3.1.1.1.1.3a.cmml">gt</mtext></msup><mo id="S3.E7.m1.4.4.2.2.2.5" xref="S3.E7.m1.4.4.2.2.3.cmml">;</mo><msup id="S3.E7.m1.4.4.2.2.2.2" xref="S3.E7.m1.4.4.2.2.2.2.cmml"><mi id="S3.E7.m1.4.4.2.2.2.2.2" xref="S3.E7.m1.4.4.2.2.2.2.2.cmml">X</mi><mtext id="S3.E7.m1.4.4.2.2.2.2.3" xref="S3.E7.m1.4.4.2.2.2.2.3a.cmml">gt</mtext></msup><mo stretchy="false" id="S3.E7.m1.4.4.2.2.2.6" xref="S3.E7.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.5.5.4" xref="S3.E7.m1.5.5.4.cmml">=</mo><mrow id="S3.E7.m1.5.5.3" xref="S3.E7.m1.5.5.3.cmml"><mfrac id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><mn id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml">1</mn><mrow id="S3.E7.m1.1.1.1.1" xref="S3.E7.m1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.2.1.cmml">|</mo><msup id="S3.E7.m1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.2.cmml">X</mi><mtext id="S3.E7.m1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.3a.cmml">gt</mtext></msup><mo stretchy="false" id="S3.E7.m1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E7.m1.5.5.3.2" xref="S3.E7.m1.5.5.3.2.cmml">â€‹</mo><mrow id="S3.E7.m1.5.5.3.1" xref="S3.E7.m1.5.5.3.1.cmml"><munder id="S3.E7.m1.5.5.3.1.2" xref="S3.E7.m1.5.5.3.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E7.m1.5.5.3.1.2.2" xref="S3.E7.m1.5.5.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E7.m1.5.5.3.1.2.3" xref="S3.E7.m1.5.5.3.1.2.3.cmml"><mi id="S3.E7.m1.5.5.3.1.2.3.2" xref="S3.E7.m1.5.5.3.1.2.3.2.cmml">x</mi><mo id="S3.E7.m1.5.5.3.1.2.3.1" xref="S3.E7.m1.5.5.3.1.2.3.1.cmml">âˆˆ</mo><msup id="S3.E7.m1.5.5.3.1.2.3.3" xref="S3.E7.m1.5.5.3.1.2.3.3.cmml"><mi id="S3.E7.m1.5.5.3.1.2.3.3.2" xref="S3.E7.m1.5.5.3.1.2.3.3.2.cmml">X</mi><mtext id="S3.E7.m1.5.5.3.1.2.3.3.3" xref="S3.E7.m1.5.5.3.1.2.3.3.3a.cmml">gt</mtext></msup></mrow></munder><msubsup id="S3.E7.m1.5.5.3.1.1" xref="S3.E7.m1.5.5.3.1.1.cmml"><mrow id="S3.E7.m1.5.5.3.1.1.1.1.1" xref="S3.E7.m1.5.5.3.1.1.1.1.2.cmml"><mo fence="true" lspace="0em" rspace="0em" id="S3.E7.m1.5.5.3.1.1.1.1.1.2" xref="S3.E7.m1.5.5.3.1.1.1.1.2.1.cmml">âˆ¥</mo><mrow id="S3.E7.m1.5.5.3.1.1.1.1.1.1" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.cmml"><mrow id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.cmml"><msup id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.2" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.2.cmml">T</mi><mrow id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3.cmml"><mo id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3a" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3.cmml">âˆ’</mo><mn id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3.2" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.1" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.3" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.3.cmml">x</mi></mrow><mo id="S3.E7.m1.5.5.3.1.1.1.1.1.1.2" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.2.cmml">âˆ’</mo><mrow id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.cmml"><msup id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">T</mi><mtext id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.3a.cmml">gt</mtext></msup><mo stretchy="false" id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3.cmml"><mo id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3a" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3.cmml">âˆ’</mo><mn id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3.2" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.2" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.3" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.3.cmml">x</mi></mrow></mrow><mo fence="true" lspace="0em" id="S3.E7.m1.5.5.3.1.1.1.1.1.3" xref="S3.E7.m1.5.5.3.1.1.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S3.E7.m1.5.5.3.1.1.1.3" xref="S3.E7.m1.5.5.3.1.1.1.3.cmml">2</mn><mn id="S3.E7.m1.5.5.3.1.1.3" xref="S3.E7.m1.5.5.3.1.1.3.cmml">2</mn></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.5b"><apply id="S3.E7.m1.5.5.cmml" xref="S3.E7.m1.5.5"><eq id="S3.E7.m1.5.5.4.cmml" xref="S3.E7.m1.5.5.4"></eq><apply id="S3.E7.m1.4.4.2.cmml" xref="S3.E7.m1.4.4.2"><times id="S3.E7.m1.4.4.2.3.cmml" xref="S3.E7.m1.4.4.2.3"></times><apply id="S3.E7.m1.4.4.2.4.cmml" xref="S3.E7.m1.4.4.2.4"><csymbol cd="ambiguous" id="S3.E7.m1.4.4.2.4.1.cmml" xref="S3.E7.m1.4.4.2.4">subscript</csymbol><ci id="S3.E7.m1.4.4.2.4.2.cmml" xref="S3.E7.m1.4.4.2.4.2">ğ¿</ci><ci id="S3.E7.m1.4.4.2.4.3.cmml" xref="S3.E7.m1.4.4.2.4.3">ğ‘ƒ</ci></apply><vector id="S3.E7.m1.4.4.2.2.3.cmml" xref="S3.E7.m1.4.4.2.2.2"><ci id="S3.E7.m1.2.2.cmml" xref="S3.E7.m1.2.2">ğ‘‡</ci><apply id="S3.E7.m1.3.3.1.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1">superscript</csymbol><ci id="S3.E7.m1.3.3.1.1.1.1.2.cmml" xref="S3.E7.m1.3.3.1.1.1.1.2">ğ‘‡</ci><ci id="S3.E7.m1.3.3.1.1.1.1.3a.cmml" xref="S3.E7.m1.3.3.1.1.1.1.3"><mtext mathsize="70%" id="S3.E7.m1.3.3.1.1.1.1.3.cmml" xref="S3.E7.m1.3.3.1.1.1.1.3">gt</mtext></ci></apply><apply id="S3.E7.m1.4.4.2.2.2.2.cmml" xref="S3.E7.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.4.4.2.2.2.2.1.cmml" xref="S3.E7.m1.4.4.2.2.2.2">superscript</csymbol><ci id="S3.E7.m1.4.4.2.2.2.2.2.cmml" xref="S3.E7.m1.4.4.2.2.2.2.2">ğ‘‹</ci><ci id="S3.E7.m1.4.4.2.2.2.2.3a.cmml" xref="S3.E7.m1.4.4.2.2.2.2.3"><mtext mathsize="70%" id="S3.E7.m1.4.4.2.2.2.2.3.cmml" xref="S3.E7.m1.4.4.2.2.2.2.3">gt</mtext></ci></apply></vector></apply><apply id="S3.E7.m1.5.5.3.cmml" xref="S3.E7.m1.5.5.3"><times id="S3.E7.m1.5.5.3.2.cmml" xref="S3.E7.m1.5.5.3.2"></times><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><divide id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1"></divide><cn type="integer" id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3">1</cn><apply id="S3.E7.m1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1"><abs id="S3.E7.m1.1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.1.1.2"></abs><apply id="S3.E7.m1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1">superscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.2">ğ‘‹</ci><ci id="S3.E7.m1.1.1.1.1.1.3a.cmml" xref="S3.E7.m1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E7.m1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.3">gt</mtext></ci></apply></apply></apply><apply id="S3.E7.m1.5.5.3.1.cmml" xref="S3.E7.m1.5.5.3.1"><apply id="S3.E7.m1.5.5.3.1.2.cmml" xref="S3.E7.m1.5.5.3.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.5.5.3.1.2.1.cmml" xref="S3.E7.m1.5.5.3.1.2">subscript</csymbol><sum id="S3.E7.m1.5.5.3.1.2.2.cmml" xref="S3.E7.m1.5.5.3.1.2.2"></sum><apply id="S3.E7.m1.5.5.3.1.2.3.cmml" xref="S3.E7.m1.5.5.3.1.2.3"><in id="S3.E7.m1.5.5.3.1.2.3.1.cmml" xref="S3.E7.m1.5.5.3.1.2.3.1"></in><ci id="S3.E7.m1.5.5.3.1.2.3.2.cmml" xref="S3.E7.m1.5.5.3.1.2.3.2">ğ‘¥</ci><apply id="S3.E7.m1.5.5.3.1.2.3.3.cmml" xref="S3.E7.m1.5.5.3.1.2.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.5.5.3.1.2.3.3.1.cmml" xref="S3.E7.m1.5.5.3.1.2.3.3">superscript</csymbol><ci id="S3.E7.m1.5.5.3.1.2.3.3.2.cmml" xref="S3.E7.m1.5.5.3.1.2.3.3.2">ğ‘‹</ci><ci id="S3.E7.m1.5.5.3.1.2.3.3.3a.cmml" xref="S3.E7.m1.5.5.3.1.2.3.3.3"><mtext mathsize="50%" id="S3.E7.m1.5.5.3.1.2.3.3.3.cmml" xref="S3.E7.m1.5.5.3.1.2.3.3.3">gt</mtext></ci></apply></apply></apply><apply id="S3.E7.m1.5.5.3.1.1.cmml" xref="S3.E7.m1.5.5.3.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.5.5.3.1.1.2.cmml" xref="S3.E7.m1.5.5.3.1.1">superscript</csymbol><apply id="S3.E7.m1.5.5.3.1.1.1.cmml" xref="S3.E7.m1.5.5.3.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.5.5.3.1.1.1.2.cmml" xref="S3.E7.m1.5.5.3.1.1">subscript</csymbol><apply id="S3.E7.m1.5.5.3.1.1.1.1.2.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E7.m1.5.5.3.1.1.1.1.2.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S3.E7.m1.5.5.3.1.1.1.1.1.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1"><minus id="S3.E7.m1.5.5.3.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.2"></minus><apply id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3"><times id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.1"></times><apply id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2">superscript</csymbol><ci id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.2">ğ‘‡</ci><apply id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3"><minus id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3"></minus><cn type="integer" id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3.2.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.2.3.2">1</cn></apply></apply><ci id="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.3.3">ğ‘¥</ci></apply><apply id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1"><times id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.2"></times><apply id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘‡</ci><ci id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.1.1.1.3">gt</mtext></ci></apply><apply id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3"><minus id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3"></minus><cn type="integer" id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.1.3.2">1</cn></apply></apply><ci id="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.5.5.3.1.1.1.1.1.1.1.3">ğ‘¥</ci></apply></apply></apply><cn type="integer" id="S3.E7.m1.5.5.3.1.1.1.3.cmml" xref="S3.E7.m1.5.5.3.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E7.m1.5.5.3.1.1.3.cmml" xref="S3.E7.m1.5.5.3.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.5c">L_{P}(T,T^{\text{gt}};X^{\text{gt}})=\frac{1}{|X^{\text{gt}}|}\sum_{x\in X^{\text{gt}}}\lVert T^{-1}x-({T^{\text{gt}}})^{-1}x\rVert_{2}^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSSx2.p1.2" class="ltx_p">where <math id="S3.SS2.SSSx2.p1.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.SSSx2.p1.1.m1.1a"><mi id="S3.SS2.SSSx2.p1.1.m1.1.1" xref="S3.SS2.SSSx2.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx2.p1.1.m1.1b"><ci id="S3.SS2.SSSx2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSSx2.p1.1.m1.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx2.p1.1.m1.1c">T</annotation></semantics></math> and <math id="S3.SS2.SSSx2.p1.2.m2.1" class="ltx_Math" alttext="T^{\text{gt}}" display="inline"><semantics id="S3.SS2.SSSx2.p1.2.m2.1a"><msup id="S3.SS2.SSSx2.p1.2.m2.1.1" xref="S3.SS2.SSSx2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSSx2.p1.2.m2.1.1.2" xref="S3.SS2.SSSx2.p1.2.m2.1.1.2.cmml">T</mi><mtext id="S3.SS2.SSSx2.p1.2.m2.1.1.3" xref="S3.SS2.SSSx2.p1.2.m2.1.1.3a.cmml">gt</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx2.p1.2.m2.1b"><apply id="S3.SS2.SSSx2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSSx2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSSx2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSSx2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.SSSx2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSSx2.p1.2.m2.1.1.2">ğ‘‡</ci><ci id="S3.SS2.SSSx2.p1.2.m2.1.1.3a.cmml" xref="S3.SS2.SSSx2.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSSx2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSSx2.p1.2.m2.1.1.3">gt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx2.p1.2.m2.1c">T^{\text{gt}}</annotation></semantics></math> are the estimated and ground truth transformations.
This loss function penalizes both the translation and rotation errors.</p>
</div>
</section>
<section id="S3.SS2.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Stage 3</h4>

<div id="S3.SS2.SSSx3.p1" class="ltx_para">
<p id="S3.SS2.SSSx3.p1.1" class="ltx_p">Finally, we jointly train the entire network for sequential shape and pose estimation using the following joint loss <math id="S3.SS2.SSSx3.p1.1.m1.1" class="ltx_Math" alttext="L_{\text{J}}" display="inline"><semantics id="S3.SS2.SSSx3.p1.1.m1.1a"><msub id="S3.SS2.SSSx3.p1.1.m1.1.1" xref="S3.SS2.SSSx3.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSSx3.p1.1.m1.1.1.2" xref="S3.SS2.SSSx3.p1.1.m1.1.1.2.cmml">L</mi><mtext id="S3.SS2.SSSx3.p1.1.m1.1.1.3" xref="S3.SS2.SSSx3.p1.1.m1.1.1.3a.cmml">J</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx3.p1.1.m1.1b"><apply id="S3.SS2.SSSx3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSSx3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSSx3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSSx3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSSx3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSSx3.p1.1.m1.1.1.2">ğ¿</ci><ci id="S3.SS2.SSSx3.p1.1.m1.1.1.3a.cmml" xref="S3.SS2.SSSx3.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSSx3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSSx3.p1.1.m1.1.1.3">J</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx3.p1.1.m1.1c">L_{\text{J}}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>:</p>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.2" class="ltx_Math" alttext="L_{\text{J}}=\frac{1}{2\sigma_{\text{CD}}^{2}}L_{\text{CD}}+\frac{1}{2\sigma_{\text{P}}^{2}}L_{\text{P}}+\log({\sigma_{\text{CD}}\sigma_{\text{P}}})" display="block"><semantics id="S3.E8.m1.2a"><mrow id="S3.E8.m1.2.2" xref="S3.E8.m1.2.2.cmml"><msub id="S3.E8.m1.2.2.3" xref="S3.E8.m1.2.2.3.cmml"><mi id="S3.E8.m1.2.2.3.2" xref="S3.E8.m1.2.2.3.2.cmml">L</mi><mtext id="S3.E8.m1.2.2.3.3" xref="S3.E8.m1.2.2.3.3a.cmml">J</mtext></msub><mo id="S3.E8.m1.2.2.2" xref="S3.E8.m1.2.2.2.cmml">=</mo><mrow id="S3.E8.m1.2.2.1" xref="S3.E8.m1.2.2.1.cmml"><mrow id="S3.E8.m1.2.2.1.3" xref="S3.E8.m1.2.2.1.3.cmml"><mfrac id="S3.E8.m1.2.2.1.3.2" xref="S3.E8.m1.2.2.1.3.2.cmml"><mn id="S3.E8.m1.2.2.1.3.2.2" xref="S3.E8.m1.2.2.1.3.2.2.cmml">1</mn><mrow id="S3.E8.m1.2.2.1.3.2.3" xref="S3.E8.m1.2.2.1.3.2.3.cmml"><mn id="S3.E8.m1.2.2.1.3.2.3.2" xref="S3.E8.m1.2.2.1.3.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.1.3.2.3.1" xref="S3.E8.m1.2.2.1.3.2.3.1.cmml">â€‹</mo><msubsup id="S3.E8.m1.2.2.1.3.2.3.3" xref="S3.E8.m1.2.2.1.3.2.3.3.cmml"><mi id="S3.E8.m1.2.2.1.3.2.3.3.2.2" xref="S3.E8.m1.2.2.1.3.2.3.3.2.2.cmml">Ïƒ</mi><mtext id="S3.E8.m1.2.2.1.3.2.3.3.2.3" xref="S3.E8.m1.2.2.1.3.2.3.3.2.3a.cmml">CD</mtext><mn id="S3.E8.m1.2.2.1.3.2.3.3.3" xref="S3.E8.m1.2.2.1.3.2.3.3.3.cmml">2</mn></msubsup></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.1.3.1" xref="S3.E8.m1.2.2.1.3.1.cmml">â€‹</mo><msub id="S3.E8.m1.2.2.1.3.3" xref="S3.E8.m1.2.2.1.3.3.cmml"><mi id="S3.E8.m1.2.2.1.3.3.2" xref="S3.E8.m1.2.2.1.3.3.2.cmml">L</mi><mtext id="S3.E8.m1.2.2.1.3.3.3" xref="S3.E8.m1.2.2.1.3.3.3a.cmml">CD</mtext></msub></mrow><mo id="S3.E8.m1.2.2.1.2" xref="S3.E8.m1.2.2.1.2.cmml">+</mo><mrow id="S3.E8.m1.2.2.1.4" xref="S3.E8.m1.2.2.1.4.cmml"><mfrac id="S3.E8.m1.2.2.1.4.2" xref="S3.E8.m1.2.2.1.4.2.cmml"><mn id="S3.E8.m1.2.2.1.4.2.2" xref="S3.E8.m1.2.2.1.4.2.2.cmml">1</mn><mrow id="S3.E8.m1.2.2.1.4.2.3" xref="S3.E8.m1.2.2.1.4.2.3.cmml"><mn id="S3.E8.m1.2.2.1.4.2.3.2" xref="S3.E8.m1.2.2.1.4.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.1.4.2.3.1" xref="S3.E8.m1.2.2.1.4.2.3.1.cmml">â€‹</mo><msubsup id="S3.E8.m1.2.2.1.4.2.3.3" xref="S3.E8.m1.2.2.1.4.2.3.3.cmml"><mi id="S3.E8.m1.2.2.1.4.2.3.3.2.2" xref="S3.E8.m1.2.2.1.4.2.3.3.2.2.cmml">Ïƒ</mi><mtext id="S3.E8.m1.2.2.1.4.2.3.3.2.3" xref="S3.E8.m1.2.2.1.4.2.3.3.2.3a.cmml">P</mtext><mn id="S3.E8.m1.2.2.1.4.2.3.3.3" xref="S3.E8.m1.2.2.1.4.2.3.3.3.cmml">2</mn></msubsup></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.1.4.1" xref="S3.E8.m1.2.2.1.4.1.cmml">â€‹</mo><msub id="S3.E8.m1.2.2.1.4.3" xref="S3.E8.m1.2.2.1.4.3.cmml"><mi id="S3.E8.m1.2.2.1.4.3.2" xref="S3.E8.m1.2.2.1.4.3.2.cmml">L</mi><mtext id="S3.E8.m1.2.2.1.4.3.3" xref="S3.E8.m1.2.2.1.4.3.3a.cmml">P</mtext></msub></mrow><mo id="S3.E8.m1.2.2.1.2a" xref="S3.E8.m1.2.2.1.2.cmml">+</mo><mrow id="S3.E8.m1.2.2.1.1.1" xref="S3.E8.m1.2.2.1.1.2.cmml"><mi id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml">log</mi><mo id="S3.E8.m1.2.2.1.1.1a" xref="S3.E8.m1.2.2.1.1.2.cmml">â¡</mo><mrow id="S3.E8.m1.2.2.1.1.1.1" xref="S3.E8.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E8.m1.2.2.1.1.1.1.2" xref="S3.E8.m1.2.2.1.1.2.cmml">(</mo><mrow id="S3.E8.m1.2.2.1.1.1.1.1" xref="S3.E8.m1.2.2.1.1.1.1.1.cmml"><msub id="S3.E8.m1.2.2.1.1.1.1.1.2" xref="S3.E8.m1.2.2.1.1.1.1.1.2.cmml"><mi id="S3.E8.m1.2.2.1.1.1.1.1.2.2" xref="S3.E8.m1.2.2.1.1.1.1.1.2.2.cmml">Ïƒ</mi><mtext id="S3.E8.m1.2.2.1.1.1.1.1.2.3" xref="S3.E8.m1.2.2.1.1.1.1.1.2.3a.cmml">CD</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.1.1.1.1.1.1" xref="S3.E8.m1.2.2.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.E8.m1.2.2.1.1.1.1.1.3" xref="S3.E8.m1.2.2.1.1.1.1.1.3.cmml"><mi id="S3.E8.m1.2.2.1.1.1.1.1.3.2" xref="S3.E8.m1.2.2.1.1.1.1.1.3.2.cmml">Ïƒ</mi><mtext id="S3.E8.m1.2.2.1.1.1.1.1.3.3" xref="S3.E8.m1.2.2.1.1.1.1.1.3.3a.cmml">P</mtext></msub></mrow><mo stretchy="false" id="S3.E8.m1.2.2.1.1.1.1.3" xref="S3.E8.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.2b"><apply id="S3.E8.m1.2.2.cmml" xref="S3.E8.m1.2.2"><eq id="S3.E8.m1.2.2.2.cmml" xref="S3.E8.m1.2.2.2"></eq><apply id="S3.E8.m1.2.2.3.cmml" xref="S3.E8.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.3.1.cmml" xref="S3.E8.m1.2.2.3">subscript</csymbol><ci id="S3.E8.m1.2.2.3.2.cmml" xref="S3.E8.m1.2.2.3.2">ğ¿</ci><ci id="S3.E8.m1.2.2.3.3a.cmml" xref="S3.E8.m1.2.2.3.3"><mtext mathsize="70%" id="S3.E8.m1.2.2.3.3.cmml" xref="S3.E8.m1.2.2.3.3">J</mtext></ci></apply><apply id="S3.E8.m1.2.2.1.cmml" xref="S3.E8.m1.2.2.1"><plus id="S3.E8.m1.2.2.1.2.cmml" xref="S3.E8.m1.2.2.1.2"></plus><apply id="S3.E8.m1.2.2.1.3.cmml" xref="S3.E8.m1.2.2.1.3"><times id="S3.E8.m1.2.2.1.3.1.cmml" xref="S3.E8.m1.2.2.1.3.1"></times><apply id="S3.E8.m1.2.2.1.3.2.cmml" xref="S3.E8.m1.2.2.1.3.2"><divide id="S3.E8.m1.2.2.1.3.2.1.cmml" xref="S3.E8.m1.2.2.1.3.2"></divide><cn type="integer" id="S3.E8.m1.2.2.1.3.2.2.cmml" xref="S3.E8.m1.2.2.1.3.2.2">1</cn><apply id="S3.E8.m1.2.2.1.3.2.3.cmml" xref="S3.E8.m1.2.2.1.3.2.3"><times id="S3.E8.m1.2.2.1.3.2.3.1.cmml" xref="S3.E8.m1.2.2.1.3.2.3.1"></times><cn type="integer" id="S3.E8.m1.2.2.1.3.2.3.2.cmml" xref="S3.E8.m1.2.2.1.3.2.3.2">2</cn><apply id="S3.E8.m1.2.2.1.3.2.3.3.cmml" xref="S3.E8.m1.2.2.1.3.2.3.3"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.1.3.2.3.3.1.cmml" xref="S3.E8.m1.2.2.1.3.2.3.3">superscript</csymbol><apply id="S3.E8.m1.2.2.1.3.2.3.3.2.cmml" xref="S3.E8.m1.2.2.1.3.2.3.3"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.1.3.2.3.3.2.1.cmml" xref="S3.E8.m1.2.2.1.3.2.3.3">subscript</csymbol><ci id="S3.E8.m1.2.2.1.3.2.3.3.2.2.cmml" xref="S3.E8.m1.2.2.1.3.2.3.3.2.2">ğœ</ci><ci id="S3.E8.m1.2.2.1.3.2.3.3.2.3a.cmml" xref="S3.E8.m1.2.2.1.3.2.3.3.2.3"><mtext mathsize="70%" id="S3.E8.m1.2.2.1.3.2.3.3.2.3.cmml" xref="S3.E8.m1.2.2.1.3.2.3.3.2.3">CD</mtext></ci></apply><cn type="integer" id="S3.E8.m1.2.2.1.3.2.3.3.3.cmml" xref="S3.E8.m1.2.2.1.3.2.3.3.3">2</cn></apply></apply></apply><apply id="S3.E8.m1.2.2.1.3.3.cmml" xref="S3.E8.m1.2.2.1.3.3"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.1.3.3.1.cmml" xref="S3.E8.m1.2.2.1.3.3">subscript</csymbol><ci id="S3.E8.m1.2.2.1.3.3.2.cmml" xref="S3.E8.m1.2.2.1.3.3.2">ğ¿</ci><ci id="S3.E8.m1.2.2.1.3.3.3a.cmml" xref="S3.E8.m1.2.2.1.3.3.3"><mtext mathsize="70%" id="S3.E8.m1.2.2.1.3.3.3.cmml" xref="S3.E8.m1.2.2.1.3.3.3">CD</mtext></ci></apply></apply><apply id="S3.E8.m1.2.2.1.4.cmml" xref="S3.E8.m1.2.2.1.4"><times id="S3.E8.m1.2.2.1.4.1.cmml" xref="S3.E8.m1.2.2.1.4.1"></times><apply id="S3.E8.m1.2.2.1.4.2.cmml" xref="S3.E8.m1.2.2.1.4.2"><divide id="S3.E8.m1.2.2.1.4.2.1.cmml" xref="S3.E8.m1.2.2.1.4.2"></divide><cn type="integer" id="S3.E8.m1.2.2.1.4.2.2.cmml" xref="S3.E8.m1.2.2.1.4.2.2">1</cn><apply id="S3.E8.m1.2.2.1.4.2.3.cmml" xref="S3.E8.m1.2.2.1.4.2.3"><times id="S3.E8.m1.2.2.1.4.2.3.1.cmml" xref="S3.E8.m1.2.2.1.4.2.3.1"></times><cn type="integer" id="S3.E8.m1.2.2.1.4.2.3.2.cmml" xref="S3.E8.m1.2.2.1.4.2.3.2">2</cn><apply id="S3.E8.m1.2.2.1.4.2.3.3.cmml" xref="S3.E8.m1.2.2.1.4.2.3.3"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.1.4.2.3.3.1.cmml" xref="S3.E8.m1.2.2.1.4.2.3.3">superscript</csymbol><apply id="S3.E8.m1.2.2.1.4.2.3.3.2.cmml" xref="S3.E8.m1.2.2.1.4.2.3.3"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.1.4.2.3.3.2.1.cmml" xref="S3.E8.m1.2.2.1.4.2.3.3">subscript</csymbol><ci id="S3.E8.m1.2.2.1.4.2.3.3.2.2.cmml" xref="S3.E8.m1.2.2.1.4.2.3.3.2.2">ğœ</ci><ci id="S3.E8.m1.2.2.1.4.2.3.3.2.3a.cmml" xref="S3.E8.m1.2.2.1.4.2.3.3.2.3"><mtext mathsize="70%" id="S3.E8.m1.2.2.1.4.2.3.3.2.3.cmml" xref="S3.E8.m1.2.2.1.4.2.3.3.2.3">P</mtext></ci></apply><cn type="integer" id="S3.E8.m1.2.2.1.4.2.3.3.3.cmml" xref="S3.E8.m1.2.2.1.4.2.3.3.3">2</cn></apply></apply></apply><apply id="S3.E8.m1.2.2.1.4.3.cmml" xref="S3.E8.m1.2.2.1.4.3"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.1.4.3.1.cmml" xref="S3.E8.m1.2.2.1.4.3">subscript</csymbol><ci id="S3.E8.m1.2.2.1.4.3.2.cmml" xref="S3.E8.m1.2.2.1.4.3.2">ğ¿</ci><ci id="S3.E8.m1.2.2.1.4.3.3a.cmml" xref="S3.E8.m1.2.2.1.4.3.3"><mtext mathsize="70%" id="S3.E8.m1.2.2.1.4.3.3.cmml" xref="S3.E8.m1.2.2.1.4.3.3">P</mtext></ci></apply></apply><apply id="S3.E8.m1.2.2.1.1.2.cmml" xref="S3.E8.m1.2.2.1.1.1"><log id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1"></log><apply id="S3.E8.m1.2.2.1.1.1.1.1.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1"><times id="S3.E8.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.1"></times><apply id="S3.E8.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E8.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.2.2">ğœ</ci><ci id="S3.E8.m1.2.2.1.1.1.1.1.2.3a.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.E8.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.2.3">CD</mtext></ci></apply><apply id="S3.E8.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E8.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.3.2">ğœ</ci><ci id="S3.E8.m1.2.2.1.1.1.1.1.3.3a.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E8.m1.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E8.m1.2.2.1.1.1.1.1.3.3">P</mtext></ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.2c">L_{\text{J}}=\frac{1}{2\sigma_{\text{CD}}^{2}}L_{\text{CD}}+\frac{1}{2\sigma_{\text{P}}^{2}}L_{\text{P}}+\log({\sigma_{\text{CD}}\sigma_{\text{P}}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSSx3.p1.3" class="ltx_p">where <math id="S3.SS2.SSSx3.p1.2.m1.1" class="ltx_Math" alttext="\sigma_{\text{CD}}" display="inline"><semantics id="S3.SS2.SSSx3.p1.2.m1.1a"><msub id="S3.SS2.SSSx3.p1.2.m1.1.1" xref="S3.SS2.SSSx3.p1.2.m1.1.1.cmml"><mi id="S3.SS2.SSSx3.p1.2.m1.1.1.2" xref="S3.SS2.SSSx3.p1.2.m1.1.1.2.cmml">Ïƒ</mi><mtext id="S3.SS2.SSSx3.p1.2.m1.1.1.3" xref="S3.SS2.SSSx3.p1.2.m1.1.1.3a.cmml">CD</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx3.p1.2.m1.1b"><apply id="S3.SS2.SSSx3.p1.2.m1.1.1.cmml" xref="S3.SS2.SSSx3.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSSx3.p1.2.m1.1.1.1.cmml" xref="S3.SS2.SSSx3.p1.2.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSSx3.p1.2.m1.1.1.2.cmml" xref="S3.SS2.SSSx3.p1.2.m1.1.1.2">ğœ</ci><ci id="S3.SS2.SSSx3.p1.2.m1.1.1.3a.cmml" xref="S3.SS2.SSSx3.p1.2.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSSx3.p1.2.m1.1.1.3.cmml" xref="S3.SS2.SSSx3.p1.2.m1.1.1.3">CD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx3.p1.2.m1.1c">\sigma_{\text{CD}}</annotation></semantics></math> and <math id="S3.SS2.SSSx3.p1.3.m2.1" class="ltx_Math" alttext="\sigma_{\text{P}}" display="inline"><semantics id="S3.SS2.SSSx3.p1.3.m2.1a"><msub id="S3.SS2.SSSx3.p1.3.m2.1.1" xref="S3.SS2.SSSx3.p1.3.m2.1.1.cmml"><mi id="S3.SS2.SSSx3.p1.3.m2.1.1.2" xref="S3.SS2.SSSx3.p1.3.m2.1.1.2.cmml">Ïƒ</mi><mtext id="S3.SS2.SSSx3.p1.3.m2.1.1.3" xref="S3.SS2.SSSx3.p1.3.m2.1.1.3a.cmml">P</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx3.p1.3.m2.1b"><apply id="S3.SS2.SSSx3.p1.3.m2.1.1.cmml" xref="S3.SS2.SSSx3.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSSx3.p1.3.m2.1.1.1.cmml" xref="S3.SS2.SSSx3.p1.3.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSSx3.p1.3.m2.1.1.2.cmml" xref="S3.SS2.SSSx3.p1.3.m2.1.1.2">ğœ</ci><ci id="S3.SS2.SSSx3.p1.3.m2.1.1.3a.cmml" xref="S3.SS2.SSSx3.p1.3.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSSx3.p1.3.m2.1.1.3.cmml" xref="S3.SS2.SSSx3.p1.3.m2.1.1.3">P</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx3.p1.3.m2.1c">\sigma_{\text{P}}</annotation></semantics></math> are learnable parameters, denoting the uncertainty of each prediction task. By training the two tasks simultaneously, learning in one
task benefits the other, since the tasks are coupled. Moreover, the network can learn a more general representation capturing both tasks and is less likely to overfit.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Automatic Amodal Segmentation Labeling</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We extend our sequential shape and pose estimation algorithm in point clouds
to automatic amodal segmentation labeling in images, building on the following key observation.
An object instance commonly appears in multiple frames, as driving data is collected in a continuous sequence. Thus, a part of an amodal mask occluded in one frame may be visible in other frames. If we can fuse the information from multiple frames, we can produce amodal masks that are consistent across time frames.
One naive way is to fuse 2D inmodal masks in the image space â€” 2D inmodal masks are easier to annotate or can be obtained from existing segmentation algorithms like Mask-RCNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. This method, however, has several challenges. First, an objectâ€™s 2D size and shape vary across frames depending on its relative pose to the camera. Second, the mask shape is also influenced by the objectâ€™s 3D orientation, a piece of information that is missing in 2D. Third, while multiple frames provide extra information about an object, some parts may never be seen in any frames.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">To solve these issues, we propose to fuse the information over time in 3D by taking advantage of 1) the 3D LiDAR signal that is commonly available in the autonomous driving configuration, and 2) our learned network for shape completion that encodes the prior of complete shapes from training data.
We feed the point cloud sequence into our sequential point completion network in order to obtain the full amodal information of the object. The complete point cloud is then projected to the image space and post-processed to get an alpha shape as the amodal mask label. Finally, the occlusion ordering is reasoned from depth information.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We present two different scenarios where our automatic labeling pipeline can be used. The first assumes the availability of 3D bounding box ground truth and association, which are typically available on a public driving dataset, on the data to be labeled. In this case, the 3D bounding boxes are used to segment the point cloud objects to get a sequence of partial point cloud objects containing only the target vehicle. Given a segmented point cloud sequence and ground truth transformations, one can simply accumulate these segmented point clouds using the ground truth transformations and use the symmetrical property of vehicles to mirror the point cloud with respect to its heading axis in order to obtain the amodal shape information. However, this may still result in incomplete amodal information for many cases, as will be shown in <a href="#S5.SS3" title="V-C Automatic Labeling of Amodal Segmentation on KITTI â€£ V Experiments â€£ Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsectionÂ <span class="ltx_text">V-C</span></span></a>. So instead, we leverage our sequential point completion network to fuse and complete the amodal information. As our goal is labeling, we can train and evaluate the network on the same dataset.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">In the second scenario, we handle situations where 3D bounding box labels are unavailable. In this case, we deploy a pretrained Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> inmodal instance segmentation and LDLS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> (a 2D to 3D label diffusion framework) to segment the point cloud, followed by data association. Since 3D bounding box labels are not available, we deploy our sequential completion network that is pretrained on another dataset, e.g., synthetic dataset.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We first evaluate the quality of the completed point cloud and pose estimates on the synthetic and Argoverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> real dataset. The translation and rotation errors are computed to evaluate the pose estimate quality. Besides Chamfer Distance (CD), Earth Mover Distance (EMD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> is also computed to evaluate the completed point cloud through different aspects. The CD captures the global structure of the point cloud, while the EMD captures the point density.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Next, we evaluate our automatic amodal labeling pipeline on the KITTI tracking dataset, comparing our generated masks to the manually annotated labels from KINS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> by computing the mean intersection over union (mIoU), as well as % miss, i.e., the percentage of instances that do not have a matching or IoU <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.p2.1.m1.1a"><mo id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><lt id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">&lt;</annotation></semantics></math> 0.5.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Data Generation</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The synthetic dataset can generate perfect and complete point clouds required for training and evaluation. However, the synthetic dataset does not contain real-world challenges such as occlusions, contamination from ground points, LiDAR time synchronization issues, and data mislabeling. Thus, we complement our analysis by training and evaluating on the real-world Argoverse dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</p>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS1.5.1.1" class="ltx_text">V-A</span>1 </span>Synthetic Dataset</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">We obtain 183 vehicle CAD models from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> and randomly split them to 168/15 for training and validation. Each CAD model is fit into 11 different trajectories, where each may contain a different number of frames, resulting in 1848/165 tracks for training and validation. To mimic the real-world behavior of vehicles, we use the trajectory log from the Argoverse dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
To simulate partial point cloud measurements, we transform the CAD models to the desired poses in the track and apply raytracing using the Trimesh library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. We generate a synthetic dataset with VLP-16 LiDAR configuration placed at a 2m height from the ground. To generate a complete point cloud shape, we use Open3D library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> to sample points from the CAD models and remove interior points that are not visible from the outside.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS2.5.1.1" class="ltx_text">V-A</span>2 </span>Argoverse</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">We evaluate our results on car objects in the Argoverse tracking dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> split into 2298/638 tracks for training and validation, where each track contains an average of 70 frames. As we do not focus on segmentation and data association problems, ground truth bounding boxes and track ids are used to crop the LiDAR scene.</p>
</div>
<div id="S5.SS1.SSS2.p2" class="ltx_para">
<p id="S5.SS1.SSS2.p2.1" class="ltx_p">While ground truth pose is provided by Argoverse, complete point clouds are not available. Thus, to acquire a shape reference, LiDAR point clouds of vehicles are accumulated over multiple frames. Furthermore, since vehicles are usually symmetrical about their heading axes, we mirror the points along the symmetrical heading axes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
Note that even after the LiDAR point clouds are accumulated and mirrored, the developed shape reference may still be non-ideal (e.g., displaying noise, inaccuracy, incompleteness, and non-uniform point density), as it is built from measurements.
As EMD is highly sensitive to these properties, evaluating EMD against such point cloud reference is not meaningful. Thus, we do not report the EMD for Argoverse evaluation.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Shape and Pose Estimation</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We run our method recursively from each vehicleâ€™s initial to final detection. The shape and pose errors are evaluated at each time step.
We compare our method to the joint shape and pose estimation from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> as a baseline. Different from our proposed approach, the baseline makes a prediction on each frame independently.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS1.5.1.1" class="ltx_text">V-B</span>1 </span>Results on Synthetic Dataset</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p"><a href="#S5.T1" title="TABLE I â€£ V-B1 Results on Synthetic Dataset â€£ V-B Shape and Pose Estimation â€£ V Experiments â€£ Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ I</span></a> shows the
sequential shape and pose estimation results on the synthetic dataset. Our method outperforms the baseline in both shape completion and pose estimation tasks. While CD is the direct metric for training, EMD quantifies the quality of point density. The reported EMD value is much higher than CD due to requiring one-to-one correspondences between points. Nevertheless, our method still outperforms the baseline even on the EMD metric.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Quantitative result on the synthetic dataset.</figcaption>
<table id="S5.T1.2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.2.2.3.1" class="ltx_tr">
<td id="S5.T1.2.2.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T1.2.2.3.1.1.1" class="ltx_text">Method</span></td>
<td id="S5.T1.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">Shape Error</td>
<td id="S5.T1.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">Pose Error</td>
</tr>
<tr id="S5.T1.2.2.4.2" class="ltx_tr">
<td id="S5.T1.2.2.4.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CD</td>
<td id="S5.T1.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EMD</td>
<td id="S5.T1.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Translation</td>
<td id="S5.T1.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Rotation</td>
</tr>
<tr id="S5.T1.1.1.1" class="ltx_tr">
<td id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Baseline</td>
<td id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.1 cm</td>
<td id="S5.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.50 m</td>
<td id="S5.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.2 cm</td>
<td id="S5.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.2<sup id="S5.T1.1.1.1.1.1" class="ltx_sup"><span id="S5.T1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">o</span></sup>
</td>
</tr>
<tr id="S5.T1.2.2.2" class="ltx_tr">
<td id="S5.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Ours</td>
<td id="S5.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2.3 cm</td>
<td id="S5.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.25 m</td>
<td id="S5.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">9.4 cm</td>
<td id="S5.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">12.0<sup id="S5.T1.2.2.2.1.1" class="ltx_sup"><span id="S5.T1.2.2.2.1.1.1" class="ltx_text ltx_font_italic">o</span></sup>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2109.09840/assets/figs/Diagram4.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="247" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Qualitative shape completion results on a track from the synthetic data over time. Top: Ours. Bottom: Baseline. </figcaption>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2109.09840/assets/figs/argo_updated.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="521" height="864" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Statistical error analysis on the shape and pose accuracy as a function of number of detections on the Argoverse dataset. Lower is better. </figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2109.09840/assets/figs/amodal-comparison.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1148" height="350" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Amodal mask labels from our proposed pipelines. Top: GT-Accumulation. Bottom: GT-SC. The masks inside the red boxes highlight the benefit of using our sequential completion network. </figcaption>
</figure>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p">To complement the quantitative analysis, shape estimate results at three different time steps are shown in <a href="#S5.F2" title="Figure 2 â€£ V-B1 Results on Synthetic Dataset â€£ V-B Shape and Pose Estimation â€£ V Experiments â€£ Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 2</span></a>. In this case, the sensor data is initially (frame 1) very sparse. As a result, both methods do not produce accurate shape estimates; they also mispredict the carâ€™s orientation by roughly 180<sup id="S5.SS2.SSS1.p2.1.1" class="ltx_sup"><span id="S5.SS2.SSS1.p2.1.1.1" class="ltx_text ltx_font_italic">o</span></sup>. At frame 40, the sensor data is denser, and both methods produce more accurate shape estimates. Nevertheless, our shape estimate still fits the ground truth better. At frame 80, the sensor data becomes sparse again. As a result, the baseline approach produces an inaccurate shape estimate and a significant orientation error, as seen from <a href="#S5.F2" title="Figure 2 â€£ V-B1 Results on Synthetic Dataset â€£ V-B Shape and Pose Estimation â€£ V Experiments â€£ Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 2</span></a>. Yet, at this time, our method produces an accurate shape estimate despite the challenging sensor measurement. This shows that by leveraging temporal information, we can produce more accurate and consistent estimates over time.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS2.5.1.1" class="ltx_text">V-B</span>2 </span>Results on Argoverse Dataset</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p"><a href="#S5.F3" title="Figure 3 â€£ V-B1 Results on Synthetic Dataset â€£ V-B Shape and Pose Estimation â€£ V Experiments â€£ Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 3</span></a> plots the shape and pose estimate error metrics with respect to the number of detections. Our method outperforms the baseline in all metrics and shows a general trend of improvement in the quality of shape estimates with more detections. This aligns with our initial claim that performance improves with more frames if fused properly.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Automatic Labeling of Amodal Segmentation on KITTI</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We compare our automatically-generated label qualities under different ablations and cases to KINS manual annotation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> in <a href="#S5.T2" title="TABLE II â€£ V-C Automatic Labeling of Amodal Segmentation on KITTI â€£ V Experiments â€£ Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ II</span></a>.
KINS-manual mIoU represents the consistency of manually annotated labels across different annotators, denoting the quality of the manual labels.
GT-Accumulation is our method of accumulating and mirroring partial point cloud sequences. SC-GT refers to our improvement of GT-Accumulation by utilizing our sequential completion. Finally, SC-MRCNN refers to sequential completion using Mask-RCNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and label diffusion when 3D ground truth bounding boxes are unavailable.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">In all three cases (with and without 3D bounding box labels), our proposed labeling approaches achieve comparable mIoU consistency to human-level performance, i.e., KINS cross-annotators consistency. In the case where 3D ground truth bounding boxes are available, running through sequential completion (SC-GT) yields less missing instance match than just accumulating and mirroring (GT-Accumulation). It is important to note that we compare our generated labels to the manually annotated KINS labels and not to the actual amodal mask ground truth that is practically unavailable. The KINS label itself has some noise, i.e., its internal consistency across different annotators is <span class="ltx_ref ltx_nolink ltx_ref_self">~</span>0.8. Thus, achieving a higher mIoU than that does not necessarily imply a better quality mask.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Amodal mask consistency.</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">3D GT label</th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">% Miss</th>
<th id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">mIoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<td id="S5.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">KINS-manual</td>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.809</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<td id="S5.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">GT-Accumulation</td>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Available</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.69</td>
<td id="S5.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.813</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<td id="S5.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SC-GT</td>
<td id="S5.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Available</td>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.90</td>
<td id="S5.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.813</td>
</tr>
<tr id="S5.T2.1.5.4" class="ltx_tr">
<td id="S5.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">SC-MRCNN</td>
<td id="S5.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">No</td>
<td id="S5.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">5.53</td>
<td id="S5.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.788</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">To give a better insight into the advantage of our sequential completion network, we qualitatively show several amodal mask examples in <a href="#S5.F4" title="Figure 4 â€£ V-B1 Results on Synthetic Dataset â€£ V-B Shape and Pose Estimation â€£ V Experiments â€£ Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 4</span></a>. Specifically, note the area inside the red boxes. We observe cases where merely accumulating and mirroring (GT-Accumulation) is not enough to cover the complete amodal information. Additionally, GT-Accumulation is prone to error or noise in the point cloud, as shown in the white car inside the red box in <a href="#S5.F4" title="Figure 4 â€£ V-B1 Results on Synthetic Dataset â€£ V-B Shape and Pose Estimation â€£ V Experiments â€£ Sequential Joint Shape and Pose Estimation of Vehicles with Application to Automatic Amodal Segmentation Labeling" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 4</span></a> top left. On the other hand, despite the noise, our sequential completion network has a smoothing effect that filters out this noise.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">Finally, in the absence of 3D ground truth bounding boxes, we can not perform our accumulation and mirroring method (GT-Accumulation). But our sequential completion network (SC-MRCNN) still provides comparable quality masks to human-annotated labels despite the absence of any labels. Although as expected, the performance of SC-MRCNN is slightly worse than SC-GT. This is because segmenting out point cloud using Mask-RCNN and LDLS is not as accurate as using human-annotated 3D ground truth bounding boxes. Additionally, due to the absence of 3D ground truth bounding boxes, the sequential completion network in SC-MRCNN is trained on the synthetic data, which introduces some domain adaptation gap when applied to the KITTI data.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We propose a learning-based approach for joint point completion and pose estimation of vehicles from LiDAR point cloud measurements. Uniquely, our method explicitly leverages the temporal information of tracks. We evaluate our method on synthetic and real-world datasets, showing better performance in both shape and pose estimation tasks against the baseline approach. We demonstrate that properly fusing extra temporal information benefits shape and pose estimation. Finally, we propose a novel automatic amodal labeling pipeline using our sequential completion network.
We evaluate our automatic amodal labeling pipeline and demonstrate comparable quality to human annotations.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research is supported by grants from the National Science Foundation NSF (IIS-1724282, IIS-2107077, OAC-2118240, and OAC-2112606).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and Martial Hebert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">PCN: point completion network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, abs/1808.00671, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Xiaogang Wang, MarceloÂ H AngÂ Jr, and GimÂ Hee Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Point cloud completion by learning shape priors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2008.00394</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Hunter Goforth, Xiaoyan Hu, Michael Happold, and Simon Lucey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Joint pose and shape estimation of vehicles from lidar data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.03964</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Jiayuan Gu, Wei-Chiu Ma, Sivabalan Manivasagam, Wenyuan Zeng, Zihao Wang, Yuwen
Xiong, Hao Su, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Weakly-supervised 3d shape completion in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of the European Conf. on Computer Vision (ECCV)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Junbo Yin, Jianbing Shen, Chenye Guan, Dingfu Zhou, and Ruigang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Lidar-based online 3d video object detection with graph-based message
passing and spatiotemporal transformer attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Xinshuo Weng and Kris Kitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">A baseline for 3d multi-object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.03961</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp KrÃ¤henbÃ¼hl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Center-based 3d object detection and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2006.11275</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Empirical evaluation of gated recurrent neural networks on sequence
modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.3555</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
KeÂ Li and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Amodal instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 677â€“693.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Yan Zhu, Yuandong Tian, Dimitris Metaxas, and Piotr DollÃ¡r.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Semantic amodal segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 1464â€“1472, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Yuting Xiao, Yanyu Xu, Ziming Zhong, Weixin Luo, Jiawei Li, and Shenghua Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Amodal segmentation based on visible region segmentation and shape
prior.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2012.05598</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Xiaohang Zhan, Xingang Pan, BoÂ Dai, Ziwei Liu, Dahua Lin, and ChenÂ Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Self-supervised scene de-occlusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 3784â€“3792, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
David Held, Jesse Levinson, Sebastian Thrun, and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Robust real-time tracking combining 3d shape, color, and motion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The International Journal of Robotics Research</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 35(1-3):30â€“49,
2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Kevin Wyffels and Mark Campbell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Precision tracking via joint detailed shape estimation of arbitrary
extended objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Robotics</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 33(2):313â€“332, 2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Stefan KrÃ¤mer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Lidar-based object tracking and shape estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Josephine Monica and Mark Campbell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Vision only 3-d shape estimation for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems, IROS 2020, Las Vegas, NV, USA (Virtual), October 25-29,
2020</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Abhijit Kundu, Yin Li, and JamesÂ M Rehg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">3d-rcnn: Instance-level 3d object reconstruction via
render-and-compare.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 3559â€“3568, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Lei Ke, Shichao Li, Yanan Sun, Yu-Wing Tai, and Chi-Keung Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Gsnet: Joint vehicle pose and shape reconstruction with geometrical
and scene-aware supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 515â€“532.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
LyneÂ P Tchapmi, Vineet Kosaraju, Hamid Rezatofighi, Ian Reid, and Silvio
Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Topnet: Structural point cloud decoder.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 383â€“392, 2019.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, and Xinyi Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Pf-net: Point fractal network for 3d point cloud completion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 7662â€“7670, 2020.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Xinshuo Weng and Kris Kitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">A baseline for 3d multi-object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, abs/1907.03961, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Adam Feldman, Maria Hybinette, and Tucker Balch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">The multi-iterative closest point tracker: An online algorithm for
tracking multiple interacting targets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Field Robotics</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 29.2:258â€“276, 2012.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Johannes GroÃŸ, Aljosa Osep, and Bastian Leibe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Alignnet-3d: Fast point cloud registration of partially observed
objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, abs/1910.04668, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Wentao Yuan, David Held, Christoph Mertz, and Martial Hebert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Iterative transformer network for 3d point cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.11209</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Wentao Yuan, Benjamin Eckart, Kihwan Kim, Varun Jampani, Dieter Fox, and Jan
Kautz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Deepgmr: Learning latent gaussian mixture models for registration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 733â€“750.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Frederik HagelskjÃ¦r and AndersÂ Glent Buch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Pointvotenet: Accurate object detection and 6 dof pose estimation in
point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 IEEE International Conference on Image Processing
(ICIP)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 2641â€“2645. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
LuÂ Qi, LiÂ Jiang, Shu Liu, Xiaoyong Shen, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Amodal instance segmentation with kins dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 3014â€“3023, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Vision meets robotics: The kitti dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The International Journal of Robotics Research</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">,
32(11):1231â€“1237, 2013.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Abhishek Kar, Shubham Tulsiani, Joao Carreira, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Amodal completion and size constancy in natural scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 127â€“135, 2015.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
YuÂ Xiang, Roozbeh Mottaghi, and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Beyond pascal: A benchmark for 3d object detection in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE winter conference on applications of computer vision</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">,
pages 75â€“82. IEEE, 2014.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Mark Everingham and John Winn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">The pascal visual object classes challenge 2012 (voc2012) development
kit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern Analysis, Statistical Modelling and Computational
Learning, Tech. Rep</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 8:5, 2011.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
CharlesÂ Ruizhongtai Qi, Hao Su, Kaichun Mo, and LeonidasÂ J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Pointnet: Deep learning on point sets for 3d classification and
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, abs/1612.00593, 2016.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Pointrcnn: 3d object proposal generation and detection from point
cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
CharlesÂ R Qi, Wei Liu, Chenxia Wu, Hao Su, and LeonidasÂ J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Frustum pointnets for 3d object detection from rgb-d data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
AlexÂ H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar
Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Pointpillars: Fast encoders for object detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 12697â€“12705, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Yin Zhou and Oncel Tuzel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Voxelnet: End-to-end learning for point cloud based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
BrianÂ H Wang, Wei-Lun Chao, Yan Wang, Bharath Hariharan, KilianÂ Q Weinberger,
and Mark Campbell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Ldls: 3-d object segmentation through label diffusion from 2-d
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 4(3):2902â€“2909, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Bichen Wu, Alvin Wan, Xiangyu Yue, and Kurt Keutzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Squeezeseg: Convolutional neural nets with recurrent crf for
real-time road-object segmentation from 3d lidar point cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE International Conference on Robotics and Automation
(ICRA)</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 1887â€“1893. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Dirk Schulz, Wolfram Burgard, Dieter Fox, and ArminÂ B Cremers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Tracking multiple moving targets with a mobile robot using particle
filters and statistical data association.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings 2001 ICRA. IEEE International Conference on
Robotics and Automation (Cat. No. 01CH37164)</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, volumeÂ 2, pages 1665â€“1670.
IEEE, 2001.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Kevin Wyffels and Mark Campbell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Joint tracking and non-parametric shape estimation of arbitrary
extended objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2015 IEEE International Conference on Robotics and Automation
(ICRA)</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 3360â€“3367. IEEE, 2015.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Haoqiang Fan, Hao Su, and LeonidasÂ J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">A point set generation network for 3d object reconstruction from a
single image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, abs/1612.00603, 2016.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Stefan Holzer, Gary
Bradski, Kurt Konolige, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Model based training, detection and pose estimation of texture-less
3d objects in heavily cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Asian conference on computer vision</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 548â€“562.
Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Alex Kendall, Yarin Gal, and Roberto Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Multi-task learning using uncertainty to weigh losses for scene
geometry and semantics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 7482â€“7491, 2018.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak,
Andrew Hartnett, DeÂ Wang, Peter Carr, Simon Lucey, Deva Ramanan, and James
Hays.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Argoverse: 3d tracking and forecasting with rich maps.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, abs/1911.02620, 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Minghua Liu, LuÂ Sheng, Sheng Yang, Jing Shao, and Shi-Min Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Morphing and sampling network for dense point cloud completion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1912.00280</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Sanja Fidler, Sven Dickinson, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">3d object detection and viewpoint estimation with a deformable 3d
cuboid model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Dawson-Haggerty et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">trimesh.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://trimsh.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://trimsh.org/</a><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.4.1" class="ltx_text" style="font-size:90%;">version: 3.2.0.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Open3D: A modern library for 3D data processing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1801.09847</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Sivabalan Manivasagam, Shenlong Wang, Kelvin Wong, Wenyuan Zeng, Mikita
Sazanovich, Shuhan Tan, Bin Yang, Wei-Chiu Ma, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Lidarsim: Realistic lidar simulation by leveraging the real world.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages 11167â€“11176, 2020.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.09839" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.09840" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.09840">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.09840" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.09841" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 14:45:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
