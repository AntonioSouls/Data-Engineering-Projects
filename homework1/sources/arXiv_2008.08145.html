<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2008.08145] Category Level Object Pose Estimation via Neural Analysis-by-Synthesis</title><meta property="og:description" content="Many object pose estimation algorithms rely on the analysis-by-synthesis framework which requires explicit representations of individual object instances. In this paper we combine a gradient-based fitting procedure witâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Category Level Object Pose Estimation via Neural Analysis-by-Synthesis">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Category Level Object Pose Estimation via Neural Analysis-by-Synthesis">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2008.08145">

<!--Generated on Thu Mar 14 11:49:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="category-level object pose,  6DoF pose estimation">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>ETH ZÃ¼rich </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>University of TÃ¼bingen </span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Max Planck ETH Center for Learning Systems </span></span></span><span id="id4" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Max Planck Institute for Intelligent Systems, TÃ¼bingen
</span></span></span>
<h1 class="ltx_title ltx_title_document">Category Level Object Pose Estimation via Neural Analysis-by-Synthesis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xu Chen
</span><span class="ltx_author_notes">Equal contribution.1133</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zijian Dong<sup id="id1.1.id1" class="ltx_sup">*</sup>
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jie Song
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andreas Geiger
</span><span class="ltx_author_notes">2244</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Otmar Hilliges
</span><span class="ltx_author_notes">11</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Many object pose estimation algorithms rely on the analysis-by-synthesis framework which requires explicit representations of individual object instances. In this paper we combine a gradient-based fitting procedure with a parametric neural image synthesis module that is capable of implicitly representing the appearance, shape and pose of entire object categories, thus rendering the need for explicit CAD models per object instance unnecessary. The image synthesis network is designed to efficiently span the pose configuration space so that model capacity can be used to capture the shape and local appearance (i.e., texture) variations jointly. At inference time the synthesized images are compared to the target via an appearance based loss and the error signal is backpropagated through the network to the input parameters. Keeping the network parameters fixed, this allows for iterative optimization of the object pose, shape and appearance in a joint manner and we experimentally show that the method can recover orientation of objects with high accuracy from 2D images alone. When provided with depth measurements, to overcome scale ambiguities, the method can accurately recover the full 6DOF pose successfully.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>category-level object pose, 6DoF pose estimation
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Estimating the 3D pose
of objects from 2D images alone is a long-standing problem in computer vision and has many down-stream applications such as in robotics, autonomous driving and human-computer interaction. One popular class of solutions to this problem is based on the analysis-by-synthesis approach.
The key idea of analysis-by-synthesis is to leverage a forward model (e.g., graphics pipeline) to generate different images corresponding to possible geometric and semantic states of the world.
Subsequently, the candidate that best agrees with the measured visual evidence is selected.
In the context of object pose estimation, the visual evidence may comprise RGB images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, depth measurements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> or features extracted using deep networks such as keypoints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> or dense correspondence maps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.
While such algorithms can successfully recover the object pose, a major limiting factor is the requirement to i) know which object is processed and ii) to have access to an explicit representation of the object for example, in the form of a 3D CAD model.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Embracing this challenge, in this paper we propose an algorithm that can be categorized as analysis-by-synthesis but overcomes the requirement of a known, explicit instance representation.
At the core of our method lies a synthesis module that is based on recent advancements in the area of deep learning based image synthesis.
More specifically, we train a pose-aware neural network to predict 2D images of objects with desired poses, shapes and appearances.
In contrast to traditional static object representations, neural representations are able to jointly describe a large variety of instances, thereby extrapolating beyond the training set and allowing for continuous manipulation of object appearance and shape.
In addition, our network is designed to efficiently span the space of 3D transformations, thus enabling image synthesis of objects in unseen poses.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">After training, the synthesis module is leveraged in a gradient-based model fitting algorithm to jointly recover pose, shape and appearance of unseen object instances from single RGB or RGB-D images.
Since our neural image synthesis module takes initial pose and latent codes (e.g., extracted from an image) as input, it can be used as a drop-in replacement in existing optimization-based model fitting frameworks.
Given the initial pose and latent code, we generate an image and compare it with the target RGB image via an appearance based loss.
The discrepancy in appearance produces error gradients which back-propagate through the network to the pose parameters and the latent code.
Note that instead of updating the network parameters as during training time, we now keep the network weights fixed and instead update the appearance, shape and pose parameters.
We repeat this procedure until convergence.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We evaluate our method on a publicly available real-world dataset on the task of category-level object pose estimation. Using RGB images, our method is able to estimate the 3D object orientation with an accuracy on par with and sometimes even better than a state-of-the-art method which leverages RGB-D input.
As demonstrated in previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, RGB-only methods suffer strongly from inherent scale ambiguities.
Therefore, we also investigate an RGB-D version of our model which faithfully recovers both 3D translation and orientation.
We systematically study algorithmic design choices and the hyper-parameters involved during both image generation and model fitting. In summary:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We integrate a neural synthesis module into an optimization based model fitting framework to simultaneously recover object pose, shape and appearance from a single RGB or RGB-D image.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">This module is implemented as a deep network that can generate images of objects with control over poses and variations in shapes and appearances.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Experiments show that our generative model reaches parity with and sometimes outperforms a strong RGB-D baseline. Furthermore, it significantly outperforms discriminative pose regression.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_tag ltx_tag_note">*</span>Project homepage: <a href="ait.ethz.ch/projects/2020/neural-object-fitting" title="" class="ltx_ref ltx_url ltx_font_typewriter">ait.ethz.ch/projects/2020/neural-object-fitting</a></span></span></span></p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Object Pose Estimation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Given its practical importance, there is a large body of work focusing on object pose estimation. The state-of-the-art can be broadly categorized into template matching and regression techniques. Template matching techniques align 3D CAD models to observed 3D point clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, learned keypoints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> or correspondence features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.
In contrast, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> tackle object pose estimation as a classification or regression problem. However, to achieve high accuracy, these methods typically require template-based refinement,Â e.g., using ICP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. While yielding impressive results, all aforementioned methods require access to an instance specific 3D CAD model of the object, both during training and test time. This greatly limits their applicability since storing and comparing to all possible 3D CAD models at test time is impractical in many situations. Moreover, capturing high-fidelity and complete 3D models is often difficult and for some applications even impossible.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Only recently, researchers started the attempt to address object pose estimation without requiring access to instance-specific 3D object models at test time. NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> proposes to tackle this problem by learning to reconstruct the 3D object model in a canonical coordinate frame from RGB images and then align the reconstruction to depth measurements. They train their reconstruction network using objects from the same categories, which is expected to generalize to unseen instances within the same category at test time. While our method also learns to solve the task from objects within the same category, our method is a fully generative approach which simultaneously recovers object pose, shape and appearance.
Thus, it allows for directly synthesizing the object appearance, eliminating the intermediate step of reconstructing the object in 3D. LatentFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> proposes a 3D latent space based object representation for unseen object pose estimation. In contrast to ours, it requires multi-view imagery of the test object to form the latent space and depth measurements at test time.
In contrast to both NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and LatentFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, our model enables 3D object pose estimation from a single RGB image as input.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pose Dependent Image Generation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Pose or viewpoint dependent image generation has been studied in two settings. One line of work focuses on synthesizing novel views for a given source image by directly generating pixels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> or by warping pixels from source to target view <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. While such techniques can be used to render objects in different poses, the object appearance and shape is controlled by the source image which cannot be optimized.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Another line of work tackles the problem of disentangled image generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, considering object pose as one factor among many. Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> achieve appealing results on viewpoint/pose disentanglement using a 3D latent space. While all mentioned methods are able to generate objects in different poses, shape and appearances, the pose cannot be controlled precisely (e.g., rotation by a set amount of degrees), rendering their application to absolute object pose estimation tasks difficult. Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, our network also adopts a 3D latent space. However, we utilize the model in a supervised fashion to integrate precise absolute pose knowledge into the latent representation during training. This is achieved by integrating the 3D latent space with a conditional VAE framework. In contrast to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> which also utilizes a 3D latent space, our model can jointly represent multiple instances of an object category and to generalize to unseen instances. Similar and concurrent to ours, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> uses pose-aware image generation for viewpoint estimation, but in a discriminative manner.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2008.08145/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.7.2.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method overview.<span id="S2.F1.2.1.1" class="ltx_text ltx_font_medium">
We leverage a learned pose-aware image generator <math id="S2.F1.2.1.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S2.F1.2.1.1.m1.1b"><mi id="S2.F1.2.1.1.m1.1.1" xref="S2.F1.2.1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.F1.2.1.1.m1.1c"><ci id="S2.F1.2.1.1.m1.1.1.cmml" xref="S2.F1.2.1.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.2.1.1.m1.1d">G</annotation></semantics></math> for object pose estimation.
<em id="S2.F1.2.1.1.1" class="ltx_emph ltx_font_italic">Training:</em> The generator is trained in the VAE framework by leveraging multi-view images of synthetic objects. We minimize the reconstruction loss between generated and ground-truth images of known orientation together with the KL divergence. After training, the generator can produce images that faithfully reflect the (latent) appearance and desired pose.
<em id="S2.F1.2.1.1.2" class="ltx_emph ltx_font_italic">Inference:</em> To estimate object pose from a segmented real image as input, our method iteratively optimizes the object pose and shape, minimizing the perceptual loss between the input and the generated image while keeping the weights of the trained networks fixed. </span></span></figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>3D Representations for Objects</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Several works have addressed the problem of generating 3D geometric representations including meshes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, point sets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, voxels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and implicit functions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. While these generative models are also able to represent objects at category level and could theoretically be used for category-level object fitting in combination with a differential rendering algorithm, all aforementioned techniques only consider geometry, but not the appearance. As a result depth measurements are required and the rich information underlying in the objectâ€™s appearance is discarded. In contrast, our method allows for leveraging appearance information and does not require depth maps as input. While <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> is able to generate textured objects, it is limited by its reliance on 3D meshes. In contrast, we completely forego the intermediate geometry estimation task, instead focusing on pose-conditioned appearance generation.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Latent Space Optimization</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The idea of updating latent representations by iterative energy minimization has been exploited for other tasks. CodeSLAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> learns a latent representation of depth maps, optimizing the latent representation instead of per-pixel depth values during bundle adjustment. GANFit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> represents texture maps with GANs and jointly fits the latent code and 3DMM parameters to face images. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> embeds natural images into the latent space of GANs by iteratively minimizing the image reconstruction error for image editing. Our method is inspired by these works, but targets a different application where latent appearance codes and geometric parameters such as poses must be optimized jointly.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We propose a neural analysis-by-synthesis approach to category-level object pose estimation. Leveraging a learned image synthesis module, our approach is able to recover the 3D pose of an object from a single RGB or RGB-D image without requiring access to instance-specific 3D CAD models.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.6" class="ltx_p">Fig.Â <a href="#S2.F1" title="Figure 1 â€£ 2.2 Pose Dependent Image Generation â€£ 2 Related Work â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives an overview of our approach. We first train a pose-aware image generator <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">G</annotation></semantics></math> with multi-view images of synthetic objects from the ShapeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> dataset, which is able to generate object images <math id="S3.p2.2.m2.3" class="ltx_Math" alttext="\hat{I}=G(R,T,z)" display="inline"><semantics id="S3.p2.2.m2.3a"><mrow id="S3.p2.2.m2.3.4" xref="S3.p2.2.m2.3.4.cmml"><mover accent="true" id="S3.p2.2.m2.3.4.2" xref="S3.p2.2.m2.3.4.2.cmml"><mi id="S3.p2.2.m2.3.4.2.2" xref="S3.p2.2.m2.3.4.2.2.cmml">I</mi><mo id="S3.p2.2.m2.3.4.2.1" xref="S3.p2.2.m2.3.4.2.1.cmml">^</mo></mover><mo id="S3.p2.2.m2.3.4.1" xref="S3.p2.2.m2.3.4.1.cmml">=</mo><mrow id="S3.p2.2.m2.3.4.3" xref="S3.p2.2.m2.3.4.3.cmml"><mi id="S3.p2.2.m2.3.4.3.2" xref="S3.p2.2.m2.3.4.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.p2.2.m2.3.4.3.1" xref="S3.p2.2.m2.3.4.3.1.cmml">â€‹</mo><mrow id="S3.p2.2.m2.3.4.3.3.2" xref="S3.p2.2.m2.3.4.3.3.1.cmml"><mo stretchy="false" id="S3.p2.2.m2.3.4.3.3.2.1" xref="S3.p2.2.m2.3.4.3.3.1.cmml">(</mo><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">R</mi><mo id="S3.p2.2.m2.3.4.3.3.2.2" xref="S3.p2.2.m2.3.4.3.3.1.cmml">,</mo><mi id="S3.p2.2.m2.2.2" xref="S3.p2.2.m2.2.2.cmml">T</mi><mo id="S3.p2.2.m2.3.4.3.3.2.3" xref="S3.p2.2.m2.3.4.3.3.1.cmml">,</mo><mi id="S3.p2.2.m2.3.3" xref="S3.p2.2.m2.3.3.cmml">z</mi><mo stretchy="false" id="S3.p2.2.m2.3.4.3.3.2.4" xref="S3.p2.2.m2.3.4.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.3b"><apply id="S3.p2.2.m2.3.4.cmml" xref="S3.p2.2.m2.3.4"><eq id="S3.p2.2.m2.3.4.1.cmml" xref="S3.p2.2.m2.3.4.1"></eq><apply id="S3.p2.2.m2.3.4.2.cmml" xref="S3.p2.2.m2.3.4.2"><ci id="S3.p2.2.m2.3.4.2.1.cmml" xref="S3.p2.2.m2.3.4.2.1">^</ci><ci id="S3.p2.2.m2.3.4.2.2.cmml" xref="S3.p2.2.m2.3.4.2.2">ğ¼</ci></apply><apply id="S3.p2.2.m2.3.4.3.cmml" xref="S3.p2.2.m2.3.4.3"><times id="S3.p2.2.m2.3.4.3.1.cmml" xref="S3.p2.2.m2.3.4.3.1"></times><ci id="S3.p2.2.m2.3.4.3.2.cmml" xref="S3.p2.2.m2.3.4.3.2">ğº</ci><vector id="S3.p2.2.m2.3.4.3.3.1.cmml" xref="S3.p2.2.m2.3.4.3.3.2"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">ğ‘…</ci><ci id="S3.p2.2.m2.2.2.cmml" xref="S3.p2.2.m2.2.2">ğ‘‡</ci><ci id="S3.p2.2.m2.3.3.cmml" xref="S3.p2.2.m2.3.3">ğ‘§</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.3c">\hat{I}=G(R,T,z)</annotation></semantics></math> that faithfully reflect the input object pose (<math id="S3.p2.3.m3.2" class="ltx_Math" alttext="R,T" display="inline"><semantics id="S3.p2.3.m3.2a"><mrow id="S3.p2.3.m3.2.3.2" xref="S3.p2.3.m3.2.3.1.cmml"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">R</mi><mo id="S3.p2.3.m3.2.3.2.1" xref="S3.p2.3.m3.2.3.1.cmml">,</mo><mi id="S3.p2.3.m3.2.2" xref="S3.p2.3.m3.2.2.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.2b"><list id="S3.p2.3.m3.2.3.1.cmml" xref="S3.p2.3.m3.2.3.2"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">ğ‘…</ci><ci id="S3.p2.3.m3.2.2.cmml" xref="S3.p2.3.m3.2.2">ğ‘‡</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.2c">R,T</annotation></semantics></math>) and appearance code <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.p2.4.m4.1a"><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">z</annotation></semantics></math>. At inference time with a segmented image <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p2.5.m5.1a"><mi id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><ci id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">I</annotation></semantics></math> as input, our method estimates object pose by iteratively optimizing the object pose and shape to minimize the discrepancy between the input image and the synthesized image <math id="S3.p2.6.m6.3" class="ltx_math_unparsed" alttext="G(R,T,z" display="inline"><semantics id="S3.p2.6.m6.3a"><mrow id="S3.p2.6.m6.3b"><mi id="S3.p2.6.m6.3.4">G</mi><mrow id="S3.p2.6.m6.3.5"><mo stretchy="false" id="S3.p2.6.m6.3.5.1">(</mo><mi id="S3.p2.6.m6.1.1">R</mi><mo id="S3.p2.6.m6.3.5.2">,</mo><mi id="S3.p2.6.m6.2.2">T</mi><mo id="S3.p2.6.m6.3.5.3">,</mo><mi id="S3.p2.6.m6.3.3">z</mi></mrow></mrow><annotation encoding="application/x-tex" id="S3.p2.6.m6.3c">G(R,T,z</annotation></semantics></math>).</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2008.08145/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="175" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.17.8.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.14.7" class="ltx_text ltx_font_bold" style="font-size:90%;">Pose-aware image generator.<span id="S3.F2.14.7.7" class="ltx_text ltx_font_medium"> To generate an image in the desired pose <math id="S3.F2.8.1.1.m1.2" class="ltx_Math" alttext="(R,T)" display="inline"><semantics id="S3.F2.8.1.1.m1.2b"><mrow id="S3.F2.8.1.1.m1.2.3.2" xref="S3.F2.8.1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.F2.8.1.1.m1.2.3.2.1" xref="S3.F2.8.1.1.m1.2.3.1.cmml">(</mo><mi id="S3.F2.8.1.1.m1.1.1" xref="S3.F2.8.1.1.m1.1.1.cmml">R</mi><mo id="S3.F2.8.1.1.m1.2.3.2.2" xref="S3.F2.8.1.1.m1.2.3.1.cmml">,</mo><mi id="S3.F2.8.1.1.m1.2.2" xref="S3.F2.8.1.1.m1.2.2.cmml">T</mi><mo stretchy="false" id="S3.F2.8.1.1.m1.2.3.2.3" xref="S3.F2.8.1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.8.1.1.m1.2c"><interval closure="open" id="S3.F2.8.1.1.m1.2.3.1.cmml" xref="S3.F2.8.1.1.m1.2.3.2"><ci id="S3.F2.8.1.1.m1.1.1.cmml" xref="S3.F2.8.1.1.m1.1.1">ğ‘…</ci><ci id="S3.F2.8.1.1.m1.2.2.cmml" xref="S3.F2.8.1.1.m1.2.2">ğ‘‡</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.1.1.m1.2d">(R,T)</annotation></semantics></math>, the out-of-plane rotation <math id="S3.F2.9.2.2.m2.1" class="ltx_Math" alttext="R_{x}R_{y}" display="inline"><semantics id="S3.F2.9.2.2.m2.1b"><mrow id="S3.F2.9.2.2.m2.1.1" xref="S3.F2.9.2.2.m2.1.1.cmml"><msub id="S3.F2.9.2.2.m2.1.1.2" xref="S3.F2.9.2.2.m2.1.1.2.cmml"><mi id="S3.F2.9.2.2.m2.1.1.2.2" xref="S3.F2.9.2.2.m2.1.1.2.2.cmml">R</mi><mi id="S3.F2.9.2.2.m2.1.1.2.3" xref="S3.F2.9.2.2.m2.1.1.2.3.cmml">x</mi></msub><mo lspace="0em" rspace="0em" id="S3.F2.9.2.2.m2.1.1.1" xref="S3.F2.9.2.2.m2.1.1.1.cmml">â€‹</mo><msub id="S3.F2.9.2.2.m2.1.1.3" xref="S3.F2.9.2.2.m2.1.1.3.cmml"><mi id="S3.F2.9.2.2.m2.1.1.3.2" xref="S3.F2.9.2.2.m2.1.1.3.2.cmml">R</mi><mi id="S3.F2.9.2.2.m2.1.1.3.3" xref="S3.F2.9.2.2.m2.1.1.3.3.cmml">y</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.9.2.2.m2.1c"><apply id="S3.F2.9.2.2.m2.1.1.cmml" xref="S3.F2.9.2.2.m2.1.1"><times id="S3.F2.9.2.2.m2.1.1.1.cmml" xref="S3.F2.9.2.2.m2.1.1.1"></times><apply id="S3.F2.9.2.2.m2.1.1.2.cmml" xref="S3.F2.9.2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.F2.9.2.2.m2.1.1.2.1.cmml" xref="S3.F2.9.2.2.m2.1.1.2">subscript</csymbol><ci id="S3.F2.9.2.2.m2.1.1.2.2.cmml" xref="S3.F2.9.2.2.m2.1.1.2.2">ğ‘…</ci><ci id="S3.F2.9.2.2.m2.1.1.2.3.cmml" xref="S3.F2.9.2.2.m2.1.1.2.3">ğ‘¥</ci></apply><apply id="S3.F2.9.2.2.m2.1.1.3.cmml" xref="S3.F2.9.2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.F2.9.2.2.m2.1.1.3.1.cmml" xref="S3.F2.9.2.2.m2.1.1.3">subscript</csymbol><ci id="S3.F2.9.2.2.m2.1.1.3.2.cmml" xref="S3.F2.9.2.2.m2.1.1.3.2">ğ‘…</ci><ci id="S3.F2.9.2.2.m2.1.1.3.3.cmml" xref="S3.F2.9.2.2.m2.1.1.3.3">ğ‘¦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.9.2.2.m2.1d">R_{x}R_{y}</annotation></semantics></math> is first applied to the 3D feature volume, and then the 2D projection of the feature volume is decoded into an image. Subsequently, this image undergoes a 2D similarity transformation derived from the translation <math id="S3.F2.10.3.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.F2.10.3.3.m3.1b"><mi id="S3.F2.10.3.3.m3.1.1" xref="S3.F2.10.3.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.F2.10.3.3.m3.1c"><ci id="S3.F2.10.3.3.m3.1.1.cmml" xref="S3.F2.10.3.3.m3.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.10.3.3.m3.1d">T</annotation></semantics></math> and in-plane rotation <math id="S3.F2.11.4.4.m4.1" class="ltx_Math" alttext="R_{z}" display="inline"><semantics id="S3.F2.11.4.4.m4.1b"><msub id="S3.F2.11.4.4.m4.1.1" xref="S3.F2.11.4.4.m4.1.1.cmml"><mi id="S3.F2.11.4.4.m4.1.1.2" xref="S3.F2.11.4.4.m4.1.1.2.cmml">R</mi><mi id="S3.F2.11.4.4.m4.1.1.3" xref="S3.F2.11.4.4.m4.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.11.4.4.m4.1c"><apply id="S3.F2.11.4.4.m4.1.1.cmml" xref="S3.F2.11.4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.F2.11.4.4.m4.1.1.1.cmml" xref="S3.F2.11.4.4.m4.1.1">subscript</csymbol><ci id="S3.F2.11.4.4.m4.1.1.2.cmml" xref="S3.F2.11.4.4.m4.1.1.2">ğ‘…</ci><ci id="S3.F2.11.4.4.m4.1.1.3.cmml" xref="S3.F2.11.4.4.m4.1.1.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.11.4.4.m4.1d">R_{z}</annotation></semantics></math> to form the final output. The latent code <math id="S3.F2.12.5.5.m5.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.F2.12.5.5.m5.1b"><mi id="S3.F2.12.5.5.m5.1.1" xref="S3.F2.12.5.5.m5.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.F2.12.5.5.m5.1c"><ci id="S3.F2.12.5.5.m5.1.1.cmml" xref="S3.F2.12.5.5.m5.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.12.5.5.m5.1d">z</annotation></semantics></math> is injected into the 3D feature volume generator <math id="S3.F2.13.6.6.m6.1" class="ltx_Math" alttext="Dec_{3D}" display="inline"><semantics id="S3.F2.13.6.6.m6.1b"><mrow id="S3.F2.13.6.6.m6.1.1" xref="S3.F2.13.6.6.m6.1.1.cmml"><mi id="S3.F2.13.6.6.m6.1.1.2" xref="S3.F2.13.6.6.m6.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.F2.13.6.6.m6.1.1.1" xref="S3.F2.13.6.6.m6.1.1.1.cmml">â€‹</mo><mi id="S3.F2.13.6.6.m6.1.1.3" xref="S3.F2.13.6.6.m6.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.F2.13.6.6.m6.1.1.1b" xref="S3.F2.13.6.6.m6.1.1.1.cmml">â€‹</mo><msub id="S3.F2.13.6.6.m6.1.1.4" xref="S3.F2.13.6.6.m6.1.1.4.cmml"><mi id="S3.F2.13.6.6.m6.1.1.4.2" xref="S3.F2.13.6.6.m6.1.1.4.2.cmml">c</mi><mrow id="S3.F2.13.6.6.m6.1.1.4.3" xref="S3.F2.13.6.6.m6.1.1.4.3.cmml"><mn id="S3.F2.13.6.6.m6.1.1.4.3.2" xref="S3.F2.13.6.6.m6.1.1.4.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.F2.13.6.6.m6.1.1.4.3.1" xref="S3.F2.13.6.6.m6.1.1.4.3.1.cmml">â€‹</mo><mi id="S3.F2.13.6.6.m6.1.1.4.3.3" xref="S3.F2.13.6.6.m6.1.1.4.3.3.cmml">D</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.13.6.6.m6.1c"><apply id="S3.F2.13.6.6.m6.1.1.cmml" xref="S3.F2.13.6.6.m6.1.1"><times id="S3.F2.13.6.6.m6.1.1.1.cmml" xref="S3.F2.13.6.6.m6.1.1.1"></times><ci id="S3.F2.13.6.6.m6.1.1.2.cmml" xref="S3.F2.13.6.6.m6.1.1.2">ğ·</ci><ci id="S3.F2.13.6.6.m6.1.1.3.cmml" xref="S3.F2.13.6.6.m6.1.1.3">ğ‘’</ci><apply id="S3.F2.13.6.6.m6.1.1.4.cmml" xref="S3.F2.13.6.6.m6.1.1.4"><csymbol cd="ambiguous" id="S3.F2.13.6.6.m6.1.1.4.1.cmml" xref="S3.F2.13.6.6.m6.1.1.4">subscript</csymbol><ci id="S3.F2.13.6.6.m6.1.1.4.2.cmml" xref="S3.F2.13.6.6.m6.1.1.4.2">ğ‘</ci><apply id="S3.F2.13.6.6.m6.1.1.4.3.cmml" xref="S3.F2.13.6.6.m6.1.1.4.3"><times id="S3.F2.13.6.6.m6.1.1.4.3.1.cmml" xref="S3.F2.13.6.6.m6.1.1.4.3.1"></times><cn type="integer" id="S3.F2.13.6.6.m6.1.1.4.3.2.cmml" xref="S3.F2.13.6.6.m6.1.1.4.3.2">3</cn><ci id="S3.F2.13.6.6.m6.1.1.4.3.3.cmml" xref="S3.F2.13.6.6.m6.1.1.4.3.3">ğ·</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.13.6.6.m6.1d">Dec_{3D}</annotation></semantics></math> and the 2D decoder <math id="S3.F2.14.7.7.m7.1" class="ltx_Math" alttext="Dec_{2D}" display="inline"><semantics id="S3.F2.14.7.7.m7.1b"><mrow id="S3.F2.14.7.7.m7.1.1" xref="S3.F2.14.7.7.m7.1.1.cmml"><mi id="S3.F2.14.7.7.m7.1.1.2" xref="S3.F2.14.7.7.m7.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.F2.14.7.7.m7.1.1.1" xref="S3.F2.14.7.7.m7.1.1.1.cmml">â€‹</mo><mi id="S3.F2.14.7.7.m7.1.1.3" xref="S3.F2.14.7.7.m7.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.F2.14.7.7.m7.1.1.1b" xref="S3.F2.14.7.7.m7.1.1.1.cmml">â€‹</mo><msub id="S3.F2.14.7.7.m7.1.1.4" xref="S3.F2.14.7.7.m7.1.1.4.cmml"><mi id="S3.F2.14.7.7.m7.1.1.4.2" xref="S3.F2.14.7.7.m7.1.1.4.2.cmml">c</mi><mrow id="S3.F2.14.7.7.m7.1.1.4.3" xref="S3.F2.14.7.7.m7.1.1.4.3.cmml"><mn id="S3.F2.14.7.7.m7.1.1.4.3.2" xref="S3.F2.14.7.7.m7.1.1.4.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.F2.14.7.7.m7.1.1.4.3.1" xref="S3.F2.14.7.7.m7.1.1.4.3.1.cmml">â€‹</mo><mi id="S3.F2.14.7.7.m7.1.1.4.3.3" xref="S3.F2.14.7.7.m7.1.1.4.3.3.cmml">D</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.14.7.7.m7.1c"><apply id="S3.F2.14.7.7.m7.1.1.cmml" xref="S3.F2.14.7.7.m7.1.1"><times id="S3.F2.14.7.7.m7.1.1.1.cmml" xref="S3.F2.14.7.7.m7.1.1.1"></times><ci id="S3.F2.14.7.7.m7.1.1.2.cmml" xref="S3.F2.14.7.7.m7.1.1.2">ğ·</ci><ci id="S3.F2.14.7.7.m7.1.1.3.cmml" xref="S3.F2.14.7.7.m7.1.1.3">ğ‘’</ci><apply id="S3.F2.14.7.7.m7.1.1.4.cmml" xref="S3.F2.14.7.7.m7.1.1.4"><csymbol cd="ambiguous" id="S3.F2.14.7.7.m7.1.1.4.1.cmml" xref="S3.F2.14.7.7.m7.1.1.4">subscript</csymbol><ci id="S3.F2.14.7.7.m7.1.1.4.2.cmml" xref="S3.F2.14.7.7.m7.1.1.4.2">ğ‘</ci><apply id="S3.F2.14.7.7.m7.1.1.4.3.cmml" xref="S3.F2.14.7.7.m7.1.1.4.3"><times id="S3.F2.14.7.7.m7.1.1.4.3.1.cmml" xref="S3.F2.14.7.7.m7.1.1.4.3.1"></times><cn type="integer" id="S3.F2.14.7.7.m7.1.1.4.3.2.cmml" xref="S3.F2.14.7.7.m7.1.1.4.3.2">2</cn><ci id="S3.F2.14.7.7.m7.1.1.4.3.3.cmml" xref="S3.F2.14.7.7.m7.1.1.4.3.3">ğ·</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.14.7.7.m7.1d">Dec_{2D}</annotation></semantics></math> via adaIN to control the variation in shapes and appearances.</span></span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Pose-aware Image Generator</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.16" class="ltx_p">To generate images of different instances of a given category, we seek to generate images with significant but controllable shape and appearance variation. We encode shape and appearance via latent variable <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">z</annotation></semantics></math>, and the desired 6 DoF object pose, comprising 3D rotation <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="R=R_{x}R_{y}R_{z}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">R</mi><mo id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml"><msub id="S3.SS1.p1.2.m2.1.1.3.2" xref="S3.SS1.p1.2.m2.1.1.3.2.cmml"><mi id="S3.SS1.p1.2.m2.1.1.3.2.2" xref="S3.SS1.p1.2.m2.1.1.3.2.2.cmml">R</mi><mi id="S3.SS1.p1.2.m2.1.1.3.2.3" xref="S3.SS1.p1.2.m2.1.1.3.2.3.cmml">x</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.3.1" xref="S3.SS1.p1.2.m2.1.1.3.1.cmml">â€‹</mo><msub id="S3.SS1.p1.2.m2.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.3.3.2" xref="S3.SS1.p1.2.m2.1.1.3.3.2.cmml">R</mi><mi id="S3.SS1.p1.2.m2.1.1.3.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3.3.cmml">y</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.3.1a" xref="S3.SS1.p1.2.m2.1.1.3.1.cmml">â€‹</mo><msub id="S3.SS1.p1.2.m2.1.1.3.4" xref="S3.SS1.p1.2.m2.1.1.3.4.cmml"><mi id="S3.SS1.p1.2.m2.1.1.3.4.2" xref="S3.SS1.p1.2.m2.1.1.3.4.2.cmml">R</mi><mi id="S3.SS1.p1.2.m2.1.1.3.4.3" xref="S3.SS1.p1.2.m2.1.1.3.4.3.cmml">z</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><eq id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></eq><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ğ‘…</ci><apply id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><times id="S3.SS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3.1"></times><apply id="S3.SS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.3.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.3.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2.2">ğ‘…</ci><ci id="S3.SS1.p1.2.m2.1.1.3.2.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2.3">ğ‘¥</ci></apply><apply id="S3.SS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3.2">ğ‘…</ci><ci id="S3.SS1.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3.3">ğ‘¦</ci></apply><apply id="S3.SS1.p1.2.m2.1.1.3.4.cmml" xref="S3.SS1.p1.2.m2.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.3.4.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3.4">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.3.4.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.4.2">ğ‘…</ci><ci id="S3.SS1.p1.2.m2.1.1.3.4.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.4.3">ğ‘§</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">R=R_{x}R_{y}R_{z}</annotation></semantics></math> and 3D translation <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="T=[t_{x}\ t_{y}\ t_{z}]" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">T</mi><mo id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">=</mo><mrow id="S3.SS1.p1.3.m3.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.3.m3.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.2.1.cmml">[</mo><mrow id="S3.SS1.p1.3.m3.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.cmml"><msub id="S3.SS1.p1.3.m3.1.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2.2.cmml">t</mi><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2.3.cmml">x</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.SS1.p1.3.m3.1.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.3.2.cmml">t</mi><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.1.1.1.3.3.cmml">y</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.1.1.1.1.1a" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.SS1.p1.3.m3.1.1.1.1.1.4" xref="S3.SS1.p1.3.m3.1.1.1.1.1.4.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.4.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.4.2.cmml">t</mi><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.4.3" xref="S3.SS1.p1.3.m3.1.1.1.1.1.4.3.cmml">z</mi></msub></mrow><mo stretchy="false" id="S3.SS1.p1.3.m3.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><eq id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"></eq><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ğ‘‡</ci><apply id="S3.SS1.p1.3.m3.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.3.m3.1.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1"></times><apply id="S3.SS1.p1.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2.2">ğ‘¡</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2.3">ğ‘¥</ci></apply><apply id="S3.SS1.p1.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.3.2">ğ‘¡</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.3.3">ğ‘¦</ci></apply><apply id="S3.SS1.p1.3.m3.1.1.1.1.1.4.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.1.4.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.4">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.4.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.4.2">ğ‘¡</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.4.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.4.3">ğ‘§</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">T=[t_{x}\ t_{y}\ t_{z}]</annotation></semantics></math>, via <math id="S3.SS1.p1.4.m4.2" class="ltx_Math" alttext="(R,T)" display="inline"><semantics id="S3.SS1.p1.4.m4.2a"><mrow id="S3.SS1.p1.4.m4.2.3.2" xref="S3.SS1.p1.4.m4.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m4.2.3.2.1" xref="S3.SS1.p1.4.m4.2.3.1.cmml">(</mo><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">R</mi><mo id="S3.SS1.p1.4.m4.2.3.2.2" xref="S3.SS1.p1.4.m4.2.3.1.cmml">,</mo><mi id="S3.SS1.p1.4.m4.2.2" xref="S3.SS1.p1.4.m4.2.2.cmml">T</mi><mo stretchy="false" id="S3.SS1.p1.4.m4.2.3.2.3" xref="S3.SS1.p1.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.2b"><interval closure="open" id="S3.SS1.p1.4.m4.2.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.2"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ğ‘…</ci><ci id="S3.SS1.p1.4.m4.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2">ğ‘‡</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.2c">(R,T)</annotation></semantics></math>. The <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">z</annotation></semantics></math>-axis is defined to align with the principal axis of the camera and the <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">y</annotation></semantics></math>-axis points upwards.
For efficiency and to increase the capacity of our model in terms of representing the large variability of shapes and appearances, we decouple the image generation pipeline into two stages.
First, we observe that 3D translations <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">T</annotation></semantics></math> and in-plane rotations <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="R_{z}" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">R</mi><mi id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">ğ‘…</ci><ci id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">R_{z}</annotation></semantics></math> can be modeled using 2D operations and thus do not need to be learned.
Therefore, we constrain our network <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="G_{3D}" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><msub id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">G</mi><mrow id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml"><mn id="S3.SS1.p1.9.m9.1.1.3.2" xref="S3.SS1.p1.9.m9.1.1.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p1.9.m9.1.1.3.1" xref="S3.SS1.p1.9.m9.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.9.m9.1.1.3.3" xref="S3.SS1.p1.9.m9.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">ğº</ci><apply id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3"><times id="S3.SS1.p1.9.m9.1.1.3.1.cmml" xref="S3.SS1.p1.9.m9.1.1.3.1"></times><cn type="integer" id="S3.SS1.p1.9.m9.1.1.3.2.cmml" xref="S3.SS1.p1.9.m9.1.1.3.2">3</cn><ci id="S3.SS1.p1.9.m9.1.1.3.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">G_{3D}</annotation></semantics></math> to generate only images with out-of-plane rotations <math id="S3.SS1.p1.10.m10.3" class="ltx_Math" alttext="\hat{I}_{rot}=G_{3D}(R_{x},R_{y},z)" display="inline"><semantics id="S3.SS1.p1.10.m10.3a"><mrow id="S3.SS1.p1.10.m10.3.3" xref="S3.SS1.p1.10.m10.3.3.cmml"><msub id="S3.SS1.p1.10.m10.3.3.4" xref="S3.SS1.p1.10.m10.3.3.4.cmml"><mover accent="true" id="S3.SS1.p1.10.m10.3.3.4.2" xref="S3.SS1.p1.10.m10.3.3.4.2.cmml"><mi id="S3.SS1.p1.10.m10.3.3.4.2.2" xref="S3.SS1.p1.10.m10.3.3.4.2.2.cmml">I</mi><mo id="S3.SS1.p1.10.m10.3.3.4.2.1" xref="S3.SS1.p1.10.m10.3.3.4.2.1.cmml">^</mo></mover><mrow id="S3.SS1.p1.10.m10.3.3.4.3" xref="S3.SS1.p1.10.m10.3.3.4.3.cmml"><mi id="S3.SS1.p1.10.m10.3.3.4.3.2" xref="S3.SS1.p1.10.m10.3.3.4.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.10.m10.3.3.4.3.1" xref="S3.SS1.p1.10.m10.3.3.4.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.10.m10.3.3.4.3.3" xref="S3.SS1.p1.10.m10.3.3.4.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.10.m10.3.3.4.3.1a" xref="S3.SS1.p1.10.m10.3.3.4.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.10.m10.3.3.4.3.4" xref="S3.SS1.p1.10.m10.3.3.4.3.4.cmml">t</mi></mrow></msub><mo id="S3.SS1.p1.10.m10.3.3.3" xref="S3.SS1.p1.10.m10.3.3.3.cmml">=</mo><mrow id="S3.SS1.p1.10.m10.3.3.2" xref="S3.SS1.p1.10.m10.3.3.2.cmml"><msub id="S3.SS1.p1.10.m10.3.3.2.4" xref="S3.SS1.p1.10.m10.3.3.2.4.cmml"><mi id="S3.SS1.p1.10.m10.3.3.2.4.2" xref="S3.SS1.p1.10.m10.3.3.2.4.2.cmml">G</mi><mrow id="S3.SS1.p1.10.m10.3.3.2.4.3" xref="S3.SS1.p1.10.m10.3.3.2.4.3.cmml"><mn id="S3.SS1.p1.10.m10.3.3.2.4.3.2" xref="S3.SS1.p1.10.m10.3.3.2.4.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p1.10.m10.3.3.2.4.3.1" xref="S3.SS1.p1.10.m10.3.3.2.4.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.10.m10.3.3.2.4.3.3" xref="S3.SS1.p1.10.m10.3.3.2.4.3.3.cmml">D</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.10.m10.3.3.2.3" xref="S3.SS1.p1.10.m10.3.3.2.3.cmml">â€‹</mo><mrow id="S3.SS1.p1.10.m10.3.3.2.2.2" xref="S3.SS1.p1.10.m10.3.3.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.10.m10.3.3.2.2.2.3" xref="S3.SS1.p1.10.m10.3.3.2.2.3.cmml">(</mo><msub id="S3.SS1.p1.10.m10.2.2.1.1.1.1" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.cmml"><mi id="S3.SS1.p1.10.m10.2.2.1.1.1.1.2" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.2.cmml">R</mi><mi id="S3.SS1.p1.10.m10.2.2.1.1.1.1.3" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.3.cmml">x</mi></msub><mo id="S3.SS1.p1.10.m10.3.3.2.2.2.4" xref="S3.SS1.p1.10.m10.3.3.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.10.m10.3.3.2.2.2.2" xref="S3.SS1.p1.10.m10.3.3.2.2.2.2.cmml"><mi id="S3.SS1.p1.10.m10.3.3.2.2.2.2.2" xref="S3.SS1.p1.10.m10.3.3.2.2.2.2.2.cmml">R</mi><mi id="S3.SS1.p1.10.m10.3.3.2.2.2.2.3" xref="S3.SS1.p1.10.m10.3.3.2.2.2.2.3.cmml">y</mi></msub><mo id="S3.SS1.p1.10.m10.3.3.2.2.2.5" xref="S3.SS1.p1.10.m10.3.3.2.2.3.cmml">,</mo><mi id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml">z</mi><mo stretchy="false" id="S3.SS1.p1.10.m10.3.3.2.2.2.6" xref="S3.SS1.p1.10.m10.3.3.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.3b"><apply id="S3.SS1.p1.10.m10.3.3.cmml" xref="S3.SS1.p1.10.m10.3.3"><eq id="S3.SS1.p1.10.m10.3.3.3.cmml" xref="S3.SS1.p1.10.m10.3.3.3"></eq><apply id="S3.SS1.p1.10.m10.3.3.4.cmml" xref="S3.SS1.p1.10.m10.3.3.4"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.3.3.4.1.cmml" xref="S3.SS1.p1.10.m10.3.3.4">subscript</csymbol><apply id="S3.SS1.p1.10.m10.3.3.4.2.cmml" xref="S3.SS1.p1.10.m10.3.3.4.2"><ci id="S3.SS1.p1.10.m10.3.3.4.2.1.cmml" xref="S3.SS1.p1.10.m10.3.3.4.2.1">^</ci><ci id="S3.SS1.p1.10.m10.3.3.4.2.2.cmml" xref="S3.SS1.p1.10.m10.3.3.4.2.2">ğ¼</ci></apply><apply id="S3.SS1.p1.10.m10.3.3.4.3.cmml" xref="S3.SS1.p1.10.m10.3.3.4.3"><times id="S3.SS1.p1.10.m10.3.3.4.3.1.cmml" xref="S3.SS1.p1.10.m10.3.3.4.3.1"></times><ci id="S3.SS1.p1.10.m10.3.3.4.3.2.cmml" xref="S3.SS1.p1.10.m10.3.3.4.3.2">ğ‘Ÿ</ci><ci id="S3.SS1.p1.10.m10.3.3.4.3.3.cmml" xref="S3.SS1.p1.10.m10.3.3.4.3.3">ğ‘œ</ci><ci id="S3.SS1.p1.10.m10.3.3.4.3.4.cmml" xref="S3.SS1.p1.10.m10.3.3.4.3.4">ğ‘¡</ci></apply></apply><apply id="S3.SS1.p1.10.m10.3.3.2.cmml" xref="S3.SS1.p1.10.m10.3.3.2"><times id="S3.SS1.p1.10.m10.3.3.2.3.cmml" xref="S3.SS1.p1.10.m10.3.3.2.3"></times><apply id="S3.SS1.p1.10.m10.3.3.2.4.cmml" xref="S3.SS1.p1.10.m10.3.3.2.4"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.3.3.2.4.1.cmml" xref="S3.SS1.p1.10.m10.3.3.2.4">subscript</csymbol><ci id="S3.SS1.p1.10.m10.3.3.2.4.2.cmml" xref="S3.SS1.p1.10.m10.3.3.2.4.2">ğº</ci><apply id="S3.SS1.p1.10.m10.3.3.2.4.3.cmml" xref="S3.SS1.p1.10.m10.3.3.2.4.3"><times id="S3.SS1.p1.10.m10.3.3.2.4.3.1.cmml" xref="S3.SS1.p1.10.m10.3.3.2.4.3.1"></times><cn type="integer" id="S3.SS1.p1.10.m10.3.3.2.4.3.2.cmml" xref="S3.SS1.p1.10.m10.3.3.2.4.3.2">3</cn><ci id="S3.SS1.p1.10.m10.3.3.2.4.3.3.cmml" xref="S3.SS1.p1.10.m10.3.3.2.4.3.3">ğ·</ci></apply></apply><vector id="S3.SS1.p1.10.m10.3.3.2.2.3.cmml" xref="S3.SS1.p1.10.m10.3.3.2.2.2"><apply id="S3.SS1.p1.10.m10.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.10.m10.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.2">ğ‘…</ci><ci id="S3.SS1.p1.10.m10.2.2.1.1.1.1.3.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.3">ğ‘¥</ci></apply><apply id="S3.SS1.p1.10.m10.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.10.m10.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.3.3.2.2.2.2.1.cmml" xref="S3.SS1.p1.10.m10.3.3.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.10.m10.3.3.2.2.2.2.2.cmml" xref="S3.SS1.p1.10.m10.3.3.2.2.2.2.2">ğ‘…</ci><ci id="S3.SS1.p1.10.m10.3.3.2.2.2.2.3.cmml" xref="S3.SS1.p1.10.m10.3.3.2.2.2.2.3">ğ‘¦</ci></apply><ci id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">ğ‘§</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.3c">\hat{I}_{rot}=G_{3D}(R_{x},R_{y},z)</annotation></semantics></math>, i.e., elevation <math id="S3.SS1.p1.11.m11.1" class="ltx_Math" alttext="R_{x}" display="inline"><semantics id="S3.SS1.p1.11.m11.1a"><msub id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml"><mi id="S3.SS1.p1.11.m11.1.1.2" xref="S3.SS1.p1.11.m11.1.1.2.cmml">R</mi><mi id="S3.SS1.p1.11.m11.1.1.3" xref="S3.SS1.p1.11.m11.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><apply id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m11.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.p1.11.m11.1.1.2.cmml" xref="S3.SS1.p1.11.m11.1.1.2">ğ‘…</ci><ci id="S3.SS1.p1.11.m11.1.1.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">R_{x}</annotation></semantics></math> and azimuth <math id="S3.SS1.p1.12.m12.1" class="ltx_Math" alttext="R_{y}" display="inline"><semantics id="S3.SS1.p1.12.m12.1a"><msub id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml"><mi id="S3.SS1.p1.12.m12.1.1.2" xref="S3.SS1.p1.12.m12.1.1.2.cmml">R</mi><mi id="S3.SS1.p1.12.m12.1.1.3" xref="S3.SS1.p1.12.m12.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><apply id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.12.m12.1.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">subscript</csymbol><ci id="S3.SS1.p1.12.m12.1.1.2.cmml" xref="S3.SS1.p1.12.m12.1.1.2">ğ‘…</ci><ci id="S3.SS1.p1.12.m12.1.1.3.cmml" xref="S3.SS1.p1.12.m12.1.1.3">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">R_{y}</annotation></semantics></math>. The remaining transformations are modeled using 2D image warping operations <math id="S3.SS1.p1.13.m13.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S3.SS1.p1.13.m13.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml">ğ’²</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><ci id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1">ğ’²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">\mathcal{W}</annotation></semantics></math> derived from 3D translations <math id="S3.SS1.p1.14.m14.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p1.14.m14.1a"><mi id="S3.SS1.p1.14.m14.1.1" xref="S3.SS1.p1.14.m14.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m14.1b"><ci id="S3.SS1.p1.14.m14.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m14.1c">T</annotation></semantics></math> and in-plane rotation <math id="S3.SS1.p1.15.m15.1" class="ltx_Math" alttext="R_{z}" display="inline"><semantics id="S3.SS1.p1.15.m15.1a"><msub id="S3.SS1.p1.15.m15.1.1" xref="S3.SS1.p1.15.m15.1.1.cmml"><mi id="S3.SS1.p1.15.m15.1.1.2" xref="S3.SS1.p1.15.m15.1.1.2.cmml">R</mi><mi id="S3.SS1.p1.15.m15.1.1.3" xref="S3.SS1.p1.15.m15.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m15.1b"><apply id="S3.SS1.p1.15.m15.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.15.m15.1.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1">subscript</csymbol><ci id="S3.SS1.p1.15.m15.1.1.2.cmml" xref="S3.SS1.p1.15.m15.1.1.2">ğ‘…</ci><ci id="S3.SS1.p1.15.m15.1.1.3.cmml" xref="S3.SS1.p1.15.m15.1.1.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m15.1c">R_{z}</annotation></semantics></math>. The full generation process is defined as <math id="S3.SS1.p1.16.m16.1" class="ltx_Math" alttext="G=\mathcal{W}\circ G_{3D}" display="inline"><semantics id="S3.SS1.p1.16.m16.1a"><mrow id="S3.SS1.p1.16.m16.1.1" xref="S3.SS1.p1.16.m16.1.1.cmml"><mi id="S3.SS1.p1.16.m16.1.1.2" xref="S3.SS1.p1.16.m16.1.1.2.cmml">G</mi><mo id="S3.SS1.p1.16.m16.1.1.1" xref="S3.SS1.p1.16.m16.1.1.1.cmml">=</mo><mrow id="S3.SS1.p1.16.m16.1.1.3" xref="S3.SS1.p1.16.m16.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.16.m16.1.1.3.2" xref="S3.SS1.p1.16.m16.1.1.3.2.cmml">ğ’²</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.16.m16.1.1.3.1" xref="S3.SS1.p1.16.m16.1.1.3.1.cmml">âˆ˜</mo><msub id="S3.SS1.p1.16.m16.1.1.3.3" xref="S3.SS1.p1.16.m16.1.1.3.3.cmml"><mi id="S3.SS1.p1.16.m16.1.1.3.3.2" xref="S3.SS1.p1.16.m16.1.1.3.3.2.cmml">G</mi><mrow id="S3.SS1.p1.16.m16.1.1.3.3.3" xref="S3.SS1.p1.16.m16.1.1.3.3.3.cmml"><mn id="S3.SS1.p1.16.m16.1.1.3.3.3.2" xref="S3.SS1.p1.16.m16.1.1.3.3.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p1.16.m16.1.1.3.3.3.1" xref="S3.SS1.p1.16.m16.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.16.m16.1.1.3.3.3.3" xref="S3.SS1.p1.16.m16.1.1.3.3.3.3.cmml">D</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.16.m16.1b"><apply id="S3.SS1.p1.16.m16.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1"><eq id="S3.SS1.p1.16.m16.1.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1.1"></eq><ci id="S3.SS1.p1.16.m16.1.1.2.cmml" xref="S3.SS1.p1.16.m16.1.1.2">ğº</ci><apply id="S3.SS1.p1.16.m16.1.1.3.cmml" xref="S3.SS1.p1.16.m16.1.1.3"><compose id="S3.SS1.p1.16.m16.1.1.3.1.cmml" xref="S3.SS1.p1.16.m16.1.1.3.1"></compose><ci id="S3.SS1.p1.16.m16.1.1.3.2.cmml" xref="S3.SS1.p1.16.m16.1.1.3.2">ğ’²</ci><apply id="S3.SS1.p1.16.m16.1.1.3.3.cmml" xref="S3.SS1.p1.16.m16.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.16.m16.1.1.3.3.1.cmml" xref="S3.SS1.p1.16.m16.1.1.3.3">subscript</csymbol><ci id="S3.SS1.p1.16.m16.1.1.3.3.2.cmml" xref="S3.SS1.p1.16.m16.1.1.3.3.2">ğº</ci><apply id="S3.SS1.p1.16.m16.1.1.3.3.3.cmml" xref="S3.SS1.p1.16.m16.1.1.3.3.3"><times id="S3.SS1.p1.16.m16.1.1.3.3.3.1.cmml" xref="S3.SS1.p1.16.m16.1.1.3.3.3.1"></times><cn type="integer" id="S3.SS1.p1.16.m16.1.1.3.3.3.2.cmml" xref="S3.SS1.p1.16.m16.1.1.3.3.3.2">3</cn><ci id="S3.SS1.p1.16.m16.1.1.3.3.3.3.cmml" xref="S3.SS1.p1.16.m16.1.1.3.3.3.3">ğ·</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.16.m16.1c">G=\mathcal{W}\circ G_{3D}</annotation></semantics></math>.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Appearance and 3D Rotation.</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.3" class="ltx_p">In order to generate images of objects in diverse appearances, shapes and poses, we adopt a 3D style-based image generation network similar to the one proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> as illustrated in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 Method â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
This network combines a 3D feature volume which faithfully captures 3D rotations with a style-based generator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
This enables our model to disentangle global appearance variation from geometric factors such as pose and shape. The 3D style-based image generation network consists of four main steps: i) generating the 3D feature volume, ii) transforming the feature volume based on the pose, iii) projecting the 3D volume into a 2D feature map, and iv) decoding the feature map into the predicted image <math id="S3.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\hat{I}" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mover accent="true" id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml">I</mi><mo id="S3.SS1.SSS1.p1.1.m1.1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1"><ci id="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.1">^</ci><ci id="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.2">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">\hat{I}</annotation></semantics></math>.
Both, the 3D generator and the 2D generator are conditioned on the latent code <math id="S3.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><mi id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><ci id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">z</annotation></semantics></math> via adaptive instance normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to model the variance in shape and appearance respectively. The object orientation <math id="S3.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><mi id="S3.SS1.SSS1.p1.3.m3.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><ci id="S3.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">R</annotation></semantics></math> controls the transformation that is applied to the 3D feature volume.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Translation and 2D Rotation.</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.3" class="ltx_p">While the 3D style-based decoder can in principle cover the entire space of 6 DoF poses,
the resulting model would require a very large capacity and dataset to be trained on.
We therefore constrain the decoder to out-of-plane rotations and handle all remaining transformation using a similarity 2D transformation. The warping field is given by:</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.6" class="ltx_Math" alttext="\displaystyle\mathcal{W}(T,R_{z}):\begin{bmatrix}u\\
v\end{bmatrix}\mapsto\frac{f}{t_{z}}\cdot\left(R_{z}\begin{bmatrix}u\\
v\end{bmatrix}+\begin{bmatrix}t_{x}\\
t_{y}\end{bmatrix}\right)" display="inline"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.5.5.1.3" xref="S3.E1.m1.5.5.1.3.cmml">ğ’²</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.1.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml">(</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">T</mi><mo id="S3.E1.m1.5.5.1.1.1.3" xref="S3.E1.m1.5.5.1.1.2.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">R</mi><mi id="S3.E1.m1.5.5.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.3.cmml">z</mi></msub><mo rspace="0.278em" stretchy="false" id="S3.E1.m1.5.5.1.1.1.4" xref="S3.E1.m1.5.5.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.278em" id="S3.E1.m1.6.6.3" xref="S3.E1.m1.6.6.3.cmml">:</mo><mrow id="S3.E1.m1.6.6.2" xref="S3.E1.m1.6.6.2.cmml"><mrow id="S3.E1.m1.1.1a.3" xref="S3.E1.m1.1.1a.2.cmml"><mo id="S3.E1.m1.1.1a.3.1" xref="S3.E1.m1.1.1a.2.1.cmml">[</mo><mtable rowspacing="0pt" id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mtr id="S3.E1.m1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1b" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">u</mi></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1c" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1d" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.cmml">v</mi></mtd></mtr></mtable><mo id="S3.E1.m1.1.1a.3.2" xref="S3.E1.m1.1.1a.2.1.cmml">]</mo></mrow><mo stretchy="false" id="S3.E1.m1.6.6.2.2" xref="S3.E1.m1.6.6.2.2.cmml">â†¦</mo><mrow id="S3.E1.m1.6.6.2.1" xref="S3.E1.m1.6.6.2.1.cmml"><mstyle displaystyle="true" id="S3.E1.m1.6.6.2.1.3" xref="S3.E1.m1.6.6.2.1.3.cmml"><mfrac id="S3.E1.m1.6.6.2.1.3a" xref="S3.E1.m1.6.6.2.1.3.cmml"><mi id="S3.E1.m1.6.6.2.1.3.2" xref="S3.E1.m1.6.6.2.1.3.2.cmml">f</mi><msub id="S3.E1.m1.6.6.2.1.3.3" xref="S3.E1.m1.6.6.2.1.3.3.cmml"><mi id="S3.E1.m1.6.6.2.1.3.3.2" xref="S3.E1.m1.6.6.2.1.3.3.2.cmml">t</mi><mi id="S3.E1.m1.6.6.2.1.3.3.3" xref="S3.E1.m1.6.6.2.1.3.3.3.cmml">z</mi></msub></mfrac></mstyle><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.6.6.2.1.2" xref="S3.E1.m1.6.6.2.1.2.cmml">â‹…</mo><mrow id="S3.E1.m1.6.6.2.1.1.1" xref="S3.E1.m1.6.6.2.1.1.1.1.cmml"><mo id="S3.E1.m1.6.6.2.1.1.1.2" xref="S3.E1.m1.6.6.2.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.6.6.2.1.1.1.1" xref="S3.E1.m1.6.6.2.1.1.1.1.cmml"><mrow id="S3.E1.m1.6.6.2.1.1.1.1.2" xref="S3.E1.m1.6.6.2.1.1.1.1.2.cmml"><msub id="S3.E1.m1.6.6.2.1.1.1.1.2.2" xref="S3.E1.m1.6.6.2.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.6.6.2.1.1.1.1.2.2.2" xref="S3.E1.m1.6.6.2.1.1.1.1.2.2.2.cmml">R</mi><mi id="S3.E1.m1.6.6.2.1.1.1.1.2.2.3" xref="S3.E1.m1.6.6.2.1.1.1.1.2.2.3.cmml">z</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.2.1.1.1.1.2.1" xref="S3.E1.m1.6.6.2.1.1.1.1.2.1.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2a.3" xref="S3.E1.m1.2.2a.2.cmml"><mo id="S3.E1.m1.2.2a.3.1" xref="S3.E1.m1.2.2a.2.1.cmml">[</mo><mtable rowspacing="0pt" id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mtr id="S3.E1.m1.2.2.1.1a" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1b" xref="S3.E1.m1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml">u</mi></mtd></mtr><mtr id="S3.E1.m1.2.2.1.1c" xref="S3.E1.m1.2.2.1.1.cmml"><mtd id="S3.E1.m1.2.2.1.1d" xref="S3.E1.m1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.cmml">v</mi></mtd></mtr></mtable><mo id="S3.E1.m1.2.2a.3.2" xref="S3.E1.m1.2.2a.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.6.6.2.1.1.1.1.1" xref="S3.E1.m1.6.6.2.1.1.1.1.1.cmml">+</mo><mrow id="S3.E1.m1.3.3a.3" xref="S3.E1.m1.3.3a.2.cmml"><mo id="S3.E1.m1.3.3a.3.1" xref="S3.E1.m1.3.3a.2.1.cmml">[</mo><mtable rowspacing="0pt" id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mtr id="S3.E1.m1.3.3.1.1a" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1b" xref="S3.E1.m1.3.3.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml">t</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.3.cmml">x</mi></msub></mtd></mtr><mtr id="S3.E1.m1.3.3.1.1c" xref="S3.E1.m1.3.3.1.1.cmml"><mtd id="S3.E1.m1.3.3.1.1d" xref="S3.E1.m1.3.3.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.2.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.2.1.1.2" xref="S3.E1.m1.3.3.1.1.2.1.1.2.cmml">t</mi><mi id="S3.E1.m1.3.3.1.1.2.1.1.3" xref="S3.E1.m1.3.3.1.1.2.1.1.3.cmml">y</mi></msub></mtd></mtr></mtable><mo id="S3.E1.m1.3.3a.3.2" xref="S3.E1.m1.3.3a.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.6.6.2.1.1.1.3" xref="S3.E1.m1.6.6.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"><ci id="S3.E1.m1.6.6.3.cmml" xref="S3.E1.m1.6.6.3">:</ci><apply id="S3.E1.m1.5.5.1.cmml" xref="S3.E1.m1.5.5.1"><times id="S3.E1.m1.5.5.1.2.cmml" xref="S3.E1.m1.5.5.1.2"></times><ci id="S3.E1.m1.5.5.1.3.cmml" xref="S3.E1.m1.5.5.1.3">ğ’²</ci><interval closure="open" id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1"><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">ğ‘‡</ci><apply id="S3.E1.m1.5.5.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2">ğ‘…</ci><ci id="S3.E1.m1.5.5.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3">ğ‘§</ci></apply></interval></apply><apply id="S3.E1.m1.6.6.2.cmml" xref="S3.E1.m1.6.6.2"><csymbol cd="latexml" id="S3.E1.m1.6.6.2.2.cmml" xref="S3.E1.m1.6.6.2.2">maps-to</csymbol><apply id="S3.E1.m1.1.1a.2.cmml" xref="S3.E1.m1.1.1a.3"><csymbol cd="latexml" id="S3.E1.m1.1.1a.2.1.cmml" xref="S3.E1.m1.1.1a.3.1">matrix</csymbol><matrix id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><matrixrow id="S3.E1.m1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1.1"><ci id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">ğ‘¢</ci></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1.1"><ci id="S3.E1.m1.1.1.1.1.2.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1">ğ‘£</ci></matrixrow></matrix></apply><apply id="S3.E1.m1.6.6.2.1.cmml" xref="S3.E1.m1.6.6.2.1"><ci id="S3.E1.m1.6.6.2.1.2.cmml" xref="S3.E1.m1.6.6.2.1.2">â‹…</ci><apply id="S3.E1.m1.6.6.2.1.3.cmml" xref="S3.E1.m1.6.6.2.1.3"><divide id="S3.E1.m1.6.6.2.1.3.1.cmml" xref="S3.E1.m1.6.6.2.1.3"></divide><ci id="S3.E1.m1.6.6.2.1.3.2.cmml" xref="S3.E1.m1.6.6.2.1.3.2">ğ‘“</ci><apply id="S3.E1.m1.6.6.2.1.3.3.cmml" xref="S3.E1.m1.6.6.2.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.2.1.3.3.1.cmml" xref="S3.E1.m1.6.6.2.1.3.3">subscript</csymbol><ci id="S3.E1.m1.6.6.2.1.3.3.2.cmml" xref="S3.E1.m1.6.6.2.1.3.3.2">ğ‘¡</ci><ci id="S3.E1.m1.6.6.2.1.3.3.3.cmml" xref="S3.E1.m1.6.6.2.1.3.3.3">ğ‘§</ci></apply></apply><apply id="S3.E1.m1.6.6.2.1.1.1.1.cmml" xref="S3.E1.m1.6.6.2.1.1.1"><plus id="S3.E1.m1.6.6.2.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.2.1.1.1.1.1"></plus><apply id="S3.E1.m1.6.6.2.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.2.1.1.1.1.2"><times id="S3.E1.m1.6.6.2.1.1.1.1.2.1.cmml" xref="S3.E1.m1.6.6.2.1.1.1.1.2.1"></times><apply id="S3.E1.m1.6.6.2.1.1.1.1.2.2.cmml" xref="S3.E1.m1.6.6.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.2.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.6.6.2.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.6.6.2.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.6.6.2.1.1.1.1.2.2.2">ğ‘…</ci><ci id="S3.E1.m1.6.6.2.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.6.6.2.1.1.1.1.2.2.3">ğ‘§</ci></apply><apply id="S3.E1.m1.2.2a.2.cmml" xref="S3.E1.m1.2.2a.3"><csymbol cd="latexml" id="S3.E1.m1.2.2a.2.1.cmml" xref="S3.E1.m1.2.2a.3.1">matrix</csymbol><matrix id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><matrixrow id="S3.E1.m1.2.2.1.1a.cmml" xref="S3.E1.m1.2.2.1.1"><ci id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1">ğ‘¢</ci></matrixrow><matrixrow id="S3.E1.m1.2.2.1.1b.cmml" xref="S3.E1.m1.2.2.1.1"><ci id="S3.E1.m1.2.2.1.1.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1">ğ‘£</ci></matrixrow></matrix></apply></apply><apply id="S3.E1.m1.3.3a.2.cmml" xref="S3.E1.m1.3.3a.3"><csymbol cd="latexml" id="S3.E1.m1.3.3a.2.1.cmml" xref="S3.E1.m1.3.3a.3.1">matrix</csymbol><matrix id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1"><matrixrow id="S3.E1.m1.3.3.1.1a.cmml" xref="S3.E1.m1.3.3.1.1"><apply id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2">ğ‘¡</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3">ğ‘¥</ci></apply></matrixrow><matrixrow id="S3.E1.m1.3.3.1.1b.cmml" xref="S3.E1.m1.3.3.1.1"><apply id="S3.E1.m1.3.3.1.1.2.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.2">ğ‘¡</ci><ci id="S3.E1.m1.3.3.1.1.2.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.3">ğ‘¦</ci></apply></matrixrow></matrix></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">\displaystyle\mathcal{W}(T,R_{z}):\begin{bmatrix}u\\
v\end{bmatrix}\mapsto\frac{f}{t_{z}}\cdot\left(R_{z}\begin{bmatrix}u\\
v\end{bmatrix}+\begin{bmatrix}t_{x}\\
t_{y}\end{bmatrix}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS2.p1.2" class="ltx_p">We use <math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">ğ’²</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><ci id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">ğ’²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">\mathcal{W}</annotation></semantics></math> to warp the generated image, yielding the final image <math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\hat{I}" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mover accent="true" id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p1.2.m2.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml">I</mi><mo id="S3.SS1.SSS2.p1.2.m2.1.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><apply id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1"><ci id="S3.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.1">^</ci><ci id="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.2">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">\hat{I}</annotation></semantics></math>:</p>
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.8" class="ltx_Math" alttext="\displaystyle\hat{I}=G(R,T,z)=\mathcal{W}(T,R_{z})\circ G_{3D}(R_{x},R_{y},z)" display="inline"><semantics id="S3.E2.m1.8a"><mrow id="S3.E2.m1.8.8" xref="S3.E2.m1.8.8.cmml"><mover accent="true" id="S3.E2.m1.8.8.5" xref="S3.E2.m1.8.8.5.cmml"><mi id="S3.E2.m1.8.8.5.2" xref="S3.E2.m1.8.8.5.2.cmml">I</mi><mo id="S3.E2.m1.8.8.5.1" xref="S3.E2.m1.8.8.5.1.cmml">^</mo></mover><mo id="S3.E2.m1.8.8.6" xref="S3.E2.m1.8.8.6.cmml">=</mo><mrow id="S3.E2.m1.8.8.7" xref="S3.E2.m1.8.8.7.cmml"><mi id="S3.E2.m1.8.8.7.2" xref="S3.E2.m1.8.8.7.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.8.8.7.1" xref="S3.E2.m1.8.8.7.1.cmml">â€‹</mo><mrow id="S3.E2.m1.8.8.7.3.2" xref="S3.E2.m1.8.8.7.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.8.8.7.3.2.1" xref="S3.E2.m1.8.8.7.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">R</mi><mo id="S3.E2.m1.8.8.7.3.2.2" xref="S3.E2.m1.8.8.7.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">T</mi><mo id="S3.E2.m1.8.8.7.3.2.3" xref="S3.E2.m1.8.8.7.3.1.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">z</mi><mo stretchy="false" id="S3.E2.m1.8.8.7.3.2.4" xref="S3.E2.m1.8.8.7.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.8.8.8" xref="S3.E2.m1.8.8.8.cmml">=</mo><mrow id="S3.E2.m1.8.8.3" xref="S3.E2.m1.8.8.3.cmml"><mrow id="S3.E2.m1.6.6.1.1" xref="S3.E2.m1.6.6.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.1" xref="S3.E2.m1.6.6.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.6.6.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.3.cmml">ğ’²</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.2.cmml">(</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">T</mi><mo id="S3.E2.m1.6.6.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.2.cmml">,</mo><msub id="S3.E2.m1.6.6.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.cmml">R</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.3.cmml">z</mi></msub><mo rspace="0.055em" stretchy="false" id="S3.E2.m1.6.6.1.1.1.1.1.4" xref="S3.E2.m1.6.6.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E2.m1.6.6.1.1.2" xref="S3.E2.m1.6.6.1.1.2.cmml">âˆ˜</mo><msub id="S3.E2.m1.6.6.1.1.3" xref="S3.E2.m1.6.6.1.1.3.cmml"><mi id="S3.E2.m1.6.6.1.1.3.2" xref="S3.E2.m1.6.6.1.1.3.2.cmml">G</mi><mrow id="S3.E2.m1.6.6.1.1.3.3" xref="S3.E2.m1.6.6.1.1.3.3.cmml"><mn id="S3.E2.m1.6.6.1.1.3.3.2" xref="S3.E2.m1.6.6.1.1.3.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.1.1.3.3.1" xref="S3.E2.m1.6.6.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.6.6.1.1.3.3.3" xref="S3.E2.m1.6.6.1.1.3.3.3.cmml">D</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.8.8.3.4" xref="S3.E2.m1.8.8.3.4.cmml">â€‹</mo><mrow id="S3.E2.m1.8.8.3.3.2" xref="S3.E2.m1.8.8.3.3.3.cmml"><mo stretchy="false" id="S3.E2.m1.8.8.3.3.2.3" xref="S3.E2.m1.8.8.3.3.3.cmml">(</mo><msub id="S3.E2.m1.7.7.2.2.1.1" xref="S3.E2.m1.7.7.2.2.1.1.cmml"><mi id="S3.E2.m1.7.7.2.2.1.1.2" xref="S3.E2.m1.7.7.2.2.1.1.2.cmml">R</mi><mi id="S3.E2.m1.7.7.2.2.1.1.3" xref="S3.E2.m1.7.7.2.2.1.1.3.cmml">x</mi></msub><mo id="S3.E2.m1.8.8.3.3.2.4" xref="S3.E2.m1.8.8.3.3.3.cmml">,</mo><msub id="S3.E2.m1.8.8.3.3.2.2" xref="S3.E2.m1.8.8.3.3.2.2.cmml"><mi id="S3.E2.m1.8.8.3.3.2.2.2" xref="S3.E2.m1.8.8.3.3.2.2.2.cmml">R</mi><mi id="S3.E2.m1.8.8.3.3.2.2.3" xref="S3.E2.m1.8.8.3.3.2.2.3.cmml">y</mi></msub><mo id="S3.E2.m1.8.8.3.3.2.5" xref="S3.E2.m1.8.8.3.3.3.cmml">,</mo><mi id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">z</mi><mo stretchy="false" id="S3.E2.m1.8.8.3.3.2.6" xref="S3.E2.m1.8.8.3.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.8b"><apply id="S3.E2.m1.8.8.cmml" xref="S3.E2.m1.8.8"><and id="S3.E2.m1.8.8a.cmml" xref="S3.E2.m1.8.8"></and><apply id="S3.E2.m1.8.8b.cmml" xref="S3.E2.m1.8.8"><eq id="S3.E2.m1.8.8.6.cmml" xref="S3.E2.m1.8.8.6"></eq><apply id="S3.E2.m1.8.8.5.cmml" xref="S3.E2.m1.8.8.5"><ci id="S3.E2.m1.8.8.5.1.cmml" xref="S3.E2.m1.8.8.5.1">^</ci><ci id="S3.E2.m1.8.8.5.2.cmml" xref="S3.E2.m1.8.8.5.2">ğ¼</ci></apply><apply id="S3.E2.m1.8.8.7.cmml" xref="S3.E2.m1.8.8.7"><times id="S3.E2.m1.8.8.7.1.cmml" xref="S3.E2.m1.8.8.7.1"></times><ci id="S3.E2.m1.8.8.7.2.cmml" xref="S3.E2.m1.8.8.7.2">ğº</ci><vector id="S3.E2.m1.8.8.7.3.1.cmml" xref="S3.E2.m1.8.8.7.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ğ‘…</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">ğ‘‡</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">ğ‘§</ci></vector></apply></apply><apply id="S3.E2.m1.8.8c.cmml" xref="S3.E2.m1.8.8"><eq id="S3.E2.m1.8.8.8.cmml" xref="S3.E2.m1.8.8.8"></eq><share href="#S3.E2.m1.8.8.7.cmml" id="S3.E2.m1.8.8d.cmml" xref="S3.E2.m1.8.8"></share><apply id="S3.E2.m1.8.8.3.cmml" xref="S3.E2.m1.8.8.3"><times id="S3.E2.m1.8.8.3.4.cmml" xref="S3.E2.m1.8.8.3.4"></times><apply id="S3.E2.m1.6.6.1.1.cmml" xref="S3.E2.m1.6.6.1.1"><compose id="S3.E2.m1.6.6.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.2"></compose><apply id="S3.E2.m1.6.6.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1"><times id="S3.E2.m1.6.6.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.2"></times><ci id="S3.E2.m1.6.6.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.3">ğ’²</ci><interval closure="open" id="S3.E2.m1.6.6.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1"><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">ğ‘‡</ci><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2">ğ‘…</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.3">ğ‘§</ci></apply></interval></apply><apply id="S3.E2.m1.6.6.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.3">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.3.2">ğº</ci><apply id="S3.E2.m1.6.6.1.1.3.3.cmml" xref="S3.E2.m1.6.6.1.1.3.3"><times id="S3.E2.m1.6.6.1.1.3.3.1.cmml" xref="S3.E2.m1.6.6.1.1.3.3.1"></times><cn type="integer" id="S3.E2.m1.6.6.1.1.3.3.2.cmml" xref="S3.E2.m1.6.6.1.1.3.3.2">3</cn><ci id="S3.E2.m1.6.6.1.1.3.3.3.cmml" xref="S3.E2.m1.6.6.1.1.3.3.3">ğ·</ci></apply></apply></apply><vector id="S3.E2.m1.8.8.3.3.3.cmml" xref="S3.E2.m1.8.8.3.3.2"><apply id="S3.E2.m1.7.7.2.2.1.1.cmml" xref="S3.E2.m1.7.7.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.2.2.1.1.1.cmml" xref="S3.E2.m1.7.7.2.2.1.1">subscript</csymbol><ci id="S3.E2.m1.7.7.2.2.1.1.2.cmml" xref="S3.E2.m1.7.7.2.2.1.1.2">ğ‘…</ci><ci id="S3.E2.m1.7.7.2.2.1.1.3.cmml" xref="S3.E2.m1.7.7.2.2.1.1.3">ğ‘¥</ci></apply><apply id="S3.E2.m1.8.8.3.3.2.2.cmml" xref="S3.E2.m1.8.8.3.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.3.3.2.2.1.cmml" xref="S3.E2.m1.8.8.3.3.2.2">subscript</csymbol><ci id="S3.E2.m1.8.8.3.3.2.2.2.cmml" xref="S3.E2.m1.8.8.3.3.2.2.2">ğ‘…</ci><ci id="S3.E2.m1.8.8.3.3.2.2.3.cmml" xref="S3.E2.m1.8.8.3.3.2.2.3">ğ‘¦</ci></apply><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">ğ‘§</ci></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.8c">\displaystyle\hat{I}=G(R,T,z)=\mathcal{W}(T,R_{z})\circ G_{3D}(R_{x},R_{y},z)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We train our image generator in a conditional VAE framework as illustrated in Fig.Â <a href="#S2.F1" title="Figure 1 â€£ 2.2 Pose Dependent Image Generation â€£ 2 Related Work â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> in order to achieve precise pose control over the generated image. A VAE is an auto-encoder trained by minimizing a reconstruction term and the KL divergence between the latent space distribution and a normalized Gaussian. We use our 3D style-based image generation network as decoder and a standard CNN as encoder.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">At each training iteration, the encoder first extracts the latent code from an image of a randomly chosen training object. Then the 3D image generation network takes this latent code together with the desired pose as input to generate an image <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\hat{I}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mover accent="true" id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">I</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><ci id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1">^</ci><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\hat{I}</annotation></semantics></math> of the chosen object in the <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">desired</em> pose. The encoder and decoder are jointly trained by minimizing the reconstruction loss between the generated image and the ground-truth, regularized via the KL divergence:</p>
<table id="S5.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}=\|I-\hat{I}\|_{1}+\lambda_{KL}~{}D_{KL}" display="inline"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml">â„’</mi><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.2.cmml">I</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mover accent="true" id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.cmml">I</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><msub id="S3.E3.m1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.3.2.cmml"><mi id="S3.E3.m1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.3.2.2.cmml">Î»</mi><mrow id="S3.E3.m1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.3.2.3.cmml"><mi id="S3.E3.m1.1.1.1.3.2.3.2" xref="S3.E3.m1.1.1.1.3.2.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.2.3.1" xref="S3.E3.m1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.3.2.3.3" xref="S3.E3.m1.1.1.1.3.2.3.3.cmml">L</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.3.1.cmml">â€‹</mo><msub id="S3.E3.m1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.3.3.2.cmml">D</mi><mrow id="S3.E3.m1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.1.3.3.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.3.3.1" xref="S3.E3.m1.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.1.3.3.3.3.cmml">L</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><ci id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3">â„’</ci><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><plus id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></plus><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><minus id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2">ğ¼</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3"><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2">ğ¼</ci></apply></apply></apply><cn type="integer" id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3">1</cn></apply><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><times id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3.1"></times><apply id="S3.E3.m1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.3.2.2">ğœ†</ci><apply id="S3.E3.m1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.3.2.3"><times id="S3.E3.m1.1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.1.3.2.3.1"></times><ci id="S3.E3.m1.1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.1.1.1.3.2.3.2">ğ¾</ci><ci id="S3.E3.m1.1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.1.1.1.3.2.3.3">ğ¿</ci></apply></apply><apply id="S3.E3.m1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.3.3.2">ğ·</ci><apply id="S3.E3.m1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3.3"><times id="S3.E3.m1.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.1.3.3.3.1"></times><ci id="S3.E3.m1.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.1.3.3.3.2">ğ¾</ci><ci id="S3.E3.m1.1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3.3.3">ğ¿</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle\mathcal{L}=\|I-\hat{I}\|_{1}+\lambda_{KL}~{}D_{KL}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.3" class="ltx_p">where <math id="S3.SS2.p2.2.m1.1" class="ltx_Math" alttext="\lambda_{KL}" display="inline"><semantics id="S3.SS2.p2.2.m1.1a"><msub id="S3.SS2.p2.2.m1.1.1" xref="S3.SS2.p2.2.m1.1.1.cmml"><mi id="S3.SS2.p2.2.m1.1.1.2" xref="S3.SS2.p2.2.m1.1.1.2.cmml">Î»</mi><mrow id="S3.SS2.p2.2.m1.1.1.3" xref="S3.SS2.p2.2.m1.1.1.3.cmml"><mi id="S3.SS2.p2.2.m1.1.1.3.2" xref="S3.SS2.p2.2.m1.1.1.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m1.1.1.3.1" xref="S3.SS2.p2.2.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.2.m1.1.1.3.3" xref="S3.SS2.p2.2.m1.1.1.3.3.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m1.1b"><apply id="S3.SS2.p2.2.m1.1.1.cmml" xref="S3.SS2.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m1.1.1.1.cmml" xref="S3.SS2.p2.2.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m1.1.1.2.cmml" xref="S3.SS2.p2.2.m1.1.1.2">ğœ†</ci><apply id="S3.SS2.p2.2.m1.1.1.3.cmml" xref="S3.SS2.p2.2.m1.1.1.3"><times id="S3.SS2.p2.2.m1.1.1.3.1.cmml" xref="S3.SS2.p2.2.m1.1.1.3.1"></times><ci id="S3.SS2.p2.2.m1.1.1.3.2.cmml" xref="S3.SS2.p2.2.m1.1.1.3.2">ğ¾</ci><ci id="S3.SS2.p2.2.m1.1.1.3.3.cmml" xref="S3.SS2.p2.2.m1.1.1.3.3">ğ¿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m1.1c">\lambda_{KL}</annotation></semantics></math> weights the regularization term and is set to <math id="S3.SS2.p2.3.m2.1" class="ltx_Math" alttext="1e^{-2}" display="inline"><semantics id="S3.SS2.p2.3.m2.1a"><mrow id="S3.SS2.p2.3.m2.1.1" xref="S3.SS2.p2.3.m2.1.1.cmml"><mn id="S3.SS2.p2.3.m2.1.1.2" xref="S3.SS2.p2.3.m2.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m2.1.1.1" xref="S3.SS2.p2.3.m2.1.1.1.cmml">â€‹</mo><msup id="S3.SS2.p2.3.m2.1.1.3" xref="S3.SS2.p2.3.m2.1.1.3.cmml"><mi id="S3.SS2.p2.3.m2.1.1.3.2" xref="S3.SS2.p2.3.m2.1.1.3.2.cmml">e</mi><mrow id="S3.SS2.p2.3.m2.1.1.3.3" xref="S3.SS2.p2.3.m2.1.1.3.3.cmml"><mo id="S3.SS2.p2.3.m2.1.1.3.3a" xref="S3.SS2.p2.3.m2.1.1.3.3.cmml">âˆ’</mo><mn id="S3.SS2.p2.3.m2.1.1.3.3.2" xref="S3.SS2.p2.3.m2.1.1.3.3.2.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m2.1b"><apply id="S3.SS2.p2.3.m2.1.1.cmml" xref="S3.SS2.p2.3.m2.1.1"><times id="S3.SS2.p2.3.m2.1.1.1.cmml" xref="S3.SS2.p2.3.m2.1.1.1"></times><cn type="integer" id="S3.SS2.p2.3.m2.1.1.2.cmml" xref="S3.SS2.p2.3.m2.1.1.2">1</cn><apply id="S3.SS2.p2.3.m2.1.1.3.cmml" xref="S3.SS2.p2.3.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m2.1.1.3.1.cmml" xref="S3.SS2.p2.3.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.3.m2.1.1.3.2.cmml" xref="S3.SS2.p2.3.m2.1.1.3.2">ğ‘’</ci><apply id="S3.SS2.p2.3.m2.1.1.3.3.cmml" xref="S3.SS2.p2.3.m2.1.1.3.3"><minus id="S3.SS2.p2.3.m2.1.1.3.3.1.cmml" xref="S3.SS2.p2.3.m2.1.1.3.3"></minus><cn type="integer" id="S3.SS2.p2.3.m2.1.1.3.3.2.cmml" xref="S3.SS2.p2.3.m2.1.1.3.3.2">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m2.1c">1e^{-2}</annotation></semantics></math> in our experiments. The required training data, namely images of objects in difference poses and the corresponding pose label, is obtained by rendering synthetic objects from the ShapeNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Since translation and 2D rotation is modelled by similarity transformation which does not require training, we only generate training samples with out-of-plane rotations for the sake of training efficiency.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Object Pose Estimation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">The trained pose-aware image generator can render objects in various shapes, appearances and poses.
Since the forward process is differentiable, we can solve the inverse problem of recovering its pose, shape and appearance by iteratively refining the network inputs (i.e., the pose parameters and the latent code), so that the discrepancy between the generated and the target image is minimized:</p>
<table id="S5.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.10" class="ltx_Math" alttext="\displaystyle R^{*},T^{*},z^{*}=\operatorname*{argmin~{}}_{R,T,z}~{}E(I,R,T,z)" display="inline"><semantics id="S3.E4.m1.10a"><mrow id="S3.E4.m1.10.10" xref="S3.E4.m1.10.10.cmml"><mrow id="S3.E4.m1.10.10.3.3" xref="S3.E4.m1.10.10.3.4.cmml"><msup id="S3.E4.m1.8.8.1.1.1" xref="S3.E4.m1.8.8.1.1.1.cmml"><mi id="S3.E4.m1.8.8.1.1.1.2" xref="S3.E4.m1.8.8.1.1.1.2.cmml">R</mi><mo id="S3.E4.m1.8.8.1.1.1.3" xref="S3.E4.m1.8.8.1.1.1.3.cmml">âˆ—</mo></msup><mo id="S3.E4.m1.10.10.3.3.4" xref="S3.E4.m1.10.10.3.4.cmml">,</mo><msup id="S3.E4.m1.9.9.2.2.2" xref="S3.E4.m1.9.9.2.2.2.cmml"><mi id="S3.E4.m1.9.9.2.2.2.2" xref="S3.E4.m1.9.9.2.2.2.2.cmml">T</mi><mo id="S3.E4.m1.9.9.2.2.2.3" xref="S3.E4.m1.9.9.2.2.2.3.cmml">âˆ—</mo></msup><mo id="S3.E4.m1.10.10.3.3.5" xref="S3.E4.m1.10.10.3.4.cmml">,</mo><msup id="S3.E4.m1.10.10.3.3.3" xref="S3.E4.m1.10.10.3.3.3.cmml"><mi id="S3.E4.m1.10.10.3.3.3.2" xref="S3.E4.m1.10.10.3.3.3.2.cmml">z</mi><mo id="S3.E4.m1.10.10.3.3.3.3" xref="S3.E4.m1.10.10.3.3.3.3.cmml">âˆ—</mo></msup></mrow><mo rspace="0.1389em" id="S3.E4.m1.10.10.4" xref="S3.E4.m1.10.10.4.cmml">=</mo><mrow id="S3.E4.m1.10.10.5" xref="S3.E4.m1.10.10.5.cmml"><mrow id="S3.E4.m1.10.10.5.2" xref="S3.E4.m1.10.10.5.2.cmml"><munder id="S3.E4.m1.10.10.5.2.1" xref="S3.E4.m1.10.10.5.2.1.cmml"><mo lspace="0.1389em" rspace="0.497em" id="S3.E4.m1.10.10.5.2.1.2" xref="S3.E4.m1.10.10.5.2.1.2.cmml">argmin</mo><mrow id="S3.E4.m1.3.3.3.5" xref="S3.E4.m1.3.3.3.4.cmml"><mi id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml">R</mi><mo id="S3.E4.m1.3.3.3.5.1" xref="S3.E4.m1.3.3.3.4.cmml">,</mo><mi id="S3.E4.m1.2.2.2.2" xref="S3.E4.m1.2.2.2.2.cmml">T</mi><mo id="S3.E4.m1.3.3.3.5.2" xref="S3.E4.m1.3.3.3.4.cmml">,</mo><mi id="S3.E4.m1.3.3.3.3" xref="S3.E4.m1.3.3.3.3.cmml">z</mi></mrow></munder><mi id="S3.E4.m1.10.10.5.2.2" xref="S3.E4.m1.10.10.5.2.2.cmml">E</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.10.10.5.1" xref="S3.E4.m1.10.10.5.1.cmml">â€‹</mo><mrow id="S3.E4.m1.10.10.5.3.2" xref="S3.E4.m1.10.10.5.3.1.cmml"><mo stretchy="false" id="S3.E4.m1.10.10.5.3.2.1" xref="S3.E4.m1.10.10.5.3.1.cmml">(</mo><mi id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml">I</mi><mo id="S3.E4.m1.10.10.5.3.2.2" xref="S3.E4.m1.10.10.5.3.1.cmml">,</mo><mi id="S3.E4.m1.5.5" xref="S3.E4.m1.5.5.cmml">R</mi><mo id="S3.E4.m1.10.10.5.3.2.3" xref="S3.E4.m1.10.10.5.3.1.cmml">,</mo><mi id="S3.E4.m1.6.6" xref="S3.E4.m1.6.6.cmml">T</mi><mo id="S3.E4.m1.10.10.5.3.2.4" xref="S3.E4.m1.10.10.5.3.1.cmml">,</mo><mi id="S3.E4.m1.7.7" xref="S3.E4.m1.7.7.cmml">z</mi><mo stretchy="false" id="S3.E4.m1.10.10.5.3.2.5" xref="S3.E4.m1.10.10.5.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.10b"><apply id="S3.E4.m1.10.10.cmml" xref="S3.E4.m1.10.10"><eq id="S3.E4.m1.10.10.4.cmml" xref="S3.E4.m1.10.10.4"></eq><list id="S3.E4.m1.10.10.3.4.cmml" xref="S3.E4.m1.10.10.3.3"><apply id="S3.E4.m1.8.8.1.1.1.cmml" xref="S3.E4.m1.8.8.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.8.8.1.1.1.1.cmml" xref="S3.E4.m1.8.8.1.1.1">superscript</csymbol><ci id="S3.E4.m1.8.8.1.1.1.2.cmml" xref="S3.E4.m1.8.8.1.1.1.2">ğ‘…</ci><times id="S3.E4.m1.8.8.1.1.1.3.cmml" xref="S3.E4.m1.8.8.1.1.1.3"></times></apply><apply id="S3.E4.m1.9.9.2.2.2.cmml" xref="S3.E4.m1.9.9.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.9.9.2.2.2.1.cmml" xref="S3.E4.m1.9.9.2.2.2">superscript</csymbol><ci id="S3.E4.m1.9.9.2.2.2.2.cmml" xref="S3.E4.m1.9.9.2.2.2.2">ğ‘‡</ci><times id="S3.E4.m1.9.9.2.2.2.3.cmml" xref="S3.E4.m1.9.9.2.2.2.3"></times></apply><apply id="S3.E4.m1.10.10.3.3.3.cmml" xref="S3.E4.m1.10.10.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.10.10.3.3.3.1.cmml" xref="S3.E4.m1.10.10.3.3.3">superscript</csymbol><ci id="S3.E4.m1.10.10.3.3.3.2.cmml" xref="S3.E4.m1.10.10.3.3.3.2">ğ‘§</ci><times id="S3.E4.m1.10.10.3.3.3.3.cmml" xref="S3.E4.m1.10.10.3.3.3.3"></times></apply></list><apply id="S3.E4.m1.10.10.5.cmml" xref="S3.E4.m1.10.10.5"><times id="S3.E4.m1.10.10.5.1.cmml" xref="S3.E4.m1.10.10.5.1"></times><apply id="S3.E4.m1.10.10.5.2.cmml" xref="S3.E4.m1.10.10.5.2"><apply id="S3.E4.m1.10.10.5.2.1.cmml" xref="S3.E4.m1.10.10.5.2.1"><csymbol cd="ambiguous" id="S3.E4.m1.10.10.5.2.1.1.cmml" xref="S3.E4.m1.10.10.5.2.1">subscript</csymbol><ci id="S3.E4.m1.10.10.5.2.1.2.cmml" xref="S3.E4.m1.10.10.5.2.1.2">argmin</ci><list id="S3.E4.m1.3.3.3.4.cmml" xref="S3.E4.m1.3.3.3.5"><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">ğ‘…</ci><ci id="S3.E4.m1.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2">ğ‘‡</ci><ci id="S3.E4.m1.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3">ğ‘§</ci></list></apply><ci id="S3.E4.m1.10.10.5.2.2.cmml" xref="S3.E4.m1.10.10.5.2.2">ğ¸</ci></apply><vector id="S3.E4.m1.10.10.5.3.1.cmml" xref="S3.E4.m1.10.10.5.3.2"><ci id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4">ğ¼</ci><ci id="S3.E4.m1.5.5.cmml" xref="S3.E4.m1.5.5">ğ‘…</ci><ci id="S3.E4.m1.6.6.cmml" xref="S3.E4.m1.6.6">ğ‘‡</ci><ci id="S3.E4.m1.7.7.cmml" xref="S3.E4.m1.7.7">ğ‘§</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.10c">\displaystyle R^{*},T^{*},z^{*}=\operatorname*{argmin~{}}_{R,T,z}~{}E(I,R,T,z)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.1" class="ltx_p">In the following sections, we will discuss the choice of energy function <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">E</annotation></semantics></math>, our initialization strategy and the optimizer in detail. Note that we assume that the object is segmented from the background which can be achieved using off-the-shelf image segmentation networks such as Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Energy Function.</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.4" class="ltx_p">We require a distance function <math id="S3.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><mi id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><ci id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">d</annotation></semantics></math> that measures the discrepancy between two images. Common choices include the <math id="S3.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS3.SSS1.p1.2.m2.1a"><msub id="S3.SS3.SSS1.p1.2.m2.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS1.p1.2.m2.1.1.2" xref="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml">L</mi><mn id="S3.SS3.SSS1.p1.2.m2.1.1.3" xref="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.2.m2.1b"><apply id="S3.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.2.m2.1c">L_{1}</annotation></semantics></math> and <math id="S3.SS3.SSS1.p1.3.m3.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S3.SS3.SSS1.p1.3.m3.1a"><msub id="S3.SS3.SSS1.p1.3.m3.1.1" xref="S3.SS3.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSS1.p1.3.m3.1.1.2" xref="S3.SS3.SSS1.p1.3.m3.1.1.2.cmml">L</mi><mn id="S3.SS3.SSS1.p1.3.m3.1.1.3" xref="S3.SS3.SSS1.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.3.m3.1b"><apply id="S3.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS3.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.3.m3.1c">L_{2}</annotation></semantics></math> per-pixel differences, the Structural Similarity Index Metric (SSIM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and the perceptual loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> which computes the <math id="S3.SS3.SSS1.p1.4.m4.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S3.SS3.SSS1.p1.4.m4.1a"><msub id="S3.SS3.SSS1.p1.4.m4.1.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.cmml"><mi id="S3.SS3.SSS1.p1.4.m4.1.1.2" xref="S3.SS3.SSS1.p1.4.m4.1.1.2.cmml">L</mi><mn id="S3.SS3.SSS1.p1.4.m4.1.1.3" xref="S3.SS3.SSS1.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.4.m4.1b"><apply id="S3.SS3.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS3.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.4.m4.1c">L_{2}</annotation></semantics></math> difference in deep feature space. To gain robustness with respect to domain shifts we adopt the perceptual loss as distance function and experimentally validate that it yields the best results.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.6" class="ltx_p">Without further regularization, we found that the model may converge to degenerate solutions by pushing the latent code beyond the valid domain. To avoid such undesirable solutions, we penalize the distance to the origin of the latent space. Due to the KL divergence term used during training, codes near the origin are more likely to be useful for the decoder.
Our final energy function is then given by:</p>
<table id="S5.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.8" class="ltx_Math" alttext="\displaystyle E(I,R,T,z)=\|F_{vgg}(I)-F_{vgg}(\hat{I})\|_{2}+\|z\|_{2}," display="inline"><semantics id="S3.E5.m1.8a"><mrow id="S3.E5.m1.8.8.1" xref="S3.E5.m1.8.8.1.1.cmml"><mrow id="S3.E5.m1.8.8.1.1" xref="S3.E5.m1.8.8.1.1.cmml"><mrow id="S3.E5.m1.8.8.1.1.3" xref="S3.E5.m1.8.8.1.1.3.cmml"><mi id="S3.E5.m1.8.8.1.1.3.2" xref="S3.E5.m1.8.8.1.1.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.1.1.3.1" xref="S3.E5.m1.8.8.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E5.m1.8.8.1.1.3.3.2" xref="S3.E5.m1.8.8.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.E5.m1.8.8.1.1.3.3.2.1" xref="S3.E5.m1.8.8.1.1.3.3.1.cmml">(</mo><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">I</mi><mo id="S3.E5.m1.8.8.1.1.3.3.2.2" xref="S3.E5.m1.8.8.1.1.3.3.1.cmml">,</mo><mi id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml">R</mi><mo id="S3.E5.m1.8.8.1.1.3.3.2.3" xref="S3.E5.m1.8.8.1.1.3.3.1.cmml">,</mo><mi id="S3.E5.m1.3.3" xref="S3.E5.m1.3.3.cmml">T</mi><mo id="S3.E5.m1.8.8.1.1.3.3.2.4" xref="S3.E5.m1.8.8.1.1.3.3.1.cmml">,</mo><mi id="S3.E5.m1.4.4" xref="S3.E5.m1.4.4.cmml">z</mi><mo stretchy="false" id="S3.E5.m1.8.8.1.1.3.3.2.5" xref="S3.E5.m1.8.8.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.8.8.1.1.2" xref="S3.E5.m1.8.8.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.8.8.1.1.1" xref="S3.E5.m1.8.8.1.1.1.cmml"><msub id="S3.E5.m1.8.8.1.1.1.1" xref="S3.E5.m1.8.8.1.1.1.1.cmml"><mrow id="S3.E5.m1.8.8.1.1.1.1.1.1" xref="S3.E5.m1.8.8.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.8.8.1.1.1.1.1.1.2" xref="S3.E5.m1.8.8.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E5.m1.8.8.1.1.1.1.1.1.1" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.cmml"><mrow id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.2" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.2.cmml">F</mi><mrow id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.2" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.1" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.3" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.1a" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.4" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.4.cmml">g</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.1" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.1.cmml">â€‹</mo><mrow id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.3.2" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.3.2.1" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E5.m1.5.5" xref="S3.E5.m1.5.5.cmml">I</mi><mo stretchy="false" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.3.2.2" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.8.8.1.1.1.1.1.1.1.1" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.cmml"><msub id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.2" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.2.cmml">F</mi><mrow id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.cmml"><mi id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.2" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.1" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.3" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.1a" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.4" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.4.cmml">g</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.1" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.3.2" xref="S3.E5.m1.6.6.cmml"><mo stretchy="false" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.3.2.1" xref="S3.E5.m1.6.6.cmml">(</mo><mover accent="true" id="S3.E5.m1.6.6" xref="S3.E5.m1.6.6.cmml"><mi id="S3.E5.m1.6.6.2" xref="S3.E5.m1.6.6.2.cmml">I</mi><mo id="S3.E5.m1.6.6.1" xref="S3.E5.m1.6.6.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.3.2.2" xref="S3.E5.m1.6.6.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E5.m1.8.8.1.1.1.1.1.1.3" xref="S3.E5.m1.8.8.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E5.m1.8.8.1.1.1.1.3" xref="S3.E5.m1.8.8.1.1.1.1.3.cmml">2</mn></msub><mo id="S3.E5.m1.8.8.1.1.1.2" xref="S3.E5.m1.8.8.1.1.1.2.cmml">+</mo><msub id="S3.E5.m1.8.8.1.1.1.3" xref="S3.E5.m1.8.8.1.1.1.3.cmml"><mrow id="S3.E5.m1.8.8.1.1.1.3.2.2" xref="S3.E5.m1.8.8.1.1.1.3.2.1.cmml"><mo stretchy="false" id="S3.E5.m1.8.8.1.1.1.3.2.2.1" xref="S3.E5.m1.8.8.1.1.1.3.2.1.1.cmml">â€–</mo><mi id="S3.E5.m1.7.7" xref="S3.E5.m1.7.7.cmml">z</mi><mo stretchy="false" id="S3.E5.m1.8.8.1.1.1.3.2.2.2" xref="S3.E5.m1.8.8.1.1.1.3.2.1.1.cmml">â€–</mo></mrow><mn id="S3.E5.m1.8.8.1.1.1.3.3" xref="S3.E5.m1.8.8.1.1.1.3.3.cmml">2</mn></msub></mrow></mrow><mo id="S3.E5.m1.8.8.1.2" xref="S3.E5.m1.8.8.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.8b"><apply id="S3.E5.m1.8.8.1.1.cmml" xref="S3.E5.m1.8.8.1"><eq id="S3.E5.m1.8.8.1.1.2.cmml" xref="S3.E5.m1.8.8.1.1.2"></eq><apply id="S3.E5.m1.8.8.1.1.3.cmml" xref="S3.E5.m1.8.8.1.1.3"><times id="S3.E5.m1.8.8.1.1.3.1.cmml" xref="S3.E5.m1.8.8.1.1.3.1"></times><ci id="S3.E5.m1.8.8.1.1.3.2.cmml" xref="S3.E5.m1.8.8.1.1.3.2">ğ¸</ci><vector id="S3.E5.m1.8.8.1.1.3.3.1.cmml" xref="S3.E5.m1.8.8.1.1.3.3.2"><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">ğ¼</ci><ci id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2">ğ‘…</ci><ci id="S3.E5.m1.3.3.cmml" xref="S3.E5.m1.3.3">ğ‘‡</ci><ci id="S3.E5.m1.4.4.cmml" xref="S3.E5.m1.4.4">ğ‘§</ci></vector></apply><apply id="S3.E5.m1.8.8.1.1.1.cmml" xref="S3.E5.m1.8.8.1.1.1"><plus id="S3.E5.m1.8.8.1.1.1.2.cmml" xref="S3.E5.m1.8.8.1.1.1.2"></plus><apply id="S3.E5.m1.8.8.1.1.1.1.cmml" xref="S3.E5.m1.8.8.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.1.1.1.1.2.cmml" xref="S3.E5.m1.8.8.1.1.1.1">subscript</csymbol><apply id="S3.E5.m1.8.8.1.1.1.1.1.2.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.8.8.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E5.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1"><minus id="S3.E5.m1.8.8.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.1"></minus><apply id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2"><times id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.1"></times><apply id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.2">ğ¹</ci><apply id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3"><times id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.1"></times><ci id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.2">ğ‘£</ci><ci id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.3">ğ‘”</ci><ci id="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.4.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.2.2.3.4">ğ‘”</ci></apply></apply><ci id="S3.E5.m1.5.5.cmml" xref="S3.E5.m1.5.5">ğ¼</ci></apply><apply id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3"><times id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.1"></times><apply id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.2">ğ¹</ci><apply id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3"><times id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.1"></times><ci id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.2.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.2">ğ‘£</ci><ci id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.3.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.3">ğ‘”</ci><ci id="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.4.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.2.3.4">ğ‘”</ci></apply></apply><apply id="S3.E5.m1.6.6.cmml" xref="S3.E5.m1.8.8.1.1.1.1.1.1.1.3.3.2"><ci id="S3.E5.m1.6.6.1.cmml" xref="S3.E5.m1.6.6.1">^</ci><ci id="S3.E5.m1.6.6.2.cmml" xref="S3.E5.m1.6.6.2">ğ¼</ci></apply></apply></apply></apply><cn type="integer" id="S3.E5.m1.8.8.1.1.1.1.3.cmml" xref="S3.E5.m1.8.8.1.1.1.1.3">2</cn></apply><apply id="S3.E5.m1.8.8.1.1.1.3.cmml" xref="S3.E5.m1.8.8.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.1.1.1.3.1.cmml" xref="S3.E5.m1.8.8.1.1.1.3">subscript</csymbol><apply id="S3.E5.m1.8.8.1.1.1.3.2.1.cmml" xref="S3.E5.m1.8.8.1.1.1.3.2.2"><csymbol cd="latexml" id="S3.E5.m1.8.8.1.1.1.3.2.1.1.cmml" xref="S3.E5.m1.8.8.1.1.1.3.2.2.1">norm</csymbol><ci id="S3.E5.m1.7.7.cmml" xref="S3.E5.m1.7.7">ğ‘§</ci></apply><cn type="integer" id="S3.E5.m1.8.8.1.1.1.3.3.cmml" xref="S3.E5.m1.8.8.1.1.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.8c">\displaystyle E(I,R,T,z)=\|F_{vgg}(I)-F_{vgg}(\hat{I})\|_{2}+\|z\|_{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.SSS1.p2.5" class="ltx_p">where <math id="S3.SS3.SSS1.p2.1.m1.1" class="ltx_Math" alttext="F_{vgg}" display="inline"><semantics id="S3.SS3.SSS1.p2.1.m1.1a"><msub id="S3.SS3.SSS1.p2.1.m1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.1.m1.1.1.2" xref="S3.SS3.SSS1.p2.1.m1.1.1.2.cmml">F</mi><mrow id="S3.SS3.SSS1.p2.1.m1.1.1.3" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS3.SSS1.p2.1.m1.1.1.3.2" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p2.1.m1.1.1.3.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.SSS1.p2.1.m1.1.1.3.3" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p2.1.m1.1.1.3.1a" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.SSS1.p2.1.m1.1.1.3.4" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.1.m1.1b"><apply id="S3.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.2">ğ¹</ci><apply id="S3.SS3.SSS1.p2.1.m1.1.1.3.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.3"><times id="S3.SS3.SSS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.1"></times><ci id="S3.SS3.SSS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.2">ğ‘£</ci><ci id="S3.SS3.SSS1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.3">ğ‘”</ci><ci id="S3.SS3.SSS1.p2.1.m1.1.1.3.4.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.4">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.1.m1.1c">F_{vgg}</annotation></semantics></math> is a VGG network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for deep feature extraction and <math id="S3.SS3.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\hat{I}" display="inline"><semantics id="S3.SS3.SSS1.p2.2.m2.1a"><mover accent="true" id="S3.SS3.SSS1.p2.2.m2.1.1" xref="S3.SS3.SSS1.p2.2.m2.1.1.cmml"><mi id="S3.SS3.SSS1.p2.2.m2.1.1.2" xref="S3.SS3.SSS1.p2.2.m2.1.1.2.cmml">I</mi><mo id="S3.SS3.SSS1.p2.2.m2.1.1.1" xref="S3.SS3.SSS1.p2.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.2.m2.1b"><apply id="S3.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1"><ci id="S3.SS3.SSS1.p2.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1.1">^</ci><ci id="S3.SS3.SSS1.p2.2.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1.2">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.2.m2.1c">\hat{I}</annotation></semantics></math> is the image generated based on <math id="S3.SS3.SSS1.p2.3.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS3.SSS1.p2.3.m3.1a"><mi id="S3.SS3.SSS1.p2.3.m3.1.1" xref="S3.SS3.SSS1.p2.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.3.m3.1b"><ci id="S3.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.3.m3.1c">R</annotation></semantics></math>, <math id="S3.SS3.SSS1.p2.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS3.SSS1.p2.4.m4.1a"><mi id="S3.SS3.SSS1.p2.4.m4.1.1" xref="S3.SS3.SSS1.p2.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.4.m4.1b"><ci id="S3.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.4.m4.1c">T</annotation></semantics></math> and <math id="S3.SS3.SSS1.p2.5.m5.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS3.SSS1.p2.5.m5.1a"><mi id="S3.SS3.SSS1.p2.5.m5.1.1" xref="S3.SS3.SSS1.p2.5.m5.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.5.m5.1b"><ci id="S3.SS3.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.5.m5.1c">z</annotation></semantics></math> according to Eq.Â <a href="#S3.E2" title="In 3.1.2 Translation and 2D Rotation. â€£ 3.1 Pose-aware Image Generator â€£ 3 Method â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Initialization Strategy.</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">Since the above energy function is non-convex, gradient-based optimization techniques are prone to local minima. Therefore, we start the optimization process from multiple different initial states which we execute in parallel for efficiency. The optimal solution results in an image that aligns with the target image as well as possible, with any differences in pose or appearance leading to an increase in the reconstruction error. We thus choose the best solution by evaluating the optimization objective.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">We sample the initial poses uniformly from the space of valid poses.
As the latent dimension is high, random sampling can be inefficient due to the curse of dimensionality.
We therefore leverage our (jointly trained) encoder to obtain mean and variance for the conditional distribution of the latent code and sample from the corresponding Gaussian distribution.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We first compare our approach with the state-of-the-art category-level object pose estimation method NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. To validate the effectiveness of our method, we then also compare our method with several other baselines for category-level object model fitting. Furthermore, we systematically vary the architecture and hyper-parameters of our image generation network to analyze their influence on the fitting result. We also study the influence of the hyper-parameters for optimization. Finally, we evaluate the robustness of our generative model with respect to domain shifts, comparing it to a discriminative model.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparison with state-of-the-art</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Baseline.</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">To the best of our knowledge, NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> is the only method for category-level object pose estimation. It uses both synthetic data generated from ShapeNet CAD models, and real data to train a network that is able to reconstruct objects from a RGB image in a canonical coordinate frame. Subsequently, the object pose is recovered by aligning the reconstruction to the depth map. NOCS uses both simulated (CAMERA) and real data (REAL275) for training. Their simulated data is generated by compositing synthetic images from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> onto real-world tables. REAL275 contains real RGB-D images (4300 for training and 2750 for testing), capturing 42 real object instances of 6 categories (camera, can, bottle, bowl, laptop and mug) in 18 different scenes.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">Our method is trained with the same set of synthetic objects used in the CAMERA dataset. However our method does not require superimposing the objects onto real contexts. More importantly, in contrast to NOCS, our method does not require real images and their pose annotations for training. Note that we rely only on cheap to acquire 2D annotations of real images to fine-tune the object segmentation network (we use the REAL275 dataset for this purpose).</p>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p id="S4.SS1.SSS1.p3.1" class="ltx_p">Since there is no public method capable of category-level object pose estimation from RGB images alone, we introduce a simple baseline consisting of a VGG16 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> network that regresses object orientations directly from 2D images, trained on the same synthetic data as ours. Note that this is a fair comparison since the baseline and our method use the same training and test data.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Metrics.</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">We follow the evaluation protocol defined by NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. NOCS reports average precision which considers object detection, classification and pose estimation accuracy. They set a detection threshold of 10% bounding box overlap between prediction and
ground truth to ensure that most objects are included in the
evaluation. Next, they compute the average precision for different thresholds of rotation and translation error as performance indicator.
Although our method assumes that the target image does not contain background and we rely on a off-the shelf 2D image segmentation network for object detection, we follow the same protocol for a fair comparison with NOCS. To remove the influence of varying detection accuracy from the comparison, we use the trained Mask-RCNN network from the NOCS Github repository.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.2" class="ltx_p">The error in rotation <math id="S4.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="e_{R}" display="inline"><semantics id="S4.SS1.SSS2.p2.1.m1.1a"><msub id="S4.SS1.SSS2.p2.1.m1.1.1" xref="S4.SS1.SSS2.p2.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p2.1.m1.1.1.2" xref="S4.SS1.SSS2.p2.1.m1.1.1.2.cmml">e</mi><mi id="S4.SS1.SSS2.p2.1.m1.1.1.3" xref="S4.SS1.SSS2.p2.1.m1.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.1.m1.1b"><apply id="S4.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p2.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1.2">ğ‘’</ci><ci id="S4.SS1.SSS2.p2.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1.3">ğ‘…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.1.m1.1c">e_{R}</annotation></semantics></math> and translation <math id="S4.SS1.SSS2.p2.2.m2.1" class="ltx_Math" alttext="e_{t}" display="inline"><semantics id="S4.SS1.SSS2.p2.2.m2.1a"><msub id="S4.SS1.SSS2.p2.2.m2.1.1" xref="S4.SS1.SSS2.p2.2.m2.1.1.cmml"><mi id="S4.SS1.SSS2.p2.2.m2.1.1.2" xref="S4.SS1.SSS2.p2.2.m2.1.1.2.cmml">e</mi><mi id="S4.SS1.SSS2.p2.2.m2.1.1.3" xref="S4.SS1.SSS2.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.2.m2.1b"><apply id="S4.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p2.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.1.1.2">ğ‘’</ci><ci id="S4.SS1.SSS2.p2.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.2.m2.1c">e_{t}</annotation></semantics></math> is defined by:</p>
<table id="S5.EGx6" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E6.m1.2" class="ltx_Math" alttext="\displaystyle e_{R}=arccos\frac{Tr(\tilde{R}\cdot R^{T})-1}{2}\text{ and }e_{t}=\|\tilde{t}-t\|_{2}" display="inline"><semantics id="S4.E6.m1.2a"><mrow id="S4.E6.m1.2.2" xref="S4.E6.m1.2.2.cmml"><msub id="S4.E6.m1.2.2.3" xref="S4.E6.m1.2.2.3.cmml"><mi id="S4.E6.m1.2.2.3.2" xref="S4.E6.m1.2.2.3.2.cmml">e</mi><mi id="S4.E6.m1.2.2.3.3" xref="S4.E6.m1.2.2.3.3.cmml">R</mi></msub><mo id="S4.E6.m1.2.2.4" xref="S4.E6.m1.2.2.4.cmml">=</mo><mrow id="S4.E6.m1.2.2.5" xref="S4.E6.m1.2.2.5.cmml"><mi id="S4.E6.m1.2.2.5.2" xref="S4.E6.m1.2.2.5.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.5.1" xref="S4.E6.m1.2.2.5.1.cmml">â€‹</mo><mi id="S4.E6.m1.2.2.5.3" xref="S4.E6.m1.2.2.5.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.5.1a" xref="S4.E6.m1.2.2.5.1.cmml">â€‹</mo><mi id="S4.E6.m1.2.2.5.4" xref="S4.E6.m1.2.2.5.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.5.1b" xref="S4.E6.m1.2.2.5.1.cmml">â€‹</mo><mi id="S4.E6.m1.2.2.5.5" xref="S4.E6.m1.2.2.5.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.5.1c" xref="S4.E6.m1.2.2.5.1.cmml">â€‹</mo><mi id="S4.E6.m1.2.2.5.6" xref="S4.E6.m1.2.2.5.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.5.1d" xref="S4.E6.m1.2.2.5.1.cmml">â€‹</mo><mi id="S4.E6.m1.2.2.5.7" xref="S4.E6.m1.2.2.5.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.5.1e" xref="S4.E6.m1.2.2.5.1.cmml">â€‹</mo><mstyle displaystyle="true" id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml"><mfrac id="S4.E6.m1.1.1a" xref="S4.E6.m1.1.1.cmml"><mrow id="S4.E6.m1.1.1.1" xref="S4.E6.m1.1.1.1.cmml"><mrow id="S4.E6.m1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.cmml"><mi id="S4.E6.m1.1.1.1.1.3" xref="S4.E6.m1.1.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.1.1.1.2" xref="S4.E6.m1.1.1.1.1.2.cmml">â€‹</mo><mi id="S4.E6.m1.1.1.1.1.4" xref="S4.E6.m1.1.1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.1.1.1.2a" xref="S4.E6.m1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E6.m1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E6.m1.1.1.1.1.1.1.2" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E6.m1.1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E6.m1.1.1.1.1.1.1.1.2" xref="S4.E6.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E6.m1.1.1.1.1.1.1.1.2.2" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2.cmml">R</mi><mo id="S4.E6.m1.1.1.1.1.1.1.1.2.1" xref="S4.E6.m1.1.1.1.1.1.1.1.2.1.cmml">~</mo></mover><mo lspace="0.222em" rspace="0.222em" id="S4.E6.m1.1.1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.1.1.cmml">â‹…</mo><msup id="S4.E6.m1.1.1.1.1.1.1.1.3" xref="S4.E6.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E6.m1.1.1.1.1.1.1.1.3.2" xref="S4.E6.m1.1.1.1.1.1.1.1.3.2.cmml">R</mi><mi id="S4.E6.m1.1.1.1.1.1.1.1.3.3" xref="S4.E6.m1.1.1.1.1.1.1.1.3.3.cmml">T</mi></msup></mrow><mo stretchy="false" id="S4.E6.m1.1.1.1.1.1.1.3" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E6.m1.1.1.1.2" xref="S4.E6.m1.1.1.1.2.cmml">âˆ’</mo><mn id="S4.E6.m1.1.1.1.3" xref="S4.E6.m1.1.1.1.3.cmml">1</mn></mrow><mn id="S4.E6.m1.1.1.3" xref="S4.E6.m1.1.1.3.cmml">2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.5.1f" xref="S4.E6.m1.2.2.5.1.cmml">â€‹</mo><mtext id="S4.E6.m1.2.2.5.8" xref="S4.E6.m1.2.2.5.8a.cmml">Â andÂ </mtext><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.5.1g" xref="S4.E6.m1.2.2.5.1.cmml">â€‹</mo><msub id="S4.E6.m1.2.2.5.9" xref="S4.E6.m1.2.2.5.9.cmml"><mi id="S4.E6.m1.2.2.5.9.2" xref="S4.E6.m1.2.2.5.9.2.cmml">e</mi><mi id="S4.E6.m1.2.2.5.9.3" xref="S4.E6.m1.2.2.5.9.3.cmml">t</mi></msub></mrow><mo id="S4.E6.m1.2.2.6" xref="S4.E6.m1.2.2.6.cmml">=</mo><msub id="S4.E6.m1.2.2.1" xref="S4.E6.m1.2.2.1.cmml"><mrow id="S4.E6.m1.2.2.1.1.1" xref="S4.E6.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S4.E6.m1.2.2.1.1.1.2" xref="S4.E6.m1.2.2.1.1.2.1.cmml">â€–</mo><mrow id="S4.E6.m1.2.2.1.1.1.1" xref="S4.E6.m1.2.2.1.1.1.1.cmml"><mover accent="true" id="S4.E6.m1.2.2.1.1.1.1.2" xref="S4.E6.m1.2.2.1.1.1.1.2.cmml"><mi id="S4.E6.m1.2.2.1.1.1.1.2.2" xref="S4.E6.m1.2.2.1.1.1.1.2.2.cmml">t</mi><mo id="S4.E6.m1.2.2.1.1.1.1.2.1" xref="S4.E6.m1.2.2.1.1.1.1.2.1.cmml">~</mo></mover><mo id="S4.E6.m1.2.2.1.1.1.1.1" xref="S4.E6.m1.2.2.1.1.1.1.1.cmml">âˆ’</mo><mi id="S4.E6.m1.2.2.1.1.1.1.3" xref="S4.E6.m1.2.2.1.1.1.1.3.cmml">t</mi></mrow><mo stretchy="false" id="S4.E6.m1.2.2.1.1.1.3" xref="S4.E6.m1.2.2.1.1.2.1.cmml">â€–</mo></mrow><mn id="S4.E6.m1.2.2.1.3" xref="S4.E6.m1.2.2.1.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.2b"><apply id="S4.E6.m1.2.2.cmml" xref="S4.E6.m1.2.2"><and id="S4.E6.m1.2.2a.cmml" xref="S4.E6.m1.2.2"></and><apply id="S4.E6.m1.2.2b.cmml" xref="S4.E6.m1.2.2"><eq id="S4.E6.m1.2.2.4.cmml" xref="S4.E6.m1.2.2.4"></eq><apply id="S4.E6.m1.2.2.3.cmml" xref="S4.E6.m1.2.2.3"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.3.1.cmml" xref="S4.E6.m1.2.2.3">subscript</csymbol><ci id="S4.E6.m1.2.2.3.2.cmml" xref="S4.E6.m1.2.2.3.2">ğ‘’</ci><ci id="S4.E6.m1.2.2.3.3.cmml" xref="S4.E6.m1.2.2.3.3">ğ‘…</ci></apply><apply id="S4.E6.m1.2.2.5.cmml" xref="S4.E6.m1.2.2.5"><times id="S4.E6.m1.2.2.5.1.cmml" xref="S4.E6.m1.2.2.5.1"></times><ci id="S4.E6.m1.2.2.5.2.cmml" xref="S4.E6.m1.2.2.5.2">ğ‘</ci><ci id="S4.E6.m1.2.2.5.3.cmml" xref="S4.E6.m1.2.2.5.3">ğ‘Ÿ</ci><ci id="S4.E6.m1.2.2.5.4.cmml" xref="S4.E6.m1.2.2.5.4">ğ‘</ci><ci id="S4.E6.m1.2.2.5.5.cmml" xref="S4.E6.m1.2.2.5.5">ğ‘</ci><ci id="S4.E6.m1.2.2.5.6.cmml" xref="S4.E6.m1.2.2.5.6">ğ‘œ</ci><ci id="S4.E6.m1.2.2.5.7.cmml" xref="S4.E6.m1.2.2.5.7">ğ‘ </ci><apply id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1"><divide id="S4.E6.m1.1.1.2.cmml" xref="S4.E6.m1.1.1"></divide><apply id="S4.E6.m1.1.1.1.cmml" xref="S4.E6.m1.1.1.1"><minus id="S4.E6.m1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.2"></minus><apply id="S4.E6.m1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1"><times id="S4.E6.m1.1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.1.2"></times><ci id="S4.E6.m1.1.1.1.1.3.cmml" xref="S4.E6.m1.1.1.1.1.3">ğ‘‡</ci><ci id="S4.E6.m1.1.1.1.1.4.cmml" xref="S4.E6.m1.1.1.1.1.4">ğ‘Ÿ</ci><apply id="S4.E6.m1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1"><ci id="S4.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1">â‹…</ci><apply id="S4.E6.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2"><ci id="S4.E6.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.1">~</ci><ci id="S4.E6.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2">ğ‘…</ci></apply><apply id="S4.E6.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E6.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.3.2">ğ‘…</ci><ci id="S4.E6.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.3.3">ğ‘‡</ci></apply></apply></apply><cn type="integer" id="S4.E6.m1.1.1.1.3.cmml" xref="S4.E6.m1.1.1.1.3">1</cn></apply><cn type="integer" id="S4.E6.m1.1.1.3.cmml" xref="S4.E6.m1.1.1.3">2</cn></apply><ci id="S4.E6.m1.2.2.5.8a.cmml" xref="S4.E6.m1.2.2.5.8"><mtext id="S4.E6.m1.2.2.5.8.cmml" xref="S4.E6.m1.2.2.5.8">Â andÂ </mtext></ci><apply id="S4.E6.m1.2.2.5.9.cmml" xref="S4.E6.m1.2.2.5.9"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.5.9.1.cmml" xref="S4.E6.m1.2.2.5.9">subscript</csymbol><ci id="S4.E6.m1.2.2.5.9.2.cmml" xref="S4.E6.m1.2.2.5.9.2">ğ‘’</ci><ci id="S4.E6.m1.2.2.5.9.3.cmml" xref="S4.E6.m1.2.2.5.9.3">ğ‘¡</ci></apply></apply></apply><apply id="S4.E6.m1.2.2c.cmml" xref="S4.E6.m1.2.2"><eq id="S4.E6.m1.2.2.6.cmml" xref="S4.E6.m1.2.2.6"></eq><share href="#S4.E6.m1.2.2.5.cmml" id="S4.E6.m1.2.2d.cmml" xref="S4.E6.m1.2.2"></share><apply id="S4.E6.m1.2.2.1.cmml" xref="S4.E6.m1.2.2.1"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.1.2.cmml" xref="S4.E6.m1.2.2.1">subscript</csymbol><apply id="S4.E6.m1.2.2.1.1.2.cmml" xref="S4.E6.m1.2.2.1.1.1"><csymbol cd="latexml" id="S4.E6.m1.2.2.1.1.2.1.cmml" xref="S4.E6.m1.2.2.1.1.1.2">norm</csymbol><apply id="S4.E6.m1.2.2.1.1.1.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1"><minus id="S4.E6.m1.2.2.1.1.1.1.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1"></minus><apply id="S4.E6.m1.2.2.1.1.1.1.2.cmml" xref="S4.E6.m1.2.2.1.1.1.1.2"><ci id="S4.E6.m1.2.2.1.1.1.1.2.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.2.1">~</ci><ci id="S4.E6.m1.2.2.1.1.1.1.2.2.cmml" xref="S4.E6.m1.2.2.1.1.1.1.2.2">ğ‘¡</ci></apply><ci id="S4.E6.m1.2.2.1.1.1.1.3.cmml" xref="S4.E6.m1.2.2.1.1.1.1.3">ğ‘¡</ci></apply></apply><cn type="integer" id="S4.E6.m1.2.2.1.3.cmml" xref="S4.E6.m1.2.2.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.2c">\displaystyle e_{R}=arccos\frac{Tr(\tilde{R}\cdot R^{T})-1}{2}\text{ and }e_{t}=\|\tilde{t}-t\|_{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.SSS2.p2.3" class="ltx_p">where <math id="S4.SS1.SSS2.p2.3.m1.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS1.SSS2.p2.3.m1.1a"><mrow id="S4.SS1.SSS2.p2.3.m1.1.1" xref="S4.SS1.SSS2.p2.3.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p2.3.m1.1.1.2" xref="S4.SS1.SSS2.p2.3.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p2.3.m1.1.1.1" xref="S4.SS1.SSS2.p2.3.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.SSS2.p2.3.m1.1.1.3" xref="S4.SS1.SSS2.p2.3.m1.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.3.m1.1b"><apply id="S4.SS1.SSS2.p2.3.m1.1.1.cmml" xref="S4.SS1.SSS2.p2.3.m1.1.1"><times id="S4.SS1.SSS2.p2.3.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p2.3.m1.1.1.1"></times><ci id="S4.SS1.SSS2.p2.3.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p2.3.m1.1.1.2">ğ‘‡</ci><ci id="S4.SS1.SSS2.p2.3.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p2.3.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.3.m1.1c">Tr</annotation></semantics></math> represents the trace of the matrix. For symmetric object categories (bottle, bowl, and can), we allow the predicted 3D bounding box to freely rotate around the objectâ€™s vertical axis with no penalty, as done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Results: Translation.</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">We first evaluate the translation accuracy, summarized in Fig.Â <a href="#S4.F3" title="Figure 3 â€£ 4.1.3 Results: Translation. â€£ 4.1 Comparison with state-of-the-art â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. When using depth as input, both models perform comparable. We simply treat depth as an additional color channel during fitting and the translation parameter <math id="S4.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="T_{z}" display="inline"><semantics id="S4.SS1.SSS3.p1.1.m1.1a"><msub id="S4.SS1.SSS3.p1.1.m1.1.1" xref="S4.SS1.SSS3.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS3.p1.1.m1.1.1.2" xref="S4.SS1.SSS3.p1.1.m1.1.1.2.cmml">T</mi><mi id="S4.SS1.SSS3.p1.1.m1.1.1.3" xref="S4.SS1.SSS3.p1.1.m1.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.1.m1.1b"><apply id="S4.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1.2">ğ‘‡</ci><ci id="S4.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.1.m1.1c">T_{z}</annotation></semantics></math> is directly added to the generated depth map. This version is trained on synthetic RGB-D data instead of RGB data. Without using depth at training and test time, our RGB only version does not achieve the same accuracy. This is however to be expected due to the inherent scale-ambiguity of 2D observations. We remark that similar observations have been made in the instance-level pose estimation literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2008.08145/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.4.2" class="ltx_text" style="font-size:90%;"> <span id="S4.F3.4.2.1" class="ltx_text ltx_font_bold">Comparison with NOCS: Translation.</span> Average precision at different translation thresholds. When using depth, our method achieves comparable results to the RGB-D method. When using RGB only, our method yields higher errors due to scale ambiguities.</span></figcaption>
</figure>
</section>
<section id="S4.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Results: Orientation.</h4>

<div id="S4.SS1.SSS4.p1" class="ltx_para">
<p id="S4.SS1.SSS4.p1.1" class="ltx_p">The AP curve for our rotation estimation experiment is shown in Fig.Â <a href="#S4.F4" title="Figure 4 â€£ 4.1.4 Results: Orientation. â€£ 4.1 Comparison with state-of-the-art â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Despite using only RGB inputs, on average we achieve results on par or better than the privileged NOCS baseline which uses RGB and depth as well as real-images with paired pose annotations during training. Taking a closer look at each category, our method outperforms NOCS on the bottle, can and camera categories. We hypothesize that the complex textures of cans and bottles is problematic for the regression of the NOCS features, and the complex geometry of cameras poses a challenge for ICP. Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.1.4 Results: Orientation. â€£ 4.1 Comparison with state-of-the-art â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows qualitative results. It can be seen that our method produces more accurate results than NOCS, especially for geometrically complex objects. This may also provide an explanation for the narrow performance advantage of NOCS on bowl and laptop; both object types have many planar regions which favor the point-to-plane metric used in ICP.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2008.08145/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison with NOCS: Orientation.<span id="S4.F4.4.2.1" class="ltx_text ltx_font_medium"> Average precision at different rotation error thresholds. Using only RGB as input, on average we achieve results on par or better than the NOCS baseline which uses RGB-D input and real training images with pose annotations. Our method can handle objects with complex geometry (camera) and textures (can) better than NOCS. See Fig. <a href="#S4.F5" title="Figure 5 â€£ 4.1.4 Results: Orientation. â€£ 4.1 Comparison with state-of-the-art â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</span></span></figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2008.08145/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="157" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Qualitative comparison to NOCS.<span id="S4.F5.4.2.1" class="ltx_text ltx_font_medium"> Our method can handle geometrically complex objects such as cameras better. Objects in insets.</span></span></figcaption>
</figure>
<div id="S4.SS1.SSS4.p2" class="ltx_para">
<p id="S4.SS1.SSS4.p2.1" class="ltx_p">Finally, our method significantly outperforms the discriminative RGB-based baseline which directly regresses 3D orientation from images. This large gap is partially due to the sensitivity of discriminative methods towards distribution shifts and domain gaps between the training and test data (e.g., instances, lighting, occlusion and segmentation imperfections) and proves the generalization power of generative models for pose estimation.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Component Analysis</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Image Generation.</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">We evaluate the influence of our design choices in the image generation network on both the image generation and the object pose fitting task. First, we train a network in which the poses are directly concatenated to the latent code and then decoded to images with 2D convolutions only, the latter essentially being a standard conditional VAE (denoted by <span id="S4.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">w/o 3D</span>). Tab.Â <a href="#S4.F5.sf1" title="In Table 1 â€£ 4.2.1 Image Generation. â€£ 4.2 Component Analysis â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a> and Fig.Â <a href="#S4.F6" title="Figure 6 â€£ 4.2.1 Image Generation. â€£ 4.2 Component Analysis â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> clearly show that the lack of a 3D latent space leads to poor image generation results and consequently the fitting fails.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">We further train our network without the KL divergence term (denoted by <span id="S4.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">w/o VAE</span> in Tab.Â <a href="#S4.F5.sf1" title="In Table 1 â€£ 4.2.1 Image Generation. â€£ 4.2 Component Analysis â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a>). While the image generation network achieves a lower training objective without regularization, this results in a non-smooth latent space, i.e. only few samples in the latent space are used for image generation. In consequence, the fitting is negatively impacted since no informative gradient can be produced to guide the updates of the latent code.</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p">Finally, we study the influence of the dimension of the latent space. We train several networks with different dimensions. Tab.Â <a href="#S4.F5.sf2" title="In Table 1 â€£ 4.2.1 Image Generation. â€£ 4.2 Component Analysis â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(b)</span></a> summarizes the results. Low-dimensional latent spaces are not capable of representing a large variety of objects, and inflect high image reconstruction errors and high pose fitting error. On the other extreme, we find that higher dimensionality leads to better image reconstruction quality, whereas it poses difficulty for fitting. We hypothesize that this is due the more complex structure of the latent space which is required to obtain high reconstruction fidelity. Empirically, we find that a 16-dimensional latent space leads to a good trade-off between image quality and pose accuracy.</p>
</div>
<figure id="S4.T1" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Effect of network design choices on object pose estimation<span id="S4.T1.4.2.1" class="ltx_text ltx_font_medium">. (a) demonstrates that both the VAE training scheme and the 3D feature volume are benefical for object pose estimation. (b) shows that overly high- or low-dimensional latent space can negatively influence the pose estimation.</span></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.sf1.5.2" class="ltx_text" style="font-size:90%;">Network architectures.</span></figcaption>
<table id="S4.F5.sf1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F5.sf1.2.2" class="ltx_tr">
<th id="S4.F5.sf1.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"></th>
<th id="S4.F5.sf1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">L1</th>
<th id="S4.F5.sf1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"><math id="S4.F5.sf1.1.1.1.m1.1" class="ltx_Math" alttext="AP_{10}" display="inline"><semantics id="S4.F5.sf1.1.1.1.m1.1a"><mrow id="S4.F5.sf1.1.1.1.m1.1.1" xref="S4.F5.sf1.1.1.1.m1.1.1.cmml"><mi id="S4.F5.sf1.1.1.1.m1.1.1.2" xref="S4.F5.sf1.1.1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.F5.sf1.1.1.1.m1.1.1.1" xref="S4.F5.sf1.1.1.1.m1.1.1.1.cmml">â€‹</mo><msub id="S4.F5.sf1.1.1.1.m1.1.1.3" xref="S4.F5.sf1.1.1.1.m1.1.1.3.cmml"><mi id="S4.F5.sf1.1.1.1.m1.1.1.3.2" xref="S4.F5.sf1.1.1.1.m1.1.1.3.2.cmml">P</mi><mn id="S4.F5.sf1.1.1.1.m1.1.1.3.3" xref="S4.F5.sf1.1.1.1.m1.1.1.3.3.cmml">10</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.sf1.1.1.1.m1.1b"><apply id="S4.F5.sf1.1.1.1.m1.1.1.cmml" xref="S4.F5.sf1.1.1.1.m1.1.1"><times id="S4.F5.sf1.1.1.1.m1.1.1.1.cmml" xref="S4.F5.sf1.1.1.1.m1.1.1.1"></times><ci id="S4.F5.sf1.1.1.1.m1.1.1.2.cmml" xref="S4.F5.sf1.1.1.1.m1.1.1.2">ğ´</ci><apply id="S4.F5.sf1.1.1.1.m1.1.1.3.cmml" xref="S4.F5.sf1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.F5.sf1.1.1.1.m1.1.1.3.1.cmml" xref="S4.F5.sf1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S4.F5.sf1.1.1.1.m1.1.1.3.2.cmml" xref="S4.F5.sf1.1.1.1.m1.1.1.3.2">ğ‘ƒ</ci><cn type="integer" id="S4.F5.sf1.1.1.1.m1.1.1.3.3.cmml" xref="S4.F5.sf1.1.1.1.m1.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.sf1.1.1.1.m1.1c">AP_{10}</annotation></semantics></math></th>
<th id="S4.F5.sf1.2.2.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"><math id="S4.F5.sf1.2.2.2.m1.1" class="ltx_Math" alttext="AP_{60}" display="inline"><semantics id="S4.F5.sf1.2.2.2.m1.1a"><mrow id="S4.F5.sf1.2.2.2.m1.1.1" xref="S4.F5.sf1.2.2.2.m1.1.1.cmml"><mi id="S4.F5.sf1.2.2.2.m1.1.1.2" xref="S4.F5.sf1.2.2.2.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.F5.sf1.2.2.2.m1.1.1.1" xref="S4.F5.sf1.2.2.2.m1.1.1.1.cmml">â€‹</mo><msub id="S4.F5.sf1.2.2.2.m1.1.1.3" xref="S4.F5.sf1.2.2.2.m1.1.1.3.cmml"><mi id="S4.F5.sf1.2.2.2.m1.1.1.3.2" xref="S4.F5.sf1.2.2.2.m1.1.1.3.2.cmml">P</mi><mn id="S4.F5.sf1.2.2.2.m1.1.1.3.3" xref="S4.F5.sf1.2.2.2.m1.1.1.3.3.cmml">60</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.sf1.2.2.2.m1.1b"><apply id="S4.F5.sf1.2.2.2.m1.1.1.cmml" xref="S4.F5.sf1.2.2.2.m1.1.1"><times id="S4.F5.sf1.2.2.2.m1.1.1.1.cmml" xref="S4.F5.sf1.2.2.2.m1.1.1.1"></times><ci id="S4.F5.sf1.2.2.2.m1.1.1.2.cmml" xref="S4.F5.sf1.2.2.2.m1.1.1.2">ğ´</ci><apply id="S4.F5.sf1.2.2.2.m1.1.1.3.cmml" xref="S4.F5.sf1.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.F5.sf1.2.2.2.m1.1.1.3.1.cmml" xref="S4.F5.sf1.2.2.2.m1.1.1.3">subscript</csymbol><ci id="S4.F5.sf1.2.2.2.m1.1.1.3.2.cmml" xref="S4.F5.sf1.2.2.2.m1.1.1.3.2">ğ‘ƒ</ci><cn type="integer" id="S4.F5.sf1.2.2.2.m1.1.1.3.3.cmml" xref="S4.F5.sf1.2.2.2.m1.1.1.3.3">60</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.sf1.2.2.2.m1.1c">AP_{60}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F5.sf1.2.3.1" class="ltx_tr">
<th id="S4.F5.sf1.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">w/o 3D</th>
<th id="S4.F5.sf1.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">0.109</th>
<td id="S4.F5.sf1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">1.8%</td>
<td id="S4.F5.sf1.2.3.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">90.2%</td>
</tr>
<tr id="S4.F5.sf1.2.4.2" class="ltx_tr">
<th id="S4.F5.sf1.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:5.7pt;padding-right:5.7pt;">w/o VAE</th>
<th id="S4.F5.sf1.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S4.F5.sf1.2.4.2.2.1" class="ltx_text ltx_font_bold">0.049</span></th>
<td id="S4.F5.sf1.2.4.2.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">12.3%</td>
<td id="S4.F5.sf1.2.4.2.4" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">92.7%</td>
</tr>
<tr id="S4.F5.sf1.2.5.3" class="ltx_tr">
<th id="S4.F5.sf1.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">Ours</th>
<th id="S4.F5.sf1.2.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">0.056</th>
<td id="S4.F5.sf1.2.5.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S4.F5.sf1.2.5.3.3.1" class="ltx_text ltx_font_bold">16.5%</span></td>
<td id="S4.F5.sf1.2.5.3.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S4.F5.sf1.2.5.3.4.1" class="ltx_text ltx_font_bold">97.1%</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.4.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.sf2.5.2" class="ltx_text" style="font-size:90%;">Latent space dimension.</span></figcaption>
<table id="S4.F5.sf2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F5.sf2.2.2" class="ltx_tr">
<th id="S4.F5.sf2.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"></th>
<th id="S4.F5.sf2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">L1</th>
<th id="S4.F5.sf2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"><math id="S4.F5.sf2.1.1.1.m1.1" class="ltx_Math" alttext="AP_{10}" display="inline"><semantics id="S4.F5.sf2.1.1.1.m1.1a"><mrow id="S4.F5.sf2.1.1.1.m1.1.1" xref="S4.F5.sf2.1.1.1.m1.1.1.cmml"><mi id="S4.F5.sf2.1.1.1.m1.1.1.2" xref="S4.F5.sf2.1.1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.F5.sf2.1.1.1.m1.1.1.1" xref="S4.F5.sf2.1.1.1.m1.1.1.1.cmml">â€‹</mo><msub id="S4.F5.sf2.1.1.1.m1.1.1.3" xref="S4.F5.sf2.1.1.1.m1.1.1.3.cmml"><mi id="S4.F5.sf2.1.1.1.m1.1.1.3.2" xref="S4.F5.sf2.1.1.1.m1.1.1.3.2.cmml">P</mi><mn id="S4.F5.sf2.1.1.1.m1.1.1.3.3" xref="S4.F5.sf2.1.1.1.m1.1.1.3.3.cmml">10</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.sf2.1.1.1.m1.1b"><apply id="S4.F5.sf2.1.1.1.m1.1.1.cmml" xref="S4.F5.sf2.1.1.1.m1.1.1"><times id="S4.F5.sf2.1.1.1.m1.1.1.1.cmml" xref="S4.F5.sf2.1.1.1.m1.1.1.1"></times><ci id="S4.F5.sf2.1.1.1.m1.1.1.2.cmml" xref="S4.F5.sf2.1.1.1.m1.1.1.2">ğ´</ci><apply id="S4.F5.sf2.1.1.1.m1.1.1.3.cmml" xref="S4.F5.sf2.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.F5.sf2.1.1.1.m1.1.1.3.1.cmml" xref="S4.F5.sf2.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S4.F5.sf2.1.1.1.m1.1.1.3.2.cmml" xref="S4.F5.sf2.1.1.1.m1.1.1.3.2">ğ‘ƒ</ci><cn type="integer" id="S4.F5.sf2.1.1.1.m1.1.1.3.3.cmml" xref="S4.F5.sf2.1.1.1.m1.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.sf2.1.1.1.m1.1c">AP_{10}</annotation></semantics></math></th>
<th id="S4.F5.sf2.2.2.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"><math id="S4.F5.sf2.2.2.2.m1.1" class="ltx_Math" alttext="AP_{60}" display="inline"><semantics id="S4.F5.sf2.2.2.2.m1.1a"><mrow id="S4.F5.sf2.2.2.2.m1.1.1" xref="S4.F5.sf2.2.2.2.m1.1.1.cmml"><mi id="S4.F5.sf2.2.2.2.m1.1.1.2" xref="S4.F5.sf2.2.2.2.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.F5.sf2.2.2.2.m1.1.1.1" xref="S4.F5.sf2.2.2.2.m1.1.1.1.cmml">â€‹</mo><msub id="S4.F5.sf2.2.2.2.m1.1.1.3" xref="S4.F5.sf2.2.2.2.m1.1.1.3.cmml"><mi id="S4.F5.sf2.2.2.2.m1.1.1.3.2" xref="S4.F5.sf2.2.2.2.m1.1.1.3.2.cmml">P</mi><mn id="S4.F5.sf2.2.2.2.m1.1.1.3.3" xref="S4.F5.sf2.2.2.2.m1.1.1.3.3.cmml">60</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.sf2.2.2.2.m1.1b"><apply id="S4.F5.sf2.2.2.2.m1.1.1.cmml" xref="S4.F5.sf2.2.2.2.m1.1.1"><times id="S4.F5.sf2.2.2.2.m1.1.1.1.cmml" xref="S4.F5.sf2.2.2.2.m1.1.1.1"></times><ci id="S4.F5.sf2.2.2.2.m1.1.1.2.cmml" xref="S4.F5.sf2.2.2.2.m1.1.1.2">ğ´</ci><apply id="S4.F5.sf2.2.2.2.m1.1.1.3.cmml" xref="S4.F5.sf2.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.F5.sf2.2.2.2.m1.1.1.3.1.cmml" xref="S4.F5.sf2.2.2.2.m1.1.1.3">subscript</csymbol><ci id="S4.F5.sf2.2.2.2.m1.1.1.3.2.cmml" xref="S4.F5.sf2.2.2.2.m1.1.1.3.2">ğ‘ƒ</ci><cn type="integer" id="S4.F5.sf2.2.2.2.m1.1.1.3.3.cmml" xref="S4.F5.sf2.2.2.2.m1.1.1.3.3">60</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.sf2.2.2.2.m1.1c">AP_{60}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F5.sf2.2.3.1" class="ltx_tr">
<th id="S4.F5.sf2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">4</th>
<th id="S4.F5.sf2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">0.082</th>
<td id="S4.F5.sf2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">9.1%</td>
<td id="S4.F5.sf2.2.3.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">96.6%</td>
</tr>
<tr id="S4.F5.sf2.2.4.2" class="ltx_tr">
<th id="S4.F5.sf2.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:5.7pt;padding-right:5.7pt;">16</th>
<th id="S4.F5.sf2.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:5.7pt;padding-right:5.7pt;">0.056</th>
<td id="S4.F5.sf2.2.4.2.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S4.F5.sf2.2.4.2.3.1" class="ltx_text ltx_font_bold">16.5%</span></td>
<td id="S4.F5.sf2.2.4.2.4" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S4.F5.sf2.2.4.2.4.1" class="ltx_text ltx_font_bold">97.1%</span></td>
</tr>
<tr id="S4.F5.sf2.2.5.3" class="ltx_tr">
<th id="S4.F5.sf2.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">128</th>
<th id="S4.F5.sf2.2.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S4.F5.sf2.2.5.3.2.1" class="ltx_text ltx_font_bold">0.041</span></th>
<td id="S4.F5.sf2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">11.4%</td>
<td id="S4.F5.sf2.2.5.3.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">96.0%</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2008.08145/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Image generated with different network architectures.<span id="S4.F6.4.2.1" class="ltx_text ltx_font_medium"> Our method can generate novel images in diverse shape and appearances and in desired poses. Without using KL divergence (w/o VAE) the network cannot generate novel samples. Without using 3D structure, the network struggles to faithfully reflect rotation and hence the image quality suffers. </span></span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Optimization.</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">We study the hyper-parameters for optimization. We plot the error evolution of our energy function and the rotation error at different iterations. As evident from Tab.Â <a href="#S4.F6.sf2" title="In Table 2 â€£ 4.2.2 Optimization. â€£ 4.2 Component Analysis â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a>, with decreasing loss, the rotational error also decreases, demonstrating that our energy function provides a meaningful signal for pose estimation. In practice, our method converges in less than 50 iterations in most cases. The fitting progress is also visualized qualitatively in the figure in Tab.Â <a href="#S4.F6.sf2" title="In Table 2 â€£ 4.2.2 Optimization. â€£ 4.2 Component Analysis â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a>. The initial pose is significantly different from the target as shown in the inset. The initial appearance, which is obtained via the encoder, is close to the target appearance but still displays noticeable differences, e.g. the touchpad. Our method jointly refines both the pose and appearance. During the first few iterations, the method mainly focuses on adjusting the pose since the pose has a larger influence on the error than the appearance. In the last few iterations the method mainly focuses on fine-tuning the appearance to adapt to the image.</p>
</div>
<figure id="S4.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation study of energy functions.<span id="S4.T2.4.2.1" class="ltx_text ltx_font_medium"> As shown in (a), perceptual loss outperforms other potential error functions. (b) shows that the rotational error decreases with decreasing energy value. </span></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf1.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F6.sf1.5.2" class="ltx_text" style="font-size:90%;">Effect of energy functions.</span></figcaption>
<table id="S4.F6.sf1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F6.sf1.2.2" class="ltx_tr">
<td id="S4.F6.sf1.2.2.3" class="ltx_td ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<th id="S4.F6.sf1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"><math id="S4.F6.sf1.1.1.1.m1.1" class="ltx_Math" alttext="AP_{10}" display="inline"><semantics id="S4.F6.sf1.1.1.1.m1.1a"><mrow id="S4.F6.sf1.1.1.1.m1.1.1" xref="S4.F6.sf1.1.1.1.m1.1.1.cmml"><mi id="S4.F6.sf1.1.1.1.m1.1.1.2" xref="S4.F6.sf1.1.1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.F6.sf1.1.1.1.m1.1.1.1" xref="S4.F6.sf1.1.1.1.m1.1.1.1.cmml">â€‹</mo><msub id="S4.F6.sf1.1.1.1.m1.1.1.3" xref="S4.F6.sf1.1.1.1.m1.1.1.3.cmml"><mi id="S4.F6.sf1.1.1.1.m1.1.1.3.2" xref="S4.F6.sf1.1.1.1.m1.1.1.3.2.cmml">P</mi><mn id="S4.F6.sf1.1.1.1.m1.1.1.3.3" xref="S4.F6.sf1.1.1.1.m1.1.1.3.3.cmml">10</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F6.sf1.1.1.1.m1.1b"><apply id="S4.F6.sf1.1.1.1.m1.1.1.cmml" xref="S4.F6.sf1.1.1.1.m1.1.1"><times id="S4.F6.sf1.1.1.1.m1.1.1.1.cmml" xref="S4.F6.sf1.1.1.1.m1.1.1.1"></times><ci id="S4.F6.sf1.1.1.1.m1.1.1.2.cmml" xref="S4.F6.sf1.1.1.1.m1.1.1.2">ğ´</ci><apply id="S4.F6.sf1.1.1.1.m1.1.1.3.cmml" xref="S4.F6.sf1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.F6.sf1.1.1.1.m1.1.1.3.1.cmml" xref="S4.F6.sf1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S4.F6.sf1.1.1.1.m1.1.1.3.2.cmml" xref="S4.F6.sf1.1.1.1.m1.1.1.3.2">ğ‘ƒ</ci><cn type="integer" id="S4.F6.sf1.1.1.1.m1.1.1.3.3.cmml" xref="S4.F6.sf1.1.1.1.m1.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.sf1.1.1.1.m1.1c">AP_{10}</annotation></semantics></math></th>
<th id="S4.F6.sf1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"><math id="S4.F6.sf1.2.2.2.m1.1" class="ltx_Math" alttext="AP_{60}" display="inline"><semantics id="S4.F6.sf1.2.2.2.m1.1a"><mrow id="S4.F6.sf1.2.2.2.m1.1.1" xref="S4.F6.sf1.2.2.2.m1.1.1.cmml"><mi id="S4.F6.sf1.2.2.2.m1.1.1.2" xref="S4.F6.sf1.2.2.2.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.F6.sf1.2.2.2.m1.1.1.1" xref="S4.F6.sf1.2.2.2.m1.1.1.1.cmml">â€‹</mo><msub id="S4.F6.sf1.2.2.2.m1.1.1.3" xref="S4.F6.sf1.2.2.2.m1.1.1.3.cmml"><mi id="S4.F6.sf1.2.2.2.m1.1.1.3.2" xref="S4.F6.sf1.2.2.2.m1.1.1.3.2.cmml">P</mi><mn id="S4.F6.sf1.2.2.2.m1.1.1.3.3" xref="S4.F6.sf1.2.2.2.m1.1.1.3.3.cmml">60</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F6.sf1.2.2.2.m1.1b"><apply id="S4.F6.sf1.2.2.2.m1.1.1.cmml" xref="S4.F6.sf1.2.2.2.m1.1.1"><times id="S4.F6.sf1.2.2.2.m1.1.1.1.cmml" xref="S4.F6.sf1.2.2.2.m1.1.1.1"></times><ci id="S4.F6.sf1.2.2.2.m1.1.1.2.cmml" xref="S4.F6.sf1.2.2.2.m1.1.1.2">ğ´</ci><apply id="S4.F6.sf1.2.2.2.m1.1.1.3.cmml" xref="S4.F6.sf1.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.F6.sf1.2.2.2.m1.1.1.3.1.cmml" xref="S4.F6.sf1.2.2.2.m1.1.1.3">subscript</csymbol><ci id="S4.F6.sf1.2.2.2.m1.1.1.3.2.cmml" xref="S4.F6.sf1.2.2.2.m1.1.1.3.2">ğ‘ƒ</ci><cn type="integer" id="S4.F6.sf1.2.2.2.m1.1.1.3.3.cmml" xref="S4.F6.sf1.2.2.2.m1.1.1.3.3">60</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.sf1.2.2.2.m1.1c">AP_{60}</annotation></semantics></math></th>
</tr>
<tr id="S4.F6.sf1.2.3.1" class="ltx_tr">
<td id="S4.F6.sf1.2.3.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">L1</td>
<td id="S4.F6.sf1.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">14.3%</td>
<td id="S4.F6.sf1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">85.3%</td>
</tr>
<tr id="S4.F6.sf1.2.4.2" class="ltx_tr">
<td id="S4.F6.sf1.2.4.2.1" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">L2</td>
<td id="S4.F6.sf1.2.4.2.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">13.2%</td>
<td id="S4.F6.sf1.2.4.2.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">88.3%</td>
</tr>
<tr id="S4.F6.sf1.2.5.3" class="ltx_tr">
<td id="S4.F6.sf1.2.5.3.1" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">SSIM</td>
<td id="S4.F6.sf1.2.5.3.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">15.8%</td>
<td id="S4.F6.sf1.2.5.3.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">92.5%</td>
</tr>
<tr id="S4.F6.sf1.2.6.4" class="ltx_tr">
<td id="S4.F6.sf1.2.6.4.1" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">w/o reg</td>
<td id="S4.F6.sf1.2.6.4.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">16.2%</td>
<td id="S4.F6.sf1.2.6.4.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">95.9%</td>
</tr>
<tr id="S4.F6.sf1.2.7.5" class="ltx_tr">
<td id="S4.F6.sf1.2.7.5.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">Ours</td>
<td id="S4.F6.sf1.2.7.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S4.F6.sf1.2.7.5.2.1" class="ltx_text ltx_font_bold">16.5%</span></td>
<td id="S4.F6.sf1.2.7.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S4.F6.sf1.2.7.5.3.1" class="ltx_text ltx_font_bold">97.1%</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">Loss and pose error along iterations.</span></figcaption><img src="/html/2008.08145/assets/x7.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="142" height="70" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
</figure>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">We also evaluate other potential choices for the energy function to minimize, including the mean absolute error (L1), the mean squared error (L2) and the Structural Similarity Index Metric (SSIM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. These error functions do not perform as well as the perceptual loss as shown in Tab.Â <a href="#S4.F6.sf1" title="In Table 2 â€£ 4.2.2 Optimization. â€£ 4.2 Component Analysis â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a>. This is likely due to the fact that the perceptual loss encourages semantic alignment rather than pixel-wise alignment. Thus it produces results that are globally aligned instead of focusing on local regions. We also conduct experiments in fitting without the regularization term (<span id="S4.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">w/o reg</span>). This leads to unrealistic samples that minimizes the loss in an undesired way, resulting in a performance decrease compared to our full model.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Robustness</h3>

<figure id="S4.F7" class="ltx_figure"><img src="/html/2008.08145/assets/x8.png" id="S4.F7.g1" class="ltx_graphics ltx_img_landscape" width="460" height="132" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Study of robustness.<span id="S4.F7.4.2.1" class="ltx_text ltx_font_medium"> We study the influence of object instances, occlusion, brightness, and translation on the accuracy of rotation estimation. Compared to the VGG-based regression baseline, ours retains low error under challenging conditions even without any data augmentation at training time.</span></span></figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In order to study the robustness of our method to varying factors causing domain shift, we evaluate our method in a controlled simulation environment. We train our network using the â€œlaptopâ€ category of the synthetic ShapeNet dataset and test on unseen synthetic instances. We mainly study three factors that often differ between real and simulated data, namely lighting, occlusion and offsets in bounding box detection. At test time, we vary one of these factors at a time and evaluate the average error in terms of orientation estimation. We modify the brightness of the target image for lighting, remove certain areas of the image to simulate occlusion and translate the image in 2D to emulate inaccurate 2D detection.
For comparison, we train a VGG16 network to regress rotation angles from images using the same training data.
It is well known that discriminative approaches are more susceptible to overfitting which often results in worse generalization performance compared to generative approaches.
We verify this finding for the category-level pose estimation task.
We also randomly vary the three factors already when training the discriminative approach, but only to a limited degree (up to 20% for occlusion, 40% for lighting and 25% for translation). At test time, we test the network on samples both within and beyond the training variations.
Note that our method <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">never</span> sees augmented images during training.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Fig.Â <a href="#S4.F7" title="Figure 7 â€£ 4.3 Robustness â€£ 4 Evaluation â€£ Category Level Object Pose Estimation via Neural Analysis-by-Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the results of this experiment. First, we observe that our generative approach is less sensitive to the gap between training and test instances which is a crucial design goal of our approach in order to deal with unseen objects. When varying the three factors, the discriminative model exhibits significant performance variations especially when the factor exceeds the variations in the training distribution. In contrast, our method exhibits less performance variation which demonstrates the robustness of our method.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we propose a novel solution to category-level object pose estimation.
We combine a gradient-based fitting procedure with a parametric neural image synthesis model that is capable of implicitly representing the appearance, shape and pose of entire object categories, thus avoiding the need for instance-specific 3D CAD models at test time. We show that this approach reaches performance on par with and sometimes even outperforms a strong RGB-D baseline.
While our focus lies on rigid objects, extending our method to handle non-rigid or partially rigid objects is an interesting direction for future work.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgement.</span> This research was partially supported by the Max Planck ETH Center for Learning Systems and a research gift from NVIDIA.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Abdal, R., Qin, Y., Wonka, P.: Image2stylegan: How to embed images into the
stylegan latent space? In: Proc. of the IEEE International Conf. on Computer
Vision (ICCV) (2019)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Achlioptas, P., Diamanti, O., Mitliagkas, I., Guibas, L.: Learning
representations and generative models for 3d point clouds. In: Proc. of the
International Conf. on Machine Learning (ICML) (2018)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bau, D., Strobelt, H., Peebles, W., Wulff, J., Zhou, B., Zhu, J.Y., Torralba,
A.: Semantic photo manipulation with a generative image prior. In: ACM
Transactions on Graphics (TOG) (2019)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Besl, P.J., McKay, N.D.: Method for registration of 3-d shapes. In: Sensor
fusion IV: control paradigms and data structures (1992)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Bloesch, M., Czarnowski, J., Clark, R., Leutenegger, S., Davison, A.J.:
Codeslamâ€”learning a compact, optimisable representation for dense visual
slam. In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)
(2018)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Brock, A., Lim, T., Ritchie, J.M., Weston, N.: Generative and discriminative
voxel modeling with convolutional neural networks. In: arXiv (2016)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,
Savarese, S., Savva, M., Song, S., Su, H., etÂ al.: Shapenet: An
information-rich 3d model repository. In: arXiv (2015)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel, P.:
Infogan: Interpretable representation learning by information maximizing
generative adversarial nets. In: Advances in Neural Information Processing
Systems (NeurIPS) (2016)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Chen, X., Song, J., Hilliges, O.: Monocular neural image based rendering with
continuous view control. In: Proc. of the IEEE International Conf. on
Computer Vision (ICCV) (2019)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Chen, Z., Zhang, H.: Learning implicit fields for generative shape modeling.
In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A
large-scale hierarchical image database. In: Proc. IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR) (2009)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Gao, L., Yang, J., Wu, T., Yuan, Y.J., Fu, H., Lai, Y.K., Zhang, H.: Sdm-net:
Deep generative network for structured deformable mesh. In: ACM Transactions
on Graphics (TOG) (2019)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Gecer, B., Ploumpis, S., Kotsia, I., Zafeiriou, S.: Ganfit: Generative
adversarial network fitting for high fidelity 3d face reconstruction. In:
Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Gu, J., Shen, Y., Zhou, B.: Image processing using multi-code gan prior. In:
Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2020)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
He, K., Gkioxari, G., DollÃ¡r, P., Girshick, R.: Mask r-cnn. In: Proc. of
the IEEE International Conf. on Computer Vision (ICCV) (2017)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
Mohamed, S., Lerchner, A.: beta-vae: Learning basic visual concepts with a
constrained variational framework. In: Proc. of the International Conf. on
Learning Representations (ICLR) (2017)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Hu, Y., Hugonot, J., Fua, P., Salzmann, M.: Segmentation-driven 6d object pose
estimation. In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) (2019)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive
instance normalization. In: Proc. of the IEEE International Conf. on Computer
Vision (ICCV) (2017)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Jahanian, A., Chai, L., Isola, P.: On the â€steerabilityâ€ of generative
adversarial networks. In: Proc. of the International Conf. on Learning
Representations (ICLR) (2020)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style
transfer and super-resolution. In: Proc. of the European Conf. on Computer
Vision (ECCV) (2016)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Karras, T., Laine, S., Aila, T.: A style-based generator architecture for
generative adversarial networks. In: Proc. IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Kehl, W., Manhardt, F., Tombari, F., Ilic, S., Navab, N.: Ssd-6d: Making
rgb-based 3d detection and 6d pose estimation great again. In: Proc. of the
IEEE International Conf. on Computer Vision (ICCV) (2017)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Kulkarni, T.D., Whitney, W.F., Kohli, P., Tenenbaum, J.: Deep convolutional
inverse graphics network. In: Advances in Neural Information Processing
Systems (NeurIPS) (2015)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Li, Y., Wang, G., Ji, X., Xiang, Y., Fox, D.: Deepim: Deep iterative matching
for 6d pose estimation. In: Proc. of the European Conf. on Computer Vision
(ECCV) (2018)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Li, Z., Wang, G., Ji, X.: Cdpn: Coordinates-based disentangled pose network for
real-time rgb-based 6-dof object pose estimation. In: Proc. of the IEEE
International Conf. on Computer Vision (ICCV) (2019)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Lin, C.H., Kong, C., Lucey, S.: Learning efficient point cloud generation for
dense 3d object reconstruction. In: AAAI (2018)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Locatello, F., Bauer, S., Lucic, M., Raetsch, G., Gelly, S., SchÃ¶lkopf, B.,
Bachem, O.: Challenging common assumptions in the unsupervised learning of
disentangled representations. In: Proc. of the International Conf. on Machine
Learning (ICML) (2019)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: Smpl: A skinned
multi-person linear model. In: ACM Transactions on Graphics (TOG) (2015)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Manhardt, F., Kehl, W., Navab, N., Tombari, F.: Deep model-based 6d pose
refinement in rgb. In: Proc. of the European Conf. on Computer Vision (ECCV)
(2018)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy
networks: Learning 3d reconstruction in function space. In: Proc. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Mustikovela, S.K., Jampani, V., DeÂ Mello, S., Liu, S., Iqbal, U., Rother, C.,
Kautz, J.: Self-supervised viewpoint learning from image collections. In:
Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2020)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Nguyen-Phuoc, T., Li, C., Theis, L., Richardt, C., Yang, Y.L.: Hologan:
Unsupervised learning of 3d representations from natural images. In: Proc. of
the IEEE International Conf. on Computer Vision (ICCV) (2019)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Oberweger, M., Rad, M., Lepetit, V.: Making deep heatmaps robust to partial
occlusions for 3d object pose estimation. In: Proc. of the European Conf. on
Computer Vision (ECCV) (2018)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Olszewski, K., Tulyakov, S., Woodford, O., Li, H., Luo, L.: Transformable
bottleneck networks. In: Proc. of the IEEE International Conf. on Computer
Vision (ICCV) (2019)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Park, E., Yang, J., Yumer, E., Ceylan, D., Berg, A.C.: Transformation-grounded
image generation network for novel 3d view synthesis. In: Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR) (2017)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: Deepsdf:
Learning continuous signed distance functions for shape representation. In:
Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Park, K., Mousavian, A., Xiang, Y., Fox, D.: Latentfusion: End-to-end
differentiable reconstruction and rendering for unseen object pose
estimation. In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) (2020)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Park, K., Patten, T., Vincze, M.: Pix2pose: Pixel-wise coordinate regression of
objects for 6d pose estimation. In: Proc. of the IEEE International Conf. on
Computer Vision (ICCV) (2019)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting
network for 6dof pose estimation. In: Proc. IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Penner, E., Zhang, L.: Soft 3d reconstruction for view synthesis. In: ACM
Transactions on Graphics (TOG) (2017)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Rad, M., Lepetit, V.: Bb8: A scalable, accurate, robust to partial occlusion
method for predicting the 3d poses of challenging objects without using
depth. In: Proc. of the IEEE International Conf. on Computer Vision (ICCV)
(2017)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Shen, Y., Gu, J., Tang, X., Zhou, B.: Interpreting the latent space of gans for
semantic face editing. In: Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR) (2020)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. In: Proc. of the International Conf. on Learning
Representations (ICLR) (2015)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Sitzmann, V., Thies, J., Heide, F., NieÃŸner, M., Wetzstein, G., Zollhofer,
M.: Deepvoxels: Learning persistent 3d feature embeddings. In: Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Sitzmann, V., ZollhÃ¶fer, M., Wetzstein, G.: Scene representation networks:
Continuous 3d-structure-aware neural scene representations. In: Advances in
Neural Information Processing Systems (NeurIPS) (2019)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Sun, S.H., Huh, M., Liao, Y.H., Zhang, N., Lim, J.J.: Multi-view to novel view:
Synthesizing novel views with self-learned confidence. In: Proc. of the
European Conf. on Computer Vision (ECCV) (2018)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Tan, Q., Gao, L., Lai, Y.K., Xia, S.: Variational autoencoders for deforming 3d
mesh models. In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) (2018)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Tatarchenko, M., Dosovitskiy, A., Brox, T.: Multi-view 3d models from single
images with a convolutional network. In: Proc. of the European Conf. on
Computer Vision (ECCV) (2016)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Tekin, B., Sinha, S.N., Fua, P.: Real-time seamless single shot 6d object pose
prediction. In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) (2018)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Wang, H., Sridhar, S., Huang, J., Valentin, J., Song, S., Guibas, L.J.:
Normalized object coordinate space for category-level 6d object pose and size
estimation. In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) (2019)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality
assessment: from error visibility to structural similarity. In: IEEE
Transactions on Image Processing (TIP) (2004)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Wu, J., Zhang, C., Xue, T., Freeman, B., Tenenbaum, J.: Learning a
probabilistic latent space of object shapes via 3d generative-adversarial
modeling. In: Advances in Neural Information Processing Systems (NeurIPS)
(2016)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Xiang, Y., Schmidt, T., Narayanan, V., Fox, D.: Posecnn: A convolutional neural
network for 6d object pose estimation in cluttered scenes. In: Robotics:
Science and Systems (RSS) (2018)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Yang, G., Huang, X., Hao, Z., Liu, M.Y., Belongie, S., Hariharan, B.:
Pointflow: 3d point cloud generation with continuous normalizing flows. In:
Proc. of the IEEE International Conf. on Computer Vision (ICCV) (2019)

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Zakharov, S., Shugurov, I., Ilic, S.: Dpod: 6d pose object detector and
refiner. In: Proc. of the IEEE International Conf. on Computer Vision (ICCV)
(2019)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Zeng, A., Yu, K.T., Song, S., Suo, D., Walker, E., Rodriguez, A., Xiao, J.:
Multi-view self-supervised deep learning for 6d pose estimation in the amazon
picking challenge. In: Proc. of the International Conf. on on Robotics and
Automation (ICRA) (2017)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Zhou, T., Tulsiani, S., Sun, W., Malik, J., Efros, A.A.: View synthesis by
appearance flow. In: Proc. of the European Conf. on Computer Vision (ECCV)
(2016)

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Zhu, J.Y., Zhang, Z., Zhang, C., Wu, J., Torralba, A., Tenenbaum, J.B.,
Freeman, W.T.: Visual object networks: Image generation with disentangled
3D representations. In: Advances in Neural Information Processing Systems
(NeurIPS) (2018)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2008.08144" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2008.08145" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2008.08145">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2008.08145" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2008.08146" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 11:49:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
