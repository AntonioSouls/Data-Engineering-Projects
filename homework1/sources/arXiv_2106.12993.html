<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2106.12993] Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data</title><meta property="og:description" content="The assessment of laboratory animal behavior is of central interest in modern neuroscience research.
Behavior is typically studied in terms of pose changes, which are ideally captured in three dimensions.
This requiresâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2106.12993">

<!--Generated on Sat Mar  9 02:24:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Indrani Sarkar<sup id="id9.8.id1" class="ltx_sup"><span id="id9.8.id1.1" class="ltx_text ltx_font_italic">1</span></sup>  Â Â Â Â  Indranil Maji<sup id="id10.9.id2" class="ltx_sup">1</sup><span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> Â Â Â Â  Charitha Omprakash<sup id="id11.10.id3" class="ltx_sup">2</sup><span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> 
<br class="ltx_break">Sebastian Stober<sup id="id12.11.id4" class="ltx_sup">1</sup> Â Â Â Â  Sanja Mikulovic<sup id="id13.12.id5" class="ltx_sup">2</sup> Â Â Â Â  Pavol Bauer<sup id="id14.13.id6" class="ltx_sup"><span id="id14.13.id6.1" class="ltx_text ltx_font_italic">2</span></sup>
<br class="ltx_break"><sup id="id15.14.id7" class="ltx_sup">1</sup> Otto von Guericke University
</span><span class="ltx_author_notes">Equal contribution.</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Germany
<br class="ltx_break"><sup id="id16.2.id1" class="ltx_sup">2</sup> Leibniz Institute for Neurobiology
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Germany
<br class="ltx_break"><span id="id17.1.id1" class="ltx_text ltx_font_typewriter">pavol.bauer@lin-magdeburg.de</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id18.id1" class="ltx_p">The assessment of laboratory animal behavior is of central interest in modern neuroscience research.
Behavior is typically studied in terms of pose changes, which are ideally captured in three dimensions.
This requires triangulation over a multi-camera system which view the animal from different angles.
However, this is challenging in realistic laboratory setups due to occlusions and other technical constrains.
Here we propose the usage of lift-pose models that allow for robust 3D pose estimation of freely moving rodents from a single view camera view.
To obtain high-quality training data for the pose-lifting, we first perform geometric calibration in a camera setup involving bottom as well as side views of the behaving animal.
We then evaluate the performance of two previously proposed model architectures under given inference perspectives and conclude that reliable 3D pose inference can be obtained using temporal convolutions.
With this work we would like to contribute to a more robust and diverse behavior tracking of freely moving rodents for a wide range of experiments and setups in the neuroscience community.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A central goal of modern neuroscience research is to measure and quantify behavior of laboratory animals in order to enable correction studies to neuronal activity.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2106.12993/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A: Exemplary input data of unconstrained mouse behavior tracked with 2D keypoint detectors from 5 perspectives. B: Outline of the inference pipeline; first, we obtain high-quality triangulation data from the 5-camera setup. This data is then used to train the lift pose model for 3D inference from single camera views. </figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Animal pose estimation in two dimensions has been recently made possible using convolutional neural networks that allow keypoint detections throughout a recorded video based on the training data labeled by the user <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
If the goal is to obtain the animal pose in three dimensions, it is possible to combine the 2D keypoint detections from multiple synchronously operated cameras, and triangulate the points into a global 3D space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
However, prior to the triangulation, the cameras need to be calibrated using a target, which is often an erroneous process in practice that can result in skewed projections of points into the global space.
Moreover, during many experiments in behavioral neuroscience, it is difficult to establish continuous 3D tracking of keypoints over time as often it can not be guaranteed that two or more cameras have a view on all tracked body parts.
This can occur due to different reasons; difficulties in mounting the cameras in the desired angles, occlusion of objects or others animals, and self occlusion of the animal itself.
To overcome such shortcomings for 3D pose estimation in humans, lift pose models have been developed, which aim to infer the 3D pose directly from a single camera view <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This work contains two contributions. First, we present a simple and robust procedure for triangulation of freely moving rodents from multiple cameras that are orthogonal towards a camera positioned underneath the plane of movement (Figure <a href="#S3.F2" title="Figure 2 â€£ 3 Geometric camera calibration â€£ Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
Second, we evaluate two model architectures that have been proposed for 3D pose lifting of human poses previously, one a linear residual network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and the other a dilated temporal convolutional residual network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, aiming to establish which of both models works better for our triangulated rodent pose data.
Moreover, we evaluate the choice of the temporal window setting on the performance of the temporal convolutional model as well as the choice of the viewing perspective on the performance of both models.
The aim of this work is to pave the way for robust detection of 3D rodent poses from single poses, allowing for studies of behavior in complex laboratory as well as naturalistic environments.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Previously 3D pose estimation on humans was proposed by Martinez <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S2.p1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> where a simple linear residual network was trained on HumanEva and Human3.6M dataset. This architecture has already been evaluated by Gosztolai <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S2.p1.1.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> for 3D pose estimation in freely moving monkeys as well as rodents in a constrained behavioral setup. Pose lifting of human data using temporal 1-dimensional dilated convolutional neural networks was previously proposed by Pavllo <span id="S2.p1.1.3" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.3.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S2.p1.1.3.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>. View-invariant human pose estimation from embedding spaces was recently proposed by Sun <span id="S2.p1.1.4" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.4.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S2.p1.1.4.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>.
Approaches to triangulation of animals include Anipose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and more recently the DANNCE framework that employs 3D convolutions for improving detections of 3D poses captured from a multi-camera setup <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Geometric camera calibration</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Classical triangulation of points is based on the projection of 2D planes onto a global 3D space. This requires calibration with a flat calibration target, typically a checker-board, that needs to be visible to all cameras simultaneously in order to identify the direction of the projection.
Here we propose a simple and robust triangulation process based on orthogonality of cameras that are unable to view the classical calibration target simultaneously in their spatial configuration (Figure <a href="#S3.F2" title="Figure 2 â€£ 3 Geometric camera calibration â€£ Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.2" class="ltx_p">This approach is motivated by the fact that the most variability of rodent movement is visible from the bottom perspective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, from which the <math id="S3.p2.1.m1.2" class="ltx_Math" alttext="x,y" display="inline"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.3.2" xref="S3.p2.1.m1.2.3.1.cmml"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">x</mi><mo id="S3.p2.1.m1.2.3.2.1" xref="S3.p2.1.m1.2.3.1.cmml">,</mo><mi id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><list id="S3.p2.1.m1.2.3.1.cmml" xref="S3.p2.1.m1.2.3.2"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">ğ‘¥</ci><ci id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2">ğ‘¦</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">x,y</annotation></semantics></math> position of many body parts can be directly detected given reliable 2d pose estimation.
The additional orthogonal cameras are then needed to determine the <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">z</annotation></semantics></math> coordinate for each keypoint, which can be given by the evaluation of a polynomial function determined during the calibration process.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.4" class="ltx_p">To perform the experiments, we let one C57BL/6J wildtype animal freely explore a circular arena with a transparent Plexiglas floor.
We recorded 60 minutes of unconstrained behavior using 5 synchronous cameras operating at 50 Hz that have been mounted as shown in Figure <a href="#S3.F2" title="Figure 2 â€£ 3 Geometric camera calibration â€£ Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
We then labeled 8 body parts in <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="1e3" display="inline"><semantics id="S3.p3.1.m1.1a"><mrow id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mn id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.p3.1.m1.1.1.1" xref="S3.p3.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.p3.1.m1.1.1.1a" xref="S3.p3.1.m1.1.1.1.cmml">â€‹</mo><mn id="S3.p3.1.m1.1.1.4" xref="S3.p3.1.m1.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><times id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">1</cn><ci id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">ğ‘’</ci><cn type="integer" id="S3.p3.1.m1.1.1.4.cmml" xref="S3.p3.1.m1.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">1e3</annotation></semantics></math> images and trained a ResNet CNN model provided in the <span id="S3.p3.4.1" class="ltx_text ltx_font_italic">DeepLabCut 2.2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to detect the keypoints from the bottom view as well as side view recordings (Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
The image augmentation routines supplied in the <span id="S3.p3.4.2" class="ltx_text ltx_font_italic">DeepLabCut</span> package were used to improve the generalization of the network.
The keypoint coordinates were egocentrically aligned, so that the nosetip and tailbase marker form a vector parallel to the <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">x</annotation></semantics></math>-axis and the center of the animal body is approximately at the origin of the <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">x</annotation></semantics></math> and <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">y</annotation></semantics></math> axes.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2106.12993/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="177" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Multi camera setup for geometric triangulation.</figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.5" class="ltx_p">The triangulation procedure was performed as follows. The algorithm iterated over the tuples of detected <math id="S3.p4.1.m1.2" class="ltx_Math" alttext="x,y" display="inline"><semantics id="S3.p4.1.m1.2a"><mrow id="S3.p4.1.m1.2.3.2" xref="S3.p4.1.m1.2.3.1.cmml"><mi id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">x</mi><mo id="S3.p4.1.m1.2.3.2.1" xref="S3.p4.1.m1.2.3.1.cmml">,</mo><mi id="S3.p4.1.m1.2.2" xref="S3.p4.1.m1.2.2.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.2b"><list id="S3.p4.1.m1.2.3.1.cmml" xref="S3.p4.1.m1.2.3.2"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">ğ‘¥</ci><ci id="S3.p4.1.m1.2.2.cmml" xref="S3.p4.1.m1.2.2">ğ‘¦</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.2c">x,y</annotation></semantics></math> coordinates obtained from the bottom perspective. For each timestep and keypoint, the algorithm then searched for detections from the side camera that were obtained from the ResNet at a high accuracy (<math id="S3.p4.2.m2.1" class="ltx_Math" alttext="p&gt;0.95" display="inline"><semantics id="S3.p4.2.m2.1a"><mrow id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><mi id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml">p</mi><mo id="S3.p4.2.m2.1.1.1" xref="S3.p4.2.m2.1.1.1.cmml">&gt;</mo><mn id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><gt id="S3.p4.2.m2.1.1.1.cmml" xref="S3.p4.2.m2.1.1.1"></gt><ci id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2">ğ‘</ci><cn type="float" id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">p&gt;0.95</annotation></semantics></math>).
The height of the pixel on the side frame was converted into a physical <math id="S3.p4.3.m3.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.p4.3.m3.1a"><mi id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><ci id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">z</annotation></semantics></math> value via the evaluation of a polynomial function, which was obtained a priori using a cylindrical calibration object with physical markers at three heights that was moved over the arena (Figure <a href="#S3.F3" title="Figure 3 â€£ 3 Geometric camera calibration â€£ Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
If more then one side camera reliably detected a keypoint, an average of the detections was assigned to the <math id="S3.p4.4.m4.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.p4.4.m4.1a"><mi id="S3.p4.4.m4.1.1" xref="S3.p4.4.m4.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p4.4.m4.1b"><ci id="S3.p4.4.m4.1.1.cmml" xref="S3.p4.4.m4.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.4.m4.1c">z</annotation></semantics></math> value of the particular keypoint.
In total, in about <math id="S3.p4.5.m5.1" class="ltx_Math" alttext="86\%" display="inline"><semantics id="S3.p4.5.m5.1a"><mrow id="S3.p4.5.m5.1.1" xref="S3.p4.5.m5.1.1.cmml"><mn id="S3.p4.5.m5.1.1.2" xref="S3.p4.5.m5.1.1.2.cmml">86</mn><mo id="S3.p4.5.m5.1.1.1" xref="S3.p4.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.5.m5.1b"><apply id="S3.p4.5.m5.1.1.cmml" xref="S3.p4.5.m5.1.1"><csymbol cd="latexml" id="S3.p4.5.m5.1.1.1.cmml" xref="S3.p4.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S3.p4.5.m5.1.1.2.cmml" xref="S3.p4.5.m5.1.1.2">86</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.5.m5.1c">86\%</annotation></semantics></math> of the time frames all body parts could be detected in one of the side cameras. The remaining points were interpolated using an exponentially weighted moving average filter. Finally, the mean centered and z-scored values have been used for training of the pose-lifting models.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2106.12993/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="267" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Polynomial fitting function used for translating the <math id="S3.F3.2.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.F3.2.m1.1b"><mi id="S3.F3.2.m1.1.1" xref="S3.F3.2.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.m1.1c"><ci id="S3.F3.2.m1.1.1.cmml" xref="S3.F3.2.m1.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.m1.1d">y</annotation></semantics></math> coordinate of the calibration marker heights to the physical height in the arena, given the distance of the calibration target to the camera.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Lifting models</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Linear Model </h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">Linear model</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> architecture consists of a linear layer followed by a batchnorm layer, a ReLU activation function and dropout with 0.25 probability (Figure <a href="#S4.F4" title="Figure 4 â€£ 4.1 Linear Model â€£ 4 Lifting models â€£ Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). The linear layer is then followed by a residual block consisting of two linear layers followed by batchnorm layer, ReLU and dropout, where the input and output of this block are connected by a residual connection.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2106.12993/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Main Model Architecture of the <span id="S4.F4.2.1" class="ltx_text ltx_font_italic">Linear model</span>.</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.5" class="ltx_p">We used the MPJPE (Mean Per Joint Position Error) to compute the loss at every iteration,</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.2" class="ltx_Math" alttext="\text{MPJPE}=\frac{1}{N}\frac{1}{K}\sum_{i=1}^{N}\sum_{k=1}^{K}\parallel f(x)-y\parallel," display="block"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2.1" xref="S4.E1.m1.2.2.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.cmml"><mtext id="S4.E1.m1.2.2.1.1.3" xref="S4.E1.m1.2.2.1.1.3a.cmml">MPJPE</mtext><mo id="S4.E1.m1.2.2.1.1.2" xref="S4.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.2.2.1.1.1" xref="S4.E1.m1.2.2.1.1.1.cmml"><mfrac id="S4.E1.m1.2.2.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.3.cmml"><mn id="S4.E1.m1.2.2.1.1.1.3.2" xref="S4.E1.m1.2.2.1.1.1.3.2.cmml">1</mn><mi id="S4.E1.m1.2.2.1.1.1.3.3" xref="S4.E1.m1.2.2.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.2.cmml">â€‹</mo><mfrac id="S4.E1.m1.2.2.1.1.1.4" xref="S4.E1.m1.2.2.1.1.1.4.cmml"><mn id="S4.E1.m1.2.2.1.1.1.4.2" xref="S4.E1.m1.2.2.1.1.1.4.2.cmml">1</mn><mi id="S4.E1.m1.2.2.1.1.1.4.3" xref="S4.E1.m1.2.2.1.1.1.4.3.cmml">K</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.1.2a" xref="S4.E1.m1.2.2.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E1.m1.2.2.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.cmml"><munderover id="S4.E1.m1.2.2.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S4.E1.m1.2.2.1.1.1.1.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.2.2.3.2" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E1.m1.2.2.1.1.1.1.2.2.3.1" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E1.m1.2.2.1.1.1.1.2.2.3.3" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.2.2.1.1.1.1.2.3" xref="S4.E1.m1.2.2.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S4.E1.m1.2.2.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.cmml"><munderover id="S4.E1.m1.2.2.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S4.E1.m1.2.2.1.1.1.1.1.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.2" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.2.cmml">k</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.1" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.3" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.2.2.1.1.1.1.1.2.3" xref="S4.E1.m1.2.2.1.1.1.1.1.2.3.cmml">K</mi></munderover><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.1.cmml">â€‹</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.2.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">y</mi></mrow><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow></mrow></mrow></mrow></mrow><mo id="S4.E1.m1.2.2.1.2" xref="S4.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1"><eq id="S4.E1.m1.2.2.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.2"></eq><ci id="S4.E1.m1.2.2.1.1.3a.cmml" xref="S4.E1.m1.2.2.1.1.3"><mtext id="S4.E1.m1.2.2.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.3">MPJPE</mtext></ci><apply id="S4.E1.m1.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1"><times id="S4.E1.m1.2.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.2"></times><apply id="S4.E1.m1.2.2.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.3"><divide id="S4.E1.m1.2.2.1.1.1.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.3"></divide><cn type="integer" id="S4.E1.m1.2.2.1.1.1.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.3.2">1</cn><ci id="S4.E1.m1.2.2.1.1.1.3.3.cmml" xref="S4.E1.m1.2.2.1.1.1.3.3">ğ‘</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.4.cmml" xref="S4.E1.m1.2.2.1.1.1.4"><divide id="S4.E1.m1.2.2.1.1.1.4.1.cmml" xref="S4.E1.m1.2.2.1.1.1.4"></divide><cn type="integer" id="S4.E1.m1.2.2.1.1.1.4.2.cmml" xref="S4.E1.m1.2.2.1.1.1.4.2">1</cn><ci id="S4.E1.m1.2.2.1.1.1.4.3.cmml" xref="S4.E1.m1.2.2.1.1.1.4.3">ğ¾</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1"><apply id="S4.E1.m1.2.2.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.2.2.1.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="S4.E1.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2"></sum><apply id="S4.E1.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3"><eq id="S4.E1.m1.2.2.1.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.1"></eq><ci id="S4.E1.m1.2.2.1.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S4.E1.m1.2.2.1.1.1.1.2.2.3.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.2.2.1.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.3">ğ‘</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1"><apply id="S4.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2">subscript</csymbol><sum id="S4.E1.m1.2.2.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2.2"></sum><apply id="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2.3"><eq id="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.2">ğ‘˜</ci><cn type="integer" id="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.3">ğ¾</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1"><minus id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2"><times id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.1"></times><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2">ğ‘“</ci><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">ğ‘¥</ci></apply><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3">ğ‘¦</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\text{MPJPE}=\frac{1}{N}\frac{1}{K}\sum_{i=1}^{N}\sum_{k=1}^{K}\parallel f(x)-y\parallel,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p2.4" class="ltx_p">where <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">N</annotation></semantics></math> is the total number of samples in the dataset , <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">K</annotation></semantics></math> is the total number of keypoints we are considering for our experiments, <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="f(x)" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.2" xref="S4.SS1.p2.3.m3.1.2.cmml"><mi id="S4.SS1.p2.3.m3.1.2.2" xref="S4.SS1.p2.3.m3.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.2.1" xref="S4.SS1.p2.3.m3.1.2.1.cmml">â€‹</mo><mrow id="S4.SS1.p2.3.m3.1.2.3.2" xref="S4.SS1.p2.3.m3.1.2.cmml"><mo stretchy="false" id="S4.SS1.p2.3.m3.1.2.3.2.1" xref="S4.SS1.p2.3.m3.1.2.cmml">(</mo><mi id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">x</mi><mo stretchy="false" id="S4.SS1.p2.3.m3.1.2.3.2.2" xref="S4.SS1.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.2.cmml" xref="S4.SS1.p2.3.m3.1.2"><times id="S4.SS1.p2.3.m3.1.2.1.cmml" xref="S4.SS1.p2.3.m3.1.2.1"></times><ci id="S4.SS1.p2.3.m3.1.2.2.cmml" xref="S4.SS1.p2.3.m3.1.2.2">ğ‘“</ci><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">f(x)</annotation></semantics></math> is the predicted 3D pose coordinate by the model and <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mi id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">y</annotation></semantics></math> is our triangulated 3D coordinate used as target data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.2" class="ltx_p">The model operates on keypoints detected in a single time step, <span id="S4.SS1.p3.2.1" class="ltx_ERROR undefined">\ie</span><math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">z</annotation></semantics></math> is predicted from the tuple <math id="S4.SS1.p3.2.m2.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S4.SS1.p3.2.m2.2a"><mrow id="S4.SS1.p3.2.m2.2.3.2" xref="S4.SS1.p3.2.m2.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p3.2.m2.2.3.2.1" xref="S4.SS1.p3.2.m2.2.3.1.cmml">(</mo><mi id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">x</mi><mo id="S4.SS1.p3.2.m2.2.3.2.2" xref="S4.SS1.p3.2.m2.2.3.1.cmml">,</mo><mi id="S4.SS1.p3.2.m2.2.2" xref="S4.SS1.p3.2.m2.2.2.cmml">y</mi><mo stretchy="false" id="S4.SS1.p3.2.m2.2.3.2.3" xref="S4.SS1.p3.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.2b"><interval closure="open" id="S4.SS1.p3.2.m2.2.3.1.cmml" xref="S4.SS1.p3.2.m2.2.3.2"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">ğ‘¥</ci><ci id="S4.SS1.p3.2.m2.2.2.cmml" xref="S4.SS1.p3.2.m2.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.2c">(x,y)</annotation></semantics></math>.
We furthermore added the decaying momentum to the batch normalization layers to make the model more comparable to the <span id="S4.SS1.p3.2.2" class="ltx_text ltx_font_italic">Temporal Convolutional Model</span>. The momentum parameter decides how much of the statistics of the input variables for a layer is used to normalize the input distribution between the layers. The model was trained using the Adam Optimizer and uses the Kaiming Initializers for initialization of the weights.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Temporal Convolutional Model</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">Temporal Convolutional model<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS2.p1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> architecture consists of 1-dimensional convolutional layer followed by a batch normalisation layer, ReLU and dropout (Figure <a href="#S4.F5" title="Figure 5 â€£ 4.2 Temporal Convolutional Model â€£ 4 Lifting models â€£ Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). This arrangement is followed by <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">N</annotation></semantics></math> number of residual blocks where each of consists of a combination of 1-dimensional convolution layer, a batch norm layer, ReLU and dropout followed repeatedly by the same combination again. Finally there exists another 1-dimensional convolutional layer before the output layer.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2106.12993/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Model Architecture of the <span id="S4.F5.2.1" class="ltx_text ltx_font_italic">Temporal Convolutional model</span>.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">As proposed in the original publication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, the model exists in two variants. One variant of this model uses strided convolutions for training, as it is takes better care of unused intermediate results in the hidden layers. The other variant uses dilated convolutions which uses all the intermediate results for the prediction of the 3D pose output. In our case, we carried out experiments using strided convolutions both for training and prediction of 3D pose coordinates.
Furthermore, we used symmetric convolutions only in the model, as causal convolutions are rather practical for real-time inference scenarios<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.8" class="ltx_p">For prediction of 3D poses at a particular time step <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">t</annotation></semantics></math> we consider a temporal window of size <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mi id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">T</annotation></semantics></math> . This temporal window contains input coordinates from timestep <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="(t-T)" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><mrow id="S4.SS2.p3.3.m3.1.1.1" xref="S4.SS2.p3.3.m3.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p3.3.m3.1.1.1.2" xref="S4.SS2.p3.3.m3.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p3.3.m3.1.1.1.1" xref="S4.SS2.p3.3.m3.1.1.1.1.cmml"><mi id="S4.SS2.p3.3.m3.1.1.1.1.2" xref="S4.SS2.p3.3.m3.1.1.1.1.2.cmml">t</mi><mo id="S4.SS2.p3.3.m3.1.1.1.1.1" xref="S4.SS2.p3.3.m3.1.1.1.1.1.cmml">âˆ’</mo><mi id="S4.SS2.p3.3.m3.1.1.1.1.3" xref="S4.SS2.p3.3.m3.1.1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S4.SS2.p3.3.m3.1.1.1.3" xref="S4.SS2.p3.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><apply id="S4.SS2.p3.3.m3.1.1.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1"><minus id="S4.SS2.p3.3.m3.1.1.1.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.1"></minus><ci id="S4.SS2.p3.3.m3.1.1.1.1.2.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.2">ğ‘¡</ci><ci id="S4.SS2.p3.3.m3.1.1.1.1.3.cmml" xref="S4.SS2.p3.3.m3.1.1.1.1.3">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">(t-T)</annotation></semantics></math> to timestep <math id="S4.SS2.p3.4.m4.1" class="ltx_Math" alttext="(t+T)" display="inline"><semantics id="S4.SS2.p3.4.m4.1a"><mrow id="S4.SS2.p3.4.m4.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p3.4.m4.1.1.1.2" xref="S4.SS2.p3.4.m4.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p3.4.m4.1.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.1.cmml"><mi id="S4.SS2.p3.4.m4.1.1.1.1.2" xref="S4.SS2.p3.4.m4.1.1.1.1.2.cmml">t</mi><mo id="S4.SS2.p3.4.m4.1.1.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.1.1.cmml">+</mo><mi id="S4.SS2.p3.4.m4.1.1.1.1.3" xref="S4.SS2.p3.4.m4.1.1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S4.SS2.p3.4.m4.1.1.1.3" xref="S4.SS2.p3.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><apply id="S4.SS2.p3.4.m4.1.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1"><plus id="S4.SS2.p3.4.m4.1.1.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.1"></plus><ci id="S4.SS2.p3.4.m4.1.1.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.2">ğ‘¡</ci><ci id="S4.SS2.p3.4.m4.1.1.1.1.3.cmml" xref="S4.SS2.p3.4.m4.1.1.1.1.3">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">(t+T)</annotation></semantics></math>.
Hence as input we consider the past timestep <math id="S4.SS2.p3.5.m5.1" class="ltx_Math" alttext="(t-T)" display="inline"><semantics id="S4.SS2.p3.5.m5.1a"><mrow id="S4.SS2.p3.5.m5.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p3.5.m5.1.1.1.2" xref="S4.SS2.p3.5.m5.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p3.5.m5.1.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.1.cmml"><mi id="S4.SS2.p3.5.m5.1.1.1.1.2" xref="S4.SS2.p3.5.m5.1.1.1.1.2.cmml">t</mi><mo id="S4.SS2.p3.5.m5.1.1.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.1.1.cmml">âˆ’</mo><mi id="S4.SS2.p3.5.m5.1.1.1.1.3" xref="S4.SS2.p3.5.m5.1.1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S4.SS2.p3.5.m5.1.1.1.3" xref="S4.SS2.p3.5.m5.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><apply id="S4.SS2.p3.5.m5.1.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1"><minus id="S4.SS2.p3.5.m5.1.1.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1.1.1"></minus><ci id="S4.SS2.p3.5.m5.1.1.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.1.1.2">ğ‘¡</ci><ci id="S4.SS2.p3.5.m5.1.1.1.1.3.cmml" xref="S4.SS2.p3.5.m5.1.1.1.1.3">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">(t-T)</annotation></semantics></math> together with future timestep <math id="S4.SS2.p3.6.m6.1" class="ltx_Math" alttext="(t+T)" display="inline"><semantics id="S4.SS2.p3.6.m6.1a"><mrow id="S4.SS2.p3.6.m6.1.1.1" xref="S4.SS2.p3.6.m6.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p3.6.m6.1.1.1.2" xref="S4.SS2.p3.6.m6.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p3.6.m6.1.1.1.1" xref="S4.SS2.p3.6.m6.1.1.1.1.cmml"><mi id="S4.SS2.p3.6.m6.1.1.1.1.2" xref="S4.SS2.p3.6.m6.1.1.1.1.2.cmml">t</mi><mo id="S4.SS2.p3.6.m6.1.1.1.1.1" xref="S4.SS2.p3.6.m6.1.1.1.1.1.cmml">+</mo><mi id="S4.SS2.p3.6.m6.1.1.1.1.3" xref="S4.SS2.p3.6.m6.1.1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S4.SS2.p3.6.m6.1.1.1.3" xref="S4.SS2.p3.6.m6.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.6.m6.1b"><apply id="S4.SS2.p3.6.m6.1.1.1.1.cmml" xref="S4.SS2.p3.6.m6.1.1.1"><plus id="S4.SS2.p3.6.m6.1.1.1.1.1.cmml" xref="S4.SS2.p3.6.m6.1.1.1.1.1"></plus><ci id="S4.SS2.p3.6.m6.1.1.1.1.2.cmml" xref="S4.SS2.p3.6.m6.1.1.1.1.2">ğ‘¡</ci><ci id="S4.SS2.p3.6.m6.1.1.1.1.3.cmml" xref="S4.SS2.p3.6.m6.1.1.1.1.3">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.6.m6.1c">(t+T)</annotation></semantics></math> input coordinates along with the present timestep <math id="S4.SS2.p3.7.m7.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.p3.7.m7.1a"><mi id="S4.SS2.p3.7.m7.1.1" xref="S4.SS2.p3.7.m7.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.7.m7.1b"><ci id="S4.SS2.p3.7.m7.1.1.cmml" xref="S4.SS2.p3.7.m7.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.7.m7.1c">t</annotation></semantics></math> coordinates. Hence, the network predicts the current timestep <math id="S4.SS2.p3.8.m8.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.p3.8.m8.1a"><mi id="S4.SS2.p3.8.m8.1.1" xref="S4.SS2.p3.8.m8.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.8.m8.1b"><ci id="S4.SS2.p3.8.m8.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.8.m8.1c">t</annotation></semantics></math> as output.
The 1-dimensional input kernel convolves vertically along the axis of data samples. The individual keypoints or features for each instance act as different channels for the convolution.
The model was trained using the Adam optimizer and the loss was the MPJPE loss (<a href="#S4.E1" title="In 4.1 Linear Model â€£ 4 Lifting models â€£ Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.8" class="ltx_p">In this section we compare the performance of models, the <span id="S5.p1.8.1" class="ltx_text ltx_font_italic">Linear model</span> and the <span id="S5.p1.8.2" class="ltx_text ltx_font_italic">Temporal Convolutional Model</span>, on the described dataset.
For comparison, we consider three scenarios: the first scenario predicts the <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S5.p1.1.m1.1a"><mi id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">z</annotation></semantics></math> coordinate given the tuple <math id="S5.p1.2.m2.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S5.p1.2.m2.2a"><mrow id="S5.p1.2.m2.2.3.2" xref="S5.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S5.p1.2.m2.2.3.2.1" xref="S5.p1.2.m2.2.3.1.cmml">(</mo><mi id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">x</mi><mo id="S5.p1.2.m2.2.3.2.2" xref="S5.p1.2.m2.2.3.1.cmml">,</mo><mi id="S5.p1.2.m2.2.2" xref="S5.p1.2.m2.2.2.cmml">y</mi><mo stretchy="false" id="S5.p1.2.m2.2.3.2.3" xref="S5.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.2b"><interval closure="open" id="S5.p1.2.m2.2.3.1.cmml" xref="S5.p1.2.m2.2.3.2"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">ğ‘¥</ci><ci id="S5.p1.2.m2.2.2.cmml" xref="S5.p1.2.m2.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.2c">(x,y)</annotation></semantics></math>. The second scenario predicts the <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S5.p1.3.m3.1a"><mi id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><ci id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">y</annotation></semantics></math> coordinate given the tuple <math id="S5.p1.4.m4.2" class="ltx_Math" alttext="(x,z)" display="inline"><semantics id="S5.p1.4.m4.2a"><mrow id="S5.p1.4.m4.2.3.2" xref="S5.p1.4.m4.2.3.1.cmml"><mo stretchy="false" id="S5.p1.4.m4.2.3.2.1" xref="S5.p1.4.m4.2.3.1.cmml">(</mo><mi id="S5.p1.4.m4.1.1" xref="S5.p1.4.m4.1.1.cmml">x</mi><mo id="S5.p1.4.m4.2.3.2.2" xref="S5.p1.4.m4.2.3.1.cmml">,</mo><mi id="S5.p1.4.m4.2.2" xref="S5.p1.4.m4.2.2.cmml">z</mi><mo stretchy="false" id="S5.p1.4.m4.2.3.2.3" xref="S5.p1.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.4.m4.2b"><interval closure="open" id="S5.p1.4.m4.2.3.1.cmml" xref="S5.p1.4.m4.2.3.2"><ci id="S5.p1.4.m4.1.1.cmml" xref="S5.p1.4.m4.1.1">ğ‘¥</ci><ci id="S5.p1.4.m4.2.2.cmml" xref="S5.p1.4.m4.2.2">ğ‘§</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.4.m4.2c">(x,z)</annotation></semantics></math>, resembling the depth inference from the side view of the rodent. In the third scenario also predicts the <math id="S5.p1.5.m5.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S5.p1.5.m5.1a"><mi id="S5.p1.5.m5.1.1" xref="S5.p1.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S5.p1.5.m5.1b"><ci id="S5.p1.5.m5.1.1.cmml" xref="S5.p1.5.m5.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.5.m5.1c">y</annotation></semantics></math> coordinate from <math id="S5.p1.6.m6.2" class="ltx_Math" alttext="(x,z)" display="inline"><semantics id="S5.p1.6.m6.2a"><mrow id="S5.p1.6.m6.2.3.2" xref="S5.p1.6.m6.2.3.1.cmml"><mo stretchy="false" id="S5.p1.6.m6.2.3.2.1" xref="S5.p1.6.m6.2.3.1.cmml">(</mo><mi id="S5.p1.6.m6.1.1" xref="S5.p1.6.m6.1.1.cmml">x</mi><mo id="S5.p1.6.m6.2.3.2.2" xref="S5.p1.6.m6.2.3.1.cmml">,</mo><mi id="S5.p1.6.m6.2.2" xref="S5.p1.6.m6.2.2.cmml">z</mi><mo stretchy="false" id="S5.p1.6.m6.2.3.2.3" xref="S5.p1.6.m6.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.6.m6.2b"><interval closure="open" id="S5.p1.6.m6.2.3.1.cmml" xref="S5.p1.6.m6.2.3.2"><ci id="S5.p1.6.m6.1.1.cmml" xref="S5.p1.6.m6.1.1">ğ‘¥</ci><ci id="S5.p1.6.m6.2.2.cmml" xref="S5.p1.6.m6.2.2">ğ‘§</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.6.m6.2c">(x,z)</annotation></semantics></math> tuples, however the perspective is rotated by <math id="S5.p1.7.m7.1" class="ltx_Math" alttext="45\degree" display="inline"><semantics id="S5.p1.7.m7.1a"><mrow id="S5.p1.7.m7.1.1" xref="S5.p1.7.m7.1.1.cmml"><mn id="S5.p1.7.m7.1.1.2" xref="S5.p1.7.m7.1.1.2.cmml">45</mn><mo lspace="0em" rspace="0em" id="S5.p1.7.m7.1.1.1" xref="S5.p1.7.m7.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S5.p1.7.m7.1.1.3" xref="S5.p1.7.m7.1.1.3.cmml">Â°</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.7.m7.1b"><apply id="S5.p1.7.m7.1.1.cmml" xref="S5.p1.7.m7.1.1"><times id="S5.p1.7.m7.1.1.1.cmml" xref="S5.p1.7.m7.1.1.1"></times><cn type="integer" id="S5.p1.7.m7.1.1.2.cmml" xref="S5.p1.7.m7.1.1.2">45</cn><ci id="S5.p1.7.m7.1.1.3.cmml" xref="S5.p1.7.m7.1.1.3">Â°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.7.m7.1c">45\degree</annotation></semantics></math> along the <math id="S5.p1.8.m8.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.p1.8.m8.1a"><mi id="S5.p1.8.m8.1.1" xref="S5.p1.8.m8.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.p1.8.m8.1b"><ci id="S5.p1.8.m8.1.1.cmml" xref="S5.p1.8.m8.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.8.m8.1c">x</annotation></semantics></math>-axis, resembling a diagonal top-down view on the animal as it could be obtained under realistic experimental conditions.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Before using the data we shuffled the data in order to improve the generalization over the test set.
For the <span id="S5.p2.1.1" class="ltx_text ltx_font_italic">Temporal Convolution model</span>, we initially divided the data into chunks of the temporal window size and re-arranged them in a random order.
For the <span id="S5.p2.1.2" class="ltx_text ltx_font_italic">Linear model</span>, we initially shuffled the data at every time step.
Each model evaluation was repeated for 10 times, where each run was started with random initialization weights and a new random test/train split (<math id="S5.p2.1.m1.1" class="ltx_Math" alttext="20\%/80\%" display="inline"><semantics id="S5.p2.1.m1.1a"><mrow id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mrow id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml"><mn id="S5.p2.1.m1.1.1.2.2" xref="S5.p2.1.m1.1.1.2.2.cmml">20</mn><mo id="S5.p2.1.m1.1.1.2.1" xref="S5.p2.1.m1.1.1.2.1.cmml">%</mo></mrow><mo id="S5.p2.1.m1.1.1.1" xref="S5.p2.1.m1.1.1.1.cmml">/</mo><mrow id="S5.p2.1.m1.1.1.3" xref="S5.p2.1.m1.1.1.3.cmml"><mn id="S5.p2.1.m1.1.1.3.2" xref="S5.p2.1.m1.1.1.3.2.cmml">80</mn><mo id="S5.p2.1.m1.1.1.3.1" xref="S5.p2.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><divide id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1.1"></divide><apply id="S5.p2.1.m1.1.1.2.cmml" xref="S5.p2.1.m1.1.1.2"><csymbol cd="latexml" id="S5.p2.1.m1.1.1.2.1.cmml" xref="S5.p2.1.m1.1.1.2.1">percent</csymbol><cn type="integer" id="S5.p2.1.m1.1.1.2.2.cmml" xref="S5.p2.1.m1.1.1.2.2">20</cn></apply><apply id="S5.p2.1.m1.1.1.3.cmml" xref="S5.p2.1.m1.1.1.3"><csymbol cd="latexml" id="S5.p2.1.m1.1.1.3.1.cmml" xref="S5.p2.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S5.p2.1.m1.1.1.3.2.cmml" xref="S5.p2.1.m1.1.1.3.2">80</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">20\%/80\%</annotation></semantics></math>).
The evaluation criteria for each model and setting was the final test error that was obtained after 150 training epochs.
Both models were evaluated at the same input data for the MPJPE loss. In both models, the initial learning rate was 0.001 and the decay rate of per epoch was 0.95.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2106.12993/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="396" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Mean test error of the <span id="S5.F6.2.1" class="ltx_text ltx_font_italic">Temporal Convolutional Model</span> obtained at increasing temporal window size. The error bars show the loss standard deviation of the 10 shuffled evaluation runs.</figcaption>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">For the <span id="S5.p3.1.1" class="ltx_text ltx_font_italic">Temporal Convolutional Model</span> we first determined the optimal temporal window in terms of the test loss.
As shown in Figure <a href="#S5.F6" title="Figure 6 â€£ 5 Results â€£ Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the lowest test loss was obtained for window setting 135, which is 2.7 seconds in physical time.
Note that the number of trainable parameters was 6.75 million for the model at temporal window size 15 and 16.8 million for the model at a temporal window size of 243.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<div id="S5.T1.21" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:230.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(67.1pt,-35.7pt) scale(1.4478065143274,1.4478065143274) ;">
<table id="S5.T1.21.21" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.21.21.22.1" class="ltx_tr">
<th id="S5.T1.21.21.22.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:4.30554pt;">
<span id="S5.T1.21.21.22.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.21.21.22.1.1.1.1" class="ltx_p" style="width:86.7pt;">Scenario</span>
</span>
</th>
<th id="S5.T1.21.21.22.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:4.30554pt;">
<span id="S5.T1.21.21.22.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.21.21.22.1.2.1.1" class="ltx_p" style="width:86.7pt;">Linear</span>
</span>
</th>
<th id="S5.T1.21.21.22.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:4.30554pt;">
<span id="S5.T1.21.21.22.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.21.21.22.1.3.1.1" class="ltx_p" style="width:86.7pt;">Temporal Convolutions</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.5.5.5" class="ltx_tr">
<td id="S5.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.1.1.1.1" class="ltx_p" style="width:86.7pt;"><math id="S5.T1.1.1.1.1.1.1.m1.2" class="ltx_Math" alttext="(x,y)\rightarrow z" display="inline"><semantics id="S5.T1.1.1.1.1.1.1.m1.2a"><mrow id="S5.T1.1.1.1.1.1.1.m1.2.3" xref="S5.T1.1.1.1.1.1.1.m1.2.3.cmml"><mrow id="S5.T1.1.1.1.1.1.1.m1.2.3.2.2" xref="S5.T1.1.1.1.1.1.1.m1.2.3.2.1.cmml"><mo stretchy="false" id="S5.T1.1.1.1.1.1.1.m1.2.3.2.2.1" xref="S5.T1.1.1.1.1.1.1.m1.2.3.2.1.cmml">(</mo><mi id="S5.T1.1.1.1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.1.1.1.m1.1.1.cmml">x</mi><mo id="S5.T1.1.1.1.1.1.1.m1.2.3.2.2.2" xref="S5.T1.1.1.1.1.1.1.m1.2.3.2.1.cmml">,</mo><mi id="S5.T1.1.1.1.1.1.1.m1.2.2" xref="S5.T1.1.1.1.1.1.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S5.T1.1.1.1.1.1.1.m1.2.3.2.2.3" xref="S5.T1.1.1.1.1.1.1.m1.2.3.2.1.cmml">)</mo></mrow><mo stretchy="false" id="S5.T1.1.1.1.1.1.1.m1.2.3.1" xref="S5.T1.1.1.1.1.1.1.m1.2.3.1.cmml">â†’</mo><mi id="S5.T1.1.1.1.1.1.1.m1.2.3.3" xref="S5.T1.1.1.1.1.1.1.m1.2.3.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.1.1.m1.2b"><apply id="S5.T1.1.1.1.1.1.1.m1.2.3.cmml" xref="S5.T1.1.1.1.1.1.1.m1.2.3"><ci id="S5.T1.1.1.1.1.1.1.m1.2.3.1.cmml" xref="S5.T1.1.1.1.1.1.1.m1.2.3.1">â†’</ci><interval closure="open" id="S5.T1.1.1.1.1.1.1.m1.2.3.2.1.cmml" xref="S5.T1.1.1.1.1.1.1.m1.2.3.2.2"><ci id="S5.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.1.1.m1.1.1">ğ‘¥</ci><ci id="S5.T1.1.1.1.1.1.1.m1.2.2.cmml" xref="S5.T1.1.1.1.1.1.1.m1.2.2">ğ‘¦</ci></interval><ci id="S5.T1.1.1.1.1.1.1.m1.2.3.3.cmml" xref="S5.T1.1.1.1.1.1.1.m1.2.3.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.1.1.m1.2c">(x,y)\rightarrow z</annotation></semantics></math></span>
</span>
</td>
<td id="S5.T1.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.3.3.3.3.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.3.3.3.2.2" class="ltx_p" style="width:86.7pt;">
<span id="S5.T1.3.3.3.3.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.2.2.2.2.1.1.1.1" class="ltx_p">Train: <math id="S5.T1.2.2.2.2.1.1.1.1.m1.1" class="ltx_Math" alttext="4.17\pm 1.519" display="inline"><semantics id="S5.T1.2.2.2.2.1.1.1.1.m1.1a"><mrow id="S5.T1.2.2.2.2.1.1.1.1.m1.1.1" xref="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.cmml"><mn id="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.2" xref="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.2.cmml">4.17</mn><mo id="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.1" xref="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.3" xref="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.3.cmml">1.519</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.1.1.1.1.m1.1b"><apply id="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.cmml" xref="S5.T1.2.2.2.2.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.2">4.17</cn><cn type="float" id="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.2.2.2.2.1.1.1.1.m1.1.1.3">1.519</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.1.1.1.1.m1.1c">4.17\pm 1.519</annotation></semantics></math></span>
<span id="S5.T1.3.3.3.3.2.2.2.2" class="ltx_p">Test: <math id="S5.T1.3.3.3.3.2.2.2.2.m1.1" class="ltx_Math" alttext="1.47\pm 0.031" display="inline"><semantics id="S5.T1.3.3.3.3.2.2.2.2.m1.1a"><mrow id="S5.T1.3.3.3.3.2.2.2.2.m1.1.1" xref="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.cmml"><mn id="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.2" xref="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.2.cmml">1.47</mn><mo id="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.1" xref="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.3" xref="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.3.cmml">0.031</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.3.2.2.2.2.m1.1b"><apply id="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.cmml" xref="S5.T1.3.3.3.3.2.2.2.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.1.cmml" xref="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.2.cmml" xref="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.2">1.47</cn><cn type="float" id="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.3.cmml" xref="S5.T1.3.3.3.3.2.2.2.2.m1.1.1.3">0.031</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.3.2.2.2.2.m1.1c">1.47\pm 0.031</annotation></semantics></math></span>
</span></span>
</span>
</td>
<td id="S5.T1.5.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.5.5.5.5.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.5.5.5.5.2.2" class="ltx_p" style="width:86.7pt;">
<span id="S5.T1.5.5.5.5.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.4.4.4.1.1.1.1" class="ltx_p">Train: <math id="S5.T1.4.4.4.4.1.1.1.1.m1.1" class="ltx_Math" alttext="0.72\pm 0.008" display="inline"><semantics id="S5.T1.4.4.4.4.1.1.1.1.m1.1a"><mrow id="S5.T1.4.4.4.4.1.1.1.1.m1.1.1" xref="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.cmml"><mn id="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.2" xref="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.2.cmml">0.72</mn><mo id="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.1" xref="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.3" xref="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.3.cmml">0.008</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.4.1.1.1.1.m1.1b"><apply id="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.cmml" xref="S5.T1.4.4.4.4.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.2">0.72</cn><cn type="float" id="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.4.4.4.4.1.1.1.1.m1.1.1.3">0.008</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.4.1.1.1.1.m1.1c">0.72\pm 0.008</annotation></semantics></math></span>
<span id="S5.T1.5.5.5.5.2.2.2.2" class="ltx_p">Test: <math id="S5.T1.5.5.5.5.2.2.2.2.m1.1" class="ltx_Math" alttext="0.46\pm 0.006" display="inline"><semantics id="S5.T1.5.5.5.5.2.2.2.2.m1.1a"><mrow id="S5.T1.5.5.5.5.2.2.2.2.m1.1.1" xref="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.cmml"><mn id="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.2" xref="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.2.cmml">0.46</mn><mo id="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.1" xref="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.3" xref="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.3.cmml">0.006</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.5.5.2.2.2.2.m1.1b"><apply id="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.cmml" xref="S5.T1.5.5.5.5.2.2.2.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.1.cmml" xref="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.2.cmml" xref="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.2">0.46</cn><cn type="float" id="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.3.cmml" xref="S5.T1.5.5.5.5.2.2.2.2.m1.1.1.3">0.006</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.5.5.2.2.2.2.m1.1c">0.46\pm 0.006</annotation></semantics></math></span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.10.10.10" class="ltx_tr">
<td id="S5.T1.6.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.6.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.6.6.6.1.1.1" class="ltx_p" style="width:86.7pt;"><math id="S5.T1.6.6.6.1.1.1.m1.2" class="ltx_Math" alttext="(x,z)\rightarrow y" display="inline"><semantics id="S5.T1.6.6.6.1.1.1.m1.2a"><mrow id="S5.T1.6.6.6.1.1.1.m1.2.3" xref="S5.T1.6.6.6.1.1.1.m1.2.3.cmml"><mrow id="S5.T1.6.6.6.1.1.1.m1.2.3.2.2" xref="S5.T1.6.6.6.1.1.1.m1.2.3.2.1.cmml"><mo stretchy="false" id="S5.T1.6.6.6.1.1.1.m1.2.3.2.2.1" xref="S5.T1.6.6.6.1.1.1.m1.2.3.2.1.cmml">(</mo><mi id="S5.T1.6.6.6.1.1.1.m1.1.1" xref="S5.T1.6.6.6.1.1.1.m1.1.1.cmml">x</mi><mo id="S5.T1.6.6.6.1.1.1.m1.2.3.2.2.2" xref="S5.T1.6.6.6.1.1.1.m1.2.3.2.1.cmml">,</mo><mi id="S5.T1.6.6.6.1.1.1.m1.2.2" xref="S5.T1.6.6.6.1.1.1.m1.2.2.cmml">z</mi><mo stretchy="false" id="S5.T1.6.6.6.1.1.1.m1.2.3.2.2.3" xref="S5.T1.6.6.6.1.1.1.m1.2.3.2.1.cmml">)</mo></mrow><mo stretchy="false" id="S5.T1.6.6.6.1.1.1.m1.2.3.1" xref="S5.T1.6.6.6.1.1.1.m1.2.3.1.cmml">â†’</mo><mi id="S5.T1.6.6.6.1.1.1.m1.2.3.3" xref="S5.T1.6.6.6.1.1.1.m1.2.3.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.6.1.1.1.m1.2b"><apply id="S5.T1.6.6.6.1.1.1.m1.2.3.cmml" xref="S5.T1.6.6.6.1.1.1.m1.2.3"><ci id="S5.T1.6.6.6.1.1.1.m1.2.3.1.cmml" xref="S5.T1.6.6.6.1.1.1.m1.2.3.1">â†’</ci><interval closure="open" id="S5.T1.6.6.6.1.1.1.m1.2.3.2.1.cmml" xref="S5.T1.6.6.6.1.1.1.m1.2.3.2.2"><ci id="S5.T1.6.6.6.1.1.1.m1.1.1.cmml" xref="S5.T1.6.6.6.1.1.1.m1.1.1">ğ‘¥</ci><ci id="S5.T1.6.6.6.1.1.1.m1.2.2.cmml" xref="S5.T1.6.6.6.1.1.1.m1.2.2">ğ‘§</ci></interval><ci id="S5.T1.6.6.6.1.1.1.m1.2.3.3.cmml" xref="S5.T1.6.6.6.1.1.1.m1.2.3.3">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.6.1.1.1.m1.2c">(x,z)\rightarrow y</annotation></semantics></math></span>
</span>
</td>
<td id="S5.T1.8.8.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.8.8.8.3.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.8.8.8.3.2.2" class="ltx_p" style="width:86.7pt;">
<span id="S5.T1.8.8.8.3.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.7.7.7.2.1.1.1.1" class="ltx_p">Train: <math id="S5.T1.7.7.7.2.1.1.1.1.m1.1" class="ltx_Math" alttext="3.40\pm 0.482" display="inline"><semantics id="S5.T1.7.7.7.2.1.1.1.1.m1.1a"><mrow id="S5.T1.7.7.7.2.1.1.1.1.m1.1.1" xref="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.cmml"><mn id="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.2" xref="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.2.cmml">3.40</mn><mo id="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.1" xref="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.3" xref="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.3.cmml">0.482</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.7.2.1.1.1.1.m1.1b"><apply id="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.cmml" xref="S5.T1.7.7.7.2.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.2">3.40</cn><cn type="float" id="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.7.7.7.2.1.1.1.1.m1.1.1.3">0.482</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.7.2.1.1.1.1.m1.1c">3.40\pm 0.482</annotation></semantics></math></span>
<span id="S5.T1.8.8.8.3.2.2.2.2" class="ltx_p">Test: <math id="S5.T1.8.8.8.3.2.2.2.2.m1.1" class="ltx_Math" alttext="1.34\pm 0.064" display="inline"><semantics id="S5.T1.8.8.8.3.2.2.2.2.m1.1a"><mrow id="S5.T1.8.8.8.3.2.2.2.2.m1.1.1" xref="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.cmml"><mn id="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.2" xref="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.2.cmml">1.34</mn><mo id="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.1" xref="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.3" xref="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.3.cmml">0.064</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.8.3.2.2.2.2.m1.1b"><apply id="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.cmml" xref="S5.T1.8.8.8.3.2.2.2.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.1.cmml" xref="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.2.cmml" xref="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.2">1.34</cn><cn type="float" id="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.3.cmml" xref="S5.T1.8.8.8.3.2.2.2.2.m1.1.1.3">0.064</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.8.3.2.2.2.2.m1.1c">1.34\pm 0.064</annotation></semantics></math></span>
</span></span>
</span>
</td>
<td id="S5.T1.10.10.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.10.10.10.5.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.10.10.10.5.2.2" class="ltx_p" style="width:86.7pt;">
<span id="S5.T1.10.10.10.5.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.9.9.9.4.1.1.1.1" class="ltx_p">Train: <math id="S5.T1.9.9.9.4.1.1.1.1.m1.1" class="ltx_Math" alttext="1.01\pm 0.005" display="inline"><semantics id="S5.T1.9.9.9.4.1.1.1.1.m1.1a"><mrow id="S5.T1.9.9.9.4.1.1.1.1.m1.1.1" xref="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.cmml"><mn id="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.2" xref="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.2.cmml">1.01</mn><mo id="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.1" xref="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.3" xref="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.3.cmml">0.005</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.9.4.1.1.1.1.m1.1b"><apply id="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.cmml" xref="S5.T1.9.9.9.4.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.2">1.01</cn><cn type="float" id="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.9.9.9.4.1.1.1.1.m1.1.1.3">0.005</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.9.4.1.1.1.1.m1.1c">1.01\pm 0.005</annotation></semantics></math></span>
<span id="S5.T1.10.10.10.5.2.2.2.2" class="ltx_p">Test: <math id="S5.T1.10.10.10.5.2.2.2.2.m1.1" class="ltx_Math" alttext="0.89\pm 0.007" display="inline"><semantics id="S5.T1.10.10.10.5.2.2.2.2.m1.1a"><mrow id="S5.T1.10.10.10.5.2.2.2.2.m1.1.1" xref="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.cmml"><mn id="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.2" xref="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.2.cmml">0.89</mn><mo id="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.1" xref="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.3" xref="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.3.cmml">0.007</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.10.10.10.5.2.2.2.2.m1.1b"><apply id="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.cmml" xref="S5.T1.10.10.10.5.2.2.2.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.1.cmml" xref="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.2.cmml" xref="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.2">0.89</cn><cn type="float" id="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.3.cmml" xref="S5.T1.10.10.10.5.2.2.2.2.m1.1.1.3">0.007</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.10.10.5.2.2.2.2.m1.1c">0.89\pm 0.007</annotation></semantics></math></span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.16.16.16" class="ltx_tr">
<td id="S5.T1.12.12.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.12.12.12.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.12.12.12.2.2.2" class="ltx_p" style="width:86.7pt;">
<span id="S5.T1.12.12.12.2.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.11.11.1.1.1.1.1" class="ltx_p"><math id="S5.T1.11.11.11.1.1.1.1.1.m1.2" class="ltx_Math" alttext="(x,z)\rightarrow y" display="inline"><semantics id="S5.T1.11.11.11.1.1.1.1.1.m1.2a"><mrow id="S5.T1.11.11.11.1.1.1.1.1.m1.2.3" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.cmml"><mrow id="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.2.2" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.2.1.cmml"><mo stretchy="false" id="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.2.2.1" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.2.1.cmml">(</mo><mi id="S5.T1.11.11.11.1.1.1.1.1.m1.1.1" xref="S5.T1.11.11.11.1.1.1.1.1.m1.1.1.cmml">x</mi><mo id="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.2.2.2" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.2.1.cmml">,</mo><mi id="S5.T1.11.11.11.1.1.1.1.1.m1.2.2" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.2.cmml">z</mi><mo stretchy="false" id="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.2.2.3" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.2.1.cmml">)</mo></mrow><mo stretchy="false" id="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.1" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.1.cmml">â†’</mo><mi id="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.3" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.11.11.11.1.1.1.1.1.m1.2b"><apply id="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.cmml" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.3"><ci id="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.1.cmml" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.1">â†’</ci><interval closure="open" id="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.2.1.cmml" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.2.2"><ci id="S5.T1.11.11.11.1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.11.11.11.1.1.1.1.1.m1.1.1">ğ‘¥</ci><ci id="S5.T1.11.11.11.1.1.1.1.1.m1.2.2.cmml" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.2">ğ‘§</ci></interval><ci id="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.3.cmml" xref="S5.T1.11.11.11.1.1.1.1.1.m1.2.3.3">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.11.11.11.1.1.1.1.1.m1.2c">(x,z)\rightarrow y</annotation></semantics></math></span>
<span id="S5.T1.12.12.12.2.2.2.2.2" class="ltx_p"><math id="S5.T1.12.12.12.2.2.2.2.2.m1.1" class="ltx_Math" alttext="45\degree" display="inline"><semantics id="S5.T1.12.12.12.2.2.2.2.2.m1.1a"><mrow id="S5.T1.12.12.12.2.2.2.2.2.m1.1.1" xref="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.cmml"><mn id="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.2" xref="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.2.cmml">45</mn><mo lspace="0em" rspace="0em" id="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.1" xref="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.3" xref="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.3.cmml">Â°</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.12.12.12.2.2.2.2.2.m1.1b"><apply id="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.cmml" xref="S5.T1.12.12.12.2.2.2.2.2.m1.1.1"><times id="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.1.cmml" xref="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.1"></times><cn type="integer" id="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.2.cmml" xref="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.2">45</cn><ci id="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.3.cmml" xref="S5.T1.12.12.12.2.2.2.2.2.m1.1.1.3">Â°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.12.12.12.2.2.2.2.2.m1.1c">45\degree</annotation></semantics></math> rotation</span>
</span></span>
</span>
</td>
<td id="S5.T1.14.14.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.14.14.14.4.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.14.14.14.4.2.2" class="ltx_p" style="width:86.7pt;">
<span id="S5.T1.14.14.14.4.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.13.13.13.3.1.1.1.1" class="ltx_p">Train: <math id="S5.T1.13.13.13.3.1.1.1.1.m1.1" class="ltx_Math" alttext="3.17\pm 0.519" display="inline"><semantics id="S5.T1.13.13.13.3.1.1.1.1.m1.1a"><mrow id="S5.T1.13.13.13.3.1.1.1.1.m1.1.1" xref="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.cmml"><mn id="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.2" xref="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.2.cmml">3.17</mn><mo id="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.1" xref="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.3" xref="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.3.cmml">0.519</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.13.13.13.3.1.1.1.1.m1.1b"><apply id="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.cmml" xref="S5.T1.13.13.13.3.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.2">3.17</cn><cn type="float" id="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.13.13.13.3.1.1.1.1.m1.1.1.3">0.519</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.13.13.13.3.1.1.1.1.m1.1c">3.17\pm 0.519</annotation></semantics></math></span>
<span id="S5.T1.14.14.14.4.2.2.2.2" class="ltx_p">Test: <math id="S5.T1.14.14.14.4.2.2.2.2.m1.1" class="ltx_Math" alttext="0.99\pm 0.037" display="inline"><semantics id="S5.T1.14.14.14.4.2.2.2.2.m1.1a"><mrow id="S5.T1.14.14.14.4.2.2.2.2.m1.1.1" xref="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.cmml"><mn id="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.2" xref="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.2.cmml">0.99</mn><mo id="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.1" xref="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.3" xref="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.3.cmml">0.037</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.14.14.14.4.2.2.2.2.m1.1b"><apply id="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.cmml" xref="S5.T1.14.14.14.4.2.2.2.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.1.cmml" xref="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.2.cmml" xref="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.2">0.99</cn><cn type="float" id="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.3.cmml" xref="S5.T1.14.14.14.4.2.2.2.2.m1.1.1.3">0.037</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.14.14.14.4.2.2.2.2.m1.1c">0.99\pm 0.037</annotation></semantics></math></span>
</span></span>
</span>
</td>
<td id="S5.T1.16.16.16.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.16.16.16.6.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.16.16.16.6.2.2" class="ltx_p" style="width:86.7pt;">
<span id="S5.T1.16.16.16.6.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.15.15.15.5.1.1.1.1" class="ltx_p">Train: <math id="S5.T1.15.15.15.5.1.1.1.1.m1.1" class="ltx_Math" alttext="0.61\pm 0.006" display="inline"><semantics id="S5.T1.15.15.15.5.1.1.1.1.m1.1a"><mrow id="S5.T1.15.15.15.5.1.1.1.1.m1.1.1" xref="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.cmml"><mn id="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.2" xref="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.2.cmml">0.61</mn><mo id="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.1" xref="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.3" xref="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.3.cmml">0.006</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.15.15.15.5.1.1.1.1.m1.1b"><apply id="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.cmml" xref="S5.T1.15.15.15.5.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.2">0.61</cn><cn type="float" id="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.15.15.15.5.1.1.1.1.m1.1.1.3">0.006</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.15.15.15.5.1.1.1.1.m1.1c">0.61\pm 0.006</annotation></semantics></math></span>
<span id="S5.T1.16.16.16.6.2.2.2.2" class="ltx_p">Test: <math id="S5.T1.16.16.16.6.2.2.2.2.m1.1" class="ltx_Math" alttext="0.45\pm 0.003" display="inline"><semantics id="S5.T1.16.16.16.6.2.2.2.2.m1.1a"><mrow id="S5.T1.16.16.16.6.2.2.2.2.m1.1.1" xref="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.cmml"><mn id="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.2" xref="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.2.cmml">0.45</mn><mo id="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.1" xref="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.3" xref="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.3.cmml">0.003</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.16.16.16.6.2.2.2.2.m1.1b"><apply id="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.cmml" xref="S5.T1.16.16.16.6.2.2.2.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.1.cmml" xref="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.2.cmml" xref="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.2">0.45</cn><cn type="float" id="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.3.cmml" xref="S5.T1.16.16.16.6.2.2.2.2.m1.1.1.3">0.003</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.16.16.16.6.2.2.2.2.m1.1c">0.45\pm 0.003</annotation></semantics></math></span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.21.21.21" class="ltx_tr">
<td id="S5.T1.17.17.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.17.17.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.17.17.17.1.1.1" class="ltx_p" style="width:86.7pt;"><math id="S5.T1.17.17.17.1.1.1.m1.2" class="ltx_Math" alttext="(z,y)\rightarrow x" display="inline"><semantics id="S5.T1.17.17.17.1.1.1.m1.2a"><mrow id="S5.T1.17.17.17.1.1.1.m1.2.3" xref="S5.T1.17.17.17.1.1.1.m1.2.3.cmml"><mrow id="S5.T1.17.17.17.1.1.1.m1.2.3.2.2" xref="S5.T1.17.17.17.1.1.1.m1.2.3.2.1.cmml"><mo stretchy="false" id="S5.T1.17.17.17.1.1.1.m1.2.3.2.2.1" xref="S5.T1.17.17.17.1.1.1.m1.2.3.2.1.cmml">(</mo><mi id="S5.T1.17.17.17.1.1.1.m1.1.1" xref="S5.T1.17.17.17.1.1.1.m1.1.1.cmml">z</mi><mo id="S5.T1.17.17.17.1.1.1.m1.2.3.2.2.2" xref="S5.T1.17.17.17.1.1.1.m1.2.3.2.1.cmml">,</mo><mi id="S5.T1.17.17.17.1.1.1.m1.2.2" xref="S5.T1.17.17.17.1.1.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S5.T1.17.17.17.1.1.1.m1.2.3.2.2.3" xref="S5.T1.17.17.17.1.1.1.m1.2.3.2.1.cmml">)</mo></mrow><mo stretchy="false" id="S5.T1.17.17.17.1.1.1.m1.2.3.1" xref="S5.T1.17.17.17.1.1.1.m1.2.3.1.cmml">â†’</mo><mi id="S5.T1.17.17.17.1.1.1.m1.2.3.3" xref="S5.T1.17.17.17.1.1.1.m1.2.3.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.17.17.17.1.1.1.m1.2b"><apply id="S5.T1.17.17.17.1.1.1.m1.2.3.cmml" xref="S5.T1.17.17.17.1.1.1.m1.2.3"><ci id="S5.T1.17.17.17.1.1.1.m1.2.3.1.cmml" xref="S5.T1.17.17.17.1.1.1.m1.2.3.1">â†’</ci><interval closure="open" id="S5.T1.17.17.17.1.1.1.m1.2.3.2.1.cmml" xref="S5.T1.17.17.17.1.1.1.m1.2.3.2.2"><ci id="S5.T1.17.17.17.1.1.1.m1.1.1.cmml" xref="S5.T1.17.17.17.1.1.1.m1.1.1">ğ‘§</ci><ci id="S5.T1.17.17.17.1.1.1.m1.2.2.cmml" xref="S5.T1.17.17.17.1.1.1.m1.2.2">ğ‘¦</ci></interval><ci id="S5.T1.17.17.17.1.1.1.m1.2.3.3.cmml" xref="S5.T1.17.17.17.1.1.1.m1.2.3.3">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.17.17.17.1.1.1.m1.2c">(z,y)\rightarrow x</annotation></semantics></math></span>
</span>
</td>
<td id="S5.T1.19.19.19.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.19.19.19.3.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.19.19.19.3.2.2" class="ltx_p" style="width:86.7pt;">
<span id="S5.T1.19.19.19.3.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.18.18.18.2.1.1.1.1" class="ltx_p">Train: <math id="S5.T1.18.18.18.2.1.1.1.1.m1.1" class="ltx_Math" alttext="3.18\pm 0.337" display="inline"><semantics id="S5.T1.18.18.18.2.1.1.1.1.m1.1a"><mrow id="S5.T1.18.18.18.2.1.1.1.1.m1.1.1" xref="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.cmml"><mn id="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.2" xref="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.2.cmml">3.18</mn><mo id="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.1" xref="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.3" xref="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.3.cmml">0.337</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.18.18.18.2.1.1.1.1.m1.1b"><apply id="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.cmml" xref="S5.T1.18.18.18.2.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.2">3.18</cn><cn type="float" id="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.18.18.18.2.1.1.1.1.m1.1.1.3">0.337</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.18.18.18.2.1.1.1.1.m1.1c">3.18\pm 0.337</annotation></semantics></math></span>
<span id="S5.T1.19.19.19.3.2.2.2.2" class="ltx_p">Test: <math id="S5.T1.19.19.19.3.2.2.2.2.m1.1" class="ltx_Math" alttext="1.21\pm 0.025" display="inline"><semantics id="S5.T1.19.19.19.3.2.2.2.2.m1.1a"><mrow id="S5.T1.19.19.19.3.2.2.2.2.m1.1.1" xref="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.cmml"><mn id="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.2" xref="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.2.cmml">1.21</mn><mo id="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.1" xref="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.3" xref="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.3.cmml">0.025</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.19.19.19.3.2.2.2.2.m1.1b"><apply id="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.cmml" xref="S5.T1.19.19.19.3.2.2.2.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.1.cmml" xref="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.2.cmml" xref="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.2">1.21</cn><cn type="float" id="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.3.cmml" xref="S5.T1.19.19.19.3.2.2.2.2.m1.1.1.3">0.025</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.19.19.19.3.2.2.2.2.m1.1c">1.21\pm 0.025</annotation></semantics></math></span>
</span></span>
</span>
</td>
<td id="S5.T1.21.21.21.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S5.T1.21.21.21.5.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.21.21.21.5.2.2" class="ltx_p" style="width:86.7pt;">
<span id="S5.T1.21.21.21.5.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.20.20.20.4.1.1.1.1" class="ltx_p">Train: <math id="S5.T1.20.20.20.4.1.1.1.1.m1.1" class="ltx_Math" alttext="0.69\pm 0.003" display="inline"><semantics id="S5.T1.20.20.20.4.1.1.1.1.m1.1a"><mrow id="S5.T1.20.20.20.4.1.1.1.1.m1.1.1" xref="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.cmml"><mn id="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.2" xref="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.2.cmml">0.69</mn><mo id="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.1" xref="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.3" xref="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.3.cmml">0.003</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.20.20.20.4.1.1.1.1.m1.1b"><apply id="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.cmml" xref="S5.T1.20.20.20.4.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.2">0.69</cn><cn type="float" id="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.20.20.20.4.1.1.1.1.m1.1.1.3">0.003</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.20.20.20.4.1.1.1.1.m1.1c">0.69\pm 0.003</annotation></semantics></math></span>
<span id="S5.T1.21.21.21.5.2.2.2.2" class="ltx_p">Test: <math id="S5.T1.21.21.21.5.2.2.2.2.m1.1" class="ltx_Math" alttext="0.53\pm 0.004" display="inline"><semantics id="S5.T1.21.21.21.5.2.2.2.2.m1.1a"><mrow id="S5.T1.21.21.21.5.2.2.2.2.m1.1.1" xref="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.cmml"><mn id="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.2" xref="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.2.cmml">0.53</mn><mo id="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.1" xref="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.1.cmml">Â±</mo><mn id="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.3" xref="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.3.cmml">0.004</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.21.21.21.5.2.2.2.2.m1.1b"><apply id="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.cmml" xref="S5.T1.21.21.21.5.2.2.2.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.1.cmml" xref="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.2.cmml" xref="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.2">0.53</cn><cn type="float" id="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.3.cmml" xref="S5.T1.21.21.21.5.2.2.2.2.m1.1.1.3">0.004</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.21.21.21.5.2.2.2.2.m1.1c">0.53\pm 0.004</annotation></semantics></math></span>
</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance evaluation of the <span id="S5.T1.24.1" class="ltx_text ltx_font_italic">Linear model</span> and <span id="S5.T1.25.2" class="ltx_text ltx_font_italic">Temporal Convolutional model</span> for different prediction directions.</figcaption>
</figure>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.3" class="ltx_p">Finaly, the results for predictions from different perspectives for both models are in Table <a href="#S5.T1" title="Table 1 â€£ 5 Results â€£ Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
In this table, we evaluated the <span id="S5.p4.3.1" class="ltx_text ltx_font_italic">Temporal Convolutional model</span> at the optimal window size of 135 timesteps.
We found that under all perspectives, the <span id="S5.p4.3.2" class="ltx_text ltx_font_italic">Temporal Convolutional model</span> performed better than the <span id="S5.p4.3.3" class="ltx_text ltx_font_italic">Linear model</span>.
Moreoever, we found that both models achieve the lowest test loss at the prediction of <math id="S5.p4.1.m1.2" class="ltx_Math" alttext="(x,z)\rightarrow y" display="inline"><semantics id="S5.p4.1.m1.2a"><mrow id="S5.p4.1.m1.2.3" xref="S5.p4.1.m1.2.3.cmml"><mrow id="S5.p4.1.m1.2.3.2.2" xref="S5.p4.1.m1.2.3.2.1.cmml"><mo stretchy="false" id="S5.p4.1.m1.2.3.2.2.1" xref="S5.p4.1.m1.2.3.2.1.cmml">(</mo><mi id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">x</mi><mo id="S5.p4.1.m1.2.3.2.2.2" xref="S5.p4.1.m1.2.3.2.1.cmml">,</mo><mi id="S5.p4.1.m1.2.2" xref="S5.p4.1.m1.2.2.cmml">z</mi><mo stretchy="false" id="S5.p4.1.m1.2.3.2.2.3" xref="S5.p4.1.m1.2.3.2.1.cmml">)</mo></mrow><mo stretchy="false" id="S5.p4.1.m1.2.3.1" xref="S5.p4.1.m1.2.3.1.cmml">â†’</mo><mi id="S5.p4.1.m1.2.3.3" xref="S5.p4.1.m1.2.3.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.2b"><apply id="S5.p4.1.m1.2.3.cmml" xref="S5.p4.1.m1.2.3"><ci id="S5.p4.1.m1.2.3.1.cmml" xref="S5.p4.1.m1.2.3.1">â†’</ci><interval closure="open" id="S5.p4.1.m1.2.3.2.1.cmml" xref="S5.p4.1.m1.2.3.2.2"><ci id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">ğ‘¥</ci><ci id="S5.p4.1.m1.2.2.cmml" xref="S5.p4.1.m1.2.2">ğ‘§</ci></interval><ci id="S5.p4.1.m1.2.3.3.cmml" xref="S5.p4.1.m1.2.3.3">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.2c">(x,z)\rightarrow y</annotation></semantics></math> (side view along the mouse body), when the perspective is rotated at 45 degrees around the x-axis.
This is followed by the <math id="S5.p4.2.m2.2" class="ltx_Math" alttext="(x,y)\rightarrow z" display="inline"><semantics id="S5.p4.2.m2.2a"><mrow id="S5.p4.2.m2.2.3" xref="S5.p4.2.m2.2.3.cmml"><mrow id="S5.p4.2.m2.2.3.2.2" xref="S5.p4.2.m2.2.3.2.1.cmml"><mo stretchy="false" id="S5.p4.2.m2.2.3.2.2.1" xref="S5.p4.2.m2.2.3.2.1.cmml">(</mo><mi id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml">x</mi><mo id="S5.p4.2.m2.2.3.2.2.2" xref="S5.p4.2.m2.2.3.2.1.cmml">,</mo><mi id="S5.p4.2.m2.2.2" xref="S5.p4.2.m2.2.2.cmml">y</mi><mo stretchy="false" id="S5.p4.2.m2.2.3.2.2.3" xref="S5.p4.2.m2.2.3.2.1.cmml">)</mo></mrow><mo stretchy="false" id="S5.p4.2.m2.2.3.1" xref="S5.p4.2.m2.2.3.1.cmml">â†’</mo><mi id="S5.p4.2.m2.2.3.3" xref="S5.p4.2.m2.2.3.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.2b"><apply id="S5.p4.2.m2.2.3.cmml" xref="S5.p4.2.m2.2.3"><ci id="S5.p4.2.m2.2.3.1.cmml" xref="S5.p4.2.m2.2.3.1">â†’</ci><interval closure="open" id="S5.p4.2.m2.2.3.2.1.cmml" xref="S5.p4.2.m2.2.3.2.2"><ci id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1">ğ‘¥</ci><ci id="S5.p4.2.m2.2.2.cmml" xref="S5.p4.2.m2.2.2">ğ‘¦</ci></interval><ci id="S5.p4.2.m2.2.3.3.cmml" xref="S5.p4.2.m2.2.3.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.2c">(x,y)\rightarrow z</annotation></semantics></math> (bottom perspective) for the <span id="S5.p4.3.4" class="ltx_text ltx_font_italic">Temporal Convolutional model</span> and the <math id="S5.p4.3.m3.2" class="ltx_Math" alttext="(x,z)\rightarrow y" display="inline"><semantics id="S5.p4.3.m3.2a"><mrow id="S5.p4.3.m3.2.3" xref="S5.p4.3.m3.2.3.cmml"><mrow id="S5.p4.3.m3.2.3.2.2" xref="S5.p4.3.m3.2.3.2.1.cmml"><mo stretchy="false" id="S5.p4.3.m3.2.3.2.2.1" xref="S5.p4.3.m3.2.3.2.1.cmml">(</mo><mi id="S5.p4.3.m3.1.1" xref="S5.p4.3.m3.1.1.cmml">x</mi><mo id="S5.p4.3.m3.2.3.2.2.2" xref="S5.p4.3.m3.2.3.2.1.cmml">,</mo><mi id="S5.p4.3.m3.2.2" xref="S5.p4.3.m3.2.2.cmml">z</mi><mo stretchy="false" id="S5.p4.3.m3.2.3.2.2.3" xref="S5.p4.3.m3.2.3.2.1.cmml">)</mo></mrow><mo stretchy="false" id="S5.p4.3.m3.2.3.1" xref="S5.p4.3.m3.2.3.1.cmml">â†’</mo><mi id="S5.p4.3.m3.2.3.3" xref="S5.p4.3.m3.2.3.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.2b"><apply id="S5.p4.3.m3.2.3.cmml" xref="S5.p4.3.m3.2.3"><ci id="S5.p4.3.m3.2.3.1.cmml" xref="S5.p4.3.m3.2.3.1">â†’</ci><interval closure="open" id="S5.p4.3.m3.2.3.2.1.cmml" xref="S5.p4.3.m3.2.3.2.2"><ci id="S5.p4.3.m3.1.1.cmml" xref="S5.p4.3.m3.1.1">ğ‘¥</ci><ci id="S5.p4.3.m3.2.2.cmml" xref="S5.p4.3.m3.2.2">ğ‘§</ci></interval><ci id="S5.p4.3.m3.2.3.3.cmml" xref="S5.p4.3.m3.2.3.3">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.2c">(x,z)\rightarrow y</annotation></semantics></math> prediction for the <span id="S5.p4.3.5" class="ltx_text ltx_font_italic">Linear model</span>.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and outlook</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we present a simple and robust procedure for triangulation of freely moving rodents from multiple cameras that are orthogonal towards a camera positioned underneath the plane of movement.
Using the triangulated data, we trained and evaluated two types of deep lift posing models that are able to predict the depth coordinate from single camera views, a linear ResNet model and a temporal convolutional ResNet model.
We show that the <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">Temporal Convolutional model</span> attained a lower test loss at all viewing angles then the <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">Linear model</span>.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.2" class="ltx_p">During the evaluation of our dataset, we also found that some viewing directions could be more effectively predicted than others.
For example, the diagonal top-down view yielded a lower test-loss than the orthogonal side view.
We believe that due to the body symmetry some body parts (paws, ears) are hard to distinguish by the network in this perspective.
Interestingly, the <math id="S6.p2.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S6.p2.1.m1.1a"><mi id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><ci id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">z</annotation></semantics></math> height of body parts given the <math id="S6.p2.2.m2.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S6.p2.2.m2.2a"><mrow id="S6.p2.2.m2.2.3.2" xref="S6.p2.2.m2.2.3.1.cmml"><mo stretchy="false" id="S6.p2.2.m2.2.3.2.1" xref="S6.p2.2.m2.2.3.1.cmml">(</mo><mi id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml">x</mi><mo id="S6.p2.2.m2.2.3.2.2" xref="S6.p2.2.m2.2.3.1.cmml">,</mo><mi id="S6.p2.2.m2.2.2" xref="S6.p2.2.m2.2.2.cmml">y</mi><mo stretchy="false" id="S6.p2.2.m2.2.3.2.3" xref="S6.p2.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.2b"><interval closure="open" id="S6.p2.2.m2.2.3.1.cmml" xref="S6.p2.2.m2.2.3.2"><ci id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1">ğ‘¥</ci><ci id="S6.p2.2.m2.2.2.cmml" xref="S6.p2.2.m2.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.2c">(x,y)</annotation></semantics></math> coordinates from the bottom view can be rather efficiently estimated by both models, in comparison to the other directions.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">In regards to previous literature, Wiltschko. al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> state that the auto correlation of mouse pose dynamics decrease after a period of approximately 500 ms. Interestingly, the optimal temporal window size determined for the <span id="S6.p3.1.1" class="ltx_text ltx_font_italic">Temporal Convolutional model</span> was significantly larger (3.7 s).
This suggests that additional behavioral information can be extracted at longer time spans, adding onto the discussion on the relevance of multi-scale dependencies in behavioral models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">For future work, we aim to implement semi-supervised learning as suggested in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to establish view-invariant pose-lifting for freely moving rodents.
Moreover, we need to evaluate the model under different scenarios, such es occlusions by objects or other animals.
Finally, we hope that this work paves the way to robust 3d pose estimation in complex laboratory environments, allowing for behavior quantification based on 3D pose data for a variety of experimental designs.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Realtime multi-person 2d pose estimation using part affinity fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
SandeepÂ Robert Datta, DavidÂ J. Anderson, Kristin Branson, Pietro Perona, and
Andrew Leifer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Computational Neuroethology: A Call to Action.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neuron</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 104(1):11â€“24, Oct. 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
TimothyÂ W. Dunn, JesseÂ D. Marshall, KyleÂ S. Severson, DiegoÂ E. Aldarondo, David
G.Â C. Hildebrand, SelmaanÂ N. Chettih, WilliamÂ L. Wang, AmandaÂ J. Gellis,
DavidÂ E. Carlson, Dmitriy Aronov, WinrichÂ A. Freiwald, Fan Wang, and BenceÂ P.
Ã–lveczky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Geometric deep learning enables 3D kinematic profiling across
species and environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature Methods</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, Apr. 2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Adam Gosztolai, Semih GÃ¼nel, MarcoÂ Pietro Abrate, Daniel Morales,
VictorÂ Lobato Rios, Helge Rhodin, PascalFua, and Pavan Ramdya.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Liftpose3d, a deep learning-based approach for transforming 2d to 3d
pose in laboratory experiments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">bioRxiv</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
PolÂ Perez Granero.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">2d to 3d body pose estimation for sign language with deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Richard Hartley and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Multiple View Geometry in Computer Vision</span><span id="bib.bib6.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">Cambridge University Press, Cambridge, 2 edition, 2004.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Pierre Karashchuk, KatieÂ L. Rupp, EvynÂ S. Dickinson, Elischa Sanders, Eiman
Azim, BingniÂ W. Brunton, and JohnÂ C. Tuthill.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Anipose: a toolkit for robust markerless 3d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">bioRxiv</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Kevin Luxem, Falko Fuhrmann, Johannes KÃ¼rsch, Stefan Remy, and Pavol Bauer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Identifying Behavioral Structure from Deep Variational
Embeddings of Animal Motion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">bioRxiv</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, page 2020.05.14.095430, Oct. 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.5.1" class="ltx_text" style="font-size:90%;">Publisher: Cold Spring Harbor Laboratory Section: New Results.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Julieta Martinez, Rayat Hossain, Javier Romero, and JamesÂ J. Little.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">A simple yet effective baseline for 3d human pose estimation, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Alexander Mathis, Pranav Mamidanna, KevinÂ M. Cury, Taiga Abe, VenkateshÂ N.
Murthy, MackenzieÂ Weygandt Mathis, and Matthias Bethge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Deeplabcut: markerless pose estimation of user-defined body parts
with deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature Neuroscience</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 21(9):1281â€“1289, 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">3d human pose estimation in video with temporal convolutions and
semi-supervised training, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
TalmoÂ D. Pereira, DiegoÂ E. Aldarondo, Lindsay Willmore, Mikhail Kislin, Samuel
S.-H. Wang, Mala Murthy, and JoshuaÂ W. Shaevitz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Fast animal pose estimation using deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature Methods</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 16(1):117â€“125, 2019.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
JenniferÂ J. Sun, Jiaping Zhao, Liang-Chieh Chen, Florian Schroff, Hartwig Adam,
and Ting Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">View-Invariant Probabilistic Embedding for Human Pose.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1912.01001 [cs]</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, Oct. 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.5.1" class="ltx_text" style="font-size:90%;">arXiv: 1912.01001.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
AlexanderÂ B. Wiltschko, MatthewÂ J. Johnson, Giuliano Iurilli, RalphÂ E.
Peterson, JesseÂ M. Katon, StanÂ L. Pashkovski, VictoriaÂ E. Abraira, RyanÂ P.
Adams, and SandeepÂ Robert Datta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Mapping sub-second structure in mouse behavior.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neuron</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 88(6):1121â€“1135, 2021/04/06 2015.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2106.12992" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2106.12993" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2106.12993">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2106.12993" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2106.12994" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 02:24:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
