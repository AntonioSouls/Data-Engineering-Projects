<!DOCTYPE html><html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>A quest through interconnected datasets: lessons from highly-cited ICASSP papers</title>
<!--Generated on Thu Sep 19 14:24:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">




<meta content="
Annotation practices,  data quality,  data provenance,  responsible research,  applied machine learning
" lang="en" name="keywords">
<base href="https://arxiv.org/html/2410.03676v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2410.03676v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2410.03676v1/#myForm">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2410.03676v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2410.03676v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#S1" title="In A quest through interconnected datasets: lessons from highly-cited ICASSP papers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#S2" title="In A quest through interconnected datasets: lessons from highly-cited ICASSP papers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#S3" title="In A quest through interconnected datasets: lessons from highly-cited ICASSP papers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Results and discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#S4" title="In A quest through interconnected datasets: lessons from highly-cited ICASSP papers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Conclusions and future work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div class="package-alerts ltx_document" role="status" aria-label="Conversion errors have been found">
      <button aria-label="Dismiss alert">
          <span aria-hidden="true"><svg role="presentation" width="20" height="20" viewBox="0 0 44 44" aria-hidden="true" focusable="false">
          <path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
          <path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
          </svg></span>
      </button>
      <p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
          <ul arial-label="Unsupported packages used in this paper">
              <li>failed: CJKutf8</li>
          </ul>
      <p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
    </div><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: arXiv.org perpetual non-exclusive license</a><div id="watermark-tr">arXiv:2410.03676v1 [cs.SD] 19 Sep 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A quest through interconnected datasets:
<br class="ltx_break">lessons from highly-cited ICASSP papers</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cynthia C.&nbsp;S. Liem

 and Doğa Taşcılar

 and Andrew M. Demetriou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">Delft University of Technology
<br class="ltx_break"></span>Delft, The Netherlands
<br class="ltx_break">c.c.s.liem@tudelft.nl
<br class="ltx_break">https://orcid.org/0000-0002-5385-7695
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id2.2.id1">Delft University of Technology
<br class="ltx_break"></span>Delft, The Netherlands
<br class="ltx_break">dtascilar.cse@gmail.com
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.3.id1">Delft University of Technology
<br class="ltx_break"></span>Delft, The Netherlands
<br class="ltx_break">a.m.demetriou@tudelft.nl
<br class="ltx_break">https://orcid.org/0000-0002-0724-2278
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id4.id1">As audio machine learning outcomes are deployed in societally impactful applications, it is important to have a sense of the quality and origins of the data used. Noticing that being explicit about this sense is not trivially rewarded in academic publishing in applied machine learning domains, and neither is included in typical applied machine learning curricula, we present a study into dataset usage connected to the top-5 cited papers at the International Conference on Acoustics, Speech, and Signal Processing (ICASSP). In this, we conduct thorough depth-first analyses towards origins of used datasets, often leading to searches that had to go beyond what was reported in official papers, and ending into unclear or entangled origins. Especially in the current pull towards larger, and possibly generative AI models, awareness of the need for accountability on data provenance is increasing. With this, we call on the community to not only focus on engineering larger models, but create more room and reward for explicitizing the foundations on which such models should be built.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
Annotation practices, data quality, data provenance, responsible research, applied machine learning

</div>
<div class="ltx_para" id="p1">
<svg class="ltx_picture" height="66.72" id="p1.pic1" overflow="visible" version="1.1" width="604.52"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,66.72) matrix(1 0 0 -1 0 0) translate(-122.74,0) translate(0,-14.11) matrix(1.0 0.0 0.0 1.0 127.35 42.63)"><foreignObject height="57.5" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="595.3"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="p1.pic1.1.1.1.1" style="border-color: #000000;">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="p1.pic1.1.1.1.1.1" style="width:430.2pt;">
<span class="ltx_p" id="p1.pic1.1.1.1.1.1.1"><span class="ltx_text" id="p1.pic1.1.1.1.1.1.1.1" style="font-size:80%;">©2024 IEEE. Personal use of this material is permitted.
Permission from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers or
lists, or reuse of any copyrighted component of this work in other works.</span></span>
</span></span></foreignObject></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Today, many people have integrated audio services in their daily lives: they listen to music services, and make use of voice assistants. Many of these are enabled by audio machine learning. As with any machine learning task, the quality of a model is highly dependent on that of the dataset provided <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib1" title="">1</a>]</cite>. Ideally, for high-stakes applications, data should be responsibly sourced, well-documented, and available for independent quality auditing. However, current publication culture has disincentivized these aspects. In practice, paper authors are not eager to share datasets upon request (even when having promised so in their papers)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib2" title="">2</a>]</cite>, annotation errors exist and impact classification outcomes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib3" title="">3</a>]</cite>, and annotation practice standards are missing, causing ‘garbage in, garbage out’ effects in outcomes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib4" title="">4</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To add to this, doing predictive-focused modeling work within a machine learning pipeline is professionally more valued than working on data quality. This has both emerged in interviews with practitioners&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib5" title="">5</a>]</cite>, as well as from studies into self-reported contributions in highly-cited works at major technical machine learning venues&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib6" title="">6</a>]</cite>, which mostly highlight performance, generalization, quantitative evidence, efficiency, building on past work, and novelty&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib6" title="">6</a>]</cite>. As a consequence, this is what peer reviewers typically look for when assessing the merit of a submission, while work seeking to ask more critical questions on the foundations on which work is built has a harder time getting through. Thanks to the OpenReview platform, it can e.g.&nbsp;be seen that an earlier submission of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib6" title="">6</a>]</cite> was rejected at NeurIPS for being “not surprising or deep”<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=oioB7Te7Bo" title="">https://openreview.net/forum?id=oioB7Te7Bo</a>, accessed July 21, 2024.</span></span></span>, while a recent analysis of the ImageNet dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib7" title="">7</a>]</cite> also received reviewer criticisms that no new idea or methodology was proposed<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=jh0ck1bPGF" title="">https://openreview.net/forum?id=jh0ck1bPGF</a>, accessed July 21, 2024.</span></span></span>. More broadly in computer science, concerns have been voiced on whether peer review currently too strongly focuses on novelty, at the cost of gaining bigger-picture insights&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib8" title="">8</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The work presented in this article follows from these notions, with the first and last author of this work being members of an interdisciplinary lab. Having a background in psychology, the last author has been trained in data acquisition and annotation methods that ensure that <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">constructs</em>, i.e.&nbsp;phenomena that rely on human interpretation (and cannot directly physically be measured), are measured in valid and reliable ways. For this, the basics of psychometrics&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib9" title="">9</a>]</cite> and survey science <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib10" title="">10</a>]</cite> are often taught as entry-level courses to social science students, which inform robust sampling and survey design for gathering human responses. Additionally, due to perverse publication and academic rewarding culture, the psychological research field went through a crisis in which many published claims turned out both irreproducible and irreplicable. Ever since, it has been reforming and transforming to improve credibility&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib11" title="">11</a>]</cite>. Nowadays, more rigorous reporting standards are required, and early transparency on what will be researched (e.g.&nbsp;through pre-registration) is frequently given, or even expected. Furthermore, the field has generally been displaying more awareness that in order to make general claims, purposeful strategy needs to have been established on whether sufficiently representative samples of the human population are approached, and what questions they are asked to answer. Authoritative psychological publication venues now have base expectations of such strategies existing and being followed, for any work to be submitted.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In contrast, this type of awareness typically has not existed in technical applied machine learning communities. Here, the dominant way of working has primarily focused on engineering technical system innovations that can echo ‘ground truth’ human responses—and as such, may be able to automatically scale up the prediction of human-like responses in various applications. However, the ground-truth responses often are conveniently or sparsely sampled, without clear philosophy on whether they can indeed be considered sufficiently robust and representative to be used as a golden standard. For example, in response to the question, “How many annotators would be needed for NLP corpus ground truth?”, a well-cited book on natural language annotation for machine learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib12" title="">12</a>]</cite> suggests to “have your corpus annotated by at least two people (more is preferable, but not always practical)”. This is a remarkably low number, which would not meet the bar of rigor expected today in the quantitative social sciences, and without clear substantiation of whether this indeed would be sufficient.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Some awareness on these issues have been arising in recent years in several works targeting computationally oriented communities, mostly from a responsible computing perspective. For example, several articles raised awareness on how metrology and measurement best practices can be helpful in more robustly dealing with human responses to subjective and more normative questions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib14" title="">14</a>]</cite>, and best practices are pointed to with regard to purposeful archiving&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib15" title="">15</a>]</cite>. In several applied machine learning venues, we do notice calls for increased rigor, accountability and ethical reflection, which typically manifests in mandatory inclusion of ethics statements, or checklists&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib16" title="">16</a>]</cite> accompanying submissions, as e.g.&nbsp;can today be seen at NeurIPS and the *CL venues, which partially are inspired by proposals for better documentation through data sheets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib17" title="">17</a>]</cite> and model cards<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib18" title="">18</a>]</cite>. Still, institutionalized uptake of the expertise remains relatively rare, and the most highly cited papers at prestigious venues still appear to favor technical innovation over more foundational reflection. As a further recent example, warnings have emerged that key claims in currently popular Large Language Model research often are overblown&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib19" title="">19</a>]</cite>, and many issues exist with regard to test data contamination. In fact, contamination happened for the GPT-3 model, where the authors reported that “a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible
to retrain the model”&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib20" title="">20</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The first author of this work has a background in music information retrieval. Here, data issues were raised in literature too: faults&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib21" title="">21</a>]</cite> and a likely lack of ecological validity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib22" title="">22</a>]</cite> in datasets caused for automatically trained classifiers to seemingly perform well in their original evaluations, but—when more thorough testing was applied—not actually pick up the concepts they were purported to capture. Now being a senior lab lead and educator at a public institution, she has sought to raise awareness about these issues with students and peers. However, beyond first-principle argumentation, the first and last author of this paper were frequently requested to deliver more tangible evidence that current common ways of dataset handling indeed are problematic.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Surveys on annotation practices in several applied machine learning domains did emerge, yielding empirical evidence that explicit justification of annotation choices are typically lacking in published work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib23" title="">23</a>]</cite>, and annotation quality indeed affects model performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib24" title="">24</a>]</cite>. To add to this body of insight, in Summer 2023, a topic proposal was submitted to the 3rd year undergraduate Research Project in the Computer Science and Engineering curriculum at Delft University of Technology. Here, students were invited to choose an impactful applied machine learning domain or publication venue, and conduct a systematic literature review in which annotation practices were documented for corresponding top-cited publications.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">The current paper presents the results of an analysis performed by a then-undergraduate student (the second author), focusing on highly-cited audio research papers. In this particular work, where the original intention had been to conduct a systematic literature review, it turned out non-trivial to get sufficiently in-depth information on the data resources used in the papers. As a consequence, the research project turned into a depth-first investigation into dataset origins. We are aware this is not what typical machine learning venues consider publishable, but will show that these outcomes are relevant, contributing tangible insights on what currently is under-reported in typical academic literature.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Methodology</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">For the current paper, we built upon the student analysis as performed in Summer 2023, but re-verified links and references, updated bibliometrics where relevant, and took care to more explicitly position this work in relation to data-centered and responsible research literature.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">In terms of the investigation into dataset origins, the first steps of a systematic literature review were conducted. The International Conference on Acoustics, Speech, and Signal Processing (ICASSP) was chosen as a prestigious and English-language venue, the years 2021 and 2022 were chosen to focus on recent work, and sources were sorted by the number of citations to focus on the most impactful work. For each paper, the intention was to systematically collect annotation practice information for every dataset used to train a machine learning model, following the questions as used in Geiger et al.’s survey on annotation practices&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib4" title="">4</a>]</cite>. However, often, the origins of datasets turned out unclear. When relevant information was absent in a given paper itself, a deeper inquiry was performed through the paper’s references, and by searching for any other available documentation, e.g.&nbsp;dataset websites, repository wikis, and README files. Often, the latter was done through multiple queries into terms, technologies and datasets as mentioned in the papers. In case no paper or online resource was found providing sufficient information on dataset origins and annotation practice, attempts were undertaken to contact the original authors through their provided e-mails and LinkedIn accounts, although these only rarely yielded responses. In some cases, original resources also were not in English, and neither was their documentation (e.g.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib25" title="">25</a>]</cite> in Chinese and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib31" title="">31</a>]</cite> in Japanese). In such situations, the second author of this work used Google Translate on the provided documentation text. Furthermore, beyond datasets being used in their original, <em class="ltx_emph ltx_font_italic" id="S2.p2.1.1">initial</em> reported form, we also frequently encountered situations in which earlier-published datasets were <em class="ltx_emph ltx_font_italic" id="S2.p2.1.2">modified</em> or <em class="ltx_emph ltx_font_italic" id="S2.p2.1.3">combined</em>. Finally, the use of <em class="ltx_emph ltx_font_italic" id="S2.p2.1.4">pre-trained</em> models in a given approach also will bring in knowledge inferred from datasets on which this pre-training was performed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">For each resource, the second author documented whether the style of reporting on data is <em class="ltx_emph ltx_font_italic" id="S2.p3.1.1">explicit</em> (giving clear attention to the dataset, e.g.&nbsp;through dedicated sections or mentions in the abstract) or <em class="ltx_emph ltx_font_italic" id="S2.p3.1.2">implicit</em> (only naming the name of a dataset in passing), and whether any justification was given for this degree of reporting. Then, for initial datasets, structured notes were taken on whether the name of the original dataset matched that of later references to it; who owned the data; in case of speech, what the content is about and who owned the speech; whether any formal instructions were reported on the recording protocol; whether any formal instructions were reported on the annotation protocol; whether any quality verification happened, and what the dataset size was. For modified and combined datasets, we documented type and details of the modification or combination, and continued searching until reaching initial datasets. For pre-trained models, we recorded the name of the dataset that the model was trained on, and then deepened the search until reaching initial datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Because of the considerable work involved into researching these dataset origins, we limited a full analysis to the top-5 cited papers from 2021-2022, according to Scopus bibliometrics in Spring 2023. Rechecking the bibliometrics a year later for the current manuscript, these articles still are among the top-5 highest-cited ICASSP papers from 2021-2022 (Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#S2.T1" title="TABLE I ‣ II Methodology ‣ A quest through interconnected datasets: lessons from highly-cited ICASSP papers"><span class="ltx_text ltx_ref_tag">I</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1">Paper</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2">2023 citations</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.3">2023 rank</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.4">2024 citations</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.5">2024 rank</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.2.2.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib32" title="">32</a>]</cite></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.2">156</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.4">385</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.5">1</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S2.T1.1.3.3.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib33" title="">33</a>]</cite></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.3.3.2">121</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.3.3.3">2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.3.3.4">276</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.3.3.5">2</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S2.T1.1.4.4.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib34" title="">34</a>]</cite></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.4.2">80</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.4.3">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.4.4">157</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.4.5">3</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S2.T1.1.5.5.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib35" title="">35</a>]</cite></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.5.2">66</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.5.3">4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.5.4">132</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.5.5">5</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S2.T1.1.6.6.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib36" title="">36</a>]</cite></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T1.1.6.6.2">62</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T1.1.6.6.3">5</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T1.1.6.6.4">154</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T1.1.6.6.5">4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Citation bibliometrics for ICASSP papers studied in this paper (reference dates: May 8, 2023 and April 10, 2024)</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.1" style="font-size:70%;">Type of Resource</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.2.1">
<span class="ltx_p" id="S2.T2.1.1.1.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.2.1.1.1" style="font-size:70%;">Style of Reporting</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.3.1">
<span class="ltx_p" id="S2.T2.1.1.1.3.1.1" style="width:284.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.3.1.1.1" style="font-size:70%;">Resource</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.2.1.1"><span class="ltx_text" id="S2.T2.1.2.1.1.1" style="font-size:70%;">Selected Literature</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.2.1">
<span class="ltx_p" id="S2.T2.1.2.1.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T2.1.2.1.2.1.1.1" style="font-size:70%;">Explicit</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.3.1">
<span class="ltx_p" id="S2.T2.1.2.1.3.1.1" style="width:284.5pt;"><span class="ltx_text" id="S2.T2.1.2.1.3.1.1.1" style="font-size:70%;">Attention is all you need in speech separation </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.2.1.3.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib33" title="">33</a><span class="ltx_text" id="S2.T2.1.2.1.3.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.3.2.1"><span class="ltx_text" id="S2.T2.1.3.2.1.1" style="font-size:70%;">Selected Literature</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.2.1">
<span class="ltx_p" id="S2.T2.1.3.2.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T2.1.3.2.2.1.1.1" style="font-size:70%;">Implicit</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.3.1">
<span class="ltx_p" id="S2.T2.1.3.2.3.1.1" style="width:284.5pt;">
<span class="ltx_tabular ltx_align_top" id="S2.T2.1.3.2.3.1.1.1">
<span class="ltx_tr" id="S2.T2.1.3.2.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.3.2.3.1.1.1.1.1"><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.1.1.1" style="font-size:70%;">SA-Net: Shuffle attention for deep convolutional neural networks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib32" title="">32</a><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.1.1.3.2" style="font-size:70%;">]</span></cite></span></span>
<span class="ltx_tr" id="S2.T2.1.3.2.3.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.3.2.3.1.1.1.2.1"><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.2.1.1" style="font-size:70%;">Recent developments on ESPNeT toolkit boosted by conformer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib34" title="">34</a><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.2.1.3.2" style="font-size:70%;">]</span></cite></span></span>
<span class="ltx_tr" id="S2.T2.1.3.2.3.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.3.2.3.1.1.1.3.1"><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.3.1.1" style="font-size:70%;">FastPitch: Parallel text-to-speech with pitch prediction </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib35" title="">35</a><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.3.1.3.2" style="font-size:70%;">]</span></cite></span></span>
<span class="ltx_tr" id="S2.T2.1.3.2.3.1.1.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.3.2.3.1.1.1.4.1"><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.4.1.1" style="font-size:70%;">End-to-end anti-spoofing with rawnet2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib36" title="">36</a><span class="ltx_text" id="S2.T2.1.3.2.3.1.1.1.4.1.3.2" style="font-size:70%;">]</span></cite></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.4.3.1"><span class="ltx_text" id="S2.T2.1.4.3.1.1" style="font-size:70%;">Initial Dataset</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.2.1">
<span class="ltx_p" id="S2.T2.1.4.3.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T2.1.4.3.2.1.1.1" style="font-size:70%;">Explicit</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.3.1">
<span class="ltx_p" id="S2.T2.1.4.3.3.1.1" style="width:284.5pt;">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.1.4.3.3.1.1.1">
<span class="ltx_tr" id="S2.T2.1.4.3.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.4.3.3.1.1.1.1.1"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.1" style="font-size:70%;">WSJ0 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib37" title="">37</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.4" style="font-size:70%;">,
Datatang </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib26" title="">26</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.7" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib27" title="">27</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.9.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.10" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.11.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib28" title="">28</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.12.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.13" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.14.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib29" title="">29</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.15.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.16" style="font-size:70%;">,
AISHELL-2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.17.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib38" title="">38</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.18.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.19" style="font-size:70%;">,
CSJ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.20.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib30" title="">30</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.21.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.22" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.23.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib31" title="">31</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.24.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.1.1.25" style="font-size:70%;">,
HKUST</span></span></span>
<span class="ltx_tr" id="S2.T2.1.4.3.3.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.4.3.3.1.1.1.2.1"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.2.1.1" style="font-size:70%;">Mandarin Telephone Speech </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib39" title="">39</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.2.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.2.1.4" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.2.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib40" title="">40</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.2.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.2.1.7" style="font-size:70%;">,
HKUST Mandarin Telephone Transcript</span></span></span>
<span class="ltx_tr" id="S2.T2.1.4.3.3.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.4.3.3.1.1.1.3.1"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.3.1.1" style="font-size:70%;">Data </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib41" title="">41</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.3.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.3.1.4" style="font-size:70%;">,
LDC Fisher Spanish Speech </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.3.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib42" title="">42</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.3.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.3.1.7" style="font-size:70%;">,
LDC Fisher Spanish - Transcripts </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.3.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib43" title="">43</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.3.1.9.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.3.1.10" style="font-size:70%;">,</span></span></span>
<span class="ltx_tr" id="S2.T2.1.4.3.3.1.1.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.4.3.3.1.1.1.4.1"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.4.1.1" style="font-size:70%;">The CALLHOME Spanish Speech </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib44" title="">44</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.4.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.4.1.4" style="font-size:70%;">,
The CALLHOME Spanish Transcripts </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.4.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib45" title="">45</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.4.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.4.1.7" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.4.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib44" title="">44</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.4.1.9.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.4.1.10" style="font-size:70%;">,</span></span></span>
<span class="ltx_tr" id="S2.T2.1.4.3.3.1.1.1.5">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.4.3.3.1.1.1.5.1"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.1" style="font-size:70%;">JSUT </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib46" title="">46</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.4" style="font-size:70%;">,
VoxForge English </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib47" title="">47</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.7" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib48" title="">48</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.9.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.10" style="font-size:70%;">,
AISHELL-ASR0009 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.11.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib25" title="">25</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.12.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.13" style="font-size:70%;">,
LibriVox </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.14.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib49" title="">49</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.15.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.5.1.16" style="font-size:70%;">,</span></span></span>
<span class="ltx_tr" id="S2.T2.1.4.3.3.1.1.1.6">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.4.3.3.1.1.1.6.1"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.6.1.1" style="font-size:70%;">SWITCHBOARD </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.6.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib50" title="">50</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.6.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.6.1.4" style="font-size:70%;">,
TED Talks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.6.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib51" title="">51</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.6.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.6.1.7" style="font-size:70%;">,
FreeSound sound library </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.6.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib52" title="">52</a><span class="ltx_text" id="S2.T2.1.4.3.3.1.1.1.6.1.9.2" style="font-size:70%;">]</span></cite></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.5.4.1"><span class="ltx_text" id="S2.T2.1.5.4.1.1" style="font-size:70%;">Initial Dataset</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.2.1">
<span class="ltx_p" id="S2.T2.1.5.4.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T2.1.5.4.2.1.1.1" style="font-size:70%;">Implicit</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.3.1">
<span class="ltx_p" id="S2.T2.1.5.4.3.1.1" style="width:284.5pt;">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.1.5.4.3.1.1.1">
<span class="ltx_tr" id="S2.T2.1.5.4.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.5.4.3.1.1.1.1.1"><span class="ltx_text" id="S2.T2.1.5.4.3.1.1.1.1.1.1" style="font-size:70%;">WordNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.5.4.3.1.1.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib53" title="">53</a><span class="ltx_text" id="S2.T2.1.5.4.3.1.1.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.5.4.3.1.1.1.1.1.4" style="font-size:70%;">,
LibriVox data of Linda Johnson </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.5.4.3.1.1.1.1.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib54" title="">54</a><span class="ltx_text" id="S2.T2.1.5.4.3.1.1.1.1.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.5.4.3.1.1.1.1.1.7" style="font-size:70%;">,
MagnaTune Song Dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.5.4.3.1.1.1.1.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib55" title="">55</a><span class="ltx_text" id="S2.T2.1.5.4.3.1.1.1.1.1.9.2" style="font-size:70%;">]</span></cite></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.6.5.1"><span class="ltx_text" id="S2.T2.1.6.5.1.1" style="font-size:70%;">Modified Dataset</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.2.1">
<span class="ltx_p" id="S2.T2.1.6.5.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T2.1.6.5.2.1.1.1" style="font-size:70%;">Explicit</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.3.1">
<span class="ltx_p" id="S2.T2.1.6.5.3.1.1" style="width:284.5pt;">
<span class="ltx_tabular ltx_align_top" id="S2.T2.1.6.5.3.1.1.1">
<span class="ltx_tr" id="S2.T2.1.6.5.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.6.5.3.1.1.1.1.1"><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.1.1.1" style="font-size:70%;">AMT (for checking ImageNet) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib56" title="">56</a><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.1.1.4" style="font-size:70%;">,
WSJ0-2/3mix </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.1.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib57" title="">57</a><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.1.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.1.1.7" style="font-size:70%;">, AMT for evaluating the</span></span></span>
<span class="ltx_tr" id="S2.T2.1.6.5.3.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.6.5.3.1.1.1.2.1"><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.1" style="font-size:70%;">FastPitch algorithm </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib56" title="">56</a><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.4" style="font-size:70%;">,
Switchboard-1 Release 2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib58" title="">58</a><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.7" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib59" title="">59</a><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.9.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.10" style="font-size:70%;"> ,
TED-LIUMv2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.11.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib60" title="">60</a><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.12.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.13" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.14.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib61" title="">61</a><span class="ltx_text" id="S2.T2.1.6.5.3.1.1.1.2.1.15.2" style="font-size:70%;">]</span></cite></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.7.6.1"><span class="ltx_text" id="S2.T2.1.7.6.1.1" style="font-size:70%;">Modified Dataset</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.6.2.1">
<span class="ltx_p" id="S2.T2.1.7.6.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T2.1.7.6.2.1.1.1" style="font-size:70%;">Implicit</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.6.3.1">
<span class="ltx_p" id="S2.T2.1.7.6.3.1.1" style="width:284.5pt;">
<span class="ltx_tabular ltx_align_top" id="S2.T2.1.7.6.3.1.1.1">
<span class="ltx_tr" id="S2.T2.1.7.6.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.7.6.3.1.1.1.1.1"><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.1" style="font-size:70%;">Aurora-4 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib62" title="">62</a><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.4" style="font-size:70%;">,
LibriSpeech ASR corpus </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib63" title="">63</a><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.7" style="font-size:70%;">,
WSJCAM0 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib64" title="">64</a><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.9.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.10" style="font-size:70%;">,
MC-WSJ-AV </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.11.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib65" title="">65</a><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.12.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.13" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.14.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib66" title="">66</a><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.15.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.1.1.16" style="font-size:70%;">,</span></span></span>
<span class="ltx_tr" id="S2.T2.1.7.6.3.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.7.6.3.1.1.1.2.1"><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.2.1.1" style="font-size:70%;">AMT for translation (Fisher and CALLHOME) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib56" title="">56</a><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.2.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.2.1.4" style="font-size:70%;">,
LJSpeech </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.2.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib67" title="">67</a><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.2.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.2.1.7" style="font-size:70%;">,</span></span></span>
<span class="ltx_tr" id="S2.T2.1.7.6.3.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.7.6.3.1.1.1.3.1"><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.3.1.1" style="font-size:70%;">MagnaTagATune dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib55" title="">55</a><span class="ltx_text" id="S2.T2.1.7.6.3.1.1.1.3.1.3.2" style="font-size:70%;">]</span></cite></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.8.7.1"><span class="ltx_text" id="S2.T2.1.8.7.1.1" style="font-size:70%;">Combined Dataset</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.8.7.2.1">
<span class="ltx_p" id="S2.T2.1.8.7.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T2.1.8.7.2.1.1.1" style="font-size:70%;">Explicit</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.8.7.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.8.7.3.1">
<span class="ltx_p" id="S2.T2.1.8.7.3.1.1" style="width:284.5pt;">
<span class="ltx_tabular ltx_align_top" id="S2.T2.1.8.7.3.1.1.1">
<span class="ltx_tr" id="S2.T2.1.8.7.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.8.7.3.1.1.1.1.1"><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.1" style="font-size:70%;">ImageNet-1k </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib68" title="">68</a><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.4" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib69" title="">69</a><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.7" style="font-size:70%;">,
Fisher and CALLHOME </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib70" title="">70</a><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.9.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.10" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.11.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib71" title="">71</a><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.12.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.13" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.14.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib72" title="">72</a><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.15.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.16" style="font-size:70%;">,
TED-LIUM Release 3 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.17.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib73" title="">73</a><span class="ltx_text" id="S2.T2.1.8.7.3.1.1.1.1.1.18.2" style="font-size:70%;">]</span></cite></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.9.8.1"><span class="ltx_text" id="S2.T2.1.9.8.1.1" style="font-size:70%;">Combined Dataset</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.9.8.2.1">
<span class="ltx_p" id="S2.T2.1.9.8.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T2.1.9.8.2.1.1.1" style="font-size:70%;">Implicit</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.9.8.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.9.8.3.1">
<span class="ltx_p" id="S2.T2.1.9.8.3.1.1" style="width:284.5pt;">
<span class="ltx_tabular ltx_align_top" id="S2.T2.1.9.8.3.1.1.1">
<span class="ltx_tr" id="S2.T2.1.9.8.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.9.8.3.1.1.1.1.1"><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.1" style="font-size:70%;">MS COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib74" title="">74</a><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.4" style="font-size:70%;">,
AISHELL-1 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib75" title="">75</a><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.7" style="font-size:70%;">,
4th CHiME </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib76" title="">76</a><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.9.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.10" style="font-size:70%;">,
The REVERB </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.11.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib77" title="">77</a><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.12.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.13" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.14.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib78" title="">78</a><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.15.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.1.1.16" style="font-size:70%;">,</span></span></span>
<span class="ltx_tr" id="S2.T2.1.9.8.3.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.9.8.3.1.1.1.2.1"><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.2.1.1" style="font-size:70%;">ASVspoof2019 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib79" title="">79</a><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.2.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.2.1.4" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.2.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib80" title="">80</a><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.2.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.2.1.7" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.2.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib81" title="">81</a><span class="ltx_text" id="S2.T2.1.9.8.3.1.1.1.2.1.9.2" style="font-size:70%;">]</span></cite></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.10.9.1"><span class="ltx_text" id="S2.T2.1.10.9.1.1" style="font-size:70%;">Pre-trained Model</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.10.9.2.1">
<span class="ltx_p" id="S2.T2.1.10.9.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T2.1.10.9.2.1.1.1" style="font-size:70%;">Explicit</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.10.9.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.10.9.3.1">
<span class="ltx_p" id="S2.T2.1.10.9.3.1.1" style="width:284.5pt;">
<span class="ltx_tabular ltx_align_top" id="S2.T2.1.10.9.3.1.1.1">
<span class="ltx_tr" id="S2.T2.1.10.9.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.10.9.3.1.1.1.1.1"><span class="ltx_text" id="S2.T2.1.10.9.3.1.1.1.1.1.1" style="font-size:70%;">ResNet50 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.10.9.3.1.1.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib82" title="">82</a><span class="ltx_text" id="S2.T2.1.10.9.3.1.1.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.10.9.3.1.1.1.1.1.4" style="font-size:70%;">,
Deep Voice 3 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.10.9.3.1.1.1.1.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib83" title="">83</a><span class="ltx_text" id="S2.T2.1.10.9.3.1.1.1.1.1.6.2" style="font-size:70%;">]</span></cite></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.11.10.1"><span class="ltx_text" id="S2.T2.1.11.10.1.1" style="font-size:70%;">Pre-trained Model</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.1.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.11.10.2.1">
<span class="ltx_p" id="S2.T2.1.11.10.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T2.1.11.10.2.1.1.1" style="font-size:70%;">Implicit</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.1.11.10.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.11.10.3.1">
<span class="ltx_p" id="S2.T2.1.11.10.3.1.1" style="width:284.5pt;">
<span class="ltx_tabular ltx_align_top" id="S2.T2.1.11.10.3.1.1.1">
<span class="ltx_tr" id="S2.T2.1.11.10.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.11.10.3.1.1.1.1.1"><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.1.1.1" style="font-size:70%;">Kaldi framwork for WSJ0 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib84" title="">84</a><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.1.1.4" style="font-size:70%;">,
Kaldi HKUST recipe </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.1.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib85" title="">85</a><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.1.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.1.1.7" style="font-size:70%;">,
Tacotron 2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.1.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib86" title="">86</a><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.1.1.9.2" style="font-size:70%;">]</span></cite></span></span>
<span class="ltx_tr" id="S2.T2.1.11.10.3.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.11.10.3.1.1.1.2.1"><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.2.1.1" style="font-size:70%;">WaveNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib87" title="">87</a><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.2.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.2.1.4" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.2.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib88" title="">88</a><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.2.1.6.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.2.1.7" style="font-size:70%;">,
Tag A Tune </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.2.1.8.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib89" title="">89</a><span class="ltx_text" id="S2.T2.1.11.10.3.1.1.1.2.1.9.2" style="font-size:70%;">]</span></cite></span></span>
</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Overview of all investigated resources relating to the papers surveyed in our study.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Results and discussion</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">All resources found through our search are summarized in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#S2.T2" title="TABLE II ‣ II Methodology ‣ A quest through interconnected datasets: lessons from highly-cited ICASSP papers"><span class="ltx_text ltx_ref_tag">II</span></a>. Our full research notes are publicly released with this paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib90" title="">90</a>]</cite>.
As can be seen, we notice many implicit styles of reporting. While this improves for initial datasets, several datasets are even in their initial form not explicitly described. Furthermore, while initial datasets frequently report on audio recording protocols, much less explicit reporting is done on whether there were annotator instructions, and the degree of quality verification starkly differs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To visualize the paths from a main paper down to initial datasets, we furthermore drew graphs to visualize connections between the found resources.
As this led to entangled and interconnected graphs, we now show and discuss main observations for each of the surveyed papers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="148" id="S3.F1.g1" src="https://arxiv.org/html/2410.03676v1/extracted/5866097/Pap1.png" width="299">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Dataset connections for&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib32" title="">32</a>]</cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The highest-cited ICASSP paper from 2021/2022 is&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib32" title="">32</a>]</cite>, for which the graph is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#S3.F1" title="Figure 1 ‣ III Results and discussion ‣ A quest through interconnected datasets: lessons from highly-cited ICASSP papers"><span class="ltx_text ltx_ref_tag">1</span></a>. The paper proposes a novel Shuffle Attention (SA) mechanism, in which spatial and channel attention mechanisms are combined. Performance is compared to the ResNet model, using ImageNet data. Additionally, experiments on object detection and instance segmentation are reported on MS COCO. Following the paper reporting paths as found through our depth-first search methodology, for ImageNet, several origins of data (Flickr, search engines) are not further traceable. Furthermore, some ambiguity can be found with respect to ResNet: in some experiments, ResNet-50 and ResNet-101 are used as a backbone for the SA mechanism, while it is unclear whether this would be the architecture or a pre-trained model (which commonly would have been done on ImageNet).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="145" id="S3.F2.g1" src="https://arxiv.org/html/2410.03676v1/extracted/5866097/Pap2.png" width="299">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Dataset connections for&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib33" title="">33</a>]</cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib33" title="">33</a>]</cite> proposes a new RNN-free Transformer-based neural network for speech separation, called SepFormer. For experiments, a WSJ0-2/3mix is used, which sources from the WSJ0 dataset. ‘2’ and ‘3’ in WSJ0-2/3mix refer to mixing 2 or 3 different datapoints of WSJ0 to synthesize multiple-speaker recordings.
While this makes the origin graph (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#S3.F2" title="Figure 2 ‣ III Results and discussion ‣ A quest through interconnected datasets: lessons from highly-cited ICASSP papers"><span class="ltx_text ltx_ref_tag">2</span></a>) straightforward, validity concerns could be raised on this way of using data. WSJ0 was published in 1991, containing readings from the Wall Street Journal. This will bias vocabulary towards that used in American news of more than 30 years ago. Furthermore, while the way of synthesis is well-controlled, it may not yield realistic multiple-speaker data. Such realistic data actually exists, e.g.&nbsp;in the Spanish-spoken Fisher and CALLHOME datasets&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib72" title="">72</a>]</cite>. Finally, we would expect newspaper readings to be more monotonous than the spontaneous speech on which speech separation would more likely be applied.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="263" id="S3.F3.g1" src="https://arxiv.org/html/2410.03676v1/extracted/5866097/Pap3.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Dataset connections for&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib34" title="">34</a>]</cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">As shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#S3.F3" title="Figure 3 ‣ III Results and discussion ‣ A quest through interconnected datasets: lessons from highly-cited ICASSP papers"><span class="ltx_text ltx_ref_tag">3</span></a>, a very complicated graph was found for paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib34" title="">34</a>]</cite>, reporting on developments on the ESPnet End-to-end Speech Processing toolkit. This paper reports on the inclusion of a Conformer (convolution-augmented transformer) architecture, and reports on multiple end-
to-end speech processing applications, as should be reflected by 25 automated speech recognition corpora, a speech translation corpus, a speech separation corpus, and three text-to-speech corpora. However, the graph following from our research unveils that these datasets have substantial overlaps. For example, Aurora-4, 4th CHiME and WSJ0-2/3mix all come from the WSJ0 dataset. Likewise, half of TED-LIUM Release 3 is the same as all of the TED-LIUMv2 dataset, and the AISHELL-1 dataset was created by modifying HKUST Mandarin Telephone Transcript Data. Comparing the data resources, they also have considerable disbalances. For example, JSUT represents 10 hours of one woman’s voice recording, while LibriSpeech ASR corpus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib63" title="">63</a>]</cite> represents the sound data gathered from an audio book website having more than 100 speakers with long content. This can lead to undesired biases to be passed on to a model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib1" title="">1</a>]</cite>. Next to this, it is striking that ‘multi-lingual’ speech datasets heavily overrepresent English language. No considerations of bias mitigation were discussed. Thus, while the toolkit and the paper are meant to evidence generalizable applicability of the toolkit, questions can be raised on how generalizable the outcomes really are.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="145" id="S3.F4.g1" src="https://arxiv.org/html/2410.03676v1/extracted/5866097/Pap4.png" width="299">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Dataset connections for&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib35" title="">35</a>]</cite></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">Paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib35" title="">35</a>]</cite> presents FastPitch, a fully-parallel text-to-speech model based on FastSpeech. The corresponding graph is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#S3.F4" title="Figure 4 ‣ III Results and discussion ‣ A quest through interconnected datasets: lessons from highly-cited ICASSP papers"><span class="ltx_text ltx_ref_tag">4</span></a>. The model is trained on the LJSpeech dataset, and subsequently makes use of Amazon Mechanical Turk for algorithm evaluations. The LJSpeech dataset contains e-book voice recordings by Linda Johnson, to which Keith Ito aligned text and the speech data manually. While we initially could not fully obtain information on this dataset from published text, this was the one case in which we had fruitful and active direct interaction after reaching out to the original creator (Keith Ito), who was easily reachable and promptly responded to additional inquiries informing our analysis.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="158" id="S3.F5.g1" src="https://arxiv.org/html/2410.03676v1/extracted/5866097/Pap5.png" width="299">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Dataset connections for&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib36" title="">36</a>]</cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.1">Finally, paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib36" title="">36</a>]</cite> describes the application of the RawNet2 deep neural classifier to anti-spoofing in automated speaker verification. The paper mentions using the ASVspoof 2019 logical access (LA) dataset. Looking into this dataset, sparse information is given on its origins. It is derived from the VCTK dataset, and contains bona fide speech and spoofed speech data generated using 17 different text-to-speech and voice conversion systems. The systems used for this generation allegedly also are trained on VCTK data, although it is explicitly mentioned this data does not overlap with data in the ASVspoof 2019 LA dataset. While specification of these 17 systems is reported in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib79" title="">79</a>]</cite>, as usual in the domain, the focus most strongly is on describing architectural components, while less attention is paid to explicitizing whether these architectural components may themselves have been trained on data. We noticed explicit mentioning of WaveNet and TacoTron2. WaveNet originally was presented in a more general-purpose way, and as such also incorporates knowledge on (likely copyrighted) music audio. The relations we could trace down are displayed in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#S3.F5" title="Figure 5 ‣ III Results and discussion ‣ A quest through interconnected datasets: lessons from highly-cited ICASSP papers"><span class="ltx_text ltx_ref_tag">5</span></a>, but it is likely that a full overview will be more elaborate.
Ultimately, the ASVspoof 2019 LA dataset has been presented as a benchmark dataset and common public resource; as such, it indeed is frequently cited today, although our discussion above once more raises questions on what actually is in the data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Conclusions and future work</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this article, we presented a depth-first inquiry into the origins of datasets used in the top-5 cited papers of ICASSP 2021 and 2022. In this, we found noteworthy information on interconnected dependencies, disbalances and unclear origins for the datasets used. This raises questions on the validity and integrity of outcomes trained on these datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Our current work does have several limitations. Because of the substantial amount of deeper searching that was needed to truly get down into the datasets’ origins, we only managed surveying five papers. In line with existing surveys&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib23" title="">23</a>]</cite>, in this, we encountered a large degree of implicit reporting of dataset origins and annotation practices. This makes it likely that more links may exist than we reported. Furthermore, the initial search and annotation, most of the verification, as well as the visualizations, were carried out by one researcher (the second author), while independent verification by another researcher (the first author) was only done in case of doubt. As such, the present work may carry annotator bias itself, and our current illustrations of dataset connections should not be read as final and complete overviews, but rather as first information to what data at least may lie underneath a given contribution.
In releasing our annotations with this manuscript&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib90" title="">90</a>]</cite>, further interested researchers can independently re-verify them, and possibly expand on them.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">We do wish to acknowledge that in research on audio, speech and music (as well as broader multimedia), data often is copyrighted. As such, only few usable resources may be available, and researchers may for pragmatic reasons be highly incentivized to resort to conveniently available datasets. As we showed in our analysis, this may be problematic. Interestingly, for the papers meant to provide more common shared resources (the end-to-end toolkit in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib34" title="">34</a>]</cite> and benchmarking dataset used in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib35" title="">35</a>]</cite>), it was more difficult to trace down to dataset origins. Furthermore, while these push towards more generalizable application, still, there always are articulated tasks of primary interest. In such situations, apart from data being larger, having been used for more applications, or being used in state-of-the-art work considered to be trained for general-purpose applications, there is very little explicit justification on whether this aligns with the primarily intended tasks at hand. As has already been raised in literature, it is highly questionable whether datasets aimed at a lot of generic tasks will actually turn out the right choice for application in multiple specialized tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib91" title="">91</a>]</cite>. Furthermore, it may generally be the case that datasets required for general tasks will require an intractable amount of resources to carefully collect and curate in ways that are representative of general phenomena, rather than specific ones.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Once again, this historically has not been a focus of interest in our fields. Additionally, many canonical datasets that powered the renewed interest in deep learning (e.g.&nbsp;ImageNet, LAION and the Million Song Dataset) are becoming more controversial today&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib92" title="">92</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib93" title="">93</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib94" title="">94</a>]</cite>. Questions arise on whether their content was actually justified and verified, and during the times of dataset and label acquisition, awareness lacked on whether there actually was consent on its use and resharing. Furthermore, resharing practice itself may in situations of copyright only be limited to more privileged parties.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">In the current pull towards larger (and possibly generative) machine learning models, again, there is increased demand for more data, while the origins of this data and trustworthiness of corresponding ground-truth annotations may not always be as clear or well-documented. In intended applications, any necessary annotations and outcomes will furthermore to a large degree rely on human judgement.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">With this, apart from focusing on more prediction<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>or, finding ‘more’ and larger data for broader reuse, while prioritizing size and convenient availability over purposeful quality control</span></span></span>, we strongly call on the applied machine learning research communities to create more room and incentive for purposeful attention to data provenance and annotation quality in academic publishing. We need to set clearer academic integrity examples that normalize paying attention to this in our peer communities. This will need dedicated community effort, that ideally should be backed by senior colleagues, and link to well-established publication venues. Such efforts will naturally lead to more trustworthy bases to depart from—and as such, stronger scientific foundations to the results of our research.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1">We have seen successful precedents in the establishment of dedicated benchmarking efforts (e.g.&nbsp;the MediaEval Benchmarking Initiative for Multimedia Evaluation<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://multimediaeval.org/" title="">http://multimediaeval.org/</a>, accessed July 21, 2024.</span></span></span> and the Datasets and Benchmarks track at NeurIPS), and increasing attention to data-centric AI. As examples of concrete possible actions, we can e.g.&nbsp;think of focused calls or special sessions to dig more deeply into the origins of commonly used datasets, institutionalized extensions to current checklists and documentation protocols&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03676v1#bib.bib18" title="">18</a>]</cite> with focus on data provenance and ground-truthing procedures, and activities to create better datasets and more robust ground-truth annotations. Alternatively, if applied machine learning communities consider the process of ground-truthing out of scope, it may need to be established as a dedicated academic field.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p8">
<p class="ltx_p" id="S4.p8.1">Finally, we wish to point out that attention to good practices in dealing with human subjectivity in data acquisition and annotation is a core concern in psychology education, but lacking in computing curricula. Many examples in introductory machine learning courses depart from theoretical datasets (e.g.&nbsp;synthesized data drawn from known distributions, where parameters are known and controllable), or relate to examples from the natural sciences (e.g.&nbsp;iris petal length measurements, where increasing the sample size will indeed be sufficient to get better measurements). However, in reality, many real-world applications will have degrees of ambiguity and reliance on subjective human judgement. As such, we plead for more attention to this in computing education, so future generations of professionals will have sufficient awareness of this.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">CRediT author statement</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Following the CRediT Contributor Roles Taxonomy, we list author contributions in descending order of degree of contribution: <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.1">Conceptualization</span> AD, DT, CL; <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.2">data curation</span> DT; <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.3">formal analysis</span> DT; <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.4">investigation</span> DT; <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.5">literature review</span> DT, CL; <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.6">methodology</span> DT, AD, CL; <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.7">project administration</span> DT, AD; <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.8">supervision</span> AD, CL; <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.9">validation</span> CL, DT; <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.10">visualization</span> DT; <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.11">writing - original draft</span> CL, DT; <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.12">writing - review and editing</span> CL, DT, AD.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Jain, H.&nbsp;Patel, L.&nbsp;Nagalapatti, N.&nbsp;Gupta, S.&nbsp;Mehta, S.&nbsp;Guttula, S.&nbsp;Mujumdar,
S.&nbsp;Afzal, R.&nbsp;Sharma&nbsp;Mittal, and V.&nbsp;Munigala, “Overview and importance of
data quality for machine learning tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>,
2020, pp. 3561–3562.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Gabelica, R.&nbsp;Bojčić, and L.&nbsp;Puljak, “Many researchers were not
compliant with their published data sharing statement: a mixed-methods
study,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Journal of Clinical Epidemiology</em>, vol. 150, pp. 33–41, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Thyagarajan, E.&nbsp;Snorrason, C.&nbsp;Northcutt, and J.&nbsp;Mueller, “Identifying
incorrect annotations in multi-label classification data,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv
preprint arXiv:2211.13895</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;S. Geiger, K.&nbsp;Yu, Y.&nbsp;Yang, M.&nbsp;Dai, J.&nbsp;Qiu, R.&nbsp;Tang, and J.&nbsp;Huang, “Garbage
in, garbage out? Do machine learning application papers in social computing
report where human-labeled training data comes from?” in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings
of the 2020 Conference on Fairness, Accountability, and Transparency</em>, 2020,
pp. 325–336. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/abs/10.1145/3351095.3372862" title="">https://dl.acm.org/doi/abs/10.1145/3351095.3372862</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Sambasivan, S.&nbsp;Kapania, H.&nbsp;Highfill, D.&nbsp;Akrong, P.&nbsp;Paritosh, and L.&nbsp;Aroyo,
““Everyone wants to do the model work, not the data work”: Data Cascades
in High-Stakes AI,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the CHI Conference on Human
Factors in Computing Systems (CHI ’21)</em>.&nbsp;&nbsp;&nbsp;ACM, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Birhane, P.&nbsp;Kalluri, D.&nbsp;Card, W.&nbsp;Agnew, R.&nbsp;Dotan, and M.&nbsp;Bao, “The Values
Encoded in Machine Learning Research,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 2022 ACM
Conference on Fairness, Accountability, and Transparency (FAccT ’22)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Luccioni and K.&nbsp;Crawford, “The Nine Lives of ImageNet: A Sociotechnical
Retrospective of a Foundation Dataset and the Limits of Automated
Essentialism,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Journal of Data-centric Machine Learning Research</em>,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Lee, “The Toxic Culture of Rejection in Computer Science,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://sigbed.org/2022/08/22/the-toxic-culture-of-rejection-in-computer-science/" title="">https://sigbed.org/2022/08/22/the-toxic-culture-of-rejection-in-computer-science/</a>,
August 2022, SIGBED Blog.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;M. Furr and V.&nbsp;R. Bacharach, <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Psychometrics : an introduction</em>,
second edition&nbsp;ed.&nbsp;&nbsp;&nbsp;SAGE Publications,
2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;M. Groves, F.&nbsp;J. Fowler&nbsp;Jr, M.&nbsp;P. Couper, J.&nbsp;M. Lepkowski, E.&nbsp;Singer, and
R.&nbsp;Tourangeau, <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Survey methodology</em>.&nbsp;&nbsp;&nbsp;John Wiley &amp; Sons, 2009, vol. 561.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Vazire, “Implications of the credibility revolution for productivity,
creativity, and progress,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Perspectives on Psychological Science</em>,
vol.&nbsp;13, no.&nbsp;4, pp. 411–417, 2018, pMID: 29961410. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1177/1745691617751884" title="">https://doi.org/10.1177/1745691617751884</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Pustejovsky and A.&nbsp;Stubbs, <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Natural language annotation for machine
learning </em>, first edition&nbsp;ed.&nbsp;&nbsp;&nbsp;O’Reilly Media, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Welty, P.&nbsp;K. Paritosh, and L.&nbsp;Aroyo, “Metrology for AI: from benchmarks
to instruments,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:1911.01875</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Z. Jacobs and H.&nbsp;Wallach, “Measurement and fairness,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">FAccT ’21:
2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;S. Jo and T.&nbsp;Gebru, “Lessons from archives: Strategies for collecting
sociocultural data in machine learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Conference on Fairness,
Accountability and Transparency (FAT ’20) January 27-30 2020, Barcelona,
Spain</em>.&nbsp;&nbsp;&nbsp;ACM, New York, NY, USA, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Rogers, T.&nbsp;Baldwin, and K.&nbsp;Leins, “‘Just What do You Think You’re
Doing, Dave?’ A Checklist for Responsible Data Use in NLP,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Findings of the Association for Computational Linguistics: EMNLP
2021</em>.&nbsp;&nbsp;&nbsp;Association for Computational
Linguistics, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Gebru, J.&nbsp;Morgenstern, B.&nbsp;Vecchione, J.&nbsp;W. Vaughan, H.&nbsp;Wallach, H.&nbsp;D. III,
and K.&nbsp;Crawford, “Datasheets for datasets,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Communications of the
ACM</em>, vol.&nbsp;64, no.&nbsp;12, p. 86–92, nov 2021. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3458723" title="">https://doi.org/10.1145/3458723</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Mitchell, S.&nbsp;Wu, A.&nbsp;Zaldivar, P.&nbsp;Barnes, L.&nbsp;Vasserman, B.&nbsp;Hutchinson,
E.&nbsp;Spitzer, I.&nbsp;D. Raji, and T.&nbsp;Gebru, “Model cards for model reporting,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the Conference on Fairness, Accountability, and
Transparency (FAT* ’19)</em>.&nbsp;&nbsp;&nbsp;ACM, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Rogers and A.&nbsp;S. Luccioni, “Position: Key Claims in LLM Research Have a
Long Tail of Footnotes,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 41st International
Conference on Machine Learning</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Brown, B.&nbsp;Mann, N.&nbsp;Ryder, M.&nbsp;Subbiah, J.&nbsp;D. Kaplan, P.&nbsp;Dhariwal,
A.&nbsp;Neelakantan, P.&nbsp;Shyam, G.&nbsp;Sastry, A.&nbsp;Askell, S.&nbsp;Agarwal, A.&nbsp;Herbert-Voss,
G.&nbsp;Krueger, T.&nbsp;Henighan, R.&nbsp;Child, A.&nbsp;Ramesh, D.&nbsp;Ziegler, J.&nbsp;Wu, C.&nbsp;Winter,
C.&nbsp;Hesse, M.&nbsp;Chen, E.&nbsp;Sigler, M.&nbsp;Litwin, S.&nbsp;Gray, B.&nbsp;Chess, J.&nbsp;Clark,
C.&nbsp;Berner, S.&nbsp;McCandlish, A.&nbsp;Radford, I.&nbsp;Sutskever, and D.&nbsp;Amodei, “Language
models are few-shot learners,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Advances in Neural Information
Processing Systems</em>, H.&nbsp;Larochelle, M.&nbsp;Ranzato, R.&nbsp;Hadsell, M.&nbsp;Balcan, and
H.&nbsp;Lin, Eds., vol.&nbsp;33.&nbsp;&nbsp;&nbsp;Curran
Associates, Inc., 2020, pp. 1877–1901. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;L. Sturm, “The State of the Art Ten Years After a State of the Art: Future
Research in Music Information Retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Journal of New Music
Research</em>, vol.&nbsp;43, pp. 147–172, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;C.&nbsp;S. Liem and C.&nbsp;Mostert, “Can’t trust the feeling? how open data reveals
unexpected behavior of high-level music descriptors,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings
of the 21st International Society for Music Information Retrieval
Conference</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;S. Geiger, D.&nbsp;Cope, J.&nbsp;Ip, M.&nbsp;Lotosh, A.&nbsp;Shah, J.&nbsp;Weng, and R.&nbsp;Tang,
““Garbage In, Garbage Out” Revisited: What Do Machine Learning
Application Papers Report About Human-Labeled Training Data?”
<em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Quantitative Science Studies</em>, vol.&nbsp;2, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Kern, S.&nbsp;Eckman, J.&nbsp;Beck, R.&nbsp;Chew, B.&nbsp;Ma, and F.&nbsp;Kreuter, “Annotation
sensitivity: Training data collection methods affect model performance,”
<em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2311.14212</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Beijing Shell Shell Technology Co., Ltd, “Open Source Mandarin Speech
Corpus [AISHELL-ASR0009-OS1] Training and Test Data,” online, Nov 2018,
documentation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Beijing DataTang Technology Co., Ltd, “AI data annotation data
customization Datatang — datatang.ai,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.datatang.ai/annotation" title="">https://www.datatang.ai/annotation</a>, date unknown, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
——, “Papers with Code - aidatatang 200zh Dataset
paperswithcode.com,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://paperswithcode.com/dataset/aidatatang-200zh" title="">https://paperswithcode.com/dataset/aidatatang-200zh</a>, 2010, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
SpeechOcean, “The largest open source Chinese corpus and another five
speech recognition datasets — en.speechocean.com,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.speechocean.com/Cy/778.html" title="">https://en.speechocean.com/Cy/778.html</a>, date unknown, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Beijing DataTang Technology Co., Ltd, “aidatatang 200zh,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://openslr.org/62/" title="">http://openslr.org/62/</a>, date unknown, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Sugito, K.&nbsp;Maekawa, H.&nbsp;Koiso, K.&nbsp;Nishikawa, and Y.&nbsp;Mabuchi, “Documents -
Corpus of Spontaneous Japanese — clrd.ninjal.ac.jp,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://clrd.ninjal.ac.jp/csj/en/document.html" title="">https://clrd.ninjal.ac.jp/csj/en/document.html</a>, 2006, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
National Institute for Japanese Language
(<span class="ltx_ERROR undefined" id="bib.bib31.1.1">{CJK*}</span>UTF8gbsn国立国語研究所), <em class="ltx_emph ltx_font_italic" id="bib.bib31.2.2">How to
build a corpus of spoken Japanese
(<span class="ltx_ERROR undefined" id="bib.bib31.2.2.1">{CJK*}</span>UTF8gbsn日本語話し言葉コーパスの構築法)</em>.&nbsp;&nbsp;&nbsp;National Institute for Japanese Language
(<span class="ltx_ERROR undefined" id="bib.bib31.3.3">{CJK*}</span>UTF8gbsn国立国語研究所), Mar 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Q.-L. Zhang and Y.-B. Yang, “Sa-net: Shuffle attention for deep convolutional
neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">ICASSP 2021-2021 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2021, pp. 2235–2239.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Subakan, M.&nbsp;Ravanelli, S.&nbsp;Cornell, M.&nbsp;Bronzi, and J.&nbsp;Zhong, “Attention is
all you need in speech separation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">ICASSP 2021-2021 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2021, pp. 21–25.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Guo, F.&nbsp;Boyer, X.&nbsp;Chang, T.&nbsp;Hayashi, Y.&nbsp;Higuchi, H.&nbsp;Inaguma, N.&nbsp;Kamo, C.&nbsp;Li,
D.&nbsp;Garcia-Romero, J.&nbsp;Shi, J.&nbsp;Shi, S.&nbsp;Watanabe, K.&nbsp;Wei, W.&nbsp;Zhang, and
Y.&nbsp;Zhang, “Recent developments on espnet toolkit boosted by conformer,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</em>.&nbsp;&nbsp;&nbsp;IEEE,
2021, pp. 5874–5878.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Łańcucki, “Fastpitch: Parallel text-to-speech with pitch
prediction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">ICASSP 2021-2021 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2021, pp. 6588–6592.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Tak, J.&nbsp;Patino, M.&nbsp;Todisco, A.&nbsp;Nautsch, N.&nbsp;Evans, and A.&nbsp;Larcher,
“End-to-end anti-spoofing with rawnet2,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">ICASSP 2021-2021 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2021, pp. 6369–6373.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Phillips, J.&nbsp;Glass, J.&nbsp;Polifroni, and V.&nbsp;Zue, “Collection and Analyses of
WSJ-CSR Data at MIT,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Speech and Natural Language: Proceedings of
a Workshop Held at Harriman, New York, February 23-26, 1992</em>, 1992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Du, X.&nbsp;Na, X.&nbsp;Liu, and H.&nbsp;Bu, “AISHELL-2: Transforming Mandarin ASR
Research Into Industrial Scale,” <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:1808.10583</em>,
2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Fung, S.&nbsp;Huang, and D.&nbsp;Graff, “HKUST Mandarin Telephone Speech, part 1,”
<em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">LDC2005S15. Web download</em>, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
——, “Mandarin Chinese Conversational Telephone Speech &amp; Transcripts,
PART 1,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://catalog.ldc.upenn.edu/docs/LDC2005S15/" title="">https://catalog.ldc.upenn.edu/docs/LDC2005S15/</a>, 2005,
[Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hong Kong University of Science and Technology, “Linguistic Data
Consortium Catalog,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://catalog.ldc.upenn.edu/docs/LDC2005S15/" title="">https://catalog.ldc.upenn.edu/docs/LDC2005S15/</a>, 2007, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Graff, S.&nbsp;Huang, I.&nbsp;Cartagena, K.&nbsp;Walker, and C.&nbsp;Cieri, “Fisher Spanish
Speech - Linguistic Data Consortium — catalog.ldc.upenn.edu,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://catalog.ldc.upenn.edu/LDC2010S01" title="">https://catalog.ldc.upenn.edu/LDC2010S01</a>, 2010, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
——, “Fisher spanish - transcripts,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://catalog.ldc.upenn.edu/LDC2010T04" title="">https://catalog.ldc.upenn.edu/LDC2010T04</a>, 2010, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Wheatley, “CALLHOME Spanish Transcripts - Linguistic
Data Consortium — catalog.ldc.upenn.edu,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://catalog.ldc.upenn.edu/docs/LDC96T17/" title="">https://catalog.ldc.upenn.edu/docs/LDC96T17/</a>, 1997, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
——, “CALLHOME Spanish Transcripts - Linguistic
Data Consortium — catalog.ldc.upenn.edu,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://catalog.ldc.upenn.edu/LDC96T17" title="">https://catalog.ldc.upenn.edu/LDC96T17</a>, 1997, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Sonobe, S.&nbsp;Takamichi, and H.&nbsp;Saruwatari, “JSUT corpus: free large-scale
Japanese speech corpus for end-to-end speech synthesis,” <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv
preprint arXiv:1711.00354</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;MacLean, “Free Speech… Recognition (Linux, Windows and Mac) -
voxforge.org,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.voxforge.org/" title="">https://www.voxforge.org/</a>, date unknown, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
VoxForge, “Papers with Code - VoxForge Dataset,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://paperswithcode.com/dataset/voxforge" title="">https://paperswithcode.com/dataset/voxforge</a>, 2019, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;McGuire, “LibriVox — free public domain audiobooks — librivox.org,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://librivox.org/" title="">https://librivox.org/</a>, 2005, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;J. Godfrey, E.&nbsp;C. Holliman, and J.&nbsp;McDaniel, “SWITCHBOARD: Telephone
speech corpus for research and development,” in <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Acoustics, Speech,
and Signal Processing, IEEE International Conference on</em>, vol.&nbsp;1.&nbsp;&nbsp;&nbsp;IEEE Computer Society, 1992, pp. 517–520.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
TED Conferences LLC, “translate transcribe — ted.com,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ted.com/participate/translate/transcribe" title="">https://www.ted.com/participate/translate/transcribe</a>, 2022, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;T.&nbsp;G. of&nbsp;Universitat Pompeu&nbsp;Fabra, “Freesound — freesound.org,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://freesound.org/" title="">https://freesound.org/</a>, 2005, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;A. Miller, R.&nbsp;Beckwith, C.&nbsp;Fellbaum, D.&nbsp;Gross, and K.&nbsp;J. Miller,
“Introduction to WordNet: An on-line lexical database,”
<em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">International journal of lexicography</em>, vol.&nbsp;3, no.&nbsp;4, pp. 235–244,
1990.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
LibriVox, “Linda johnson LibriVox — librivox.org,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://librivox.org/sections/readers/11049" title="">https://librivox.org/sections/readers/11049</a>, 2016, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Buckman, “Information: about Magnatune — magnatune.com,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://magnatune.com/info/" title="">http://magnatune.com/info/</a>, 2003, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
“Amazon Mechanical Turk — mturk.com,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mturk.com/" title="">https://www.mturk.com/</a>,
date unknown, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;R. Hershey, Z.&nbsp;Chen, J.&nbsp;Le&nbsp;Roux, and S.&nbsp;Watanabe, “Deep clustering:
Discriminative embeddings for segmentation and separation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">2016
IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2016, pp. 31–35.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;J. Godfrey and E.&nbsp;Holliman, “Switchboard-1 release 2,” <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Linguistic
Data Consortium, Philadelphia</em>, vol. 926, p. 927, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
——, “Switchboard-1 Release 2 - Linguistic Data Consortium —
catalog.ldc.upenn.edu,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://catalog.ldc.upenn.edu/LDC97S62" title="">https://catalog.ldc.upenn.edu/LDC97S62</a>, 1997,
[Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Rousseau, P.&nbsp;Deléglise, and Y.&nbsp;Estève, “Ted-liumv2,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openslr.magicdatatech.com/19/" title="">https://openslr.magicdatatech.com/19/</a>, 2014, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
——, “Enhancing the ted-lium corpus with selected data for language
modeling and more ted talks.” in <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">LREC</em>, 2014, pp. 3935–3939.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Pearce and J.&nbsp;Picone, “Aurora working group: DSR front end LVCSR
evaluation AU/384/02,” <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Inst. for Signal &amp; Inform. Process.,
Mississippi State Univ., Tech. Rep</em>, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Panayotov, G.&nbsp;Chen, D.&nbsp;Povey, and S.&nbsp;Khudanpur, “Librispeech: an ASR
corpus based on public domain audio books,” in <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">2015 IEEE
international conference on acoustics, speech and signal processing
(ICASSP)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2015, pp. 5206–5210.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Robinson, J.&nbsp;Fransen, D.&nbsp;Pye, J.&nbsp;Foote, and S.&nbsp;Renals, “WSJCAMO: a British
English speech corpus for large vocabulary continuous speech recognition,”
in <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">1995 International Conference on Acoustics, Speech, and Signal
Processing</em>, vol.&nbsp;1.&nbsp;&nbsp;&nbsp;IEEE, 1995, pp.
81–84.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Lincoln, I.&nbsp;McCowan, J.&nbsp;Vepa, and H.&nbsp;K. Maganti, “The multi-channel Wall
Street Journal audio visual corpus (MC-WSJ-AV): Specification and initial
experiments,” in <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">IEEE Workshop on Automatic Speech Recognition and
Understanding, 2005.</em>&nbsp;&nbsp;&nbsp;IEEE, 2005, pp.
357–362.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;L. Erich&nbsp;Zwyssig, “Mc-wsj-av corpus — catalog.ldc.upenn.edu,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://catalog.ldc.upenn.edu/docs/LDC2014S03/README.pdf" title="">https://catalog.ldc.upenn.edu/docs/LDC2014S03/README.pdf</a>, 2012,
[Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Ito and L.&nbsp;Johnson, “The LJ Speech Dataset,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://keithito.com/LJ-Speech-Dataset/" title="">https://keithito.com/LJ-Speech-Dataset/</a>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
“imagenet-1k · Datasets at Hugging Face — huggingface.co,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/imagenet-1k" title="">https://huggingface.co/datasets/imagenet-1k</a>, date unknown, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Deng, W.&nbsp;Dong, R.&nbsp;Socher, L.-J. Li, K.&nbsp;Li, and L.&nbsp;Fei-Fei, “ImageNet: A
large-scale hierarchical image database,” in <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">2009 IEEE Conference on
Computer Vision and Pattern Recognition</em>, 2009, pp. 248–255.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Post, G.&nbsp;Kumar, A.&nbsp;Lopez, D.&nbsp;Karakos, C.&nbsp;Callison-Burch, and S.&nbsp;Khudanpur,
“Fisher and CALLHOME Spanish–English speech translation,”
<em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">LDC2014T23. Web Download. Philadelphia: Linguistic Data Consortium</em>,
2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
——, “Improved speech-to-text translation with the Fisher and Callhome
Spanish–English speech translation corpus,” in <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Proceedings of the
International Workshop on Spoken Language Translation (IWSLT)</em>, Heidelberg,
Germany, December 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
——, “Linguistic Data Consortium Catalog —
catalog.ldc.upenn.edu,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://catalog.ldc.upenn.edu/docs/LDC2014T23/" title="">https://catalog.ldc.upenn.edu/docs/LDC2014T23/</a>, date unknown, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Hernandez, V.&nbsp;Nguyen, S.&nbsp;Ghannay, N.&nbsp;Tomashenko, and Y.&nbsp;Esteve, “TED-LIUM
3: Twice as much data and corpus repartition for experiments on speaker
adaptation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Speech and Computer: 20th International Conference,
SPECOM 2018, Leipzig, Germany, September 18–22, 2018, Proceedings 20</em>.&nbsp;&nbsp;&nbsp;Springer, 2018, pp. 198–208.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.-Y. Lin, M.&nbsp;Maire, S.&nbsp;Belongie, J.&nbsp;Hays, P.&nbsp;Perona, D.&nbsp;Ramanan,
P.&nbsp;Dollár, and C.&nbsp;L. Zitnick, “Microsoft COCO: Common objects in
context,” in <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>.&nbsp;&nbsp;&nbsp;Springer, 2014, pp. 740–755.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Bu, J.&nbsp;Du, X.&nbsp;Na, B.&nbsp;Wu, and H.&nbsp;Zheng, “AISHELL-1: An Open-Source Mandarin
Speech Corpus and A Speech Recognition Baseline,” in <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">2017 20th
conference of the oriental chapter of the international coordinating
committee on speech databases and speech I/O systems and assessment
(O-COCOSDA)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2017, pp. 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Chin, “The 4th CHiME Speech Separation and Recognition
Challenge — spandh.dcs.shef.ac.uk,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://spandh.dcs.shef.ac.uk/chime_challenge/CHiME4/" title="">https://spandh.dcs.shef.ac.uk/chime_challenge/CHiME4/</a>, 2016, [Accessed
25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Kinoshita, M.&nbsp;Delcroix, S.&nbsp;Gannot, E.&nbsp;A. P.&nbsp;Habets, R.&nbsp;Haeb-Umbach,
W.&nbsp;Kellermann, V.&nbsp;Leutnant, R.&nbsp;Maas, T.&nbsp;Nakatani, B.&nbsp;Raj <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">et&nbsp;al.</em>, “A
summary of the REVERB challenge: state-of-the-art and remaining challenges in
reverberant speech processing research,” <em class="ltx_emph ltx_font_italic" id="bib.bib77.2.2">EURASIP Journal on Advances
in Signal Processing</em>, vol. 2016, pp. 1–19, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Kinoshita, M.&nbsp;Delcroix, T.&nbsp;Yoshioka, and T.&nbsp;Nakatani, “The
REVERB challenge - Evaluating de-reverberation and ASR
techniques in reverberant environments — reverb2014.dereverberation.com,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://reverb2014.dereverberation.com/" title="">https://reverb2014.dereverberation.com</a>, 2014, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Wang, J.&nbsp;Yamagishi, M.&nbsp;Todisco, H.&nbsp;Delgado, A.&nbsp;Nautsch, N.&nbsp;Evans,
M.&nbsp;Sahidullah, V.&nbsp;Vestman, T.&nbsp;Kinnunen, K.&nbsp;A. Lee, L.&nbsp;Juvela, P.&nbsp;Alku, Y.-H.
Peng, H.-T. Hwang, Y.&nbsp;Tsao, H.-M. Wang, S.&nbsp;L. Maguer, M.&nbsp;Becker,
F.&nbsp;Henderson, R.&nbsp;Clark, Y.&nbsp;Zhang, Q.&nbsp;Wang, Y.&nbsp;Jia, K.&nbsp;Onuma, K.&nbsp;Mushika,
T.&nbsp;Kaneda, Y.&nbsp;Jiang, L.-J. Liu, Y.-C. Wu, W.-C. Huang, T.&nbsp;Toda, K.&nbsp;Tanaka,
H.&nbsp;Kameoka, I.&nbsp;Steiner, D.&nbsp;Matrouf, J.-F. Bonastre, A.&nbsp;Govender, S.&nbsp;Ronanki,
J.-X. Zhang, and Z.-H. Ling, “ASVspoof 2019: A large-scale public database
of synthesized, converted and replayed speech,” <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Computer Speech &amp;
Language</em>, vol.&nbsp;64, p. 101114, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Yamagishi, “— ASVspoof 2019 — asvspoof.org,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.asvspoof.org/index2019.html" title="">https://www.asvspoof.org/index2019.html</a>, 2019, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Nautsch, X.&nbsp;Wang, N.&nbsp;Evans, T.&nbsp;H. Kinnunen, V.&nbsp;Vestman, M.&nbsp;Todisco,
H.&nbsp;Delgado, M.&nbsp;Sahidullah, J.&nbsp;Yamagishi, and K.&nbsp;A. Lee, “ASVspoof 2019:
spoofing countermeasures for the detection of synthesized, converted and
replayed speech,” <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">IEEE Transactions on Biometrics, Behavior, and
Identity Science</em>, vol.&nbsp;3, no.&nbsp;2, pp. 252–265, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
“ResNet-50 convolutional neural network - MATLAB resnet50 -
MathWorks Benelux — nl.mathworks.com,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://nl.mathworks.com/help/deeplearning/ref/resnet50.html" title="">https://nl.mathworks.com/help/deeplearning/ref/resnet50.html</a>, date
unknown, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Ping, K.&nbsp;Peng, A.&nbsp;Gibiansky, S.&nbsp;O. Arik, A.&nbsp;Kannan, S.&nbsp;Narang, J.&nbsp;Raiman,
and J.&nbsp;Miller, “Deep voice 3: 2000-speaker neural text-to-speech,”
<em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">proc. ICLR</em>, pp. 214–217, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Povey, A.&nbsp;Ghoshal, G.&nbsp;Boulianne, L.&nbsp;Burget, O.&nbsp;Glembek, N.&nbsp;Goel,
M.&nbsp;Hannemann, P.&nbsp;Motlíček, Y.&nbsp;Qian, P.&nbsp;Schwarz, J.&nbsp;Silovský,
G.&nbsp;Stemmer, and K.&nbsp;Veselý, “The Kaldi speech recognition toolkit,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">IEEE 2011 workshop on automatic speech recognition and
understanding</em>.&nbsp;&nbsp;&nbsp;IEEE Signal Processing
Society, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Kakarla, “Kaldi hkust,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/kaldi-asr/kaldi/tree/master/egs/hkust" title="">https://github.com/kaldi-asr/kaldi/tree/master/egs/hkust</a>, 2016,
[Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Shen, R.&nbsp;Pang, R.&nbsp;J. Weiss, M.&nbsp;Schuster, N.&nbsp;Jaitly, Z.&nbsp;Yang, Z.&nbsp;Chen,
Y.&nbsp;Zhang, Y.&nbsp;Wang, R.&nbsp;Skerry-Ryan, R.&nbsp;A. Saurous, Y.&nbsp;Agiomyrgiannakis, and
Y.&nbsp;Wu, “Natural TTS synthesis by conditioning WaveNet on Mel Spectrogram
predictions,” in <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">2018 IEEE international conference on acoustics,
speech and signal processing (ICASSP)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2018, pp. 4779–4783.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;v.&nbsp;d. Oord, S.&nbsp;Dieleman, H.&nbsp;Zen, K.&nbsp;Simonyan, O.&nbsp;Vinyals, A.&nbsp;Graves,
N.&nbsp;Kalchbrenner, A.&nbsp;Senior, and K.&nbsp;Kavukcuoglu, “WaveNet: A generative
model for raw audio,” <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">arXiv preprint arXiv:1609.03499</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;van&nbsp;den Oord and S.&nbsp;Dieleman, “WaveNet: A generative model for raw
audio — deepmind.com,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio" title="">https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio</a>,
2016, [Accessed 25-Jun-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Law and L.&nbsp;Von&nbsp;Ahn, “Input-agreement: a new mechanism for collecting data
using human computation games,” in <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems</em>, 2009, pp. 1197–1206.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Taşcılar, C.&nbsp;C.&nbsp;S. Liem, and A.&nbsp;M. Demetriou, “Supplemental
material for ‘A quest through interconnected datasets: lessons from
highly-cited ICASSP papers,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5281/zenodo.12789876" title="">https://doi.org/10.5281/zenodo.12789876</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;D. Raji, E.&nbsp;Denton, E.&nbsp;M. Bender, A.&nbsp;Hanna, and A.&nbsp;Paullada, “AI and the
Everything in the Whole Wide World Benchmark ,” in <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">NeurIPS 2021
Datasets and Benchmarks track</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Denton, A.&nbsp;Hanna, R.&nbsp;Amironesei, A.&nbsp;Smart, and H.&nbsp;Nicole, “On the genealogy
of machine learning datasets: A critical history of imagenet,” <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">Big
Data &amp; Society</em>, vol.&nbsp;8, no.&nbsp;2, 2021. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1177/20539517211035955" title="">https://doi.org/10.1177/20539517211035955</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Birhane, V.&nbsp;Prabhu, S.&nbsp;Han, vishnu Naresh&nbsp;Boddeti, and A.&nbsp;S. Luccioni,
“Into the LAION’s Den: Investigating Hate in Multimodal Datasets,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">NeurIPS 2023 Datasets and Benchmarks track</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Kim, K.&nbsp;Choi, M.&nbsp;Modrzejewski, and C.&nbsp;C.&nbsp;S. Liem, “The Biased Journey of
MSD AUDIO.ZIP,” <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">arXiv preprint arXiv:2308.16389</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body></html>