<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Progress Report: Towards European LLMs</title>
<!--Generated on Mon Sep 30 15:56:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.03730v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S1" title="In Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S2" title="In Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S3" title="In Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Pre-Training Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S4" title="In Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Multilingual Tokenization and Fertility Impact on Model Efficiency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S5" title="In Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Base Model</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S5.SS1" title="In 5 Base Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Model Architecture &amp; Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S5.SS2" title="In 5 Base Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Software</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S5.SS3" title="In 5 Base Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Training Infrastructure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6" title="In Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Instruction Tuned Model</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6.SS1" title="In 6 Instruction Tuned Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6.SS1.SSS0.Px1" title="In 6.1 Data ‣ 6 Instruction Tuned Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title">English</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6.SS1.SSS0.Px2" title="In 6.1 Data ‣ 6 Instruction Tuned Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title">Multilingual</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6.SS1.SSS0.Px3" title="In 6.1 Data ‣ 6 Instruction Tuned Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title">Translation to German</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6.SS2" title="In 6 Instruction Tuned Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Post-Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6.SS3" title="In 6 Instruction Tuned Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Multilingual Instruction Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7" title="In Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7.SS1" title="In 7 Results ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Performance on 21 European Languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7.SS2" title="In 7 Results ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Performance on Common European Languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7.SS3" title="In 7 Results ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Performance on Exclusive European Languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7.SS4" title="In 7 Results ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Comparison with Aya-23-8B on Common Languages</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S8" title="In Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#A1" title="In Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#A1.SS1" title="In Appendix A Appendix ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#A1.SS1.SSS1" title="In A.1 Data ‣ Appendix A Appendix ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.1 </span>Dumps</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#A1.SS2" title="In Appendix A Appendix ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Instruction-tuning Evaluation Results</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Progress Report: Towards European LLMs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Mehdi Ali<sup class="ltx_sup" id="id6.6.id1">1,2</sup><sup class="ltx_sup" id="id7.7.id2">†</sup> Michael Fromm<sup class="ltx_sup" id="id8.8.id3">1,2</sup><sup class="ltx_sup" id="id9.9.id4">†</sup> Klaudia Thellmann<sup class="ltx_sup" id="id10.10.id5">3</sup><sup class="ltx_sup" id="id11.11.id6">†</sup> Jan Ebert<sup class="ltx_sup" id="id12.12.id7">4</sup><sup class="ltx_sup" id="id13.13.id8">†</sup> Alexander Arno Weber<sup class="ltx_sup" id="id14.14.id9">1,2</sup><sup class="ltx_sup" id="id15.15.id10">†</sup>
<br class="ltx_break"/>Richard Rutmann<sup class="ltx_sup" id="id16.16.id11">1,2</sup> Charvi Jain<sup class="ltx_sup" id="id17.17.id12">1,2</sup> Max Lübbering<sup class="ltx_sup" id="id18.18.id13">1,2</sup> Daniel Steinigen<sup class="ltx_sup" id="id19.19.id14">1</sup> Johannes Leveling<sup class="ltx_sup" id="id20.20.id15">1</sup> Katrin Klug<sup class="ltx_sup" id="id21.21.id16">1</sup>
<br class="ltx_break"/>Jasper Schulze Buschhoff<sup class="ltx_sup" id="id22.22.id17">1</sup> Lena Jurkschat<sup class="ltx_sup" id="id23.23.id18">3</sup> Hammam Abdelwahab<sup class="ltx_sup" id="id24.24.id19">1</sup> Benny Jörg Stein<sup class="ltx_sup" id="id25.25.id20">1</sup>
<br class="ltx_break"/>Karl-Heinz<sup class="ltx_sup" id="id26.26.id21">1</sup> Sylla Pavel Denisov<sup class="ltx_sup" id="id27.27.id22">1</sup> Nicolo Brandizzi<sup class="ltx_sup" id="id28.28.id23">1</sup> Qasid Saleem<sup class="ltx_sup" id="id29.29.id24">1</sup> Bhowmick Anirban<sup class="ltx_sup" id="id30.30.id25">1</sup>
<br class="ltx_break"/>Chelsea John<sup class="ltx_sup" id="id31.31.id26">4</sup> Pedro Ortiz Suarez<sup class="ltx_sup" id="id32.32.id27">5</sup> Malte Ostendorff<sup class="ltx_sup" id="id33.33.id28">5</sup> Alex Jude<sup class="ltx_sup" id="id34.34.id29">1</sup> Lalith Manjunath<sup class="ltx_sup" id="id35.35.id30">3</sup>
<br class="ltx_break"/>Samuel Weinbach<sup class="ltx_sup" id="id36.36.id31">7</sup> Carolin Penke<sup class="ltx_sup" id="id37.37.id32">4</sup> Shima Asaadi<sup class="ltx_sup" id="id38.38.id33">6</sup> Fabio Barth<sup class="ltx_sup" id="id39.39.id34">5</sup> Rafet Sifa<sup class="ltx_sup" id="id40.40.id35">1</sup> Fabian Küch<sup class="ltx_sup" id="id41.41.id36">6</sup>
<br class="ltx_break"/>René Jäkel<sup class="ltx_sup" id="id42.42.id37">5</sup> Georg Rehm<sup class="ltx_sup" id="id43.43.id38">5</sup> Stefan Kesselheim<sup class="ltx_sup" id="id44.44.id39">4</sup> Joachim Köhler<sup class="ltx_sup" id="id45.45.id40">1</sup> Nicolas Flores-Herr<sup class="ltx_sup" id="id46.46.id41">1</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id47.47.id42">1</sup>Fraunhofer IAIS <sup class="ltx_sup" id="id48.48.id43">2</sup>Lamarr Institute
<sup class="ltx_sup" id="id49.49.id44">3</sup>TU Dresden <sup class="ltx_sup" id="id50.50.id45">4</sup>FZ Jülich <sup class="ltx_sup" id="id51.51.id46">5</sup>DFKI
<sup class="ltx_sup" id="id52.52.id47">6</sup>Fraunhofer IIS <sup class="ltx_sup" id="id53.53.id48">7</sup>Aleph Alpha
<br class="ltx_break"/><span class="ltx_note ltx_role_thanks" id="id54.54.id49"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>†Equal contribution.</span></span></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id55.id1">We present preliminary results of the project OpenGPT-X. At present, the project has developed two multilingual <span class="ltx_glossaryref" title="Large Language Model"><span class="ltx_text ltx_glossary_long-plural">Large Language Models</span></span> designed to embrace Europe’s linguistic diversity by supporting all 24 official languages of the European Union.
Trained on a dataset comprising around 60% non-English data and utilizing a custom multilingual tokenizer, our models address the limitations of existing <abbr class="ltx_glossaryref" title="Large Language Model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr> that predominantly focus on English or a few high-resource languages.
We detail the models’ development principles, data processing techniques, tokenizer optimization, and training methodologies.
The models demonstrate competitive performance across multilingual benchmarks, as evidenced by its performance on European versions of ARC, HellaSwag, MMLU, and TruthfulQA.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.5">
<p class="ltx_p" id="p1.5.6"><span class="ltx_text ltx_font_bold" id="p1.5.6.1">Progress Report: Towards European LLMs</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.5.5" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.5.5.5" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.5.5.5.5">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.5.5"><span class="ltx_text ltx_font_bold" id="p1.5.5.5.5.5.5.1">
</span>Mehdi Ali<sup class="ltx_sup" id="p1.5.5.5.5.5.5.2">1,2</sup><sup class="ltx_sup" id="p1.5.5.5.5.5.5.3">†</sup> Michael Fromm<sup class="ltx_sup" id="p1.5.5.5.5.5.5.4">1,2</sup><sup class="ltx_sup" id="p1.5.5.5.5.5.5.5">†</sup> Klaudia Thellmann<sup class="ltx_sup" id="p1.5.5.5.5.5.5.6">3</sup><sup class="ltx_sup" id="p1.5.5.5.5.5.5.7">†</sup> Jan Ebert<sup class="ltx_sup" id="p1.5.5.5.5.5.5.8">4</sup><sup class="ltx_sup" id="p1.5.5.5.5.5.5.9">†</sup> Alexander Arno Weber<sup class="ltx_sup" id="p1.5.5.5.5.5.5.10">1,2</sup><sup class="ltx_sup" id="p1.5.5.5.5.5.5.11">†</sup></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.6.1">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.6.1.1">Richard Rutmann<sup class="ltx_sup" id="p1.5.5.5.5.6.1.1.1">1,2</sup> Charvi Jain<sup class="ltx_sup" id="p1.5.5.5.5.6.1.1.2">1,2</sup> Max Lübbering<sup class="ltx_sup" id="p1.5.5.5.5.6.1.1.3">1,2</sup> Daniel Steinigen<sup class="ltx_sup" id="p1.5.5.5.5.6.1.1.4">1</sup> Johannes Leveling<sup class="ltx_sup" id="p1.5.5.5.5.6.1.1.5">1</sup> Katrin Klug<sup class="ltx_sup" id="p1.5.5.5.5.6.1.1.6">1</sup></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.7.2">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.7.2.1">Jasper Schulze Buschhoff<sup class="ltx_sup" id="p1.5.5.5.5.7.2.1.1">1</sup> Lena Jurkschat<sup class="ltx_sup" id="p1.5.5.5.5.7.2.1.2">3</sup> Hammam Abdelwahab<sup class="ltx_sup" id="p1.5.5.5.5.7.2.1.3">1</sup> Benny Jörg Stein<sup class="ltx_sup" id="p1.5.5.5.5.7.2.1.4">1</sup></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.8.3">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.8.3.1">Karl-Heinz<sup class="ltx_sup" id="p1.5.5.5.5.8.3.1.1">1</sup> Sylla Pavel Denisov<sup class="ltx_sup" id="p1.5.5.5.5.8.3.1.2">1</sup> Nicolo Brandizzi<sup class="ltx_sup" id="p1.5.5.5.5.8.3.1.3">1</sup> Qasid Saleem<sup class="ltx_sup" id="p1.5.5.5.5.8.3.1.4">1</sup> Bhowmick Anirban<sup class="ltx_sup" id="p1.5.5.5.5.8.3.1.5">1</sup></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.9.4">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.9.4.1">Chelsea John<sup class="ltx_sup" id="p1.5.5.5.5.9.4.1.1">4</sup> Pedro Ortiz Suarez<sup class="ltx_sup" id="p1.5.5.5.5.9.4.1.2">5</sup> Malte Ostendorff<sup class="ltx_sup" id="p1.5.5.5.5.9.4.1.3">5</sup> Alex Jude<sup class="ltx_sup" id="p1.5.5.5.5.9.4.1.4">1</sup> Lalith Manjunath<sup class="ltx_sup" id="p1.5.5.5.5.9.4.1.5">3</sup></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.10.5">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.10.5.1">Samuel Weinbach<sup class="ltx_sup" id="p1.5.5.5.5.10.5.1.1">7</sup> Carolin Penke<sup class="ltx_sup" id="p1.5.5.5.5.10.5.1.2">4</sup> Shima Asaadi<sup class="ltx_sup" id="p1.5.5.5.5.10.5.1.3">6</sup> Fabio Barth<sup class="ltx_sup" id="p1.5.5.5.5.10.5.1.4">5</sup> Rafet Sifa<sup class="ltx_sup" id="p1.5.5.5.5.10.5.1.5">1</sup> Fabian Küch<sup class="ltx_sup" id="p1.5.5.5.5.10.5.1.6">6</sup></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.11.6">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.11.6.1">René Jäkel<sup class="ltx_sup" id="p1.5.5.5.5.11.6.1.1">5</sup> Georg Rehm<sup class="ltx_sup" id="p1.5.5.5.5.11.6.1.2">5</sup> Stefan Kesselheim<sup class="ltx_sup" id="p1.5.5.5.5.11.6.1.3">4</sup> Joachim Köhler<sup class="ltx_sup" id="p1.5.5.5.5.11.6.1.4">1</sup> Nicolas Flores-Herr<sup class="ltx_sup" id="p1.5.5.5.5.11.6.1.5">1</sup></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.12.7">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.12.7.1"><sup class="ltx_sup" id="p1.5.5.5.5.12.7.1.1">1</sup>Fraunhofer IAIS <sup class="ltx_sup" id="p1.5.5.5.5.12.7.1.2">2</sup>Lamarr Institute
<sup class="ltx_sup" id="p1.5.5.5.5.12.7.1.3">3</sup>TU Dresden <sup class="ltx_sup" id="p1.5.5.5.5.12.7.1.4">4</sup>FZ Jülich <sup class="ltx_sup" id="p1.5.5.5.5.12.7.1.5">5</sup>DFKI
<sup class="ltx_sup" id="p1.5.5.5.5.12.7.1.6">6</sup>Fraunhofer IIS <sup class="ltx_sup" id="p1.5.5.5.5.12.7.1.7">7</sup>Aleph Alpha</span></span>
<span class="ltx_tr" id="p1.5.5.5.5.13.8">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.13.8.1"><span class="ltx_note ltx_role_thanks" id="p1.5.5.5.5.13.8.1.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>†Equal contribution.</span></span></span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1"><abbr class="ltx_glossaryref" title="Large Language Model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr> represents a disruptive technology that has the potential to be applied in numerous applications.
Therefore, it is crucial that the technology and expertise to build these models is democratized to enable different communities and organizations to employ these models for their use cases.
Many efforts in developing open-source <abbr class="ltx_glossaryref" title="Large Language Model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr> have been undertaken, such as BLOOM <cite class="ltx_cite ltx_citemacro_cite">Scao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib17" title="">2022</a>)</cite>, LLaMA-3 <cite class="ltx_cite ltx_citemacro_cite">Dubey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib8" title="">2024</a>)</cite>, OLMo <cite class="ltx_cite ltx_citemacro_cite">Groeneveld et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib9" title="">2024</a>)</cite>, Aya <cite class="ltx_cite ltx_citemacro_cite">Üstün et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib21" title="">2024</a>)</cite>, and Mistral <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib11" title="">2023</a>)</cite>.
These can be split into efforts focusing on building monolingual English models and efforts focusing on multilingual models.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The current open-source models are predominantly English-centric, limiting their use in a multilingual context such as within the European Union.
Furthermore, the open-source efforts disclose different levels of granularity regarding sharing details about the development of the models.
While information regarding the model architecture is usually shared, the dataset composition and filtering are often not described in detail, hindering the reproduction of the works.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address the aforementioned limitations, we present our effort in developing a multilingual base model that has been trained on top of all 24 European official languages and the corresponding instruction-tuned model.
The results represent our preliminary results. An open-source publication of the models is planned for the near future.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Since the introduction of GPT-3 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib4" title="">2020</a>)</cite>, several open-source/open-weights efforts have been undertaken to train <abbr class="ltx_glossaryref" title="Large Language Model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr>.
While the large majority of the work focus on English-centric models <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib27" title="">2022</a>); Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib19" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib20" title="">b</a>); Groeneveld et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib9" title="">2024</a>); Dubey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib8" title="">2024</a>)</cite>, there have been also efforts training multilingual models.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">One of the most prominent examples is BLOOM <cite class="ltx_cite ltx_citemacro_cite">Scao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib17" title="">2022</a>)</cite>, a 176B <abbr class="ltx_glossaryref" title="Large Language Model"><span class="ltx_text ltx_glossary_short">LLM</span></abbr> trained on 46 natural languages.
Further examples that specifically address multilingualism are the encoder-decoder models mT5 <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib25" title="">2021</a>)</cite>, XLM <cite class="ltx_cite ltx_citemacro_cite">Lample and Conneau (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib13" title="">2019</a>)</cite>, XLM-R <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib6" title="">2020</a>)</cite>, and the encoder model mBERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib7" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Unlike the previously mentioned efforts, we specifically address 24 official European languages and focus on ensuring that a large fraction of the training data is composed of non-English data, representing a major step towards European <abbr class="ltx_glossaryref" title="Large Language Model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr>.
Concurrent to our work, EuroLLM <cite class="ltx_cite ltx_citemacro_cite">Martins et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib15" title="">2024</a>)</cite>, a 1.7B decoder-only <abbr class="ltx_glossaryref" title="Large Language Model"><span class="ltx_text ltx_glossary_short">LLM</span></abbr> that follows the same spirit as our undertaking by addressing all 24 European languages, has been presented.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Pre-Training Data</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our training dataset contains approximately 4 trillion tokens, of which 13.45% is curated data (cf. <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:curated</span>), while the remaining 86.55% (cf.  <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#A1.T8" title="In A.2 Instruction-tuning Evaluation Results ‣ Appendix A Appendix ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>) originates from web data.
We generated our web data by processing 60 WET dumps (cf.  <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#A1.SS1.SSS1" title="A.1.1 Dumps ‣ A.1 Data ‣ Appendix A Appendix ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.1.1</span></a>) from CommonCrawl with the Ungoliant pipeline <cite class="ltx_cite ltx_citemacro_cite">Abadji et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib1" title="">2022</a>)</cite>.
The resulting web documents were additionally filtered by removing all documents with <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">at least one</em> quality warning<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The Ungoliant pipeline annotates documents with the following warnings: <span class="ltx_text ltx_font_typewriter" id="footnote1.1">tiny</span>, <span class="ltx_text ltx_font_typewriter" id="footnote1.2">noisy</span>, <span class="ltx_text ltx_font_typewriter" id="footnote1.3">header</span>, <span class="ltx_text ltx_font_typewriter" id="footnote1.4">footer</span>, and <span class="ltx_text ltx_font_typewriter" id="footnote1.5">short_sentences</span>, see <cite class="ltx_cite ltx_citemacro_citet">Abadji et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib1" title="">2022</a>)</cite></span></span></span> and harmful perplexities <em class="ltx_emph ltx_font_italic" id="S3.p1.1.2">smaller than 5</em>.
We removed around 90% of the documents by filtering based on these annotations.
In a final step, we performed MinHash deduplication and further removed 50% of the filtered documents.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="414" id="S3.F1.g1" src="x1.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Language distribution of the tokenized dataset, comparing the presence of English and other European languages.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="42" id="S3.F2.g1" src="x2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Breakdown of the "OTHER" category.</figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Regarding language distribution, the dataset covers all 24 official European languages.
As illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S3.F1" title="In 3 Pre-Training Data ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S3.F2" title="In 3 Pre-Training Data ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>, 41.70% of the tokens stem from English content.
By additionally including German, French, and Spanish, we approach around two-thirds of the total tokens.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We found that there is a strong correlation between the volume of linguistic data and the population of the countries where these languages are spoken, with a correlation coefficient of 0.987 (<math alttext="P&lt;.001" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mrow id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">P</mi><mo id="S3.p3.1.m1.1.1.1" xref="S3.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><lt id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1"></lt><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">𝑃</ci><cn id="S3.p3.1.m1.1.1.3.cmml" type="float" xref="S3.p3.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">P&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_P &lt; .001</annotation></semantics></math>)<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Population data sourced from <a class="ltx_ref ltx_href" href="https://european-union.europa.eu/principles-countries-history/key-facts-and-figures/life-eu_en" title="">EU stats</a>, accessed in September 2024.</span></span></span>.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Multilingual Tokenization and Fertility Impact on Model Efficiency</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In multilingual natural language processing (NLP), it is crucial to train balanced multilingual tokenizers <cite class="ltx_cite ltx_citemacro_cite">Petrov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib16" title="">2023</a>); Ali et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib3" title="">2024</a>)</cite>.
English-centric tokenizers or unbalanced multilingual tokenizers affect inference costs and latency during inference for non-English queries.
Furthermore, it prevents the model from learning long-range dependencies in limited context windows <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib22" title="">2017</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">To address these limitations, we developed a custom multilingual tokenizer, closely following <cite class="ltx_cite ltx_citemacro_citet">Ali et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib3" title="">2024</a>)</cite>, that is optimized for all 24 official European languages.
It aims to reduce excessive text fragmentation. This phenomenon, termed as high "fertility", refers to the average number of tokens generated per word.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.3">Fertility (<math alttext="F" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mi id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">F</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">italic_F</annotation></semantics></math>) is defined as the ratio of the total number of tokens (<math alttext="T" class="ltx_Math" display="inline" id="S4.p3.2.m2.1"><semantics id="S4.p3.2.m2.1a"><mi id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.1d">italic_T</annotation></semantics></math>) to the total number of words (<math alttext="W" class="ltx_Math" display="inline" id="S4.p3.3.m3.1"><semantics id="S4.p3.3.m3.1a"><mi id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><ci id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">W</annotation><annotation encoding="application/x-llamapun" id="S4.p3.3.m3.1d">italic_W</annotation></semantics></math>) in a text, as shown in the following equation:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F=\frac{T}{W}" class="ltx_Math" display="block" id="S4.Ex1.m1.1"><semantics id="S4.Ex1.m1.1a"><mrow id="S4.Ex1.m1.1.1" xref="S4.Ex1.m1.1.1.cmml"><mi id="S4.Ex1.m1.1.1.2" xref="S4.Ex1.m1.1.1.2.cmml">F</mi><mo id="S4.Ex1.m1.1.1.1" xref="S4.Ex1.m1.1.1.1.cmml">=</mo><mfrac id="S4.Ex1.m1.1.1.3" xref="S4.Ex1.m1.1.1.3.cmml"><mi id="S4.Ex1.m1.1.1.3.2" xref="S4.Ex1.m1.1.1.3.2.cmml">T</mi><mi id="S4.Ex1.m1.1.1.3.3" xref="S4.Ex1.m1.1.1.3.3.cmml">W</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.1b"><apply id="S4.Ex1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1"><eq id="S4.Ex1.m1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1"></eq><ci id="S4.Ex1.m1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.2">𝐹</ci><apply id="S4.Ex1.m1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.3"><divide id="S4.Ex1.m1.1.1.3.1.cmml" xref="S4.Ex1.m1.1.1.3"></divide><ci id="S4.Ex1.m1.1.1.3.2.cmml" xref="S4.Ex1.m1.1.1.3.2">𝑇</ci><ci id="S4.Ex1.m1.1.1.3.3.cmml" xref="S4.Ex1.m1.1.1.3.3">𝑊</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.1c">F=\frac{T}{W}</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m1.1d">italic_F = divide start_ARG italic_T end_ARG start_ARG italic_W end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">We conducted a fertility analysis on 2,000 sentences from the FLORES-200 dataset to compare tokenizers.
Because the dataset is translated across languages, i.e., the analysis is conducted on semantic equivalent content, it provides a reliable basis for evaluation. A comparison with other widely-used tokenizers is presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S4.F3" title="In 4 Multilingual Tokenization and Fertility Impact on Model Efficiency ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Our custom tokenizer demonstrates that for 19 out of the 24 languages, fertility values are similar to or lower than those of related tokenizers.
This effect is especially pronounced in languages with complex morphology or long word structures, such as Finnish, German, and Hungarian.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">Lowering fertility enables longer queries and documents to be processed without exceeding the context window.
This is particularly advantageous in tasks that require the processing of legal or medical documents, where maintaining the integrity of long documents is essential for accurate understanding.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="974" id="S4.F3.g1" src="extracted/5890271/images/output-6.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Fertility across the official 24 European languages.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Base Model</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In the following, we describe the model architecture and training (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S5.SS1" title="5.1 Model Architecture &amp; Training ‣ 5 Base Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>), the used training framework (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S5.SS2" title="5.2 Software ‣ 5 Base Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>), and the training infrastructure (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S5.SS3" title="5.3 Training Infrastructure ‣ 5 Base Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Model Architecture &amp; Training</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Our model is a 7B transformer-based decoder-only model.
<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S5.T1" title="In 5.1 Model Architecture &amp; Training ‣ 5 Base Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> provides an overview of our model architecture.
We want to highlight some of the architectural choices that are derived from internal ablation studies and findings from related work.
Our models have a sequence length of 4096 tokens and employ Rotary <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib18" title="">2024</a>)</cite> positional embeddings that are employed to train state-of-the-art models <cite class="ltx_cite ltx_citemacro_cite">Dubey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib8" title="">2024</a>)</cite>.
To accelerate inference and reduce memory requirements, we employed grouped query attention <cite class="ltx_cite ltx_citemacro_cite">Ainslie et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib2" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Using the causal language modelling training objective, we trained our model on 4T tokens covering all 24 European languages as described in <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S3" title="3 Pre-Training Data ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a>.
As an optimizer, we employed AdamW and used a cosine learning rate schedule starting with learning rate of 3e-5, increasing it to the maximum learning rate of 3e-4 within the first 10,000 steps, and decaying it afterwards.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1">Hyper-Parameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.2.2.1">Training Objective</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.1.2.2.2">CLM</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.3.3.1">Activation Function</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.3.3.2">SwiGLU</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.4.4.1">Seq Length</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.4.4.2">4096</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.5.5.1">Position Embeddings</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.5.5.2">Rotary</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.6.6.1">Num Layers</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.6.6.2">32</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.7.7.1">Hidden Size</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.7.7.2">4096</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.8.8.1">FFN Hidden Size</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.8.8.2">13440</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.9.9.1">Num Attention Heads</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.9.9.2">32</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.10.10.1">Head Dim</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.10.10.2">128</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.11.11.1">Group Query Attention</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.11.11.2">yes</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.12.12.1">Num Query Groups</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.12.12.2">2</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.13.13.1">Normalization</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.13.13.2">RMSNorm</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.14.14.1">Learning rate</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.14.14.2">3e-4</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.15.15.1">Min learning rate</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.15.15.2">3e-5</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.16.16.1">Disable bias in linear</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.16.16.2">yes</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.17.17.1">Hidden dropout</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.17.17.2">0.0</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.18.18.1">Attention dropout</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.18.18.2">0.0</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.19.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.19.19.1">Optimizer</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.19.19.2">AdamW</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.20.20.1">Beta1</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.20.20.2">0.9</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.21.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.21.21.1">Beta2</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.21.21.2">0.95</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.22.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.22.22.1">Data-type</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.22.22.2">bf16</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.23.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.23.23.1">Recompute-activations</th>
<td class="ltx_td ltx_align_left" id="S5.T1.1.23.23.2">yes</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T1.1.24.24.1">Distributed-optimizers</th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T1.1.24.24.2">yes</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Hyper-Parameter Configuration</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Software</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We selected our training framework based on efficiency in terms of throughput (TFLOP/s) and maintenance.
Therefore, we decided to train our models based on a fork of MegatronLM <cite class="ltx_cite ltx_citemacro_cite">Korthikanti et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib12" title="">2023</a>)</cite> that supports various scalability features ensuring efficient training of transformer-based decoder-only models.
In particular, we made use of 3D parallelism, i.e., data, tensor, and pipeline parallelism.
Additionally, we used ZeRO <cite class="ltx_cite ltx_citemacro_cite">Korthikanti et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib12" title="">2023</a>)</cite> to reduce memory requirements further by sharding the optimizer state.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Training Infrastructure</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We trained our models on JUWELS Booster <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_href" href="https://jlsrf.org/index.php/lsf/article/view/171" title="">https://jlsrf.org/index.php/lsf/article/view/171</a></span></span></span> that comprises 936 compute nodes, each of which contains 4<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mo id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><times id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">×</annotation></semantics></math> NVIDIA A100 (40 GB) GPUs connected via NVLink3 intra-node, and through Mellanox HDR200 InfiniBand inter-node.
For our training runs, we utilized up to 512 GPUs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Instruction Tuned Model</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we describe the creation of the instruction tuned model, including the dataset composition (cf. <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6.SS1" title="6.1 Data ‣ 6 Instruction Tuned Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">6.1</span></a>), the instruction tuning (cf. <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6.SS2" title="6.2 Post-Training ‣ 6 Instruction Tuned Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">6.2</span></a>) and the evaluation (cf. <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6.SS3" title="6.3 Multilingual Instruction Evaluation ‣ 6 Instruction Tuned Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">6.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Data</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">The dataset for instruction tuning is composed of English data translated into German, English data, and multilingual data covering all 24 official European languages. The dataset is composed of several subsets, as listed in <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6.T2" title="In 6.1 Data ‣ 6 Instruction Tuned Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S6.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T2.1.1.1.1">Subset</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T2.1.1.1.2">Language</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T2.1.1.1.3">Count</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T2.1.1.1.4">Ø Turns</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.1.2.2.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/CohereForAI/aya_collection" title="">CohereForAI/aya_collection</a></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.1.2.2.2">Multilingual</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.2.2.3">39.9K</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.2.2.4">1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.3.3">
<td class="ltx_td ltx_align_left" id="S6.T2.1.3.3.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/MBZUAI/Bactrian-X" title="">MBZUAI/Bactrian-X</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.3.3.2">Multilingual</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.3.3.3">32.3K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.3.3.4">1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.4.4">
<td class="ltx_td ltx_align_left" id="S6.T2.1.4.4.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/CohereForAI/aya_dataset" title="">CohereForAI/aya_dataset</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.4.4.2">Multilingual</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.4.4.3">13.1K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.4.4.4">1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.1.5.5.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered" title="">anon8231489123/ShareGPT_Vicuna_unfiltered</a></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.1.5.5.2">EN</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.5.5.3">37.7K</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.5.5.4">4.6</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.6.6">
<td class="ltx_td ltx_align_left" id="S6.T2.1.6.6.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k" title="">sahil2801/CodeAlpaca-20k</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.6.6.2">EN</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.6.6.3">12.1K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.6.6.4">1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.7.7">
<td class="ltx_td ltx_align_left" id="S6.T2.1.7.7.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m" title="">lmsys/lmsys-chat-1m</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.7.7.2">EN</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.7.7.3">11.2K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.7.7.4">1.5</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.8.8">
<td class="ltx_td ltx_align_left" id="S6.T2.1.8.8.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k" title="">WizardLM/WizardLM_evol_instruct_V2_196k</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.8.8.2">EN</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.8.8.3">8.1K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.8.8.4">1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.9.9">
<td class="ltx_td ltx_align_left" id="S6.T2.1.9.9.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/Open-Orca/OpenOrca" title="">Open-Orca/OpenOrca</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.9.9.2">EN</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.9.9.3">8.1K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.9.9.4">1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.10.10">
<td class="ltx_td ltx_align_left" id="S6.T2.1.10.10.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_70k" title="">WizardLM/WizardLM_evol_instruct_70k</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.10.10.2">EN</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.10.10.3">8.1K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.10.10.4">1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.11.11">
<td class="ltx_td ltx_align_left" id="S6.T2.1.11.11.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k" title="">HuggingFaceH4/ultrachat_200k</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.11.11.2">EN</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.11.11.3">6.9K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.11.11.4">3.1</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.1.12.12.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered" title="">anon8231489123/ShareGPT_Vicuna_unfiltered</a></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.1.12.12.2">transl. DE</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.12.12.3">37.6K</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.12.12.4">4.6</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.13.13">
<td class="ltx_td ltx_align_left" id="S6.T2.1.13.13.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m" title="">lmsys/lmsys-chat-1m</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.13.13.2">transl. DE</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.13.13.3">11.2K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.13.13.4">1.5</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.14.14">
<td class="ltx_td ltx_align_left" id="S6.T2.1.14.14.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_70k" title="">WizardLM/WizardLM_evol_instruct_70k</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.14.14.2">transl. DE</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.14.14.3">8.1K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.14.14.4">1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.15.15">
<td class="ltx_td ltx_align_left" id="S6.T2.1.15.15.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/Open-Orca/OpenOrca" title="">Open-Orca/OpenOrca</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.15.15.2">transl. DE</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.15.15.3">8.1K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.15.15.4">1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.16.16">
<td class="ltx_td ltx_align_left" id="S6.T2.1.16.16.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k" title="">WizardLM/WizardLM_evol_instruct_V2_196k</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.16.16.2">transl. DE</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.16.16.3">8.1K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.16.16.4">1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.17.17">
<td class="ltx_td ltx_align_left" id="S6.T2.1.17.17.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k" title="">HuggingFaceH4/ultrachat_200k</a></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.17.17.2">transl. DE</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.17.17.3">6.9K</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.17.17.4">3.1</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.18.18">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.1.18.18.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/FreedomIntelligence/sharegpt-deutsch" title="">FreedomIntelligence/sharegpt-deutsch</a></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.1.18.18.2">DE</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.18.18.3">5.7K</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.18.18.4">2.9</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.19.19">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.1.19.19.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/bjoernp/ultrachat_de" title="">bjoernp/ultrachat_de</a></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.1.19.19.2">DE</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.1.19.19.3">905</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.1.19.19.4">1.0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Instruction-tuning training dataset composition.</figcaption>
</figure>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">English</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p1.1">To select high-quality English data, we employed a rigorous multi-step filtering process.
First, we computed reward scores for all English examples using Starling-RM-7B-alpha<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha" title="">https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha</a></span></span></span>. Our objective was to balance the dataset, aiming to include a similar number of English and multilingual examples. We achieved this by including all multi-turn examples, the complete code alpaca‘ subset, and the entire high-quality portion of the LMSYS-Chat dataset subset. For the remaining subsets — Open Orca and both versions of the Evol instruct datasets — we selected the highest-scoring examples to ensure that each subset contributed an equal amount of high-quality data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Multilingual</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px2.p1.1">For the multilingual data selection, we incorporated several datasets to ensure broad linguistic coverage. We included data from the 17 official EU languages present in the Bactrian-X dataset, as <cite class="ltx_cite ltx_citemacro_citet">Weber et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib23" title="">2024</a>)</cite> demonstrated that semantically parallel data enhances cross-lingual model performance. Additionally, we integrated the 14 official EU languages from the Aya Dataset and the 21 official EU languages from the translated FLAN-CoT subset of the Aya Collection. With this selection, we cover all 24 EU languages.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Translation to German</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px3.p1.1">Since large-scale German instruction-tuning data is less abundant, the English portion of the dataset was translated into German using the Alma-13B <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib24" title="">2024</a>)</cite> model. To preserve the integrity of code snippets during the translation process, a regex-based code detection method was implemented. This ensured that code was excluded from translation and then reinserted afterward, allowing for a faithful German version of the dataset.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Post-Training</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">For instruction-tuning the model, we extended FastChat<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/lm-sys/FastChat/tree/main/fastchat" title="">https://github.com/lm-sys/FastChat/tree/main/fastchat</a></span></span></span> for multilingual system prompts and translated the system prompt "<em class="ltx_emph ltx_font_italic" id="S6.SS2.p1.1.1">A chat between a human and an artificial intelligence assistant. The assistant gives helpful and polite answers to the human’s questions.</em>" for all 24 official EU languages. We employed standard instruction tuning by calculating the loss only for the assistant responses of a multi-turn conversation. We trained the model on 8xH100 GPUs for 2.5 days for three epochs.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Multilingual Instruction Evaluation</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">For evaluating the multilingual instruction-following capabilities of the model, we utilized MT-Bench-X <cite class="ltx_cite ltx_citemacro_cite">Weber et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib23" title="">2024</a>)</cite> across all five available evaluation languages: English, German, French, Italian, and Spanish. The results are presented in the <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#A1.SS2" title="A.2 Instruction-tuning Evaluation Results ‣ Appendix A Appendix ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.2</span></a>. While the creative writing skills of the model show good performance, the mathematical, coding, and reasoning skills are inferior across languages.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Results</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this section, we describe the results of our base and instruction-tuned models.
We focus on a multilingual evaluation because our effort targets multilingualism, especially in the official European languages.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">We conducted our evaluation based on ARC <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib5" title="">2018</a>)</cite>, HellaSwag <cite class="ltx_cite ltx_citemacro_cite">Zellers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib26" title="">2019</a>)</cite>, TruthfullQA <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib14" title="">2022</a>)</cite>, and MMLU <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib10" title="">2021</a>)</cite> which has been translated into 21 out of 24 official European languages providing a comprehensive assessment of the models’ capabilities.
We compared our models on these benchmarks against Aya-23-8B, LLaMA 3.1 8B, LLaMA 3.1 8B Instruct, Mistral 7B v0.3, and Mistral 7B Instruct v0.3. Note that the Mistral models compared to in this study were not trained as multilingual models and are intended to be used as English-only models. Results from other models are available on the European Leaderboard<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/openGPT-X/european-llm-leaderboard" title="">https://huggingface.co/spaces/openGPT-X/european-llm-leaderboard</a></span></span></span>.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7.SS1" title="7.1 Performance on 21 European Languages ‣ 7 Results ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.1</span></a> presents our results across all investigated 21 languages, <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7.SS2" title="7.2 Performance on Common European Languages ‣ 7 Results ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2</span></a> discusses the performance of our models on the six widely spoken languages English, German, French, Italian, Spanish, and Portuguese, and <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7.SS3" title="7.3 Performance on Exclusive European Languages ‣ 7 Results ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.3</span></a> presents our results on the remaining 15 languages.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Performance on 21 European Languages</h3>
<figure class="ltx_table" id="S7.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S7.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T3.1.1.1.1">Model</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.1.1.2">Average</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.1.1.3">ARC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.1.1.4">HellaSwag</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.1.1.5">TruthfulQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.1.1.6">MMLU</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T3.1.2.2.1">Aya-23-8B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.2.2.2">.485</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.2.2.3">.475 ± .134</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.2.2.4">.535 ± .145</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.2.2.5">.476 ± .028</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.2.2.6">.455 ± .070</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T3.1.3.3.1">LLaMA 3.1 8B</th>
<td class="ltx_td ltx_align_center" id="S7.T3.1.3.3.2">.547</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.3.3.3">.554 ± .071</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.3.3.4">.588 ± .090</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.3.3.5">.495 ± .026</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.3.3.6">.556 ± .047</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T3.1.4.4.1">LLaMA 3.1 8B Instruct</th>
<td class="ltx_td ltx_align_center" id="S7.T3.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S7.T3.1.4.4.2.1">.563</span></td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.4.4.3">.563 ± .075</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.4.4.4">.579 ± .089</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.4.4.5">.532 ± .023</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.4.4.6"><span class="ltx_text ltx_font_bold" id="S7.T3.1.4.4.6.1">.576 ± .051</span></td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T3.1.5.5.1">Mistral 7B v0.3</th>
<td class="ltx_td ltx_align_center" id="S7.T3.1.5.5.2">.505</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.5.5.3">.513 ± .128</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.5.5.4">.534 ± .124</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.5.5.5">.472 ± .034</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.5.5.6">.501 ± .075</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T3.1.6.6.1">Mistral 7B Instruct v0.3</th>
<td class="ltx_td ltx_align_center" id="S7.T3.1.6.6.2">.527</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.6.6.3">.530 ± .136</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.6.6.4">.538 ± .129</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.6.6.5">
<span class="ltx_text ltx_font_bold" id="S7.T3.1.6.6.5.1">.548</span> ± .048</td>
<td class="ltx_td ltx_align_center" id="S7.T3.1.6.6.6">.490 ± .075</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S7.T3.1.7.7.1">Ours (Base)</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T3.1.7.7.2">.496</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T3.1.7.7.3">.550 ± .043</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T3.1.7.7.4">.615 ± .048</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T3.1.7.7.5">.469 ± .031</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T3.1.7.7.6">.349 ± .024</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S7.T3.1.8.8.1">Ours (Instruct)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T3.1.8.8.2">.537</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T3.1.8.8.3">
<span class="ltx_text ltx_font_bold" id="S7.T3.1.8.8.3.1">.576</span> ± .047</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T3.1.8.8.4">
<span class="ltx_text ltx_font_bold" id="S7.T3.1.8.8.4.1">.621</span> ± .052</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T3.1.8.8.5">.511 ± .019</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T3.1.8.8.6">.441 ± .022</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results on multilingual benchmarks for 21 European languages, including mean accuracy and corresponding standard deviations across languages.</figcaption>
</figure>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7.T3" title="In 7.1 Performance on 21 European Languages ‣ 7 Results ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we present the performance of various language models on multilingual benchmarks across 21 European languages.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">Ours (Instruct) demonstrates strong performance, particularly excelling in the ARC and HellaSwag benchmarks.
It achieves the highest accuracy on HellaSwag with 62.1%, outperforming all other models.
On the ARC benchmark, "Ours (Instruct)" also attains the top score of 57.6%, indicating superior reasoning and commonsense understanding in multilingual contexts.
This suggests that our instruction-tuning approach effectively enhances the model’s ability to handle diverse languages in complex reasoning tasks.</p>
</div>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1">However, Ours (Instruct) shows a lower performance on the MMLU benchmark, scoring 44.1%, which is below the scores of LLaMA 3.1 8B Instruct (57.6%) and LLaMA 3.1 8B (55.6%). This indicates that while our model excels in certain areas, there is room for improvement in domain-specific knowledge across multiple languages.
On TruthfulQA, "Ours (Instruct)" achieves a moderate score of 51.1%, which is competitive but still trails behind Mistral 7B Instruct v0.3 (54.8%) and LLaMA 3.1 8B Instruct (53.2%).</p>
</div>
<div class="ltx_para" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.1">The average score of "Ours (Instruct)" is 53.7%, placing it below LLaMA 3.1 8B Instruct (56.3%) but above Mistral 7B Instruct v0.3 (52.7%).
The standard deviations across the languages are relatively low for our models, suggesting more consistent performance across different European languages compared to models like Aya-23-8B and Mistral 7B variants, which exhibit higher variability.
This consistency is crucial for applications requiring reliable performance in multilingual settings.</p>
</div>
<div class="ltx_para" id="S7.SS1.p5">
<p class="ltx_p" id="S7.SS1.p5.1">In summary, "Ours (Instruct)" shows promising results, particularly in reasoning and commonsense tasks, and maintains consistent performance across languages. Further work is needed to enhance its domain-specific knowledge and truthfulness in responses to match or surpass the leading models in all benchmarks.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Performance on Common European Languages</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7.T4" title="In 7.2 Performance on Common European Languages ‣ 7 Results ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a> focuses on the performance of the models on six widely spoken European languages: English, German, French, Italian, Spanish, and Portuguese. These languages are well-represented in training data and are commonly used in evaluating multilingual models.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">Ours (Instruct) model achieves an average score of 56.9%, which is lower than the top-performing LLaMA 3.1 8B Instruct model at 62.3%.
However, it should be highlighted that our model has been trained based on around 4T tokens, which is significantly less data than LLaMA 3.1 8B, which has been trained on 15T tokens.
The training dataset sizes of the other models are not revealed.
Notably, "Ours (Instruct)" performs competitively on the HellaSwag benchmark, attaining 67.9%, which is slightly higher than LLaMA 3.1 8B Instruct (67.7%) and Mistral 7B Instruct v0.3 (67.0%).
This reinforces our model’s strength in understanding and generating coherent continuations in context-rich scenarios.</p>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1">On the ARC benchmark, "Ours (Instruct)" scores 63.2%, which is close to Mistral 7B Instruct v0.3 (65.4%) and LLaMA 3.1 8B Instruct (64.8%), indicating good performance in multiple-choice question answering.
However, in the MMLU benchmark, our model scores 46.5%, which is significantly lower than LLaMA 3.1 8B Instruct’s 63.2%. This suggests that our model may have limitations in specialized knowledge domains within these common languages.</p>
</div>
<div class="ltx_para" id="S7.SS2.p4">
<p class="ltx_p" id="S7.SS2.p4.1">For TruthfulQA, "Ours (Instruct)" achieves 49.9%, below Mistral 7B Instruct v0.3’s 56.8% and LLaMA 3.1 8B Instruct’s 53.5%.
This indicates that while our model is proficient in generating plausible responses, it may sometimes sacrifice factual accuracy, highlighting an area for improvement in ensuring truthfulness.</p>
</div>
<div class="ltx_para" id="S7.SS2.p5">
<p class="ltx_p" id="S7.SS2.p5.1">Overall, "Ours (Instruct)" demonstrates strong capabilities in certain tasks within common European languages but falls short in others compared to the leading models. Enhancing its domain-specific knowledge and factual accuracy could help bridge this performance gap.</p>
</div>
<figure class="ltx_table" id="S7.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S7.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T4.1.1.1.1">Model</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.1.1.2">Average</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.1.1.3">ARC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.1.1.4">HellaSwag</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.1.1.5">TruthfulQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.1.1.6">MMLU</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T4.1.2.2.1">Aya-23-8B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.2.2.2">.574</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.2.2.3">.614 ± .029</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.2.2.4">
<span class="ltx_text ltx_font_bold" id="S7.T4.1.2.2.4.1">.687</span> ± .047</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.2.2.5">.470 ± .023</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.2.2.6">.526 ± .019</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T4.1.3.3.1">Meta-Llama-3.1-8B</th>
<td class="ltx_td ltx_align_center" id="S7.T4.1.3.3.2">.601</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.3.3.3">.634 ± .042</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.3.3.4">
<span class="ltx_text ltx_font_bold" id="S7.T4.1.3.3.4.1">.687</span> ± .067</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.3.3.5">.474 ± .015</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.3.3.6">.607 ± .027</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T4.1.4.4.1">Meta-Llama-3.1-8B-Instruct</th>
<td class="ltx_td ltx_align_center" id="S7.T4.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S7.T4.1.4.4.2.1">.623</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.4.4.3">.648 ± .045</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.4.4.4">.677 ± .064</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.4.4.5">.535 ± .016</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.4.4.6">
<span class="ltx_text ltx_font_bold" id="S7.T4.1.4.4.6.1">.632</span> ± .029</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T4.1.5.5.1">Mistral-7B-Instruct-v0.3</th>
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.5.2">.613</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.5.3">
<span class="ltx_text ltx_font_bold" id="S7.T4.1.5.5.3.1">.654</span> ± .050</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.5.4">.670 ± .088</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.5.5">
<span class="ltx_text ltx_font_bold" id="S7.T4.1.5.5.5.1">.568</span> ± .015</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.5.6">.560 ± .033</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T4.1.6.6.1">Mistral-7B-v0.3</th>
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.6.2">.580</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.6.3">.632 ± .048</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.6.4">.662 ± .084</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.6.5">.453 ± .016</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.6.6">.571 ± .031</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S7.T4.1.7.7.1">Ours (Base)</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T4.1.7.7.2">.521</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T4.1.7.7.3">.599 ± .038</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T4.1.7.7.4">.667 ± .042</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T4.1.7.7.5">.445 ± .017</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T4.1.7.7.6">.371 ± .023</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S7.T4.1.8.8.1">Ours (Instruct)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.8.8.2">.569</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.8.8.3">.632 ± .033</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.8.8.4">.679 ± .043</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.8.8.5">.499 ± .018</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.8.8.6">.465 ± .015</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results on multilingual benchmarks for six Languages (English, German, French, Italian, Spanish, Portuguese) including mean accuracy and corresponding standard deviations across languages.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Performance on Exclusive European Languages</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7.T5" title="In 7.3 Performance on Exclusive European Languages ‣ 7 Results ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> presents the models’ performance on 15 less commonly evaluated European languages, which we refer to as exclusive languages.
These include Romanian, Czech, Danish, Greek, Estonian, Finnish, Hungarian, Lithuanian, Latvian, Dutch, Bulgarian, Polish, Slovak, Slovenian, and Swedish.</p>
</div>
<div class="ltx_para" id="S7.SS3.p2">
<p class="ltx_p" id="S7.SS3.p2.1">In this setting, our "Ours (Instruct)" model is on average slightly behind LLaMA 3.1 8B Instruct mainly due to our performance on MMLU indicating that learning domain-specific knowledge in exclusive languages is an area for improvement.
At the same time, our model obtains the strongest results on the ARC (56.3%) and HellaSwag (60.8%) benchmarks among all evaluated models.
This suggests that our model is particularly effective in reasoning and understanding context in less commonly represented languages, which is a significant achievement given the challenges associated with limited training data in these languages.
On TruthfulQA, "Ours (Instruct)" attains 51.3%, which is competitive but still trails behind Mistral 7B Instruct v0.3’s 54.3%.</p>
</div>
<div class="ltx_para" id="S7.SS3.p3">
<p class="ltx_p" id="S7.SS3.p3.1">The standard deviations for our models are lower compared to others, reflecting more consistent performance across the exclusive languages.
This consistency is crucial for real-world applications where reliability across languages is necessary.</p>
</div>
<div class="ltx_para" id="S7.SS3.p4">
<p class="ltx_p" id="S7.SS3.p4.1">In conclusion, "Ours (Instruct)" exhibits excellent reasoning and contextual understanding in less commonly evaluated European languages, outperforming other models in specific benchmarks.
Enhancing its knowledge base and factual accuracy in these languages could further elevate its overall performance.</p>
</div>
<figure class="ltx_table" id="S7.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S7.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T5.1.1.1.1">Model</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.1.1.2">Average</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.1.1.3">ARC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.1.1.4">HellaSwag</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.1.1.5">TruthfulQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.1.1.6">MMLU</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T5.1.2.2.1">Aya-23-8B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.2.2.2">.460</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.2.2.3">.435 ± .130</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.2.2.4">.493 ± .142</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.2.2.5">.477 ± .030</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T5.1.2.2.6">.436 ± .070</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.3.3.1">Meta-Llama-3.1-8B</th>
<td class="ltx_td ltx_align_center" id="S7.T5.1.3.3.2">.536</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.3.3.3">.534 ± .070</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.3.3.4">.566 ± .092</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.3.3.5">.500 ± .028</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.3.3.6">.543 ± .047</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.4.4.1">Meta-Llama-3.1-8B-Instruct</th>
<td class="ltx_td ltx_align_center" id="S7.T5.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S7.T5.1.4.4.2.1">.548</span></td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.4.4.3">.542 ± .075</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.4.4.4">.556 ± .090</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.4.4.5">.531 ± .025</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.4.4.6">
<span class="ltx_text ltx_font_bold" id="S7.T5.1.4.4.6.1">.562</span> ± .051</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.5.5.1">Mistral-7B-Instruct-v0.3</th>
<td class="ltx_td ltx_align_center" id="S7.T5.1.5.5.2">.505</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.5.5.3">.497 ± .141</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.5.5.4">.508 ± .134</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.5.5.5">
<span class="ltx_text ltx_font_bold" id="S7.T5.1.5.5.5.1">.543</span> ± .055</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.5.5.6">.473 ± .078</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.6.6.1">Mistral-7B-v0.3</th>
<td class="ltx_td ltx_align_center" id="S7.T5.1.6.6.2">.486</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.6.6.3">.482 ± .132</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.6.6.4">.504 ± .128</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.6.6.5">.476 ± .038</td>
<td class="ltx_td ltx_align_center" id="S7.T5.1.6.6.6">.483 ± .078</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S7.T5.1.7.7.1">Ours (Base)</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T5.1.7.7.2">.491</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T5.1.7.7.3">.540 ± .045</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T5.1.7.7.4">.603 ± .050</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T5.1.7.7.5">.474 ± .034</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T5.1.7.7.6">.345 ± .026</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S7.T5.1.8.8.1">Ours (Instruct)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T5.1.8.8.2">.530</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T5.1.8.8.3">
<span class="ltx_text ltx_font_bold" id="S7.T5.1.8.8.3.1">.563</span> ± .046</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T5.1.8.8.4">
<span class="ltx_text ltx_font_bold" id="S7.T5.1.8.8.4.1">.608</span> ± .054</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T5.1.8.8.5">.513 ± .021</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T5.1.8.8.6">.436 ± .022</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Results on multilingual benchmarks across the 15 exclusive European languages (Romanian, Czech, Danish, Greek, Estonian, Finnish, Hungarian, Lithuanian, Latvian, Dutch, Bulgarian, Polish, Slovak, Slovenian, and Swedish.). The results depict mean accuracy and corresponding standard deviations across languages.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Comparison with Aya-23-8B on Common Languages</h3>
<div class="ltx_para" id="S7.SS4.p1">
<p class="ltx_p" id="S7.SS4.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S7.T6" title="In 7.4 Comparison with Aya-23-8B on Common Languages ‣ 7 Results ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a> compares our models with Aya-23-8B on 11 common European languages shared between the datasets used by both models.
These languages include Czech, Dutch, English, French, German, Greek, Italian, Polish, Portuguese, Romanian, and Spanish.</p>
</div>
<div class="ltx_para" id="S7.SS4.p2">
<p class="ltx_p" id="S7.SS4.p2.1">Our "Ours (Instruct)" model achieves an average score of 55.5% competitive to Aya-23-8B’s 56.3%.
On the ARC benchmark, "Ours (Instruct)" surpasses Aya-23-8B with a score of 60.6% compared to 59.3%, demonstrating strong reasoning abilities in these languages.
Additionally, "Ours (Instruct)" attains a higher score on TruthfulQA (51.2% vs. 48.5%).</p>
</div>
<div class="ltx_para" id="S7.SS4.p3">
<p class="ltx_p" id="S7.SS4.p3.1">However, Aya-23-8B outperforms our model on the HellaSwag benchmark with a score of 66.0% versus "Ours (Instruct)" at 65.1%.
Aya-23-8B also leads on the MMLU benchmark with 51.3%, compared to our model’s 45.2%.
These results suggest that while "Ours (Instruct)" is competitive, particularly in reasoning and truthfulness, there is room for improvement in contextual understanding and domain-specific knowledge.</p>
</div>
<div class="ltx_para" id="S7.SS4.p4">
<p class="ltx_p" id="S7.SS4.p4.1">Our "Ours (Base)" model, without instruction tuning, achieves an average score of 51.1%, which is lower than both "Ours (Instruct)" and Aya-23-8B.
This highlights the effectiveness of instruction tuning in enhancing the model’s performance across multiple tasks and languages.</p>
</div>
<div class="ltx_para" id="S7.SS4.p5">
<p class="ltx_p" id="S7.SS4.p5.1">In summary, "Ours (Instruct)" is competitive to Aya-23-8B in common European languages, especially in reasoning tasks and truthfulness. Continued refinement and training could further improve its performance, making it a more robust model for multilingual applications.</p>
</div>
<figure class="ltx_table" id="S7.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S7.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S7.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.2">Average</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.3">ARC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.4">HellaSwag</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.5">TruthfulQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.6">MMLU</th>
</tr>
<tr class="ltx_tr" id="S7.T6.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.2.2.1">Aya-23-8B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S7.T6.1.2.2.2.1">.563</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.2.2.3">.593 ± .032</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.2.2.4">
<span class="ltx_text ltx_font_bold" id="S7.T6.1.2.2.4.1">.660</span> ± .047</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.2.2.5">.485 ± .028</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.2.2.6">
<span class="ltx_text ltx_font_bold" id="S7.T6.1.2.2.6.1">.513</span> ± .022</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T6.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S7.T6.1.3.1.1">Ours (Base)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T6.1.3.1.2">.511</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T6.1.3.1.3">.578 ± .037</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T6.1.3.1.4">.642 ± .043</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T6.1.3.1.5">.464 ± .027</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T6.1.3.1.6">.361 ± .024</td>
</tr>
<tr class="ltx_tr" id="S7.T6.1.4.2">
<td class="ltx_td ltx_align_left ltx_border_b" id="S7.T6.1.4.2.1">Ours (Instruct)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.4.2.2">.555</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.4.2.3">
<span class="ltx_text ltx_font_bold" id="S7.T6.1.4.2.3.1">.606</span> ± .038</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.4.2.4">.651 ± .045</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.4.2.5">
<span class="ltx_text ltx_font_bold" id="S7.T6.1.4.2.5.1">.512</span> ± .021</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.4.2.6">.452 ± .021</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Results on multilingual benchmarks across the 11 common languages (Czech, Dutch, English, French, German, Greek, Italian, Polish, Portuguese, Romanian, and Spanish.) with Aya-23. It includes mean accuracy and corresponding standard deviations across languages.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this work, we presented the development of two multilingual large language models, "Ours (Base)" and "Ours (Instruct)", tailored to support the linguistic diversity of Europe by encompassing all 24 official EU languages.
Through the use of a custom multilingual tokenizer and a dataset prioritizing non-English content, our models address limitations found in existing multilingual models, particularly their English-centric bias.
Preliminary results demonstrate competitive performance across multiple benchmarks, including ARC, HellaSwag, MMLU, and TruthfulQA.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Our ongoing efforts aim to further improve the models’ performance and efficiency, ensuring they can better serve the needs of diverse European communities and facilitate the broader democratization of <abbr class="ltx_glossaryref" title="Large Language Model"><span class="ltx_text ltx_glossary_short">LLM</span></abbr> technology across multilingual environments.
Future work will focus on improving the models in specialized knowledge domains, math and code capabilities.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadji et al. (2022)</span>
<span class="ltx_bibblock">
Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoît Sagot. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.lrec-1.463" title="">Towards a cleaner document-oriented multilingual crawled corpus</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the Thirteenth Language Resources and Evaluation Conference</em>, pages 4344–4355, Marseille, France. European Language Resources Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ainslie et al. (2023)</span>
<span class="ltx_bibblock">
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023.

</span>
<span class="ltx_bibblock">GQA: training generalized multi-query transformer models from multi-head checkpoints.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">EMNLP</em>, pages 4895–4901. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ali et al. (2024)</span>
<span class="ltx_bibblock">
Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max Lübbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper Buschhoff, Charvi Jain, Alexander Weber, Lena Jurkschat, Hammam Abdelwahab, Chelsea John, Pedro Ortiz Suarez, Malte Ostendorff, Samuel Weinbach, Rafet Sifa, Stefan Kesselheim, and Nicolas Flores-Herr. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2024.findings-naacl.247" title="">Tokenizer choice for LLM training: Negligible or crucial?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Findings of the Association for Computational Linguistics: NAACL 2024</em>, pages 3907–3924, Mexico City, Mexico. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">NeurIPS</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the AI2 reasoning challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">CoRR</em>, abs/1803.05457.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.747" title="">Unsupervised cross-lingual representation learning at scale</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 8440–8451, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1423" title="">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier
Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu,
Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl
Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James
Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini
Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta,
Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2407.21783" title="">The llama 3 herd of models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Preprint</em>, arXiv:2407.21783.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Groeneveld et al. (2024)</span>
<span class="ltx_bibblock">
Dirk Groeneveld, Iz Beltagy, Evan Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024.

</span>
<span class="ltx_bibblock">Olmo: Accelerating the science of language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">ACL (1)</em>, pages 15789–15809. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">ICLR</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">CoRR</em>, abs/2310.06825.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korthikanti et al. (2023)</span>
<span class="ltx_bibblock">
Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2023.

</span>
<span class="ltx_bibblock">Reducing activation recomputation in large transformer models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of Machine Learning and Systems</em>, 5:341–353.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lample and Conneau (2019)</span>
<span class="ltx_bibblock">
Guillaume Lample and Alexis Conneau. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1901.07291" title="">Cross-lingual language model pretraining</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Preprint</em>, arXiv:1901.07291.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">ACL (1)</em>, pages 3214–3252. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martins et al. (2024)</span>
<span class="ltx_bibblock">
Pedro Henrique Martins, Patrick Fernandes, João Alves, Nuno M Guerreiro, Ricardo Rei, Duarte M Alves, José Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, et al. 2024.

</span>
<span class="ltx_bibblock">Eurollm: Multilingual language models for europe.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2409.16235</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrov et al. (2023)</span>
<span class="ltx_bibblock">
Aleksandar Petrov, Emanuele La Malfa, Philip H. S. Torr, and Adel Bibi. 2023.

</span>
<span class="ltx_bibblock">Language model tokenizers introduce unfairness between languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">NeurIPS</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et al. (2022)</span>
<span class="ltx_bibblock">
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. 2022.

</span>
<span class="ltx_bibblock">BLOOM: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">CoRR</em>, abs/2211.05100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2024)</span>
<span class="ltx_bibblock">
Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Neurocomputing</em>, 568:127063.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">CoRR</em>, abs/2302.13971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">CoRR</em>, abs/2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Üstün et al. (2024)</span>
<span class="ltx_bibblock">
Ahmet Üstün, Viraat Aryabumi, Zheng Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024.

</span>
<span class="ltx_bibblock">Aya model: An instruction finetuned open-access multilingual language model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ACL (1)</em>, pages 15894–15939. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">NIPS</em>, pages 5998–6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weber et al. (2024)</span>
<span class="ltx_bibblock">
Alexander Arno Weber, Klaudia Thellmann, Jan Ebert, Nicolas Flores-Herr, Jens Lehmann, Michael Fromm, and Mehdi Ali. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2402.13703" title="">Investigating multilingual instruction-tuning: Do polyglot models demand for multilingual instructions?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">CoRR</em>, abs/2402.13703.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=farT6XXntP" title="">A paradigm shift in machine translation: Boosting translation performance of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2021)</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021.

</span>
<span class="ltx_bibblock">mt5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">NAACL-HLT</em>, pages 483–498. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">ACL (1)</em>, pages 4791–4800. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.

</span>
<span class="ltx_bibblock">OPT: open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">CoRR</em>, abs/2205.01068.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Data</h3>
<section class="ltx_subsubsection" id="A1.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.1 </span>Dumps</h4>
<div class="ltx_para" id="A1.SS1.SSS1.p1">
<p class="ltx_p" id="A1.SS1.SSS1.p1.1">In this section, we outline the Common Crawl data utilized in our LLM training, detailing the cutoff dates and the data collection period. The dumps span multiple years, with varying distributions of weeks per year (cf. <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#A1.T7" title="In A.1.1 Dumps ‣ A.1 Data ‣ Appendix A Appendix ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a>), which is critical for understanding the temporal coverage of the training data.</p>
</div>
<figure class="ltx_table" id="A1.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T7.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1.1">Year</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.2.1">Week</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T7.1.2.1.1">2014</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.1.2.1.2">42</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T7.1.3.2.1">2015</th>
<td class="ltx_td ltx_align_left" id="A1.T7.1.3.2.2">14, 48</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T7.1.4.3.1">2016</th>
<td class="ltx_td ltx_align_left" id="A1.T7.1.4.3.2">22, 44</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T7.1.5.4.1">2017</th>
<td class="ltx_td ltx_align_left" id="A1.T7.1.5.4.2">13, 47, 51</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T7.1.6.5.1">2018</th>
<td class="ltx_td ltx_align_left" id="A1.T7.1.6.5.2">5, 9, 13, 17, 22, 26, 30, 34, 39, 43, 47, 51</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T7.1.7.6.1">2019</th>
<td class="ltx_td ltx_align_left" id="A1.T7.1.7.6.2">4, 9, 13, 18, 22, 26, 30, 35, 39, 47, 51</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T7.1.8.7.1">2020</th>
<td class="ltx_td ltx_align_left" id="A1.T7.1.8.7.2">5, 10, 16, 24, 29, 34, 40, 45, 50</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T7.1.9.8.1">2021</th>
<td class="ltx_td ltx_align_left" id="A1.T7.1.9.8.2">4, 10, 17, 21, 25, 31, 39, 43, 49</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T7.1.10.9.1">2022</th>
<td class="ltx_td ltx_align_left" id="A1.T7.1.10.9.2">5, 21, 27, 33, 40, 49</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A1.T7.1.11.10.1">2023</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T7.1.11.10.2">6, 14, 23, 40, 50</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>List of dumps by year and week.</figcaption>
</figure>
<div class="ltx_para" id="A1.SS1.SSS1.p2">
<p class="ltx_p" id="A1.SS1.SSS1.p2.1">The data spans from 2014 to 2023, with earlier years (2014-2016) containing fewer dumps but often representing a larger dataset per dump. The latest available data cutoff is from 2023 (week 50), giving the model a comprehensive span of nearly a decade of web data.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS1.p3">
<p class="ltx_p" id="A1.SS1.SSS1.p3.1">This periodization ensures that the model has exposure to a wide temporal range of web content, with a balanced emphasis on both earlier, higher-density dumps and more recent, frequent dumps, providing a robust training corpus.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Instruction-tuning Evaluation Results</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#A1.F4" title="In A.2 Instruction-tuning Evaluation Results ‣ Appendix A Appendix ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a> we present the evaluation results of MT-Bench-X <cite class="ltx_cite ltx_citemacro_cite">Weber et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#bib.bib23" title="">2024</a>)</cite>. For the discussion of the results, we refer to <a class="ltx_ref" href="https://arxiv.org/html/2410.03730v1#S6.SS3" title="6.3 Multilingual Instruction Evaluation ‣ 6 Instruction Tuned Model ‣ Progress Report: Towards European LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.3</span></a>.</p>
</div>
<figure class="ltx_table" id="A1.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T8.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.1">LAN</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.2">Curated (M)</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.3">Web (M)</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.4">Curated in %</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.5">Total (M)</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.6">%</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.1.2.1.1">bg</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T8.1.2.1.2">21 233</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T8.1.2.1.3">23 810</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T8.1.2.1.4">47.14</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T8.1.2.1.5">45 042</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T8.1.2.1.6">1.13</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.3.2">
<td class="ltx_td ltx_align_left" id="A1.T8.1.3.2.1">cs</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.3.2.2">6 229</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.3.2.3">46 919</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.3.2.4">11.72</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.3.2.5">53 148</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.3.2.6">1.33</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.4.3">
<td class="ltx_td ltx_align_left" id="A1.T8.1.4.3.1">da</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.4.3.2">6 220</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.4.3.3">18 257</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.4.3.4">25.41</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.4.3.5">24 477</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.4.3.6">0.61</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.5.4">
<td class="ltx_td ltx_align_left" id="A1.T8.1.5.4.1">de</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.5.4.2">36 850</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.5.4.3">312 006</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.5.4.4">10.56</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.5.4.5">348 856</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.5.4.6">8.72</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.6.5">
<td class="ltx_td ltx_align_left" id="A1.T8.1.6.5.1">el</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.6.5.2">21 078</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.6.5.3">40 494</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.6.5.4">34.23</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.6.5.5">61 572</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.6.5.6">1.54</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.7.6">
<td class="ltx_td ltx_align_left" id="A1.T8.1.7.6.1">en</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.7.6.2">215 204</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.7.6.3">1 452 819</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.7.6.4">12.90</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.7.6.5">1 668 024</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.7.6.6">41.70</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.8.7">
<td class="ltx_td ltx_align_left" id="A1.T8.1.8.7.1">es</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.8.7.2">23 788</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.8.7.3">295 850</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.8.7.4">7.44</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.8.7.5">319 638</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.8.7.6">7.99</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.9.8">
<td class="ltx_td ltx_align_left" id="A1.T8.1.9.8.1">et</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.9.8.2">8 785</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.9.8.3">6 253</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.9.8.4">58.42</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.9.8.5">15 038</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.9.8.6">0.38</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.10.9">
<td class="ltx_td ltx_align_left" id="A1.T8.1.10.9.1">fi</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.10.9.2">3 234</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.10.9.3">35 996</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.10.9.4">8.24</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.10.9.5">39 231</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.10.9.6">0.98</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.11.10">
<td class="ltx_td ltx_align_left" id="A1.T8.1.11.10.1">fr</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.11.10.2">30 606</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.11.10.3">333 399</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.11.10.4">8.41</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.11.10.5">364 005</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.11.10.6">9.10</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.12.11">
<td class="ltx_td ltx_align_left" id="A1.T8.1.12.11.1">ga</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.12.11.2">474</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.12.11.3">75</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.12.11.4">86.26</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.12.11.5">549</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.12.11.6">0.01</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.13.12">
<td class="ltx_td ltx_align_left" id="A1.T8.1.13.12.1">hr</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.13.12.2">15 296</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.13.12.3">1</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.13.12.4">99.99</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.13.12.5">15 298</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.13.12.6">0.38</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.14.13">
<td class="ltx_td ltx_align_left" id="A1.T8.1.14.13.1">hu</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.14.13.2">3 376</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.14.13.3">37 551</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.14.13.4">8.25</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.14.13.5">40 927</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.14.13.6">1.02</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.15.14">
<td class="ltx_td ltx_align_left" id="A1.T8.1.15.14.1">it</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.15.14.2">19 834</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.15.14.3">169 069</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.15.14.4">10.50</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.15.14.5">188 903</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.15.14.6">4.72</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.16.15">
<td class="ltx_td ltx_align_left" id="A1.T8.1.16.15.1">lt</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.16.15.2">1 990</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.16.15.3">9 141</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.16.15.4">17.88</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.16.15.5">11 131</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.16.15.6">0.28</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.17.16">
<td class="ltx_td ltx_align_left" id="A1.T8.1.17.16.1">lv</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.17.16.2">1 980</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.17.16.3">5 683</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.17.16.4">25.84</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.17.16.5">7 663</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.17.16.6">0.19</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.18.17">
<td class="ltx_td ltx_align_left" id="A1.T8.1.18.17.1">mt</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.18.17.2">3 515</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.18.17.3">0.5</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.18.17.4">99.99</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.18.17.5">3 516</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.18.17.6">0.09</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.19.18">
<td class="ltx_td ltx_align_left" id="A1.T8.1.19.18.1">nl</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.19.18.2">6 788</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.19.18.3">125 106</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.19.18.4">5.15</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.19.18.5">131 893</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.19.18.6">3.30</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.20.19">
<td class="ltx_td ltx_align_left" id="A1.T8.1.20.19.1">pl</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.20.19.2">9 397</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.20.19.3">67 361</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.20.19.4">12.24</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.20.19.5">76 758</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.20.19.6">1.92</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.21.20">
<td class="ltx_td ltx_align_left" id="A1.T8.1.21.20.1">pt</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.21.20.2">6 670</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.21.20.3">136 284</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.21.20.4">4.67</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.21.20.5">142 954</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.21.20.6">3.57</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.22.21">
<td class="ltx_td ltx_align_left" id="A1.T8.1.22.21.1">ro</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.22.21.2">4 524</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.22.21.3">25 988</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.22.21.4">14.83</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.22.21.5">30 512</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.22.21.6">0.76</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.23.22">
<td class="ltx_td ltx_align_left" id="A1.T8.1.23.22.1">sk</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.23.22.2">40 114</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.23.22.3">11 162</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.23.22.4">78.23</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.23.22.5">51 276</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.23.22.6">1.28</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.24.23">
<td class="ltx_td ltx_align_left" id="A1.T8.1.24.23.1">sl</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.24.23.2">12 599</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.24.23.3">1 228</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.24.23.4">91.12</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.24.23.5">13 827</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.24.23.6">0.35</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.25.24">
<td class="ltx_td ltx_align_left" id="A1.T8.1.25.24.1">sv</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.25.24.2">4 079</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.25.24.3">40 070</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.25.24.4">9.24</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.25.24.5">44 148</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.25.24.6">1.10</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.26.25">
<td class="ltx_td ltx_align_left" id="A1.T8.1.26.25.1">code</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.26.25.2">301 612</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.26.25.3">-</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.26.25.4">100.00</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.26.25.5">301 612</td>
<td class="ltx_td ltx_align_right" id="A1.T8.1.26.25.6">7.54</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.27.26">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A1.T8.1.27.26.1"><span class="ltx_text ltx_font_bold" id="A1.T8.1.27.26.1.1">Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A1.T8.1.27.26.2">805 477</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A1.T8.1.27.26.3">3 194 523</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A1.T8.1.27.26.4">51.57</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A1.T8.1.27.26.5">4 000 000</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A1.T8.1.27.26.6">100.00</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Comparison of token counts (in millions) between curated and web data across different languages. The total word count reflects the sum of both curated and web-sourced data after deduplication and filtering. Source code is treated separately from natural language data.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_table" id="A1.T9">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Overview of large-scale multilingual and domain-specific datasets, including their respective word counts and usage percentages across various sectors such as source code, academia, law, medicine, and open knowledge bases. Each dataset’s URL provides access to further information.</figcaption>
<table class="ltx_tabular" id="A1.T9.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T9.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T9.1.1.1.1">Name</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T9.1.1.1.2">Language/s</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T9.1.1.1.3">Domain</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="A1.T9.1.1.1.4"># Words</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="A1.T9.1.1.1.5">Percentage</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T9.1.2.1">
<td class="ltx_td ltx_align_left" id="A1.T9.1.2.1.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/bigcode/starcoderdata" title="">StarCoder data</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.2.1.2">CODE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.2.1.3">Source Code</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.2.1.4">73.064.206.834</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.2.1.5">25,35%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.3.2">
<td class="ltx_td ltx_align_left" id="A1.T9.1.3.2.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/allenai/peS2o" title="">peS2o</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.3.2.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.3.2.3">Academic</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.3.2.4">42.172.944.553</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.3.2.5">14,63%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.4.3">
<td class="ltx_td ltx_align_left" id="A1.T9.1.4.3.1"><a class="ltx_ref ltx_href" href="https://macocu.eu/" title="">MaCoCu</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.4.3.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.4.3.3">Web</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.4.3.4">27.930.391.197</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.4.3.5">9,69%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.5.4">
<td class="ltx_td ltx_align_left" id="A1.T9.1.5.4.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/joelniklaus/legal-mc4" title="">Legal MC4</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.5.4.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.5.4.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.5.4.4">13.892.450.758</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.5.4.5">4,82%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.6.5">
<td class="ltx_td ltx_align_left" id="A1.T9.1.6.5.1"><a class="ltx_ref ltx_href" href="https://eur-lex.europa.eu/homepage.html?locale=en" title="">European Union Law: EurLex</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.6.5.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.6.5.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.6.5.4">13.235.730.604</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.6.5.5">4,59%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.7.6">
<td class="ltx_td ltx_align_left" id="A1.T9.1.7.6.1"><a class="ltx_ref ltx_href" href="https://pile.eleuther.ai/" title="">Pile: PMC extracts</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.7.6.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.7.6.3">Medical</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.7.6.4">12.113.007.827</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.7.6.5">4,20%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.8.7">
<td class="ltx_td ltx_align_left" id="A1.T9.1.8.7.1"><a class="ltx_ref ltx_href" href="https://en.wikipedia.org/wiki/Main_Page" title="">Wikimedia: Wikipedia</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.8.7.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.8.7.3">Knowledge Base</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.8.7.4">11.382.015.197</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.8.7.5">3,95%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.9.8">
<td class="ltx_td ltx_align_left" id="A1.T9.1.9.8.1"><a class="ltx_ref ltx_href" href="https://pile.eleuther.ai/" title="">Pile: Openwebtext2</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.9.8.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.9.8.3">Books</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.9.8.4">10.633.550.340</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.9.8.5">3,69%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.10.9">
<td class="ltx_td ltx_align_left" id="A1.T9.1.10.9.1"><a class="ltx_ref ltx_href" href="https://pile.eleuther.ai/" title="">Pile: Free law opinions V2</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.10.9.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.10.9.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.10.9.4">10.408.105.722</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.10.9.5">3,61%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.11.10">
<td class="ltx_td ltx_align_left" id="A1.T9.1.11.10.1"><a class="ltx_ref ltx_href" href="https://github.com/togethercomputer/RedPajama-Data" title="">RedPajama: arXiv</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.11.10.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.11.10.3">Academic</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.11.10.4">10.202.502.052</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.11.10.5">3,54%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.12.11">
<td class="ltx_td ltx_align_left" id="A1.T9.1.12.11.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/open-web-math/open-web-math" title="">OpenWebMath</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.12.11.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.12.11.3">Math</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.12.11.4">7.378.503.091</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.12.11.5">2,56%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.13.12">
<td class="ltx_td ltx_align_left" id="A1.T9.1.13.12.1"><a class="ltx_ref ltx_href" href="https://archive.org/details/stackexchange" title="">StackExchange</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.13.12.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.13.12.3">Forum</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.13.12.4">7.311.540.145</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.13.12.5">2,54%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.14.13">
<td class="ltx_td ltx_align_left" id="A1.T9.1.14.13.1"><a class="ltx_ref ltx_href" href="https://wacky.sslmit.unibo.it/doku.php?id=start" title="">Wacky (ukWaC, deWaC, itWaC, frWaC)</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.14.13.2">EN,DE,IT</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.14.13.3">Web</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.14.13.4">6.440.088.223</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.14.13.5">2,23%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.15.14">
<td class="ltx_td ltx_align_left" id="A1.T9.1.15.14.1"><a class="ltx_ref ltx_href" href="https://kleineanfragen.de/info/daten" title="">Kleine (und große) Anfragen</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.15.14.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.15.14.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.15.14.4">6.151.085.999</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.15.14.5">2,13%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.16.15">
<td class="ltx_td ltx_align_left" id="A1.T9.1.16.15.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/hoskinson-center/proof-pile" title="">Proof pile</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.16.15.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.16.15.3">Math</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.16.15.4">4.495.525.078</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.16.15.5">1,56%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.17.16">
<td class="ltx_td ltx_align_left" id="A1.T9.1.17.16.1"><a class="ltx_ref ltx_href" href="https://www.projekt-gutenberg.org/" title="">Projekt Gutenberg</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.17.16.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.17.16.3">Books</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.17.16.4">3.374.582.910</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.17.16.5">1,17%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.18.17">
<td class="ltx_td ltx_align_left" id="A1.T9.1.18.17.1"><a class="ltx_ref ltx_href" href="https://pile.eleuther.ai/" title="">Pile: PMC abstracts</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.18.17.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.18.17.3">Medical</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.18.17.4">3.173.320.716</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.18.17.5">1,10%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.19.18">
<td class="ltx_td ltx_align_left" id="A1.T9.1.19.18.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/NbAiLab/NCC" title="">Norwegian Collossal Corpus (NCC)</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.19.18.2">NO</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.19.18.3">Knowledge Base</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.19.18.4">2.920.299.614</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.19.18.5">1,01%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.20.19">
<td class="ltx_td ltx_align_left" id="A1.T9.1.20.19.1"><a class="ltx_ref ltx_href" href="https://www.juls.savba.sk/justicecorp.html" title="">SK court decisions</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.20.19.2">SVK</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.20.19.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.20.19.4">2.174.828.161</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.20.19.5">0,75%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.21.20">
<td class="ltx_td ltx_align_left" id="A1.T9.1.21.20.1"><a class="ltx_ref ltx_href" href="http://www.opensubtitles.org/" title="">Opensubtitles 2018</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.21.20.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.21.20.3">Culture</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.21.20.4">1.398.063.978</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.21.20.5">0,49%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.22.21">
<td class="ltx_td ltx_align_left" id="A1.T9.1.22.21.1"><a class="ltx_ref ltx_href" href="https://zenodo.org/records/5495529" title="">Spanish legal corpra</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.22.21.2">ES</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.22.21.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.22.21.4">1383749504</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.22.21.5">0,48%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.23.22">
<td class="ltx_td ltx_align_left" id="A1.T9.1.23.22.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/coastalcph/multi_eurlex" title="">Multilingual European Union Law: MultiEurLex</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.23.22.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.23.22.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.23.22.4">1.184.706.666</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.23.22.5">0,41%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.24.23">
<td class="ltx_td ltx_align_left" id="A1.T9.1.24.23.1"><a class="ltx_ref ltx_href" href="https://joint-research-centre.ec.europa.eu/language-technology-resources/dcep-digital-corpus-european-parliament_en" title="">Digital Corpus of the European Parliament: DCEP</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.24.23.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.24.23.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.24.23.4">1.117.868.910</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.24.23.5">0,39%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.25.24">
<td class="ltx_td ltx_align_left" id="A1.T9.1.25.24.1"><a class="ltx_ref ltx_href" href="https://www.euromatrixplus.net/multi-un/" title="">Multi UN corpus: MultiUN</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.25.24.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.25.24.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.25.24.4">1.096.690.756</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.25.24.5">0,38%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.26.25">
<td class="ltx_td ltx_align_left" id="A1.T9.1.26.25.1"><a class="ltx_ref ltx_href" href="https://www.clarin.eu/parlamint" title="">ParlaMint corpus of parliamentary debates</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.26.25.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.26.25.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.26.25.4">1.095.629.337</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.26.25.5">0,38%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.27.26">
<td class="ltx_td ltx_align_left" id="A1.T9.1.27.26.1"><a class="ltx_ref ltx_href" href="https://dcl.bas.bg/BulNC-registration/?lang=EN" title="">Admin EUR corpus of EU legislation</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.27.26.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.27.26.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.27.26.4">1.079.748.261</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.27.26.5">0,37%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.28.27">
<td class="ltx_td ltx_align_left" id="A1.T9.1.28.27.1"><a class="ltx_ref ltx_href" href="https://www.sketchengine.eu/estonian-national-corpus/#:~:text=Estonian%20National%20Corpus%202021%20(Estonian%20NC%202021)%20%E2%80%93%202.4%20billion,academic%20writing%20(2020%E2%80%932021)" title="">Estonian National Corpus 2021: ENC 2021</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.28.27.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.28.27.3">Web</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.28.27.4">1.004.815.512</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.28.27.5">0,35%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.29.28">
<td class="ltx_td ltx_align_left" id="A1.T9.1.29.28.1"><a class="ltx_ref ltx_href" href="http://nlp.ffzg.hr/resources/corpora/slwac/" title="">slWaC</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.29.28.2">SL</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.29.28.3">Web</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.29.28.4">953.803.486</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.29.28.5">0,33%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.30.29">
<td class="ltx_td ltx_align_left" id="A1.T9.1.30.29.1"><a class="ltx_ref ltx_href" href="https://gigaword.dk/" title="">Danish Gigaword: DaGW</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.30.29.2">DA</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.30.29.3">Web</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.30.29.4">922.700.245</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.30.29.5">0,32%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.31.30">
<td class="ltx_td ltx_align_left" id="A1.T9.1.31.30.1"><a class="ltx_ref ltx_href" href="https://en.wikisource.org/wiki/Main_Page" title="">Wikimedia: WikiSource</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.31.30.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.31.30.3">Books</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.31.30.4">798.322.622</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.31.30.5">0,28%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.32.31">
<td class="ltx_td ltx_align_left" id="A1.T9.1.32.31.1"><a class="ltx_ref ltx_href" href="https://openlegaldata.io/research/2019/02/19/court-decision-dataset.html" title="">German Court Decision Dataset</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.32.31.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.32.31.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.32.31.4">749.060.810</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.32.31.5">0,26%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.33.32">
<td class="ltx_td ltx_align_left" id="A1.T9.1.33.32.1"><a class="ltx_ref ltx_href" href="https://www.statmt.org/europarl/" title="">European Parliament: EuroParl</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.33.32.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.33.32.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.33.32.4">660.716.717</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.33.32.5">0,23%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.34.33">
<td class="ltx_td ltx_align_left" id="A1.T9.1.34.33.1"><a class="ltx_ref ltx_href" href="https://opendata.swiss/en/dataset?keywords_en=environmental--policykeywords_en=policy-analysis" title="">Swiss policy documents</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.34.33.2">DE,FR,IT</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.34.33.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.34.33.4">561.574.452</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.34.33.5">0,19%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.35.34">
<td class="ltx_td ltx_align_left" id="A1.T9.1.35.34.1"><a class="ltx_ref ltx_href" href="https://zenodo.org/records/4643066" title="">Corpus der Drucksachen des Deutschen Bundestages</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.35.34.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.35.34.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.35.34.4">515.053.512</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.35.34.5">0,18%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.36.35">
<td class="ltx_td ltx_align_left" id="A1.T9.1.36.35.1"><a class="ltx_ref ltx_href" href="https://pile.eleuther.ai/" title="">Pile: Philarchive V2</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.36.35.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.36.35.3">Academic</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.36.35.4">487.032.008</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.36.35.5">0,17%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.37.36">
<td class="ltx_td ltx_align_left" id="A1.T9.1.37.36.1"><a class="ltx_ref ltx_href" href="https://www.sketchengine.eu/polish-parliamentary-corpus/#:~:text=The%20Polish%20Parliamentary%20Corpus%20(PPC,segments%20of%20interpellations%20and%20questions." title="">Polish Parliamentary Corpus: PPC</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.37.36.2">PL</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.37.36.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.37.36.4">472.193.828</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.37.36.5">0,16%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.38.37">
<td class="ltx_td ltx_align_left" id="A1.T9.1.38.37.1"><a class="ltx_ref ltx_href" href="https://marcell-project.eu/" title="">MARCELL CEF: MARCELL</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.38.37.2">BG,HR,HU,RO,SL,SK,PL</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.38.37.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.38.37.4">373.283.188</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.38.37.5">0,13%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.39.38">
<td class="ltx_td ltx_align_left" id="A1.T9.1.39.38.1"><a class="ltx_ref ltx_href" href="http://openlegaldata.io/research/2019/02/19/court-decision-dataset.html" title="">OpenLegalData</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.39.38.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.39.38.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.39.38.4">363.962.090</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.39.38.5">0,13%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.40.39">
<td class="ltx_td ltx_align_left" id="A1.T9.1.40.39.1"><a class="ltx_ref ltx_href" href="https://pile.eleuther.ai/" title="">Pile: NIH exporter</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.40.39.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.40.39.3">Medical</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.40.39.4">307.055.044</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.40.39.5">0,11%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.41.40">
<td class="ltx_td ltx_align_left" id="A1.T9.1.41.40.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/MLRS/korpus_malti" title="">Korpus Malti</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.41.40.2">MA</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.41.40.3">Web</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.41.40.4">276.204.170</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.41.40.5">0,10%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.42.41">
<td class="ltx_td ltx_align_left" id="A1.T9.1.42.41.1"><a class="ltx_ref ltx_href" href="https://github.com/EleutherAI/hn-scraper" title="">Pile: HackerNews</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.42.41.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.42.41.3">Web</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.42.41.4">246.149.295</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.42.41.5">0,09%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.43.42">
<td class="ltx_td ltx_align_left" id="A1.T9.1.43.42.1"><a class="ltx_ref ltx_href" href="https://zenodo.org/records/4542662" title="">Corpus der Plenarprotokolle des Deutschen Bundestages</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.43.42.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.43.42.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.43.42.4">235.502.510</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.43.42.5">0,08%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.44.43">
<td class="ltx_td ltx_align_left" id="A1.T9.1.44.43.1"><a class="ltx_ref ltx_href" href="https://github.com/hendrycks/math" title="">AmpsMath: Mathematica</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.44.43.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.44.43.3">Math</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.44.43.4">232.502.096</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.44.43.5">0,08%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.45.44">
<td class="ltx_td ltx_align_left" id="A1.T9.1.45.44.1"><a class="ltx_ref ltx_href" href="https://www.cl.ut.ee/korpused/segakorpus/" title="">Estonian Reference Corpus</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.45.44.2">ET</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.45.44.3">Web</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.45.44.4">227.978.247</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.45.44.5">0,08%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.46.45">
<td class="ltx_td ltx_align_left" id="A1.T9.1.46.45.1"><a class="ltx_ref ltx_href" href="https://www.corpusitaliano.it/en/" title="">Paisa</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.46.45.2">IT</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.46.45.3">Web</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.46.45.4">207.382.616</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.46.45.5">0,07%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.47.46">
<td class="ltx_td ltx_align_left" id="A1.T9.1.47.46.1"><a class="ltx_ref ltx_href" href="https://www.deutschestextarchiv.de/" title="">Deutsches TextArchiv: DTA</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.47.46.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.47.46.3">Books</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.47.46.4">197.339.728</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.47.46.5">0,07%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.48.47">
<td class="ltx_td ltx_align_left" id="A1.T9.1.48.47.1"><a class="ltx_ref ltx_href" href="https://en.wikibooks.org/wiki/Main_Page" title="">Wikimedia: WikiBooks</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.48.47.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.48.47.3">Books</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.48.47.4">168.473.660</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.48.47.5">0,06%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.49.48">
<td class="ltx_td ltx_align_left" id="A1.T9.1.49.48.1"><a class="ltx_ref ltx_href" href="https://opus.nlpl.eu/EMEA/corpus/version/EMEA" title="">European Medicines Agency Corpus: EMEA</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.49.48.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.49.48.3">Medical</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.49.48.4">152.252.365</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.49.48.5">0,05%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.50.49">
<td class="ltx_td ltx_align_left" id="A1.T9.1.50.49.1"><a class="ltx_ref ltx_href" href="https://www.wikiquote.org/" title="">Wikimedia: WikiQuote</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.50.49.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.50.49.3">Recreation</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.50.49.4">146.497.554</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.50.49.5">0,05%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.51.50">
<td class="ltx_td ltx_align_left" id="A1.T9.1.51.50.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/rcds/swiss_judgment_prediction" title="">Swiss Judgment Prediction</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.51.50.2">DE,FR</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.51.50.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.51.50.4">144.599.546</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.51.50.5">0,05%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.52.51">
<td class="ltx_td ltx_align_left" id="A1.T9.1.52.51.1"><a class="ltx_ref ltx_href" href="https://ucsb.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358" title="">WikiHow</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.52.51.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.52.51.3">Knowledge Base</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.52.51.4">109.611.227</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.52.51.5">0,04%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.53.52">
<td class="ltx_td ltx_align_left" id="A1.T9.1.53.52.1"><a class="ltx_ref ltx_href" href="https://zenodo.org/records/7699032" title="">Corpus der Entscheidungen: Bundesgerichtshofs</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.53.52.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.53.52.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.53.52.4">104.002.842</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.53.52.5">0,04%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.54.53">
<td class="ltx_td ltx_align_left" id="A1.T9.1.54.53.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/HiTZ/Multilingual-Medical-Corpus" title="">Multilingual Medical Corpora</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.54.53.2">DE,EN,ES,FR</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.54.53.3">Medical</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.54.53.4">90.741.906</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.54.53.5">0,03%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.55.54">
<td class="ltx_td ltx_align_left" id="A1.T9.1.55.54.1"><a class="ltx_ref ltx_href" href="https://opus.nlpl.eu/legacy/SETIMES.php" title="">SE times corpus</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.55.54.2">BG,BS,EL,EN,HR,MK,RO,SQ,TR</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.55.54.3">News</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.55.54.4">69.279.571</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.55.54.5">0,02%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.56.55">
<td class="ltx_td ltx_align_left" id="A1.T9.1.56.55.1"><a class="ltx_ref ltx_href" href="https://en.wikivoyage.org/wiki/Main_Page" title="">Wikimedia: Wikivoyage</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.56.55.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.56.55.3">Culture</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.56.55.4">66.278.289</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.56.55.5">0,02%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.57.56">
<td class="ltx_td ltx_align_left" id="A1.T9.1.57.56.1"><a class="ltx_ref ltx_href" href="https://zenodo.org/records/7767295" title="">Corpus der Entscheidungen: Bundespatentgerichts</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.57.56.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.57.56.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.57.56.4">66.029.205</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.57.56.5">0,02%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.58.57">
<td class="ltx_td ltx_align_left" id="A1.T9.1.58.57.1"><a class="ltx_ref ltx_href" href="https://www.wikiversity.org/" title="">Wikiversity</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.58.57.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.58.57.3">Books</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.58.57.4">65.482.211</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.58.57.5">0,02%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.59.58">
<td class="ltx_td ltx_align_left" id="A1.T9.1.59.58.1"><a class="ltx_ref ltx_href" href="https://opus.nlpl.eu/ECB/corpus/version/ECB" title="">EventCorefBank: EC</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.59.58.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.59.58.3">Finance</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.59.58.4">53.894.980</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.59.58.5">0,02%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.60.59">
<td class="ltx_td ltx_align_left" id="A1.T9.1.60.59.1"><a class="ltx_ref ltx_href" href="https://pub.cl.uzh.ch/wiki/public/pacoco/medi-notice" title="">Drug Information Corpus: Medi-Notice</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.60.59.2">DE,FR,IT</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.60.59.3">Medical</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.60.59.4">52.068.268</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.60.59.5">0,02%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.61.60">
<td class="ltx_td ltx_align_left" id="A1.T9.1.61.60.1"><a class="ltx_ref ltx_href" href="https://zenodo.org/records/7749683" title="">Corpus der Entscheidungen: Bundesverwaltungsgerichts</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.61.60.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.61.60.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.61.60.4">49.664.175</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.61.60.5">0,02%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.62.61">
<td class="ltx_td ltx_align_left" id="A1.T9.1.62.61.1"><a class="ltx_ref ltx_href" href="https://data.europa.eu/data/datasets/elrc_1188?locale=en" title="">Seimas transcripts</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.62.61.2">EN,LT</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.62.61.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.62.61.4">47.319.919</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.62.61.5">0,02%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.63.62">
<td class="ltx_td ltx_align_left" id="A1.T9.1.63.62.1"><a class="ltx_ref ltx_href" href="https://en.wikinews.org/wiki/Main_Page" title="">Wikimedia: WikiNews</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.63.62.2">EU24</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.63.62.3">News</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.63.62.4">46.936.064</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.63.62.5">0,02%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.64.63">
<td class="ltx_td ltx_align_left" id="A1.T9.1.64.63.1"><a class="ltx_ref ltx_href" href="https://www.gaois.ie/en/corpora/parallel/data/" title="">Irish legislation</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.64.63.2">GA</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.64.63.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.64.63.4">29.136.258</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.64.63.5">0,01%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.65.64">
<td class="ltx_td ltx_align_left" id="A1.T9.1.65.64.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/AI-team-UoA/greek_legal_code" title="">Greek legal code</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.65.64.2">EL</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.65.64.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.65.64.4">28.791.315</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.65.64.5">0,01%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.66.65">
<td class="ltx_td ltx_align_left" id="A1.T9.1.66.65.1"><a class="ltx_ref ltx_href" href="https://zenodo.org/records/10030647" title="">Corpus of Decisions: International Court of Justice</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.66.65.2">EN, FR</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.66.65.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.66.65.4">25.979.848</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.66.65.5">0,01%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.67.66">
<td class="ltx_td ltx_align_left" id="A1.T9.1.67.66.1"><a class="ltx_ref ltx_href" href="https://elrc-share.eu/repository/search/?q=PRINCIPLEselected_facets=languageNameFilter_exact%3AIrish" title="">PRINCIPLE Anonymized English-Irish DCHG</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.67.66.2">GA,EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.67.66.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.67.66.4">25.776.098</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.67.66.5">0,01%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.68.67">
<td class="ltx_td ltx_align_left" id="A1.T9.1.68.67.1"><a class="ltx_ref ltx_href" href="https://zenodo.org/records/7691841" title="">Corpus der Entscheidungen: Bundesfinanzhofs</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.68.67.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.68.67.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.68.67.4">22.635.865</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.68.67.5">0,01%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.69.68">
<td class="ltx_td ltx_align_left" id="A1.T9.1.69.68.1"><a class="ltx_ref ltx_href" href="https://zenodo.org/records/5910152" title="">Corpus der Entscheidungen: Bundesverfassungsgerichts</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.69.68.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.69.68.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.69.68.4">20.679.001</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.69.68.5">0,01%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.70.69">
<td class="ltx_td ltx_align_left" id="A1.T9.1.70.69.1"><a class="ltx_ref ltx_href" href="https://zenodo.org/records/4006645" title="">Corpus der Entscheidungen: Bundesarbeitsgerichts</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.70.69.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.70.69.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.70.69.4">18.487.621</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.70.69.5">0,01%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.71.70">
<td class="ltx_td ltx_align_left" id="A1.T9.1.71.70.1"><a class="ltx_ref ltx_href" href="https://github.com/hendrycks/math" title="">AmpsMath: Khan</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.71.70.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.71.70.3">Math</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.71.70.4">12.081.447</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.71.70.5">0,00%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.72.71">
<td class="ltx_td ltx_align_left" id="A1.T9.1.72.71.1"><a class="ltx_ref ltx_href" href="https://doi.org/10.5281/zenodo.3611246" title="">German Polticial Speeches Corpus</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.72.71.2">DE</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.72.71.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.72.71.4">11.366.979</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.72.71.5">0,00%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.73.72">
<td class="ltx_td ltx_align_left" id="A1.T9.1.73.72.1"><a class="ltx_ref ltx_href" href="https://pub.cl.uzh.ch/wiki/public/pacoco/swiss_legislation_corpus" title="">Swiss Legislation Corpus (SLC)</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.73.72.2">DE,FR</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.73.72.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.73.72.4">10.407.743</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.73.72.5">0,00%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.74.73">
<td class="ltx_td ltx_align_left" id="A1.T9.1.74.73.1"><a class="ltx_ref ltx_href" href="https://github.com/nkrusch/fi-news-corpus" title="">Finnish Language Text Corpus: FI news</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.74.73.2">FI</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.74.73.3">News</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.74.73.4">3.650.602</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.74.73.5">0,00%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.75.74">
<td class="ltx_td ltx_align_left" id="A1.T9.1.75.74.1"><a class="ltx_ref ltx_href" href="https://www.gaois.ie/en/corpora/monolingual" title="">Corpus of Contemporary Irish: Gaois corpus</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.75.74.2">GA</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.75.74.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.75.74.4">2.168.867</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.75.74.5">0,00%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.76.75">
<td class="ltx_td ltx_align_left" id="A1.T9.1.76.75.1"><a class="ltx_ref ltx_href" href="https://zenodo.org/records/7051934" title="">Corpus of Decisions: Permanent Court of International Justice</a></td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.76.75.2">EN</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.76.75.3">Law and Admin.</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.76.75.4">2.117.333</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.76.75.5">0,00%</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="A1.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="472" id="A1.F4.sf1.g1" src="x3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>MT-Bench-EN</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="472" id="A1.F4.sf2.g1" src="x4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>MT-Bench-DE</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="472" id="A1.F4.sf3.g1" src="x5.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>MT-Bench-FR</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F4.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="472" id="A1.F4.sf4.g1" src="x6.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>MT-Bench-IT</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F4.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="472" id="A1.F4.sf5.g1" src="x7.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>MT-Bench-ES</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>In-depth MT-Bench-X single score quality assessment by GPT-4.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 30 15:56:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
