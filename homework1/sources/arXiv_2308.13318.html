<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.13318] iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention</title><meta property="og:description" content="This paper explores the role of eye gaze in human-robot interactions and proposes a novel system for detecting objects gazed by the human using solely visual feedback. The system leverages on face detection, human atte…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.13318">

<!--Generated on Wed Feb 28 11:08:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shiva Hanifi<sup id="id8.7.id1" class="ltx_sup"><span id="id8.7.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Elisa Maiettini<sup id="id9.8.id2" class="ltx_sup"><span id="id9.8.id2.1" class="ltx_text ltx_font_italic">1†</span></sup>, Maria Lombardi<sup id="id10.9.id3" class="ltx_sup"><span id="id10.9.id3.1" class="ltx_text ltx_font_italic">1†</span></sup> and Lorenzo Natale<sup id="id11.10.id4" class="ltx_sup"><span id="id11.10.id4.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes">*This work received funding by the Italian National Institute for Insurance against Accidents at Work (INAIL) ergoCub Project, the project Fit for Medical Robotics (Fit4MedRob) - PNRR MUR Cod. PNC0000007 - CUP: B53C22006960001 and Future Artificial Intelligence Research (FAIR) – PNRR MUR Cod. PE0000013 - CUP: E63C22001940006.<math id="id5.5.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id5.5.m1.1a"><mo id="id5.5.m1.1.1" xref="id5.5.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id5.5.m1.1b"><ci id="id5.5.m1.1.1.cmml" xref="id5.5.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m1.1c">\dagger</annotation></semantics></math>These authors contributed equally to the work<sup id="id12.11.id1" class="ltx_sup"><span id="id12.11.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Shiva Hanifi, Elisa Maiettini, Maria Lombardi and Lorenzo Natale are with Humanoid, Sensing and Perception Group, Istituito Italiano di Tecnologia, Genoa, Italy.
<span id="id13.12.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">shiva.hanifi@iit.it, elisa.maiettini@iit.it, maria.lombardi1@iit.it lorenzo.natale@iit.it</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.1" class="ltx_p">This paper explores the role of eye gaze in human-robot interactions and proposes a novel system for detecting objects gazed by the human using solely visual feedback. The system leverages on face detection, human attention prediction, and online object detection, and it allows the robot to perceive and interpret human gaze accurately, paving the way for establishing joint attention with human partners. Additionally, a novel dataset collected with the humanoid robot iCub is introduced, comprising over <math id="id7.1.m1.2" class="ltx_Math" alttext="22,000" display="inline"><semantics id="id7.1.m1.2a"><mrow id="id7.1.m1.2.3.2" xref="id7.1.m1.2.3.1.cmml"><mn id="id7.1.m1.1.1" xref="id7.1.m1.1.1.cmml">22</mn><mo id="id7.1.m1.2.3.2.1" xref="id7.1.m1.2.3.1.cmml">,</mo><mn id="id7.1.m1.2.2" xref="id7.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="id7.1.m1.2b"><list id="id7.1.m1.2.3.1.cmml" xref="id7.1.m1.2.3.2"><cn type="integer" id="id7.1.m1.1.1.cmml" xref="id7.1.m1.1.1">22</cn><cn type="integer" id="id7.1.m1.2.2.cmml" xref="id7.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="id7.1.m1.2c">22,000</annotation></semantics></math> images from ten participants gazing at different annotated objects. This dataset serves as a benchmark for evaluating the performance of the proposed pipeline. The paper also includes an experimental analysis of the pipeline’s effectiveness in a human-robot interaction setting, examining the performance of each component. Furthermore, the developed system is deployed on the humanoid robot iCub, and a supplementary video showcases its functionality. The results demonstrate the potential of the proposed approach to enhance social awareness and responsiveness in social robotics, as well as improve assistance and support in collaborative scenarios, promoting efficient human-robot collaboration.
The code and the collected dataset will be released upon acceptance.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Any face-to-face interaction between two people is characterised by a continuous exchanging of social signals, such as gaze, gestures, facial expression and so on. Such kind of non-verbal communication is possible because both interacting individuals are able to see each other, perceive and understand the social information enclosed in such cues. In this paper, we focus on one of the most crucial social cue, that is the eye gaze. Eye gaze plays a pivotal role in many mechanisms of social cognition, e.g. joint attention, in regulating and monitoring turn taking, signalling attention and intention. Much neuropsychological evidences highlighted the close relationship between gaze direction and attention, indicating that gaze functions are actively involved and influenced by spatial attention systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. For example, it is more likely that the gaze is directed toward an object rather than toward empty space.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this context, the ability of a robot to determine what the human is looking at (e.g., an object) has numerous practical implications across various domains. In social robotics, it can enhance the robot’s social awareness and responsiveness, allowing it to engage in more natural and contextually appropriate interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. For instance, a robot capable of recognizing the object a human is looking at can infer the person’s preferences and tailor its actions or suggestions accordingly, thereby fostering personalized and adaptive experiences. Moreover, in collaborative scenarios, such as industrial settings or domestic environments, the robot’s capacity to understand human attention can significantly improve its ability to assist and support humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. By detecting the objects that humans focus on during a task, the robot can anticipate their needs, provide relevant information, or even proactively offer assistance, thereby streamlining workflow and promoting efficient human-robot collaboration.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2308.13318/assets/Images/DataCollectionSetup.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.6.2.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ObjectAttention<span id="S1.F1.2.1.1" class="ltx_text ltx_font_upright"> dataset: data collection setup with <math id="S1.F1.2.1.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S1.F1.2.1.1.m1.1b"><mn id="S1.F1.2.1.1.m1.1.1" xref="S1.F1.2.1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S1.F1.2.1.1.m1.1c"><cn type="integer" id="S1.F1.2.1.1.m1.1.1.cmml" xref="S1.F1.2.1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.1.1.m1.1d">6</annotation></semantics></math> objects from the YCB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> placed on a table between a human partner and iCub. Objects from left to right: </span>Driller, Bleach, Sugarbox, Mustard, MasterChef, Pringles</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we present a novel application for human-robot interaction that leverages computer vision techniques to enable robots to detect the object the human partner is gazing at. Our proposed system combines an online object detection algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> with gaze tracking technologies, providing the robot with online information about the objects that capture the human’s attention. This integration offers a valuable cognitive capability for the robot, empowering it to perceive and interpret the human gaze within its environment accurately. That can be the first step to make the robot able to establish a conscious joint attention with the human partner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The contribution of this work are as follows:</p>
</div>
<div id="S1.p5" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a pipeline to detect the target of human attention during an interaction with a robot. This pipeline leverages on a face detection, a human attention prediction, and an online object detection to detect the object that the human focuses on.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.2" class="ltx_p">We present the ObjectAttention dataset collected with the humanoid robot iCub <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> where <math id="S1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S1.I1.i2.p1.1.m1.1a"><mn id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><cn type="integer" id="S1.I1.i2.p1.1.m1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">10</annotation></semantics></math> participants gaze at different objects placed randomly on a table in front of the robot, totalling an amount of over <math id="S1.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="22K" display="inline"><semantics id="S1.I1.i2.p1.2.m2.1a"><mrow id="S1.I1.i2.p1.2.m2.1.1" xref="S1.I1.i2.p1.2.m2.1.1.cmml"><mn id="S1.I1.i2.p1.2.m2.1.1.2" xref="S1.I1.i2.p1.2.m2.1.1.2.cmml">22</mn><mo lspace="0em" rspace="0em" id="S1.I1.i2.p1.2.m2.1.1.1" xref="S1.I1.i2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S1.I1.i2.p1.2.m2.1.1.3" xref="S1.I1.i2.p1.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.2.m2.1b"><apply id="S1.I1.i2.p1.2.m2.1.1.cmml" xref="S1.I1.i2.p1.2.m2.1.1"><times id="S1.I1.i2.p1.2.m2.1.1.1.cmml" xref="S1.I1.i2.p1.2.m2.1.1.1"></times><cn type="integer" id="S1.I1.i2.p1.2.m2.1.1.2.cmml" xref="S1.I1.i2.p1.2.m2.1.1.2">22</cn><ci id="S1.I1.i2.p1.2.m2.1.1.3.cmml" xref="S1.I1.i2.p1.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.2.m2.1c">22K</annotation></semantics></math> images. This dataset serves as a benchmark for application performance evaluation.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We perform an experimental analysis of the proposed pipeline to evaluate its effectiveness in the considered HRI setting. For doing that we use the collected dataset and we study the performance of the components of the entire system.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Finally, we deploy the system on the humanoid robot iCub. The video submitted as supplementary material shows the functioning of the developed system.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The rest of the paper is organised as follows. In Sec. <a href="#S2" title="II RELATED WORK ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we overview the state of art on robots having social skills and on HRI-based learning pipeline. The proposed architecture and the collected dataset are described in Sec. <a href="#S3" title="III METHODS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> and in Sec. <a href="#S4" title="IV DATASET ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, respectively. Sec. <a href="#S5" title="V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> reports the results obtained in our experimental analysis. Finally, in Sec. <a href="#S6" title="VI CONCLUSIONS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> we conclude the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">RELATED WORK</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The problem of endowing robot with the ability to understand human behaviour and specifically the social cue of the gaze has been largely studied in the literature. It is mainly addressed following two different strategies: 1) gaze estimation (i.e. estimating the gaze vector or mutual gaze events) and 2) gaze attention prediction (i.e. understand where the human is visually attending in terms of saliency map).</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Following the <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">gaze estimation strategy</span>, a learning architecture to detect events of mutual gaze was proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. This study emphasized the importance of mutual eye contact as crucial social cue in face-to-face interactions since it can be a signal of the readiness and attention of the interacting partner. In addition to the detection of mutual gaze events, several works focused on the estimation of the human gaze as a 2D or 3D vector. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> a feed-forward convolutional neural network was used to extract features from the RGB image (face image, right and left eye images) and produce as output the <math id="S2.p2.1.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S2.p2.1.m1.2a"><mrow id="S2.p2.1.m1.2.3.2" xref="S2.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S2.p2.1.m1.2.3.2.1" xref="S2.p2.1.m1.2.3.1.cmml">(</mo><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">x</mi><mo id="S2.p2.1.m1.2.3.2.2" xref="S2.p2.1.m1.2.3.1.cmml">,</mo><mi id="S2.p2.1.m1.2.2" xref="S2.p2.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S2.p2.1.m1.2.3.2.3" xref="S2.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.2b"><interval closure="open" id="S2.p2.1.m1.2.3.1.cmml" xref="S2.p2.1.m1.2.3.2"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">𝑥</ci><ci id="S2.p2.1.m1.2.2.cmml" xref="S2.p2.1.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.2c">(x,y)</annotation></semantics></math> coordinate of the gaze prediction. The use of the CNN architecture to estimate the 2D gaze vector is also proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, but with the novelty to extract features from only one eye. Strength of such an One-eye Gaze estimation approach is that in real worlds conditions it is very common that the human face and/or their eyes are partially obscured (i.e. in this case the two-eyes models would probably fail). An example of predicting 3D gaze vector can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The authors proposed a combination of a regression network (FAR-net) and an evaluation network (E-Net) able to exploit the asymmetry and the difference between left and right eyes.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Much effort has been spent in building <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">attention architectures</span> that, for example, enable joint attention between a human and the humanoid robot iCub with the aim to improve the performance of visual learning methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Another example is in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> where the authors proposed a method, called Attention Flow, to learn joint attention in an end-to-end fashion by employing saliency-augmented attention maps and two innovative convolutional attention mechanisms that choose important cues and improve joint attention localization. Moreover, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, a spatial-temporal neural network (LSTM network) was proposed to detect shared attention intervals in third-person social videos and predict shared attention locations in frames.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">The problem of the gaze following was addressed, instead, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The former used a CNN architecture that leverages the geometry of the scene. Specifically, it takes as input the RGB frame (with the person’s head location and the eye coordinates within that frame) and a set of neighboring frames from the same video and identifies which of the neighboring frames, if any, contain the object being looked at and the coordinates of the person’s gaze. The latter proposed a two-stage solution for gaze following. In the first stage, which is a gaze direction pathway, it takes as an input the head image and the head position and generates multi-scale gaze direction fields by using ResNet-50 as features extractor. In the second stage, which is a heatmap pathway, the framework tries to infer the gaze point taking into account the gaze direction and the context information of the objects along the gaze direction.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2308.13318/assets/Images/pipeline_humanattention.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="274" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.5.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.6.2" class="ltx_text" style="font-size:90%;">Pipeline of the presented architecture. The <span id="S2.F2.6.2.1" class="ltx_text ltx_font_italic">Human Attention Estimation</span> pathway produces a heatmap of the gaze target. This heatmap as well as the bouding boxes and labels from the <span id="S2.F2.6.2.2" class="ltx_text ltx_font_italic">Object Detection</span> module are then used by the <span id="S2.F2.6.2.3" class="ltx_text ltx_font_italic">Attentive Object Detection</span> module to predict the specific object that is visually attended by the human.</span></figcaption>
</figure>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Despite gaze estimation and human attention have been largely studied, very few works exist that combine the estimation of the human attention with the prediction of the target object. Among these few, authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> proposed an approach to predict human referential gaze having both the person and object of attention visible in the image. The proposed network contains two pathways: one for estimating the head direction and another for salient objects in the image. Such a work was used as backbone in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and extended in order to address out-of-frame gaze targets by simultaneously learning gaze angle and saliency. Moreover, differently from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, an LSTM-based spatio-temporal model is used to leverage on the temporal coherence of the frames in videos to improve gaze direction estimation. However, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, only the gaze direction of the human is predicted, while the information of the target object is not provided.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">In this paper, we exploit the LSTM-based spatio-temporal model presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and we adapt it to the considered HRI setting by fine-tuning it on the proposed ObjectAttention dataset. Moreover, we use it in conjunction with a human pose estimation and a face detector to make the pipeline run online on the humanoid robot iCub. Finally, by integrating an online object detection method, we allow the system to predict the class label and location of the gaze target object. Note that, differently from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, by using <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> for object detection, the entire system can be easily and quickly (just few seconds) adapted to detect novel target objects.
All the mentioned improvements result in an online robotic application that makes the robot capable of inferring where the human partner’s attention is targeted while interacting with them.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">METHODS</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The pipeline proposed in this study is made out of three major pathways as represented in Figure <a href="#S2.F2" title="Figure 2 ‣ II RELATED WORK ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>: the <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">Human Attention Estimation</span> pathway that aims at detecting the attention target of the human, the <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">Object Detection</span> pathway that recognizes and localizes the objects in the scene, and finally the <span id="S3.p1.1.3" class="ltx_text ltx_font_italic">Attentive Object Detection</span> pathway that provides the gazed object from the human.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Human Attention Estimation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Human Attention Estimation</span> pathway is characterised mainly by three distinct modules: 1) Human Pose Estimation, 2) Face Detection, and 3) Visual Target Detection. Having the RGB image as input, the final output of this pathway is the real-time prediction of the attention target of the human, provided in the form of a heatmap. Figure <a href="#S2.F2" title="Figure 2 ‣ II RELATED WORK ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows how the three modules are connected. In the following paragraphs we describe each of them.
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.5" class="ltx_p"><span id="S3.SS1.p2.5.1" class="ltx_text ltx_font_bold">Human Pose Estimation</span>. We rely on the OpenPose architecture, proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Briefly, OpenPose is a system for multi-human pose estimation, that receives in input RGB frames and predicts the location in pixel <math id="S3.SS1.p2.1.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S3.SS1.p2.1.m1.2a"><mrow id="S3.SS1.p2.1.m1.2.3.2" xref="S3.SS1.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.2.3.2.1" xref="S3.SS1.p2.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">x</mi><mo id="S3.SS1.p2.1.m1.2.3.2.2" xref="S3.SS1.p2.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.2.3.2.3" xref="S3.SS1.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.2b"><interval closure="open" id="S3.SS1.p2.1.m1.2.3.1.cmml" xref="S3.SS1.p2.1.m1.2.3.2"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑥</ci><ci id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.2c">(x,y)</annotation></semantics></math> of <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="135" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">135</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn type="integer" id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">135</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">135</annotation></semantics></math> anatomical key-points of each person in the image (<math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mn id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><cn type="integer" id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">25</annotation></semantics></math> and <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="70" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mn id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">70</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><cn type="integer" id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">70</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">70</annotation></semantics></math> key-points for the body pose and for the face, respectively), associating also a confidence level <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">k</annotation></semantics></math> to each prediction. In our pipeline, we use this approach to detect the human in front of the robot.
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Face Detection module</span>. We rely on the method for face recognition presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to detect and extract the human face from image. Specifically, the face key-points extracted by the <span id="S3.SS1.p3.1.2" class="ltx_text ltx_font_italic">Human Pose Estimation</span> are used as input for this module. The output is the bounding box that indicates the head of the person in front of the robot. Note that, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, they assume that the information of the face location is available, reading it from a txt file. That is a strong limit in applying the method in online robotic applications. In this work, we use the <span id="S3.SS1.p3.1.3" class="ltx_text ltx_font_italic">Human Pose Estimation</span> jointly with the <span id="S3.SS1.p3.1.4" class="ltx_text ltx_font_italic">Face Detection</span> to provide, online, the input to the <span id="S3.SS1.p3.1.5" class="ltx_text ltx_font_italic">Visual Target Detection module</span>, allowing the pipeline to run on the real robot.
<br class="ltx_break"></p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F3.17" class="ltx_p ltx_figure_panel"><span id="S3.F3.17.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S3.F3.17.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:533.7pt;height:350.5pt;vertical-align:-350.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F3.17.1.1.1" class="ltx_p"><span id="S3.F3.17.1.1.1.1" class="ltx_text">




</span></span>
</span></span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.1" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s1_francesco_1.jpg" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.2" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s2_francesco_2.jpg" id="S3.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.3" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s32_francesco_3.jpg" id="S3.F3.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.4" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s42_francesco_2.jpg" id="S3.F3.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.5" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s5_francesco_1.jpeg" id="S3.F3.5.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F3.18" class="ltx_p ltx_figure_panel"><span id="S3.F3.18.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S3.F3.18.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:533.7pt;height:338.5pt;vertical-align:-338.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F3.18.1.1.1" class="ltx_p"><span id="S3.F3.18.1.1.1.1" class="ltx_text">




</span></span>
</span></span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.6" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s12_andrea_1.jpg" id="S3.F3.6.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.7" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s22_andrea_2.jpg" id="S3.F3.7.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.8" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s32_andrea_3.jpg" id="S3.F3.8.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.9" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s42_andrea_3.jpg" id="S3.F3.9.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.10" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s52_andrea_2.jpg" id="S3.F3.10.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.10.3.2" class="ltx_text" style="font-size:90%;">Sample frames extracted from the proposed <span id="S3.F3.10.3.2.1" class="ltx_text ltx_font_italic">ObjectAttention</span> dataset, representing different participants performing the gazing task, over five sessions with different number of objects.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F3.19" class="ltx_p ltx_figure_panel"><span id="S3.F3.19.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S3.F3.19.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:533.7pt;height:338.5pt;vertical-align:-338.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F3.19.1.1.1" class="ltx_p"><span id="S3.F3.19.1.1.1.1" class="ltx_text">




</span></span>
</span></span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.11" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s12_shiva_1.jpg" id="S3.F3.11.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.12" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s21_shiva_1.jpg" id="S3.F3.12.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.13" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s31_shiva_1.jpg" id="S3.F3.13.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.14" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s41_shiva_2.jpg" id="S3.F3.14.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.15" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/s52_shiva_1.jpg" id="S3.F3.15.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
</div>
</figure>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Visual Target Detection module</span>. This module takes as input the RGB image from the camera of the robot and the bounding box of the face extracted by the previous module (namely, the <span id="S3.SS1.p4.1.2" class="ltx_text ltx_font_italic">Face Detection</span>). It provides as output the heatmap representing the area in the image that are more likely to be the target of the human attention. Specifically, this is a matrix of the same dimensions of the image, where each cell corresponds to a pixel in the image. The value of each cell ranges from 0 to 1 (respectively, the lowest and the highest probability to be –or to be close to– the target of human attention). For this module, we rely on the network presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. This is composed by three main parts. The first one is called <span id="S3.SS1.p4.1.3" class="ltx_text ltx_font_italic">Head Conditioning Branch</span> and it uses the head bounding box, encoded into a convolutional feature map (head feature map) together with the information of the location of the human’s head in the image to predict a first attention map. The second part is the <span id="S3.SS1.p4.1.4" class="ltx_text ltx_font_italic">Main Scene Branch</span>, that multiplies the convolutional feature map of the entire image with the attention map and concatenates the result with the previously computed head feature map. The final tensor represents the input for the third and last part, namely, the <span id="S3.SS1.p4.1.5" class="ltx_text ltx_font_italic">Recurrent Attention Prediction Branch</span>. This, firstly encodes the tensor in order to be used as input for a convolutional Long Short-Term Memory network. Then, the output of this latter is up-sampled by a decoder into the final attention heatmap. In this work, we fine-tuned the weights of the network by using our dataset and the resulting model is used for the developed application and for the experimental analysis. The details regarding the dataset and the training process are reported in Sec. <a href="#S4" title="IV DATASET ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and <a href="#S5.SS1" title="V-A Model training ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a>, respectively.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Object detection</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">Object Detection</span> pathway is mainly characterized by one module that takes the RGB images from the camera of the robot as input and gives as output the bounding boxes of the objects of interest present in the scene. For doing that, we rely on the On-line object detection approach, presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This system is based on Mask R-CNN architecture conveniently adapted so that it can be re-trained online with a few seconds of training time, allowing for a fast adaptation without a performance loss. We trained the On-line object detection with data acquired using the pipeline described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. More details about the training process are reported in Sec. <a href="#S5.SS1" title="V-A Model training ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Attentive object detection</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The third pathway combines all the extracted information from the human attention with the objects in the scene to detect the object that is the target of the human gaze. Specifically, it takes as input the RGB image, the heatmap from the <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">Visual Target Detection Module</span>, and all the bounding boxes and labels predicted by the <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">Object Detection</span>. The output of this module is the bounding box and label of the attended object.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Firstly, the heatmap is processed to identify the contour of the area with the highest values which corresponds to the area of the image where the human is focusing their gaze on (the hottest part of the heatmap). Then, we compute the center of the obtained area and the surrounding bounding box. We use this information to select the object that, most likely, is the focus of human attention. Precisely, we choose the object that either presents the smallest Intersection over Union (IoU) with the bounding box of the hottest part of the heatmap or, if this latter does not intersect any object box, we select the one whose center is the closest to the center of the hottest part.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">DATASET</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">A major contribution of this work is the ObjectAttention dataset. We collected it with the aim of representing a human-robot collaboration in a table-top scenario. In this setting, the human partner gazes at different objects and the robot partner understands the gaze direction and the target object.
In this work, we used this dataset to train and evaluate the performance of each component of the presented architecture and to validate the entire system. The dataset will be made available for reproducibility.
In the next paragraphs, we present the dataset by describing the data collection and annotation processes that we performed.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Data collection sessions</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.3" class="ltx_p">For the data collection, we recruited a total of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">10</annotation></semantics></math> participants, consisting of <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><cn type="integer" id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">4</annotation></semantics></math> females and <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mn id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><cn type="integer" id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">6</annotation></semantics></math> males and all of them had normal or corrected normal vision. The data collection was conducted with the iCub Humanoid robot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and all participants provided written informed consent.
To collect the dataset we positioned the iCub, with a RealSense 415 camera mounted on its head, on one side of a table. On the table, we placed up to five distinct objects from the YCB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (an example of this setup is illustrated in Fig. <a href="#S1.F1" title="Figure 1 ‣ I INTRODUCTION ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The objects were placed in various arrangements, different for all the participants. For different sessions, the participants are instructed to stand on the other side of the table facing the robot and look at the requested object in a natural and spontaneous manner. The frames were recorded using the RealSense 415 camera and the YARP middleware <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.2" class="ltx_p">For the sake of diversity, we collected data in <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">5</annotation></semantics></math> different sessions for each participant, starting with one object in the scene and gradually increasing the number of objects up to five, in each session.
For each session, we performed two distinct trials, with two different settings obtained keeping the same number of objects but changing the object types and their arrangements on the table. For each session, for each trial, each object is gazed at for a 5-second period by the participant and the gaze target ground truth is annotated considering the instructions given to the participant. We collect one video for each different object lasting <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn type="integer" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">5</annotation></semantics></math> seconds.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.5" class="ltx_p">The resulting dataset consists of <math id="S4.SS1.p3.1.m1.2" class="ltx_Math" alttext="22,732" display="inline"><semantics id="S4.SS1.p3.1.m1.2a"><mrow id="S4.SS1.p3.1.m1.2.3.2" xref="S4.SS1.p3.1.m1.2.3.1.cmml"><mn id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">22</mn><mo id="S4.SS1.p3.1.m1.2.3.2.1" xref="S4.SS1.p3.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.p3.1.m1.2.2" xref="S4.SS1.p3.1.m1.2.2.cmml">732</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.2b"><list id="S4.SS1.p3.1.m1.2.3.1.cmml" xref="S4.SS1.p3.1.m1.2.3.2"><cn type="integer" id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">22</cn><cn type="integer" id="S4.SS1.p3.1.m1.2.2.cmml" xref="S4.SS1.p3.1.m1.2.2">732</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.2c">22,732</annotation></semantics></math> frames. When considered as a sequence there are <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="250" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mn id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><cn type="integer" id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">250</annotation></semantics></math> number of videos, depicting <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mn id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><cn type="integer" id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">10</annotation></semantics></math> participants in <math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mn id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><cn type="integer" id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">2</annotation></semantics></math> different trials for each of the <math id="S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS1.p3.5.m5.1a"><mn id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><cn type="integer" id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">5</annotation></semantics></math> sessions, gazing at the different objects. Additionally, for at least one trial in each of the sessions, we placed a distracting object (i.e., the <span id="S4.SS1.p3.5.1" class="ltx_text ltx_font_italic">pringles</span> object) on the table, at which the participant was not asked to gaze. Example frames for three participants for different sessions are reported in Fig. <a href="#S3.F3.10" title="Figure 3 ‣ III-A Human Attention Estimation ‣ III METHODS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Finally, one of the motivations that have driven us to collect and annotate a new dataset is the fact that the dataset used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> contains much more conditions in which the gaze was directed toward the upper part of the map. This is clearly visible in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-A Data collection sessions ‣ IV DATASET ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> that depicts the density maps of the gaze targets for the dataset in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> (left) and the one we collected (right). Our dataset, in contrast, considers the situation in which the human and robot are looking at objects placed on a table. To improve performance in the considered setting, the method proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> was fined-tuned using our dataset.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2308.13318/assets/Images/target_density.jpeg" id="S4.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="126" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2308.13318/assets/Images/object_location_density_circles_blurred.jpg" id="S4.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="126" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Gaze target location density. (a) The density map of the gaze targets for the dataset in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and (b) for our dataset.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Data annotation</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For each setting, the bounding box of the participant’s head and the target object are required.
The participants’ head bounding box was extracted using the key-points estimated by <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">Openpose</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and manually refined to be considered as ground truth. Furthermore, we manually annotated the bounding boxes and classes of all the objects on the table, highlighting the one that is the target of the participant’s attention. The gaze target point is chosen as the center of the gazed object. The bounding boxes labeling was done using the <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">LabelImg<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_upright">1</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_font_upright ltx_ref_self">https://github.com/tzutalin/labelImg</span></span></span></span></span> framework.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">EXPERIMENTS</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">While the video submitted as supplementary material shows the functioning of the proposed application, in this section, we demonstrate its effectiveness by reporting on the experimental evaluation. Firstly, we describe how we trained some of the components of the pipeline (Sec. <a href="#S5.SS1" title="V-A Model training ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a>) and the experimental conditions used for our analysis (Sec. <a href="#S5.SS2" title="V-B Experimental Setup ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>). Then, we present the obtained results, i.e., we demonstrate the improvement in using the collected data to fine-tune the <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">Visual Target Detection module</span> (sec. <a href="#S5.SS3" title="V-C Fine-tuning of the Visual Target Detection module ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-C</span></span></a>) and we conduct experiments to evaluate the overall performance of our presented architecture (Sec. <a href="#S5.SS4" title="V-D Accuracy evaluation of the pipeline ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-D</span></span></a> and <a href="#S5.SS5" title="V-E Performance analysis ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-E</span></span></a>).</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Model training</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In this work, two different modules have been re-trained to better fit the considered conditions, namely, the <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">Object Detection</span> and the <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_italic">Visual Target Detection</span>.
<br class="ltx_break"></p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Object Detection training</span>. We trained the On-line object detection with data acquired using the pipeline described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Specifically, a human teacher shows the objects of interest to the robot, one at a time, holding them in their hand and moving it in front of the robot for around <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mn id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><cn type="integer" id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">30</annotation></semantics></math> seconds. The information from the robot’s depth sensors is used to localise the object and follow it with the robot’s gaze. The latter can be segmented and the corresponding bounding box automatically assigned and gathered as ground truth together with the object’s label, provided verbally. After each object demonstration, the collected data is used to update the current object detection model, optimizing its weights in few seconds.
<br class="ltx_break"></p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.7" class="ltx_p"><span id="S5.SS1.p3.7.4" class="ltx_text ltx_font_bold">Visual Target Detection fine-tuning</span>. To fine-tune the Visual Target Detection module, we randomly split the ObjectAttention dataset by participants, considering approximately <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mn id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">70</mn><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">70\%</annotation></semantics></math> of the dataset (data from <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mn id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><cn type="integer" id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">7</annotation></semantics></math> participants) as train set, and the remaining <math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="30\%" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><mrow id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml"><mn id="S5.SS1.p3.3.m3.1.1.2" xref="S5.SS1.p3.3.m3.1.1.2.cmml">30</mn><mo id="S5.SS1.p3.3.m3.1.1.1" xref="S5.SS1.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><apply id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1"><csymbol cd="latexml" id="S5.SS1.p3.3.m3.1.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.p3.3.m3.1.1.2.cmml" xref="S5.SS1.p3.3.m3.1.1.2">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">30\%</annotation></semantics></math> as test set, ensuring no overlap of data between the train and test splits.
We fine-tuned the spatio-temporal model of the <span id="S5.SS1.p3.7.5" class="ltx_text ltx_font_italic">Visual Target Detection</span> on the train set performing a warm training re-start with the pre-trained weights provided by the authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and empirically choosing the hyper-parameters as follows: <span id="S5.SS1.p3.7.6" class="ltx_text ltx_font_italic">learning rate</span> <math id="S5.SS1.p3.4.m4.1" class="ltx_Math" alttext="=5e^{-5}" display="inline"><semantics id="S5.SS1.p3.4.m4.1a"><mrow id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml"><mi id="S5.SS1.p3.4.m4.1.1.2" xref="S5.SS1.p3.4.m4.1.1.2.cmml"></mi><mo id="S5.SS1.p3.4.m4.1.1.1" xref="S5.SS1.p3.4.m4.1.1.1.cmml">=</mo><mrow id="S5.SS1.p3.4.m4.1.1.3" xref="S5.SS1.p3.4.m4.1.1.3.cmml"><mn id="S5.SS1.p3.4.m4.1.1.3.2" xref="S5.SS1.p3.4.m4.1.1.3.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p3.4.m4.1.1.3.1" xref="S5.SS1.p3.4.m4.1.1.3.1.cmml">​</mo><msup id="S5.SS1.p3.4.m4.1.1.3.3" xref="S5.SS1.p3.4.m4.1.1.3.3.cmml"><mi id="S5.SS1.p3.4.m4.1.1.3.3.2" xref="S5.SS1.p3.4.m4.1.1.3.3.2.cmml">e</mi><mrow id="S5.SS1.p3.4.m4.1.1.3.3.3" xref="S5.SS1.p3.4.m4.1.1.3.3.3.cmml"><mo id="S5.SS1.p3.4.m4.1.1.3.3.3a" xref="S5.SS1.p3.4.m4.1.1.3.3.3.cmml">−</mo><mn id="S5.SS1.p3.4.m4.1.1.3.3.3.2" xref="S5.SS1.p3.4.m4.1.1.3.3.3.2.cmml">5</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><apply id="S5.SS1.p3.4.m4.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1"><eq id="S5.SS1.p3.4.m4.1.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1.1"></eq><csymbol cd="latexml" id="S5.SS1.p3.4.m4.1.1.2.cmml" xref="S5.SS1.p3.4.m4.1.1.2">absent</csymbol><apply id="S5.SS1.p3.4.m4.1.1.3.cmml" xref="S5.SS1.p3.4.m4.1.1.3"><times id="S5.SS1.p3.4.m4.1.1.3.1.cmml" xref="S5.SS1.p3.4.m4.1.1.3.1"></times><cn type="integer" id="S5.SS1.p3.4.m4.1.1.3.2.cmml" xref="S5.SS1.p3.4.m4.1.1.3.2">5</cn><apply id="S5.SS1.p3.4.m4.1.1.3.3.cmml" xref="S5.SS1.p3.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S5.SS1.p3.4.m4.1.1.3.3.1.cmml" xref="S5.SS1.p3.4.m4.1.1.3.3">superscript</csymbol><ci id="S5.SS1.p3.4.m4.1.1.3.3.2.cmml" xref="S5.SS1.p3.4.m4.1.1.3.3.2">𝑒</ci><apply id="S5.SS1.p3.4.m4.1.1.3.3.3.cmml" xref="S5.SS1.p3.4.m4.1.1.3.3.3"><minus id="S5.SS1.p3.4.m4.1.1.3.3.3.1.cmml" xref="S5.SS1.p3.4.m4.1.1.3.3.3"></minus><cn type="integer" id="S5.SS1.p3.4.m4.1.1.3.3.3.2.cmml" xref="S5.SS1.p3.4.m4.1.1.3.3.3.2">5</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">=5e^{-5}</annotation></semantics></math>, <span id="S5.SS1.p3.5.1" class="ltx_text ltx_font_italic">batch size<math id="S5.SS1.p3.5.1.m1.1" class="ltx_Math" alttext="=4" display="inline"><semantics id="S5.SS1.p3.5.1.m1.1a"><mrow id="S5.SS1.p3.5.1.m1.1.1" xref="S5.SS1.p3.5.1.m1.1.1.cmml"><mi id="S5.SS1.p3.5.1.m1.1.1.2" xref="S5.SS1.p3.5.1.m1.1.1.2.cmml"></mi><mo id="S5.SS1.p3.5.1.m1.1.1.1" xref="S5.SS1.p3.5.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.5.1.m1.1.1.3" xref="S5.SS1.p3.5.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.5.1.m1.1b"><apply id="S5.SS1.p3.5.1.m1.1.1.cmml" xref="S5.SS1.p3.5.1.m1.1.1"><eq id="S5.SS1.p3.5.1.m1.1.1.1.cmml" xref="S5.SS1.p3.5.1.m1.1.1.1"></eq><csymbol cd="latexml" id="S5.SS1.p3.5.1.m1.1.1.2.cmml" xref="S5.SS1.p3.5.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S5.SS1.p3.5.1.m1.1.1.3.cmml" xref="S5.SS1.p3.5.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.5.1.m1.1c">=4</annotation></semantics></math></span>, <span id="S5.SS1.p3.6.2" class="ltx_text ltx_font_italic">chunk size<math id="S5.SS1.p3.6.2.m1.1" class="ltx_Math" alttext="=3" display="inline"><semantics id="S5.SS1.p3.6.2.m1.1a"><mrow id="S5.SS1.p3.6.2.m1.1.1" xref="S5.SS1.p3.6.2.m1.1.1.cmml"><mi id="S5.SS1.p3.6.2.m1.1.1.2" xref="S5.SS1.p3.6.2.m1.1.1.2.cmml"></mi><mo id="S5.SS1.p3.6.2.m1.1.1.1" xref="S5.SS1.p3.6.2.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.6.2.m1.1.1.3" xref="S5.SS1.p3.6.2.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.6.2.m1.1b"><apply id="S5.SS1.p3.6.2.m1.1.1.cmml" xref="S5.SS1.p3.6.2.m1.1.1"><eq id="S5.SS1.p3.6.2.m1.1.1.1.cmml" xref="S5.SS1.p3.6.2.m1.1.1.1"></eq><csymbol cd="latexml" id="S5.SS1.p3.6.2.m1.1.1.2.cmml" xref="S5.SS1.p3.6.2.m1.1.1.2">absent</csymbol><cn type="integer" id="S5.SS1.p3.6.2.m1.1.1.3.cmml" xref="S5.SS1.p3.6.2.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.6.2.m1.1c">=3</annotation></semantics></math></span>, <span id="S5.SS1.p3.7.3" class="ltx_text ltx_font_italic">number of epochs<math id="S5.SS1.p3.7.3.m1.1" class="ltx_Math" alttext="=10" display="inline"><semantics id="S5.SS1.p3.7.3.m1.1a"><mrow id="S5.SS1.p3.7.3.m1.1.1" xref="S5.SS1.p3.7.3.m1.1.1.cmml"><mi id="S5.SS1.p3.7.3.m1.1.1.2" xref="S5.SS1.p3.7.3.m1.1.1.2.cmml"></mi><mo id="S5.SS1.p3.7.3.m1.1.1.1" xref="S5.SS1.p3.7.3.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.7.3.m1.1.1.3" xref="S5.SS1.p3.7.3.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.7.3.m1.1b"><apply id="S5.SS1.p3.7.3.m1.1.1.cmml" xref="S5.SS1.p3.7.3.m1.1.1"><eq id="S5.SS1.p3.7.3.m1.1.1.1.cmml" xref="S5.SS1.p3.7.3.m1.1.1.1"></eq><csymbol cd="latexml" id="S5.SS1.p3.7.3.m1.1.1.2.cmml" xref="S5.SS1.p3.7.3.m1.1.1.2">absent</csymbol><cn type="integer" id="S5.SS1.p3.7.3.m1.1.1.3.cmml" xref="S5.SS1.p3.7.3.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.7.3.m1.1c">=10</annotation></semantics></math></span>.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.2" class="ltx_p">To ensure the statistical relevance of the presented experiments, we repeat the training and evaluation of the model for <math id="S5.SS1.p4.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mn id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><cn type="integer" id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">3</annotation></semantics></math> times, by performing <math id="S5.SS1.p4.2.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S5.SS1.p4.2.m2.1a"><mn id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.1b"><cn type="integer" id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">3</annotation></semantics></math> different splits of the dataset as described above. We report performance in terms of mean and standard deviation for the considered metrics over all the repetitions as shown in Tab. <a href="#S5.T1" title="TABLE I ‣ V-C Fine-tuning of the Visual Target Detection module ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Experimental Setup</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The performance of the <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">Visual Target Detection</span> was evaluated in terms of the <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">Area Under the Curve</span> (AUC) and <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_italic">Distance</span> metrics. For the first metric, each cell in the spatially-discretized image is classified as either the gaze target or not. The ground truth comes from thresholding a Gaussian confidence mask centered at the human annotator’s target location. The final heatmap provides the prediction confidence score which is evaluated at different thresholds in the ROC curve. The AUC of this ROC curve is considered. The <span id="S5.SS2.p1.1.4" class="ltx_text ltx_font_italic">Distance</span> metric, instead, is defined as the <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="\L_{2}" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><msub id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">Ł</mi><mn id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">italic-Ł</ci><cn type="integer" id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\L_{2}</annotation></semantics></math> distance between the annotated target location and the prediction given by the pixel of the maximum value in the heatmap, with image width and height normalized to 1.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Finally, the performance for the entire pipeline is measured as the <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">Accuracy</span> of the detected gazed objects. Specifically, for each image, we compare the bounding box of the predicted gazed object with the one of the corresponding ground truth object. If the gazed object is correctly identified, the prediction is counted as a true positive, otherwise it is predicted as a false negative.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">To run the experiments, the <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_italic">Human Pose Estimation</span> module was run on an Alienware workstation with an external NVIDIA GTX 1080TI GPU while the remaining modules were executed on another Alienware workstation with graphics NVIDIA GeForce RTX 3080 additionally equipped with an external NVIDIA GTX 2080TI GPU.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Fine-tuning of the Visual Target Detection module</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.2" class="ltx_p">We firstly analyze the impact of fine-tuning the <span id="S5.SS3.p1.2.1" class="ltx_text ltx_font_italic">Visual Target Detection</span> module on our dataset. In Tab. <a href="#S5.T1" title="TABLE I ‣ V-C Fine-tuning of the Visual Target Detection module ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we report the performance comparison of the model trained as explained in Sec. <a href="#S5.SS1" title="V-A Model training ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a> (<span id="S5.SS3.p1.2.2" class="ltx_text ltx_font_bold">Fine-tuning</span> in Tab. <a href="#S5.T1" title="TABLE I ‣ V-C Fine-tuning of the Visual Target Detection module ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>) with the model presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> (<span id="S5.SS3.p1.2.3" class="ltx_text ltx_font_bold">Pre-trained model</span> in Tab. <a href="#S5.T1" title="TABLE I ‣ V-C Fine-tuning of the Visual Target Detection module ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>). As it can be seen fine-tuning the model on the proposed ObjectAttention dataset allows to get better performance. Specifically, the predicted hottest point in the heat-map is closer to the true gazed point of <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mo id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\sim</annotation></semantics></math>0.04. Note that this is a relevant difference since the Distance metric is computed on an image with width and height normalized to 1. This result is also supported by the improvement of the <span id="S5.SS3.p1.2.4" class="ltx_text ltx_font_italic">AUC</span> metric of <math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><mrow id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mn id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">5</mn><mo id="S5.SS3.p1.2.m2.1.1.1" xref="S5.SS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">5\%</annotation></semantics></math>. Getting a precise heat-map is crucial in order to get a better selection of the bounding box of the gazed object. This improvement is due to the fact that the model was fine-tuned using a dataset that more closely represents the target scenario (table-top). Nevertheless we show qualitatively that the fine-tuned model is still able to predict the gaze direction, for those areas in the map that are not on the table-top and that are under-represented in our dataset (see video submitted as supplementary material). This is due to the fact that we initialized the network for our fine-tuning with the weights presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, that have been trained on the VideoAttentionTarget dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.3.3" class="ltx_tr">
<th id="S5.T1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.3.3.4.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S5.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.1" class="ltx_text ltx_font_bold">AUC (%) <math id="S5.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T1.1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S5.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<math id="S5.T1.2.2.2.m1.1" class="ltx_Math" alttext="\L_{2}" display="inline"><semantics id="S5.T1.2.2.2.m1.1a"><msub id="S5.T1.2.2.2.m1.1.1" xref="S5.T1.2.2.2.m1.1.1.cmml"><mi id="S5.T1.2.2.2.m1.1.1.2" xref="S5.T1.2.2.2.m1.1.1.2.cmml">Ł</mi><mn id="S5.T1.2.2.2.m1.1.1.3" xref="S5.T1.2.2.2.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m1.1b"><apply id="S5.T1.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T1.2.2.2.m1.1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1">subscript</csymbol><ci id="S5.T1.2.2.2.m1.1.1.2.cmml" xref="S5.T1.2.2.2.m1.1.1.2">italic-Ł</ci><cn type="integer" id="S5.T1.2.2.2.m1.1.1.3.cmml" xref="S5.T1.2.2.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m1.1c">\L_{2}</annotation></semantics></math><span id="S5.T1.3.3.3.1" class="ltx_text ltx_font_bold"> distance <math id="S5.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T1.3.3.3.1.m1.1a"><mo stretchy="false" id="S5.T1.3.3.3.1.m1.1.1" xref="S5.T1.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.1.m1.1b"><ci id="S5.T1.3.3.3.1.m1.1.1.cmml" xref="S5.T1.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.1.m1.1c">\downarrow</annotation></semantics></math></span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.5.5" class="ltx_tr">
<td id="S5.T1.5.5.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><span id="S5.T1.5.5.3.1" class="ltx_text ltx_font_bold">Pre-trained model</span></td>
<td id="S5.T1.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">87.5 <math id="S5.T1.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.4.4.1.m1.1a"><mo id="S5.T1.4.4.1.m1.1.1" xref="S5.T1.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.1.m1.1b"><csymbol cd="latexml" id="S5.T1.4.4.1.m1.1.1.cmml" xref="S5.T1.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.1.m1.1c">\pm</annotation></semantics></math> 0.9</td>
<td id="S5.T1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.131 <math id="S5.T1.5.5.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.5.5.2.m1.1a"><mo id="S5.T1.5.5.2.m1.1.1" xref="S5.T1.5.5.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.2.m1.1b"><csymbol cd="latexml" id="S5.T1.5.5.2.m1.1.1.cmml" xref="S5.T1.5.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.2.m1.1c">\pm</annotation></semantics></math> 0.014</td>
</tr>
<tr id="S5.T1.7.7" class="ltx_tr">
<td id="S5.T1.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.7.7.3.1" class="ltx_text ltx_font_bold">Fine-tuning</span></td>
<td id="S5.T1.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">92.5 <math id="S5.T1.6.6.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.6.6.1.m1.1a"><mo id="S5.T1.6.6.1.m1.1.1" xref="S5.T1.6.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.1.m1.1b"><csymbol cd="latexml" id="S5.T1.6.6.1.m1.1.1.cmml" xref="S5.T1.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.1.m1.1c">\pm</annotation></semantics></math> 1.9</td>
<td id="S5.T1.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.089 <math id="S5.T1.7.7.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.7.7.2.m1.1a"><mo id="S5.T1.7.7.2.m1.1.1" xref="S5.T1.7.7.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.2.m1.1b"><csymbol cd="latexml" id="S5.T1.7.7.2.m1.1.1.cmml" xref="S5.T1.7.7.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.2.m1.1c">\pm</annotation></semantics></math> 0.014</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.11.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S5.T1.12.2" class="ltx_text" style="font-size:90%;">Quantitative evaluation of the <span id="S5.T1.12.2.1" class="ltx_text ltx_font_italic">Visual Target Detection</span> model on the presented <span id="S5.T1.12.2.2" class="ltx_text ltx_font_italic">ObjectAttention</span> dataset.</span></figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.4.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.5.2" class="ltx_text ltx_font_italic">Accuracy evaluation of the pipeline</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">We study the performance of the overall pipeline presented in Sec. <a href="#S3" title="III METHODS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> evaluating it on the proposed ObjectAttention dataset. Specifically, for the <span id="S5.SS4.p1.1.1" class="ltx_text ltx_font_italic">Visual Target Detection</span> module, we chose one of the model trained on the three dataset splits from the experiment reported in Sec. <a href="#S5.SS3" title="V-C Fine-tuning of the Visual Target Detection module ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-C</span></span></a> and we use it in our pipeline. To get quantitative results, we consider the same test set of three participants used to previously evaluate the fine-tuned model (i.e., the three participants that have not been used for the training). However, differently from the previous section, since we need to analyze the performance of the entire pipeline of detecting the target gazed object, we will consider, as ground truth, the bounding boxes and labels of the objects on the table and of the target gazed object.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Firstly, we analyze the overall accuracy of the pipeline in detecting the gazed target object by the three different participants in the test set. Specifically, from our experiments it came out that the pipeline succeeds in detecting the right target object for the <math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="79.5\%" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><mrow id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml"><mn id="S5.SS4.p2.1.m1.1.1.2" xref="S5.SS4.p2.1.m1.1.1.2.cmml">79.5</mn><mo id="S5.SS4.p2.1.m1.1.1.1" xref="S5.SS4.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><apply id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS4.p2.1.m1.1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS4.p2.1.m1.1.1.2.cmml" xref="S5.SS4.p2.1.m1.1.1.2">79.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">79.5\%</annotation></semantics></math> of the times. Note that, this accuracy number represents the performance of the integration of the <span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_italic">Visual Target Detection</span>, the <span id="S5.SS4.p2.1.2" class="ltx_text ltx_font_italic">Object Detection</span> and the <span id="S5.SS4.p2.1.3" class="ltx_text ltx_font_italic">Attentive Object Detection</span>.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">It is also possible to assess the performance of the model qualitatively by examining the video in the supporting materials. Additionally, Fig. <a href="#S5.F5.4" title="Figure 5 ‣ V-D Accuracy evaluation of the pipeline ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates a selection of sample frames from the output, including attention heatmaps and bounding boxes, highlighting the head of the participant (detected by the <span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_italic">Face Detection</span> module) and the hottest areas of the heatmap in the frames of the top row, while the final gaze object bounding box and label are presented in the bottom row frames.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F5.10" class="ltx_p ltx_figure_panel"><span id="S5.F5.10.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S5.F5.10.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:429.6pt;height:337.5pt;vertical-align:-337.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S5.F5.10.1.1.1" class="ltx_p"><span id="S5.F5.10.1.1.1.1" class="ltx_text">



</span></span>
</span></span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.1" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/heatmap_s42_andrea_1.jpg" id="S5.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F5.2" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/heatmap_s51_Giulia_1.jpg" id="S5.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F5.3" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/heatmap_s51_simone_1.jpg" id="S5.F5.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F5.4" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/heatmap_s42_giulia_1.jpg" id="S5.F5.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.4.3.2" class="ltx_text" style="font-size:90%;">Selection of sample output frames of the proposed pipeline. The first row depicts the scene image as well as the head bounding box of the participant detected by the <span id="S5.F5.4.3.2.1" class="ltx_text ltx_font_italic">Face Detection</span> module, the attention heatmap of the participant, and the bounding box of the hottest area of the heatmap. While the second row depicts the related gaze target selections for the frames of the first row.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F5.11" class="ltx_p ltx_figure_panel"><span id="S5.F5.11.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S5.F5.11.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:429.6pt;height:337.5pt;vertical-align:-337.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S5.F5.11.1.1.1" class="ltx_p"><span id="S5.F5.11.1.1.1.1" class="ltx_text">



</span></span>
</span></span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.5" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/output_s42_andrea_1.jpg" id="S5.F5.5.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F5.6" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/output_s51_Giulia_1.jpg" id="S5.F5.6.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F5.7" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/output_s51_simone_1.jpg" id="S5.F5.7.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F5.8" class="ltx_figure ltx_figure_panel"><img src="/html/2308.13318/assets/Images/output_s42_giulia_1.jpg" id="S5.F5.8.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="[Uncaptioned image]">
</figure>
</div>
</div>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS5.4.1.1" class="ltx_text">V-E</span> </span><span id="S5.SS5.5.2" class="ltx_text ltx_font_italic">Performance analysis</span>
</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">To further study the proposed pipeline, in the next paragraphs we present the analysis of the performance of the system for different objects, sessions showing also the effect of the distractor object in the scene.
<br class="ltx_break"></p>
</div>
<div id="S5.SS5.p2" class="ltx_para ltx_noindent">
<p id="S5.SS5.p2.1" class="ltx_p"><span id="S5.SS5.p2.1.1" class="ltx_text ltx_font_bold">Per object performance</span>. In Fig. <a href="#S5.F6" title="Figure 6 ‣ V-E Performance analysis ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we report the level of accuracy achieved when the different objects are considered as target. As it can be observed, the system has a high performance for almost all the objects except for the object class <span id="S5.SS5.p2.1.2" class="ltx_text ltx_font_italic">Bleach</span>. By analyzing the errors, we could find that this is mainly due to a problem of the object detection that sometimes does not locate correctly this object in the image. This might be due to the fact that the training conditions for the detector were really different from the testing ones. Specifically, even if it proved to be effective the most of the time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, training the detection model with handheld objects, with the pipeline proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> might cause some generalization problems due to the so called domain shift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> between the train and the test sets. However, it has been shown in previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> that this can be solved by, for instance, integrating (i) a robot’s autonomous exploration of the new domain and (ii) weakly-supervised learning techniques.
<br class="ltx_break"></p>
</div>
<div id="S5.SS5.p3" class="ltx_para ltx_noindent">
<p id="S5.SS5.p3.1" class="ltx_p"><span id="S5.SS5.p3.1.1" class="ltx_text ltx_font_bold">Per session performance</span>. Fig. <a href="#S5.F7" title="Figure 7 ‣ V-E Performance analysis ‣ V EXPERIMENTS ‣ iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> depicts the accuracy levels of the overall pipeline on various sessions. As it can be observed, performance of the system slightly decreases for higher sessions. This is reasonable since, for higher sessions, the number of objects in the scene increases and thus the table becomes more cluttered. However, the accuracy level is still acceptable even with the most cluttered scenes, showing that this is not a limitation for the proposed system.
<br class="ltx_break"></p>
</div>
<div id="S5.SS5.p4" class="ltx_para ltx_noindent">
<p id="S5.SS5.p4.1" class="ltx_p"><span id="S5.SS5.p4.1.1" class="ltx_text ltx_font_bold">The effect of the distractor</span>. Finally, we report on the impact of the distracting objects on the system performance. Of course, all the objects present on the table are distracting objects when the human is gazing at only one of them. However, to analyze their impact, we select one sample object (i.e., the <span id="S5.SS5.p4.1.2" class="ltx_text ltx_font_italic">pringles</span>) as distractor in our dataset. Then, the <span id="S5.SS5.p4.1.3" class="ltx_text ltx_font_italic">Object Detection</span> module has been trained to detect it, but the participants was not asked to gaze at it. Our aim in this experiment is to verify if the presence of the distractor object causes particular problems in identifying the correct target object. From our experiments, we find that only for the <math id="S5.SS5.p4.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS5.p4.1.m1.1a"><mo id="S5.SS5.p4.1.m1.1.1" xref="S5.SS5.p4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.p4.1.m1.1b"><csymbol cd="latexml" id="S5.SS5.p4.1.m1.1.1.cmml" xref="S5.SS5.p4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p4.1.m1.1c">\sim</annotation></semantics></math>3% of the frames were the distracting object is present, this causes a mistake in the predictions, demonstrating that this is not a limitation of the proposed system.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2308.13318/assets/Images/accuracy_object_no_pringles.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="129" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">Accuracy analysis on each of the objects involved in the experiments.</span></figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2308.13318/assets/Images/accuracy_session_overall.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">Average accuracy on each of the sessions for all the participants.</span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">CONCLUSIONS</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we presented a novel system for detecting the focus of attention of a human in relation to the objects in the scene. Our method combines an online object detection algorithm with a network for gaze estimation conditioned on the pose of the human, estimated using a pose estimation method. The system provides information about the objects that may capture the human’s visual attention. Through the experimental analysis of our pipeline and the evaluation using a novel dataset collected with the iCub humanoid robot, we have demonstrated the effectiveness of our approach in accurately detecting the target of human attention.
Our results indicate that the integration of face detection, human attention prediction, and online object detection in our pipeline enables the robot to perceive and interpret human gaze within its environment, which is a fundamental cue to establish joint attention with human partners. The developed system is promising in enhancing the robot’s social awareness and responsiveness, allowing for more natural and contextually appropriate interactions in social robotics.
The presented pipeline and the accompanying dataset provide a valuable resource for further research in the field of human-robot interaction and gaze detection. Future work will explore the integration of additional cues related to the object of interests, such as language, and other social cues, such as facial expressions and gestures, to further enhance the robot’s understanding of human intentions and emotions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T. Allison, A. Puce, and G. McCarthy, “Social perception from visual cues:
role of the sts region,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Trends in cognitive sciences</em>, vol. 4, no. 7,
pp. 267–278, 2000.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K. A. Pelphrey, J. D. Singerman, T. Allison, and G. McCarthy, “Brain
activation evoked by perception of gaze shifts: the influence of context,”
<em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Neuropsychologia</em>, vol. 41, no. 2, pp. 156–170, 2003.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F. Babel, J. Kraus, L. Miller, M. Kraus, N. Wagner, W. Minker, and M. Baumann,
“Small talk with a robot? the impact of dialog content, talk initiative, and
gaze behavior of a social robot on trust, acceptance, and proximity,”
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Journal of Social Robotics</em>, pp. 1–14, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B. Holman, A. Anwar, A. Singh, M. Tec, J. Hart, and P. Stone, “Watch where
you’re going! gaze and head orientation as predictors for social robot
navigation,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Robotics and
Automation (ICRA)</em>.   IEEE, 2021, pp.
3553–3559.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
U. Kurylo and J. R. Wilson, “Using human eye gaze patterns as indicators of
need for assistance from a socially assistive robot,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Social
Robotics: 11th International Conference, ICSR 2019, Madrid, Spain, November
26–29, 2019, Proceedings 11</em>.   Springer, 2019, pp. 200–210.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
B. Calli, A. Singh, A. Walsman, S. Srinivasa, P. Abbeel, and A. M. Dollar,
“The ycb object and model set: Towards common benchmarks for manipulation
research,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2015 international conference on advanced robotics
(ICAR)</em>.   IEEE, 2015, pp. 510–517.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
F. Ceola, E. Maiettini, G. Pasquale, L. Rosasco, and L. Natale, “Fast object
segmentation learning with kernel-based methods for robotics,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2021
IEEE International Conference on Robotics and Automation (ICRA)</em>, 2021, pp.
13 581–13 588.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
E. Maiettini, G. Pasquale, L. Rosasco, and L. Natale, “On-line object
detection: a robotics challenge,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Autonomous Robots</em>, Nov 2019.
[Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://doi.org/10.1007/s10514-019-09894-9</span>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
P. Chevalier, K. Kompatsiari, F. Ciardo, and A. Wykowska, “Examining joint
attention with the use of humanoid robots-a new approach to study fundamental
mechanisms of social cognition,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Psychonomic Bulletin &amp; Review</em>,
vol. 27, pp. 217–236, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
G. Metta, L. Natale, F. Nori, G. Sandini, D. Vernon, L. Fadiga, C. von Hofsten,
K. Rosander, M. Lopes, J. Santos-Victor, A. Bernardino, and L. Montesano,
“The icub humanoid robot: an open-systems platform for research in cognitive
development.” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Neural networks : the official journal of the
International Neural Network Society</em>, vol. 23, no. 8-9, pp. 1125–34, 1
2010.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. Lombardi, E. Maiettini, D. De Tommaso, A. Wykowska, and L. Natale, “Toward
an attentive robotic architecture: Learning-based mutual gaze estimation in
human–robot interaction,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Frontiers in Robotics and AI</em>, vol. 9, p.
770165, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K. Krafka, A. Khosla, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik, and
A. Torralba, “Eye tracking for everyone,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2016, pp. 2176–2184.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
R. Athavale, L. S. Motati, and R. Kalahasty, “One eye is all you need:
Lightweight ensembles for gaze estimation with single encoders,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2211.11936</em>, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Y. Cheng, X. Zhang, F. Lu, and Y. Sato, “Gaze estimation by exploring two-eye
asymmetry,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image Processing</em>, vol. 29, pp.
5259–5272, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Lombardi, E. Maiettini, V. Tikhanoff, and L. Natale, “icub knows where you
look: Exploiting social cues for interactive object detection learning,” in
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2022 IEEE-RAS 21st International Conference on Humanoid Robots
(Humanoids)</em>, 2022, pp. 480–487.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
O. Sumer, P. Gerjets, U. Trautwein, and E. Kasneci, “Attention flow:
End-to-end joint attention estimation,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision</em>, 2020, pp. 3327–3336.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
L. Fan, Y. Chen, P. Wei, W. Wang, and S.-C. Zhu, “Inferring shared attention
in social scene videos,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on
computer vision and pattern recognition</em>, 2018, pp. 6460–6468.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Recasens, C. Vondrick, A. Khosla, and A. Torralba, “Following gaze in
video,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on
Computer Vision</em>, 2017, pp. 1435–1443.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
E. Chong, N. Ruiz, Y. Wang, Y. Zhang, A. Rozga, and J. M. Rehg, “Connecting
gaze, scene, and attention: Generalized attention estimation via joint
modeling of gaze and scene saliency,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European
conference on computer vision (ECCV)</em>, 2018, pp. 383–398.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Saran, S. Majumdar, E. S. Short, A. Thomaz, and S. Niekum, “Human gaze
following for human-robot interaction,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS)</em>.   IEEE, 2018, pp. 8615–8621.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
E. Chong, Y. Wang, N. Ruiz, and J. M. Rehg, “Detecting attended visual targets
in video,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition</em>, 2020, pp. 5396–5406.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
E. Maiettini, G. Pasquale, V. Tikhanoff, L. Rosasco, and L. Natale, “A weakly
supervised strategy for learning object detection on a humanoid robot,” in
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2019 IEEE-RAS 19th International Conference on Humanoid Robots
(Humanoids)</em>, 2019, pp. 194–201.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh, “Openpose: realtime
multi-person 2d pose estimation using part affinity fields,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE
transactions on pattern analysis and machine intelligence</em>, vol. 43, no. 1,
pp. 172–186, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
E. Maiettini, G. Pasquale, L. Rosasco, and L. Natale, “Interactive data
collection for deep learning object detectors on humanoid robots,” in
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">2017 IEEE-RAS 17th International Conference on Humanoid Robotics
(Humanoids)</em>, Nov 2017, pp. 862–868.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
G. Metta, P. Fitzpatrick, and L. Natale, “Yarp: Yet another robot platform,”
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Journal of Advanced Robotic Systems</em>, vol. 3, no. 1,
p. 8, 2006. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://doi.org/10.5772/5761</span>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2d pose
estimation using part affinity fields,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2017, pp. 7291–7299.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
E. Maiettini, “From constraints to opportunities: Efficient object detection
learning for humanoid robots,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Ph.D. thesis, University of Genoa</em>,
2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
E. Maiettini, V. Tikhanoff, and L. Natale, “Weakly-supervised object detection
learning through human-robot interaction,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">2020 IEEE-RAS 20th
International Conference on Humanoid Robots (Humanoids)</em>, 2021, pp. 392–399.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.13317" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.13318" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.13318">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.13318" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.13319" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 11:08:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
