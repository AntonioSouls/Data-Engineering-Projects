<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.12779] On Offline Evaluation of 3D Object Detection for Autonomous Driving</title><meta property="og:description" content="Prior work in 3D object detection evaluates models using offline metrics like average precision since closed-loop online evaluation on the downstream driving task is costly. However, it is unclear how indicative offlin…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="On Offline Evaluation of 3D Object Detection for Autonomous Driving">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="On Offline Evaluation of 3D Object Detection for Autonomous Driving">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.12779">

<!--Generated on Wed Feb 28 10:15:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">On Offline Evaluation of 3D Object Detection for Autonomous Driving</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Tim Schreier  Katrin Renz  Andreas Geiger  Kashyap Chitta 
<br class="ltx_break">University of Tübingen  Tübingen AI Center
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">tschreier2@gmail.com  {katrin.renz, a.geiger, kashyap.chitta}@uni-tuebingen.de</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Prior work in 3D object detection evaluates models using offline metrics like average precision since closed-loop online evaluation on the downstream driving task is costly. However, it is unclear how indicative offline results are of driving performance.
In this work, we perform the first empirical evaluation measuring how predictive different detection metrics are of driving performance when detectors are integrated into a full self-driving stack. We conduct extensive experiments on urban driving in the CARLA simulator using 16 object detection models. We find that the nuScenes Detection Score has a higher correlation to driving performance than the widely used average precision metric. In addition, our results call for caution on the exclusive reliance on the emerging class of ‘planner-centric’ metrics.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Ever since the first object detection benchmark challenges like the PASCAL VOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> became popular, mean average precision (mAP) has been used as the standard metric for evaluating the performance of detection models.
Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> have criticized mAP for its task-agnostic design as the metric assigns equal importance to all objects, which does not reflect real-world priorities for self-driving.
Therefore, different task-specific modifications of mAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and planner-centric approaches to detection evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> have been proposed.
These offline metrics are useful because they are quick, cheap, and safe to evaluate compared to online tests. However, relying solely on offline evaluation is only useful if it strongly correlates with the actual driving performance.
With the influx of new detection metrics, it has become unclear which metric researchers should rely on and how the metrics compare.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2308.12779/assets/images/teaser5.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="329" height="108" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text ltx_font_bold">Summary of findings.</span>
nuScenes Detection Score is most predictive of driving performance in an extensive empirical study on the CARLA simulator.
</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this work<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>To read the full-length report, please visit: <a target="_blank" href="https://t.ly/CsIrt" title="" class="ltx_ref ltx_href">https://t.ly/CsIrt</a>.</span></span></span>, we provide the first empirical evidence of how predictive detection metrics are of downstream driving performance. We train <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S1.p2.1.m1.1a"><mn id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><cn type="integer" id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">16</annotation></semantics></math> modern 3D detectors, integrate them into a self-driving pipeline, and evaluate their performance in the CARLA simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. This allows us to study how strongly these metrics correlate with driving outcomes.
We find that even though mAP is highly correlated with driving performance, the nuScenes Detection Score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, a task-specific variation, is even more predictive. Furthermore, the planner-centric metrics we examine, which measure the impact of inaccurate detections on planner outcomes, are significantly less indicative of driving performance. Our key findings are summarized in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ On Offline Evaluation of 3D Object Detection for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Task-specific Detection Metrics.</span>
To track algorithmic advancements, researchers compare object detection models on dataset-based competitions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> using the mAP metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, which does not take the egocentric nature and task-specific characteristics of driving into account. Therefore, prior work has proposed task-specific object detection metrics for self-driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. One approach is taking mAP and adapting it to self-driving.
mAPH, the principal metric of the Waymo challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and the Average Orientation Similarity (AOS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> are both designed to account for the importance of correct heading estimation for behaviour planning and weigh detections accordingly.
Similarly, Deng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> suggest evaluating detections from an egocentric perspective and introduce the Support Distance Error (SDE).</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Another approach focuses on evaluating the effects of perception errors on the planning module. The planning-KL-divergence (PKL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> measures the KL-divergences between distributions of waypoint locations conditioned either on noisy perception or ground truth object annotations.
Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> emphasize the importance of understanding the internal reasoning of a planner, as some detection failures do not cause immediate behaviour change. However, their approach focuses on model-based planners and does not apply to neural architectures.
Ivanovic and Pavone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> describe an approach where local gradients of a hand-crafted planning function are used to assign object weights.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">None of these prior works have provided quantitative results demonstrating that the metric they propose is more closely related to measures of driving performance than the standard mAP metric. We seek to address this gap in the literature by comparing offline evaluations with online tests.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Online and Offline Evaluation.</span>
Deep neural networks for self-driving applications are commonly first tested offline with the use of a pre-recorded dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
Offline results are used as proxy performance indicators for online evaluations, which are more expensive to conduct.
It is often unclear how offline measurements relate to system-level functionality in embedded systems. Haq et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> evaluate the correlations between online and offline performance of a camera-based end-to-end lane-keeping model.
The authors conclude that offline evaluations cannot be used for safety testing in the context of steering prediction. However, a recent replication of this study with improved methodology obtains a tighter relationship between online and offline results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
In similar experiments, prior work has found little to no correlation when comparing online driving performance to offline prediction accuracy for agents in the CARLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and nuPlan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> simulators.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Contrary to these efforts, we do not focus on steering prediction but aim to evaluate the effects of detection errors on driving outcomes.
We aim to provide the first quantitative evidence comparing detection metrics for self-driving to help the community choose relevant metrics.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Modular Pipeline for Autonomous Driving</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present our driving agent. We introduce the problem setting and describe the modular structure of our agent. Next, we discuss the modules for object detection, tracking, and motion planning.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Task and Setup.</span>
We address the task of urban driving with the objective of navigating safely along a predetermined route while adhering to traffic regulations. The agent predicts steering and throttle from sensor inputs. While all traffic participants need to be recognized via a LiDAR-based object detector, our agent has privileged access to simulator information concerning the ego lane and traffic light states.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Pipeline.</span>
At every timestep,
the point cloud of a <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="360^{\circ}" display="inline"><semantics id="S3.p3.1.m1.1a"><msup id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mn id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">360</mn><mo id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">360</cn><compose id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">360^{\circ}</annotation></semantics></math> LiDAR sensor is processed by a 3D object detector which produces a set of oriented bounding box predictions of traffic participants in the scene.
Detections are tracked over time
to yield consistent predictions and speed estimates for detected objects. Given its high-level navigation goal, the planner then uses these predictions to decide on an appropriate set of target waypoints that encode the planned trajectory. A PID controller then processes the waypoints to compute appropriate lateral and longitudinal controls.
To study the effects of the performance of specific object detectors on the downstream task of driving, all pipeline elements except the detector are constant across experiments.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Detection and Tracking.</span>
For our experiments, we consider eight different LiDAR-based 3D object detection architectures. To cover a breadth of architectures in our analysis, we include voxel-based detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, a pillar-based detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and approaches that also incorporate point-based information from the point cloud <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. This array of detection architectures further includes a mix of anchor-based and anchor-free detection heads as well as single-stage and two-stage approaches. In our setup, detections are associated across frames using Hungarian matching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The speed of tracked objects is approximated via a simple heuristic: per object track, the bounding box centers of the last two timesteps are projected to the ground plane. The L2 norm between these points is divided by the timestep length to approximate speed.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">Planning.</span>
For our experiments, we choose PlanT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> for motion planning. PlanT is a transformer-based planner with state-of-the-art performance in the CARLA simulator.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Metrics</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section introduces the metrics we use in our experiments: the online metrics that quantify driving performance, the mAP metric, its task-specific variations, and finally two planner-centric metrics.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Online Metrics.</span>

Online evaluations provide the most reliable estimates of system-level performance. In this work, we use the CARLA Driving Score, the official metric for the CARLA leaderboard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and the number of collisions.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_italic">Driving Score.</span>
The CARLA Driving Score (DS) is a composite metric combining route completion with the infraction score.
The route completion (RC) describes the percentage of the route the agent completed and the infraction score (IS) measures collisions or violations of traffic rules.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_italic">Collision Count.</span>
As a second metric, we count the total number of collisions per evaluation (#Col.) in which the ego vehicle is involved. This metric exclusively focuses on safety since it does not depend on the route completion.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Average Precision based Metrics.</span>

<span id="S4.p5.1.2" class="ltx_text ltx_font_italic">Average precision</span> is the standard metric to measure performance on detection tasks.
It is defined as the area under the precision-recall curve.
Following the KITTI protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, we use an intersection over union (IoU) threshold of <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="70" display="inline"><semantics id="S4.p5.1.m1.1a"><mn id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">70</mn><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><cn type="integer" id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">70</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">70</annotation></semantics></math>% as the true positive criterion and compute the integral using a 40-point interpolation with equidistant recall values.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_italic">Average Orientation Similarity.</span>
 As the original mAP metric does not account for a notion of heading, we have included the average orientation similarity metric (AOS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> in our analysis. AOS modifies AP by weighting true positive detections according to the accuracy of their heading predictions. The heading angles of vehicles in the scene provide essential information for motion forecasting and behavior planning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S4.p7" class="ltx_para ltx_noindent">
<p id="S4.p7.1" class="ltx_p"><span id="S4.p7.1.1" class="ltx_text ltx_font_italic">Inverse Distance weighted AP.</span>

Another approach for modifying AP is weighting detections by their inverse distance to the ego vehicle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Closer objects are inherently more safety-critical than those far away. Thus, one should expect the weighting of AP by the inverse distance (ID-AP) to produce a metric that better reflects the ego-centric nature of detection for self-driving.</p>
</div>
<div id="S4.p8" class="ltx_para ltx_noindent">
<p id="S4.p8.1" class="ltx_p"><span id="S4.p8.1.1" class="ltx_text ltx_font_bold">nuScenes Detection Score.</span>
The <span id="S4.p8.1.2" class="ltx_text ltx_font_italic">nuScenes Detection Score</span> (NDS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is a popular task-specific detection metric that uses mAP as a starting point. It relies on center distance in birds-eye view (BEV) for the true positive criterion instead of the standard IoU-based approach.
Here, we use a fixed center distance of 1 meter to not confound our results by averaging across thresholds (the authors suggest averaging over four thresholds).
By relying on center distances, the authors decouple mAP from object size and orientation, which they account for separately. They argue that center distance covers objects of different sizes more evenly because smaller volume objects like pedestrians can quickly achieve an IoU of zero if predictions have minor translation errors.
NDS also includes five explicit detection quality measures for all true positive detections. These measures are weighted equally and are, in total, given as much weight as average precision in the NDS. The five true positive metrics are (1) Average Translation Error (ATE): Euclidean center distance in BEV in meters; (2) Average Scale Error (ASE): Calculated as 1 - IOU after aligning centers and orientation; (3) Average Orientation Error (AOE): The smaller yaw angle between GT and prediction in radians; (4) Average Velocity Error (AVE): Absolute velocity error in m/s; and (5) Average Attribute Error (AAE): (1-Acc) for the prediction accuracy of additional attributes. We do not include the AAE in our experiments, as the additional attributes are specific to the nuScenes dataset.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">Similar to mAP, NDS does not account for an object’s distance to the ego vehicle. We thus also test an inverse distance-weighted version of the NDS (ID-NDS).</p>
</div>
<div id="S4.p10" class="ltx_para ltx_noindent">
<p id="S4.p10.1" class="ltx_p"><span id="S4.p10.1.1" class="ltx_text ltx_font_bold">Planner-Centric Detection Metrics.</span> 
Several authors have recently pursued a novel approach to detection metrics for self-driving: focusing on the planner instead of evaluating object detections directly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. To evaluate the planner-centric metrics, we first compute the planner’s output for a scene given the ground truth object information. We then also calculate the planner’s output given the noisy output of the perception stack (detection + tracking) and analyse how the two predicted trajectories (i.e., the sets of waypoints) differ. This approach naturally down-weights distant and less relevant objects in the metrics as they bear little significance to planning.</p>
</div>
<div id="S4.p11" class="ltx_para">
<p id="S4.p11.1" class="ltx_p">We use the average and final displacement errors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> to quantify differences in trajectories.
The <span id="S4.p11.1.1" class="ltx_text ltx_font_italic">average displacement error</span> (ADE) at timestep <math id="S4.p11.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.p11.1.m1.1a"><mi id="S4.p11.1.m1.1.1" xref="S4.p11.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.p11.1.m1.1b"><ci id="S4.p11.1.m1.1.1.cmml" xref="S4.p11.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p11.1.m1.1c">t</annotation></semantics></math> is the average of the point-wise L2 distances between the predicted waypoints based on ground truth detections and the predicted waypoints based on noisy detections. We define the ADE for a route as the mean of all frame-based ADEs in that route. Similarly, we define the <span id="S4.p11.1.2" class="ltx_text ltx_font_italic">final displacement error</span> (FDE) as the L2 distance between the final waypoints of two trajectories, averaged over all frames of a route.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section discusses our setup for online evaluation and explains how we train and configure the object detection models. We then present the results of our analysis.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.2" class="ltx_p"><span id="S5.p2.2.1" class="ltx_text ltx_font_bold">Online Evaluation.</span>
We base our experiments on the CARLA simulator (Version: 0.9.10) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
Within CARLA, we gauge online performance for the detection models by integrating them into our modular pipeline to test driving performance.
We use the Longest6 benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> as an online evaluation protocol. Longest6 contains <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="36" display="inline"><semantics id="S5.p2.1.m1.1a"><mn id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">36</mn><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><cn type="integer" id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">36</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">36</annotation></semantics></math> <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.p2.2.m2.1a"><mo id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><csymbol cd="latexml" id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">\sim</annotation></semantics></math>1.5km long routes with high traffic density across six towns.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">While evaluating the agents on Longest6, we log detailed ground truth bounding box information about the objects in the scene, the perception stack’s predictions, and the sensor information.
Based on the resulting logs, we compute the offline detection metrics for every route. This enables us to compare the observed driving outcomes for a given route with the associated offline detection performance measures.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.5" class="ltx_p"><span id="S5.p4.5.1" class="ltx_text ltx_font_bold">Implementation.</span>
We train eight detection architectures using the open-source LiDAR detection framework OpenPCDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and the default hyperparameter configurations it provides.
All architectures are trained for <math id="S5.p4.1.m1.1" class="ltx_Math" alttext="72" display="inline"><semantics id="S5.p4.1.m1.1a"><mn id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">72</mn><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><cn type="integer" id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">72</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">72</annotation></semantics></math> hours on eight NVIDIA GeForce RTX <math id="S5.p4.2.m2.1" class="ltx_Math" alttext="2080" display="inline"><semantics id="S5.p4.2.m2.1a"><mn id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml">2080</mn><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><cn type="integer" id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1">2080</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">2080</annotation></semantics></math>TIs.
We include two checkpoints per architecture to increase the number of data points in our analysis. To ensure variance in checkpoints’ behavior, the first is extracted after only <math id="S5.p4.3.m3.1" class="ltx_Math" alttext="36" display="inline"><semantics id="S5.p4.3.m3.1a"><mn id="S5.p4.3.m3.1.1" xref="S5.p4.3.m3.1.1.cmml">36</mn><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.1b"><cn type="integer" id="S5.p4.3.m3.1.1.cmml" xref="S5.p4.3.m3.1.1">36</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.1c">36</annotation></semantics></math> hours of training and the second after <math id="S5.p4.4.m4.1" class="ltx_Math" alttext="72" display="inline"><semantics id="S5.p4.4.m4.1a"><mn id="S5.p4.4.m4.1.1" xref="S5.p4.4.m4.1.1.cmml">72</mn><annotation-xml encoding="MathML-Content" id="S5.p4.4.m4.1b"><cn type="integer" id="S5.p4.4.m4.1.1.cmml" xref="S5.p4.4.m4.1.1">72</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.4.m4.1c">72</annotation></semantics></math> hours. In total, this results in <math id="S5.p4.5.m5.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S5.p4.5.m5.1a"><mn id="S5.p4.5.m5.1.1" xref="S5.p4.5.m5.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S5.p4.5.m5.1b"><cn type="integer" id="S5.p4.5.m5.1.1.cmml" xref="S5.p4.5.m5.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.5.m5.1c">16</annotation></semantics></math> models.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.4" class="ltx_p">We generate the training data via the CARLA simulator by observing the driving behaviour of an expert algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> with privileged access to ground truth information. At every frame, we record <math id="S5.p5.1.m1.1" class="ltx_Math" alttext="360^{\circ}" display="inline"><semantics id="S5.p5.1.m1.1a"><msup id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml"><mn id="S5.p5.1.m1.1.1.2" xref="S5.p5.1.m1.1.1.2.cmml">360</mn><mo id="S5.p5.1.m1.1.1.3" xref="S5.p5.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b"><apply id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p5.1.m1.1.1.1.cmml" xref="S5.p5.1.m1.1.1">superscript</csymbol><cn type="integer" id="S5.p5.1.m1.1.1.2.cmml" xref="S5.p5.1.m1.1.1.2">360</cn><compose id="S5.p5.1.m1.1.1.3.cmml" xref="S5.p5.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">360^{\circ}</annotation></semantics></math> LiDAR data with a rotation frequency of <math id="S5.p5.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S5.p5.2.m2.1a"><mn id="S5.p5.2.m2.1.1" xref="S5.p5.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.p5.2.m2.1b"><cn type="integer" id="S5.p5.2.m2.1.1.cmml" xref="S5.p5.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.2.m2.1c">20</annotation></semantics></math> Hz.
We set the minimum confidence threshold for detections to <math id="S5.p5.3.m3.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="S5.p5.3.m3.1a"><mn id="S5.p5.3.m3.1.1" xref="S5.p5.3.m3.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S5.p5.3.m3.1b"><cn type="float" id="S5.p5.3.m3.1.1.cmml" xref="S5.p5.3.m3.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.3.m3.1c">0.3</annotation></semantics></math>, apply non-maximum suppression with an IoU threshold of <math id="S5.p5.4.m4.1" class="ltx_Math" alttext="0.2" display="inline"><semantics id="S5.p5.4.m4.1a"><mn id="S5.p5.4.m4.1.1" xref="S5.p5.4.m4.1.1.cmml">0.2</mn><annotation-xml encoding="MathML-Content" id="S5.p5.4.m4.1b"><cn type="float" id="S5.p5.4.m4.1.1.cmml" xref="S5.p5.4.m4.1.1">0.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.4.m4.1c">0.2</annotation></semantics></math> and only present object tracks to the planner that are tracked for four consecutive frames.</p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p"><span id="S5.p6.1.1" class="ltx_text ltx_font_bold">Metric Evaluation Protocol.</span> After we evaluate the driving performance for all 16 models on the Longest6 benchmark, we average scores across the benchmark’s 36 routes to attain one data point per detector for every metric. For the 16 detector-wise data points, we then calculate Pearson’s correlation coefficients between offline and online metrics. For ease of comparison and interpretation, we provide all Pearson’s <math id="S5.p6.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S5.p6.1.m1.1a"><mi id="S5.p6.1.m1.1.1" xref="S5.p6.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S5.p6.1.m1.1b"><ci id="S5.p6.1.m1.1.1.cmml" xref="S5.p6.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.1.m1.1c">r</annotation></semantics></math> correlation coefficients as absolute values.</p>
</div>
<div id="S5.p7" class="ltx_para ltx_noindent">
<p id="S5.p7.1" class="ltx_p"><span id="S5.p7.1.1" class="ltx_text ltx_font_bold">Correlation Results.</span></p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.10" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.10.11.1" class="ltx_tr">
<th id="S5.T1.10.11.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.10.11.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Metric</span></th>
<th id="S5.T1.10.11.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.10.11.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Correlation: DS</span></th>
<th id="S5.T1.10.11.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.10.11.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Correlation: #Col.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.2.2" class="ltx_tr">
<th id="S5.T1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S5.T1.2.2.3.1" class="ltx_text" style="font-size:80%;">nuScenes Detection Score </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T1.2.2.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S5.T1.2.2.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T1.1.1.1.m1.1" class="ltx_Math" alttext="0.852" display="inline"><semantics id="S5.T1.1.1.1.m1.1a"><mn mathcolor="#009900" mathsize="80%" id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml">0.852</mn><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><cn type="float" id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">0.852</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">0.852</annotation></semantics></math></td>
<td id="S5.T1.2.2.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T1.2.2.2.m1.1" class="ltx_Math" alttext="0.907" display="inline"><semantics id="S5.T1.2.2.2.m1.1a"><mn mathcolor="#009900" mathsize="80%" id="S5.T1.2.2.2.m1.1.1" xref="S5.T1.2.2.2.m1.1.1.cmml">0.907</mn><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m1.1b"><cn type="float" id="S5.T1.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1">0.907</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m1.1c">0.907</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.4.4" class="ltx_tr">
<th id="S5.T1.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S5.T1.4.4.3.1" class="ltx_text" style="font-size:80%;">Average Precision </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T1.4.4.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S5.T1.4.4.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T1.3.3.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T1.3.3.1.m1.1" class="ltx_Math" alttext="0.805" display="inline"><semantics id="S5.T1.3.3.1.m1.1a"><mn mathsize="80%" id="S5.T1.3.3.1.m1.1.1" xref="S5.T1.3.3.1.m1.1.1.cmml">0.805</mn><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.1.m1.1b"><cn type="float" id="S5.T1.3.3.1.m1.1.1.cmml" xref="S5.T1.3.3.1.m1.1.1">0.805</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.1.m1.1c">0.805</annotation></semantics></math></td>
<td id="S5.T1.4.4.2" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T1.4.4.2.m1.1" class="ltx_Math" alttext="0.903" display="inline"><semantics id="S5.T1.4.4.2.m1.1a"><mn mathsize="80%" id="S5.T1.4.4.2.m1.1.1" xref="S5.T1.4.4.2.m1.1.1.cmml">0.903</mn><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.2.m1.1b"><cn type="float" id="S5.T1.4.4.2.m1.1.1.cmml" xref="S5.T1.4.4.2.m1.1.1">0.903</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.2.m1.1c">0.903</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.6.6" class="ltx_tr">
<th id="S5.T1.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S5.T1.6.6.3.1" class="ltx_text" style="font-size:80%;">Avg. Displacement Error </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T1.6.6.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S5.T1.6.6.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T1.5.5.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T1.5.5.1.m1.1" class="ltx_Math" alttext="0.784" display="inline"><semantics id="S5.T1.5.5.1.m1.1a"><mn mathsize="80%" id="S5.T1.5.5.1.m1.1.1" xref="S5.T1.5.5.1.m1.1.1.cmml">0.784</mn><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.1.m1.1b"><cn type="float" id="S5.T1.5.5.1.m1.1.1.cmml" xref="S5.T1.5.5.1.m1.1.1">0.784</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.1.m1.1c">0.784</annotation></semantics></math></td>
<td id="S5.T1.6.6.2" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T1.6.6.2.m1.1" class="ltx_Math" alttext="0.770" display="inline"><semantics id="S5.T1.6.6.2.m1.1a"><mn mathsize="80%" id="S5.T1.6.6.2.m1.1.1" xref="S5.T1.6.6.2.m1.1.1.cmml">0.770</mn><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.2.m1.1b"><cn type="float" id="S5.T1.6.6.2.m1.1.1.cmml" xref="S5.T1.6.6.2.m1.1.1">0.770</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.2.m1.1c">0.770</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.8.8" class="ltx_tr">
<th id="S5.T1.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S5.T1.8.8.3.1" class="ltx_text" style="font-size:80%;">Avg. Orientation Similarity </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T1.8.8.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S5.T1.8.8.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T1.7.7.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T1.7.7.1.m1.1" class="ltx_Math" alttext="0.742" display="inline"><semantics id="S5.T1.7.7.1.m1.1a"><mn mathsize="80%" id="S5.T1.7.7.1.m1.1.1" xref="S5.T1.7.7.1.m1.1.1.cmml">0.742</mn><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.1.m1.1b"><cn type="float" id="S5.T1.7.7.1.m1.1.1.cmml" xref="S5.T1.7.7.1.m1.1.1">0.742</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.1.m1.1c">0.742</annotation></semantics></math></td>
<td id="S5.T1.8.8.2" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T1.8.8.2.m1.1" class="ltx_Math" alttext="0.894" display="inline"><semantics id="S5.T1.8.8.2.m1.1a"><mn mathsize="80%" id="S5.T1.8.8.2.m1.1.1" xref="S5.T1.8.8.2.m1.1.1.cmml">0.894</mn><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.2.m1.1b"><cn type="float" id="S5.T1.8.8.2.m1.1.1.cmml" xref="S5.T1.8.8.2.m1.1.1">0.894</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.2.m1.1c">0.894</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.10.10" class="ltx_tr">
<th id="S5.T1.10.10.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S5.T1.10.10.3.1" class="ltx_text" style="font-size:80%;">Final Displacement Error </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T1.10.10.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S5.T1.10.10.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S5.T1.9.9.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T1.9.9.1.m1.1" class="ltx_Math" alttext="0.703" display="inline"><semantics id="S5.T1.9.9.1.m1.1a"><mn mathcolor="#BD0000" mathsize="80%" id="S5.T1.9.9.1.m1.1.1" xref="S5.T1.9.9.1.m1.1.1.cmml">0.703</mn><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.1.m1.1b"><cn type="float" id="S5.T1.9.9.1.m1.1.1.cmml" xref="S5.T1.9.9.1.m1.1.1">0.703</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.1.m1.1c">0.703</annotation></semantics></math></td>
<td id="S5.T1.10.10.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T1.10.10.2.m1.1" class="ltx_Math" alttext="0.653" display="inline"><semantics id="S5.T1.10.10.2.m1.1a"><mn mathcolor="#BD0000" mathsize="80%" id="S5.T1.10.10.2.m1.1.1" xref="S5.T1.10.10.2.m1.1.1.cmml">0.653</mn><annotation-xml encoding="MathML-Content" id="S5.T1.10.10.2.m1.1b"><cn type="float" id="S5.T1.10.10.2.m1.1.1.cmml" xref="S5.T1.10.10.2.m1.1.1">0.653</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.10.2.m1.1c">0.653</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S5.T1.14.1" class="ltx_text ltx_font_bold">Pearson correlation between online and offline metrics</span>. Absolute values are shown for clarity.</figcaption>
</figure>
<div id="S5.p8" class="ltx_para">
<p id="S5.p8.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5 Experiments ‣ On Offline Evaluation of 3D Object Detection for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the correlation coefficients for the base metrics. Note that we only discuss Pearson correlation here for the sake of conciseness but found very similar relations when computing Spearman coefficients.
The first clear result from the analysis is that all offline metrics correlate highly to driving outcomes. NDS achieves the highest correlation values, with the standard mAP value in second place. The planner-centric metrics achieve less impressive results than their AP-based alternatives.
The strength of the correlation indicates that offline metrics can provide reasonable heuristics for a detector’s performance in online tests. Offline metrics thus seem to be a reliable proxy for rougher comparisons among models and quick hypotheses testing. This insight is significant as the computational cost for online tests is high. The offline metrics only take around four minutes to evaluate, while testing a single model on Longest6 using an NVIDIA GeForce RTX <math id="S5.p8.1.m1.1" class="ltx_Math" alttext="2080" display="inline"><semantics id="S5.p8.1.m1.1a"><mn id="S5.p8.1.m1.1.1" xref="S5.p8.1.m1.1.1.cmml">2080</mn><annotation-xml encoding="MathML-Content" id="S5.p8.1.m1.1b"><cn type="integer" id="S5.p8.1.m1.1.1.cmml" xref="S5.p8.1.m1.1.1">2080</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p8.1.m1.1c">2080</annotation></semantics></math>Ti takes three days.</p>
</div>
<div id="S5.p9" class="ltx_para ltx_noindent">
<p id="S5.p9.3" class="ltx_p"><span id="S5.p9.3.1" class="ltx_text ltx_font_bold">nuScenes Detection Score Ablation.</span>
Table <a href="#S5.T1" title="Table 1 ‣ 5 Experiments ‣ On Offline Evaluation of 3D Object Detection for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows that NDS correlates more to the CARLA Driving Score than the standard mAP metric (<math id="S5.p9.1.m1.1" class="ltx_Math" alttext="0.85" display="inline"><semantics id="S5.p9.1.m1.1a"><mn id="S5.p9.1.m1.1.1" xref="S5.p9.1.m1.1.1.cmml">0.85</mn><annotation-xml encoding="MathML-Content" id="S5.p9.1.m1.1b"><cn type="float" id="S5.p9.1.m1.1.1.cmml" xref="S5.p9.1.m1.1.1">0.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p9.1.m1.1c">0.85</annotation></semantics></math> vs. <math id="S5.p9.2.m2.1" class="ltx_Math" alttext="0.80" display="inline"><semantics id="S5.p9.2.m2.1a"><mn id="S5.p9.2.m2.1.1" xref="S5.p9.2.m2.1.1.cmml">0.80</mn><annotation-xml encoding="MathML-Content" id="S5.p9.2.m2.1b"><cn type="float" id="S5.p9.2.m2.1.1.cmml" xref="S5.p9.2.m2.1.1">0.80</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p9.2.m2.1c">0.80</annotation></semantics></math>). Both metrics have similar correlations to the number of collisions (<math id="S5.p9.3.m3.1" class="ltx_Math" alttext="0.90" display="inline"><semantics id="S5.p9.3.m3.1a"><mn id="S5.p9.3.m3.1.1" xref="S5.p9.3.m3.1.1.cmml">0.90</mn><annotation-xml encoding="MathML-Content" id="S5.p9.3.m3.1b"><cn type="float" id="S5.p9.3.m3.1.1.cmml" xref="S5.p9.3.m3.1.1">0.90</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p9.3.m3.1c">0.90</annotation></semantics></math>).</p>
</div>
<div id="S5.p10" class="ltx_para">
<p id="S5.p10.2" class="ltx_p">In the original NDS metric, all detection quality measures are given equal importance, and their sum is given as much weight as mAP.
We find that even though all detection quality metrics contribute a little bit toward the strong correlation, switching all of them off also produces a metric that correlates more with the Driving Score than mAP (<math id="S5.p10.1.m1.1" class="ltx_Math" alttext="0.82" display="inline"><semantics id="S5.p10.1.m1.1a"><mn id="S5.p10.1.m1.1.1" xref="S5.p10.1.m1.1.1.cmml">0.82</mn><annotation-xml encoding="MathML-Content" id="S5.p10.1.m1.1b"><cn type="float" id="S5.p10.1.m1.1.1.cmml" xref="S5.p10.1.m1.1.1">0.82</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p10.1.m1.1c">0.82</annotation></semantics></math> vs. <math id="S5.p10.2.m2.1" class="ltx_Math" alttext="0.80" display="inline"><semantics id="S5.p10.2.m2.1a"><mn id="S5.p10.2.m2.1.1" xref="S5.p10.2.m2.1.1.cmml">0.80</mn><annotation-xml encoding="MathML-Content" id="S5.p10.2.m2.1b"><cn type="float" id="S5.p10.2.m2.1.1.cmml" xref="S5.p10.2.m2.1.1">0.80</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p10.2.m2.1c">0.80</annotation></semantics></math>). This suggests that the center distance approach is indeed preferable to an IoU based TP-criterion.</p>
</div>
<div id="S5.p11" class="ltx_para ltx_noindent">
<p id="S5.p11.1" class="ltx_p"><span id="S5.p11.1.1" class="ltx_text ltx_font_bold">Inverse Distance Weighting.</span></p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2308.12779/assets/images/double_final.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="329" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S5.F2.2.1" class="ltx_text ltx_font_bold">Correlation analysis summary.</span> The plot marks correlation coefficients for planner-centric metrics with a star symbol. The inverse distance weighted metrics are marked with triangles.</figcaption>
</figure>
<div id="S5.p12" class="ltx_para">
<p id="S5.p12.2" class="ltx_p">We include inverse distance weighted variations for mAP and NDS. We observe that the correlation to Driving Score drops for both metrics (comparing triangles with circles in Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Experiments ‣ On Offline Evaluation of 3D Object Detection for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
In contrast, the correlation to the number of collisions increases compared to the original metrics. The fact that inverse distance weighting increases predictive power for collisions is expected. Many collisions likely occur when the perception stack misses traffic participants directly in front of the ego.
However, inverse distance weighting rewards defensive agents, that achieve less route completion.
ID-MAP is especially tightly connected to the collision count, with a Pearson’s <math id="S5.p12.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S5.p12.1.m1.1a"><mi id="S5.p12.1.m1.1.1" xref="S5.p12.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S5.p12.1.m1.1b"><ci id="S5.p12.1.m1.1.1.cmml" xref="S5.p12.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p12.1.m1.1c">r</annotation></semantics></math> coefficient of <math id="S5.p12.2.m2.1" class="ltx_Math" alttext="0.955" display="inline"><semantics id="S5.p12.2.m2.1a"><mn id="S5.p12.2.m2.1.1" xref="S5.p12.2.m2.1.1.cmml">0.955</mn><annotation-xml encoding="MathML-Content" id="S5.p12.2.m2.1b"><cn type="float" id="S5.p12.2.m2.1.1.cmml" xref="S5.p12.2.m2.1.1">0.955</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p12.2.m2.1c">0.955</annotation></semantics></math>.</p>
</div>
<div id="S5.p13" class="ltx_para ltx_noindent">
<p id="S5.p13.2" class="ltx_p"><span id="S5.p13.2.1" class="ltx_text ltx_font_bold">Planner-Centric Metrics.</span>
Our results show that ADE marks a better indicator for driving performance than FDE. While the planner-centric ADE does correlate to Driving Score and collision count (<math id="S5.p13.1.m1.1" class="ltx_Math" alttext="0.78" display="inline"><semantics id="S5.p13.1.m1.1a"><mn id="S5.p13.1.m1.1.1" xref="S5.p13.1.m1.1.1.cmml">0.78</mn><annotation-xml encoding="MathML-Content" id="S5.p13.1.m1.1b"><cn type="float" id="S5.p13.1.m1.1.1.cmml" xref="S5.p13.1.m1.1.1">0.78</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p13.1.m1.1c">0.78</annotation></semantics></math> &amp; <math id="S5.p13.2.m2.1" class="ltx_Math" alttext="0.77" display="inline"><semantics id="S5.p13.2.m2.1a"><mn id="S5.p13.2.m2.1.1" xref="S5.p13.2.m2.1.1.cmml">0.77</mn><annotation-xml encoding="MathML-Content" id="S5.p13.2.m2.1b"><cn type="float" id="S5.p13.2.m2.1.1.cmml" xref="S5.p13.2.m2.1.1">0.77</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p13.2.m2.1c">0.77</annotation></semantics></math>, respectively), these correlations pale in comparison to those of the mAP-based metrics (see Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Experiments ‣ On Offline Evaluation of 3D Object Detection for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The significant discrepancies in correlations we observe between the approaches, therefore, contra-indicate a reliance on planner-centric metrics alone when evaluating object detection for self-driving.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we demonstrate that common metrics for 3D object detection are highly correlated with online driving performance.
Our extensive evaluation shows that the nuScenes Detection Score is more predictive of closed-loop outcomes than the standard mean average precision. While we find the standard mAP score to yield strong correlation nonetheless, our results invoke skepticism regarding detection benchmarks that exclusively rely on planner-centric approaches or use a strong focus on heading accuracy.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">There are two important limitations that constrain the degree to which one can generalize from our results. First, we base all our experiments on the same neural planning architecture. In our experiments, PlanT acts as a mediator between the detection performance and the driving outcomes. Different planners might focus on other object cues for motion forecasting and behavior planning.
Second, the online metric Driving Score is a relatively simple heuristic for evaluating overall driving performance, and more accurate online metrics might yield different correlation outcomes.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Acknowledgements.</span> This work was supported by the BMBF (Tübingen AI Center, FKZ: 01IS18039A), the DFG (SFB 1233, TP 17, project number: 276693517), and by EXC (number 2064/1 – project number 390727645). We thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting K. Renz and K. Chitta. The authors also thank Luis Winckelmann for several helpful discussions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Carla leaderboard.
</span>
</span>
<span class="ltx_bibblock"><a href="leaderboard.carla.org" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">leaderboard.carla.org</a><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">Accessed: 2023-06-20.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Ayoosh Bansal, Jayati Singh, Micaela Verucchi, Marco Caccamo, and Lui Sha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Risk ranked recall: Collision safety metric for object detection
systems in autonomous vehicles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Mediterranean Conference on Embedded Computing (MECO)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">nuscenes: A multimodal dataset for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex
Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">nuplan: A closed-loop ml-based planning benchmark for autonomous
vehicles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2106.11810</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak,
Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Argoverse: 3d tracking and forecasting with rich maps.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and
Andreas Geiger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Transfuser: Imitation with transformer-based sensor fusion for
autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Felipe Codevilla, Antonio M Lopez, Vladlen Koltun, and Alexey Dosovitskiy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">On offline evaluation of vision-based driving models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">,
2018.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Daniel Dauner, Marcel Hallgarten, Andreas Geiger, and Kashyap Chitta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Parting with misconceptions about learning-based vehicle motion
planning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.07962</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Boyang Deng, Charles R Qi, Mahyar Najibi, Thomas Funkhouser, Yin Zhou, and
Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Revisiting 3d object detection from an egocentric perspective.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and
Houqiang Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Voxel r-cnn: Towards high performance voxel-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Carla: An open urban driving simulator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on robot learning</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Mark Everingham, Andrew Zisserman, Christopher KI Williams, Luc Van Gool, Moray
Allan, Christopher M Bishop, Olivier Chapelle, Navneet Dalal, Thomas
Deselaers, Gyuri Dorkó, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">The 2005 pascal visual object classes challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Machine Learning Challenges</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2006.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Are we ready for autonomous driving? the kitti vision benchmark
suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on computer vision and pattern recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Fitash Ul Haq, Donghwan Shin, Shiva Nejati, and Lionel Briand.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Can offline testing of deep neural networks replace their online
testing? a case study of automated driving systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Empirical Software Engineering</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Derek Hoiem, Santosh K Divvala, and James H Hays.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Pascal voc 2008 challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">World Literature Today</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain,
Sammy Omari, Vladimir Iglovikov, and Peter Ondruska.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">One thousand and one hours: Self-driving motion prediction dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Robot Learning</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Boris Ivanovic and Marco Pavone.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Injecting planning-awareness into prediction and detection
evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Intelligent Vehicles Symposium (IV)</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Harold W Kuhn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">The hungarian method for the assignment problem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Naval research logistics quarterly</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 1955.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Jacob Lambert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Project 3d_lidar_detection_evaluation.
</span>
</span>
<span class="ltx_bibblock"><a href="github.com/jacoblambert/3d_lidar_detection_evaluation" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">github.com/jacoblambert/3d_lidar_detection_evaluation</a><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">Accessed: 2023-05-20.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar
Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Pointpillars: Fast encoders for object detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Wei-Xin Li and Xiaodong Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Transcendental idealism of planner: Evaluating perception from
planning perspective for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.07276</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
OD-Team.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Openpcdet: An open-source toolbox for 3d object detection from point
clouds.
</span>
</span>
<span class="ltx_bibblock"><a href="github.com/open-mmlab/OpenPCDet" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">github.com/open-mmlab/OpenPCDet</a><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:90%;">Accessed: 2023-02-20.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Jonah Philion, Amlan Kar, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Learning to evaluate perception models using planner-centric metrics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Rui Qian, Xin Lai, and Xirong Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">3d object detection for autonomous driving: a survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern Recognition</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Katrin Renz, Kashyap Chitta, Otniel-Bogdan Mercea, A Koepke, Zeynep Akata, and
Andreas Geiger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Plant: Explainable planning transformers via object-level
representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.14222</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Vincenzo Riccio, Gunel Jahangirova, Andrea Stocco, Nargiz Humbatova, Michael
Weiss, and Paolo Tonella.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Testing machine learning based systems: a systematic mapping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Empirical Software Engineering</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Guangsheng Shi, Ruifeng Li, and Chao Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Pillarnet: Real-time and high-performance pillar-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and
Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi,
Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Pv-rcnn++: Point-voxel feature set abstraction with local vector
representation for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">From points to parts: 3d object detection from point cloud with
part-aware and part-aggregation network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Andrea Stocco, Brian Pulfer, and Paolo Tonella.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Model vs system level testing of autonomous driving systems: a
replication and extension study.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Empirical Software Engineering</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, and Benjamin Caine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Scalability in perception for autonomous driving: Waymo open dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Yan Yan, Yuxing Mao, and Bo Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Second: Sparsely embedded convolutional detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.12778" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.12779" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.12779">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.12779" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.12780" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 10:15:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
