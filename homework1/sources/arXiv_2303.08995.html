<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.08995] Fast and Accurate Object Detection on Asymmetrical Receptive Field</title><meta property="og:description" content="Object detection has been used in a wide range of industries. For example, in autonomous driving, the task of object detection is to accurately and efficiently identify and locate a large number of predefined classes o…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fast and Accurate Object Detection on Asymmetrical Receptive Field">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fast and Accurate Object Detection on Asymmetrical Receptive Field">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.08995">

<!--Generated on Thu Feb 29 19:30:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fast and Accurate Object Detection on Asymmetrical 
<br class="ltx_break">Receptive Field</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liguo Zhou, Tianhao Lin, Alois Knoll
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Object detection has been used in a wide range of industries. For example, in autonomous driving, the task of object detection is to accurately and efficiently identify and locate a large number of predefined classes of object instances (vehicles, pedestrians, traffic signs, etc.) from videos of roads. In robotics, the industry robot needs to recognize specific machine elements. In the security field, the camera should accurately recognize each face of people. With the wide application of deep learning, the accuracy and efficiency of object detection have been greatly improved, but object detection based on deep learning still faces challenges. Different applications of object detection have different requirements, including highly accurate detection, multi-category object detection, real-time detection, robustness to occlusions, etc. To address the above challenges, based on extensive literature research, this paper analyzes methods for improving and optimizing mainstream object detection algorithms from the perspective of evolution of one-stage and two-stage object detection algorithms. Furthermore, this article proposes methods for improving object detection accuracy from the perspective of changing receptive fields. The new model is based on the original YOLOv5 (You Look Only Once) with some modifications. The structure of the head part of YOLOv5 is modified by adding asymmetrical pooling layers. As a result, the accuracy of the algorithm is improved while ensuring the speed. The performances of the new model in this article are compared with original YOLOv5 model and analyzed from several parameters. And the evaluation of the new model is presented in four situations. Moreover, the summary and outlooks are made on the problems to be solved and the research directions in the future.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, object detection has always been a fundamental problem in computer vision. Object detection can be divided into two major schools of thought due to different tendencies for effectiveness. One is two-stage object detection, which focuses more on accuracy, and the other is one-stage object detection, which focuses more on speed. Two-stage object detection, as the name implies, solves the problem in two stages. The first stage is the generation of the regions of interest(RoI) which is called Region Proposal and the extraction of features using convolutional neural networks. The second stage is to put the output of the first stage into the support vector machine (SVM) or CNN-based classifier to classify objects and then correct the objects’ positions using the bounding box regression. Two-stage object detection originated from Regions with CNN features (R-CNN) (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>). R-CNN uses a heuristic (Selective search) to reduce the redundancy of information and improve the speed of detection by firstly forming Region proposal before detection. In addition, the robustness of feature extraction is improved. The researchers then proposed a new neural network by applying a technique named Spatial Pyramid Pooling (SPP) (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>), which not only reduces computational redundancy, but more importantly, breaks the bound of fixed-size input of fully collected layer. After SPP Net, Fast R-CNN (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>) emerged. Compared with the original Slow R-CNN, it has been optimized for speed. It changes the original serial structure to parallel structure. The algorithm performs regression on bounding box (Bbox) while classifying. But this was not good enough, so the researchers proposed Faster R-CNN (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>). Different from the previous heuristic algorithm to produce region proposals, Faster R-CNN proposes a concept of Region Proposal Networks(RPN), which use neural networks to learn to generate region proposals. Meanwhile, the concept of anchor has been introduced in RPNs. The object detection of R-CNN series has been improved and evolved step by step to get the final Faster R-CNN algorithm, which has a great improvement in both accuracy and speed. However, there is still no way to achieve real-time object detection, so the One-Stage object detection algorithm was proposed later.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">One-stage object detection is a one-shot solution that directly regresses on the predicted object of interest. Compared to two-stage object detection, it is very fast and finds a balance between fast and accurate. ’You only look once’ (YOLO) (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>) is one of the representative algorithms. YOLO first resizes the image to a fixed size, then passes it through a set of convolutional neural networks, and finally connects it to fully convolutional layer to output the result directly, which is the basic structure of the whole network. The latest algorithms YOLOv5 has been able to obtain relatively satisfactory results. It’s very fast while ensuring sufficient accuracy. Throughout the neural network model, we believe that the final feature map plays a critical role in the results. Each pixel in feature map has a corresponding receptive field (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>). The depth of feature map is deeper, the receptive field is larger. In the end of the neural network, YOLOv5 algorithm generates three different sizes of feature map. All of those pixels from these three feature maps have the same shape of receptive field——square. Moreover, for better detection of different shapes objects, each feature map has three different shapes of anchors. Now we have a new conjecture that if we change the shape of receptive field, the detection capability of the algorithm will be improved, and it will be easier to detect objects having different shapes. The feature map whose pixels have square receptive field can detect square object more easily. Conversely, the feature map whose pixels have rectangular receptive field can detect rectangular object more easily. Based on this conjecture, we make some modifications to the YOLOv5 model so that we can change the receptive fields of the final feature maps.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we firstly introduce the development of YOLO (You Look Only Once). Then we introduce COCO (Common Objects in Context) dataset (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>). Finally, we explain the metrics in the evaluation of YOLO algorithm.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Development of YOLO</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">As the pioneer of one-stage algorithm, YOLOv1 (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>) was a big hit with its simple network structure and real-time detection speed on GPU, breaking the ”monopoly” of R-CNN series and bringing a huge change to the field of object detection. YOLOv1 has many drawbacks when viewed from today’s perspective, but back then, YOLOv1 was very popular and provided the framework basis for many one-stage algorithms later. The most important feature of YOLOv1 is that it uses only one convolutional neural network to achieve the purpose of object detection end-to-end. At CVPR 2016, following the YOLOv1 work, the original authors reintroduced YOLOv2 (or YOLO9000) (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>). Compared with YOLOv1, YOLOv2
introduced the anchor box mechanism proposed by the Faster R-CNN, the use of K-means clustering algorithm to obtain a better anchor box. The regression method of the bounding box was also adjusted. Later, YOLOv3 (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>) is proposed. The changes of YOLOv3 is not only using a better backbone network: DarkNet-53, but also using Feature Pyramid Networks (FPN) (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>) technology and multi-level detection methods. YOLOv4 (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>) was proposed in April 2020. It achieves 43.5% AP accuracy and 65 FPS on MS COCO dataset, a 10% and 12% improvement compared to YOLOv3, respectively. YOLOv4 uses the CSPDarkNet-53 network as a new backbone network, which had excellent speed and accuracy at the time. Shortly after YOLOv4 was proposed, Ultralytics came up with YOLOv5. YOLOv5 has no particular changes in network structure, but it has better performance on speed and accuracy.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>COCO Dataset</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.4" class="ltx_p">The COCO dataset (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>) is a large-scale dataset that can be used for image detection, semantic segmentation and image captioning. It has more than 330K images (220K of them are annotated), containing 1.5 million objects, 80 object categories (pedestrian, car, bicycle, etc.), 91 stuff categories (grass, wall, sky, etc.), five image descriptions per image, and 250K pedestrians with key point annotation. As for object detection, we use COCO2017. The number of training images are 118K, the number of validation images are 5K and there are 40K test images. Each label of images has 5 parameters, which are category, <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">x</annotation></semantics></math> coordinate of centroid, <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">y</annotation></semantics></math> coordinate of centroid, width <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">w</annotation></semantics></math>, and height <math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><mi id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><ci id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">h</annotation></semantics></math>. The dataset contains 80 categories covering a large number of real-life scenarios, such as traffic, interviews, dances, animals, etc. These objects differ in scale, occlusion, pose, expression, and lighting conditions. Therefore, the training data is large enough to be a challenge for the detector.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.4.5.1" class="ltx_tr">
<th id="S2.T1.4.5.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<td id="S2.T1.4.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training set</td>
<td id="S2.T1.4.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Validation set</td>
<td id="S2.T1.4.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Testing set</td>
<td id="S2.T1.4.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Total</td>
</tr>
<tr id="S2.T1.4.6.2" class="ltx_tr">
<th id="S2.T1.4.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Nr. of images</th>
<td id="S2.T1.4.6.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">118,287</td>
<td id="S2.T1.4.6.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5,000</td>
<td id="S2.T1.4.6.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40,670</td>
<td id="S2.T1.4.6.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">163,957</td>
</tr>
<tr id="S2.T1.4.4" class="ltx_tr">
<th id="S2.T1.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Percentage</th>
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S2.T1.1.1.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S2.T1.1.1.1.m1.1a"><mrow id="S2.T1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.1.1.1.m1.1.1.2" xref="S2.T1.1.1.1.m1.1.1.2.cmml">70</mn><mo id="S2.T1.1.1.1.m1.1.1.1" xref="S2.T1.1.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S2.T1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S2.T1.1.1.1.m1.1.1.2.cmml" xref="S2.T1.1.1.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">70\%</annotation></semantics></math></td>
<td id="S2.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S2.T1.2.2.2.m1.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="S2.T1.2.2.2.m1.1a"><mrow id="S2.T1.2.2.2.m1.1.1" xref="S2.T1.2.2.2.m1.1.1.cmml"><mn id="S2.T1.2.2.2.m1.1.1.2" xref="S2.T1.2.2.2.m1.1.1.2.cmml">5</mn><mo id="S2.T1.2.2.2.m1.1.1.1" xref="S2.T1.2.2.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.m1.1b"><apply id="S2.T1.2.2.2.m1.1.1.cmml" xref="S2.T1.2.2.2.m1.1.1"><csymbol cd="latexml" id="S2.T1.2.2.2.m1.1.1.1.cmml" xref="S2.T1.2.2.2.m1.1.1.1">percent</csymbol><cn type="integer" id="S2.T1.2.2.2.m1.1.1.2.cmml" xref="S2.T1.2.2.2.m1.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.m1.1c">5\%</annotation></semantics></math></td>
<td id="S2.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S2.T1.3.3.3.m1.1" class="ltx_Math" alttext="25\%" display="inline"><semantics id="S2.T1.3.3.3.m1.1a"><mrow id="S2.T1.3.3.3.m1.1.1" xref="S2.T1.3.3.3.m1.1.1.cmml"><mn id="S2.T1.3.3.3.m1.1.1.2" xref="S2.T1.3.3.3.m1.1.1.2.cmml">25</mn><mo id="S2.T1.3.3.3.m1.1.1.1" xref="S2.T1.3.3.3.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.m1.1b"><apply id="S2.T1.3.3.3.m1.1.1.cmml" xref="S2.T1.3.3.3.m1.1.1"><csymbol cd="latexml" id="S2.T1.3.3.3.m1.1.1.1.cmml" xref="S2.T1.3.3.3.m1.1.1.1">percent</csymbol><cn type="integer" id="S2.T1.3.3.3.m1.1.1.2.cmml" xref="S2.T1.3.3.3.m1.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.m1.1c">25\%</annotation></semantics></math></td>
<td id="S2.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S2.T1.4.4.4.m1.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S2.T1.4.4.4.m1.1a"><mrow id="S2.T1.4.4.4.m1.1.1" xref="S2.T1.4.4.4.m1.1.1.cmml"><mn id="S2.T1.4.4.4.m1.1.1.2" xref="S2.T1.4.4.4.m1.1.1.2.cmml">100</mn><mo id="S2.T1.4.4.4.m1.1.1.1" xref="S2.T1.4.4.4.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.m1.1b"><apply id="S2.T1.4.4.4.m1.1.1.cmml" xref="S2.T1.4.4.4.m1.1.1"><csymbol cd="latexml" id="S2.T1.4.4.4.m1.1.1.1.cmml" xref="S2.T1.4.4.4.m1.1.1.1">percent</csymbol><cn type="integer" id="S2.T1.4.4.4.m1.1.1.2.cmml" xref="S2.T1.4.4.4.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.m1.1c">100\%</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.6.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.7.2" class="ltx_text" style="font-size:90%;">Basic statistics of the COCO dataset.</span></figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Metrics</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Several metrics are widely used to evaluate the performance of object detection, which mainly include Precision-Recall (PR) curves and Average Precision (AP) (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>). Before we explain this two metrics, we need first introduce the confusion matrix.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.3.4.1" class="ltx_tr">
<th id="S2.T2.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.3.4.1.1.1" class="ltx_text">Actual condition</span></th>
<th id="S2.T2.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="2">Predicted condition</th>
</tr>
<tr id="S2.T2.3.5.2" class="ltx_tr">
<td id="S2.T2.3.5.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Positive</td>
<td id="S2.T2.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Negative</td>
</tr>
<tr id="S2.T2.1.1" class="ltx_tr">
<th id="S2.T2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Positive</th>
<td id="S2.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">TP (True Positive)</td>
<td id="S2.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S2.T2.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{FN}" display="inline"><semantics id="S2.T2.1.1.1.m1.1a"><mi id="S2.T2.1.1.1.m1.1.1" xref="S2.T2.1.1.1.m1.1.1.cmml">FN</mi><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.m1.1b"><ci id="S2.T2.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.m1.1.1">FN</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.m1.1c">\mathrm{FN}</annotation></semantics></math> (False Negative)</td>
</tr>
<tr id="S2.T2.3.3" class="ltx_tr">
<th id="S2.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Negative</th>
<td id="S2.T2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<math id="S2.T2.2.2.1.m1.1" class="ltx_Math" alttext="\mathrm{FP}" display="inline"><semantics id="S2.T2.2.2.1.m1.1a"><mi id="S2.T2.2.2.1.m1.1.1" xref="S2.T2.2.2.1.m1.1.1.cmml">FP</mi><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.1.m1.1b"><ci id="S2.T2.2.2.1.m1.1.1.cmml" xref="S2.T2.2.2.1.m1.1.1">FP</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.1.m1.1c">\mathrm{FP}</annotation></semantics></math> (False Positive)</td>
<td id="S2.T2.3.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<math id="S2.T2.3.3.2.m1.1" class="ltx_Math" alttext="\mathrm{TN}" display="inline"><semantics id="S2.T2.3.3.2.m1.1a"><mi id="S2.T2.3.3.2.m1.1.1" xref="S2.T2.3.3.2.m1.1.1.cmml">TN</mi><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.2.m1.1b"><ci id="S2.T2.3.3.2.m1.1.1.cmml" xref="S2.T2.3.3.2.m1.1.1">TN</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.2.m1.1c">\mathrm{TN}</annotation></semantics></math> (True Negative)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.5.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S2.T2.6.2" class="ltx_text" style="font-size:90%;">Confusion table for classification results.</span></figcaption>
</figure>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">True Positive (TP) means that the sample is actually positive and the network also predicts the sample as positive. True Negative (TN) means that the sample is actually negative and the network also predicts the sample as negative. Therefore, if the result is TP or TN, the network makes true predictions. False Positive (FP) means the prediction is wrong, because the sample is actually negative but the network predicts it as positive. False Negative: the sample is actually positive but the network predicts it as negative, so this prediction is also wrong. Precision and Recall is a common pair of performance metrics based on confusion matrix.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\text{ Precision }=\frac{TP}{TP+FP}" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mtext id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2a.cmml"> Precision </mtext><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mfrac id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mrow id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2" xref="S2.E1.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.2.1" xref="S2.E1.m1.1.1.3.2.1.cmml">​</mo><mi id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><mrow id="S2.E1.m1.1.1.3.3.2" xref="S2.E1.m1.1.1.3.3.2.cmml"><mi id="S2.E1.m1.1.1.3.3.2.2" xref="S2.E1.m1.1.1.3.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.3.2.1" xref="S2.E1.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S2.E1.m1.1.1.3.3.2.3" xref="S2.E1.m1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S2.E1.m1.1.1.3.3.1" xref="S2.E1.m1.1.1.3.3.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.3.3.3" xref="S2.E1.m1.1.1.3.3.3.cmml"><mi id="S2.E1.m1.1.1.3.3.3.2" xref="S2.E1.m1.1.1.3.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.3.3.1" xref="S2.E1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.3.3.3.3" xref="S2.E1.m1.1.1.3.3.3.3.cmml">P</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><ci id="S2.E1.m1.1.1.2a.cmml" xref="S2.E1.m1.1.1.2"><mtext id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"> Precision </mtext></ci><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><divide id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3"></divide><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><times id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2.1"></times><ci id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2">𝑇</ci><ci id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3">𝑃</ci></apply><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><plus id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.1"></plus><apply id="S2.E1.m1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2"><times id="S2.E1.m1.1.1.3.3.2.1.cmml" xref="S2.E1.m1.1.1.3.3.2.1"></times><ci id="S2.E1.m1.1.1.3.3.2.2.cmml" xref="S2.E1.m1.1.1.3.3.2.2">𝑇</ci><ci id="S2.E1.m1.1.1.3.3.2.3.cmml" xref="S2.E1.m1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3"><times id="S2.E1.m1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.3.1"></times><ci id="S2.E1.m1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2">𝐹</ci><ci id="S2.E1.m1.1.1.3.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\text{ Precision }=\frac{TP}{TP+FP}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.1" class="ltx_Math" alttext="\text{ Recall }=\frac{TP}{TP+FN}" display="block"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><mtext id="S2.E2.m1.1.1.2" xref="S2.E2.m1.1.1.2a.cmml"> Recall </mtext><mo id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml">=</mo><mfrac id="S2.E2.m1.1.1.3" xref="S2.E2.m1.1.1.3.cmml"><mrow id="S2.E2.m1.1.1.3.2" xref="S2.E2.m1.1.1.3.2.cmml"><mi id="S2.E2.m1.1.1.3.2.2" xref="S2.E2.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.3.2.1" xref="S2.E2.m1.1.1.3.2.1.cmml">​</mo><mi id="S2.E2.m1.1.1.3.2.3" xref="S2.E2.m1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S2.E2.m1.1.1.3.3" xref="S2.E2.m1.1.1.3.3.cmml"><mrow id="S2.E2.m1.1.1.3.3.2" xref="S2.E2.m1.1.1.3.3.2.cmml"><mi id="S2.E2.m1.1.1.3.3.2.2" xref="S2.E2.m1.1.1.3.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.3.3.2.1" xref="S2.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S2.E2.m1.1.1.3.3.2.3" xref="S2.E2.m1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S2.E2.m1.1.1.3.3.1" xref="S2.E2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S2.E2.m1.1.1.3.3.3" xref="S2.E2.m1.1.1.3.3.3.cmml"><mi id="S2.E2.m1.1.1.3.3.3.2" xref="S2.E2.m1.1.1.3.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.3.3.3.1" xref="S2.E2.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S2.E2.m1.1.1.3.3.3.3" xref="S2.E2.m1.1.1.3.3.3.3.cmml">N</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><eq id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"></eq><ci id="S2.E2.m1.1.1.2a.cmml" xref="S2.E2.m1.1.1.2"><mtext id="S2.E2.m1.1.1.2.cmml" xref="S2.E2.m1.1.1.2"> Recall </mtext></ci><apply id="S2.E2.m1.1.1.3.cmml" xref="S2.E2.m1.1.1.3"><divide id="S2.E2.m1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.3"></divide><apply id="S2.E2.m1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.3.2"><times id="S2.E2.m1.1.1.3.2.1.cmml" xref="S2.E2.m1.1.1.3.2.1"></times><ci id="S2.E2.m1.1.1.3.2.2.cmml" xref="S2.E2.m1.1.1.3.2.2">𝑇</ci><ci id="S2.E2.m1.1.1.3.2.3.cmml" xref="S2.E2.m1.1.1.3.2.3">𝑃</ci></apply><apply id="S2.E2.m1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.3.3"><plus id="S2.E2.m1.1.1.3.3.1.cmml" xref="S2.E2.m1.1.1.3.3.1"></plus><apply id="S2.E2.m1.1.1.3.3.2.cmml" xref="S2.E2.m1.1.1.3.3.2"><times id="S2.E2.m1.1.1.3.3.2.1.cmml" xref="S2.E2.m1.1.1.3.3.2.1"></times><ci id="S2.E2.m1.1.1.3.3.2.2.cmml" xref="S2.E2.m1.1.1.3.3.2.2">𝑇</ci><ci id="S2.E2.m1.1.1.3.3.2.3.cmml" xref="S2.E2.m1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S2.E2.m1.1.1.3.3.3.cmml" xref="S2.E2.m1.1.1.3.3.3"><times id="S2.E2.m1.1.1.3.3.3.1.cmml" xref="S2.E2.m1.1.1.3.3.3.1"></times><ci id="S2.E2.m1.1.1.3.3.3.2.cmml" xref="S2.E2.m1.1.1.3.3.3.2">𝐹</ci><ci id="S2.E2.m1.1.1.3.3.3.3.cmml" xref="S2.E2.m1.1.1.3.3.3.3">𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\text{ Recall }=\frac{TP}{TP+FN}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">Average Precision (AP) represents the area under the Precision-Recall curve. Generally, the higher the value of AP, the better the performance of the classifier. The value of AP lies in [0,1]. A perfect classifier will have an AP value of 1. Each class has a AP (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>). The mean Average Precision (mAP) is calculated by finding AP for each class and then average over a number of classes.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.1" class="ltx_Math" alttext="\mathrm{mAP}=\frac{1}{N}\sum_{i=1}^{N}\mathrm{AP}_{i}" display="block"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml"><mi id="S2.E3.m1.1.1.2" xref="S2.E3.m1.1.1.2.cmml">mAP</mi><mo id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml">=</mo><mrow id="S2.E3.m1.1.1.3" xref="S2.E3.m1.1.1.3.cmml"><mfrac id="S2.E3.m1.1.1.3.2" xref="S2.E3.m1.1.1.3.2.cmml"><mn id="S2.E3.m1.1.1.3.2.2" xref="S2.E3.m1.1.1.3.2.2.cmml">1</mn><mi id="S2.E3.m1.1.1.3.2.3" xref="S2.E3.m1.1.1.3.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.3.1" xref="S2.E3.m1.1.1.3.1.cmml">​</mo><mrow id="S2.E3.m1.1.1.3.3" xref="S2.E3.m1.1.1.3.3.cmml"><munderover id="S2.E3.m1.1.1.3.3.1" xref="S2.E3.m1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S2.E3.m1.1.1.3.3.1.2.2" xref="S2.E3.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S2.E3.m1.1.1.3.3.1.2.3" xref="S2.E3.m1.1.1.3.3.1.2.3.cmml"><mi id="S2.E3.m1.1.1.3.3.1.2.3.2" xref="S2.E3.m1.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S2.E3.m1.1.1.3.3.1.2.3.1" xref="S2.E3.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S2.E3.m1.1.1.3.3.1.2.3.3" xref="S2.E3.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E3.m1.1.1.3.3.1.3" xref="S2.E3.m1.1.1.3.3.1.3.cmml">N</mi></munderover><msub id="S2.E3.m1.1.1.3.3.2" xref="S2.E3.m1.1.1.3.3.2.cmml"><mi id="S2.E3.m1.1.1.3.3.2.2" xref="S2.E3.m1.1.1.3.3.2.2.cmml">AP</mi><mi id="S2.E3.m1.1.1.3.3.2.3" xref="S2.E3.m1.1.1.3.3.2.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1"><eq id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"></eq><ci id="S2.E3.m1.1.1.2.cmml" xref="S2.E3.m1.1.1.2">mAP</ci><apply id="S2.E3.m1.1.1.3.cmml" xref="S2.E3.m1.1.1.3"><times id="S2.E3.m1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.3.1"></times><apply id="S2.E3.m1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.3.2"><divide id="S2.E3.m1.1.1.3.2.1.cmml" xref="S2.E3.m1.1.1.3.2"></divide><cn type="integer" id="S2.E3.m1.1.1.3.2.2.cmml" xref="S2.E3.m1.1.1.3.2.2">1</cn><ci id="S2.E3.m1.1.1.3.2.3.cmml" xref="S2.E3.m1.1.1.3.2.3">𝑁</ci></apply><apply id="S2.E3.m1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.3.3"><apply id="S2.E3.m1.1.1.3.3.1.cmml" xref="S2.E3.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.3.3.1.1.cmml" xref="S2.E3.m1.1.1.3.3.1">superscript</csymbol><apply id="S2.E3.m1.1.1.3.3.1.2.cmml" xref="S2.E3.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.3.3.1.2.1.cmml" xref="S2.E3.m1.1.1.3.3.1">subscript</csymbol><sum id="S2.E3.m1.1.1.3.3.1.2.2.cmml" xref="S2.E3.m1.1.1.3.3.1.2.2"></sum><apply id="S2.E3.m1.1.1.3.3.1.2.3.cmml" xref="S2.E3.m1.1.1.3.3.1.2.3"><eq id="S2.E3.m1.1.1.3.3.1.2.3.1.cmml" xref="S2.E3.m1.1.1.3.3.1.2.3.1"></eq><ci id="S2.E3.m1.1.1.3.3.1.2.3.2.cmml" xref="S2.E3.m1.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S2.E3.m1.1.1.3.3.1.2.3.3.cmml" xref="S2.E3.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E3.m1.1.1.3.3.1.3.cmml" xref="S2.E3.m1.1.1.3.3.1.3">𝑁</ci></apply><apply id="S2.E3.m1.1.1.3.3.2.cmml" xref="S2.E3.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.3.3.2.1.cmml" xref="S2.E3.m1.1.1.3.3.2">subscript</csymbol><ci id="S2.E3.m1.1.1.3.3.2.2.cmml" xref="S2.E3.m1.1.1.3.3.2.2">AP</ci><ci id="S2.E3.m1.1.1.3.3.2.3.cmml" xref="S2.E3.m1.1.1.3.3.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\mathrm{mAP}=\frac{1}{N}\sum_{i=1}^{N}\mathrm{AP}_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.p7" class="ltx_para ltx_noindent">
<p id="S2.SS3.p7.1" class="ltx_p">The mAP incorporates the trade-off between precision and recall and considers both false positives (FP) and false negatives (FN). This property makes mAP a suitable metric for most detection applications.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Methodology</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Architecture of YOLOv5</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In the previous chapter, we briefly introduced the YOLO family. In this section, we will specifically introduce the latest version of the YOLO algorithm, i.e., YOLOv5 and its network structure. Similar to the previous version of YOLO, the whole YOLOv5 can still be divided into three parts, namely backbone, neck and head, see figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Architecture of YOLOv5 ‣ 3 Proposed Methodology ‣ Fast and Accurate Object Detection on Asymmetrical Receptive Field" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S3.F2" title="Figure 2 ‣ 3.1 Architecture of YOLOv5 ‣ 3 Proposed Methodology ‣ Fast and Accurate Object Detection on Asymmetrical Receptive Field" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Backbone can be regarded as the feature extraction network of YOLOv5, and according to its structure and the previous YOLOv4 backbone, we can generally call it CSPDarknet. The input images are first extracted in CSPDarknet, and the extracted features can be called feature maps. In the backbone part, we obtain three feature maps for the next step of network, i.e., the neck part. This part can be also called the enhanced feature extraction network of YOLOv5. The three feature maps obtained in the backbone part are fused in this part, and the purpose of the feature fusion is to combine the feature information from different scales. In the neck part, , the Path Aggregation Network (PAN) structure is used (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>), where we not only upsample the features to achieve feature fusion, but also downsample the features again to achieve feature fusion. Head is the classifier and regressor of YOLOv5. With backbone and neck, we have access to three enhanced feature maps. Each feature map has a width, height and number of channels, so we can think of the feature map as a collection of feature pixels, each of which has a number of channels. As in previous versions of YOLO, the detector head in YOLOv5 is composite, i.e., the classification and bounding box regression are implemented by a 1 × 1 convolution. In summary, the entire YOLOv5 network is doing the following: feature extraction - feature enhancement - prediction of objects corresponding to the feature pixels.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">As for the CSPDarknet in YOLOv5, it has four important features: (1) Using residual network (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>). The residual convolution in CSPDarknet can be divided into two parts, the main part is a 1 × 1 convolution and a 3 × 3 convolution; the skip connection does not do any processing, and directly combining the input and output of the main part. The whole YOLOv5 backbone is composed of residual convolution. The residual structure is characterized by its ease of optimization and its ability to improve accuracy by adding considerable depth. Its internal residual blocks use skip connections to alleviate the problem of gradient vanishing caused by increasing depth in deep neural networks. (2) Using the CSPnet structure (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>). The CSPnet structure is not too complicated, which is a splitting of the original stack of residual blocks into two parts: the main part continues the original stack of residual blocks; the other part acts like a skip connection and is directly connected to the end after a small amount of processing. Therefore, it can be considered that there is a large skip connection in the CSP. (3) Using SiLU activation function, which is an improved version of Sigmoid and ReLU. SiLU has the properties of no upper bound with lower bound, smooth, and non-monotonic. SiLU works better than ReLU on deep neural networks and it can be regarded as a smooth ReLU activation function. (4) Using SPPF structure. Feature extraction is performed by maximum pooling with different pooling kernel sizes to improve the receptive field of the network. In YOLOv4, SPP was used inside the neck, and in YOLOv5, the SPP module is used in the backbone.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2303.08995/assets/figures/Network1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="400" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Architecture of YOLOv5. The whole network is composed of Backbone, Neck and Head.
</span></figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2303.08995/assets/figures/Network2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="361" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Details of each component in the YOLOv5 backbone.
</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>New Head</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>New Head</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">In the previous section, we explained the structure of YOLOv5, which consists of backbone, neck and head. The first change to YOLOv5 in this thesis is to change the size of the feature map by adding asymmetrical pooling layer to the head part. The feature map sizes are 20 × 20 × 1024, 40 × 40 × 512, and 80 × 80 × 256 before passing through the 1 × 1 convolutional layer, respectively. Take 80 × 80 × 256 feature map as an example, after passing 1 × 1 convolutional layer, its dimension becomes 80 × 80 × 255, i.e., 80 × 80 × 3 (4 + 1 + 80). Number 3 shows that there are three feature maps of dimension 80 × 80. The difference between them is that the pixels in each feature map correspond to different sizes of anchor boxes. For example, the sizes of the three anchor boxes are 10 × 13, 16 × 30, and 33 × 18. The feature points of the three feature maps have the same characteristics, i.e., three groups of feature points have the same size of receptive fields, so they use different anchor boxes to try to fit the different shapes of the object. In our new model, we change both receptive fields and anchor boxes of each feature map. Firstly, the shape of the receptive field corresponding to each group of pixels needs to be changed. I speculate that when the receptive field corresponding to a point is square, the point has better prediction ability for objects whose shape is close to square. When the aspect ratio of the receptive field is 2:1, points can predict objects with the same shape better. The same is true for a receptive field with an aspect ratio of 1:2. So, after the 1 × 1 convolutional layer, I add two asymmetric pooling layers to head part, see figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 New Head ‣ 3.2 New Head ‣ 3 Proposed Methodology ‣ Fast and Accurate Object Detection on Asymmetrical Receptive Field" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Thus, in our new model, in order to distinguish the role of each anchor box more clearly, one anchor box is used to predict objects close to a square shape, one anchor box is used to predict rectangular objects whose width is larger than their height, and the last anchor box is used to predict rectangular objects whose width is smaller than their height. In summary, the head detector will no longer output 3 feature maps but 9 feature maps. Their sizes are 20 × 20 × 85, 20 × 19 × 85, 19 × 20 × 85, 40 × 40 × 85, 40 × 39 × 85, 39 × 40 × 85, 80 × 80 × 85, 80 × 79 × 85, and 79 × 80 × 85, respectively. For the pooling layer, this thesis chooses average pooling, in order to avoid losing too much context. Since only the pooling layer is added, the number of parameters of the new network does not increase, so it runs just as fast, and at the same time, we think its detection capability will be improved. In fact, one can also try to replace the pooling layer with a convolutional layer with kernel sizes of (1, 2) and (2, 1). Although the network’s parameters will increase, its running speed is not significantly affected.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2303.08995/assets/figures/new_head.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="414" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">The new head detector outputs 9 feature maps. For the input, it will be divided into three types of processing: Conv, Conv + (1,2) pooling and Conv + (2,1) pooling.
</span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>New Anchors</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Setting anchors in advance is an very important things. In original YOLOv5 model, depending on the size of the feature map, the anchors are divided into three groups: (10,13), (16,30), (33,23) for 80 × 80 feature map , (30,61), (62,45), (59,119) for 40 × 40 feature map and (116,90), (156,198), (373,326) for 20 × 20 feature map. In order to fit our new model better, we consider that the sizes of the anchors also needed to be modified. Therefore, the new anchors are generated. For the nine feature maps in the figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 New Head ‣ 3.2 New Head ‣ 3 Proposed Methodology ‣ Fast and Accurate Object Detection on Asymmetrical Receptive Field" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the anchors are (20,20), (40,20), (20,40), (60,60), (120,60), (60,120), (400,200), (200,400) in order. The shape of these anchors corresponds to the receptive field of each cell on the 9 feature maps. We assume that the feature map whose cells have rectangular receptive field can detect rectangular object more easily by using rectangular anchor.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>New Strategy of NMS</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">As we mentioned before, the head of YOLOv5 model is modified from 3 feature maps to 9 feature maps. Anchors are also modified to adapt to the corresponding feature maps. Finally, we divide 9 feature maps into 3 types: having square receptive fields, having receptive field with an aspect ratio of 2:1, and having receptive field with an aspect ratio of 1:2. We are going to use these three types of feature maps to detect objects with different aspect ratios. In original YOLOv5 model, the NMS is used on the whole predicted boxes and the number of times we use it is one. But in our method, we use four times NMS. NMS is first performed on the boxes predicted by each of the three types of feature maps, and after the results are fused, NMS is finally done again. Using this strategy, we hope that the new model will have better performance for multiple shapes and categories of objects.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Configuration</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The structure of the new model has the same backbone and neck as YOLOv5 (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>), only the head is different. Therefore, to ensure the training speed, YOLOv5n is chosen for the new model. We use VS Code as programming platform and the GPU for training and validation is RTX 3080. The machine learning framework is Pytorch. The version of cudatoolkit is 11.3.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training Hyperparameter</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The training parameters need to be adjusted before training, and these parameters are in train.py.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.2.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">’–weights’</th>
<td id="S4.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">default=’ ’</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">’–cfg’</th>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">default=’yolov5n.yaml’</td>
</tr>
<tr id="S4.T3.2.3.3" class="ltx_tr">
<th id="S4.T3.2.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">’–epochs’</th>
<td id="S4.T3.2.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">300</td>
</tr>
<tr id="S4.T3.2.4.4" class="ltx_tr">
<th id="S4.T3.2.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">’–batch-size’</th>
<td id="S4.T3.2.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">128</td>
</tr>
<tr id="S4.T3.2.5.5" class="ltx_tr">
<th id="S4.T3.2.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">’–imgsz’</th>
<td id="S4.T3.2.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">default=640</td>
</tr>
<tr id="S4.T3.2.6.6" class="ltx_tr">
<th id="S4.T3.2.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">”–noautoanchor”</th>
<td id="S4.T3.2.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">default=True</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">Hyperparameters for training of new model.</span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluations</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The goal of this thesis is to improve the YOLOv5 algorithm. We trained the original and modified YOLOv5n models on the COCO dataset and then compare their Precision, Recall and mAP. The trained models are divided into the following categories: (1) original models, i.e., the backbone, neck and head of the model are not changed. (2) Modified models, which can be divided into 4 kinds, contains three square anchors, three 2:1 aspect ratio, three 1:2 aspect ratio anchors, 9 anchors, respectively.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">For the evaluation of the models, we similarly divide the process of validation into the following steps: first, we validate the models which only contain anchors with a shape of square, 2:1 aspect ratio and 1:2 aspect ratio, respectively, to obtain three results, and then compare the results of them with original model. It should be noted that the validation sets of different models are different, for example, for the model with only 3 square anchors ([20, 20], [60, 60], [200, 200]), the labels in its validation set are also approximately square. And for the model with anchors with an aspect ratio of 2:1 ([40, 20], [120, 60], [400, 200]), the width of the labels in its validation set is also larger than the height. This is to verify our idea that a square receptive field with a square anchor is better at predicting objects that are approximately square, as well as rectangular receptive field with a rectangular anchor. Secondly, we validate the model having nine feature maps (the anchors are [20, 20], [60, 60], [200, 200], [40, 20], [120, 60], [400, 200], [20, 40], [60, 120], [200, 400], respectively). And the corresponding validation set contains 5000 pictures with complete labels. Finally, we validate the model with original architecture and modified <math id="S4.SS3.p2.1.m1.2" class="ltx_Math" alttext="loss.py" display="inline"><semantics id="S4.SS3.p2.1.m1.2a"><mrow id="S4.SS3.p2.1.m1.2.2.2" xref="S4.SS3.p2.1.m1.2.2.3.cmml"><mrow id="S4.SS3.p2.1.m1.1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.1.1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.1.m1.1.1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.1.1.1.1.1a" xref="S4.SS3.p2.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.1.m1.1.1.1.1.4" xref="S4.SS3.p2.1.m1.1.1.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.1.1.1.1.1b" xref="S4.SS3.p2.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.1.m1.1.1.1.1.5" xref="S4.SS3.p2.1.m1.1.1.1.1.5.cmml">s</mi></mrow><mo lspace="0em" rspace="0.167em" id="S4.SS3.p2.1.m1.2.2.2.3" xref="S4.SS3.p2.1.m1.2.2.3a.cmml">.</mo><mrow id="S4.SS3.p2.1.m1.2.2.2.2" xref="S4.SS3.p2.1.m1.2.2.2.2.cmml"><mi id="S4.SS3.p2.1.m1.2.2.2.2.2" xref="S4.SS3.p2.1.m1.2.2.2.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.2.2.2.2.1" xref="S4.SS3.p2.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S4.SS3.p2.1.m1.2.2.2.2.3" xref="S4.SS3.p2.1.m1.2.2.2.2.3.cmml">y</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.2b"><apply id="S4.SS3.p2.1.m1.2.2.3.cmml" xref="S4.SS3.p2.1.m1.2.2.2"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.2.2.3a.cmml" xref="S4.SS3.p2.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S4.SS3.p2.1.m1.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1.1"></times><ci id="S4.SS3.p2.1.m1.1.1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1.2">𝑙</ci><ci id="S4.SS3.p2.1.m1.1.1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1.3">𝑜</ci><ci id="S4.SS3.p2.1.m1.1.1.1.1.4.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1.4">𝑠</ci><ci id="S4.SS3.p2.1.m1.1.1.1.1.5.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1.5">𝑠</ci></apply><apply id="S4.SS3.p2.1.m1.2.2.2.2.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2"><times id="S4.SS3.p2.1.m1.2.2.2.2.1.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2.1"></times><ci id="S4.SS3.p2.1.m1.2.2.2.2.2.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2.2">𝑝</ci><ci id="S4.SS3.p2.1.m1.2.2.2.2.3.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2.3">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.2c">loss.py</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Validation on 3-Feature Maps Networks</h3>

<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Square-Anchor Model</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">In this section, we compares the performance of models with 3 kinds of anchors with the original model. As for the model with 3 square anchors, the validation dataset has 2,988 pictures. Each label in one picture has an aspect ratio between 1/1.2 and 1.2. Figure<a href="#S4.F4" title="Figure 4 ‣ 4.4.1 Square-Anchor Model ‣ 4.4 Validation on 3-Feature Maps Networks ‣ 4 Experiment ‣ Fast and Accurate Object Detection on Asymmetrical Receptive Field" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the PR-curve of the original model and square-anchor model. The difference between these two model is that the square-anchor model has three feature maps (20 × 20, 40 × 40, 80 × 80), but there is only one square anchor on each feature map.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2303.08995/assets/figures/square_anchor_PR.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="301" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">PR-curves of original model and square-anchor model. The mAP@0.5 of the original model is 0.202 and the mAP@0.5 of square-anchors model is 0.206. mAP@0.5 means the threshold of IoU is 0.5.
</span></figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">P</th>
<th id="S4.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">R</th>
<th id="S4.T4.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP@0.5</th>
<th id="S4.T4.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP@.5:.95</th>
<th id="S4.T4.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">pre-process</th>
<th id="S4.T4.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">inference</th>
<th id="S4.T4.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">NMS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.2.1" class="ltx_tr">
<td id="S4.T4.2.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Original</td>
<td id="S4.T4.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.243</td>
<td id="S4.T4.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.39</td>
<td id="S4.T4.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.202</td>
<td id="S4.T4.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.127</td>
<td id="S4.T4.2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.7ms</td>
<td id="S4.T4.2.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.3ms</td>
<td id="S4.T4.2.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.1ms</td>
</tr>
<tr id="S4.T4.2.3.2" class="ltx_tr">
<td id="S4.T4.2.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Square-anchors</td>
<td id="S4.T4.2.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.254</td>
<td id="S4.T4.2.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.365</td>
<td id="S4.T4.2.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.206</td>
<td id="S4.T4.2.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.13</td>
<td id="S4.T4.2.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.4ms</td>
<td id="S4.T4.2.3.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">4.1ms</td>
<td id="S4.T4.2.3.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2.3ms</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.4.2" class="ltx_text" style="font-size:90%;">Comparison of original model and square-anchor model. The first four statistics shows the performance of each model. The last three statistics show the processing speed of each image</span></figcaption>
</figure>
<div id="S4.SS4.SSS1.p2" class="ltx_para">
<p id="S4.SS4.SSS1.p2.1" class="ltx_p">The result shows that for approximately square labels, the feature maps which have square anchors and square receptive fields have better performance in terms of precision, mAP and processing speed.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Asymmetrical Average Pooling Model</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">We added each of the two asymmetrical average pooling layers to the head of the original network, thus obtaining two new models. For the model, which is added a (1, 2) pooling layer, its aspect ratio of receptive field becomes 2:1. And its validation set has 3,158 images containing 8,522 labels. Each label in one picture has an aspect ratio greater than 1.2. Figure <a href="#S4.F5" title="Figure 5 ‣ 4.4.2 Asymmetrical Average Pooling Model ‣ 4.4 Validation on 3-Feature Maps Networks ‣ 4 Experiment ‣ Fast and Accurate Object Detection on Asymmetrical Receptive Field" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the PR-curve of the original model and (1, 2) pooling model.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2303.08995/assets/figures/21_anchor_PR.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="295" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">PR-curves of original model and (1, 2) pooling model. The mAP@0.5 of the original model is 0.204 and the mAP@0.5 of square-anchors model is 0.224.
</span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.2.1.1" class="ltx_tr">
<th id="S4.T5.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T5.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">P</th>
<th id="S4.T5.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">R</th>
<th id="S4.T5.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP@0.5</th>
<th id="S4.T5.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP@.5:.95</th>
<th id="S4.T5.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">pre-process</th>
<th id="S4.T5.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">inference</th>
<th id="S4.T5.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">NMS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.2.2.1" class="ltx_tr">
<td id="S4.T5.2.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Original</td>
<td id="S4.T5.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.267</td>
<td id="S4.T5.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.356</td>
<td id="S4.T5.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.204</td>
<td id="S4.T5.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.118</td>
<td id="S4.T5.2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.7ms</td>
<td id="S4.T5.2.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.1ms</td>
<td id="S4.T5.2.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.3ms</td>
</tr>
<tr id="S4.T5.2.3.2" class="ltx_tr">
<td id="S4.T5.2.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">(1, 2) pooling</td>
<td id="S4.T5.2.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.293</td>
<td id="S4.T5.2.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.374</td>
<td id="S4.T5.2.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.224</td>
<td id="S4.T5.2.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.131</td>
<td id="S4.T5.2.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.4ms</td>
<td id="S4.T5.2.3.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">4.6ms</td>
<td id="S4.T5.2.3.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.9ms</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.4.2" class="ltx_text" style="font-size:90%;">Comparison of original model and (1, 2) pooling model. The first four statistics shows the performance of each model. The last three statistics show the processing speed of each image</span></figcaption>
</figure>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<p id="S4.SS4.SSS2.p2.1" class="ltx_p">The result shows that for the labels, whose width is greater than height, the feature maps which have rectangular anchors and rectangular receptive fields have better performance in terms of precision, mAP and processing speed. And for the model, which is added a (2, 1) pooling layer, its aspect ratio of receptive field becomes 1:2. And its validation set has 4,061 images containing 21,578 labels. Each label in one picture has an aspect ratio less than 1/1.2. Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4.2 Asymmetrical Average Pooling Model ‣ 4.4 Validation on 3-Feature Maps Networks ‣ 4 Experiment ‣ Fast and Accurate Object Detection on Asymmetrical Receptive Field" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the PR-curve of the original model and (2, 1) pooling model.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2303.08995/assets/figures/12_anchor_PR.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="298" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">PR-curves of original model and (2, 1) pooling model. The mAP@0.5 of the original model is 0.227 and the mAP@0.5 of (2, 1) pooling model is 0.289.
</span></figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.2.1.1" class="ltx_tr">
<th id="S4.T6.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T6.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">P</th>
<th id="S4.T6.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">R</th>
<th id="S4.T6.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP@0.5</th>
<th id="S4.T6.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP@.5:.95</th>
<th id="S4.T6.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">pre-process</th>
<th id="S4.T6.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">inference</th>
<th id="S4.T6.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">NMS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.2.2.1" class="ltx_tr">
<td id="S4.T6.2.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Original</td>
<td id="S4.T6.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.364</td>
<td id="S4.T6.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.386</td>
<td id="S4.T6.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.277</td>
<td id="S4.T6.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.171</td>
<td id="S4.T6.2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.7ms</td>
<td id="S4.T6.2.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.5ms</td>
<td id="S4.T6.2.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.8ms</td>
</tr>
<tr id="S4.T6.2.3.2" class="ltx_tr">
<td id="S4.T6.2.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">(2, 1) pooling</td>
<td id="S4.T6.2.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.417</td>
<td id="S4.T6.2.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.363</td>
<td id="S4.T6.2.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.289</td>
<td id="S4.T6.2.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.172</td>
<td id="S4.T6.2.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.7ms</td>
<td id="S4.T6.2.3.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">4.1ms</td>
<td id="S4.T6.2.3.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.9ms</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.3.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.4.2" class="ltx_text" style="font-size:90%;">Comparison of original model and (2, 1) pooling model. The first four statistics shows the performance of each model. The last three statistics show the processing speed of each image</span></figcaption>
</figure>
<div id="S4.SS4.SSS2.p3" class="ltx_para">
<p id="S4.SS4.SSS2.p3.1" class="ltx_p">For the labels, whose height is greater than width, the feature maps which have rectangular anchors and rectangular receptive fields also have better performance in terms of precision, mAP and processing speed.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Validation on 9-Feature Maps Network</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">From previous section, we find that all three new networks perform better than original network in specific validation sets. Now, we combine these three 3-feature maps networks to get a 9-feature maps network, as shown in figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 New Head ‣ 3.2 New Head ‣ 3 Proposed Methodology ‣ Fast and Accurate Object Detection on Asymmetrical Receptive Field" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. And the validation dataset contains complete 5,000 images, 36,335 labels.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2303.08995/assets/figures/9_feature_maps.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="297" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">PR-curves of original model and 9-feature maps model. The mAP@0.5 of the original model is 0.454 and the mAP@0.5 of 9-feature maps model is 0.456.
</span></figcaption>
</figure>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.2.1.1" class="ltx_tr">
<th id="S4.T7.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T7.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">P</th>
<th id="S4.T7.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">R</th>
<th id="S4.T7.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP@0.5</th>
<th id="S4.T7.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP@.5:.95</th>
<th id="S4.T7.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">pre-process</th>
<th id="S4.T7.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">inference</th>
<th id="S4.T7.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">NMS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.2.2.1" class="ltx_tr">
<td id="S4.T7.2.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Original</td>
<td id="S4.T7.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.597</td>
<td id="S4.T7.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.418</td>
<td id="S4.T7.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.454</td>
<td id="S4.T7.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.267</td>
<td id="S4.T7.2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6ms</td>
<td id="S4.T7.2.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.7ms</td>
<td id="S4.T7.2.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.7ms</td>
</tr>
<tr id="S4.T7.2.3.2" class="ltx_tr">
<td id="S4.T7.2.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">9-Feature Maps</td>
<td id="S4.T7.2.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.596</td>
<td id="S4.T7.2.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.427</td>
<td id="S4.T7.2.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.456</td>
<td id="S4.T7.2.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.269</td>
<td id="S4.T7.2.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.6ms</td>
<td id="S4.T7.2.3.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">5.0ms</td>
<td id="S4.T7.2.3.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2.6ms</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.3.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S4.T7.4.2" class="ltx_text" style="font-size:90%;">Comparison of original model and (2, 1) pooling model. The first four statistics shows the performance of each model. The last three statistics show the processing speed of each image</span></figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Figure <a href="#S4.T7" title="Table 7 ‣ 4.5 Validation on 9-Feature Maps Network ‣ 4 Experiment ‣ Fast and Accurate Object Detection on Asymmetrical Receptive Field" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that the 9-feature maps model performs better than original model in terms of recall and mAP, but the speed is slower. This is because the input of the image need to be processed in 9 feature maps instead of 3. Meanwhile, extra pooling layers also increase the processing time. And in NMS step, we do 4 times NMS.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Object detection has been a hot topic in recent years, and with the continuous efforts of researchers, object detection algorithms are performing better and better. Their accuracy and speed are also gradually able to meet the needs of various industries. For example, autonomous driving is also a booming industry, and its high requirements for object detection have promoted further improvements in the effectiveness of object detection algorithms.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">This article is based on the original YOLOv5 with some modifications. As a result, the accuracy of the algorithm is improved while ensuring the speed. Specifically, the backbone and neck parts of the new network are the same as the original one, because the facts tell us that they perform well enough. We finally chose to change the head part. The output of the model, i.e., three square feature maps, was changed to nine, and six of them are no longer square. The previous layer of these six feature maps was a newly added asymmetrical pooling layer, so that we can change the receptive fields of the feature maps without adding a new number of parameters, to expect the model to have better predictive power for multiple shapes of objects.
The final experimental results show that the new model is indeed improved. Its mAP is improved by 0.002 compared to the original model, but its inference speed is not affected too much. Compared with the original YOLOv5 model, the new model has advantages in terms of detection accuracy. In the future, firstly, we can continue to optimize the network structure and further improve the accuracy. In addition to modifying the head part, we can also try to modify other structures, for example, backbone and neck. Secondly, the prediction speed of the model has further room for improvement. Finally, the model can be applied to autonomous driving. For example, in an autonomous driving simulation system.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.

</span>
<span class="ltx_bibblock">Rich feature hierarchies for accurate object detection and semantic
segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 580–587, 2014.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Spatial pyramid pooling in deep convolutional networks for visual
recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>,
37(9), 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ross Girshick.

</span>
<span class="ltx_bibblock">Fast r-cnn.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 1440–1448, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 28, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.

</span>
<span class="ltx_bibblock">You only look once: Unified, real-time object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 779–788, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel.

</span>
<span class="ltx_bibblock">Understanding the effective receptive field in deep convolutional
neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 29, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">European conference on computer vision</span>, pages 740–755.
Springer, 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Joseph Redmon and Ali Farhadi.

</span>
<span class="ltx_bibblock">Yolo9000: better, faster, stronger.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 7263–7271, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Joseph Redmon and Ali Farhadi.

</span>
<span class="ltx_bibblock">Yolov3: An incremental improvement.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.02767</span>, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan,
and Serge Belongie.

</span>
<span class="ltx_bibblock">Feature pyramid networks for object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 2117–2125, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao.

</span>
<span class="ltx_bibblock">Yolov4: Optimal speed and accuracy of object detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.10934</span>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
David MW Powers.

</span>
<span class="ltx_bibblock">Evaluation: from precision, recall and f-measure to roc,
informedness, markedness and correlation.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.16061</span>, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kendrick Boyd, Kevin H Eng, and C David Page.

</span>
<span class="ltx_bibblock">Area under the precision-recall curve: point estimates and confidence
intervals.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Joint European conference on machine learning and knowledge
discovery in databases</span>, pages 451–466. Springer, 2013.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Path aggregation network for instance segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 8759–8768, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei
Hsieh, and I-Hau Yeh.

</span>
<span class="ltx_bibblock">Cspnet: A new backbone that can enhance learning capability of cnn.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition workshops</span>, pages 390–391, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Glenn Jocher, Ayush Chaurasia, Alex Stoken, Jirka Borovec, NanoCode012, Yonghye
Kwon, TaoXie, Kalen Michael, Jiacong Fang, imyhxy, Lorna, Colin Wong, Zeng
Yifu, Abhiram V, Diego Montes, Zhiqiang Wang, Cristi Fati, Jebastin Nadar,
Laughing, UnglvKitDe, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Max
Strobel, Mrinal Jain, Lorenzo Mammana, and xylieong.

</span>
<span class="ltx_bibblock">ultralytics/yolov5: v6.2 - YOLOv5 Classification Models, Apple M1,
Reproducibility, ClearML and Deci.ai integrations, August 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.08994" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.08995" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.08995">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.08995" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.08996" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 19:30:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
