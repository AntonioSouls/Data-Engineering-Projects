<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.20234] HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds</title><meta property="og:description" content="3D object detection in point clouds is important for autonomous driving systems. A primary challenge in 3D object detection stems from the sparse distribution of points within the 3D scene. Existing high-performance me…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.20234">

<!--Generated on Tue Feb 27 21:41:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gang Zhang<sup id="id11.11.id1" class="ltx_sup">1</sup>, Junnan Chen<sup id="id12.12.id2" class="ltx_sup">2</sup>, Guohuan Gao<sup id="id13.13.id3" class="ltx_sup">3</sup>, Jianmin Li<sup id="id14.14.id4" class="ltx_sup">1</sup>, Xiaolin Hu<sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">1,4,5</span></sup> 
<br class="ltx_break"><sup id="id16.16.id6" class="ltx_sup">1</sup>Department of Computer Science and Technology, Institute for AI, 
<br class="ltx_break">BNRist, THU-Bosch JCML Center, Tsinghua University
<br class="ltx_break"><sup id="id17.17.id7" class="ltx_sup">2</sup>Huazhong University of Science and Technology, <sup id="id18.18.id8" class="ltx_sup">3</sup>Beijing Institute of Technology
<br class="ltx_break"><sup id="id19.19.id9" class="ltx_sup">4</sup>Tsinghua Laboratory of Brain and Intelligence (THBI), 
<br class="ltx_break">IDG/McGovern Institute for Brain Research, Tsinghua University
<br class="ltx_break"><sup id="id20.20.id10" class="ltx_sup">5</sup> Chinese Institute for Brain Research (CIBR), Beijing 100010, China 
<br class="ltx_break"><span id="id21.21.id11" class="ltx_text ltx_font_typewriter">zhang-g19@mails.tsinghua.edu.cn,chen_jn@hust.edu.cn,gaoguohuan@bit.edu.cn
<br class="ltx_break">lijianmin@mail.tsinghua.edu.cn,xlhu@tsinghua.edu.cn</span> 
<br class="ltx_break">Code: <a href="github.com/zhanggang001/HEDNet" title="" class="ltx_ref ltx_href" style="color:#FF0000;">https://github.com/zhanggang001/HEDNet</a>
</span><span class="ltx_author_notes">Corresponding Author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id22.id1" class="ltx_p">3D object detection in point clouds is important for autonomous driving systems. A primary challenge in 3D object detection stems from the sparse distribution of points within the 3D scene. Existing high-performance methods typically employ 3D sparse convolutional neural networks with small kernels to extract features. To reduce computational costs, these methods resort to submanifold sparse convolutions, which prevent the information exchange among spatially disconnected features. Some recent approaches have attempted to address this problem by introducing large-kernel convolutions or self-attention mechanisms, but they either achieve limited accuracy improvements or incur excessive computational costs. We propose HEDNet, a hierarchical encoder-decoder network for 3D object detection, which leverages encoder-decoder blocks to capture long-range dependencies among features in the spatial space, particularly for large and distant objects. We conducted extensive experiments on the Waymo Open and nuScenes datasets. HEDNet achieved superior detection accuracy on both datasets than previous state-of-the-art methods with competitive efficiency. The code has been released.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Learning effective representations from sparse input data is a key challenge for 3D object detection in point clouds. Existing point-based methods  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> and range-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> either suffer from high computational costs or exhibit inferior detection accuracy. Currently, voxel-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> dominate high-performance 3D object detection.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The voxel-based methods partition the unstructured point clouds into regular voxels and utilize sparse conventional neural network (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> or transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> as backbones for feature extraction. Most existing sparse CNNs are primarily built by stacking submanifold sparse residual (SSR) blocks, each consisting of two submanifold sparse convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> with small kernels. However, submanifold sparse convolutions maintain the same sparsity between input and output features, and therefore hinder the exchange of information among spatially disconnected features. Consequently, models employing SSR blocks face challenges in effectively capturing long-range dependencies among features. One potential solution is to replace the submanifold sparse convolutions in SSR block with regular sparse convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>. However, this leads to a significant decrease in feature sparsity as the network deepens, resulting in substantial computational costs. Recent research has investigated the utilization of large-kernel sparse CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> and transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> to capture long-range dependencies among features. However, these approaches have either demonstrated limited improvements in detection accuracy or come with significant computational costs. Thus, the question remains: <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">is there an efficient method that enables sparse CNNs to effectively capture long-range dependencies among features?</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Revisiting backbone designs in various dense prediction tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, we observe that the encoder-decoder structure has proven effective in capturing long-range dependencies among features. These methods typically use a high-to-low resolution backbone as an encoder to extract multi-scale features and design different decoders to recover high-resolution features that can model long-range relationships. For instance, PSPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> incorporates a pyramid pooling module to capture both local and global contextual information by pooling features at multiple scales. SWFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> integrates a top-down pathway into its transformer backbone to capture cross-window correlations. However, the utilization of the encoder-decoder structure in designing sparse convolutional backbones for 3D object detection has not yet been explored, to the best of our knowledge.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we propose a sparse encoder-decoder (SED) block to overcome the limitations of the SSR block. The encoder extracts multi-scale features through feature down-sampling, facilitating information exchange among spatially disconnected regions. Meanwhile, the decoder incorporates multi-scale feature fusion to recover the lost details. A hallmark of the SED block is its ability to capture long-range dependencies while preserving the same sparsity between input and output features. Since current leading 3D detectors typically rely on object centers for detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, we further adapt the 3D SED block into a 2D dense encoder-decoder (DED) block, which expands the extracted sparse features towards object centers. Leveraging the SED block and DED block, we introduce a hierarchical encoder-decoder network named HEDNet for 3D object detection in point clouds. HEDNet can learn powerful representations for the detection of large and distant objects.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Extensive experiments were conducted on the challenging Waymo Open <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> and nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> datasets to demonstrate the effectiveness of the proposed HEDNet on 3D object detection. HEDNet achieved impressive performance, with a 75.0% L2 mAPH on the Waymo Open <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">test</span> set and a 72.0% NDS on the nuScenes <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">test</span> set, outperforming prior methods that utilize large-kernel CNNs or transformers as backbones while exhibiting higher efficiency. For instance, HEDNet was 50% faster than DSVT, the previous state-of-the-art transformer-based method, with 1.3% L2 mAPH gains.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>3D object detection in point clouds</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">For 3D object detection in point clouds, methods can be categorized into three groups: point-based, range-based, and voxel-based. Point-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> utilize the PointNet series <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> to directly extract geometric features from raw point clouds and make predictions. However, these methods require computationally intensive point sampling and neighbor search procedures. Range-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> convert point clouds into pseudo images, thus benefiting from the well-established designs of 2D object detectors. While computationally efficient, these methods often exhibit lower accuracy. Voxel-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> are currently the leading methods for high-performance 3D object detection. Most voxel-based methods employ sparse CNNs that consist of submanifold and regular sparse convolutions with small kernels to extract features. Regular sparse convolutions can capture distant contextual information but are computationally expensive. On the other hand, submanifold sparse convolutions prioritize efficiency but sacrifice the model’s ability to capture long-range dependencies.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Capturing long-range dependencies for 3D object detection</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">To capture long-range dependencies for 3D object detection, recent research has explored solutions such as large-kernel sparse convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> and self-attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>. However, directly applying plain large-kernel CNNs for 3D representation learning can lead to problems such as overfitting and reduced efficiency. Weight-sharing strategies have been proposed to mitigate overfitting, like LargeKernel3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> and Link <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, however, they still suffer from low efficiency. Other methods, such as SST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> and DSVT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, utilize transformers as replacements for sparse CNNs. SST employs a single-stride sparse transformer to preserve high-resolution features without using down-sampling operators that may cause a loss of detail. Similarly, DSVT employs a single-stride sparse transformer and performs window-based self-attention sequentially along the X-axis and Y-axis. While both large-kernel CNNs and transformers aim to capture long-range dependencies, they either achieve comparable performance or exhibit lower efficiency compared with sparse CNNs. In contrast, our proposed HEDNet effectively captures long-range dependencies with the help of encoder-decoder blocks while achieving competitive inference speed compared with existing methods.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Encoder-decoder networks for dense prediction</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The encoder-decoder structure has been extensively investigated in various dense prediction tasks. For example, the FPN-series <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> incorporates lightweight fusion modules as decoders to integrate multi-scale features extracted from image classification backbones. DeeplabV3+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> employs an atrous spatial pyramid pooling module to combine low-level features with semantically rich high-level features. SWFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> introduces a top-down pathway into its transformer backbone to capture cross-window correlations. However, there is limited exploration of the encoder-decoder structure in the design of sparse CNNs. Most voxel-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> rely on high-to-low resolution sparse CNNs to extract single-scale high-level features. Part-A2-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> adopts the UNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> to extract features, but it performs detection using the encoder of UNet while utilizing the decoder for the auxiliary segmentation and part prediction tasks. In this study, we propose HEDNet, which primarily consists of encoder-decoder blocks to effectively capture long-range dependencies.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2310.20234/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison among SSR block (a), RSR block (b), and our SED block (c). The ‘Skip conn.’ denotes the skip connection, and the orange dashed lines represent the convolution kernel space. Valid features have non-zero values. Expanded and empty features have zero values. In (b), convolution is applied to both valid and expanded features, <span id="S3.F1.5.1" class="ltx_text ltx_font_italic">i.e.,</span>the convolution kernel center traverses the regions covered by these features. The red dashed square highlights the regions from which the output feature marked by a star can receive information. In (c), we adopt a 3<math id="S3.F1.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.F1.2.m1.1b"><mo id="S3.F1.2.m1.1.1" xref="S3.F1.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.F1.2.m1.1c"><times id="S3.F1.2.m1.1.1.cmml" xref="S3.F1.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.2.m1.1d">\times</annotation></semantics></math>3 RS convolution <span id="S3.F1.6.2" class="ltx_text ltx_font_italic">with a stride of 3</span> for feature down-sampling (Down) as an example. UP denotes feature up-sampling.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Background</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The sparse CNNs adopted by most voxel-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> are primarily built by stacking SSR blocks, each consisting of two submanifold sparse (SS) convolutions  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>. In addition, they usually insert regular sparse (RS) convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> into the stacked SSR blocks to reduce the resolution of feature maps progressively (similar to ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>).</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SS convolution and SSR block.</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">We present the structure of a single SSR block in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Method ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (a). Two SS convolutions are sequentially applied to the input feature map, with skip connections incorporated between the input and output feature maps of the SSR block. The <span id="S3.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">sparsity of feature map</span> is defined as the ratio of the regions that are <span id="S3.SS1.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_bold ltx_font_italic">not</span> occupied by valid (nonzero) features to the total area of the feature map. SS convolution only operates on valid features, allowing the output feature map of the SSR block to maintain the same sparsity as the input feature map. However, this design hinders the exchange of information among spatially disconnected features. For instance, in the top feature map, the output feature marked by a star cannot receive information from the other three feature points outside the red dashed square in the bottom feature map (marked by the red triangles). This poses a challenge for the model in capturing long-range dependencies.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">RS convolution and RSR block.</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">One possible solution to problem is to replace the SS convolutions in the SSR block with RS convolutions. We call this modified structure regular sparse residual (RSR) block and illustrate its structure in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Method ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (b). RS convolution operates on both valid and expanded features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>. Expanded features correspond to the features that fall within the neighborhood of the valid features. Taking a 2D RS convolution with a kernel size of 3<math id="S3.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mo id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><times id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">\times</annotation></semantics></math>3 as an example, the neighborhood of a certain valid feature consists of the eight positions around it. This design leads to an output feature map with a lower sparsity compared with the input feature map. Stacking RS convolutions reduces the feature sparsity dramatically, which in turn leads to a notable decrease in model efficiency compared with using SS convolutions. This is why existing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> typically limit the usage of RS convolution to feature down-sampling layers.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2310.20234/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture of the SED block (a) and DED block (b). As an example, we illustrate blocks of three scales. Both designs share the same structure. F<sub id="S3.F2.16.1" class="ltx_sub">1</sub>/F<sub id="S3.F2.17.2" class="ltx_sub">2</sub>/F<sub id="S3.F2.18.3" class="ltx_sub">3</sub>/F<sub id="S3.F2.19.4" class="ltx_sub">4</sub>/F<sub id="S3.F2.20.5" class="ltx_sub">5</sub> are the names of the corresponding feature maps. The number in parentheses indicates the resolution ratio of the corresponding feature map relative to the block input. The SED block is capable of processing both 2D and 3D features, depending on whether 2D or 3D sparse convolutions are used.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>SED and DED blocks</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SED block.</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">SED block is designed to overcome the limitations of SSR block. The fundamental idea behind this design is to reduce the spatial distance between distant features through feature down-sampling and recover the lost details through multi-scale feature fusion.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p2.1" class="ltx_p">We illustrate a two-scale SED block in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Method ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (c). After feature down-sampling, the spatially disconnected valid features in the bottom feature map are integrated into the adjacent valid features in the middle feature map. An SSR block is subsequently applied to the middle feature map to promote interaction among valid features. Finally, the middle feature map is up-sampled to match the resolution of the input feature map. <span id="S3.SS2.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">Note that the feature up-sampling layer (UP) only up-samples features to the regions covered by the valid features in the input feature map.</span> As a result, the proposed SED block can maintain the same sparsity between input and output feature maps. This characteristic prevents the introduction of excessive computational costs when stacking multiple SED blocks.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px1.p3.1" class="ltx_p">The architecture of a three-scale SED block is presented in Figure <a href="#S3.F2" title="Figure 2 ‣ RS convolution and RSR block. ‣ 3.1 Background ‣ 3 Method ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a). The SED block adopts an asymmetric encoder-decoder structure similar to UNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>, with the encoder responsible for extracting multi-scale features and the decoder sequentially fusing the extracted multi-scale features with the help of skip connections. Given the input feature map <math id="S3.SS2.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="\mathrm{X}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p3.1.m1.1a"><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p3.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p3.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p3.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.1.m1.1.1">X</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p3.1.m1.1c">\mathrm{X}</annotation></semantics></math>, the function of the SED block can be formulated as follows:</p>
<table id="A3.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle\mathrm{F}_{1}" display="inline"><semantics id="S3.E1.m1.1a"><msub id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">F</mi><mn id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">F</ci><cn type="integer" id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle\mathrm{F}_{1}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m2.1" class="ltx_Math" alttext="\displaystyle=\textrm{SSR}^{m}(\mathrm{X})" display="inline"><semantics id="S3.E1.m2.1a"><mrow id="S3.E1.m2.1.2" xref="S3.E1.m2.1.2.cmml"><mi id="S3.E1.m2.1.2.2" xref="S3.E1.m2.1.2.2.cmml"></mi><mo id="S3.E1.m2.1.2.1" xref="S3.E1.m2.1.2.1.cmml">=</mo><mrow id="S3.E1.m2.1.2.3" xref="S3.E1.m2.1.2.3.cmml"><msup id="S3.E1.m2.1.2.3.2" xref="S3.E1.m2.1.2.3.2.cmml"><mtext id="S3.E1.m2.1.2.3.2.2" xref="S3.E1.m2.1.2.3.2.2a.cmml">SSR</mtext><mi id="S3.E1.m2.1.2.3.2.3" xref="S3.E1.m2.1.2.3.2.3.cmml">m</mi></msup><mo lspace="0em" rspace="0em" id="S3.E1.m2.1.2.3.1" xref="S3.E1.m2.1.2.3.1.cmml">​</mo><mrow id="S3.E1.m2.1.2.3.3.2" xref="S3.E1.m2.1.2.3.cmml"><mo stretchy="false" id="S3.E1.m2.1.2.3.3.2.1" xref="S3.E1.m2.1.2.3.cmml">(</mo><mi mathvariant="normal" id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml">X</mi><mo stretchy="false" id="S3.E1.m2.1.2.3.3.2.2" xref="S3.E1.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.1b"><apply id="S3.E1.m2.1.2.cmml" xref="S3.E1.m2.1.2"><eq id="S3.E1.m2.1.2.1.cmml" xref="S3.E1.m2.1.2.1"></eq><csymbol cd="latexml" id="S3.E1.m2.1.2.2.cmml" xref="S3.E1.m2.1.2.2">absent</csymbol><apply id="S3.E1.m2.1.2.3.cmml" xref="S3.E1.m2.1.2.3"><times id="S3.E1.m2.1.2.3.1.cmml" xref="S3.E1.m2.1.2.3.1"></times><apply id="S3.E1.m2.1.2.3.2.cmml" xref="S3.E1.m2.1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m2.1.2.3.2.1.cmml" xref="S3.E1.m2.1.2.3.2">superscript</csymbol><ci id="S3.E1.m2.1.2.3.2.2a.cmml" xref="S3.E1.m2.1.2.3.2.2"><mtext id="S3.E1.m2.1.2.3.2.2.cmml" xref="S3.E1.m2.1.2.3.2.2">SSR</mtext></ci><ci id="S3.E1.m2.1.2.3.2.3.cmml" xref="S3.E1.m2.1.2.3.2.3">𝑚</ci></apply><ci id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1">X</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.1c">\displaystyle=\textrm{SSR}^{m}(\mathrm{X})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle\mathrm{F}_{2}" display="inline"><semantics id="S3.E2.m1.1a"><msub id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi mathvariant="normal" id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">F</mi><mn id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">F</ci><cn type="integer" id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle\mathrm{F}_{2}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m2.1" class="ltx_Math" alttext="\displaystyle=\textrm{SSR}^{m}(\textrm{Down}_{1}(\mathrm{F}_{1}))" display="inline"><semantics id="S3.E2.m2.1a"><mrow id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml"><mi id="S3.E2.m2.1.1.3" xref="S3.E2.m2.1.1.3.cmml"></mi><mo id="S3.E2.m2.1.1.2" xref="S3.E2.m2.1.1.2.cmml">=</mo><mrow id="S3.E2.m2.1.1.1" xref="S3.E2.m2.1.1.1.cmml"><msup id="S3.E2.m2.1.1.1.3" xref="S3.E2.m2.1.1.1.3.cmml"><mtext id="S3.E2.m2.1.1.1.3.2" xref="S3.E2.m2.1.1.1.3.2a.cmml">SSR</mtext><mi id="S3.E2.m2.1.1.1.3.3" xref="S3.E2.m2.1.1.1.3.3.cmml">m</mi></msup><mo lspace="0em" rspace="0em" id="S3.E2.m2.1.1.1.2" xref="S3.E2.m2.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m2.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m2.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m2.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.cmml"><msub id="S3.E2.m2.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.3.cmml"><mtext id="S3.E2.m2.1.1.1.1.1.1.3.2" xref="S3.E2.m2.1.1.1.1.1.1.3.2a.cmml">Down</mtext><mn id="S3.E2.m2.1.1.1.1.1.1.3.3" xref="S3.E2.m2.1.1.1.1.1.1.3.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E2.m2.1.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m2.1.1.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m2.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E2.m2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.2.cmml">F</mi><mn id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S3.E2.m2.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m2.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b"><apply id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1"><eq id="S3.E2.m2.1.1.2.cmml" xref="S3.E2.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E2.m2.1.1.3.cmml" xref="S3.E2.m2.1.1.3">absent</csymbol><apply id="S3.E2.m2.1.1.1.cmml" xref="S3.E2.m2.1.1.1"><times id="S3.E2.m2.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.2"></times><apply id="S3.E2.m2.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.3.1.cmml" xref="S3.E2.m2.1.1.1.3">superscript</csymbol><ci id="S3.E2.m2.1.1.1.3.2a.cmml" xref="S3.E2.m2.1.1.1.3.2"><mtext id="S3.E2.m2.1.1.1.3.2.cmml" xref="S3.E2.m2.1.1.1.3.2">SSR</mtext></ci><ci id="S3.E2.m2.1.1.1.3.3.cmml" xref="S3.E2.m2.1.1.1.3.3">𝑚</ci></apply><apply id="S3.E2.m2.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1"><times id="S3.E2.m2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2"></times><apply id="S3.E2.m2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m2.1.1.1.1.1.1.3.2a.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.2"><mtext id="S3.E2.m2.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.2">Down</mtext></ci><cn type="integer" id="S3.E2.m2.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.3">1</cn></apply><apply id="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.2">F</ci><cn type="integer" id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle=\textrm{SSR}^{m}(\textrm{Down}_{1}(\mathrm{F}_{1}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle\mathrm{F}_{3}" display="inline"><semantics id="S3.E3.m1.1a"><msub id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">F</mi><mn id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2">F</ci><cn type="integer" id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle\mathrm{F}_{3}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m2.1" class="ltx_Math" alttext="\displaystyle=\textrm{SSR}^{m}(\textrm{Down}_{2}(\mathrm{F}_{2}))" display="inline"><semantics id="S3.E3.m2.1a"><mrow id="S3.E3.m2.1.1" xref="S3.E3.m2.1.1.cmml"><mi id="S3.E3.m2.1.1.3" xref="S3.E3.m2.1.1.3.cmml"></mi><mo id="S3.E3.m2.1.1.2" xref="S3.E3.m2.1.1.2.cmml">=</mo><mrow id="S3.E3.m2.1.1.1" xref="S3.E3.m2.1.1.1.cmml"><msup id="S3.E3.m2.1.1.1.3" xref="S3.E3.m2.1.1.1.3.cmml"><mtext id="S3.E3.m2.1.1.1.3.2" xref="S3.E3.m2.1.1.1.3.2a.cmml">SSR</mtext><mi id="S3.E3.m2.1.1.1.3.3" xref="S3.E3.m2.1.1.1.3.3.cmml">m</mi></msup><mo lspace="0em" rspace="0em" id="S3.E3.m2.1.1.1.2" xref="S3.E3.m2.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m2.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m2.1.1.1.1.1.2" xref="S3.E3.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m2.1.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.cmml"><msub id="S3.E3.m2.1.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.3.cmml"><mtext id="S3.E3.m2.1.1.1.1.1.1.3.2" xref="S3.E3.m2.1.1.1.1.1.1.3.2a.cmml">Down</mtext><mn id="S3.E3.m2.1.1.1.1.1.1.3.3" xref="S3.E3.m2.1.1.1.1.1.1.3.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E3.m2.1.1.1.1.1.1.2" xref="S3.E3.m2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m2.1.1.1.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m2.1.1.1.1.1.1.1.1.2" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m2.1.1.1.1.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E3.m2.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.2.cmml">F</mi><mn id="S3.E3.m2.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.3.cmml">2</mn></msub><mo stretchy="false" id="S3.E3.m2.1.1.1.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m2.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.1b"><apply id="S3.E3.m2.1.1.cmml" xref="S3.E3.m2.1.1"><eq id="S3.E3.m2.1.1.2.cmml" xref="S3.E3.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E3.m2.1.1.3.cmml" xref="S3.E3.m2.1.1.3">absent</csymbol><apply id="S3.E3.m2.1.1.1.cmml" xref="S3.E3.m2.1.1.1"><times id="S3.E3.m2.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.2"></times><apply id="S3.E3.m2.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.3.1.cmml" xref="S3.E3.m2.1.1.1.3">superscript</csymbol><ci id="S3.E3.m2.1.1.1.3.2a.cmml" xref="S3.E3.m2.1.1.1.3.2"><mtext id="S3.E3.m2.1.1.1.3.2.cmml" xref="S3.E3.m2.1.1.1.3.2">SSR</mtext></ci><ci id="S3.E3.m2.1.1.1.3.3.cmml" xref="S3.E3.m2.1.1.1.3.3">𝑚</ci></apply><apply id="S3.E3.m2.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1"><times id="S3.E3.m2.1.1.1.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2"></times><apply id="S3.E3.m2.1.1.1.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m2.1.1.1.1.1.1.3.2a.cmml" xref="S3.E3.m2.1.1.1.1.1.1.3.2"><mtext id="S3.E3.m2.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.3.2">Down</mtext></ci><cn type="integer" id="S3.E3.m2.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.3.3">2</cn></apply><apply id="S3.E3.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.2">F</ci><cn type="integer" id="S3.E3.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.1c">\displaystyle=\textrm{SSR}^{m}(\textrm{Down}_{2}(\mathrm{F}_{2}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle\mathrm{F}_{4}" display="inline"><semantics id="S3.E4.m1.1a"><msub id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi mathvariant="normal" id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">F</mi><mn id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">F</ci><cn type="integer" id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle\mathrm{F}_{4}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2.1" class="ltx_Math" alttext="\displaystyle=\textrm{UP}_{2}(\mathrm{F}_{3})+\mathrm{F}_{2}" display="inline"><semantics id="S3.E4.m2.1a"><mrow id="S3.E4.m2.1.1" xref="S3.E4.m2.1.1.cmml"><mi id="S3.E4.m2.1.1.3" xref="S3.E4.m2.1.1.3.cmml"></mi><mo id="S3.E4.m2.1.1.2" xref="S3.E4.m2.1.1.2.cmml">=</mo><mrow id="S3.E4.m2.1.1.1" xref="S3.E4.m2.1.1.1.cmml"><mrow id="S3.E4.m2.1.1.1.1" xref="S3.E4.m2.1.1.1.1.cmml"><msub id="S3.E4.m2.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.3.cmml"><mtext id="S3.E4.m2.1.1.1.1.3.2" xref="S3.E4.m2.1.1.1.1.3.2a.cmml">UP</mtext><mn id="S3.E4.m2.1.1.1.1.3.3" xref="S3.E4.m2.1.1.1.1.3.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m2.1.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m2.1.1.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E4.m2.1.1.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E4.m2.1.1.1.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.1.1.1.2.cmml">F</mi><mn id="S3.E4.m2.1.1.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.1.1.3.cmml">3</mn></msub><mo stretchy="false" id="S3.E4.m2.1.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m2.1.1.1.2" xref="S3.E4.m2.1.1.1.2.cmml">+</mo><msub id="S3.E4.m2.1.1.1.3" xref="S3.E4.m2.1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E4.m2.1.1.1.3.2" xref="S3.E4.m2.1.1.1.3.2.cmml">F</mi><mn id="S3.E4.m2.1.1.1.3.3" xref="S3.E4.m2.1.1.1.3.3.cmml">2</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.1b"><apply id="S3.E4.m2.1.1.cmml" xref="S3.E4.m2.1.1"><eq id="S3.E4.m2.1.1.2.cmml" xref="S3.E4.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E4.m2.1.1.3.cmml" xref="S3.E4.m2.1.1.3">absent</csymbol><apply id="S3.E4.m2.1.1.1.cmml" xref="S3.E4.m2.1.1.1"><plus id="S3.E4.m2.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.2"></plus><apply id="S3.E4.m2.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1"><times id="S3.E4.m2.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.2"></times><apply id="S3.E4.m2.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.3.1.cmml" xref="S3.E4.m2.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.3.2a.cmml" xref="S3.E4.m2.1.1.1.1.3.2"><mtext id="S3.E4.m2.1.1.1.1.3.2.cmml" xref="S3.E4.m2.1.1.1.1.3.2">UP</mtext></ci><cn type="integer" id="S3.E4.m2.1.1.1.1.3.3.cmml" xref="S3.E4.m2.1.1.1.1.3.3">2</cn></apply><apply id="S3.E4.m2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.2">F</ci><cn type="integer" id="S3.E4.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.3">3</cn></apply></apply><apply id="S3.E4.m2.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.3.1.cmml" xref="S3.E4.m2.1.1.1.3">subscript</csymbol><ci id="S3.E4.m2.1.1.1.3.2.cmml" xref="S3.E4.m2.1.1.1.3.2">F</ci><cn type="integer" id="S3.E4.m2.1.1.1.3.3.cmml" xref="S3.E4.m2.1.1.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.1c">\displaystyle=\textrm{UP}_{2}(\mathrm{F}_{3})+\mathrm{F}_{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\displaystyle\mathrm{F}_{5}" display="inline"><semantics id="S3.E5.m1.1a"><msub id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mi mathvariant="normal" id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">F</mi><mn id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2">F</ci><cn type="integer" id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle\mathrm{F}_{5}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E5.m2.1" class="ltx_Math" alttext="\displaystyle=\textrm{UP}_{1}(\mathrm{F}_{4})+\mathrm{F}_{1}" display="inline"><semantics id="S3.E5.m2.1a"><mrow id="S3.E5.m2.1.1" xref="S3.E5.m2.1.1.cmml"><mi id="S3.E5.m2.1.1.3" xref="S3.E5.m2.1.1.3.cmml"></mi><mo id="S3.E5.m2.1.1.2" xref="S3.E5.m2.1.1.2.cmml">=</mo><mrow id="S3.E5.m2.1.1.1" xref="S3.E5.m2.1.1.1.cmml"><mrow id="S3.E5.m2.1.1.1.1" xref="S3.E5.m2.1.1.1.1.cmml"><msub id="S3.E5.m2.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.3.cmml"><mtext id="S3.E5.m2.1.1.1.1.3.2" xref="S3.E5.m2.1.1.1.1.3.2a.cmml">UP</mtext><mn id="S3.E5.m2.1.1.1.1.3.3" xref="S3.E5.m2.1.1.1.1.3.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E5.m2.1.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m2.1.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E5.m2.1.1.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E5.m2.1.1.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.1.1.2.cmml">F</mi><mn id="S3.E5.m2.1.1.1.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.1.1.1.3.cmml">4</mn></msub><mo stretchy="false" id="S3.E5.m2.1.1.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m2.1.1.1.2" xref="S3.E5.m2.1.1.1.2.cmml">+</mo><msub id="S3.E5.m2.1.1.1.3" xref="S3.E5.m2.1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E5.m2.1.1.1.3.2" xref="S3.E5.m2.1.1.1.3.2.cmml">F</mi><mn id="S3.E5.m2.1.1.1.3.3" xref="S3.E5.m2.1.1.1.3.3.cmml">1</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.1b"><apply id="S3.E5.m2.1.1.cmml" xref="S3.E5.m2.1.1"><eq id="S3.E5.m2.1.1.2.cmml" xref="S3.E5.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E5.m2.1.1.3.cmml" xref="S3.E5.m2.1.1.3">absent</csymbol><apply id="S3.E5.m2.1.1.1.cmml" xref="S3.E5.m2.1.1.1"><plus id="S3.E5.m2.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.2"></plus><apply id="S3.E5.m2.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1"><times id="S3.E5.m2.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.2"></times><apply id="S3.E5.m2.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.3.1.cmml" xref="S3.E5.m2.1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m2.1.1.1.1.3.2a.cmml" xref="S3.E5.m2.1.1.1.1.3.2"><mtext id="S3.E5.m2.1.1.1.1.3.2.cmml" xref="S3.E5.m2.1.1.1.1.3.2">UP</mtext></ci><cn type="integer" id="S3.E5.m2.1.1.1.1.3.3.cmml" xref="S3.E5.m2.1.1.1.1.3.3">1</cn></apply><apply id="S3.E5.m2.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.2">F</ci><cn type="integer" id="S3.E5.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.3">4</cn></apply></apply><apply id="S3.E5.m2.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.3.1.cmml" xref="S3.E5.m2.1.1.1.3">subscript</csymbol><ci id="S3.E5.m2.1.1.1.3.2.cmml" xref="S3.E5.m2.1.1.1.3.2">F</ci><cn type="integer" id="S3.E5.m2.1.1.1.3.3.cmml" xref="S3.E5.m2.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.1c">\displaystyle=\textrm{UP}_{1}(\mathrm{F}_{4})+\mathrm{F}_{1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px1.p3.10" class="ltx_p">where <math id="S3.SS2.SSS0.Px1.p3.2.m1.1" class="ltx_Math" alttext="\mathrm{F}_{5}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p3.2.m1.1a"><msub id="S3.SS2.SSS0.Px1.p3.2.m1.1.1" xref="S3.SS2.SSS0.Px1.p3.2.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p3.2.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p3.2.m1.1.1.2.cmml">F</mi><mn id="S3.SS2.SSS0.Px1.p3.2.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p3.2.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p3.2.m1.1b"><apply id="S3.SS2.SSS0.Px1.p3.2.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p3.2.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.2.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p3.2.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p3.2.m1.1.1.2">F</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p3.2.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p3.2.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p3.2.m1.1c">\mathrm{F}_{5}</annotation></semantics></math> denotes the output feature map with the same resolution as the input <math id="S3.SS2.SSS0.Px1.p3.3.m2.1" class="ltx_Math" alttext="\mathrm{X}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p3.3.m2.1a"><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p3.3.m2.1.1" xref="S3.SS2.SSS0.Px1.p3.3.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p3.3.m2.1b"><ci id="S3.SS2.SSS0.Px1.p3.3.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.3.m2.1.1">X</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p3.3.m2.1c">\mathrm{X}</annotation></semantics></math>. The resolution ratios of the intermediate feature maps <math id="S3.SS2.SSS0.Px1.p3.4.m3.1" class="ltx_Math" alttext="\mathrm{F}_{1}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p3.4.m3.1a"><msub id="S3.SS2.SSS0.Px1.p3.4.m3.1.1" xref="S3.SS2.SSS0.Px1.p3.4.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p3.4.m3.1.1.2" xref="S3.SS2.SSS0.Px1.p3.4.m3.1.1.2.cmml">F</mi><mn id="S3.SS2.SSS0.Px1.p3.4.m3.1.1.3" xref="S3.SS2.SSS0.Px1.p3.4.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p3.4.m3.1b"><apply id="S3.SS2.SSS0.Px1.p3.4.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p3.4.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.4.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p3.4.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p3.4.m3.1.1.2">F</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p3.4.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p3.4.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p3.4.m3.1c">\mathrm{F}_{1}</annotation></semantics></math>, <math id="S3.SS2.SSS0.Px1.p3.5.m4.1" class="ltx_Math" alttext="\mathrm{F}_{2}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p3.5.m4.1a"><msub id="S3.SS2.SSS0.Px1.p3.5.m4.1.1" xref="S3.SS2.SSS0.Px1.p3.5.m4.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p3.5.m4.1.1.2" xref="S3.SS2.SSS0.Px1.p3.5.m4.1.1.2.cmml">F</mi><mn id="S3.SS2.SSS0.Px1.p3.5.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p3.5.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p3.5.m4.1b"><apply id="S3.SS2.SSS0.Px1.p3.5.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.5.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p3.5.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.5.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p3.5.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p3.5.m4.1.1.2">F</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p3.5.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p3.5.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p3.5.m4.1c">\mathrm{F}_{2}</annotation></semantics></math>, <math id="S3.SS2.SSS0.Px1.p3.6.m5.1" class="ltx_Math" alttext="\mathrm{F}_{3}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p3.6.m5.1a"><msub id="S3.SS2.SSS0.Px1.p3.6.m5.1.1" xref="S3.SS2.SSS0.Px1.p3.6.m5.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p3.6.m5.1.1.2" xref="S3.SS2.SSS0.Px1.p3.6.m5.1.1.2.cmml">F</mi><mn id="S3.SS2.SSS0.Px1.p3.6.m5.1.1.3" xref="S3.SS2.SSS0.Px1.p3.6.m5.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p3.6.m5.1b"><apply id="S3.SS2.SSS0.Px1.p3.6.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.6.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p3.6.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.6.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p3.6.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p3.6.m5.1.1.2">F</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p3.6.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p3.6.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p3.6.m5.1c">\mathrm{F}_{3}</annotation></semantics></math>, and <math id="S3.SS2.SSS0.Px1.p3.7.m6.1" class="ltx_Math" alttext="\mathrm{F}_{4}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p3.7.m6.1a"><msub id="S3.SS2.SSS0.Px1.p3.7.m6.1.1" xref="S3.SS2.SSS0.Px1.p3.7.m6.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p3.7.m6.1.1.2" xref="S3.SS2.SSS0.Px1.p3.7.m6.1.1.2.cmml">F</mi><mn id="S3.SS2.SSS0.Px1.p3.7.m6.1.1.3" xref="S3.SS2.SSS0.Px1.p3.7.m6.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p3.7.m6.1b"><apply id="S3.SS2.SSS0.Px1.p3.7.m6.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.7.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p3.7.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.7.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p3.7.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p3.7.m6.1.1.2">F</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p3.7.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p3.7.m6.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p3.7.m6.1c">\mathrm{F}_{4}</annotation></semantics></math> relative to the input <math id="S3.SS2.SSS0.Px1.p3.8.m7.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p3.8.m7.1a"><mi id="S3.SS2.SSS0.Px1.p3.8.m7.1.1" xref="S3.SS2.SSS0.Px1.p3.8.m7.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p3.8.m7.1b"><ci id="S3.SS2.SSS0.Px1.p3.8.m7.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.8.m7.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p3.8.m7.1c">\mathbf{X}</annotation></semantics></math> are 1, 1/2, 1/4, and 1/2, respectively. SSR<sup id="S3.SS2.SSS0.Px1.p3.10.1" class="ltx_sup"><span id="S3.SS2.SSS0.Px1.p3.10.1.1" class="ltx_text ltx_font_italic">m</span></sup> indicates <math id="S3.SS2.SSS0.Px1.p3.10.m9.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS2.SSS0.Px1.p3.10.m9.1a"><mi id="S3.SS2.SSS0.Px1.p3.10.m9.1.1" xref="S3.SS2.SSS0.Px1.p3.10.m9.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p3.10.m9.1b"><ci id="S3.SS2.SSS0.Px1.p3.10.m9.1.1.cmml" xref="S3.SS2.SSS0.Px1.p3.10.m9.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p3.10.m9.1c">m</annotation></semantics></math> consecutive SSR blocks. We adopt RS convolution as the feature down-sampling layer (Down) and sparse inverse convolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> as the feature up-sampling layer (UP). With an encoder-decoder structure, the SED block facilitates information exchange among spatially disconnected features, thereby enabling the model to capture long-range dependencies.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2310.20234/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Architecture of the proposed HEDNet. Given the raw point clouds, we first perform voxelization to generate voxels by the VFE module, then employ the 3D sparse backbone and the 2D dense backbone to extract features for the detection head. The number in the bracket denotes the resolution ratio of the corresponding feature map relative to the input. <span id="S3.F3.6.3" class="ltx_text ltx_font_italic">The RS convolutions for feature down-sampling that follow the feature maps F<sub id="S3.F3.6.3.1" class="ltx_sub"><span id="S3.F3.6.3.1.1" class="ltx_text ltx_font_upright">1</span></sub>, F<sub id="S3.F3.6.3.2" class="ltx_sub"><span id="S3.F3.6.3.2.1" class="ltx_text ltx_font_upright">2</span></sub>, and F<sub id="S3.F3.6.3.3" class="ltx_sub"><span id="S3.F3.6.3.3.1" class="ltx_text ltx_font_upright">3</span></sub> are omitted for simplicity.</span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">DED block.</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">Existing high-performance 3D object detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> usually rely on object centers for detection. However, the feature maps extracted by purely sparse CNNs may have empty holes around object centers, especially for large objects. To overcome this issue, we introduce a DED block that expands sparse features towards object centers, as shown in Figure <a href="#S3.F2" title="Figure 2 ‣ RS convolution and RSR block. ‣ 3.1 Background ‣ 3 Method ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (b). The DED block shares a similar structure with the SED block but utilizes the widely used dense convolutions instead. Specifically, we replace the SSR block in the SED block with a dense residual (DR) block, which is similar to the SSR block but consists of two dense convolutions. Furthermore, the RS convolution employed for feature down-sampling is replaced with a DR block that has a stride of 2. For feature up-sampling, we replace the sparse inverse convolution with a dense deconvolution. These modifications enable the DED block to effectively expand sparse features towards object centers.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>HEDNet</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">Based on the proposed SED block and DED block, we introduce HEDNet, a hierarchical encoder-decoder network designed for 3D object detection. The architecture of HEDNet is illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ SED block. ‣ 3.2 SED and DED blocks ‣ 3 Method ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Given the raw point clouds, a dynamic VFE module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> is used to perform voxelization to generate a grid of voxels denoted as F<sub id="S3.SS3.p1.2.1" class="ltx_sub">0</sub>. Subsequently, a sparse backbone including two SSR blocks and several SED blocks is employed to extract 3D sparse features. Before being fed into the 2D dense backbone, the sparse features are compressed into dense BEV features like in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>. The 2D dense backbone, composed of <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">n</annotation></semantics></math> DED blocks, is responsible for expanding the sparse features towards object centers. Finally, the output features are fed into the detection head for final predictions. At a macro level, HEDNet follows a hierarchical structure similar to SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, where the resolution of feature maps progressively decreases. At a micro level, the SED and DED blocks, key components of HEDNet, employ encoder-decoder structures. This is where the name HEDNet comes from. We adopt SED and DED blocks of three scales for HEDNet by default.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.6.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:489.7pt;height:486.9pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.2pt,27.0pt) scale(0.9,0.9) ;">
<table id="S3.T1.6.6.6" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.6.6.6.7.1" class="ltx_tr">
<th id="S3.T1.6.6.6.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="8">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>                                              <span id="S3.T1.6.6.6.7.1.1.1" class="ltx_text ltx_font_italic">Results on the validation data set</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.6.6.6.8.1" class="ltx_tr">
<th id="S3.T1.6.6.6.8.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S3.T1.6.6.6.8.1.1.1" class="ltx_text">Method</span></th>
<td id="S3.T1.6.6.6.8.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">mAP/mAPH</td>
<td id="S3.T1.6.6.6.8.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="2">Vehicle AP/APH</td>
<td id="S3.T1.6.6.6.8.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="2">Pedestrian AP/APH</td>
<td id="S3.T1.6.6.6.8.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="2">Cyclist AP/APH</td>
</tr>
<tr id="S3.T1.6.6.6.9.2" class="ltx_tr">
<td id="S3.T1.6.6.6.9.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L2</td>
<td id="S3.T1.6.6.6.9.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L1</td>
<td id="S3.T1.6.6.6.9.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L2</td>
<td id="S3.T1.6.6.6.9.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L1</td>
<td id="S3.T1.6.6.6.9.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L2</td>
<td id="S3.T1.6.6.6.9.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L1</td>
<td id="S3.T1.6.6.6.9.2.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">L2</td>
</tr>
<tr id="S3.T1.6.6.6.10.3" class="ltx_tr">
<th id="S3.T1.6.6.6.10.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.10.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">61.0/57.2</td>
<td id="S3.T1.6.6.6.10.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">72.3/71.7</td>
<td id="S3.T1.6.6.6.10.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">63.9/63.3</td>
<td id="S3.T1.6.6.6.10.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">68.7/58.2</td>
<td id="S3.T1.6.6.6.10.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">60.7/51.3</td>
<td id="S3.T1.6.6.6.10.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">60.6/59.3</td>
<td id="S3.T1.6.6.6.10.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">58.3/57.0</td>
</tr>
<tr id="S3.T1.6.6.6.11.4" class="ltx_tr">
<th id="S3.T1.6.6.6.11.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">PointPillar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.11.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">62.8/57.8</td>
<td id="S3.T1.6.6.6.11.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.1/71.5</td>
<td id="S3.T1.6.6.6.11.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">63.6/63.1</td>
<td id="S3.T1.6.6.6.11.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.6/56.7</td>
<td id="S3.T1.6.6.6.11.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">62.8/50.3</td>
<td id="S3.T1.6.6.6.11.4.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">64.4/62.3</td>
<td id="S3.T1.6.6.6.11.4.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">61.9/59.9</td>
</tr>
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">Lidar-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite><sup id="S3.T1.1.1.1.1.1.1" class="ltx_sup"><span id="S3.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</th>
<td id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">65.8/61.3</td>
<td id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">76.0/75.5</td>
<td id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.3/67.9</td>
<td id="S3.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.2/58.7</td>
<td id="S3.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">63.1/51.7</td>
<td id="S3.T1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.6/66.9</td>
<td id="S3.T1.1.1.1.1.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">66.1/64.4</td>
</tr>
<tr id="S3.T1.2.2.2.2" class="ltx_tr">
<th id="S3.T1.2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">Part-A2-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite><sup id="S3.T1.2.2.2.2.1.1" class="ltx_sup"><span id="S3.T1.2.2.2.2.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</th>
<td id="S3.T1.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">66.9/63.8</td>
<td id="S3.T1.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">77.1/76.5</td>
<td id="S3.T1.2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.5/68.0</td>
<td id="S3.T1.2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">75.2/66.9</td>
<td id="S3.T1.2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">66.2/58.6</td>
<td id="S3.T1.2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.6/67.4</td>
<td id="S3.T1.2.2.2.2.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">66.1/64.9</td>
</tr>
<tr id="S3.T1.6.6.6.12.5" class="ltx_tr">
<th id="S3.T1.6.6.6.12.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.12.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">67.8/64.6</td>
<td id="S3.T1.6.6.6.12.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">74.2/73.8</td>
<td id="S3.T1.6.6.6.12.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">65.5/65.1</td>
<td id="S3.T1.6.6.6.12.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.7/69.6</td>
<td id="S3.T1.6.6.6.12.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.0/61.7</td>
<td id="S3.T1.6.6.6.12.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.7/69.6</td>
<td id="S3.T1.6.6.6.12.5.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">68.0/66.9</td>
</tr>
<tr id="S3.T1.6.6.6.13.6" class="ltx_tr">
<th id="S3.T1.6.6.6.13.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.13.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.2/65.8</td>
<td id="S3.T1.6.6.6.13.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">74.2/73.6</td>
<td id="S3.T1.6.6.6.13.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">66.2/65.7</td>
<td id="S3.T1.6.6.6.13.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">76.6/70.5</td>
<td id="S3.T1.6.6.6.13.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.8/63.2</td>
<td id="S3.T1.6.6.6.13.6.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.3/71.1</td>
<td id="S3.T1.6.6.6.13.6.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">69.7/68.5</td>
</tr>
<tr id="S3.T1.3.3.3.3" class="ltx_tr">
<th id="S3.T1.3.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">PV-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite><sup id="S3.T1.3.3.3.3.1.1" class="ltx_sup"><span id="S3.T1.3.3.3.3.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</th>
<td id="S3.T1.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.6/67.2</td>
<td id="S3.T1.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.0/77.5</td>
<td id="S3.T1.3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.4/69.0</td>
<td id="S3.T1.3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">79.2/73.0</td>
<td id="S3.T1.3.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.4/64.7</td>
<td id="S3.T1.3.3.3.3.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.5/70.3</td>
<td id="S3.T1.3.3.3.3.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">69.0/67.8</td>
</tr>
<tr id="S3.T1.4.4.4.4" class="ltx_tr">
<th id="S3.T1.4.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite><sup id="S3.T1.4.4.4.4.1.1" class="ltx_sup"><span id="S3.T1.4.4.4.4.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</th>
<td id="S3.T1.4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.8/67.6</td>
<td id="S3.T1.4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">76.6/76.0</td>
<td id="S3.T1.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">68.9/68.4</td>
<td id="S3.T1.4.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">79.0/73.4</td>
<td id="S3.T1.4.4.4.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.0/65.8</td>
<td id="S3.T1.4.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.1/71.0</td>
<td id="S3.T1.4.4.4.4.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">69.5/68.5</td>
</tr>
<tr id="S3.T1.6.6.6.14.7" class="ltx_tr">
<th id="S3.T1.6.6.6.14.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SWFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.14.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-/-</td>
<td id="S3.T1.6.6.6.14.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">77.8/77.3</td>
<td id="S3.T1.6.6.6.14.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.2/68.8</td>
<td id="S3.T1.6.6.6.14.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">80.9/72.7</td>
<td id="S3.T1.6.6.6.14.7.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.5/64.9</td>
<td id="S3.T1.6.6.6.14.7.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-/-</td>
<td id="S3.T1.6.6.6.14.7.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">-/-</td>
</tr>
<tr id="S3.T1.6.6.6.15.8" class="ltx_tr">
<th id="S3.T1.6.6.6.15.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">OcTr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.15.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.7/68.2</td>
<td id="S3.T1.6.6.6.15.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.1/77.6</td>
<td id="S3.T1.6.6.6.15.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.8/69.3</td>
<td id="S3.T1.6.6.6.15.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">80.8/74.4</td>
<td id="S3.T1.6.6.6.15.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.5/66.5</td>
<td id="S3.T1.6.6.6.15.8.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.6/71.5</td>
<td id="S3.T1.6.6.6.15.8.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">69.9/68.9</td>
</tr>
<tr id="S3.T1.6.6.6.16.9" class="ltx_tr">
<th id="S3.T1.6.6.6.16.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">PillarNet-34 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.16.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.0/68.5</td>
<td id="S3.T1.6.6.6.16.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">79.1/78.6</td>
<td id="S3.T1.6.6.6.16.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.9/70.5</td>
<td id="S3.T1.6.6.6.16.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">80.6/74.0</td>
<td id="S3.T1.6.6.6.16.9.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.3/66.2</td>
<td id="S3.T1.6.6.6.16.9.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.3/71.2</td>
<td id="S3.T1.6.6.6.16.9.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">69.7/68.7</td>
</tr>
<tr id="S3.T1.6.6.6.17.10" class="ltx_tr">
<th id="S3.T1.6.6.6.17.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">AFDetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.17.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.0/68.8</td>
<td id="S3.T1.6.6.6.17.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">77.6/77.1</td>
<td id="S3.T1.6.6.6.17.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.7/69.2</td>
<td id="S3.T1.6.6.6.17.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">80.2/74.6</td>
<td id="S3.T1.6.6.6.17.10.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.2/67.0</td>
<td id="S3.T1.6.6.6.17.10.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">73.7/72.7</td>
<td id="S3.T1.6.6.6.17.10.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">71.0/70.1</td>
</tr>
<tr id="S3.T1.6.6.6.18.11" class="ltx_tr">
<th id="S3.T1.6.6.6.18.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">CenterFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.18.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.1/68.9</td>
<td id="S3.T1.6.6.6.18.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">75.0/74.4</td>
<td id="S3.T1.6.6.6.18.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.9/69.4</td>
<td id="S3.T1.6.6.6.18.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.6/73.0</td>
<td id="S3.T1.6.6.6.18.11.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">73.6/68.3</td>
<td id="S3.T1.6.6.6.18.11.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.3/71.3</td>
<td id="S3.T1.6.6.6.18.11.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">69.8/68.8</td>
</tr>
<tr id="S3.T1.6.6.6.19.12" class="ltx_tr">
<th id="S3.T1.6.6.6.19.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">LargeKernel3D<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.19.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-/-</td>
<td id="S3.T1.6.6.6.19.12.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.1/77.6</td>
<td id="S3.T1.6.6.6.19.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">69.8/69.4</td>
<td id="S3.T1.6.6.6.19.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-/-</td>
<td id="S3.T1.6.6.6.19.12.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-/-</td>
<td id="S3.T1.6.6.6.19.12.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">-/-</td>
<td id="S3.T1.6.6.6.19.12.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">-/-</td>
</tr>
<tr id="S3.T1.5.5.5.5" class="ltx_tr">
<th id="S3.T1.5.5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">PV-RCNN++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite><sup id="S3.T1.5.5.5.5.1.1" class="ltx_sup"><span id="S3.T1.5.5.5.5.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</th>
<td id="S3.T1.5.5.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.7/69.5</td>
<td id="S3.T1.5.5.5.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">79.3/78.8</td>
<td id="S3.T1.5.5.5.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.6/70.2</td>
<td id="S3.T1.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">81.3/76.3</td>
<td id="S3.T1.5.5.5.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">73.2/68.0</td>
<td id="S3.T1.5.5.5.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">73.7/72.7</td>
<td id="S3.T1.5.5.5.5.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">71.2/70.2</td>
</tr>
<tr id="S3.T1.6.6.6.6" class="ltx_tr">
<th id="S3.T1.6.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite><sup id="S3.T1.6.6.6.6.1.1" class="ltx_sup"><span id="S3.T1.6.6.6.6.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</th>
<td id="S3.T1.6.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.7/70.5</td>
<td id="S3.T1.6.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">79.5/79.0</td>
<td id="S3.T1.6.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">70.3/69.9</td>
<td id="S3.T1.6.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">83.6/78.2</td>
<td id="S3.T1.6.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">74.4/69.4</td>
<td id="S3.T1.6.6.6.6.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">75.3/74.1</td>
<td id="S3.T1.6.6.6.6.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">73.3/72.1</td>
</tr>
<tr id="S3.T1.6.6.6.20.13" class="ltx_tr">
<th id="S3.T1.6.6.6.20.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">DSVT-Voxel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.20.13.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">74.0/72.1</td>
<td id="S3.T1.6.6.6.20.13.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">79.7/79.3</td>
<td id="S3.T1.6.6.6.20.13.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.4/71.0</td>
<td id="S3.T1.6.6.6.20.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">83.7/78.9</td>
<td id="S3.T1.6.6.6.20.13.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">76.1/71.5</td>
<td id="S3.T1.6.6.6.20.13.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">77.5/76.5</td>
<td id="S3.T1.6.6.6.20.13.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">74.6/73.7</td>
</tr>
<tr id="S3.T1.6.6.6.21.14" class="ltx_tr" style="background-color:#CCFFFF;">
<th id="S3.T1.6.6.6.21.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.21.14.1.1" class="ltx_text" style="background-color:#CCFFFF;">HEDNet (ours)</span></th>
<td id="S3.T1.6.6.6.21.14.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.21.14.2.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">75.3<span id="S3.T1.6.6.6.21.14.2.1.1" class="ltx_text ltx_font_medium" style="background-color:#CCFFFF;">/</span>73.4</span></td>
<td id="S3.T1.6.6.6.21.14.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.21.14.3.1" class="ltx_text" style="background-color:#CCFFFF;">81.1/80.6</span></td>
<td id="S3.T1.6.6.6.21.14.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.21.14.4.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">73.2<span id="S3.T1.6.6.6.21.14.4.1.1" class="ltx_text ltx_font_medium" style="background-color:#CCFFFF;">/</span>72.7</span></td>
<td id="S3.T1.6.6.6.21.14.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.21.14.5.1" class="ltx_text" style="background-color:#CCFFFF;">84.4/80.0</span></td>
<td id="S3.T1.6.6.6.21.14.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.21.14.6.1" class="ltx_text" style="background-color:#CCFFFF;">76.8/72.6</span></td>
<td id="S3.T1.6.6.6.21.14.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.21.14.7.1" class="ltx_text" style="background-color:#CCFFFF;">78.7/77.7</span></td>
<td id="S3.T1.6.6.6.21.14.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.21.14.8.1" class="ltx_text" style="background-color:#CCFFFF;">75.8/74.9</span></td>
</tr>
<tr id="S3.T1.6.6.6.22.15" class="ltx_tr">
<th id="S3.T1.6.6.6.22.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="8"><span id="S3.T1.6.6.6.22.15.1.1" class="ltx_text ltx_font_italic">Results on the test data set</span></th>
</tr>
<tr id="S3.T1.6.6.6.23.16" class="ltx_tr">
<th id="S3.T1.6.6.6.23.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S3.T1.6.6.6.23.16.1.1" class="ltx_text">Method</span></th>
<td id="S3.T1.6.6.6.23.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">mAP/mAPH</td>
<td id="S3.T1.6.6.6.23.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="2">Vehicle AP/APH</td>
<td id="S3.T1.6.6.6.23.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="2">Pedestrian AP/APH</td>
<td id="S3.T1.6.6.6.23.16.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="2">Cyclist AP/APH</td>
</tr>
<tr id="S3.T1.6.6.6.24.17" class="ltx_tr">
<td id="S3.T1.6.6.6.24.17.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L2</td>
<td id="S3.T1.6.6.6.24.17.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L1</td>
<td id="S3.T1.6.6.6.24.17.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L2</td>
<td id="S3.T1.6.6.6.24.17.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L1</td>
<td id="S3.T1.6.6.6.24.17.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L2</td>
<td id="S3.T1.6.6.6.24.17.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">L1</td>
<td id="S3.T1.6.6.6.24.17.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">L2</td>
</tr>
<tr id="S3.T1.6.6.6.25.18" class="ltx_tr">
<th id="S3.T1.6.6.6.25.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">PV-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.25.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">71.3/68.8</td>
<td id="S3.T1.6.6.6.25.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">80.6/80.1</td>
<td id="S3.T1.6.6.6.25.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">72.8/72.4</td>
<td id="S3.T1.6.6.6.25.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">78.2/72.0</td>
<td id="S3.T1.6.6.6.25.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">71.8/66.0</td>
<td id="S3.T1.6.6.6.25.18.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">71.8/70.4</td>
<td id="S3.T1.6.6.6.25.18.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">69.1/67.8</td>
</tr>
<tr id="S3.T1.6.6.6.26.19" class="ltx_tr">
<th id="S3.T1.6.6.6.26.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">PV-RCNN++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.26.19.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.4/70.2</td>
<td id="S3.T1.6.6.6.26.19.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">81.6/81.2</td>
<td id="S3.T1.6.6.6.26.19.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">73.9/73.5</td>
<td id="S3.T1.6.6.6.26.19.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">80.4/75.0</td>
<td id="S3.T1.6.6.6.26.19.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">74.1/69.0</td>
<td id="S3.T1.6.6.6.26.19.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">71.9/70.8</td>
<td id="S3.T1.6.6.6.26.19.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">69.3/68.2</td>
</tr>
<tr id="S3.T1.6.6.6.27.20" class="ltx_tr">
<th id="S3.T1.6.6.6.27.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">AFDetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.27.20.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.2/70.3</td>
<td id="S3.T1.6.6.6.27.20.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">80.5/80.0</td>
<td id="S3.T1.6.6.6.27.20.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">73.0/72.6</td>
<td id="S3.T1.6.6.6.27.20.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">79.8/74.3</td>
<td id="S3.T1.6.6.6.27.20.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">73.7/68.6</td>
<td id="S3.T1.6.6.6.27.20.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">72.4/71.2</td>
<td id="S3.T1.6.6.6.27.20.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">69.8/69.7</td>
</tr>
<tr id="S3.T1.6.6.6.28.21" class="ltx_tr">
<th id="S3.T1.6.6.6.28.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>
</th>
<td id="S3.T1.6.6.6.28.21.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">74.4/72.4</td>
<td id="S3.T1.6.6.6.28.21.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">82.7/82.3</td>
<td id="S3.T1.6.6.6.28.21.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">74.4/74.1</td>
<td id="S3.T1.6.6.6.28.21.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">82.9/77.9</td>
<td id="S3.T1.6.6.6.28.21.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">75.9/71.3</td>
<td id="S3.T1.6.6.6.28.21.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">75.6/74.4</td>
<td id="S3.T1.6.6.6.28.21.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">72.9/71.8</td>
</tr>
<tr id="S3.T1.6.6.6.29.22" class="ltx_tr" style="background-color:#CCFFFF;">
<th id="S3.T1.6.6.6.29.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.29.22.1.1" class="ltx_text" style="background-color:#CCFFFF;">HEDNet (ours)</span></th>
<td id="S3.T1.6.6.6.29.22.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.29.22.2.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">76.9<span id="S3.T1.6.6.6.29.22.2.1.1" class="ltx_text ltx_font_medium" style="background-color:#CCFFFF;">/</span>75.0</span></td>
<td id="S3.T1.6.6.6.29.22.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.29.22.3.1" class="ltx_text" style="background-color:#CCFFFF;">84.2/83.8</span></td>
<td id="S3.T1.6.6.6.29.22.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.29.22.4.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">77.0<span id="S3.T1.6.6.6.29.22.4.1.1" class="ltx_text ltx_font_medium" style="background-color:#CCFFFF;">/</span>76.6</span></td>
<td id="S3.T1.6.6.6.29.22.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.29.22.5.1" class="ltx_text" style="background-color:#CCFFFF;">84.1/79.7</span></td>
<td id="S3.T1.6.6.6.29.22.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.29.22.6.1" class="ltx_text" style="background-color:#CCFFFF;">78.3/74.0</span></td>
<td id="S3.T1.6.6.6.29.22.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.29.22.7.1" class="ltx_text" style="background-color:#CCFFFF;">78.2/77.0</span></td>
<td id="S3.T1.6.6.6.29.22.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.6.6.6.29.22.8.1" class="ltx_text" style="background-color:#CCFFFF;">75.4/74.3</span></td>
</tr>
<tr id="S3.T1.6.6.6.30.23" class="ltx_tr">
<th id="S3.T1.6.6.6.30.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></th>
<td id="S3.T1.6.6.6.30.23.2" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S3.T1.6.6.6.30.23.3" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S3.T1.6.6.6.30.23.4" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S3.T1.6.6.6.30.23.5" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S3.T1.6.6.6.30.23.6" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S3.T1.6.6.6.30.23.7" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S3.T1.6.6.6.30.23.8" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison with prior methods on the Waymo Open dataset (single-frame setting). Metrics: mAP/mAPH (%)<math id="S3.T1.10.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.10.m1.1b"><mo stretchy="false" id="S3.T1.10.m1.1.1" xref="S3.T1.10.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.10.m1.1c"><ci id="S3.T1.10.m1.1.1.cmml" xref="S3.T1.10.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.m1.1d">\uparrow</annotation></semantics></math> for the overall results, and AP/APH (%)<math id="S3.T1.11.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.11.m2.1b"><mo stretchy="false" id="S3.T1.11.m2.1.1" xref="S3.T1.11.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.11.m2.1c"><ci id="S3.T1.11.m2.1.1.cmml" xref="S3.T1.11.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.m2.1d">\uparrow</annotation></semantics></math> for each category. <sup id="S3.T1.14.1" class="ltx_sup"><span id="S3.T1.14.1.1" class="ltx_text ltx_font_italic">†</span></sup>: two-stage method.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and metrics</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Waymo Open</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">contains 160k, 40k, and 30k annotated samples for training, validation, and testing, respectively. The metrics for 3D object detection include mean average precision (mAP) and mAP weighted by the heading accuracy (mAPH). Both are further broken down into two difficulty levels: L1 for objects with more than five LiDAR points and L2 for objects with at least one LiDAR point.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">nuScenes</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">consists of 28k, 6k, and 6k annotated samples for training, validation, and testing, respectively. Mean average precision (mAP) and nuScenes detection score (NDS) are used as the evaluation metrics. mAP is computed by averaging over the distance thresholds of 0.5m, 1m, 2m, 4m across all categories. NDS is a weighted average of mAP and the other five true positive metrics measuring the translation, scaling, orientation, velocity, and attribute errors.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We implemented our method using the open-source OpenPCDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite>. To build HEDNet, we set the hyperparameter <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">m</annotation></semantics></math> to 2 for all SED and DED blocks and stacked 4 DED blocks for the 2D dense backbone by default. For 3D object detection on the Waymo Open dataset, we adopted the detection head of CenterPoint and set the voxel size to (0.08m, 0.08m, 0.15m). We trained HEDNet for 24 epochs on the full training set (<span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">single-frame</span>) to compare with prior methods. For ablation experiments in Section <a href="#S4.SS4" title="4.4 Ablation studies ‣ 4 Experiments ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>, we trained the models for 30 epochs on a 20% training subset. All models were trained with a batch size of 16 on 8 RTX 3090 GPUs. The other training settings strictly followed DSVT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>. For 3D object detection on the nuScenes dataset, we adopted the detection head of TransFusion-L and set the voxel size to (0.075m, 0.075m, 0.2m). We trained HEDNet for 20 epochs with a batch size of 16 on 8 RTX 3090 GPUs. The other training settings strictly followed TransFusion-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:494.7pt;height:405pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.5pt,22.5pt) scale(0.9,0.9) ;">
<table id="S4.T2.7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.7.1.1.1" class="ltx_tr">
<th id="S4.T2.7.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="13">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>          <span id="S4.T2.7.1.1.1.1.1" class="ltx_text ltx_font_italic">Results on the validation data set</span>
</th>
</tr>
<tr id="S4.T2.7.1.2.2" class="ltx_tr">
<th id="S4.T2.7.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Method</th>
<th id="S4.T2.7.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">NDS</th>
<th id="S4.T2.7.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">mAP</th>
<th id="S4.T2.7.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Car</th>
<th id="S4.T2.7.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Truck</th>
<th id="S4.T2.7.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Bus</th>
<th id="S4.T2.7.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">T.L.</th>
<th id="S4.T2.7.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">C.V.</th>
<th id="S4.T2.7.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Ped.</th>
<th id="S4.T2.7.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">M.T.</th>
<th id="S4.T2.7.1.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Bike</th>
<th id="S4.T2.7.1.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">T.C.</th>
<th id="S4.T2.7.1.2.2.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">B.R.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.7.1.3.1" class="ltx_tr">
<th id="S4.T2.7.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>
</th>
<td id="S4.T2.7.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">66.5</td>
<td id="S4.T2.7.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">59.2</td>
<td id="S4.T2.7.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">84.9</td>
<td id="S4.T2.7.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">57.4</td>
<td id="S4.T2.7.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">70.7</td>
<td id="S4.T2.7.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">38.1</td>
<td id="S4.T2.7.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">16.9</td>
<td id="S4.T2.7.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">85.1</td>
<td id="S4.T2.7.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">59.0</td>
<td id="S4.T2.7.1.3.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">42.0</td>
<td id="S4.T2.7.1.3.1.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">69.8</td>
<td id="S4.T2.7.1.3.1.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">68.3</td>
</tr>
<tr id="S4.T2.7.1.4.2" class="ltx_tr">
<th id="S4.T2.7.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">VoxelNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>
</th>
<td id="S4.T2.7.1.4.2.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">66.7</td>
<td id="S4.T2.7.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">60.5</td>
<td id="S4.T2.7.1.4.2.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">83.9</td>
<td id="S4.T2.7.1.4.2.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">55.5</td>
<td id="S4.T2.7.1.4.2.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.5</td>
<td id="S4.T2.7.1.4.2.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">38.1</td>
<td id="S4.T2.7.1.4.2.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">21.1</td>
<td id="S4.T2.7.1.4.2.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">84.6</td>
<td id="S4.T2.7.1.4.2.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">62.8</td>
<td id="S4.T2.7.1.4.2.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">50.0</td>
<td id="S4.T2.7.1.4.2.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.4</td>
<td id="S4.T2.7.1.4.2.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.4</td>
</tr>
<tr id="S4.T2.7.1.5.3" class="ltx_tr">
<th id="S4.T2.7.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">TransFusion-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</th>
<td id="S4.T2.7.1.5.3.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.1</td>
<td id="S4.T2.7.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">65.5</td>
<td id="S4.T2.7.1.5.3.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">86.9</td>
<td id="S4.T2.7.1.5.3.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">60.8</td>
<td id="S4.T2.7.1.5.3.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">73.1</td>
<td id="S4.T2.7.1.5.3.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">43.4</td>
<td id="S4.T2.7.1.5.3.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">25.2</td>
<td id="S4.T2.7.1.5.3.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">87.5</td>
<td id="S4.T2.7.1.5.3.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">72.9</td>
<td id="S4.T2.7.1.5.3.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">57.3</td>
<td id="S4.T2.7.1.5.3.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">77.2</td>
<td id="S4.T2.7.1.5.3.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.3</td>
</tr>
<tr id="S4.T2.7.1.6.4" class="ltx_tr" style="background-color:#CCFFFF;">
<th id="S4.T2.7.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.1.1" class="ltx_text" style="background-color:#CCFFFF;">HEDNet (Ours)</span></th>
<td id="S4.T2.7.1.6.4.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.2.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">71.4</span></td>
<td id="S4.T2.7.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.3.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">66.7</span></td>
<td id="S4.T2.7.1.6.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.4.1" class="ltx_text" style="background-color:#CCFFFF;">87.7</span></td>
<td id="S4.T2.7.1.6.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.5.1" class="ltx_text" style="background-color:#CCFFFF;">60.6</span></td>
<td id="S4.T2.7.1.6.4.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.6.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">77.8</span></td>
<td id="S4.T2.7.1.6.4.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.7.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">50.7</span></td>
<td id="S4.T2.7.1.6.4.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.8.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">28.9</span></td>
<td id="S4.T2.7.1.6.4.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.9.1" class="ltx_text" style="background-color:#CCFFFF;">87.1</span></td>
<td id="S4.T2.7.1.6.4.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.10.1" class="ltx_text" style="background-color:#CCFFFF;">74.3</span></td>
<td id="S4.T2.7.1.6.4.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.11.1" class="ltx_text" style="background-color:#CCFFFF;">56.8</span></td>
<td id="S4.T2.7.1.6.4.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.12.1" class="ltx_text" style="background-color:#CCFFFF;">76.3</span></td>
<td id="S4.T2.7.1.6.4.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.6.4.13.1" class="ltx_text" style="background-color:#CCFFFF;">66.9</span></td>
</tr>
<tr id="S4.T2.7.1.7.5" class="ltx_tr">
<th id="S4.T2.7.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="13"><span id="S4.T2.7.1.7.5.1.1" class="ltx_text ltx_font_italic">Results on the test data set</span></th>
</tr>
<tr id="S4.T2.7.1.8.6" class="ltx_tr">
<th id="S4.T2.7.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Method</th>
<td id="S4.T2.7.1.8.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">NDS</td>
<td id="S4.T2.7.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">mAP</td>
<td id="S4.T2.7.1.8.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Car</td>
<td id="S4.T2.7.1.8.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Truck</td>
<td id="S4.T2.7.1.8.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Bus</td>
<td id="S4.T2.7.1.8.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">T.L.</td>
<td id="S4.T2.7.1.8.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">C.V.</td>
<td id="S4.T2.7.1.8.6.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Ped.</td>
<td id="S4.T2.7.1.8.6.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">M.T.</td>
<td id="S4.T2.7.1.8.6.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Bike</td>
<td id="S4.T2.7.1.8.6.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">T.C.</td>
<td id="S4.T2.7.1.8.6.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">B.R.</td>
</tr>
<tr id="S4.T2.7.1.9.7" class="ltx_tr">
<th id="S4.T2.7.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>
</th>
<td id="S4.T2.7.1.9.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">45.3</td>
<td id="S4.T2.7.1.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">30.5</td>
<td id="S4.T2.7.1.9.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">68.4</td>
<td id="S4.T2.7.1.9.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">23.0</td>
<td id="S4.T2.7.1.9.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">28.2</td>
<td id="S4.T2.7.1.9.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">23.4</td>
<td id="S4.T2.7.1.9.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">4.1</td>
<td id="S4.T2.7.1.9.7.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">59.7</td>
<td id="S4.T2.7.1.9.7.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">27.4</td>
<td id="S4.T2.7.1.9.7.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1.1</td>
<td id="S4.T2.7.1.9.7.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">30.8</td>
<td id="S4.T2.7.1.9.7.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">38.9</td>
</tr>
<tr id="S4.T2.7.1.10.8" class="ltx_tr">
<th id="S4.T2.7.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">3DSSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>
</th>
<td id="S4.T2.7.1.10.8.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">56.4</td>
<td id="S4.T2.7.1.10.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">42.6</td>
<td id="S4.T2.7.1.10.8.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">81.2</td>
<td id="S4.T2.7.1.10.8.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">47.2</td>
<td id="S4.T2.7.1.10.8.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">61.4</td>
<td id="S4.T2.7.1.10.8.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">30.5</td>
<td id="S4.T2.7.1.10.8.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">12.6</td>
<td id="S4.T2.7.1.10.8.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.2</td>
<td id="S4.T2.7.1.10.8.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">36.0</td>
<td id="S4.T2.7.1.10.8.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">8.6</td>
<td id="S4.T2.7.1.10.8.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">31.1</td>
<td id="S4.T2.7.1.10.8.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">47.9</td>
</tr>
<tr id="S4.T2.7.1.11.9" class="ltx_tr">
<th id="S4.T2.7.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">CBGS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>
</th>
<td id="S4.T2.7.1.11.9.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">63.3</td>
<td id="S4.T2.7.1.11.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">52.8</td>
<td id="S4.T2.7.1.11.9.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">81.1</td>
<td id="S4.T2.7.1.11.9.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">48.5</td>
<td id="S4.T2.7.1.11.9.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">54.9</td>
<td id="S4.T2.7.1.11.9.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">42.9</td>
<td id="S4.T2.7.1.11.9.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">10.5</td>
<td id="S4.T2.7.1.11.9.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">80.1</td>
<td id="S4.T2.7.1.11.9.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">51.5</td>
<td id="S4.T2.7.1.11.9.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">22.3</td>
<td id="S4.T2.7.1.11.9.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.9</td>
<td id="S4.T2.7.1.11.9.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">65.7</td>
</tr>
<tr id="S4.T2.7.1.12.10" class="ltx_tr">
<th id="S4.T2.7.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>
</th>
<td id="S4.T2.7.1.12.10.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">65.5</td>
<td id="S4.T2.7.1.12.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">58.0</td>
<td id="S4.T2.7.1.12.10.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">84.6</td>
<td id="S4.T2.7.1.12.10.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">51.0</td>
<td id="S4.T2.7.1.12.10.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">60.2</td>
<td id="S4.T2.7.1.12.10.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">53.2</td>
<td id="S4.T2.7.1.12.10.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">17.5</td>
<td id="S4.T2.7.1.12.10.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">83.4</td>
<td id="S4.T2.7.1.12.10.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">53.7</td>
<td id="S4.T2.7.1.12.10.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">28.7</td>
<td id="S4.T2.7.1.12.10.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">76.7</td>
<td id="S4.T2.7.1.12.10.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.9</td>
</tr>
<tr id="S4.T2.7.1.13.11" class="ltx_tr">
<th id="S4.T2.7.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">FCOS-LiDAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>
</th>
<td id="S4.T2.7.1.13.11.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">65.7</td>
<td id="S4.T2.7.1.13.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">60.2</td>
<td id="S4.T2.7.1.13.11.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">82.2</td>
<td id="S4.T2.7.1.13.11.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">47.7</td>
<td id="S4.T2.7.1.13.11.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">52.9</td>
<td id="S4.T2.7.1.13.11.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">48.8</td>
<td id="S4.T2.7.1.13.11.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">28.8</td>
<td id="S4.T2.7.1.13.11.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">84.5</td>
<td id="S4.T2.7.1.13.11.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">68.0</td>
<td id="S4.T2.7.1.13.11.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">39.0</td>
<td id="S4.T2.7.1.13.11.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">79.2</td>
<td id="S4.T2.7.1.13.11.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.7</td>
</tr>
<tr id="S4.T2.7.1.14.12" class="ltx_tr">
<th id="S4.T2.7.1.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">HotSpotNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>
</th>
<td id="S4.T2.7.1.14.12.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">66.0</td>
<td id="S4.T2.7.1.14.12.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">59.3</td>
<td id="S4.T2.7.1.14.12.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">83.1</td>
<td id="S4.T2.7.1.14.12.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">50.9</td>
<td id="S4.T2.7.1.14.12.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">56.4</td>
<td id="S4.T2.7.1.14.12.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">53.3</td>
<td id="S4.T2.7.1.14.12.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">23.0</td>
<td id="S4.T2.7.1.14.12.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">81.3</td>
<td id="S4.T2.7.1.14.12.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">63.5</td>
<td id="S4.T2.7.1.14.12.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">36.6</td>
<td id="S4.T2.7.1.14.12.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">73.0</td>
<td id="S4.T2.7.1.14.12.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71.6</td>
</tr>
<tr id="S4.T2.7.1.15.13" class="ltx_tr">
<th id="S4.T2.7.1.15.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">CVCNET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>
</th>
<td id="S4.T2.7.1.15.13.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">66.6</td>
<td id="S4.T2.7.1.15.13.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">58.2</td>
<td id="S4.T2.7.1.15.13.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">82.6</td>
<td id="S4.T2.7.1.15.13.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">49.5</td>
<td id="S4.T2.7.1.15.13.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">59.4</td>
<td id="S4.T2.7.1.15.13.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">51.1</td>
<td id="S4.T2.7.1.15.13.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">16.2</td>
<td id="S4.T2.7.1.15.13.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">83.0</td>
<td id="S4.T2.7.1.15.13.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">61.8</td>
<td id="S4.T2.7.1.15.13.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">38.8</td>
<td id="S4.T2.7.1.15.13.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.7</td>
<td id="S4.T2.7.1.15.13.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.7</td>
</tr>
<tr id="S4.T2.7.1.16.14" class="ltx_tr">
<th id="S4.T2.7.1.16.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">AFDetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>
</th>
<td id="S4.T2.7.1.16.14.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">68.5</td>
<td id="S4.T2.7.1.16.14.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">62.4</td>
<td id="S4.T2.7.1.16.14.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">86.3</td>
<td id="S4.T2.7.1.16.14.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">54.2</td>
<td id="S4.T2.7.1.16.14.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">62.5</td>
<td id="S4.T2.7.1.16.14.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">58.9</td>
<td id="S4.T2.7.1.16.14.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">26.7</td>
<td id="S4.T2.7.1.16.14.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">85.8</td>
<td id="S4.T2.7.1.16.14.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">63.8</td>
<td id="S4.T2.7.1.16.14.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">34.3</td>
<td id="S4.T2.7.1.16.14.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">80.1</td>
<td id="S4.T2.7.1.16.14.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71.0</td>
</tr>
<tr id="S4.T2.7.1.17.15" class="ltx_tr">
<th id="S4.T2.7.1.17.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">UVTR-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>
</th>
<td id="S4.T2.7.1.17.15.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.7</td>
<td id="S4.T2.7.1.17.15.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">63.9</td>
<td id="S4.T2.7.1.17.15.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">86.3</td>
<td id="S4.T2.7.1.17.15.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">52.2</td>
<td id="S4.T2.7.1.17.15.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">62.8</td>
<td id="S4.T2.7.1.17.15.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">59.7</td>
<td id="S4.T2.7.1.17.15.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">33.7</td>
<td id="S4.T2.7.1.17.15.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">84.5</td>
<td id="S4.T2.7.1.17.15.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">68.8</td>
<td id="S4.T2.7.1.17.15.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">41.1</td>
<td id="S4.T2.7.1.17.15.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">74.7</td>
<td id="S4.T2.7.1.17.15.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">74.9</td>
</tr>
<tr id="S4.T2.7.1.18.16" class="ltx_tr">
<th id="S4.T2.7.1.18.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">VISTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>
</th>
<td id="S4.T2.7.1.18.16.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.8</td>
<td id="S4.T2.7.1.18.16.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">63.0</td>
<td id="S4.T2.7.1.18.16.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">84.4</td>
<td id="S4.T2.7.1.18.16.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">55.1</td>
<td id="S4.T2.7.1.18.16.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">63.7</td>
<td id="S4.T2.7.1.18.16.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">54.2</td>
<td id="S4.T2.7.1.18.16.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">25.1</td>
<td id="S4.T2.7.1.18.16.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">82.8</td>
<td id="S4.T2.7.1.18.16.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.0</td>
<td id="S4.T2.7.1.18.16.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">45.4</td>
<td id="S4.T2.7.1.18.16.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">78.5</td>
<td id="S4.T2.7.1.18.16.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71.4</td>
</tr>
<tr id="S4.T2.7.1.19.17" class="ltx_tr">
<th id="S4.T2.7.1.19.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Focals Conv <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>
</th>
<td id="S4.T2.7.1.19.17.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.0</td>
<td id="S4.T2.7.1.19.17.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">63.8</td>
<td id="S4.T2.7.1.19.17.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">86.7</td>
<td id="S4.T2.7.1.19.17.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">56.3</td>
<td id="S4.T2.7.1.19.17.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">67.7</td>
<td id="S4.T2.7.1.19.17.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">59.5</td>
<td id="S4.T2.7.1.19.17.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">23.8</td>
<td id="S4.T2.7.1.19.17.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">87.5</td>
<td id="S4.T2.7.1.19.17.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">64.5</td>
<td id="S4.T2.7.1.19.17.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">36.3</td>
<td id="S4.T2.7.1.19.17.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">81.4</td>
<td id="S4.T2.7.1.19.17.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">74.1</td>
</tr>
<tr id="S4.T2.7.1.20.18" class="ltx_tr">
<th id="S4.T2.7.1.20.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">VoxelNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>
</th>
<td id="S4.T2.7.1.20.18.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.0</td>
<td id="S4.T2.7.1.20.18.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">64.5</td>
<td id="S4.T2.7.1.20.18.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">84.6</td>
<td id="S4.T2.7.1.20.18.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">53.0</td>
<td id="S4.T2.7.1.20.18.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">64.7</td>
<td id="S4.T2.7.1.20.18.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">55.8</td>
<td id="S4.T2.7.1.20.18.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">28.7</td>
<td id="S4.T2.7.1.20.18.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">85.8</td>
<td id="S4.T2.7.1.20.18.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">73.2</td>
<td id="S4.T2.7.1.20.18.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">45.7</td>
<td id="S4.T2.7.1.20.18.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">79.0</td>
<td id="S4.T2.7.1.20.18.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">74.6</td>
</tr>
<tr id="S4.T2.7.1.21.19" class="ltx_tr">
<th id="S4.T2.7.1.21.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">TransFusion-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</th>
<td id="S4.T2.7.1.21.19.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.2</td>
<td id="S4.T2.7.1.21.19.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">65.5</td>
<td id="S4.T2.7.1.21.19.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">86.2</td>
<td id="S4.T2.7.1.21.19.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">56.7</td>
<td id="S4.T2.7.1.21.19.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">66.3</td>
<td id="S4.T2.7.1.21.19.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">58.8</td>
<td id="S4.T2.7.1.21.19.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">28.2</td>
<td id="S4.T2.7.1.21.19.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">86.1</td>
<td id="S4.T2.7.1.21.19.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">68.3</td>
<td id="S4.T2.7.1.21.19.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">44.2</td>
<td id="S4.T2.7.1.21.19.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">82.0</td>
<td id="S4.T2.7.1.21.19.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">78.2</td>
</tr>
<tr id="S4.T2.7.1.22.20" class="ltx_tr">
<th id="S4.T2.7.1.22.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">LargeKernel3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>
</th>
<td id="S4.T2.7.1.22.20.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.6</td>
<td id="S4.T2.7.1.22.20.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">65.4</td>
<td id="S4.T2.7.1.22.20.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">85.5</td>
<td id="S4.T2.7.1.22.20.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">53.8</td>
<td id="S4.T2.7.1.22.20.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">64.4</td>
<td id="S4.T2.7.1.22.20.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">59.5</td>
<td id="S4.T2.7.1.22.20.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">29.7</td>
<td id="S4.T2.7.1.22.20.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">85.9</td>
<td id="S4.T2.7.1.22.20.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">72.7</td>
<td id="S4.T2.7.1.22.20.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">46.8</td>
<td id="S4.T2.7.1.22.20.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">79.9</td>
<td id="S4.T2.7.1.22.20.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">75.5</td>
</tr>
<tr id="S4.T2.7.1.23.21" class="ltx_tr">
<th id="S4.T2.7.1.23.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">LinK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>
</th>
<td id="S4.T2.7.1.23.21.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71.0</td>
<td id="S4.T2.7.1.23.21.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">66.3</td>
<td id="S4.T2.7.1.23.21.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">86.1</td>
<td id="S4.T2.7.1.23.21.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">55.7</td>
<td id="S4.T2.7.1.23.21.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">65.7</td>
<td id="S4.T2.7.1.23.21.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">62.1</td>
<td id="S4.T2.7.1.23.21.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">30.9</td>
<td id="S4.T2.7.1.23.21.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">85.8</td>
<td id="S4.T2.7.1.23.21.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">73.5</td>
<td id="S4.T2.7.1.23.21.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">47.5</td>
<td id="S4.T2.7.1.23.21.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">80.4</td>
<td id="S4.T2.7.1.23.21.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">75.5</td>
</tr>
<tr id="S4.T2.7.1.24.22" class="ltx_tr" style="background-color:#CCFFFF;">
<th id="S4.T2.7.1.24.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.1.1" class="ltx_text" style="background-color:#CCFFFF;">HEDNet (Ours)</span></th>
<td id="S4.T2.7.1.24.22.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.2.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">72.0</span></td>
<td id="S4.T2.7.1.24.22.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.3.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">67.7</span></td>
<td id="S4.T2.7.1.24.22.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.4.1" class="ltx_text" style="background-color:#CCFFFF;">87.1</span></td>
<td id="S4.T2.7.1.24.22.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.5.1" class="ltx_text" style="background-color:#CCFFFF;">56.5</span></td>
<td id="S4.T2.7.1.24.22.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.6.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">70.4</span></td>
<td id="S4.T2.7.1.24.22.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.7.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">63.5</span></td>
<td id="S4.T2.7.1.24.22.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.8.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFFF;">33.6</span></td>
<td id="S4.T2.7.1.24.22.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.9.1" class="ltx_text" style="background-color:#CCFFFF;">87.9</span></td>
<td id="S4.T2.7.1.24.22.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.10.1" class="ltx_text" style="background-color:#CCFFFF;">70.4</span></td>
<td id="S4.T2.7.1.24.22.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.11.1" class="ltx_text" style="background-color:#CCFFFF;">44.8</span></td>
<td id="S4.T2.7.1.24.22.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.12.1" class="ltx_text" style="background-color:#CCFFFF;">85.1</span></td>
<td id="S4.T2.7.1.24.22.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.7.1.24.22.13.1" class="ltx_text" style="background-color:#CCFFFF;">78.1</span></td>
</tr>
<tr id="S4.T2.7.1.25.23" class="ltx_tr">
<th id="S4.T2.7.1.25.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></th>
<td id="S4.T2.7.1.25.23.2" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.7.1.25.23.3" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.7.1.25.23.4" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.7.1.25.23.5" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.7.1.25.23.6" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.7.1.25.23.7" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.7.1.25.23.8" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.7.1.25.23.9" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.7.1.25.23.10" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.7.1.25.23.11" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.7.1.25.23.12" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.7.1.25.23.13" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison with prior methods on the nuScenes dataset. Metrics: NDS (%)<math id="S4.T2.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.4.m1.1b"><mo stretchy="false" id="S4.T2.4.m1.1.1" xref="S4.T2.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.m1.1c"><ci id="S4.T2.4.m1.1.1.cmml" xref="S4.T2.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.m1.1d">\uparrow</annotation></semantics></math> and mAP (%)<math id="S4.T2.5.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.5.m2.1b"><mo stretchy="false" id="S4.T2.5.m2.1.1" xref="S4.T2.5.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.m2.1c"><ci id="S4.T2.5.m2.1.1.cmml" xref="S4.T2.5.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.m2.1d">\uparrow</annotation></semantics></math> for the overall results, AP (%)<math id="S4.T2.6.m3.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.6.m3.1b"><mo stretchy="false" id="S4.T2.6.m3.1.1" xref="S4.T2.6.m3.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.m3.1c"><ci id="S4.T2.6.m3.1.1.cmml" xref="S4.T2.6.m3.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.m3.1d">\uparrow</annotation></semantics></math> for each category. ‘T.L.’, ‘C.V.’, ‘Ped.’, ‘M.T.’, ‘T.C.’, and ’B.R.’ denote trailer, construction vehicle, pedestrian, motor, traffic cone, and barrier, respectively.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:208.1pt;"><img src="/html/2310.20234/assets/x4.png" id="S4.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="350" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Detection accuracy (L2 mAPH) versus inference speed (FPS<math id="S4.F5.3.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.F5.3.2.m1.1b"><mo stretchy="false" id="S4.F5.3.2.m1.1.1" xref="S4.F5.3.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F5.3.2.m1.1c"><ci id="S4.F5.3.2.m1.1.1.cmml" xref="S4.F5.3.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.3.2.m1.1d">\uparrow</annotation></semantics></math>) of different models on the Waymo Open <span id="S4.F5.3.4.1" class="ltx_text ltx_font_italic">validation</span> set.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:208.1pt;"><img src="/html/2310.20234/assets/x5.png" id="S4.F5.4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="437" height="348" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Improvements decomposition of HEDNet over HEDNet-single with regard to the distance range of objects to the LiDAR sensor. </figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison with state-of-the-art methods</h3>

<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Results on the Waymo Open dataset.</h5>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">We compared the proposed HEDNet with previous methods on the Waymo Open dataset (Table <a href="#S3.T1" title="Table 1 ‣ 3.3 HEDNet ‣ 3 Method ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). On the validation set, HEDNet yielded 1.3% L2 mAP and 1.3% L2 mAPH improvements over the prior best method DSVT-Voxel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>. HEDNet also outperformed the two-stage models PV-RCNN++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> and FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>. More importantly, our method significantly outperformed the transformer-based DSVT-Voxel by 1.7% L2 mAPH on the vehicle category, where the average scale of vehicles is 10 times bigger than pedestrians and cyclists.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Results on the nuScenes dataset.</h5>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">We compared HEDNet with previous top-performing methods on the nuScenes dataset (Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Implementation details ‣ 4 Experiments ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). On the nuScenes test set, HEDNet achieved impressive results with 72.0% NDS and 67.7% mAP. Compared with TransFusion-L (which adopts the same head as HEDNet), HEDNet showcased significant improvements, with a gain of 1.8% NDS and 2.2% mAP. In addition, on the three categories with large objects, namely bus, trailer (T.L.), and construction vehicle (C.V.), HEDNet outperformed TransFusion-L by 4.1%, 4.7%, and 5.4% mAP, respectively. These results further demonstrate the effectiveness of our method.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Inference speed.</h5>

<div id="S4.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px3.p1.1" class="ltx_p">We further compared HEDNet with previous leading methods in terms of detection accuracy and inference speed, as depicted in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Implementation details ‣ 4 Experiments ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Remarkably, HEDNet achieved superior detection accuracy compared with LargeKernel3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> and DSVT-Voxel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> with faster inference speed. Note that LargeKernel3D and DSVT-Voxel were developed based on large-kernel CNN and transformer, respectively. All models were evaluated on the same NVIDIA RTX 3090 GPU.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation studies</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.6" class="ltx_p">To better investigate the effectiveness of HEDNet, we constructed two network variants: HEDNet-single and HEDNet-2D. For the HEDNet-single, we replaced all the SED and DED blocks in HEDNet with single-scale blocks, <span id="S4.SS4.p1.6.1" class="ltx_text ltx_font_italic">i.e.,</span>only keeping the first <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">m</annotation></semantics></math> SSR/DR blocks in each SED/DED block. For the HEDNet-2D, we replaced all 3D sparse convolutions in HEDNet with 2D sparse convolutions and removed the three feature down-sampling layers that follow the feature maps F<sub id="S4.SS4.p1.6.2" class="ltx_sub">1</sub>, F<sub id="S4.SS4.p1.6.3" class="ltx_sub">2</sub>, and F<sub id="S4.SS4.p1.6.4" class="ltx_sub">3</sub>, following the settings in DSVT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>. The two SSR blocks after F<sub id="S4.SS4.p1.6.5" class="ltx_sub">0</sub> were also removed. In HEDNet-2D, the resolution of the output feature map of the 2D dense backbone is same as that of the network input <math id="S4.SS4.p1.6.m6.1" class="ltx_Math" alttext="F_{0}" display="inline"><semantics id="S4.SS4.p1.6.m6.1a"><msub id="S4.SS4.p1.6.m6.1.1" xref="S4.SS4.p1.6.m6.1.1.cmml"><mi id="S4.SS4.p1.6.m6.1.1.2" xref="S4.SS4.p1.6.m6.1.1.2.cmml">F</mi><mn id="S4.SS4.p1.6.m6.1.1.3" xref="S4.SS4.p1.6.m6.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.6.m6.1b"><apply id="S4.SS4.p1.6.m6.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.6.m6.1.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS4.p1.6.m6.1.1.2.cmml" xref="S4.SS4.p1.6.m6.1.1.2">𝐹</ci><cn type="integer" id="S4.SS4.p1.6.m6.1.1.3.cmml" xref="S4.SS4.p1.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.6.m6.1c">F_{0}</annotation></semantics></math>. We conducted experiments on the Waymo Open dataset to analyze various design choices of HEDNet. All models were trained on a 20% training subset and evaluated on the validation set.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Model designs</h4>

<section id="S4.SS4.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Effectiveness of the SED block.</h5>

<div id="S4.SS4.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.Px1.p1.1" class="ltx_p">We compared the models built with RSR block, SSR block, and our proposed SED block in Table <a href="#S4.T3" title="Table 3 ‣ Effectiveness of the DED block. ‣ 4.4.1 Model designs ‣ 4.4 Ablation studies ‣ 4 Experiments ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a). For the models with RSR/SSR blocks, we replaced the SED blocks in HEDNet with RSR/SSR blocks. The 2D dense backbones in the first three models were removed to fully explore the potential of the three structures. The first model with RSR blocks achieved slightly better results than the second model with SSR blocks but with much higher runtime latency. The third model with SED blocks significantly outperformed the second model with SSR blocks by 1.96% L2 mAPH. Similar gains can be observed in the last two models with DED blocks.</p>
</div>
</section>
<section id="S4.SS4.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Effectiveness of the DED block.</h5>

<div id="S4.SS4.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS1.Px2.p1.1" class="ltx_p">The DED block is designed to expand sparse features towards object centers. We compared the models that include different numbers of DED blocks in Table <a href="#S4.T3" title="Table 3 ‣ Effectiveness of the DED block. ‣ 4.4.1 Model designs ‣ 4.4 Ablation studies ‣ 4 Experiments ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b). The models with DED blocks achieved large improvements over the model without DED blocks. The model with five blocks performed worse than the model with four blocks. The former may be overfitted to the training data. We adopted four DED blocks for HEDNet by default.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T3.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:208.1pt;">
<table id="S4.T3.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.2.3.1" class="ltx_tr">
<th id="S4.T3.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Block</th>
<th id="S4.T3.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Latency</th>
<th id="S4.T3.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">L1 mAPH</th>
<th id="S4.T3.2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">L2 mAPH</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2.4.1" class="ltx_tr">
<th id="S4.T3.2.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">RSR block</th>
<td id="S4.T3.2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">176 ms</td>
<td id="S4.T3.2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">74.61</td>
<td id="S4.T3.2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">68.30</td>
</tr>
<tr id="S4.T3.2.2.5.2" class="ltx_tr">
<th id="S4.T3.2.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SSR block</th>
<td id="S4.T3.2.2.5.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">43 ms</td>
<td id="S4.T3.2.2.5.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">74.42</td>
<td id="S4.T3.2.2.5.2.4" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">67.93</td>
</tr>
<tr id="S4.T3.2.2.6.3" class="ltx_tr">
<th id="S4.T3.2.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SED block</th>
<td id="S4.T3.2.2.6.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">48 ms</td>
<td id="S4.T3.2.2.6.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">76.13</td>
<td id="S4.T3.2.2.6.3.4" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">69.89</td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SSR block<sup id="S4.T3.1.1.1.1.1" class="ltx_sup"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</th>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">49 ms</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">76.67</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">70.49</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">SED block<sup id="S4.T3.2.2.2.1.1" class="ltx_sup"><span id="S4.T3.2.2.2.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</th>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">54 ms</td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">77.39</td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">71.37</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering">(a) Effectiveness of the SED block.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T3.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:208.1pt;">
<table id="S4.T3.fig1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.fig1.1.1.1" class="ltx_tr">
<th id="S4.T3.fig1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">#Block</th>
<th id="S4.T3.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Latency</th>
<th id="S4.T3.fig1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">L1 mAPH</th>
<th id="S4.T3.fig1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">L2 mAPH</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.fig1.1.2.1" class="ltx_tr">
<th id="S4.T3.fig1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">0</th>
<td id="S4.T3.fig1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">48 ms</td>
<td id="S4.T3.fig1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">76.13</td>
<td id="S4.T3.fig1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">69.89</td>
</tr>
<tr id="S4.T3.fig1.1.3.2" class="ltx_tr">
<th id="S4.T3.fig1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">1</th>
<td id="S4.T3.fig1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">54 ms</td>
<td id="S4.T3.fig1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">77.56</td>
<td id="S4.T3.fig1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">71.37</td>
</tr>
<tr id="S4.T3.fig1.1.4.3" class="ltx_tr">
<th id="S4.T3.fig1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">3</th>
<td id="S4.T3.fig1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">63 ms</td>
<td id="S4.T3.fig1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">77.75</td>
<td id="S4.T3.fig1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">71.64</td>
</tr>
<tr id="S4.T3.fig1.1.5.4" class="ltx_tr">
<th id="S4.T3.fig1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">4</th>
<td id="S4.T3.fig1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">67 ms</td>
<td id="S4.T3.fig1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.02</td>
<td id="S4.T3.fig1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">71.92</td>
</tr>
<tr id="S4.T3.fig1.1.6.5" class="ltx_tr">
<th id="S4.T3.fig1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">5</th>
<td id="S4.T3.fig1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">73 ms</td>
<td id="S4.T3.fig1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">77.85</td>
<td id="S4.T3.fig1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">71.77</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering">(b) Effectiveness of the DED block.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T3.fig2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:229.8pt;">
<table id="S4.T3.fig2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.fig2.1.1.1" class="ltx_tr">
<th id="S4.T3.fig2.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">Sparse back.</th>
<th id="S4.T3.fig2.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">Dense back.</th>
<th id="S4.T3.fig2.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">L1 mAPH</th>
<th id="S4.T3.fig2.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">L2 mAPH</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.fig2.1.2.1" class="ltx_tr">
<td id="S4.T3.fig2.1.2.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">8 SSR blocks</td>
<td id="S4.T3.fig2.1.2.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">1 DED block</td>
<td id="S4.T3.fig2.1.2.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">73.91</td>
<td id="S4.T3.fig2.1.2.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">67.80</td>
</tr>
<tr id="S4.T3.fig2.1.3.2" class="ltx_tr">
<td id="S4.T3.fig2.1.3.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.4pt;padding-right:1.4pt;">16 SSR blocks</td>
<td id="S4.T3.fig2.1.3.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.4pt;padding-right:1.4pt;">1 DED block</td>
<td id="S4.T3.fig2.1.3.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.4pt;padding-right:1.4pt;">73.95</td>
<td id="S4.T3.fig2.1.3.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">67.82</td>
</tr>
<tr id="S4.T3.fig2.1.4.3" class="ltx_tr">
<td id="S4.T3.fig2.1.4.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.4pt;padding-right:1.4pt;">4 SED blocks</td>
<td id="S4.T3.fig2.1.4.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.4pt;padding-right:1.4pt;">1 DED block</td>
<td id="S4.T3.fig2.1.4.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.4pt;padding-right:1.4pt;">75.39</td>
<td id="S4.T3.fig2.1.4.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">69.42</td>
</tr>
<tr id="S4.T3.fig2.1.5.4" class="ltx_tr">
<td id="S4.T3.fig2.1.5.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:1.4pt;padding-right:1.4pt;">4 SED blocks</td>
<td id="S4.T3.fig2.1.5.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:1.4pt;padding-right:1.4pt;">2 DED blocks</td>
<td id="S4.T3.fig2.1.5.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:1.4pt;padding-right:1.4pt;">75.62</td>
<td id="S4.T3.fig2.1.5.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">69.67</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering">(c) HEDNet with 2D sparse backbone.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T3.fig3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:190.8pt;">
<table id="S4.T3.fig3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.fig3.1.1.1" class="ltx_tr">
<th id="S4.T3.fig3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">#Scale</th>
<th id="S4.T3.fig3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Latency</th>
<th id="S4.T3.fig3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">L1 mAPH</th>
<th id="S4.T3.fig3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">L2 mAPH</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.fig3.1.2.1" class="ltx_tr" style="background-color:#D9D9D9;">
<th id="S4.T3.fig3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T3.fig3.1.2.1.1.1" class="ltx_text" style="background-color:#D9D9D9;">1</span></th>
<td id="S4.T3.fig3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T3.fig3.1.2.1.2.1" class="ltx_text" style="background-color:#D9D9D9;">43 ms</span></td>
<td id="S4.T3.fig3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T3.fig3.1.2.1.3.1" class="ltx_text" style="background-color:#D9D9D9;">76.18</span></td>
<td id="S4.T3.fig3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T3.fig3.1.2.1.4.1" class="ltx_text" style="background-color:#D9D9D9;">69.88</span></td>
</tr>
<tr id="S4.T3.fig3.1.3.2" class="ltx_tr">
<th id="S4.T3.fig3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">2</th>
<td id="S4.T3.fig3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">59 ms</td>
<td id="S4.T3.fig3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">77.61</td>
<td id="S4.T3.fig3.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">71.44</td>
</tr>
<tr id="S4.T3.fig3.1.4.3" class="ltx_tr" style="background-color:#CCFFFF;">
<th id="S4.T3.fig3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T3.fig3.1.4.3.1.1" class="ltx_text" style="background-color:#CCFFFF;">3</span></th>
<td id="S4.T3.fig3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T3.fig3.1.4.3.2.1" class="ltx_text" style="background-color:#CCFFFF;">67 ms</span></td>
<td id="S4.T3.fig3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T3.fig3.1.4.3.3.1" class="ltx_text" style="background-color:#CCFFFF;">78.02</span></td>
<td id="S4.T3.fig3.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T3.fig3.1.4.3.4.1" class="ltx_text" style="background-color:#CCFFFF;">71.92</span></td>
</tr>
<tr id="S4.T3.fig3.1.5.4" class="ltx_tr">
<th id="S4.T3.fig3.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">4</th>
<td id="S4.T3.fig3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78 ms</td>
<td id="S4.T3.fig3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">78.12</td>
<td id="S4.T3.fig3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">72.02</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering">(d) Effectiveness of encoder-decoder design.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablations on the Waymo Open. <sup id="S4.T3.6.1" class="ltx_sup"><span id="S4.T3.6.1.1" class="ltx_text ltx_font_italic">†</span></sup>: with 1 DED block. In (c), ‘back.’ denotes backbone. In (d), the gray line denotes the HEDNet-single, and the blue line denotes the default HEDNet. </figcaption>
</figure>
</section>
<section id="S4.SS4.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">HEDNet with 2D sparse backbone.</h5>

<div id="S4.SS4.SSS1.Px3.p1" class="ltx_para">
<p id="S4.SS4.SSS1.Px3.p1.1" class="ltx_p">We conducted experiments on HEDNet-2D to evaluate the effectiveness of our method with 2D inputs. For the construction of 2D inputs, we set the voxel size to (0.32m, 0.32m, 6m), where the size of 6m in the Z axis corresponds to the full size of the input point clouds. To compare our SED blocks with SSR blocks, we replaced each SED block in HEDNet-2D with 2 SSR blocks or 4 SSR blocks, resulting in two models of different sizes (the first two models in Table <a href="#S4.T3" title="Table 3 ‣ Effectiveness of the DED block. ‣ 4.4.1 Model designs ‣ 4.4 Ablation studies ‣ 4 Experiments ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (c)). From Table <a href="#S4.T3" title="Table 3 ‣ Effectiveness of the DED block. ‣ 4.4.1 Model designs ‣ 4.4 Ablation studies ‣ 4 Experiments ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (c), we can make the following observations. Firstly, the model with 16 SSR blocks achieved similar performance to the model with 8 SSR blocks, indicating that <span id="S4.SS4.SSS1.Px3.p1.1.1" class="ltx_text ltx_font_italic">stacking more SSR blocks could not further boost performance</span>. Secondly, the models incorporating SED blocks showed significant improvements over the models using SSR blocks (at least 1.6% gains on L2 mAPH). This observation demonstrates the effectiveness of our SED block. Thirdly, stacking two DED blocks achieved better performance than using a single one. These results clearly demonstrate the generality and effectiveness of our proposed SED block and DED block.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2310.20234/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="266" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Qualitative results on the Waymo Open. The red boxes are annotated by humans. The blue boxes and green boxes are predicted by HEDNet and the HEDNet-single, respectively. Red points correspond to the points that fall inside the human-annotated boxes. HEDNet predicted more precise bounding boxes for the objects marked by red arrows than the single-scale variant HEDNet-single.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>HEDNet versus HEDNet-single</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">We conducted a thorough comparison between the proposed HEDNet and its single-scale variant, HEDNet-single, to explore the effectiveness of the encoder-decoder structure and investigate which objects benefit from HEDNet the most. Please note that the HEDNet is designed to capture long-range dependencies among features in the spatial space, which is the core of this work.</p>
</div>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<p id="S4.SS4.SSS2.p2.1" class="ltx_p"><span id="S4.SS4.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Firstly,</span> we compared the models built with blocks of different numbers of scales to explore the effectiveness of the encoder-decoder structure. As shown in Table <a href="#S4.T3" title="Table 3 ‣ Effectiveness of the DED block. ‣ 4.4.1 Model designs ‣ 4.4 Ablation studies ‣ 4 Experiments ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (d), the models with multi-scale blocks significantly outperformed the single-scale variant HEDNet-single (the line in gray color). Using more scales achieved better performance, but introduced higher runtime latency. To strike a balance between accuracy and efficiency, we adopted three-scale blocks for HEDNet by default.</p>
</div>
<div id="S4.SS4.SSS2.p3" class="ltx_para">
<p id="S4.SS4.SSS2.p3.1" class="ltx_p"><span id="S4.SS4.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Secondly,</span> we evaluated the three-scale HEDNet and the HEDNet-single in Table <a href="#S4.T3" title="Table 3 ‣ Effectiveness of the DED block. ‣ 4.4.1 Model designs ‣ 4.4 Ablation studies ‣ 4 Experiments ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (d) separately for each category and analyzed the results based on the distance range of objects to the LiDAR sensor. We illustrate the accuracy improvements of HEDNet over HEDNet-single at various distance ranges in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Implementation details ‣ 4 Experiments ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Firstly, HEDNet showed significant improvements over HEDNet-single on the vehicle category, where the size of vehicles is 10 times larger than that of pedestrians and cyclists. This highlights the importance of capturing long-range dependencies for accurately detecting large objects. Furthermore, HEDNet achieved larger performance gains on distant objects compared with objects closer to the LiDAR sensor across all three categories. We believe this is because distant objects with fewer point clouds require more contextual information for accurate detection. Overall, these results demonstrate the effectiveness of our proposed method in detecting large and distant objects.</p>
</div>
<div id="S4.SS4.SSS2.p4" class="ltx_para">
<p id="S4.SS4.SSS2.p4.1" class="ltx_p"><span id="S4.SS4.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Thirdly,</span> we further present some visualization results of the two models in Figure <a href="#S4.F6" title="Figure 6 ‣ HEDNet with 2D sparse backbone. ‣ 4.4.1 Model designs ‣ 4.4 Ablation studies ‣ 4 Experiments ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. HEDNet-single exhibited limitations in accurately predicting boxes for large objects and the predicted boxes often only covered parts of the objects (see the top row). In addition, when dealing with objects containing a few points, HEDNet-single struggled to accurately estimate their orientations (see the bottom row). In contrast, HEDNet predicted more precise bounding boxes for both scenarios, which we believe is owed to the ability of HEDNet to capture long-range dependencies.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We propose a sparse encoder-decoder structure named SED block to capture long-range dependencies among features in the spatial space. Further, we propose a dense encoder-decoder structure named DED block to expand sparse features towards object centers. With the SED and DED blocks, we introduce a hierarchical encoder-decoder network named HEDNet for 3D object detection in point clouds. HEDNet achieved a new state-of-the-art performance on both the Waymo Open and nuScenes datasets, which demonstrates the effectiveness of our method. We hope that our work can provide some inspiration for the backbone design in 3D object detection.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Limitations</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">HEDNet mainly focuses on 3D object detection in outdoor autonomous driving scenarios. However, the application of HEDNet in other indoor applications is still an open problem.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Acknowledgements</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">This work was supported in part by the National Key Research and Development Program of China (No. 2021ZD0200301) and the National Natural Science Foundation of China (Nos. U19B2034, 61836014) and THU-Bosch JCML center.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.4.1" class="ltx_text" style="font-size:90%;">
Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.5.1" class="ltx_text" style="font-size:90%;">Frustum pointnets for 3d object detection from rgb-d data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib1.8.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.2.2.1" class="ltx_text" style="font-size:90%;">[2]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.5.1" class="ltx_text" style="font-size:90%;">Pointrcnn: 3d object proposal generation and detection from point
cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib2.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.2.2.1" class="ltx_text" style="font-size:90%;">[3]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="font-size:90%;">
Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.5.1" class="ltx_text" style="font-size:90%;">3dssd: Point-based 3d single stage object detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib3.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.2.2.1" class="ltx_text" style="font-size:90%;">[4]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.5.1" class="ltx_text" style="font-size:90%;">Deep hough voting for 3d object detection in point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib4.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.2.2.1" class="ltx_text" style="font-size:90%;">[5]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="font-size:90%;">
Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.5.1" class="ltx_text" style="font-size:90%;">Fast point r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib5.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.2.2.1" class="ltx_text" style="font-size:90%;">[6]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">
Alex Bewley, Pei Sun, Thomas Mensink, Dragomir Anguelov, and Cristian
Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.5.1" class="ltx_text" style="font-size:90%;">Range conditioned dilated convolutions for scale invariant 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRL</span><span id="bib.bib6.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.2.2.1" class="ltx_text" style="font-size:90%;">[7]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="font-size:90%;">
Lue Fan, Xuan Xiong, Feng Wang, Naiyan Wang, and ZhaoXiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.5.1" class="ltx_text" style="font-size:90%;">Rangedet: In defense of range view for lidar-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib7.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.2.2.1" class="ltx_text" style="font-size:90%;">[8]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Weiyue Wang, Yuning Chai, Gamaleldin Elsayed, Alex Bewley, Xiao Zhang,
Cristian Sminchisescu, and Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.5.1" class="ltx_text" style="font-size:90%;">RSN: Range sparse net for efficient, accurate lidar 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.2.2.1" class="ltx_text" style="font-size:90%;">[9]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.4.1" class="ltx_text" style="font-size:90%;">
Zhi Tian, Xiangxiang Chu, Xiaoming Wang, Xiaolin Wei, and Chunhua Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.5.1" class="ltx_text" style="font-size:90%;">Fully convolutional one-stage 3d object detection on liDAR range
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib9.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.2.2.1" class="ltx_text" style="font-size:90%;">[10]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.4.1" class="ltx_text" style="font-size:90%;">
Danila Rukhovich, Anna Vorontsova, and Anton Konushin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.5.1" class="ltx_text" style="font-size:90%;">Fcaf3d: fully convolutional anchor-free 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib10.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.2.2.1" class="ltx_text" style="font-size:90%;">[11]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.4.1" class="ltx_text" style="font-size:90%;">
Guangsheng Shi, Ruifeng Li, and Chao Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.5.1" class="ltx_text" style="font-size:90%;">Pillarnet: Real-time and high-performance pillar-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib11.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.2.2.1" class="ltx_text" style="font-size:90%;">[12]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Jianhui Liu, Xiaojuan Qi, Xiangyu Zhang, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.5.1" class="ltx_text" style="font-size:90%;">Scaling up kernels in 3d cnns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib12.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.2.2.1" class="ltx_text" style="font-size:90%;">[13]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Mingxing Tan, Weiyue Wang, Chenxi Liu, Fei Xia, Zhaoqi Leng, and
Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.5.1" class="ltx_text" style="font-size:90%;">Swformer: Sparse window transformer for 3d object detection in point
clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib13.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.2.2.1" class="ltx_text" style="font-size:90%;">[14]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="font-size:90%;">
Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan
Wang, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.5.1" class="ltx_text" style="font-size:90%;">Embracing Single Stride 3D Object Detector with Sparse Transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib14.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.2.2.1" class="ltx_text" style="font-size:90%;">[15]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.4.1" class="ltx_text" style="font-size:90%;">
Haiyang Wang, Chen Shi, Shaoshuai Shi, Meng Lei, Sen Wang, Di He, Bernt
Schiele, and Liwei Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.5.1" class="ltx_text" style="font-size:90%;">Dsvt: Dynamic sparse voxel transformer with rotated sets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib15.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.2.2.1" class="ltx_text" style="font-size:90%;">[16]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="font-size:90%;">
Tao Lu, Xiang Ding, Haisong Liu, Gangshan Wu, and Limin Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.5.1" class="ltx_text" style="font-size:90%;">Link: Linear kernel for lidar-based 3d perception.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.2.2.1" class="ltx_text" style="font-size:90%;">[17]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.4.1" class="ltx_text" style="font-size:90%;">
Yin Zhou and Oncel Tuzel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.5.1" class="ltx_text" style="font-size:90%;">Voxelnet: End-to-end learning for point cloud based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.8.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.2.2.1" class="ltx_text" style="font-size:90%;">[18]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="font-size:90%;">
Yan Yan, Yuxing Mao, and Bo Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.5.1" class="ltx_text" style="font-size:90%;">Second: Sparsely embedded convolutional detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib18.8.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.2.2.1" class="ltx_text" style="font-size:90%;">[19]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">
Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar
Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.5.1" class="ltx_text" style="font-size:90%;">Pointpillars: Fast encoders for object detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib19.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.2.2.1" class="ltx_text" style="font-size:90%;">[20]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="font-size:90%;">
Benjamin Graham and Laurens Van der Maaten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.5.1" class="ltx_text" style="font-size:90%;">Submanifold sparse convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1706.01307</span><span id="bib.bib20.7.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.2.2.1" class="ltx_text" style="font-size:90%;">[21]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">
Ben Graham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.5.1" class="ltx_text" style="font-size:90%;">Sparse 3d convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1505.02890</span><span id="bib.bib21.7.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.2.2.1" class="ltx_text" style="font-size:90%;">[22]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and
Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.5.1" class="ltx_text" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib22.8.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.2.2.1" class="ltx_text" style="font-size:90%;">[23]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.4.1" class="ltx_text" style="font-size:90%;">
Fisher Yu, Dequan Wang, Trevor Darrell, and Yi Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.5.1" class="ltx_text" style="font-size:90%;">Deep layer aggregation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib23.8.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.2.2.1" class="ltx_text" style="font-size:90%;">[24]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="font-size:90%;">
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.5.1" class="ltx_text" style="font-size:90%;">Pyramid scene parsing network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib24.8.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.2.2.1" class="ltx_text" style="font-size:90%;">[25]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="font-size:90%;">
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig
Adam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.5.1" class="ltx_text" style="font-size:90%;">Encoder-decoder with atrous separable convolution for semantic image
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib25.8.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.2.2.1" class="ltx_text" style="font-size:90%;">[26]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="font-size:90%;">
Gang Zhang, Ziyi Li, Jianmin Li, and Xiaolin Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.5.1" class="ltx_text" style="font-size:90%;">Cfnet: Cascade fusion network for dense prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.06052</span><span id="bib.bib26.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.2.2.1" class="ltx_text" style="font-size:90%;">[27]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.4.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.5.1" class="ltx_text" style="font-size:90%;">Center-based 3d object detection and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib27.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.2.2.1" class="ltx_text" style="font-size:90%;">[28]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text" style="font-size:90%;">
Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and
Chiew-Lan Tai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.5.1" class="ltx_text" style="font-size:90%;">Transfusion: Robust lidar-camera fusion for 3d object detection with
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib28.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.2.2.1" class="ltx_text" style="font-size:90%;">[29]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay
Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott
Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens,
Zhifeng Chen, and Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.5.1" class="ltx_text" style="font-size:90%;">Scalability in perception for autonomous driving: Waymo open dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib29.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.2.2.1" class="ltx_text" style="font-size:90%;">[30]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.4.1" class="ltx_text" style="font-size:90%;">
Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong,
Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.5.1" class="ltx_text" style="font-size:90%;">nuscenes: A multimodal dataset for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib30.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.2.2.1" class="ltx_text" style="font-size:90%;">[31]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.5.1" class="ltx_text" style="font-size:90%;">Pointnet: Deep learning on point sets for 3d classification and
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib31.8.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.2.2.1" class="ltx_text" style="font-size:90%;">[32]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.5.1" class="ltx_text" style="font-size:90%;">Pointnet++: Deep hierarchical feature learning on point sets in a
metric space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib32.8.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.2.2.1" class="ltx_text" style="font-size:90%;">[33]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.4.1" class="ltx_text" style="font-size:90%;">
Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.5.1" class="ltx_text" style="font-size:90%;">Scaling up your kernels to 31x31: Revisiting large kernel design in
cnns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib33.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.2.2.1" class="ltx_text" style="font-size:90%;">[34]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="font-size:90%;">
Mingxing Tan, Ruoming Pang, and Quoc V. Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.5.1" class="ltx_text" style="font-size:90%;">Efficientdet: Scalable and efficient object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib34.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.2.2.1" class="ltx_text" style="font-size:90%;">[35]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="font-size:90%;">
Golnaz Ghiasi, Tsung-Yi Lin, Ruoming Pang, and Quoc V. Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.5.1" class="ltx_text" style="font-size:90%;">Nas-fpn: Learning scalable feature pyramid architecture for object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib35.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.2.2.1" class="ltx_text" style="font-size:90%;">[36]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.4.1" class="ltx_text" style="font-size:90%;">
Xinqi Fan, Mingjie Jiang, Ali Raza Shahid, and Yan Hong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.5.1" class="ltx_text" style="font-size:90%;">Hierarchical scale convolutional neural network for facial expression
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Cognitive Neurodynamics</span><span id="bib.bib36.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.2.2.1" class="ltx_text" style="font-size:90%;">[37]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.4.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.5.1" class="ltx_text" style="font-size:90%;">From points to parts: 3d object detection from point cloud with
part-aware and part-aggregation network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib37.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.2.2.1" class="ltx_text" style="font-size:90%;">[38]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.4.1" class="ltx_text" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.5.1" class="ltx_text" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">MICCAI</span><span id="bib.bib38.8.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.2.2.1" class="ltx_text" style="font-size:90%;">[39]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.4.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.5.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib39.8.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.2.2.1" class="ltx_text" style="font-size:90%;">[40]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.4.1" class="ltx_text" style="font-size:90%;">
Spconv Contributors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.5.1" class="ltx_text" style="font-size:90%;">Spconv: Spatially sparse convolution library.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/traveller59/spconv" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/traveller59/spconv</a><span id="bib.bib40.6.1" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.2.2.1" class="ltx_text" style="font-size:90%;">[41]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.4.1" class="ltx_text" style="font-size:90%;">
Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang Gao, Tom Ouyang, James
Guo, Jiquan Ngiam, and Vijay Vasudevan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.5.1" class="ltx_text" style="font-size:90%;">End-to-end multi-view fusion for 3d object detection in lidar point
clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRL</span><span id="bib.bib41.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.2.2.1" class="ltx_text" style="font-size:90%;">[42]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.4.1" class="ltx_text" style="font-size:90%;">
Zhichao Li, Feng Wang, and Naiyan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.5.1" class="ltx_text" style="font-size:90%;">Lidar r-cnn: An efficient and universal 3d object detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib42.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.2.2.1" class="ltx_text" style="font-size:90%;">[43]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.4.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and
Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.5.1" class="ltx_text" style="font-size:90%;">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib43.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.2.2.1" class="ltx_text" style="font-size:90%;">[44]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.4.1" class="ltx_text" style="font-size:90%;">
Chao Zhou, Yanan Zhang, Jiaxin Chen, and Di Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.5.1" class="ltx_text" style="font-size:90%;">Octr: Octree-based transformer for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib44.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.2.2.1" class="ltx_text" style="font-size:90%;">[45]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.4.1" class="ltx_text" style="font-size:90%;">
Yihan Hu, Zhuangzhuang Ding, Runzhou Ge, Wenxin Shao, Li Huang, Kun Li, and
Qiang Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.5.1" class="ltx_text" style="font-size:90%;">Afdetv2: Rethinking the necessity of the second stage for object
detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib45.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.2.2.1" class="ltx_text" style="font-size:90%;">[46]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.4.1" class="ltx_text" style="font-size:90%;">
Zixiang Zhou, Xiangchen Zhao, Yu Wang, Panqu Wang, and Hassan Foroosh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.5.1" class="ltx_text" style="font-size:90%;">Centerformer: Center-based transformer for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib46.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.2.2.1" class="ltx_text" style="font-size:90%;">[47]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.4.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi,
Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.5.1" class="ltx_text" style="font-size:90%;">Pv-rcnn++: Point-voxel feature set abstraction with local vector
representation for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib47.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.2.2.1" class="ltx_text" style="font-size:90%;">[48]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.4.1" class="ltx_text" style="font-size:90%;">
Lue Fan, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.5.1" class="ltx_text" style="font-size:90%;">Fully Sparse 3D Object Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib48.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.2.2.1" class="ltx_text" style="font-size:90%;">[49]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.4.1" class="ltx_text" style="font-size:90%;">
OpenPCDet Development Team.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.5.1" class="ltx_text" style="font-size:90%;">Openpcdet: An open-source toolbox for 3d object detection from point
clouds.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/open-mmlab/OpenPCDet" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/open-mmlab/OpenPCDet</a><span id="bib.bib49.6.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.2.2.1" class="ltx_text" style="font-size:90%;">[50]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.4.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.5.1" class="ltx_text" style="font-size:90%;">Voxelnext: Fully sparse voxelnet for 3d object detection and
tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib50.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.2.2.1" class="ltx_text" style="font-size:90%;">[51]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.4.1" class="ltx_text" style="font-size:90%;">
Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.5.1" class="ltx_text" style="font-size:90%;">Class-balanced grouping and sampling for point cloud 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.09492</span><span id="bib.bib51.7.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.2.2.1" class="ltx_text" style="font-size:90%;">[52]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.4.1" class="ltx_text" style="font-size:90%;">
Qi Chen, Lin Sun, Zhixin Wang, Kui Jia, and Alan Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.5.1" class="ltx_text" style="font-size:90%;">Object as hotspots: An anchor-free 3d object detection approach via
firing of hotspots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1912.12791</span><span id="bib.bib52.7.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.2.2.1" class="ltx_text" style="font-size:90%;">[53]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.4.1" class="ltx_text" style="font-size:90%;">
Xiangyuan Zhu, Kehua Guo, Hui Fang, Liang Chen, Sheng Ren, and Bin Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.5.1" class="ltx_text" style="font-size:90%;">Cross view capture for stereo image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib53.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.2.2.1" class="ltx_text" style="font-size:90%;">[54]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.4.1" class="ltx_text" style="font-size:90%;">
Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.5.1" class="ltx_text" style="font-size:90%;">Unifying voxel-based representation with transformer for 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib54.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.2.2.1" class="ltx_text" style="font-size:90%;">[55]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.4.1" class="ltx_text" style="font-size:90%;">
Shengheng Deng, Zhihao Liang, Lin Sun, and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.5.1" class="ltx_text" style="font-size:90%;">Vista: Boosting 3d object detection via dual cross-view spatial
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib55.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.2.2.1" class="ltx_text" style="font-size:90%;">[56]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.4.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.5.1" class="ltx_text" style="font-size:90%;">Focal sparse convolutional networks for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib56.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.2.2.1" class="ltx_text" style="font-size:90%;">[57]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.4.1" class="ltx_text" style="font-size:90%;">
Diederik P. Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.5.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib57.8.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.2.2.1" class="ltx_text" style="font-size:90%;">[58]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.4.1" class="ltx_text" style="font-size:90%;">
Christopher Choy, JunYoung Gwak, and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.5.1" class="ltx_text" style="font-size:90%;">4d spatio-temporal convnets: Minkowski convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib58.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.2.2.1" class="ltx_text" style="font-size:90%;">[59]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.4.1" class="ltx_text" style="font-size:90%;">
Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill
Stachniss, and Jurgen Gall.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.5.1" class="ltx_text" style="font-size:90%;">Semantickitti: A dataset for semantic scene understanding of lidar
sequences.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib59.8.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.2.2.1" class="ltx_text" style="font-size:90%;">[60]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.4.1" class="ltx_text" style="font-size:90%;">
Lingdong Kong, Youquan Liu, Runnan Chen, Yuexin Ma, Xinge Zhu, Yikang Li,
Yuenan Hou, Yu Qiao, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.5.1" class="ltx_text" style="font-size:90%;">Rethinking range view representation for lidar segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib60.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Implementation details on 3D object detection</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We implemented our method with Pytorch using the open-source OpenPCDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite>.</p>
</div>
<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Waymo Open dataset.</h5>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px1.p1.1" class="ltx_p">We set the hyperparameter <math id="A1.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A1.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="A1.SS0.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.1.m1.1c">m</annotation></semantics></math> to 2 for all SED and DED blocks and stacked 4 DED blocks for the 2D dense backbone by default. We adopted the detection head of CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> for HEDNet. As metioned in the main paper, we primarily followed the training and inference schemes of DSVT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>. Specifically, the voxel size was set to (0.08m, 0.08m, 0.15m), and the detection range was set to [-75.2m, 75.2m] for X and Y axis, and [-2m, 4m] for Z axis. We trained HEDNet for 24 epochs on the entire training dataset and reported the evaluation results on the validation set to compare with previous state-of-the-art methods. For the ablation experiments, we trained all models for 30 epochs on a 20% training subset. All models were trained with a batch size of 16 on 8 RTX 3090 GPUs. We employed the Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> optimizer with a one-cycle learning rate policy, and set the weight-decay to 0.05, and the max learning rate to 0.003. We also adopted the faded training strategy in the last epoch. During inference, we applied class-specific NMS with an IoU threshold of 0.75, 0.6 and 0.55 for vehicle, pedestrian, and cyclist, respectively.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">nuScenes dataset.</h5>

<div id="A1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px2.p1.1" class="ltx_p">We set the hyperparameter <math id="A1.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A1.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="A1.SS0.SSS0.Px2.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="A1.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.1.m1.1c">m</annotation></semantics></math> to 2 for all SED and DED blocks and stacked 5 DED blocks for the 2D dense backbone. We adopted the detection head of TransFusion-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> for HEDNet and primarily followed the training and inference schemes of TransFusion-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>. The voxel size was set to (0.075m, 0.075m, 0.2m), and the detection range was set to [-54m, 54m] for X and Y axis, and [-5m, 3m] for Z axis. We trained HEDNet for 20 epochs on the combined training and validation sets with a batch size of 16 on 8 RTX 3090 GPUs and reported the results on the test set to compare with other methods. We employed the Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> optimizer with a one-cycle learning rate policy, and set the weight-decay to 0.1, the momentum to [0.85, 0.95], and the max learning rate to 0.001. The faded strategy was used during the last 5 epochs. For submission to the test server, we set the query number of detection head to 300 and did not use any test-time augmentation.</p>
</div>
<figure id="A1.T4" class="ltx_table">
<div id="A1.T4.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:477.5pt;height:83.5pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.4pt,9.1pt) scale(0.82,0.82) ;">
<table id="A1.T4.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T4.5.1.1.1" class="ltx_tr">
<th id="A1.T4.5.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">Method</th>
<th id="A1.T4.5.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:16.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:16.1pt;transform:translate(-4.65pt,-4.65pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.2.1.1" class="ltx_p">Car</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:31.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:31.3pt;transform:translate(-11.18pt,-10.21pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.3.1.1" class="ltx_p">Bicycle</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:48.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:48.4pt;transform:translate(-19.74pt,-18.76pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.4.1.1" class="ltx_p">Motorcycle</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:25.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:25.3pt;transform:translate(-9.18pt,-9.18pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.5.1.1" class="ltx_p">Truck</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:16.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:16.6pt;transform:translate(-4.88pt,-4.88pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.6.1.1" class="ltx_p">Bus</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.7.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:29.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:29.4pt;transform:translate(-11.28pt,-11.28pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.7.1.1" class="ltx_p">Person</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:37.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.4pt;transform:translate(-14.26pt,-13.29pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.8.1.1" class="ltx_p">Bicyclist</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:54.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:54.5pt;transform:translate(-22.82pt,-21.85pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.9.1.1" class="ltx_p">Motorcyclist</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.10.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:22.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:22.9pt;transform:translate(-7.99pt,-7.99pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.10.1.1" class="ltx_p">Road</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.11.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:34.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:34.1pt;transform:translate(-12.58pt,-11.61pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.11.1.1" class="ltx_p">Parking</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.12.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:38.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:38.3pt;transform:translate(-15.69pt,-15.69pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.12.1.1" class="ltx_p">Sidewalk</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.13.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:59.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:59.5pt;transform:translate(-25.31pt,-24.33pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.13.1.1" class="ltx_p">Other ground</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.14.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:37.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.1pt;transform:translate(-14.1pt,-13.13pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.14.1.1" class="ltx_p">Building</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.15.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:24.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:24.6pt;transform:translate(-8.88pt,-8.88pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.15.1.1" class="ltx_p">Fence</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.16.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:46.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.7pt;transform:translate(-18.94pt,-17.97pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.16.1.1" class="ltx_p">Vegetation</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.17" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.17.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:26.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:26.7pt;transform:translate(-9.88pt,-9.88pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.17.1.1" class="ltx_p">Trunk</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.18" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.18.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:32pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.0pt;transform:translate(-12.58pt,-12.58pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.18.1.1" class="ltx_p">Terrian</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.19" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.19.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:18.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:18.8pt;transform:translate(-5.9pt,-5.9pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.19.1.1" class="ltx_p">Pole</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.20" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<div id="A1.T4.5.1.1.1.20.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:49.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:49.3pt;transform:translate(-20.18pt,-19.21pt) rotate(-90deg) ;">
<p id="A1.T4.5.1.1.1.20.1.1" class="ltx_p">Traffic sign</p>
</span></div>
</th>
<th id="A1.T4.5.1.1.1.21" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">mIoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T4.5.1.2.1" class="ltx_tr">
<th id="A1.T4.5.1.2.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">MinkUNet34 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>
</th>
<td id="A1.T4.5.1.2.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">96.8</td>
<td id="A1.T4.5.1.2.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">55.0</td>
<td id="A1.T4.5.1.2.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">81.4</td>
<td id="A1.T4.5.1.2.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">83.2</td>
<td id="A1.T4.5.1.2.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">70.2</td>
<td id="A1.T4.5.1.2.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">79.5</td>
<td id="A1.T4.5.1.2.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">89.8</td>
<td id="A1.T4.5.1.2.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">7.8</td>
<td id="A1.T4.5.1.2.1.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">94.8</td>
<td id="A1.T4.5.1.2.1.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">54.6</td>
<td id="A1.T4.5.1.2.1.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">82.8</td>
<td id="A1.T4.5.1.2.1.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">1.5</td>
<td id="A1.T4.5.1.2.1.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">92.0</td>
<td id="A1.T4.5.1.2.1.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">68.3</td>
<td id="A1.T4.5.1.2.1.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">87.9</td>
<td id="A1.T4.5.1.2.1.17" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">69.4</td>
<td id="A1.T4.5.1.2.1.18" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">72.8</td>
<td id="A1.T4.5.1.2.1.19" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">66.1</td>
<td id="A1.T4.5.1.2.1.20" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">52.4</td>
<td id="A1.T4.5.1.2.1.21" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">68.3</td>
</tr>
<tr id="A1.T4.5.1.3.2" class="ltx_tr">
<th id="A1.T4.5.1.3.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:1.4pt;padding-right:1.4pt;">HEDNet (Ours)</th>
<td id="A1.T4.5.1.3.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">97.3</td>
<td id="A1.T4.5.1.3.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">57.2</td>
<td id="A1.T4.5.1.3.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">82.3</td>
<td id="A1.T4.5.1.3.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">88.1</td>
<td id="A1.T4.5.1.3.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">73.9</td>
<td id="A1.T4.5.1.3.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">80.4</td>
<td id="A1.T4.5.1.3.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">91.3</td>
<td id="A1.T4.5.1.3.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">23.2</td>
<td id="A1.T4.5.1.3.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">95.1</td>
<td id="A1.T4.5.1.3.2.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">51.5</td>
<td id="A1.T4.5.1.3.2.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">83.1</td>
<td id="A1.T4.5.1.3.2.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">2.8</td>
<td id="A1.T4.5.1.3.2.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">92.1</td>
<td id="A1.T4.5.1.3.2.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">69.6</td>
<td id="A1.T4.5.1.3.2.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">87.6</td>
<td id="A1.T4.5.1.3.2.17" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">69.4</td>
<td id="A1.T4.5.1.3.2.18" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">72.3</td>
<td id="A1.T4.5.1.3.2.19" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">66.6</td>
<td id="A1.T4.5.1.3.2.20" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">52.1</td>
<td id="A1.T4.5.1.3.2.21" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="A1.T4.5.1.3.2.21.1" class="ltx_text ltx_font_bold">70.3</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>3D semantic segmentation results on the SemanticKiTTI validation set. Metrics: mIoU (%)<math id="A1.T4.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A1.T4.3.m1.1b"><mo stretchy="false" id="A1.T4.3.m1.1.1" xref="A1.T4.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T4.3.m1.1c"><ci id="A1.T4.3.m1.1.1.cmml" xref="A1.T4.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T4.3.m1.1d">\uparrow</annotation></semantics></math> for the overall results, IoU (%)<math id="A1.T4.4.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A1.T4.4.m2.1b"><mo stretchy="false" id="A1.T4.4.m2.1.1" xref="A1.T4.4.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T4.4.m2.1c"><ci id="A1.T4.4.m2.1.1.cmml" xref="A1.T4.4.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T4.4.m2.1d">\uparrow</annotation></semantics></math> for each category.</figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Experiments on 3D semantic segmentation</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We conducted experiments on the popular LiDAR semantic segmentation dataset SemanticKiTTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>, It provides 22 sequences with 19 semantic classes, captured by a 64-beam LiDAR sensor. Following the standard
practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>, <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite>, we report the Intersection-over-Union (IoU) for each category and the average score (mIoU) over all categories. For the backbone network, we employed a UNet-style structure, <span id="A2.p1.1.1" class="ltx_text ltx_font_italic">i.e.,</span>the same designs as the first two layers of the MinkUNet34 are first adopted to extract sparse features with a spatial down-sampling ratio of 4, followed by 4 SED layers to transform the resulting features, finally, two symmetrical layers are used to recover high-resolution features following MinkUNet34. The other settings strictly followed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>. Table <a href="#A1.T4" title="Table 4 ‣ nuScenes dataset. ‣ Appendix A Implementation details on 3D object detection ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that the proposed model exhibited significant gains over its counterpart MinkUNet34 (<span id="A2.p1.1.2" class="ltx_text ltx_font_italic">i.e.,</span>2.0% in mIoU), which demonstrated the generality of our method.</p>
</div>
<figure id="A2.T5" class="ltx_table">
<div id="A2.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.3pt;height:114.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.9pt,6.3pt) scale(0.9,0.9) ;">
<table id="A2.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T5.1.1.1" class="ltx_tr">
<th id="A2.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.6pt;padding-right:2.6pt;">No.</th>
<th id="A2.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.6pt;padding-right:2.6pt;">VoxelNet</th>
<th id="A2.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.6pt;padding-right:2.6pt;">Tricks<sup id="A2.T5.1.1.1.1.1" class="ltx_sup">∗</sup>
</th>
<th id="A2.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.6pt;padding-right:2.6pt;">Smaller-voxel</th>
<th id="A2.T5.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.6pt;padding-right:2.6pt;">SED-block</th>
<th id="A2.T5.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.6pt;padding-right:2.6pt;">DED-block</th>
<th id="A2.T5.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.6pt;padding-right:2.6pt;">Full-data</th>
<th id="A2.T5.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.6pt;padding-right:2.6pt;">Latency</th>
<th id="A2.T5.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.6pt;padding-right:2.6pt;">L1 mAPH</th>
<th id="A2.T5.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.6pt;padding-right:2.6pt;">L2 mAPH</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T5.1.1.2.1" class="ltx_tr">
<td id="A2.T5.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.6pt;padding-right:2.6pt;">1</td>
<td id="A2.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.2.1.3" class="ltx_td ltx_border_t" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.2.1.4" class="ltx_td ltx_border_t" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.2.1.5" class="ltx_td ltx_border_t" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.2.1.6" class="ltx_td ltx_border_t" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.2.1.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.6pt;padding-right:2.6pt;">40 ms</td>
<td id="A2.T5.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.6pt;padding-right:2.6pt;">70.0</td>
<td id="A2.T5.1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.6pt;padding-right:2.6pt;">64.0</td>
</tr>
<tr id="A2.T5.1.1.3.2" class="ltx_tr">
<td id="A2.T5.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.6pt;padding-right:2.6pt;">2</td>
<td id="A2.T5.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.3.2.4" class="ltx_td" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.3.2.5" class="ltx_td" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.3.2.6" class="ltx_td" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.3.2.7" class="ltx_td ltx_border_r" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.3.2.8" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">40 ms</td>
<td id="A2.T5.1.1.3.2.9" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">75.4</td>
<td id="A2.T5.1.1.3.2.10" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">69.1</td>
</tr>
<tr id="A2.T5.1.1.4.3" class="ltx_tr">
<td id="A2.T5.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.6pt;padding-right:2.6pt;">3</td>
<td id="A2.T5.1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.4.3.5" class="ltx_td" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.4.3.6" class="ltx_td" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.4.3.7" class="ltx_td ltx_border_r" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.4.3.8" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">49 ms</td>
<td id="A2.T5.1.1.4.3.9" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">76.6</td>
<td id="A2.T5.1.1.4.3.10" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">70.2</td>
</tr>
<tr id="A2.T5.1.1.5.4" class="ltx_tr">
<td id="A2.T5.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.6pt;padding-right:2.6pt;">4</td>
<td id="A2.T5.1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.5.4.5" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.5.4.6" class="ltx_td" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.5.4.7" class="ltx_td ltx_border_r" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.5.4.8" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">55 ms</td>
<td id="A2.T5.1.1.5.4.9" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">77.6</td>
<td id="A2.T5.1.1.5.4.10" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">71.3</td>
</tr>
<tr id="A2.T5.1.1.6.5" class="ltx_tr">
<td id="A2.T5.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.6pt;padding-right:2.6pt;">5</td>
<td id="A2.T5.1.1.6.5.2" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.6.5.4" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.6.5.5" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.6.5.6" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.6.5.7" class="ltx_td ltx_border_r" style="padding-left:2.6pt;padding-right:2.6pt;"></td>
<td id="A2.T5.1.1.6.5.8" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">67 ms</td>
<td id="A2.T5.1.1.6.5.9" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">78.0</td>
<td id="A2.T5.1.1.6.5.10" class="ltx_td ltx_align_center" style="padding-left:2.6pt;padding-right:2.6pt;">71.9</td>
</tr>
<tr id="A2.T5.1.1.7.6" class="ltx_tr">
<td id="A2.T5.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.6pt;padding-right:2.6pt;">6</td>
<td id="A2.T5.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.7.6.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.6pt;padding-right:2.6pt;">✓</td>
<td id="A2.T5.1.1.7.6.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.6pt;padding-right:2.6pt;">67 ms</td>
<td id="A2.T5.1.1.7.6.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.6pt;padding-right:2.6pt;">79.5</td>
<td id="A2.T5.1.1.7.6.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.6pt;padding-right:2.6pt;">73.4</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>A step-wise ablation from VoxelNet to HEDNet. The first five models were trained on a 20% training subset and the last model was trained on the full training set. <sup id="A2.T5.5.1" class="ltx_sup">∗</sup>: training tricks used in DSVT. </figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>A step-wise ablation from VoxelNet to HEDNet</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We conducted a step-wise ablation from the standard VoxelNet to our HEDNet on the Waymo Open dataset to show the effectiveness of different components (see Table <a href="#A2.T5" title="Table 5 ‣ Appendix B Experiments on 3D semantic segmentation ‣ HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). For the second model, we employed the training tricks used by DSVT, including IoU loss, class-specific NMS, faded strategy (disabling data augmentations in the last epoch), and a weight decay of 0.05. These training tricks can significantly boost detection accuracy. Actually, most of the training tricks have been used by previous works, such as PV-RCNN++, and FSD. The codes and training configurations of the DSVT model can be found in the OpenPCDet archives. For the third model, we adopt a smaller input voxel size to keep more detailed information, which boosts the detection accuracy of pedestrian and cyclist. The 4th and 5th models sequentially incorporate our proposed SED blocks and DED blocks. The last model was trained on the full training set. Our final model (the 6th model) outperformed the previous SOTA method DSVT by 1.3% L2 mAPH while being 50% faster.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.20233" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.20234" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.20234">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.20234" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.20235" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 21:41:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
