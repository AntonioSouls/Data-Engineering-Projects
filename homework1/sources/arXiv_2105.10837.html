<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2105.10837] Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data</title><meta property="og:description" content="The ultimate goal for an inference model is to be robust and functional in real life applications. However, training vs. test data domain gaps often negatively affect model performance. This issue is especially critica‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2105.10837">

<!--Generated on Sat Mar  2 05:23:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on Received: date / Accepted: date.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="3D human pose estimation domain shift semantic aware adaptation synthetic human datasets.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">‚àé






</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>S. Liu, N. Sehgal, and S. Ostadabbas </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Augmented Cognition Lab, Electrical and Computer Engineering Department, Northeastern University.
<span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>shuliu@ece.neu.edu, sehgal.n@husky.neu.edu, ostadabbas@ece.neu.edu</span></span></span> </span></span></span>
<h1 class="ltx_title ltx_title_document">Adapted Human Pose: 
<br class="ltx_break">Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuangjun Liu
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Naveen Sehgal
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sarah Ostadabbas
</span></span>
</div>
<div class="ltx_dates">(Received: date / Accepted: date)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The ultimate goal for an inference model is to be robust and functional in real life applications. However, training vs. test data domain gaps often negatively affect model performance. This issue is especially critical for the monocular 3D human pose estimation problem, in which 3D human data is often collected in a controlled lab setting.
In this paper, we focus on alleviating the negative effect of domain shift in both appearance and pose space for 3D human pose estimation by presenting our adapted human pose (AHuP) approach. AHuP is built upon two key components: (1) semantically aware adaptation (SAA) for the cross-domain feature space adaptation, and (2) skeletal pose adaptation (SPA) for the pose space adaptation which takes only limited information from the target domain. By using zero real 3D human pose data, one of our adapted synthetic models shows comparable performance with the SOTA pose estimation models trained with large scale real 3D human datasets. The proposed SPA can be also employed independently as a light-weighted head to improve existing SOTA models in a novel context. A new 3D scan-based synthetic human dataset called ScanAva+ is also going to be publicly released with this work.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The ScanAva+ dataset is available at: <a target="_blank" href="https://web.northeastern.edu/ostadabbas/code/" title="" class="ltx_ref ltx_href">Augmented Cognition Lab Webpage.</a>. The AHuP code also can be found at GitHub <a target="_blank" href="https://github.com/ostadabbas/AdaptedHumanPose" title="" class="ltx_ref ltx_href">AdaptedHumanPose.</a></span></span></span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>3D human pose estimation domain shift semantic aware adaptation synthetic human datasets.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Because of the great endeavors of the computer vision community, significant advancements have been achieved for 3D human pose estimation with ever improving performance on well recognized benchmarks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">h36m_pami </a></cite>.
Existing approaches come from versatile genres such as end-to-end learning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib25" title="" class="ltx_ref">li20143d </a></cite>, direct 2D-to-3D lifting <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">martinez2017simple </a></cite> and even unsupervised methods <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">chen2019unsupervised </a></cite>. Although improved performance has been reported with decreased information dependence on the training sets <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">martinez2017simple </a>; <a href="#bib.bib8" title="" class="ltx_ref">chen2019unsupervised </a></cite>, the majority of these studies are conducted via a training/testing data split from the same benchmark datasets, which share very similar contexts.
In essence, these 3D human pose benchmarks could be quite different from the real-life applications, and the domain gap between the source data and target applications could lead to potential performance drops.
And, while the domain shift issue has been extensively investigated for the classification tasks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">ganin2016domain </a>; <a href="#bib.bib58" title="" class="ltx_ref">tzeng2015simultaneous </a></cite>, it has rarely been addressed in the pose regression problems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we investigate how domain shift influences the 3D pose estimation model performance and, more importantly, how to counter its adverse effects. Our approach incorporates a semantically aware adaptation (SAA) technique as well as a skeletal pose adaptation (SPA) method towards developing a robust 3D human pose estimation model. Our approach is not only capable of training a monocular 3D human pose estimation model using zero real 3D human pose data, but also can be added to the existing state-of-the-art (SOTA) models as a light-weighted head for adaptation when tested under a novel context or dataset.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Unfortunately, performance evaluation of the 3D pose estimation models under real world applications is not widely conducted, since acquiring labeled 3D human pose data often requires professional motion capture systems <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">h36m_pami </a></cite>, and a few available datasets are collected under lab settings with very limited samples in the wild <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">singleshotmultiperson2018 </a></cite>. This issue could inherently result in the similarity of the environments, the camera settings (in both extrinsic and intrinsic parameters), and also human pose distributions between training and evaluation data splits. As a consequence, training and testing on the same benchmark disproportionately take advantage from these context similarities, which do not always hold in practical applications. For a 2D-to-3D lifting approach <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">martinez2017simple </a></cite>, such mapping is conducted inherently under a fixed camera setting.
Although the model in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">chen2019unsupervised </a></cite> employs no 3D pose data directly, its 2D pose data is a direct projection of the 3D data from the same benchmark, which holds identical pose semantics and distributions of their target domains.
It is also evident in their reported performance that using pose data without adaptation leads to a significant performance drop.
Even for template-based pose estimation approaches such as <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">bogo2016keep </a></cite>, the template is explicitly adapted to the poses from the target domain using the motion and shape (Mosh) capture approach from sparse markers <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib31" title="" class="ltx_ref">loper2014mosh </a></cite>. Please note that in these approaches, to adapt the pose, the real 3D human benchmark data has to be employed in Mosh approach <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib31" title="" class="ltx_ref">loper2014mosh </a></cite>.
In the recent work <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">Li_2020_CVPR </a></cite>, few 3D human training data can be extensively augmented for training and an impressive result has been achieved. Yet the focus is on augmentation, where the real 3D human data are still needed.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Admittedly,
it is a good practice to employ as much information as we can get for an improved performance. Yet, in a real life application, we may have access to no pose or image data in the target domain, or, at best, only limited number of measurements can be collected from the target domain a priori.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Here, we introduce our adapted human pose (AHuP) approach, which allows us to study the monocular 3D human pose estimation problem under a highly unaware context, assuming zero or a very limited amount of information from the target domain.
We conducted our study on a more challenging case, when only the synthetic human data is employed for 3D information. With synthetic appearances, simplified skeletons, and different pose distributions, learning from synthetic humans and testing on real 3D human poses becomes a typical manifestation of the potential domain shift.
Introducing synthetic data for learning is not a new idea, yet competitive performance is only reported by further incorporating real 3D human data into the current models <span id="S1.p5.1.1" class="ltx_text"> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib61" title="" class="ltx_ref">varol17_surreal </a></cite></span>. Furthermore, there has been no specific discussion on how these differences affect the pose estimation performance and the proper way to address the challenges associated with the domain gap caused by training vs. testing data distributions.
From our study, on one hand, with only synthetic human data for 3D part, AHuP shows comparable performance with SOTA models despite the benefit from training on real 3D human benchmark data.
As synthetic human simulation is an efficient and easy-to-produce source pose data generator, this could potentially lead to a more economic solution for the 3D pose estimation problem.
On the other hand, for the existing SOTA 3D human pose estimation models, AHuP can be also added as a light-weighted adaptation head for performance improvement when a novel context or dataset is used during the test phase. This improvement is observed on several SOTA models and on all training and testing combinations in our study.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In short, our work makes the following contributions:
</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2105.10837/assets/figures/AHuP_diag.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.13.6.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.10.5" class="ltx_text" style="font-size:90%;">The proposed adapted human pose (AHuP) framework,
where SAA stands for the semantically aware adaptation approach, SPA stands for the skeletal pose adaptation. <math id="S1.F1.6.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S1.F1.6.1.m1.1b"><mi id="S1.F1.6.1.m1.1.1" xref="S1.F1.6.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S1.F1.6.1.m1.1c"><ci id="S1.F1.6.1.m1.1.1.cmml" xref="S1.F1.6.1.m1.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.1.m1.1d">G</annotation></semantics></math> stands for the feature extractor, <math id="S1.F1.7.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S1.F1.7.2.m2.1b"><mi id="S1.F1.7.2.m2.1.1" xref="S1.F1.7.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S1.F1.7.2.m2.1c"><ci id="S1.F1.7.2.m2.1.1.cmml" xref="S1.F1.7.2.m2.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.7.2.m2.1d">T</annotation></semantics></math> for task head, <math id="S1.F1.8.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S1.F1.8.3.m3.1b"><mi id="S1.F1.8.3.m3.1.1" xref="S1.F1.8.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.F1.8.3.m3.1c"><ci id="S1.F1.8.3.m3.1.1.cmml" xref="S1.F1.8.3.m3.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.8.3.m3.1d">D</annotation></semantics></math> for discriminator and <math id="S1.F1.9.4.m4.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S1.F1.9.4.m4.1b"><msub id="S1.F1.9.4.m4.1.1" xref="S1.F1.9.4.m4.1.1.cmml"><mi id="S1.F1.9.4.m4.1.1.2" xref="S1.F1.9.4.m4.1.1.2.cmml">D</mi><mi id="S1.F1.9.4.m4.1.1.3" xref="S1.F1.9.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.9.4.m4.1c"><apply id="S1.F1.9.4.m4.1.1.cmml" xref="S1.F1.9.4.m4.1.1"><csymbol cd="ambiguous" id="S1.F1.9.4.m4.1.1.1.cmml" xref="S1.F1.9.4.m4.1.1">subscript</csymbol><ci id="S1.F1.9.4.m4.1.1.2.cmml" xref="S1.F1.9.4.m4.1.1.2">ùê∑</ci><ci id="S1.F1.9.4.m4.1.1.3.cmml" xref="S1.F1.9.4.m4.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.9.4.m4.1d">D_{i}</annotation></semantics></math> stands for the <span id="S1.F1.10.5.1" class="ltx_text ltx_font_italic">i</span>-th channel of <math id="S1.F1.10.5.m5.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S1.F1.10.5.m5.1b"><mi id="S1.F1.10.5.m5.1.1" xref="S1.F1.10.5.m5.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.F1.10.5.m5.1c"><ci id="S1.F1.10.5.m5.1.1.cmml" xref="S1.F1.10.5.m5.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.10.5.m5.1d">D</annotation></semantics></math>.</span></figcaption>
</figure>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Presents an adapted human pose (AHuP) approach (see Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) that improves the performance of the state-of-the-art 3D human pose estimation models for both synthetic-to-real and real-to-real cases when training and test data come from different contexts. By using zero real 3D human pose data, one of our adapted synthetic models shows comparable performance with the SOTA pose estimation models trained based on a large mount of real 3D human pose data.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Proposes a semantically aware adaptation (SAA) method for the cross-domain feature space adaptation, which shows noticeable pose estimation performance improvement over conventional domain adaptation techniques.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Introduces a skeletal pose adaptation (SPA) approach, which takes only a limited amount of information from the target domain to adapt the pose, instead of requiring the whole 3D pose data from the target dataset for adaptation purposes, and which can be employed independently as a light-weighted head on top of existing SOTA models for improvement under a novel context.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Introduces and publicly releases a new 3D scan-based synthetic human dataset, called ScanAva+, which is an extended version of our previous ScanAVA dataset (which only had 15 scans), by adding 26 new synthetic human objects with higher texture and size variations.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we provide an overview of the state-of-the-art in the field of 3D human pose estimation and related works that leverage synthetic human models to deal with the 3D data scarcity issues. We also give a summary of the works that employ domain adaptation techniques to close the gap between their training and the test datasets, especially when synthetic data are employed.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>The Problem of Monocular 3D Human Pose Estimation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">For monocular 3D human pose estimation, early attempts followed the 2D pose paradigm and conducted straightforward end-to-end training directly on available 3D human pose datasets such as Human 3.6M <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">h36m_pami </a></cite> and HumanEva <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib53" title="" class="ltx_ref">sigal2010humaneva </a></cite>.
However, the resultant models are often not very generalizable, especially when applied on real world (‚Äúin the wild‚Äù) images. This is mainly due to the fact that most 3D human datasets are collected under controlled lab settings that lack meaningful variations.
To improve the generalization ability in the wild, another line of work takes a two-stage strategy, where it first trains a known 2D pose estimation model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib42" title="" class="ltx_ref">newell2016stacked </a></cite>, then recovers the 3D poses on top of that <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib69" title="" class="ltx_ref">zhou20153d </a></cite>. Some recent works further mix 2D and 3D pose data together to solve the pose
estimation problem.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref">popa2017deep </a></cite>, the authors considered 3D pose a multi-task learning process using 2D pose and depth data. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite> further incorporated a weakly-supervised loss in order to integrate 2D and 3D data. The 3D pose estimation is also solved in a 2D-to-3D lifting manner without learning from the corresponding image or even operating in an unsupervised manner <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">chen2019unsupervised </a></cite>.
A template-based approach has also been employed by fitting the projected silhouette with prior constraints <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">bogo2016keep </a></cite>. However, 2D-to-3D lifting or silhouette-based approaches inherently abandoned the image features, which may not always be optimal and which will be illustrated in our experiments. In recent studies, kinematic constraints have also been introduced where human motion sequence data is required <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib64" title="" class="ltx_ref">Xu_2020_CVPR </a>; <a href="#bib.bib43" title="" class="ltx_ref">pavllo:videopose3d:2019 </a></cite></p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Another approach in 3D pose estimation is to employ synthetic human pose data, summarized in the following section. In short, no matter what exact methodology is used, methods with competitive performance in the 3D human pose estimation field usually cannot avoid employing some real 3D human pose data in their training processes, which makes them expensive to train.
In this work, we would like to explore this exact challenging case by using not a single real 3D human dataset in the process.
Even though the very recent works in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib23" title="" class="ltx_ref">kocabas2019self </a>; <a href="#bib.bib50" title="" class="ltx_ref">Remelli_2020_CVPR </a>; <a href="#bib.bib20" title="" class="ltx_ref">Iqbal_2020_CVPR </a></cite> successfully estimate 3D poses without any explicit 3D labels, they use a multi-view setting, in which 3D coordinates under that can be estimated from the 2D pose via the multi-view geometry. However, a calibrated multi-view setting will not be always available in many applications.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Synthetic Human Pose Data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Employing synthetic data to solve real world problems is not a new concept in the computer vision field. For low level vision tasks, synthetic images have been employed for stereo vision <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib45" title="" class="ltx_ref">peris2012towards </a></cite> and optical flow estimation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref">butler2012naturalistic </a></cite>. For higher level tasks, computer-aided design (CAD) models have also been extensively used for object detection <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib27" title="" class="ltx_ref">liebelt2010multi </a>; <a href="#bib.bib44" title="" class="ltx_ref">peng2015learning </a>; <a href="#bib.bib34" title="" class="ltx_ref">marin2010learning </a></cite> or segmentation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref">hong2018virtual </a></cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Synthetic human figures have been extensively used for learning purposes, such as silhouette-based action recognition tasks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref">ragheb2008vihasi </a></cite>, and crowd counting <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib63" title="" class="ltx_ref">wang2019learning </a></cite>.
Towards the human pose estimation, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib61" title="" class="ltx_ref">varol17_surreal </a></cite>, SURREAL provides 145 subjects and over 6.5 million frames with detailed pose and segmentation labels based on a morphable human template.
Templates from the shape completion and animation of people (SCAPE) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">anguelov2005scape </a></cite> have also been employed for large-scale 3D human pose dataset forming <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">chen2016synthesizing </a></cite>.
Another branch is synthesizing human pose data directly via a human body scanning process, which could be limiting in terms of the number of scans, yet garment geometry details are better preserved, such as our recently developed Scanned Avatar (ScanAva+) dataset that contains 3D scans of 41 human subjects <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib29" title="" class="ltx_ref">liu2018semi </a></cite>. Nonetheless, no matter how realistic the simulated data look and despite applying conventional domain adaptation methods on them, models trained on the synthetic data alone perform noticeably worse than models trained on real human pose data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">chen2016synthesizing </a></cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Synthetic vs. Real Data Domain Adaptation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Domain adaptation is a long-standing topic in the computer vision field, yet synthetic-to-real data domain adaptation for task transfer learning still remains a challenge. Well-known algorithms mainly address the domain gap issue at the feature level by aligning extracted features from both domains and subsequently minimizing certain distance measures between them, such as maximum mean discrepancy <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib30" title="" class="ltx_ref">long2015learning </a></cite>, correlation distance <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref">sun2016deep </a></cite>, or adversarial discrimination loss <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">ganin2016domain </a>; <a href="#bib.bib58" title="" class="ltx_ref">tzeng2015simultaneous </a></cite>. In recent years, domain adaptation has also been introduced for segmentation <span id="S2.SS3.p1.1.1" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">hoffman2018cycada </a>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">luo2019taking</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">vu2019advent</span></cite></span> or object detection tasks <span id="S2.SS3.p1.1.2" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">raj2015subspace</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">gopalan2011domain</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2018domain</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zheng2020cross</span></cite></span>, but rarely for human pose regression.
However, such measures are usually based on the overall image features, which do not guarantee semantic consistency. Though this issue has been addressed recently <span id="S2.SS3.p1.1.3" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">Luo_2019_CVPR </a></cite></span>, it usually remains a classification problem.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">In the image synthesis approach described in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib62" title="" class="ltx_ref">Wang_2019_CVPR </a></cite>, the authors enforce semantic consistency with an <math id="S2.SS3.p2.1.m1.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S2.SS3.p2.1.m1.1a"><msub id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml"><mi id="S2.SS3.p2.1.m1.1.1.2" xref="S2.SS3.p2.1.m1.1.1.2.cmml">L</mi><mn id="S2.SS3.p2.1.m1.1.1.3" xref="S2.SS3.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><apply id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.p2.1.m1.1.1.2.cmml" xref="S2.SS3.p2.1.m1.1.1.2">ùêø</ci><cn type="integer" id="S2.SS3.p2.1.m1.1.1.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">L_{1}</annotation></semantics></math> loss. The loss is computed per-layer between features extracted from a synthetic image and a real sampled image using a pre-trained VGG model, with an adaptive weight that decreases for deeper layers. In <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref">10.1145/3357384.3357918 </a></cite>, for unsupervised domain adaptation, the authors use a two-stream convolutional neural network (CNN), one for the source domain and one for the target domain, with shared weights. When training a classifier, they assign pseudo-labels and then use an adaptive centroid alignment to offset the negative influence from false pseudo labels and enforce cross-domain class consistency. Similarly, in the segmentation task of <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">Luo_2019_CVPR </a></cite>, the authors propose a category-level adversarial network, where two classifiers identify classes whose features are distributed differently between the source and target domains and proportionally increase an adversarial loss to enforce semantic alignment across domains.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Despite being an important concept, semantic consistency is
commonly used for classification tasks, such as in <span id="S2.SS3.p3.1.1" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">Luo_2019_CVPR </a>; <a href="#bib.bib41" title="" class="ltx_ref">Morgado_2017_CVPR </a></cite></span>, or for image synthesis purposes <span id="S2.SS3.p3.1.2" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib62" title="" class="ltx_ref">Wang_2019_CVPR </a></cite></span> instead of human pose.
Though adversarial learning is introduced in human pose estimation before <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib65" title="" class="ltx_ref">yang20183d </a></cite>, it is mainly used for pose regularization purposes, where the training portion of the real 3D human pose benchmark is required.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">In this work, we explore how 3D pose estimation performance will be negatively affected by the domain shift between source and target domains through the use of synthetic 3D human pose data as our training source. More importantly, we present the AHuP approach, which incorporates our counter action adaptation techniques in both feature space and pose space.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Introducing Adapted Human Pose (AHuP)</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">When a pre-trained 3D human pose estimation model is employed in a real world application, many context elements are different from the benchmark that the model was trained on: the person will no longer wear tight clothes to facilitate the motion capture process, no tracker bead is attached, the background will be more versatile compared to a lab environment, and a different pose semantic definition may be used.
As in most cases, human pose is estimated based on extracted image patches <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a>; <a href="#bib.bib39" title="" class="ltx_ref">moon2019camera </a></cite>, so we can assume the differences mainly come from the appearance and pose semantics.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To investigate this problem under a more challenging scenario, we conducted our study on synthetic human since it has noticeable differences in both appearance and pose distributions compared to the common 3D pose benchmarks. Synthetic human usually holds visible unrealistic appearances with simplified skeletons, and their simulated poses, although similar, will never be the same as that of real humans. We proposed both a semantic aware adaptation approach (SAA) on the image feature space and also a skeletal pose adaptation (SPA) approach for skeleton space to align the domain gap as shown in <span id="S3.p2.1.1" class="ltx_text">Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></span>. In this section, we will focus on introducing the SAA and SPA design, respectively.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Semantic Aware Adaptation (SAA)</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.6" class="ltx_p">Domain shift is an common issue when a model learned from domain <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ùê¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">A</annotation></semantics></math> is applied in domain <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">B</annotation></semantics></math> for a similar task.
There are consistent efforts in the computer vision community to address such an issue. For discriminative tasks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref">tzeng2017adversarial </a></cite>, despite variations in the specifics of the model components <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">hoffman2018cycada </a>; <a href="#bib.bib58" title="" class="ltx_ref">tzeng2015simultaneous </a></cite>, a commonly employed structure is to adapt two datasets <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ùê¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">A</annotation></semantics></math> and <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">B</annotation></semantics></math> for a common task network <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">T</annotation></semantics></math>, by forming a feature extractor network <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">G</annotation></semantics></math> (or two for an asymmetric mapping case).
This will map two datasets into a common feature space by minimizing a distance measure, such as the maximum mean discrepancy <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib47" title="" class="ltx_ref">quinonero2008covariate </a></cite>, or by confusing the discriminator with a generative adversarial network (GAN) structure <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref">tzeng2017adversarial </a></cite>.
Such distance measures are usually based on the statistics of overall feature maps <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref">tzeng2017adversarial </a></cite> or local patches <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref">isola2017image </a></cite>, uniformly. Although related studies are mainly conducted for classification tasks, there are also recent works that begin to address the segmentation problems with cycle consistency <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">hoffman2018cycada </a></cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">However, one problem often overlooked is the semantic meaning consistency between two datasets, which so far has been limited to the classification problems <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">Luo_2019_CVPR </a></cite>.
If the same semantic entity demonstrates varying patterns in different domains, it is very possible that in an image/feature space, the nearest neighbors from two domains do not hold the same semantic meaning. A common solution to shorten the overall distance between domains is aligning the nearest neighbored patterns together to blur their domain identities. However, what if the well-aligned pattern comes out to hold different semantic meanings as Fig.¬†<a href="#S3.F2" title="Figure 2 ‚Ä£ 3.1 Semantic Aware Adaptation (SAA) ‚Ä£ 3 Introducing Adapted Human Pose (AHuP) ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>a displays? Such adaptation will possibly turn out to make the result even more misleading, especially for the regression tasks such as human pose estimation, where body parts are usually more similar than distinct categories in a classification task.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2105.10837/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.6.3.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;">(a) Semantically aware adaptation (SAA) vs. nearest neighbor alignment between datasets <math id="S3.F2.3.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.F2.3.1.m1.1b"><mi id="S3.F2.3.1.m1.1.1" xref="S3.F2.3.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.F2.3.1.m1.1c"><ci id="S3.F2.3.1.m1.1.1.cmml" xref="S3.F2.3.1.m1.1.1">ùê¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.1.m1.1d">A</annotation></semantics></math> and <math id="S3.F2.4.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.F2.4.2.m2.1b"><mi id="S3.F2.4.2.m2.1.1" xref="S3.F2.4.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.F2.4.2.m2.1c"><ci id="S3.F2.4.2.m2.1.1.cmml" xref="S3.F2.4.2.m2.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.2.m2.1d">B</annotation></semantics></math>, assumed to be from two different domains. (b) Skeletal pose adaptation (SPA) based on dual direction pivoting to both source and target in different representation. Black arrow indicates the forward mapping. Red arrow indicates the pivoting direction.</span></figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">We argue that the adaptation process should emphasize the semantic awareness in distance measures to achieve a semantically aware adaptation (SAA).</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.6" class="ltx_p">In AHuP, an intuitive way to achieve SAA is employing an individual discriminator for each recognizable body part. For efficiency, we use a multi-channel structure for AHuP to indicate different body parts as shown in Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">D</annotation></semantics></math> stands for the discriminator, <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">D</mi><mi id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">ùê∑</ci><ci id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">D_{i}</annotation></semantics></math> stands for the <span id="S3.SS1.p4.6.1" class="ltx_text ltx_font_italic">i</span>-th channel of the discriminator for corresponding joint, <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mi id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">G</annotation></semantics></math> stands for the feature extractor and <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mi id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><ci id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">T</annotation></semantics></math> stands for the task head as the 3D pose regression.
As the extracted features from the backbone network <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><mi id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><ci id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">G</annotation></semantics></math> suppose to be highly generalized into a low resolution, inspired by patchGAN <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref">isola2017image </a></cite>, we design the discriminator <math id="S3.SS1.p4.6.m6.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p4.6.m6.1a"><mi id="S3.SS1.p4.6.m6.1.1" xref="S3.SS1.p4.6.m6.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m6.1b"><ci id="S3.SS1.p4.6.m6.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">D</annotation></semantics></math> to be feature-wise by setting the convolution kernel size to one with sigmoid. During the training process, for each channel, only the corresponding feature will be activated based on the joint location. We avoid using any 3D human pose benchmark in this process by learning this information from the available 2D human datasets.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.8" class="ltx_p">We employ the adversarial learning strategy by training <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">D</annotation></semantics></math>, <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><mi id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">G</annotation></semantics></math>, and <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><mi id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><ci id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">T</annotation></semantics></math> networks iteratively in an adversarial manner.
For each input image <math id="S3.SS1.p5.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS1.p5.4.m4.1a"><mi id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><ci id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">x</annotation></semantics></math>, we add a domain indicator <math id="S3.SS1.p5.5.m5.1" class="ltx_Math" alttext="d(x)" display="inline"><semantics id="S3.SS1.p5.5.m5.1a"><mrow id="S3.SS1.p5.5.m5.1.2" xref="S3.SS1.p5.5.m5.1.2.cmml"><mi id="S3.SS1.p5.5.m5.1.2.2" xref="S3.SS1.p5.5.m5.1.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.5.m5.1.2.1" xref="S3.SS1.p5.5.m5.1.2.1.cmml">‚Äã</mo><mrow id="S3.SS1.p5.5.m5.1.2.3.2" xref="S3.SS1.p5.5.m5.1.2.cmml"><mo stretchy="false" id="S3.SS1.p5.5.m5.1.2.3.2.1" xref="S3.SS1.p5.5.m5.1.2.cmml">(</mo><mi id="S3.SS1.p5.5.m5.1.1" xref="S3.SS1.p5.5.m5.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS1.p5.5.m5.1.2.3.2.2" xref="S3.SS1.p5.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.5.m5.1b"><apply id="S3.SS1.p5.5.m5.1.2.cmml" xref="S3.SS1.p5.5.m5.1.2"><times id="S3.SS1.p5.5.m5.1.2.1.cmml" xref="S3.SS1.p5.5.m5.1.2.1"></times><ci id="S3.SS1.p5.5.m5.1.2.2.cmml" xref="S3.SS1.p5.5.m5.1.2.2">ùëë</ci><ci id="S3.SS1.p5.5.m5.1.1.cmml" xref="S3.SS1.p5.5.m5.1.1">ùë•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.5.m5.1c">d(x)</annotation></semantics></math> as 0 or 1 to indicate if it is real or synthetic, respectively. During <math id="S3.SS1.p5.6.m6.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p5.6.m6.1a"><mi id="S3.SS1.p5.6.m6.1.1" xref="S3.SS1.p5.6.m6.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.6.m6.1b"><ci id="S3.SS1.p5.6.m6.1.1.cmml" xref="S3.SS1.p5.6.m6.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.6.m6.1c">D</annotation></semantics></math> phase, we optimize <math id="S3.SS1.p5.7.m7.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p5.7.m7.1a"><mi id="S3.SS1.p5.7.m7.1.1" xref="S3.SS1.p5.7.m7.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.7.m7.1b"><ci id="S3.SS1.p5.7.m7.1.1.cmml" xref="S3.SS1.p5.7.m7.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.7.m7.1c">D</annotation></semantics></math> by minimizing the <math id="S3.SS1.p5.8.m8.1" class="ltx_Math" alttext="SAA_{D}" display="inline"><semantics id="S3.SS1.p5.8.m8.1a"><mrow id="S3.SS1.p5.8.m8.1.1" xref="S3.SS1.p5.8.m8.1.1.cmml"><mi id="S3.SS1.p5.8.m8.1.1.2" xref="S3.SS1.p5.8.m8.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.8.m8.1.1.1" xref="S3.SS1.p5.8.m8.1.1.1.cmml">‚Äã</mo><mi id="S3.SS1.p5.8.m8.1.1.3" xref="S3.SS1.p5.8.m8.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.8.m8.1.1.1a" xref="S3.SS1.p5.8.m8.1.1.1.cmml">‚Äã</mo><msub id="S3.SS1.p5.8.m8.1.1.4" xref="S3.SS1.p5.8.m8.1.1.4.cmml"><mi id="S3.SS1.p5.8.m8.1.1.4.2" xref="S3.SS1.p5.8.m8.1.1.4.2.cmml">A</mi><mi id="S3.SS1.p5.8.m8.1.1.4.3" xref="S3.SS1.p5.8.m8.1.1.4.3.cmml">D</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.8.m8.1b"><apply id="S3.SS1.p5.8.m8.1.1.cmml" xref="S3.SS1.p5.8.m8.1.1"><times id="S3.SS1.p5.8.m8.1.1.1.cmml" xref="S3.SS1.p5.8.m8.1.1.1"></times><ci id="S3.SS1.p5.8.m8.1.1.2.cmml" xref="S3.SS1.p5.8.m8.1.1.2">ùëÜ</ci><ci id="S3.SS1.p5.8.m8.1.1.3.cmml" xref="S3.SS1.p5.8.m8.1.1.3">ùê¥</ci><apply id="S3.SS1.p5.8.m8.1.1.4.cmml" xref="S3.SS1.p5.8.m8.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p5.8.m8.1.1.4.1.cmml" xref="S3.SS1.p5.8.m8.1.1.4">subscript</csymbol><ci id="S3.SS1.p5.8.m8.1.1.4.2.cmml" xref="S3.SS1.p5.8.m8.1.1.4.2">ùê¥</ci><ci id="S3.SS1.p5.8.m8.1.1.4.3.cmml" xref="S3.SS1.p5.8.m8.1.1.4.3">ùê∑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.8.m8.1c">SAA_{D}</annotation></semantics></math> loss as:</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle L_{SAA_{D}}" display="inline"><semantics id="S3.E1.m1.1a"><msub id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">L</mi><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1a" xref="S3.E1.m1.1.1.3.1.cmml">‚Äã</mo><msub id="S3.E1.m1.1.1.3.4" xref="S3.E1.m1.1.1.3.4.cmml"><mi id="S3.E1.m1.1.1.3.4.2" xref="S3.E1.m1.1.1.3.4.2.cmml">A</mi><mi id="S3.E1.m1.1.1.3.4.3" xref="S3.E1.m1.1.1.3.4.3.cmml">D</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">ùêø</ci><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><times id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">ùëÜ</ci><ci id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">ùê¥</ci><apply id="S3.E1.m1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.4.1.cmml" xref="S3.E1.m1.1.1.3.4">subscript</csymbol><ci id="S3.E1.m1.1.1.3.4.2.cmml" xref="S3.E1.m1.1.1.3.4.2">ùê¥</ci><ci id="S3.E1.m1.1.1.3.4.3.cmml" xref="S3.E1.m1.1.1.3.4.3">ùê∑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle L_{SAA_{D}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m2.4" class="ltx_math_unparsed" alttext="\displaystyle=-\sum_{i=1}^{N}\mathds{1}(c=J_{hm}(i))(d(x)log\,D_{i}(G(x))[c]" display="inline"><semantics id="S3.E1.m2.4a"><mrow id="S3.E1.m2.4b"><mo rspace="0em" id="S3.E1.m2.4.5">=</mo><mo lspace="0em" id="S3.E1.m2.4.6">‚àí</mo><mstyle displaystyle="true" id="S3.E1.m2.4.7"><munderover id="S3.E1.m2.4.7a"><mo movablelimits="false" id="S3.E1.m2.4.7.2.2">‚àë</mo><mrow id="S3.E1.m2.4.7.2.3"><mi id="S3.E1.m2.4.7.2.3.2">i</mi><mo id="S3.E1.m2.4.7.2.3.1">=</mo><mn id="S3.E1.m2.4.7.2.3.3">1</mn></mrow><mi id="S3.E1.m2.4.7.3">N</mi></munderover></mstyle><mn id="S3.E1.m2.4.8">ùüô</mn><mrow id="S3.E1.m2.4.9"><mo stretchy="false" id="S3.E1.m2.4.9.1">(</mo><mi id="S3.E1.m2.4.9.2">c</mi><mo id="S3.E1.m2.4.9.3">=</mo><msub id="S3.E1.m2.4.9.4"><mi id="S3.E1.m2.4.9.4.2">J</mi><mrow id="S3.E1.m2.4.9.4.3"><mi id="S3.E1.m2.4.9.4.3.2">h</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.4.9.4.3.1">‚Äã</mo><mi id="S3.E1.m2.4.9.4.3.3">m</mi></mrow></msub><mrow id="S3.E1.m2.4.9.5"><mo stretchy="false" id="S3.E1.m2.4.9.5.1">(</mo><mi id="S3.E1.m2.1.1">i</mi><mo stretchy="false" id="S3.E1.m2.4.9.5.2">)</mo></mrow><mo stretchy="false" id="S3.E1.m2.4.9.6">)</mo></mrow><mrow id="S3.E1.m2.4.10"><mo stretchy="false" id="S3.E1.m2.4.10.1">(</mo><mi id="S3.E1.m2.4.10.2">d</mi><mrow id="S3.E1.m2.4.10.3"><mo stretchy="false" id="S3.E1.m2.4.10.3.1">(</mo><mi id="S3.E1.m2.2.2">x</mi><mo stretchy="false" id="S3.E1.m2.4.10.3.2">)</mo></mrow><mi id="S3.E1.m2.4.10.4">l</mi><mi id="S3.E1.m2.4.10.5">o</mi><mi id="S3.E1.m2.4.10.6">g</mi><msub id="S3.E1.m2.4.10.7"><mi id="S3.E1.m2.4.10.7.2">D</mi><mi id="S3.E1.m2.4.10.7.3">i</mi></msub><mrow id="S3.E1.m2.4.10.8"><mo stretchy="false" id="S3.E1.m2.4.10.8.1">(</mo><mi id="S3.E1.m2.4.10.8.2">G</mi><mrow id="S3.E1.m2.4.10.8.3"><mo stretchy="false" id="S3.E1.m2.4.10.8.3.1">(</mo><mi id="S3.E1.m2.3.3">x</mi><mo stretchy="false" id="S3.E1.m2.4.10.8.3.2">)</mo></mrow><mo stretchy="false" id="S3.E1.m2.4.10.8.4">)</mo></mrow><mrow id="S3.E1.m2.4.10.9"><mo stretchy="false" id="S3.E1.m2.4.10.9.1">[</mo><mi id="S3.E1.m2.4.4">c</mi><mo stretchy="false" id="S3.E1.m2.4.10.9.2">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S3.E1.m2.4c">\displaystyle=-\sum_{i=1}^{N}\mathds{1}(c=J_{hm}(i))(d(x)log\,D_{i}(G(x))[c]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m1.3" class="ltx_math_unparsed" alttext="\displaystyle+(1-d(x))log\,(1-D_{i}(G(x))[c]))," display="inline"><semantics id="S3.Ex1.m1.3a"><mrow id="S3.Ex1.m1.3b"><mo id="S3.Ex1.m1.3.4">+</mo><mrow id="S3.Ex1.m1.3.5"><mo stretchy="false" id="S3.Ex1.m1.3.5.1">(</mo><mn id="S3.Ex1.m1.3.5.2">1</mn><mo id="S3.Ex1.m1.3.5.3">‚àí</mo><mi id="S3.Ex1.m1.3.5.4">d</mi><mrow id="S3.Ex1.m1.3.5.5"><mo stretchy="false" id="S3.Ex1.m1.3.5.5.1">(</mo><mi id="S3.Ex1.m1.1.1">x</mi><mo stretchy="false" id="S3.Ex1.m1.3.5.5.2">)</mo></mrow><mo stretchy="false" id="S3.Ex1.m1.3.5.6">)</mo></mrow><mi id="S3.Ex1.m1.3.6">l</mi><mi id="S3.Ex1.m1.3.7">o</mi><mi id="S3.Ex1.m1.3.8">g</mi><mrow id="S3.Ex1.m1.3.9"><mo lspace="0.170em" stretchy="false" id="S3.Ex1.m1.3.9.1">(</mo><mn id="S3.Ex1.m1.3.9.2">1</mn><mo id="S3.Ex1.m1.3.9.3">‚àí</mo><msub id="S3.Ex1.m1.3.9.4"><mi id="S3.Ex1.m1.3.9.4.2">D</mi><mi id="S3.Ex1.m1.3.9.4.3">i</mi></msub><mrow id="S3.Ex1.m1.3.9.5"><mo stretchy="false" id="S3.Ex1.m1.3.9.5.1">(</mo><mi id="S3.Ex1.m1.3.9.5.2">G</mi><mrow id="S3.Ex1.m1.3.9.5.3"><mo stretchy="false" id="S3.Ex1.m1.3.9.5.3.1">(</mo><mi id="S3.Ex1.m1.2.2">x</mi><mo stretchy="false" id="S3.Ex1.m1.3.9.5.3.2">)</mo></mrow><mo stretchy="false" id="S3.Ex1.m1.3.9.5.4">)</mo></mrow><mrow id="S3.Ex1.m1.3.9.6"><mo stretchy="false" id="S3.Ex1.m1.3.9.6.1">[</mo><mi id="S3.Ex1.m1.3.3">c</mi><mo stretchy="false" id="S3.Ex1.m1.3.9.6.2">]</mo></mrow><mo stretchy="false" id="S3.Ex1.m1.3.9.7">)</mo></mrow><mo stretchy="false" id="S3.Ex1.m1.3.10">)</mo><mo id="S3.Ex1.m1.3.11">,</mo></mrow><annotation encoding="application/x-tex" id="S3.Ex1.m1.3c">\displaystyle+(1-d(x))log\,(1-D_{i}(G(x))[c])),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p6.6" class="ltx_p">where, <math id="S3.SS1.p6.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p6.1.m1.1a"><mi id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><ci id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">N</annotation></semantics></math> is the total joint numbers, <math id="S3.SS1.p6.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS1.p6.2.m2.1a"><mi id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><ci id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">c</annotation></semantics></math> stands for the coordinate in the feature space, <math id="S3.SS1.p6.3.m3.2" class="ltx_Math" alttext="D_{i}(\cdot)[c]" display="inline"><semantics id="S3.SS1.p6.3.m3.2a"><mrow id="S3.SS1.p6.3.m3.2.3" xref="S3.SS1.p6.3.m3.2.3.cmml"><msub id="S3.SS1.p6.3.m3.2.3.2" xref="S3.SS1.p6.3.m3.2.3.2.cmml"><mi id="S3.SS1.p6.3.m3.2.3.2.2" xref="S3.SS1.p6.3.m3.2.3.2.2.cmml">D</mi><mi id="S3.SS1.p6.3.m3.2.3.2.3" xref="S3.SS1.p6.3.m3.2.3.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p6.3.m3.2.3.1" xref="S3.SS1.p6.3.m3.2.3.1.cmml">‚Äã</mo><mrow id="S3.SS1.p6.3.m3.2.3.3.2" xref="S3.SS1.p6.3.m3.2.3.cmml"><mo stretchy="false" id="S3.SS1.p6.3.m3.2.3.3.2.1" xref="S3.SS1.p6.3.m3.2.3.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml">‚ãÖ</mo><mo stretchy="false" id="S3.SS1.p6.3.m3.2.3.3.2.2" xref="S3.SS1.p6.3.m3.2.3.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p6.3.m3.2.3.1a" xref="S3.SS1.p6.3.m3.2.3.1.cmml">‚Äã</mo><mrow id="S3.SS1.p6.3.m3.2.3.4.2" xref="S3.SS1.p6.3.m3.2.3.4.1.cmml"><mo stretchy="false" id="S3.SS1.p6.3.m3.2.3.4.2.1" xref="S3.SS1.p6.3.m3.2.3.4.1.1.cmml">[</mo><mi id="S3.SS1.p6.3.m3.2.2" xref="S3.SS1.p6.3.m3.2.2.cmml">c</mi><mo stretchy="false" id="S3.SS1.p6.3.m3.2.3.4.2.2" xref="S3.SS1.p6.3.m3.2.3.4.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.2b"><apply id="S3.SS1.p6.3.m3.2.3.cmml" xref="S3.SS1.p6.3.m3.2.3"><times id="S3.SS1.p6.3.m3.2.3.1.cmml" xref="S3.SS1.p6.3.m3.2.3.1"></times><apply id="S3.SS1.p6.3.m3.2.3.2.cmml" xref="S3.SS1.p6.3.m3.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p6.3.m3.2.3.2.1.cmml" xref="S3.SS1.p6.3.m3.2.3.2">subscript</csymbol><ci id="S3.SS1.p6.3.m3.2.3.2.2.cmml" xref="S3.SS1.p6.3.m3.2.3.2.2">ùê∑</ci><ci id="S3.SS1.p6.3.m3.2.3.2.3.cmml" xref="S3.SS1.p6.3.m3.2.3.2.3">ùëñ</ci></apply><ci id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1">‚ãÖ</ci><apply id="S3.SS1.p6.3.m3.2.3.4.1.cmml" xref="S3.SS1.p6.3.m3.2.3.4.2"><csymbol cd="latexml" id="S3.SS1.p6.3.m3.2.3.4.1.1.cmml" xref="S3.SS1.p6.3.m3.2.3.4.2.1">delimited-[]</csymbol><ci id="S3.SS1.p6.3.m3.2.2.cmml" xref="S3.SS1.p6.3.m3.2.2">ùëê</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.2c">D_{i}(\cdot)[c]</annotation></semantics></math> stands for the <span id="S3.SS1.p6.6.1" class="ltx_text ltx_font_italic">i</span>-th channel of <math id="S3.SS1.p6.4.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p6.4.m4.1a"><mi id="S3.SS1.p6.4.m4.1.1" xref="S3.SS1.p6.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.4.m4.1b"><ci id="S3.SS1.p6.4.m4.1.1.cmml" xref="S3.SS1.p6.4.m4.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.4.m4.1c">D</annotation></semantics></math> at coordinate <math id="S3.SS1.p6.5.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS1.p6.5.m5.1a"><mi id="S3.SS1.p6.5.m5.1.1" xref="S3.SS1.p6.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.5.m5.1b"><ci id="S3.SS1.p6.5.m5.1.1.cmml" xref="S3.SS1.p6.5.m5.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.5.m5.1c">c</annotation></semantics></math>,
<math id="S3.SS1.p6.6.m6.1" class="ltx_Math" alttext="J_{hm}(i)" display="inline"><semantics id="S3.SS1.p6.6.m6.1a"><mrow id="S3.SS1.p6.6.m6.1.2" xref="S3.SS1.p6.6.m6.1.2.cmml"><msub id="S3.SS1.p6.6.m6.1.2.2" xref="S3.SS1.p6.6.m6.1.2.2.cmml"><mi id="S3.SS1.p6.6.m6.1.2.2.2" xref="S3.SS1.p6.6.m6.1.2.2.2.cmml">J</mi><mrow id="S3.SS1.p6.6.m6.1.2.2.3" xref="S3.SS1.p6.6.m6.1.2.2.3.cmml"><mi id="S3.SS1.p6.6.m6.1.2.2.3.2" xref="S3.SS1.p6.6.m6.1.2.2.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.6.m6.1.2.2.3.1" xref="S3.SS1.p6.6.m6.1.2.2.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p6.6.m6.1.2.2.3.3" xref="S3.SS1.p6.6.m6.1.2.2.3.3.cmml">m</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p6.6.m6.1.2.1" xref="S3.SS1.p6.6.m6.1.2.1.cmml">‚Äã</mo><mrow id="S3.SS1.p6.6.m6.1.2.3.2" xref="S3.SS1.p6.6.m6.1.2.cmml"><mo stretchy="false" id="S3.SS1.p6.6.m6.1.2.3.2.1" xref="S3.SS1.p6.6.m6.1.2.cmml">(</mo><mi id="S3.SS1.p6.6.m6.1.1" xref="S3.SS1.p6.6.m6.1.1.cmml">i</mi><mo stretchy="false" id="S3.SS1.p6.6.m6.1.2.3.2.2" xref="S3.SS1.p6.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.6.m6.1b"><apply id="S3.SS1.p6.6.m6.1.2.cmml" xref="S3.SS1.p6.6.m6.1.2"><times id="S3.SS1.p6.6.m6.1.2.1.cmml" xref="S3.SS1.p6.6.m6.1.2.1"></times><apply id="S3.SS1.p6.6.m6.1.2.2.cmml" xref="S3.SS1.p6.6.m6.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p6.6.m6.1.2.2.1.cmml" xref="S3.SS1.p6.6.m6.1.2.2">subscript</csymbol><ci id="S3.SS1.p6.6.m6.1.2.2.2.cmml" xref="S3.SS1.p6.6.m6.1.2.2.2">ùêΩ</ci><apply id="S3.SS1.p6.6.m6.1.2.2.3.cmml" xref="S3.SS1.p6.6.m6.1.2.2.3"><times id="S3.SS1.p6.6.m6.1.2.2.3.1.cmml" xref="S3.SS1.p6.6.m6.1.2.2.3.1"></times><ci id="S3.SS1.p6.6.m6.1.2.2.3.2.cmml" xref="S3.SS1.p6.6.m6.1.2.2.3.2">‚Ñé</ci><ci id="S3.SS1.p6.6.m6.1.2.2.3.3.cmml" xref="S3.SS1.p6.6.m6.1.2.2.3.3">ùëö</ci></apply></apply><ci id="S3.SS1.p6.6.m6.1.1.cmml" xref="S3.SS1.p6.6.m6.1.1">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.6.m6.1c">J_{hm}(i)</annotation></semantics></math> is the <span id="S3.SS1.p6.6.2" class="ltx_text ltx_font_italic">i</span>-th joint location in heatmap space. Compared to the conventional adaptation approaches <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref">sun2016deep </a>; <a href="#bib.bib58" title="" class="ltx_ref">tzeng2015simultaneous </a>; <a href="#bib.bib59" title="" class="ltx_ref">tzeng2017adversarial </a></cite>, where the feature adaptation is over the whole image region, our approach specifies the semantic meaning via multi-channel discriminator.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.4" class="ltx_p">We train the <math id="S3.SS1.p7.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS1.p7.1.m1.1a"><mi id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><ci id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">G</annotation></semantics></math> and <math id="S3.SS1.p7.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p7.2.m2.1a"><mi id="S3.SS1.p7.2.m2.1.1" xref="S3.SS1.p7.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.2.m2.1b"><ci id="S3.SS1.p7.2.m2.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.2.m2.1c">T</annotation></semantics></math> networks together by minimize the regression error and confusing the <math id="S3.SS1.p7.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p7.3.m3.1a"><mi id="S3.SS1.p7.3.m3.1.1" xref="S3.SS1.p7.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.3.m3.1b"><ci id="S3.SS1.p7.3.m3.1.1.cmml" xref="S3.SS1.p7.3.m3.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.3.m3.1c">D</annotation></semantics></math> at the same time. We employ the cross entropy loss between the <math id="S3.SS1.p7.4.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p7.4.m4.1a"><mi id="S3.SS1.p7.4.m4.1.1" xref="S3.SS1.p7.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.4.m4.1b"><ci id="S3.SS1.p7.4.m4.1.1.cmml" xref="S3.SS1.p7.4.m4.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.4.m4.1c">D</annotation></semantics></math> prediction and a uniform distribution for confusing purpose as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_math_unparsed" alttext="L_{conf}=-\sum_{i=1}^{N}\mathds{1}(c=J_{hm}(i)])(\frac{1}{2}\,\log\,D_{i}(G(x))[c])." display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1b"><msub id="S3.E2.m1.1.2"><mi id="S3.E2.m1.1.2.2">L</mi><mrow id="S3.E2.m1.1.2.3"><mi id="S3.E2.m1.1.2.3.2">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.3.1">‚Äã</mo><mi id="S3.E2.m1.1.2.3.3">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.3.1a">‚Äã</mo><mi id="S3.E2.m1.1.2.3.4">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.3.1b">‚Äã</mo><mi id="S3.E2.m1.1.2.3.5">f</mi></mrow></msub><mo rspace="0em" id="S3.E2.m1.1.3">=</mo><mo lspace="0em" rspace="0.055em" id="S3.E2.m1.1.4">‚àí</mo><munderover id="S3.E2.m1.1.5"><mo movablelimits="false" id="S3.E2.m1.1.5.2.2">‚àë</mo><mrow id="S3.E2.m1.1.5.2.3"><mi id="S3.E2.m1.1.5.2.3.2">i</mi><mo id="S3.E2.m1.1.5.2.3.1">=</mo><mn id="S3.E2.m1.1.5.2.3.3">1</mn></mrow><mi id="S3.E2.m1.1.5.3">N</mi></munderover><mn id="S3.E2.m1.1.6">ùüô</mn><mrow id="S3.E2.m1.1.7"><mo stretchy="false" id="S3.E2.m1.1.7.1">(</mo><mi id="S3.E2.m1.1.7.2">c</mi><mo id="S3.E2.m1.1.7.3">=</mo><msub id="S3.E2.m1.1.7.4"><mi id="S3.E2.m1.1.7.4.2">J</mi><mrow id="S3.E2.m1.1.7.4.3"><mi id="S3.E2.m1.1.7.4.3.2">h</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.7.4.3.1">‚Äã</mo><mi id="S3.E2.m1.1.7.4.3.3">m</mi></mrow></msub><mrow id="S3.E2.m1.1.7.5"><mo stretchy="false" id="S3.E2.m1.1.7.5.1">(</mo><mi id="S3.E2.m1.1.1">i</mi><mo stretchy="false" id="S3.E2.m1.1.7.5.2">)</mo></mrow><mo stretchy="false" id="S3.E2.m1.1.7.6">]</mo></mrow><mo stretchy="false" id="S3.E2.m1.1.8">)</mo><mo stretchy="false" id="S3.E2.m1.1.9">(</mo><mfrac id="S3.E2.m1.1.10"><mn id="S3.E2.m1.1.10.2">1</mn><mn id="S3.E2.m1.1.10.3">2</mn></mfrac><mi id="S3.E2.m1.1.11">log</mi><msub id="S3.E2.m1.1.12"><mi id="S3.E2.m1.1.12.2">D</mi><mi id="S3.E2.m1.1.12.3">i</mi></msub><mrow id="S3.E2.m1.1.13"><mo stretchy="false" id="S3.E2.m1.1.13.1">(</mo><mi id="S3.E2.m1.1.13.2">G</mi><mrow id="S3.E2.m1.1.13.3"><mo stretchy="false" id="S3.E2.m1.1.13.3.1">(</mo><mi id="S3.E2.m1.1.13.3.2">x</mi><mo stretchy="false" id="S3.E2.m1.1.13.3.3">)</mo></mrow><mo stretchy="false" id="S3.E2.m1.1.13.4">)</mo></mrow><mrow id="S3.E2.m1.1.14"><mo stretchy="false" id="S3.E2.m1.1.14.1">[</mo><mi id="S3.E2.m1.1.14.2">c</mi><mo stretchy="false" id="S3.E2.m1.1.14.3">]</mo></mrow><mo stretchy="false" id="S3.E2.m1.1.15">)</mo><mo lspace="0em" id="S3.E2.m1.1.16">.</mo></mrow><annotation encoding="application/x-tex" id="S3.E2.m1.1c">L_{conf}=-\sum_{i=1}^{N}\mathds{1}(c=J_{hm}(i)])(\frac{1}{2}\,\log\,D_{i}(G(x))[c]).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<p id="S3.SS1.p8.2" class="ltx_p">The total loss during <math id="S3.SS1.p8.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS1.p8.1.m1.1a"><mi id="S3.SS1.p8.1.m1.1.1" xref="S3.SS1.p8.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.1.m1.1b"><ci id="S3.SS1.p8.1.m1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.1.m1.1c">G</annotation></semantics></math> and <math id="S3.SS1.p8.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p8.2.m2.1a"><mi id="S3.SS1.p8.2.m2.1.1" xref="S3.SS1.p8.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.2.m2.1b"><ci id="S3.SS1.p8.2.m2.1.1.cmml" xref="S3.SS1.p8.2.m2.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.2.m2.1c">T</annotation></semantics></math> phase is given as:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="L_{SAA_{GT}}=||y_{gt}-T(G(y))||_{1}+\lambda L_{conf}," display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><msub id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml"><mi id="S3.E3.m1.2.2.1.1.3.2" xref="S3.E3.m1.2.2.1.1.3.2.cmml">L</mi><mrow id="S3.E3.m1.2.2.1.1.3.3" xref="S3.E3.m1.2.2.1.1.3.3.cmml"><mi id="S3.E3.m1.2.2.1.1.3.3.2" xref="S3.E3.m1.2.2.1.1.3.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.3.3.1" xref="S3.E3.m1.2.2.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.1.1.3.3.3" xref="S3.E3.m1.2.2.1.1.3.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.3.3.1a" xref="S3.E3.m1.2.2.1.1.3.3.1.cmml">‚Äã</mo><msub id="S3.E3.m1.2.2.1.1.3.3.4" xref="S3.E3.m1.2.2.1.1.3.3.4.cmml"><mi id="S3.E3.m1.2.2.1.1.3.3.4.2" xref="S3.E3.m1.2.2.1.1.3.3.4.2.cmml">A</mi><mrow id="S3.E3.m1.2.2.1.1.3.3.4.3" xref="S3.E3.m1.2.2.1.1.3.3.4.3.cmml"><mi id="S3.E3.m1.2.2.1.1.3.3.4.3.2" xref="S3.E3.m1.2.2.1.1.3.3.4.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.3.3.4.3.1" xref="S3.E3.m1.2.2.1.1.3.3.4.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.1.1.3.3.4.3.3" xref="S3.E3.m1.2.2.1.1.3.3.4.3.3.cmml">T</mi></mrow></msub></mrow></msub><mo id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><msub id="S3.E3.m1.2.2.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.2.cmml">y</mi><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml">t</mi></mrow></msub><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml">‚àí</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">‚Äã</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">y</mi><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.E3.m1.2.2.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.2.cmml">+</mo><mrow id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.3.2" xref="S3.E3.m1.2.2.1.1.1.3.2.cmml">Œª</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.3.1" xref="S3.E3.m1.2.2.1.1.1.3.1.cmml">‚Äã</mo><msub id="S3.E3.m1.2.2.1.1.1.3.3" xref="S3.E3.m1.2.2.1.1.1.3.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.3.3.2" xref="S3.E3.m1.2.2.1.1.1.3.3.2.cmml">L</mi><mrow id="S3.E3.m1.2.2.1.1.1.3.3.3" xref="S3.E3.m1.2.2.1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.3.3.3.2" xref="S3.E3.m1.2.2.1.1.1.3.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.3.3.3.1" xref="S3.E3.m1.2.2.1.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.1.1.1.3.3.3.3" xref="S3.E3.m1.2.2.1.1.1.3.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.3.3.3.1a" xref="S3.E3.m1.2.2.1.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.1.1.1.3.3.3.4" xref="S3.E3.m1.2.2.1.1.1.3.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.3.3.3.1b" xref="S3.E3.m1.2.2.1.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.1.1.1.3.3.3.5" xref="S3.E3.m1.2.2.1.1.1.3.3.3.5.cmml">f</mi></mrow></msub></mrow></mrow></mrow><mo id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1"><eq id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2"></eq><apply id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.3.2">ùêø</ci><apply id="S3.E3.m1.2.2.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.3.3"><times id="S3.E3.m1.2.2.1.1.3.3.1.cmml" xref="S3.E3.m1.2.2.1.1.3.3.1"></times><ci id="S3.E3.m1.2.2.1.1.3.3.2.cmml" xref="S3.E3.m1.2.2.1.1.3.3.2">ùëÜ</ci><ci id="S3.E3.m1.2.2.1.1.3.3.3.cmml" xref="S3.E3.m1.2.2.1.1.3.3.3">ùê¥</ci><apply id="S3.E3.m1.2.2.1.1.3.3.4.cmml" xref="S3.E3.m1.2.2.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.3.3.4.1.cmml" xref="S3.E3.m1.2.2.1.1.3.3.4">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.3.3.4.2.cmml" xref="S3.E3.m1.2.2.1.1.3.3.4.2">ùê¥</ci><apply id="S3.E3.m1.2.2.1.1.3.3.4.3.cmml" xref="S3.E3.m1.2.2.1.1.3.3.4.3"><times id="S3.E3.m1.2.2.1.1.3.3.4.3.1.cmml" xref="S3.E3.m1.2.2.1.1.3.3.4.3.1"></times><ci id="S3.E3.m1.2.2.1.1.3.3.4.3.2.cmml" xref="S3.E3.m1.2.2.1.1.3.3.4.3.2">ùê∫</ci><ci id="S3.E3.m1.2.2.1.1.3.3.4.3.3.cmml" xref="S3.E3.m1.2.2.1.1.3.3.4.3.3">ùëá</ci></apply></apply></apply></apply><apply id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"><plus id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2"></plus><apply id="S3.E3.m1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1"><minus id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2"></minus><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.2">ùë¶</ci><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3"><times id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.2">ùëî</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.3.3">ùë°</ci></apply></apply><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1"><times id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3">ùëá</ci><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1"><times id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">ùê∫</ci><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ùë¶</ci></apply></apply></apply></apply><cn type="integer" id="S3.E3.m1.2.2.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3">1</cn></apply><apply id="S3.E3.m1.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3"><times id="S3.E3.m1.2.2.1.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.3.1"></times><ci id="S3.E3.m1.2.2.1.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.3.2">ùúÜ</ci><apply id="S3.E3.m1.2.2.1.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.3.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.3.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3.2">ùêø</ci><apply id="S3.E3.m1.2.2.1.1.1.3.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3.3"><times id="S3.E3.m1.2.2.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3.3.1"></times><ci id="S3.E3.m1.2.2.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3.3.2">ùëê</ci><ci id="S3.E3.m1.2.2.1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3.3.3">ùëú</ci><ci id="S3.E3.m1.2.2.1.1.1.3.3.3.4.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3.3.4">ùëõ</ci><ci id="S3.E3.m1.2.2.1.1.1.3.3.3.5.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3.3.5">ùëì</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">L_{SAA_{GT}}=||y_{gt}-T(G(y))||_{1}+\lambda L_{conf},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p8.5" class="ltx_p">where <math id="S3.SS1.p8.3.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS1.p8.3.m1.1a"><mi id="S3.SS1.p8.3.m1.1.1" xref="S3.SS1.p8.3.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.3.m1.1b"><ci id="S3.SS1.p8.3.m1.1.1.cmml" xref="S3.SS1.p8.3.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.3.m1.1c">\lambda</annotation></semantics></math> is the coefficient for confusion loss, <math id="S3.SS1.p8.4.m2.1" class="ltx_Math" alttext="y_{gt}" display="inline"><semantics id="S3.SS1.p8.4.m2.1a"><msub id="S3.SS1.p8.4.m2.1.1" xref="S3.SS1.p8.4.m2.1.1.cmml"><mi id="S3.SS1.p8.4.m2.1.1.2" xref="S3.SS1.p8.4.m2.1.1.2.cmml">y</mi><mrow id="S3.SS1.p8.4.m2.1.1.3" xref="S3.SS1.p8.4.m2.1.1.3.cmml"><mi id="S3.SS1.p8.4.m2.1.1.3.2" xref="S3.SS1.p8.4.m2.1.1.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p8.4.m2.1.1.3.1" xref="S3.SS1.p8.4.m2.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p8.4.m2.1.1.3.3" xref="S3.SS1.p8.4.m2.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.4.m2.1b"><apply id="S3.SS1.p8.4.m2.1.1.cmml" xref="S3.SS1.p8.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.4.m2.1.1.1.cmml" xref="S3.SS1.p8.4.m2.1.1">subscript</csymbol><ci id="S3.SS1.p8.4.m2.1.1.2.cmml" xref="S3.SS1.p8.4.m2.1.1.2">ùë¶</ci><apply id="S3.SS1.p8.4.m2.1.1.3.cmml" xref="S3.SS1.p8.4.m2.1.1.3"><times id="S3.SS1.p8.4.m2.1.1.3.1.cmml" xref="S3.SS1.p8.4.m2.1.1.3.1"></times><ci id="S3.SS1.p8.4.m2.1.1.3.2.cmml" xref="S3.SS1.p8.4.m2.1.1.3.2">ùëî</ci><ci id="S3.SS1.p8.4.m2.1.1.3.3.cmml" xref="S3.SS1.p8.4.m2.1.1.3.3">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.4.m2.1c">y_{gt}</annotation></semantics></math> is the pose ground truth, and we use norm <math id="S3.SS1.p8.5.m3.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS1.p8.5.m3.1a"><msub id="S3.SS1.p8.5.m3.1.1" xref="S3.SS1.p8.5.m3.1.1.cmml"><mi id="S3.SS1.p8.5.m3.1.1.2" xref="S3.SS1.p8.5.m3.1.1.2.cmml">L</mi><mn id="S3.SS1.p8.5.m3.1.1.3" xref="S3.SS1.p8.5.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.5.m3.1b"><apply id="S3.SS1.p8.5.m3.1.1.cmml" xref="S3.SS1.p8.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.5.m3.1.1.1.cmml" xref="S3.SS1.p8.5.m3.1.1">subscript</csymbol><ci id="S3.SS1.p8.5.m3.1.1.2.cmml" xref="S3.SS1.p8.5.m3.1.1.2">ùêø</ci><cn type="integer" id="S3.SS1.p8.5.m3.1.1.3.cmml" xref="S3.SS1.p8.5.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.5.m3.1c">L_{1}</annotation></semantics></math> for regression supervision.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Skeletal Pose Adaptation (SPA)</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Pose adaptation has been extensively employed in many of the 3D human pose estimation works, especially when data outside of the benchmark domains are introduced. The main idea is to align the introduced pose or its 2D projection (<cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">chen2019unsupervised </a></cite>) with
the target domain either by direct mapping <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">bogo2016keep </a></cite> or via a discriminator to detect fake poses <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib65" title="" class="ltx_ref">yang20183d </a></cite>.
The underlying assumption here is that there is extensively collected target pose data to be aligned to, which is not usually the case in the real applications.
Instead, it is more reasonable to assume that only some countable low dimensional and interpretable parameters from the target domain could be gathered, such as the tailor measurements of the body‚Äôs limbs.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.5" class="ltx_p">In order to achieve the skeletal pose adaptation (SPA), we use the normalized limb length vector <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="s(y)" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.2" xref="S3.SS2.p2.1.m1.1.2.cmml"><mi id="S3.SS2.p2.1.m1.1.2.2" xref="S3.SS2.p2.1.m1.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.2.1" xref="S3.SS2.p2.1.m1.1.2.1.cmml">‚Äã</mo><mrow id="S3.SS2.p2.1.m1.1.2.3.2" xref="S3.SS2.p2.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.1.2.3.2.1" xref="S3.SS2.p2.1.m1.1.2.cmml">(</mo><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">y</mi><mo stretchy="false" id="S3.SS2.p2.1.m1.1.2.3.2.2" xref="S3.SS2.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.2"><times id="S3.SS2.p2.1.m1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.2.1"></times><ci id="S3.SS2.p2.1.m1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.2.2">ùë†</ci><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ùë¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">s(y)</annotation></semantics></math> as the skeletal descriptor with the shoulder width as its normalization factor. Due to the different pose semantic definition and subjects‚Äô physiques, this descriptor could vary among different datasets. The aligned pose is given as <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="y_{SPA}=y+f(y)" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.2" xref="S3.SS2.p2.2.m2.1.2.cmml"><msub id="S3.SS2.p2.2.m2.1.2.2" xref="S3.SS2.p2.2.m2.1.2.2.cmml"><mi id="S3.SS2.p2.2.m2.1.2.2.2" xref="S3.SS2.p2.2.m2.1.2.2.2.cmml">y</mi><mrow id="S3.SS2.p2.2.m2.1.2.2.3" xref="S3.SS2.p2.2.m2.1.2.2.3.cmml"><mi id="S3.SS2.p2.2.m2.1.2.2.3.2" xref="S3.SS2.p2.2.m2.1.2.2.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.2.2.3.1" xref="S3.SS2.p2.2.m2.1.2.2.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.2.m2.1.2.2.3.3" xref="S3.SS2.p2.2.m2.1.2.2.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.2.2.3.1a" xref="S3.SS2.p2.2.m2.1.2.2.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.2.m2.1.2.2.3.4" xref="S3.SS2.p2.2.m2.1.2.2.3.4.cmml">A</mi></mrow></msub><mo id="S3.SS2.p2.2.m2.1.2.1" xref="S3.SS2.p2.2.m2.1.2.1.cmml">=</mo><mrow id="S3.SS2.p2.2.m2.1.2.3" xref="S3.SS2.p2.2.m2.1.2.3.cmml"><mi id="S3.SS2.p2.2.m2.1.2.3.2" xref="S3.SS2.p2.2.m2.1.2.3.2.cmml">y</mi><mo id="S3.SS2.p2.2.m2.1.2.3.1" xref="S3.SS2.p2.2.m2.1.2.3.1.cmml">+</mo><mrow id="S3.SS2.p2.2.m2.1.2.3.3" xref="S3.SS2.p2.2.m2.1.2.3.3.cmml"><mi id="S3.SS2.p2.2.m2.1.2.3.3.2" xref="S3.SS2.p2.2.m2.1.2.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.2.3.3.1" xref="S3.SS2.p2.2.m2.1.2.3.3.1.cmml">‚Äã</mo><mrow id="S3.SS2.p2.2.m2.1.2.3.3.3.2" xref="S3.SS2.p2.2.m2.1.2.3.3.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.1.2.3.3.3.2.1" xref="S3.SS2.p2.2.m2.1.2.3.3.cmml">(</mo><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">y</mi><mo stretchy="false" id="S3.SS2.p2.2.m2.1.2.3.3.3.2.2" xref="S3.SS2.p2.2.m2.1.2.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.2.cmml" xref="S3.SS2.p2.2.m2.1.2"><eq id="S3.SS2.p2.2.m2.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.2.1"></eq><apply id="S3.SS2.p2.2.m2.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.2.2.1.cmml" xref="S3.SS2.p2.2.m2.1.2.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.2.2.2.cmml" xref="S3.SS2.p2.2.m2.1.2.2.2">ùë¶</ci><apply id="S3.SS2.p2.2.m2.1.2.2.3.cmml" xref="S3.SS2.p2.2.m2.1.2.2.3"><times id="S3.SS2.p2.2.m2.1.2.2.3.1.cmml" xref="S3.SS2.p2.2.m2.1.2.2.3.1"></times><ci id="S3.SS2.p2.2.m2.1.2.2.3.2.cmml" xref="S3.SS2.p2.2.m2.1.2.2.3.2">ùëÜ</ci><ci id="S3.SS2.p2.2.m2.1.2.2.3.3.cmml" xref="S3.SS2.p2.2.m2.1.2.2.3.3">ùëÉ</ci><ci id="S3.SS2.p2.2.m2.1.2.2.3.4.cmml" xref="S3.SS2.p2.2.m2.1.2.2.3.4">ùê¥</ci></apply></apply><apply id="S3.SS2.p2.2.m2.1.2.3.cmml" xref="S3.SS2.p2.2.m2.1.2.3"><plus id="S3.SS2.p2.2.m2.1.2.3.1.cmml" xref="S3.SS2.p2.2.m2.1.2.3.1"></plus><ci id="S3.SS2.p2.2.m2.1.2.3.2.cmml" xref="S3.SS2.p2.2.m2.1.2.3.2">ùë¶</ci><apply id="S3.SS2.p2.2.m2.1.2.3.3.cmml" xref="S3.SS2.p2.2.m2.1.2.3.3"><times id="S3.SS2.p2.2.m2.1.2.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.2.3.3.1"></times><ci id="S3.SS2.p2.2.m2.1.2.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.2.3.3.2">ùëì</ci><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">ùë¶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">y_{SPA}=y+f(y)</annotation></semantics></math>, where <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ùë¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">y</annotation></semantics></math> is the output of the pose estimation network as <math id="S3.SS2.p2.4.m4.2" class="ltx_Math" alttext="y=T(G(x))" display="inline"><semantics id="S3.SS2.p2.4.m4.2a"><mrow id="S3.SS2.p2.4.m4.2.2" xref="S3.SS2.p2.4.m4.2.2.cmml"><mi id="S3.SS2.p2.4.m4.2.2.3" xref="S3.SS2.p2.4.m4.2.2.3.cmml">y</mi><mo id="S3.SS2.p2.4.m4.2.2.2" xref="S3.SS2.p2.4.m4.2.2.2.cmml">=</mo><mrow id="S3.SS2.p2.4.m4.2.2.1" xref="S3.SS2.p2.4.m4.2.2.1.cmml"><mi id="S3.SS2.p2.4.m4.2.2.1.3" xref="S3.SS2.p2.4.m4.2.2.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.2.2.1.2" xref="S3.SS2.p2.4.m4.2.2.1.2.cmml">‚Äã</mo><mrow id="S3.SS2.p2.4.m4.2.2.1.1.1" xref="S3.SS2.p2.4.m4.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.4.m4.2.2.1.1.1.2" xref="S3.SS2.p2.4.m4.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p2.4.m4.2.2.1.1.1.1" xref="S3.SS2.p2.4.m4.2.2.1.1.1.1.cmml"><mi id="S3.SS2.p2.4.m4.2.2.1.1.1.1.2" xref="S3.SS2.p2.4.m4.2.2.1.1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.2.2.1.1.1.1.1" xref="S3.SS2.p2.4.m4.2.2.1.1.1.1.1.cmml">‚Äã</mo><mrow id="S3.SS2.p2.4.m4.2.2.1.1.1.1.3.2" xref="S3.SS2.p2.4.m4.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.4.m4.2.2.1.1.1.1.3.2.1" xref="S3.SS2.p2.4.m4.2.2.1.1.1.1.cmml">(</mo><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.p2.4.m4.2.2.1.1.1.1.3.2.2" xref="S3.SS2.p2.4.m4.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS2.p2.4.m4.2.2.1.1.1.3" xref="S3.SS2.p2.4.m4.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.2b"><apply id="S3.SS2.p2.4.m4.2.2.cmml" xref="S3.SS2.p2.4.m4.2.2"><eq id="S3.SS2.p2.4.m4.2.2.2.cmml" xref="S3.SS2.p2.4.m4.2.2.2"></eq><ci id="S3.SS2.p2.4.m4.2.2.3.cmml" xref="S3.SS2.p2.4.m4.2.2.3">ùë¶</ci><apply id="S3.SS2.p2.4.m4.2.2.1.cmml" xref="S3.SS2.p2.4.m4.2.2.1"><times id="S3.SS2.p2.4.m4.2.2.1.2.cmml" xref="S3.SS2.p2.4.m4.2.2.1.2"></times><ci id="S3.SS2.p2.4.m4.2.2.1.3.cmml" xref="S3.SS2.p2.4.m4.2.2.1.3">ùëá</ci><apply id="S3.SS2.p2.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.2.2.1.1.1"><times id="S3.SS2.p2.4.m4.2.2.1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.2.2.1.1.1.1.1"></times><ci id="S3.SS2.p2.4.m4.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.2.2.1.1.1.1.2">ùê∫</ci><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">ùë•</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.2c">y=T(G(x))</annotation></semantics></math> in Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">ùëì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">f</annotation></semantics></math> is a mapping function which is a multi-layer regression network with a two linear residue block in between as shown in Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. What we want from this mapping is a pose holding similar semantic meaning with the source domain and at the same time a skeleton similar to the target domain. So, we employ a dual direction pivoting strategy in both pose and skeleton spaces by pushing the mapped pose to the source pose and at the same time pushing the mapped skeleton to the target skeleton, as shown in Fig.¬†<a href="#S3.F2" title="Figure 2 ‚Ä£ 3.1 Semantic Aware Adaptation (SAA) ‚Ä£ 3 Introducing Adapted Human Pose (AHuP) ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>b.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.9" class="ltx_p">To be specific, in original pose representation, by pivoting the resultant pose back to its original pose estimation <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ùë¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">y</annotation></semantics></math>, we assume the mapped pose should not be far away from the source, namely the original pose prediction, to keep the pose semantic meaning. In the skeletal representation, this is achieved by pushing the resultant pose skeleton descriptor <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="s(y_{SPA})" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">‚Äã</mo><mrow id="S3.SS2.p3.2.m2.1.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.2.m2.1.1.1.1.2" xref="S3.SS2.p3.2.m2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p3.2.m2.1.1.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.1.1.1.2" xref="S3.SS2.p3.2.m2.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.SS2.p3.2.m2.1.1.1.1.1.3" xref="S3.SS2.p3.2.m2.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.2.m2.1.1.1.1.1.3.2" xref="S3.SS2.p3.2.m2.1.1.1.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.1.1.1.1.1.3.1" xref="S3.SS2.p3.2.m2.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.2.m2.1.1.1.1.1.3.3" xref="S3.SS2.p3.2.m2.1.1.1.1.1.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.1.1.1.1.1.3.1a" xref="S3.SS2.p3.2.m2.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.2.m2.1.1.1.1.1.3.4" xref="S3.SS2.p3.2.m2.1.1.1.1.1.3.4.cmml">A</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p3.2.m2.1.1.1.1.3" xref="S3.SS2.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><times id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2"></times><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">ùë†</ci><apply id="S3.SS2.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.2">ùë¶</ci><apply id="S3.SS2.p3.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.3"><times id="S3.SS2.p3.2.m2.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p3.2.m2.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.3.2">ùëÜ</ci><ci id="S3.SS2.p3.2.m2.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.3.3">ùëÉ</ci><ci id="S3.SS2.p3.2.m2.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.3.4">ùê¥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">s(y_{SPA})</annotation></semantics></math> to target <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="s(y_{tar})" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">‚Äã</mo><mrow id="S3.SS2.p3.3.m3.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.3.m3.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p3.3.m3.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.SS2.p3.3.m3.1.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.3.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.3.m3.1.1.1.1.1.3.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.3.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.3.m3.1.1.1.1.1.3.1a" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.3.4" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3.4.cmml">r</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p3.3.m3.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><times id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2"></times><ci id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">ùë†</ci><apply id="S3.SS2.p3.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.2">ùë¶</ci><apply id="S3.SS2.p3.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3"><times id="S3.SS2.p3.3.m3.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3.2">ùë°</ci><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3.3">ùëé</ci><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3.4">ùëü</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">s(y_{tar})</annotation></semantics></math> to make the resultant skeleton similar to target. As the skeletal descriptor <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="s(\cdot)" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mrow id="S3.SS2.p3.4.m4.1.2" xref="S3.SS2.p3.4.m4.1.2.cmml"><mi id="S3.SS2.p3.4.m4.1.2.2" xref="S3.SS2.p3.4.m4.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.4.m4.1.2.1" xref="S3.SS2.p3.4.m4.1.2.1.cmml">‚Äã</mo><mrow id="S3.SS2.p3.4.m4.1.2.3.2" xref="S3.SS2.p3.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.4.m4.1.2.3.2.1" xref="S3.SS2.p3.4.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">‚ãÖ</mo><mo stretchy="false" id="S3.SS2.p3.4.m4.1.2.3.2.2" xref="S3.SS2.p3.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.2.cmml" xref="S3.SS2.p3.4.m4.1.2"><times id="S3.SS2.p3.4.m4.1.2.1.cmml" xref="S3.SS2.p3.4.m4.1.2.1"></times><ci id="S3.SS2.p3.4.m4.1.2.2.cmml" xref="S3.SS2.p3.4.m4.1.2.2">ùë†</ci><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">‚ãÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">s(\cdot)</annotation></semantics></math> is differentiable, our network can effectively be updated to enforce the skeleton similarity during the model training process.
Similar to <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="s(y_{SPA})" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mrow id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml">‚Äã</mo><mrow id="S3.SS2.p3.5.m5.1.1.1.1" xref="S3.SS2.p3.5.m5.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.5.m5.1.1.1.1.2" xref="S3.SS2.p3.5.m5.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p3.5.m5.1.1.1.1.1" xref="S3.SS2.p3.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.1.1.1.2" xref="S3.SS2.p3.5.m5.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.SS2.p3.5.m5.1.1.1.1.1.3" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.5.m5.1.1.1.1.1.3.2" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m5.1.1.1.1.1.3.1" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.5.m5.1.1.1.1.1.3.3" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m5.1.1.1.1.1.3.1a" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.5.m5.1.1.1.1.1.3.4" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3.4.cmml">A</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p3.5.m5.1.1.1.1.3" xref="S3.SS2.p3.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><times id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2"></times><ci id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">ùë†</ci><apply id="S3.SS2.p3.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1.1.2">ùë¶</ci><apply id="S3.SS2.p3.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3"><times id="S3.SS2.p3.5.m5.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p3.5.m5.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3.2">ùëÜ</ci><ci id="S3.SS2.p3.5.m5.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3.3">ùëÉ</ci><ci id="S3.SS2.p3.5.m5.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3.4">ùê¥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">s(y_{SPA})</annotation></semantics></math>, <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="s(y_{tar})" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mrow id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml"><mi id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.6.m6.1.1.2" xref="S3.SS2.p3.6.m6.1.1.2.cmml">‚Äã</mo><mrow id="S3.SS2.p3.6.m6.1.1.1.1" xref="S3.SS2.p3.6.m6.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.6.m6.1.1.1.1.2" xref="S3.SS2.p3.6.m6.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p3.6.m6.1.1.1.1.1" xref="S3.SS2.p3.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.6.m6.1.1.1.1.1.2" xref="S3.SS2.p3.6.m6.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.SS2.p3.6.m6.1.1.1.1.1.3" xref="S3.SS2.p3.6.m6.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.6.m6.1.1.1.1.1.3.2" xref="S3.SS2.p3.6.m6.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.6.m6.1.1.1.1.1.3.1" xref="S3.SS2.p3.6.m6.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.6.m6.1.1.1.1.1.3.3" xref="S3.SS2.p3.6.m6.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.6.m6.1.1.1.1.1.3.1a" xref="S3.SS2.p3.6.m6.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.6.m6.1.1.1.1.1.3.4" xref="S3.SS2.p3.6.m6.1.1.1.1.1.3.4.cmml">r</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p3.6.m6.1.1.1.1.3" xref="S3.SS2.p3.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"><times id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2"></times><ci id="S3.SS2.p3.6.m6.1.1.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3">ùë†</ci><apply id="S3.SS2.p3.6.m6.1.1.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1.1.2">ùë¶</ci><apply id="S3.SS2.p3.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1.1.3"><times id="S3.SS2.p3.6.m6.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p3.6.m6.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1.1.3.2">ùë°</ci><ci id="S3.SS2.p3.6.m6.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1.1.3.3">ùëé</ci><ci id="S3.SS2.p3.6.m6.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p3.6.m6.1.1.1.1.1.3.4">ùëü</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">s(y_{tar})</annotation></semantics></math> inherently comes from the target pose data <math id="S3.SS2.p3.7.m7.1" class="ltx_Math" alttext="y_{tar}" display="inline"><semantics id="S3.SS2.p3.7.m7.1a"><msub id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml"><mi id="S3.SS2.p3.7.m7.1.1.2" xref="S3.SS2.p3.7.m7.1.1.2.cmml">y</mi><mrow id="S3.SS2.p3.7.m7.1.1.3" xref="S3.SS2.p3.7.m7.1.1.3.cmml"><mi id="S3.SS2.p3.7.m7.1.1.3.2" xref="S3.SS2.p3.7.m7.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.7.m7.1.1.3.1" xref="S3.SS2.p3.7.m7.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.7.m7.1.1.3.3" xref="S3.SS2.p3.7.m7.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.7.m7.1.1.3.1a" xref="S3.SS2.p3.7.m7.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.7.m7.1.1.3.4" xref="S3.SS2.p3.7.m7.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><apply id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.2">ùë¶</ci><apply id="S3.SS2.p3.7.m7.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.3"><times id="S3.SS2.p3.7.m7.1.1.3.1.cmml" xref="S3.SS2.p3.7.m7.1.1.3.1"></times><ci id="S3.SS2.p3.7.m7.1.1.3.2.cmml" xref="S3.SS2.p3.7.m7.1.1.3.2">ùë°</ci><ci id="S3.SS2.p3.7.m7.1.1.3.3.cmml" xref="S3.SS2.p3.7.m7.1.1.3.3">ùëé</ci><ci id="S3.SS2.p3.7.m7.1.1.3.4.cmml" xref="S3.SS2.p3.7.m7.1.1.3.4">ùëü</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">y_{tar}</annotation></semantics></math>.
But as target pose data is not always available in practice, given <math id="S3.SS2.p3.8.m8.1" class="ltx_Math" alttext="s(y_{tar})" display="inline"><semantics id="S3.SS2.p3.8.m8.1a"><mrow id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.3" xref="S3.SS2.p3.8.m8.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.8.m8.1.1.2" xref="S3.SS2.p3.8.m8.1.1.2.cmml">‚Äã</mo><mrow id="S3.SS2.p3.8.m8.1.1.1.1" xref="S3.SS2.p3.8.m8.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.8.m8.1.1.1.1.2" xref="S3.SS2.p3.8.m8.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p3.8.m8.1.1.1.1.1" xref="S3.SS2.p3.8.m8.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.1.1.1.2" xref="S3.SS2.p3.8.m8.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.SS2.p3.8.m8.1.1.1.1.1.3" xref="S3.SS2.p3.8.m8.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.8.m8.1.1.1.1.1.3.2" xref="S3.SS2.p3.8.m8.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.8.m8.1.1.1.1.1.3.1" xref="S3.SS2.p3.8.m8.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.8.m8.1.1.1.1.1.3.3" xref="S3.SS2.p3.8.m8.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.8.m8.1.1.1.1.1.3.1a" xref="S3.SS2.p3.8.m8.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.8.m8.1.1.1.1.1.3.4" xref="S3.SS2.p3.8.m8.1.1.1.1.1.3.4.cmml">r</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p3.8.m8.1.1.1.1.3" xref="S3.SS2.p3.8.m8.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><apply id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1"><times id="S3.SS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.2"></times><ci id="S3.SS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3">ùë†</ci><apply id="S3.SS2.p3.8.m8.1.1.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.8.m8.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1.1.2">ùë¶</ci><apply id="S3.SS2.p3.8.m8.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1.1.3"><times id="S3.SS2.p3.8.m8.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p3.8.m8.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1.1.3.2">ùë°</ci><ci id="S3.SS2.p3.8.m8.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1.1.3.3">ùëé</ci><ci id="S3.SS2.p3.8.m8.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p3.8.m8.1.1.1.1.1.3.4">ùëü</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">s(y_{tar})</annotation></semantics></math> is low dimensional, we can tailor measure <math id="S3.SS2.p3.9.m9.1" class="ltx_Math" alttext="s(y_{tar})" display="inline"><semantics id="S3.SS2.p3.9.m9.1a"><mrow id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml"><mi id="S3.SS2.p3.9.m9.1.1.3" xref="S3.SS2.p3.9.m9.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.9.m9.1.1.2" xref="S3.SS2.p3.9.m9.1.1.2.cmml">‚Äã</mo><mrow id="S3.SS2.p3.9.m9.1.1.1.1" xref="S3.SS2.p3.9.m9.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.9.m9.1.1.1.1.2" xref="S3.SS2.p3.9.m9.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p3.9.m9.1.1.1.1.1" xref="S3.SS2.p3.9.m9.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.9.m9.1.1.1.1.1.2" xref="S3.SS2.p3.9.m9.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.SS2.p3.9.m9.1.1.1.1.1.3" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.9.m9.1.1.1.1.1.3.2" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.9.m9.1.1.1.1.1.3.1" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.9.m9.1.1.1.1.1.3.3" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.9.m9.1.1.1.1.1.3.1a" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p3.9.m9.1.1.1.1.1.3.4" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.4.cmml">r</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p3.9.m9.1.1.1.1.3" xref="S3.SS2.p3.9.m9.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.1b"><apply id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1"><times id="S3.SS2.p3.9.m9.1.1.2.cmml" xref="S3.SS2.p3.9.m9.1.1.2"></times><ci id="S3.SS2.p3.9.m9.1.1.3.cmml" xref="S3.SS2.p3.9.m9.1.1.3">ùë†</ci><apply id="S3.SS2.p3.9.m9.1.1.1.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.9.m9.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.2">ùë¶</ci><apply id="S3.SS2.p3.9.m9.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3"><times id="S3.SS2.p3.9.m9.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p3.9.m9.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.2">ùë°</ci><ci id="S3.SS2.p3.9.m9.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.3">ùëé</ci><ci id="S3.SS2.p3.9.m9.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.4">ùëü</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.1c">s(y_{tar})</annotation></semantics></math> directly from the exact target or other subjects from the same dataset.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.4" class="ltx_p">The loss for SPA is given as:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="L_{SPA}=||f(y)||_{1}+||s(y_{SPA})-s(y_{tar})||_{2}," display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml"><msub id="S3.E4.m1.2.2.1.1.4" xref="S3.E4.m1.2.2.1.1.4.cmml"><mi id="S3.E4.m1.2.2.1.1.4.2" xref="S3.E4.m1.2.2.1.1.4.2.cmml">L</mi><mrow id="S3.E4.m1.2.2.1.1.4.3" xref="S3.E4.m1.2.2.1.1.4.3.cmml"><mi id="S3.E4.m1.2.2.1.1.4.3.2" xref="S3.E4.m1.2.2.1.1.4.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.4.3.1" xref="S3.E4.m1.2.2.1.1.4.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.2.2.1.1.4.3.3" xref="S3.E4.m1.2.2.1.1.4.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.4.3.1a" xref="S3.E4.m1.2.2.1.1.4.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.2.2.1.1.4.3.4" xref="S3.E4.m1.2.2.1.1.4.3.4.cmml">A</mi></mrow></msub><mo id="S3.E4.m1.2.2.1.1.3" xref="S3.E4.m1.2.2.1.1.3.cmml">=</mo><mrow id="S3.E4.m1.2.2.1.1.2" xref="S3.E4.m1.2.2.1.1.2.cmml"><msub id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml">‚Äã</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">y</mi><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.E4.m1.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E4.m1.2.2.1.1.2.3" xref="S3.E4.m1.2.2.1.1.2.3.cmml">+</mo><msub id="S3.E4.m1.2.2.1.1.2.2" xref="S3.E4.m1.2.2.1.1.2.2.cmml"><mrow id="S3.E4.m1.2.2.1.1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.2.2.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.2.2.1.1.2" xref="S3.E4.m1.2.2.1.1.2.2.1.2.1.cmml">‚Äñ</mo><mrow id="S3.E4.m1.2.2.1.1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.1" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.1a" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.4" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.4.cmml">A</mi></mrow></msub><mo stretchy="false" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.2.2.1.1.2.2.1.1.1.3" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.3.cmml">‚àí</mo><mrow id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.3" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.2" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.2.cmml">‚Äã</mo><mrow id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.2" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.cmml">(</mo><msub id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.2" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.2.cmml">y</mi><mrow id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.2" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.1" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.3" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.1a" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.4" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.4.cmml">r</mi></mrow></msub><mo stretchy="false" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.3" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E4.m1.2.2.1.1.2.2.1.1.3" xref="S3.E4.m1.2.2.1.1.2.2.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.E4.m1.2.2.1.1.2.2.3" xref="S3.E4.m1.2.2.1.1.2.2.3.cmml">2</mn></msub></mrow></mrow><mo id="S3.E4.m1.2.2.1.2" xref="S3.E4.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1"><eq id="S3.E4.m1.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.3"></eq><apply id="S3.E4.m1.2.2.1.1.4.cmml" xref="S3.E4.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.4.1.cmml" xref="S3.E4.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.4.2.cmml" xref="S3.E4.m1.2.2.1.1.4.2">ùêø</ci><apply id="S3.E4.m1.2.2.1.1.4.3.cmml" xref="S3.E4.m1.2.2.1.1.4.3"><times id="S3.E4.m1.2.2.1.1.4.3.1.cmml" xref="S3.E4.m1.2.2.1.1.4.3.1"></times><ci id="S3.E4.m1.2.2.1.1.4.3.2.cmml" xref="S3.E4.m1.2.2.1.1.4.3.2">ùëÜ</ci><ci id="S3.E4.m1.2.2.1.1.4.3.3.cmml" xref="S3.E4.m1.2.2.1.1.4.3.3">ùëÉ</ci><ci id="S3.E4.m1.2.2.1.1.4.3.4.cmml" xref="S3.E4.m1.2.2.1.1.4.3.4">ùê¥</ci></apply></apply><apply id="S3.E4.m1.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2"><plus id="S3.E4.m1.2.2.1.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2.3"></plus><apply id="S3.E4.m1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1">subscript</csymbol><apply id="S3.E4.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1"><times id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1"></times><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2">ùëì</ci><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">ùë¶</ci></apply></apply><cn type="integer" id="S3.E4.m1.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3">1</cn></apply><apply id="S3.E4.m1.2.2.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2">subscript</csymbol><apply id="S3.E4.m1.2.2.1.1.2.2.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1"><csymbol cd="latexml" id="S3.E4.m1.2.2.1.1.2.2.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.2">norm</csymbol><apply id="S3.E4.m1.2.2.1.1.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1"><minus id="S3.E4.m1.2.2.1.1.2.2.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.3"></minus><apply id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1"><times id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.2"></times><ci id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.3">ùë†</ci><apply id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.2">ùë¶</ci><apply id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3"><times id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.2">ùëÜ</ci><ci id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.3">ùëÉ</ci><ci id="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3.4">ùê¥</ci></apply></apply></apply><apply id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2"><times id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.2"></times><ci id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.3">ùë†</ci><apply id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.2">ùë¶</ci><apply id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3"><times id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.1"></times><ci id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.2">ùë°</ci><ci id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.3">ùëé</ci><ci id="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.4.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.1.1.2.1.1.1.3.4">ùëü</ci></apply></apply></apply></apply></apply><cn type="integer" id="S3.E4.m1.2.2.1.1.2.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">L_{SPA}=||f(y)||_{1}+||s(y_{SPA})-s(y_{tar})||_{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.3" class="ltx_p">where, <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="s(y_{tar})" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">‚Äã</mo><mrow id="S3.SS2.p4.1.m1.1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p4.1.m1.1.1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p4.1.m1.1.1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.SS2.p4.1.m1.1.1.1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p4.1.m1.1.1.1.1.1.3.2" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.1.1.1.3.1" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p4.1.m1.1.1.1.1.1.3.3" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.1.1.1.3.1a" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p4.1.m1.1.1.1.1.1.3.4" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.4.cmml">r</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p4.1.m1.1.1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><times id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2"></times><ci id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">ùë†</ci><apply id="S3.SS2.p4.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.2">ùë¶</ci><apply id="S3.SS2.p4.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3"><times id="S3.SS2.p4.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p4.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.2">ùë°</ci><ci id="S3.SS2.p4.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.3">ùëé</ci><ci id="S3.SS2.p4.1.m1.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.4">ùëü</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">s(y_{tar})</annotation></semantics></math> stands for the target skeletal measures. We employ norm <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><msub id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">L</mi><mn id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">ùêø</ci><cn type="integer" id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">L_{2}</annotation></semantics></math> loss for skeletal similarity pivoting and use norm <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><msub id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">L</mi><mn id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">ùêø</ci><cn type="integer" id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">L_{1}</annotation></semantics></math> loss for initial pose pivoting. In our design, SPA is trained after SAA, which acts as an additional component to the SAA network.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">We have to point out that though human geometric has been employed for human pose estimation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite>, it is given as an additional constraint in an end-to-end training process where the target 3D human pose is available.
However, SPA can be added as a light-weighted head on top of an existing pose estimation network for adaptation purposes and the training process is done without any target pose data but only with a series of low dimensional tailor measurements.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Performance Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.3" class="ltx_p">To implement the AHuP approach for monocular 3D human pose estimation, we configured the architecture in Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> by employing a ResNet <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib16" title="" class="ltx_ref">he2016deep </a></cite> and an integral human pose head <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a></cite> for <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">G</annotation></semantics></math> and <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">T</annotation></semantics></math> networks, respectively. <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.p1.3.m3.1a"><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">D</annotation></semantics></math> is similar to <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref">isola2017image </a></cite> in its a feature-wise manner with kernel size 1.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>AHuP Implementation Details</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Our work is implemented via the PyTorch framework and each configuration is trained with a Nvidia v100 GPU. For the backbone feature extractor network <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">G</annotation></semantics></math>, although training from scratch converges, initialization from pre-trained weights via ImageNet can accelerate this process. Therefore, our backbone network is initialized with the pre-trained weights from ImageNet <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref">deng2009imagenet </a></cite>.
All other networks are initialized via Xavier <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">glorot2010understanding </a></cite>.
During training and testing of the SAA network (as introduced in main text Sec.¬†<a href="#S3.SS1" title="3.1 Semantic Aware Adaptation (SAA) ‚Ä£ 3 Introducing Adapted Human Pose (AHuP) ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), we chose batch size to be 120.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.7" class="ltx_p">For feature-wise <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">D</annotation></semantics></math> shown in Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we chose a 3-layer configuration with kernel size 3 stride 1. Networks <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">G</annotation></semantics></math> and <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mi id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">T</annotation></semantics></math> were jointly trained in an adversarial learning procedure, with <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mi id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">D</annotation></semantics></math> serving as the counterpart. Learning rate is set at 1e-3 with a decreasing rate of 0.1 at epoch 11 and 13 with a total of 15. All input images are human-centered, cropped and resized to be 256<math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mo id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><times id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">\times</annotation></semantics></math>256. The output heatmap is set as 64<math id="S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.6.m6.1a"><mo id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><times id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">\times</annotation></semantics></math>64<math id="S4.SS1.p2.7.m7.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.7.m7.1a"><mo id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><times id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">\times</annotation></semantics></math>64.
During training, we initially train the 2D part at first 5 epochs to facilitate SAA process and 3D supervision is added after.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">To facilitate the training and testing across different datasets, we chose the shared or similar joints to match the 17 joint configuration of the Human3.6M dataset by reordering and renaming.
For data feeding, we followed a pivoted matching principle that whenever the leading feeder gives a batch, all subordinates will feed equivalent data to match. Since our study is focused on 3D human pose estimation, we always put the 3D pose dataset as the pivot feeder. For a fair comparison among varying size datasets, we fixed the iteration per epoch at 2500 by repeating the exhausted data loader, if any.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.2" class="ltx_p">For the SPA model, we decoupled it from the SAA net training process by learning the mapping directly from the ground truth data.
To respect the convention of avoiding using any information from the test data,
we used the mean skeletal descriptor <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="s(y_{tar})" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">‚Äã</mo><mrow id="S4.SS1.p4.1.m1.1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p4.1.m1.1.1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS1.p4.1.m1.1.1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.1.1.1.2.cmml">y</mi><mrow id="S4.SS1.p4.1.m1.1.1.1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3.cmml"><mi id="S4.SS1.p4.1.m1.1.1.1.1.1.3.2" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.1.1.1.3.1" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.1.m1.1.1.1.1.1.3.3" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.1.1.1.3.1a" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.1.m1.1.1.1.1.1.3.4" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3.4.cmml">r</mi></mrow></msub><mo stretchy="false" id="S4.SS1.p4.1.m1.1.1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><times id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2"></times><ci id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3">ùë†</ci><apply id="S4.SS1.p4.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.2">ùë¶</ci><apply id="S4.SS1.p4.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3"><times id="S4.SS1.p4.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3.1"></times><ci id="S4.SS1.p4.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3.2">ùë°</ci><ci id="S4.SS1.p4.1.m1.1.1.1.1.1.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3.3">ùëé</ci><ci id="S4.SS1.p4.1.m1.1.1.1.1.1.3.4.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3.4">ùëü</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">s(y_{tar})</annotation></semantics></math> of the training split with the assumption that it is similar to the test split within the same dataset.
For <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="f(y)" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.2" xref="S4.SS1.p4.2.m2.1.2.cmml"><mi id="S4.SS1.p4.2.m2.1.2.2" xref="S4.SS1.p4.2.m2.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.2.m2.1.2.1" xref="S4.SS1.p4.2.m2.1.2.1.cmml">‚Äã</mo><mrow id="S4.SS1.p4.2.m2.1.2.3.2" xref="S4.SS1.p4.2.m2.1.2.cmml"><mo stretchy="false" id="S4.SS1.p4.2.m2.1.2.3.2.1" xref="S4.SS1.p4.2.m2.1.2.cmml">(</mo><mi id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">y</mi><mo stretchy="false" id="S4.SS1.p4.2.m2.1.2.3.2.2" xref="S4.SS1.p4.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.2.cmml" xref="S4.SS1.p4.2.m2.1.2"><times id="S4.SS1.p4.2.m2.1.2.1.cmml" xref="S4.SS1.p4.2.m2.1.2.1"></times><ci id="S4.SS1.p4.2.m2.1.2.2.cmml" xref="S4.SS1.p4.2.m2.1.2.2">ùëì</ci><ci id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">ùë¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">f(y)</annotation></semantics></math>, the hidden neuron for each linear layer is set as 1024.
The initial learning rate is set to be 1e-4 with a decreasing rate of 0.95 at each epoch with a total 70 epoch and batch size 256.
During the training, the SURREAL data is downsampled with the rate of 90 to become balanced with the ScanAva+ dataset. Human3.6M is also downsampled with the rate 5 for training and 64 for testing, as commonly done in related studies <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">moon2019camera </a>; <a href="#bib.bib24" title="" class="ltx_ref">lassner2017unite </a>; <a href="#bib.bib66" title="" class="ltx_ref">yasin2016dual </a>; <a href="#bib.bib55" title="" class="ltx_ref">sun2017compositional </a>; <a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a></cite>.
Our augmentation includes rotation, scaling, color jittering, and synthetic occlusion <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib67" title="" class="ltx_ref">zhong2017random </a></cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Datasets</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For AHuP performance evaluation and comparison with the SOTA, we employed several publicly-available datasets in the human pose estimation field that have been used extensively. For real 3D human pose data, we chose the Human3.6M <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">h36m_pami </a></cite> and MuPoTs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">singleshotmultiperson2018 </a></cite> datasets to represent lab and outdoor environments, respectively. For real 2D human pose data, we chose the MSCOCO <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib28" title="" class="ltx_ref">lin2014microsoft </a></cite> and MPII <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">andriluka20142d </a></cite> datasets. As for synthetic human pose data, one branch comes from the deformable human template, in which we chose SURREAL dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib61" title="" class="ltx_ref">varol17_surreal </a></cite>. In SURREAL, we used the released train split to extract sample images. We kept a 0.05 portion of the whole section for validation and test purposes.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The other synthetic branch comes from the construction of virtual avatars through the direct 3D scanning of humans, in which we chose the ScanAva+ dataset developed in our lab <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib29" title="" class="ltx_ref">liu2018semi </a></cite>.
The original ScanAva has only 15 scans, which is fewer than its counterpart, SURREAL dataset. To balance the comparison in our study, we used the toolkit employed in the original ScanAva in order to collect additional human scans to augment the dataset to contain 41 full body scans and we formed the ScanAva+ dataset, in which 36 scans are used for the training.
If not specifically indicated, we employed a basic setting with pose data from SYN + MPII + MSCOCO collectively for all synthetic union cases, where SYN stands for either ScanAva+ or SURREAL datasets.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Ablation study of AHuP when tested on real 3D human pose datasets as Human3.6M Protocol#2 and MuPoTs. C stands for a conventional adaptation from <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">chen2016synthesizing </a>; <a href="#bib.bib30" title="" class="ltx_ref">long2015learning </a>; <a href="#bib.bib54" title="" class="ltx_ref">sun2016deep </a></cite>, SAA for semantic aware adaptation, Jo2D for adding 2D pose estimation tasks from additional 2D human pose dataset (MSCOCO and MPII datasets), SPA for adding skeletal pose adaptation.</span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.1.1" class="ltx_tr">
<th id="S4.T1.4.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S4.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" colspan="3"><span id="S4.T1.4.1.1.2.1" class="ltx_text" style="font-size:80%;">Human3.6M Protocol#2</span></th>
<th id="S4.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3"><span id="S4.T1.4.1.1.3.1" class="ltx_text" style="font-size:80%;">MuPoTs</span></th>
</tr>
<tr id="S4.T1.4.2.2" class="ltx_tr">
<th id="S4.T1.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.4.2.2.1.1" class="ltx_text" style="font-size:80%;">Training Dataset + Adaptation Strategies</span></th>
<th id="S4.T1.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.2.2.2.1" class="ltx_text" style="font-size:80%;">PA MPJPE</span></th>
<th id="S4.T1.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.2.2.3.1" class="ltx_text" style="font-size:80%;">3DPCK</span></th>
<th id="S4.T1.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t"><span id="S4.T1.4.2.2.4.1" class="ltx_text" style="font-size:80%;">AUC</span></th>
<th id="S4.T1.4.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.2.2.5.1" class="ltx_text" style="font-size:80%;">PA MPJPE</span></th>
<th id="S4.T1.4.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.2.2.6.1" class="ltx_text" style="font-size:80%;">3DPCK</span></th>
<th id="S4.T1.4.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.2.2.7.1" class="ltx_text" style="font-size:80%;">AUC</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.3.1" class="ltx_tr">
<th id="S4.T1.4.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T1.4.3.1.1.1" class="ltx_text" style="font-size:80%;">ScanAva+ </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib29" title="" class="ltx_ref">liu2018semi </a></cite>
</th>
<td id="S4.T1.4.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.3.1.2.1" class="ltx_text" style="font-size:80%;">180.3</span></td>
<td id="S4.T1.4.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.3.1.3.1" class="ltx_text" style="font-size:80%;">29.3</span></td>
<td id="S4.T1.4.3.1.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S4.T1.4.3.1.4.1" class="ltx_text" style="font-size:80%;">20.3</span></td>
<td id="S4.T1.4.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.3.1.5.1" class="ltx_text" style="font-size:80%;">240.8</span></td>
<td id="S4.T1.4.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.3.1.6.1" class="ltx_text" style="font-size:80%;">32.9</span></td>
<td id="S4.T1.4.3.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.3.1.7.1" class="ltx_text" style="font-size:80%;">11.9</span></td>
</tr>
<tr id="S4.T1.4.4.2" class="ltx_tr">
<th id="S4.T1.4.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.4.2.1.1" class="ltx_text" style="font-size:80%;">ScanAva+ + C </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">chen2016synthesizing </a></cite>
</th>
<td id="S4.T1.4.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.2.2.1" class="ltx_text" style="font-size:80%;">156.5</span></td>
<td id="S4.T1.4.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.2.3.1" class="ltx_text" style="font-size:80%;">58.8</span></td>
<td id="S4.T1.4.4.2.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.4.4.2.4.1" class="ltx_text" style="font-size:80%;">24.4</span></td>
<td id="S4.T1.4.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.2.5.1" class="ltx_text" style="font-size:80%;">197.7</span></td>
<td id="S4.T1.4.4.2.6" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.2.6.1" class="ltx_text" style="font-size:80%;">43.7</span></td>
<td id="S4.T1.4.4.2.7" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.2.7.1" class="ltx_text" style="font-size:80%;">17.0</span></td>
</tr>
<tr id="S4.T1.4.5.3" class="ltx_tr">
<th id="S4.T1.4.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.4.5.3.1.1" class="ltx_text" style="font-size:80%;">ScanAva+ + SAA</span></th>
<td id="S4.T1.4.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.4.5.3.2.1" class="ltx_text" style="font-size:80%;">99.9</span></td>
<td id="S4.T1.4.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.5.3.3.1" class="ltx_text" style="font-size:80%;">84.0</span></td>
<td id="S4.T1.4.5.3.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.4.5.3.4.1" class="ltx_text" style="font-size:80%;">41.7</span></td>
<td id="S4.T1.4.5.3.5" class="ltx_td ltx_align_center"><span id="S4.T1.4.5.3.5.1" class="ltx_text" style="font-size:80%;">118.4</span></td>
<td id="S4.T1.4.5.3.6" class="ltx_td ltx_align_center"><span id="S4.T1.4.5.3.6.1" class="ltx_text" style="font-size:80%;">76.1</span></td>
<td id="S4.T1.4.5.3.7" class="ltx_td ltx_align_center"><span id="S4.T1.4.5.3.7.1" class="ltx_text" style="font-size:80%;">36.2</span></td>
</tr>
<tr id="S4.T1.4.6.4" class="ltx_tr">
<th id="S4.T1.4.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.4.6.4.1.1" class="ltx_text" style="font-size:80%;">ScanAva+ + SAA + Jo2D</span></th>
<td id="S4.T1.4.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T1.4.6.4.2.1" class="ltx_text" style="font-size:80%;">88.8</span></td>
<td id="S4.T1.4.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.6.4.3.1" class="ltx_text" style="font-size:80%;">89.0</span></td>
<td id="S4.T1.4.6.4.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.4.6.4.4.1" class="ltx_text" style="font-size:80%;">45.9</span></td>
<td id="S4.T1.4.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T1.4.6.4.5.1" class="ltx_text" style="font-size:80%;">111.8</span></td>
<td id="S4.T1.4.6.4.6" class="ltx_td ltx_align_center"><span id="S4.T1.4.6.4.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">81.8</span></td>
<td id="S4.T1.4.6.4.7" class="ltx_td ltx_align_center"><span id="S4.T1.4.6.4.7.1" class="ltx_text" style="font-size:80%;">38.7</span></td>
</tr>
<tr id="S4.T1.4.7.5" class="ltx_tr">
<th id="S4.T1.4.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.4.7.5.1.1" class="ltx_text" style="font-size:80%;">ScanAva + SAA + Jo2D + SPA</span></th>
<td id="S4.T1.4.7.5.2" class="ltx_td ltx_align_center"><span id="S4.T1.4.7.5.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">85.1</span></td>
<td id="S4.T1.4.7.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.7.5.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">90.2</span></td>
<td id="S4.T1.4.7.5.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.4.7.5.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">47.9</span></td>
<td id="S4.T1.4.7.5.5" class="ltx_td ltx_align_center"><span id="S4.T1.4.7.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">111.6</span></td>
<td id="S4.T1.4.7.5.6" class="ltx_td ltx_align_center"><span id="S4.T1.4.7.5.6.1" class="ltx_text" style="font-size:80%;">81.6</span></td>
<td id="S4.T1.4.7.5.7" class="ltx_td ltx_align_center"><span id="S4.T1.4.7.5.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">39.0</span></td>
</tr>
<tr id="S4.T1.4.8.6" class="ltx_tr">
<th id="S4.T1.4.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">
<span id="S4.T1.4.8.6.1.1" class="ltx_text" style="font-size:80%;">SURREAL </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib61" title="" class="ltx_ref">varol17_surreal </a></cite>
</th>
<td id="S4.T1.4.8.6.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.4.8.6.2.1" class="ltx_text" style="font-size:80%;">181.4</span></td>
<td id="S4.T1.4.8.6.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.4.8.6.3.1" class="ltx_text" style="font-size:80%;">44.9</span></td>
<td id="S4.T1.4.8.6.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><span id="S4.T1.4.8.6.4.1" class="ltx_text" style="font-size:80%;">16.4</span></td>
<td id="S4.T1.4.8.6.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.4.8.6.5.1" class="ltx_text" style="font-size:80%;">187.0</span></td>
<td id="S4.T1.4.8.6.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.4.8.6.6.1" class="ltx_text" style="font-size:80%;">42.7</span></td>
<td id="S4.T1.4.8.6.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.4.8.6.7.1" class="ltx_text" style="font-size:80%;">15.3</span></td>
</tr>
<tr id="S4.T1.4.9.7" class="ltx_tr">
<th id="S4.T1.4.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.9.7.1.1" class="ltx_text" style="font-size:80%;">SURREAL + C </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">chen2016synthesizing </a></cite>
</th>
<td id="S4.T1.4.9.7.2" class="ltx_td ltx_align_center"><span id="S4.T1.4.9.7.2.1" class="ltx_text" style="font-size:80%;">145.5</span></td>
<td id="S4.T1.4.9.7.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.9.7.3.1" class="ltx_text" style="font-size:80%;">39.7</span></td>
<td id="S4.T1.4.9.7.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.4.9.7.4.1" class="ltx_text" style="font-size:80%;">27.8</span></td>
<td id="S4.T1.4.9.7.5" class="ltx_td ltx_align_center"><span id="S4.T1.4.9.7.5.1" class="ltx_text" style="font-size:80%;">165.1</span></td>
<td id="S4.T1.4.9.7.6" class="ltx_td ltx_align_center"><span id="S4.T1.4.9.7.6.1" class="ltx_text" style="font-size:80%;">55.5,</span></td>
<td id="S4.T1.4.9.7.7" class="ltx_td ltx_align_center"><span id="S4.T1.4.9.7.7.1" class="ltx_text" style="font-size:80%;">21.7</span></td>
</tr>
<tr id="S4.T1.4.10.8" class="ltx_tr">
<th id="S4.T1.4.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.4.10.8.1.1" class="ltx_text" style="font-size:80%;">SURREAL + SAA</span></th>
<td id="S4.T1.4.10.8.2" class="ltx_td ltx_align_center"><span id="S4.T1.4.10.8.2.1" class="ltx_text" style="font-size:80%;">138.6</span></td>
<td id="S4.T1.4.10.8.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.10.8.3.1" class="ltx_text" style="font-size:80%;">42.3</span></td>
<td id="S4.T1.4.10.8.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.4.10.8.4.1" class="ltx_text" style="font-size:80%;">29.1</span></td>
<td id="S4.T1.4.10.8.5" class="ltx_td ltx_align_center"><span id="S4.T1.4.10.8.5.1" class="ltx_text" style="font-size:80%;">145.6</span></td>
<td id="S4.T1.4.10.8.6" class="ltx_td ltx_align_center"><span id="S4.T1.4.10.8.6.1" class="ltx_text" style="font-size:80%;">64.3</span></td>
<td id="S4.T1.4.10.8.7" class="ltx_td ltx_align_center"><span id="S4.T1.4.10.8.7.1" class="ltx_text" style="font-size:80%;">26.6</span></td>
</tr>
<tr id="S4.T1.4.11.9" class="ltx_tr">
<th id="S4.T1.4.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.4.11.9.1.1" class="ltx_text" style="font-size:80%;">SURREAL + SAA +Jo2D</span></th>
<td id="S4.T1.4.11.9.2" class="ltx_td ltx_align_center"><span id="S4.T1.4.11.9.2.1" class="ltx_text" style="font-size:80%;">135.5</span></td>
<td id="S4.T1.4.11.9.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.11.9.3.1" class="ltx_text" style="font-size:80%;">46.2</span></td>
<td id="S4.T1.4.11.9.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.4.11.9.4.1" class="ltx_text" style="font-size:80%;">30.3</span></td>
<td id="S4.T1.4.11.9.5" class="ltx_td ltx_align_center"><span id="S4.T1.4.11.9.5.1" class="ltx_text" style="font-size:80%;">134.6</span></td>
<td id="S4.T1.4.11.9.6" class="ltx_td ltx_align_center"><span id="S4.T1.4.11.9.6.1" class="ltx_text" style="font-size:80%;">71.3</span></td>
<td id="S4.T1.4.11.9.7" class="ltx_td ltx_align_center"><span id="S4.T1.4.11.9.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">30.0</span></td>
</tr>
<tr id="S4.T1.4.12.10" class="ltx_tr">
<th id="S4.T1.4.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S4.T1.4.12.10.1.1" class="ltx_text" style="font-size:80%;">SURREAL + SAA + Jo2D + SPA</span></th>
<td id="S4.T1.4.12.10.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.4.12.10.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">135.3</span></td>
<td id="S4.T1.4.12.10.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.4.12.10.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">68.8</span></td>
<td id="S4.T1.4.12.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr"><span id="S4.T1.4.12.10.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">30.3</span></td>
<td id="S4.T1.4.12.10.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.4.12.10.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">134.6</span></td>
<td id="S4.T1.4.12.10.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.4.12.10.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">71.4</span></td>
<td id="S4.T1.4.12.10.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.4.12.10.7.1" class="ltx_text" style="font-size:80%;">29.8</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation Metrics</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To provide a comprehensive view in our evaluation, we employ extensively-used metrics from real human pose benchmarks to report our performance, including mean per joint position error (MPJPE) for Human3.6M <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">h36m_pami </a></cite>, 3D percentage of correct key-points (3DPCK), and the area under curve (AUC) for MuPoTS <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">singleshotmultiperson2018 </a></cite>.
For MPJPE, we also reported the Procrustes analysis (PA MPJPE) version
<cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">gower1975generalized </a></cite>, which is more reliable and fair, especially for cross-set evaluation due to varying camera parameters, joint definition, and body shape distributions.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">For 3DPCK, we follow the official configuration of <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">singleshotmultiperson2018 </a></cite> with a 15cm tolerance for joint location estimation accuracy. We assume every human is correctly detected and compare all cases with pelvis rooted error. For Human3.6M, we also follow 2nd protocol during evaluation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib66" title="" class="ltx_ref">yasin2016dual </a>; <a href="#bib.bib39" title="" class="ltx_ref">moon2019camera </a></cite>,
where subjects 9 and 11 are used for testing <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">bogo2016keep </a></cite>. Due to the joint definition differences, we use Human3.6M as a template and map the similar joints and interpolate missing ones for other datasets.
Please note that in the original protocols used in the majority of the 3D pose estimation works, the training split of the Human3.6M is employed during training, which we do not use at all. This makes our task a more challenging case (and at the same time more realistic in nature) than the original protocol.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">To evaluate how the proposed AHuP framework can enhance the 3D pose estimation with only 3D synthetic data, we added each component one-by-one to form the following settings: (1) pure 3D synthetic data based learning either with SURREAL or ScanAva+, (2) learning with conventional adaptation approach by aligning the whole feature space directly similar to <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">chen2016synthesizing </a>; <a href="#bib.bib30" title="" class="ltx_ref">long2015learning </a>; <a href="#bib.bib54" title="" class="ltx_ref">sun2016deep </a></cite>, named with suffix ‚ÄôC‚Äô, (3) semantic aware adaptation with suffix ‚ÄôSAA‚Äô, (4) further adding 2D pose task from additional 2D human pose dataset (MSCOCO and MPII datasets) with suffix ‚ÄôJo2D‚Äô, (5) further adding skeletal pose adaptation with suffix ‚ÄôSPA‚Äô.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">From the results shown in Tab.¬†<a href="#S4.T1" title="Table 1 ‚Ä£ 4.2 Evaluation Datasets ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can see that conventional adaptation by aligning the whole features <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref">tzeng2017adversarial </a>; <a href="#bib.bib58" title="" class="ltx_ref">tzeng2015simultaneous </a></cite> does improve the performance on both synthetic datasets across all real benchmarks.
By employing the SAA strategy, the improvement is significant on ScanAva+, and slightly but still noticeable on SURREAL.
Additional 2D pose tasks from the real 2D human dataset shows further improvement, which agrees with the existing studies <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite>. SPA shows noticeable improvement for both ScanAva+ and SURREAL on Human3.6M but not much difference on MuPoTS. Although MuCo is the training split for MuPoTS <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">singleshotmultiperson2018 </a></cite>, the two datasets are in fact captured in different environments separately,
which could result in differences in the skeletal descriptors.
We still notice that in the SURREAL case, although the improvement is not obvious on PA MPJPE, the 3DPCK metric is improved significantly.
It shows that by adding SPA , many of the pose errors fall back into the tolerance range.
</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Qualitative Study in Pose and Feature Spaces</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Tab.¬†<a href="#S4.T1" title="Table 1 ‚Ä£ 4.2 Evaluation Datasets ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows that applying the AHuP approach leads to pose estimation performance improvement on both real 3D pose benchmarks; however, there are much stronger improvements when ScanAva+ dataset is used for training compared to the SURREAL. To figure out the underlying reason, we further investigated the characteristics of the datasets themselves. We randomly extracted 5000 3D human pose samples from all four 3D datasets, including real (Human3.6M and MuPoTS) and synthetic (ScanAva+ and SURREAL) pose datasets, and visualized them via a t-distributed stochastic neighbor embedding (t-SNE) approach <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref">maaten2008visualizing </a></cite> in Fig.¬†<a href="#S4.F3" title="Figure 3 ‚Ä£ 4.5 Qualitative Study in Pose and Feature Spaces ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
This plot is purely based on the raw pelvis rooted 3D pose data
without filtering after matching the joint order across datasets,
in order to reflect the essential pose difference among these sets. From the plot, surprisingly we found out a higher agreement between all real datasets and ScanAva, yet a clear boundary around SURREAL.
It seems SURREAL does not hold a well-overlapping pose manifold with the others.
The causes could be multi-fold, including the camera setting and joint definition in their rendering process, which all possibly affect the final pose distribution. When SURREAL is the only 3D source, the model can hardly learn any more than its pose coverage. This presumes to be the cause of limited improvements in the case when only SURREAL data is used.
</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2105.10837/assets/figures/p5000.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">t-SNE plot of pelvis rooted 3D pose data across 5000 data samples randomly extracted from Human3.6M, MuPoTS, ScanAva+ and SURREAL. </span></figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.10837/assets/figures/D0-4.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="150" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.10837/assets/figures/C.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="150" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.10837/assets/figures/SAA.png" id="S4.F4.sf3.g1" class="ltx_graphics ltx_img_landscape" width="150" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.10837/assets/figures/SAA-y.png" id="S4.F4.sf4.g1" class="ltx_graphics ltx_img_landscape" width="150" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.4.2.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.2.1" class="ltx_text" style="font-size:90%;">t-SNE plot of network <math id="S4.F4.2.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S4.F4.2.1.m1.1b"><mi id="S4.F4.2.1.m1.1.1" xref="S4.F4.2.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.F4.2.1.m1.1c"><ci id="S4.F4.2.1.m1.1.1.cmml" xref="S4.F4.2.1.m1.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.2.1.m1.1d">G</annotation></semantics></math>‚Äôs output features applied on Human3.6M, MuPoTS, ScanAva+, and SURREAL datasets under model configurations of (a) no adaptation, (b) with a conventional adaptor C, (c) with SAA, (d) with SAA + Jo2D.</span></figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.3" class="ltx_p">It is also interesting to investigate how these approaches influence the feature space. To better illustrate this, we visualized the output features of <math id="S4.SS5.p2.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S4.SS5.p2.1.m1.1a"><mi id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><ci id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">G</annotation></semantics></math> from all four datasets under different model configurations as shown in Fig.¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.5 Qualitative Study in Pose and Feature Spaces ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Due to computational intensity and also to prevent a cluttered visualization, we evenly sampled 100 images from each dataset, with the <math id="S4.SS5.p2.2.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S4.SS5.p2.2.m2.1a"><mi id="S4.SS5.p2.2.m2.1.1" xref="S4.SS5.p2.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><ci id="S4.SS5.p2.2.m2.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">G</annotation></semantics></math> network‚Äôs output features downsampled to <math id="S4.SS5.p2.3.m3.1" class="ltx_Math" alttext="1024\times 4\times 4" display="inline"><semantics id="S4.SS5.p2.3.m3.1a"><mrow id="S4.SS5.p2.3.m3.1.1" xref="S4.SS5.p2.3.m3.1.1.cmml"><mn id="S4.SS5.p2.3.m3.1.1.2" xref="S4.SS5.p2.3.m3.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS5.p2.3.m3.1.1.1" xref="S4.SS5.p2.3.m3.1.1.1.cmml">√ó</mo><mn id="S4.SS5.p2.3.m3.1.1.3" xref="S4.SS5.p2.3.m3.1.1.3.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS5.p2.3.m3.1.1.1a" xref="S4.SS5.p2.3.m3.1.1.1.cmml">√ó</mo><mn id="S4.SS5.p2.3.m3.1.1.4" xref="S4.SS5.p2.3.m3.1.1.4.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.3.m3.1b"><apply id="S4.SS5.p2.3.m3.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1"><times id="S4.SS5.p2.3.m3.1.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1.1"></times><cn type="integer" id="S4.SS5.p2.3.m3.1.1.2.cmml" xref="S4.SS5.p2.3.m3.1.1.2">1024</cn><cn type="integer" id="S4.SS5.p2.3.m3.1.1.3.cmml" xref="S4.SS5.p2.3.m3.1.1.3">4</cn><cn type="integer" id="S4.SS5.p2.3.m3.1.1.4.cmml" xref="S4.SS5.p2.3.m3.1.1.4">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.3.m3.1c">1024\times 4\times 4</annotation></semantics></math>.
Fig.¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.5 Qualitative Study in Pose and Feature Spaces ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows how differently each dataset is in the ‚Äúeye‚Äù of these models. In Fig.¬†<a href="#S4.F4.sf1" title="In Figure 4 ‚Ä£ 4.5 Qualitative Study in Pose and Feature Spaces ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>, all datasets show clear clustering effect.
This is reasonable as the model based on only synthetic data can hardly generalize well to capture the shared features for both of real and synthetic data.
With the adaptor introduction in Fig.¬†<a href="#S4.F4.sf2" title="In Figure 4 ‚Ä£ 4.5 Qualitative Study in Pose and Feature Spaces ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a>, the clustering effect has been eliminated much, but we notice that the synthetic features are more located on the right hand side for both SURREAL and ScanAva+. With SAA as shown in Fig.¬†<a href="#S4.F4.sf3" title="In Figure 4 ‚Ä£ 4.5 Qualitative Study in Pose and Feature Spaces ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(c)</span></a>, there is no obvious improvement over the C version, but the synthetic features are more evenly distributed in the space. With additional 2D task from real 2D human pose in Fig.¬†<a href="#S4.F4.sf4" title="In Figure 4 ‚Ä£ 4.5 Qualitative Study in Pose and Feature Spaces ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(d)</span></a>, the distribution becomes flattened. One interesting observation is that SURREAL shows a more obvious clustering effect than the adaptation-only version.
Instead of losing the generalization ability, we believe this clustering effect on the contrary indicates an improved ability of the network to recognize different poses. As SURREAL does not hold a similar pose distribution to the others, it supposes to show a different pattern in a well recognized pose space.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Adaptation of SOTA Models using AHuP</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">Although our proposed AHuP approach shows improvement over the models trained on the synthetic human data,
one immediate question is why bother to use AHuP given that well-performed 3D human pose models already exist?
Here, we examine how AHuP is also capable of improving the performance of the existing SOTA 3D pose estimation models when tested under a different context or dataset from their training set.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">In order to conduct a fair comparison, we designed a cross-set evaluation experiment that all candidate models are trained and tested on different datasets to mimic the effect when they are employed in applications under a novel context.
We chose two SOTA pose estimation models introduced by Sun et al. in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a></cite> and by Zhou et al. in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite> in this experimental analysis. To cover more possible scenarios, for top performer <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a></cite>, we trained two networks with Human3.6M + MPII + MSCOCO and MuCo + MPII + MSCOCO, respectively.
For <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite>, we employed their official release of the pre-trained model on Human3.6M + MPII.
We tested both model on a 3D human pose dataset that was novel for them. As they already learned from the real domains with limited shift in the appearance, we only evaluated the effect of our SPA on these models.
Their model performance with or without SPA is reported in Tab.¬†<a href="#S4.T2" title="Table 2 ‚Ä£ 4.7 Practical Values of AHuP ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The results illustrate that adding AHuP adaptation in the form of SPA to these models leads to consistent performance improvement over the versions without SPA. These improvements are seen in both models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a>; <a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite>,in all of the training set combinations, and for different types of test sets.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Practical Values of AHuP</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">SPA can be employed as a switchable head on top of an existing network such as ResNet to act as a light-weighted adaptation strategy.
The adaptation can be achieved by simply switching the SPA head without retraining the network itself. The benefits of this SPA adaptation strategy include:
</p>
</div>
<div id="S4.SS7.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Time efficient adaptation: To evaluate a model on different benchmarks, it is common to train a specific model for each of the new benchmarks, as suggested by <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">moon2019camera </a></cite>. However, retraining the model in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a></cite> on a new dataset takes two days <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">moon2019camera </a></cite>. In contrast, training a SPA head takes less than 20 minutes.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Memory efficient storage: Suppose we train a new model for each new dataset/context; the storage cost will be proportional to the number of the datasets scaled by the size of the network. However, in SPA adaptation, we only need one copy of the model (e.g., PoseNet) with different SPA adaptation heads. An example of the storage comparison for models in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a>; <a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite> with and without SPA strategy is shown in Fig.¬†<a href="#S4.F5" title="Figure 5 ‚Ä£ 4.7 Practical Values of AHuP ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, where the more potential datasets there are to work on, the more memory saving our SPA adaptation it will lead to.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Efficient computational cost: Adding SPA in inference processes will inevitably increase the calculation cost, but the cost increase is actually negligible compared to the computation of the original network. A comparison of computational cost of the two models in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a>; <a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite> is shown in Tab.¬†<a href="#S4.T3" title="Table 3 ‚Ä£ 4.7 Practical Values of AHuP ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</li>
</ul>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.3.2" class="ltx_text" style="font-size:90%;">Cross benchmark evaluation of the 3D pose estimation models in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a></cite> and <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite>, when trained and evaluated on different 3D pose datasets, with and without SPA. MuCo is the official training portion of the MuPoTS dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">singleshotmultiperson2018 </a></cite>.</span></figcaption>
<div id="S4.T2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:173.4pt;height:286.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.7pt,60.7pt) scale(0.702522902623854,0.702522902623854) ;">
<table id="S4.T2.4.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.4.1.1.1" class="ltx_tr">
<td id="S4.T2.4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">Benchmarks</td>
<td id="S4.T2.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.4.1.1.1.2.1" class="ltx_text ltx_font_bold">SPA</span></td>
<td id="S4.T2.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">MPJPE PA</td>
<td id="S4.T2.4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">3DPCK</td>
<td id="S4.T2.4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">AUC</td>
</tr>
<tr id="S4.T2.4.1.2.2" class="ltx_tr">
<td id="S4.T2.4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t" colspan="5">Sun et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a></cite> trained on MuCo + MSCOCO + MPII</td>
</tr>
<tr id="S4.T2.4.1.3.3" class="ltx_tr">
<td id="S4.T2.4.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">Human36M</td>
<td id="S4.T2.4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">82.1</td>
<td id="S4.T2.4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">75.8</td>
<td id="S4.T2.4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">49.0</td>
</tr>
<tr id="S4.T2.4.1.4.4" class="ltx_tr">
<td id="S4.T2.4.1.4.4.1" class="ltx_td"></td>
<td id="S4.T2.4.1.4.4.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.4.1.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.4.4.3.1" class="ltx_text ltx_font_bold">77.9</span></td>
<td id="S4.T2.4.1.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.4.4.4.1" class="ltx_text ltx_font_bold">92.7</span></td>
<td id="S4.T2.4.1.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.4.4.5.1" class="ltx_text ltx_font_bold">51.5</span></td>
</tr>
<tr id="S4.T2.4.1.5.5" class="ltx_tr">
<td id="S4.T2.4.1.5.5.1" class="ltx_td ltx_align_center ltx_border_t">ScanAva+</td>
<td id="S4.T2.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">92.6</td>
<td id="S4.T2.4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">87.7</td>
<td id="S4.T2.4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">43.7</td>
</tr>
<tr id="S4.T2.4.1.6.6" class="ltx_tr">
<td id="S4.T2.4.1.6.6.1" class="ltx_td"></td>
<td id="S4.T2.4.1.6.6.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.4.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.6.6.3.1" class="ltx_text ltx_font_bold">91.7</span></td>
<td id="S4.T2.4.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.6.6.4.1" class="ltx_text ltx_font_bold">87.7</span></td>
<td id="S4.T2.4.1.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.6.6.5.1" class="ltx_text ltx_font_bold">44.2</span></td>
</tr>
<tr id="S4.T2.4.1.7.7" class="ltx_tr">
<td id="S4.T2.4.1.7.7.1" class="ltx_td ltx_align_center ltx_border_t">SURREAL</td>
<td id="S4.T2.4.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.4.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">154.8</td>
<td id="S4.T2.4.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">63.0</td>
<td id="S4.T2.4.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">26.6</td>
</tr>
<tr id="S4.T2.4.1.8.8" class="ltx_tr">
<td id="S4.T2.4.1.8.8.1" class="ltx_td"></td>
<td id="S4.T2.4.1.8.8.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.4.1.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.8.8.3.1" class="ltx_text ltx_font_bold">154.2</span></td>
<td id="S4.T2.4.1.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.8.8.4.1" class="ltx_text ltx_font_bold">63.2</span></td>
<td id="S4.T2.4.1.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.8.8.5.1" class="ltx_text ltx_font_bold">27.0</span></td>
</tr>
<tr id="S4.T2.4.1.9.9" class="ltx_tr">
<td id="S4.T2.4.1.9.9.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="5">Sun et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a></cite> trained on Human3.6M + MSCOCO + MPII</td>
</tr>
<tr id="S4.T2.4.1.10.10" class="ltx_tr">
<td id="S4.T2.4.1.10.10.1" class="ltx_td ltx_align_center ltx_border_t">MuPoTS</td>
<td id="S4.T2.4.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.4.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t">111.1</td>
<td id="S4.T2.4.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.4.1.10.10.4.1" class="ltx_text ltx_font_bold">84.1</span></td>
<td id="S4.T2.4.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t">41.0</td>
</tr>
<tr id="S4.T2.4.1.11.11" class="ltx_tr">
<td id="S4.T2.4.1.11.11.1" class="ltx_td"></td>
<td id="S4.T2.4.1.11.11.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.4.1.11.11.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.11.11.3.1" class="ltx_text ltx_font_bold">105.9</span></td>
<td id="S4.T2.4.1.11.11.4" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.11.11.4.1" class="ltx_text ltx_font_bold">84.1</span></td>
<td id="S4.T2.4.1.11.11.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.11.11.5.1" class="ltx_text ltx_font_bold">41.1</span></td>
</tr>
<tr id="S4.T2.4.1.12.12" class="ltx_tr">
<td id="S4.T2.4.1.12.12.1" class="ltx_td ltx_align_center ltx_border_t">ScanAva+</td>
<td id="S4.T2.4.1.12.12.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.4.1.12.12.3" class="ltx_td ltx_align_center ltx_border_t">111.8</td>
<td id="S4.T2.4.1.12.12.4" class="ltx_td ltx_align_center ltx_border_t">77.5</td>
<td id="S4.T2.4.1.12.12.5" class="ltx_td ltx_align_center ltx_border_t">38.1</td>
</tr>
<tr id="S4.T2.4.1.13.13" class="ltx_tr">
<td id="S4.T2.4.1.13.13.1" class="ltx_td"></td>
<td id="S4.T2.4.1.13.13.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.4.1.13.13.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.13.13.3.1" class="ltx_text ltx_font_bold">106.9</span></td>
<td id="S4.T2.4.1.13.13.4" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.13.13.4.1" class="ltx_text ltx_font_bold">79.2</span></td>
<td id="S4.T2.4.1.13.13.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.13.13.5.1" class="ltx_text ltx_font_bold">40.8</span></td>
</tr>
<tr id="S4.T2.4.1.14.14" class="ltx_tr">
<td id="S4.T2.4.1.14.14.1" class="ltx_td ltx_align_center ltx_border_t">SURREAL</td>
<td id="S4.T2.4.1.14.14.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.4.1.14.14.3" class="ltx_td ltx_align_center ltx_border_t">171.8</td>
<td id="S4.T2.4.1.14.14.4" class="ltx_td ltx_align_center ltx_border_t">54.7</td>
<td id="S4.T2.4.1.14.14.5" class="ltx_td ltx_align_center ltx_border_t">22.3</td>
</tr>
<tr id="S4.T2.4.1.15.15" class="ltx_tr">
<td id="S4.T2.4.1.15.15.1" class="ltx_td"></td>
<td id="S4.T2.4.1.15.15.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.4.1.15.15.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.15.15.3.1" class="ltx_text ltx_font_bold">170.1</span></td>
<td id="S4.T2.4.1.15.15.4" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.15.15.4.1" class="ltx_text ltx_font_bold">55.7</span></td>
<td id="S4.T2.4.1.15.15.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.15.15.5.1" class="ltx_text ltx_font_bold">23.2</span></td>
</tr>
<tr id="S4.T2.4.1.16.16" class="ltx_tr">
<td id="S4.T2.4.1.16.16.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="5">Zhou et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite> trained on Human3.6M + MPII</td>
</tr>
<tr id="S4.T2.4.1.17.17" class="ltx_tr">
<td id="S4.T2.4.1.17.17.1" class="ltx_td ltx_align_center ltx_border_t">MuPoTS</td>
<td id="S4.T2.4.1.17.17.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.4.1.17.17.3" class="ltx_td ltx_align_center ltx_border_t">111.8</td>
<td id="S4.T2.4.1.17.17.4" class="ltx_td ltx_align_center ltx_border_t">77.5</td>
<td id="S4.T2.4.1.17.17.5" class="ltx_td ltx_align_center ltx_border_t">38.1</td>
</tr>
<tr id="S4.T2.4.1.18.18" class="ltx_tr">
<td id="S4.T2.4.1.18.18.1" class="ltx_td"></td>
<td id="S4.T2.4.1.18.18.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.4.1.18.18.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.18.18.3.1" class="ltx_text ltx_font_bold">106.9</span></td>
<td id="S4.T2.4.1.18.18.4" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.18.18.4.1" class="ltx_text ltx_font_bold">79.2</span></td>
<td id="S4.T2.4.1.18.18.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.18.18.5.1" class="ltx_text ltx_font_bold">40.8</span></td>
</tr>
<tr id="S4.T2.4.1.19.19" class="ltx_tr">
<td id="S4.T2.4.1.19.19.1" class="ltx_td ltx_align_center ltx_border_t">ScanAva+</td>
<td id="S4.T2.4.1.19.19.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.4.1.19.19.3" class="ltx_td ltx_align_center ltx_border_t">91.6</td>
<td id="S4.T2.4.1.19.19.4" class="ltx_td ltx_align_center ltx_border_t">87.5</td>
<td id="S4.T2.4.1.19.19.5" class="ltx_td ltx_align_center ltx_border_t">44.0</td>
</tr>
<tr id="S4.T2.4.1.20.20" class="ltx_tr">
<td id="S4.T2.4.1.20.20.1" class="ltx_td"></td>
<td id="S4.T2.4.1.20.20.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.4.1.20.20.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.20.20.3.1" class="ltx_text ltx_font_bold">89.1</span></td>
<td id="S4.T2.4.1.20.20.4" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.20.20.4.1" class="ltx_text ltx_font_bold">87.8</span></td>
<td id="S4.T2.4.1.20.20.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.20.20.5.1" class="ltx_text ltx_font_bold">45.5</span></td>
</tr>
<tr id="S4.T2.4.1.21.21" class="ltx_tr">
<td id="S4.T2.4.1.21.21.1" class="ltx_td ltx_align_center ltx_border_t">SURREAL</td>
<td id="S4.T2.4.1.21.21.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.4.1.21.21.3" class="ltx_td ltx_align_center ltx_border_t">156.3</td>
<td id="S4.T2.4.1.21.21.4" class="ltx_td ltx_align_center ltx_border_t">62.7</td>
<td id="S4.T2.4.1.21.21.5" class="ltx_td ltx_align_center ltx_border_t">26.4</td>
</tr>
<tr id="S4.T2.4.1.22.22" class="ltx_tr">
<td id="S4.T2.4.1.22.22.1" class="ltx_td ltx_border_b"></td>
<td id="S4.T2.4.1.22.22.2" class="ltx_td ltx_align_center ltx_border_b">‚úì</td>
<td id="S4.T2.4.1.22.22.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.4.1.22.22.3.1" class="ltx_text ltx_font_bold">153.6</span></td>
<td id="S4.T2.4.1.22.22.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.4.1.22.22.4.1" class="ltx_text ltx_font_bold">63.6</span></td>
<td id="S4.T2.4.1.22.22.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.4.1.22.22.5.1" class="ltx_text ltx_font_bold">28.2</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.10837/assets/x2.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="207" height="155" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.10837/assets/x3.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="207" height="155" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Memory usage based on the number of parameters in million (M) in the SOTA models from <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a></cite> and <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite>, when customized for varying numbers of datasets with and without SPA: (a) in full scale, (b) zoomed in to show the slope differences.</span></figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.3.2" class="ltx_text" style="font-size:90%;">Computational cost of the models in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a></cite> and <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite> with and without SPA in Gigaflops.</span></figcaption>
<div id="S4.T3.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:173.4pt;height:17.8pt;vertical-align:-0.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-180.9pt,18.2pt) scale(0.324078526757009,0.324078526757009) ;">
<table id="S4.T3.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.4.1.1.1" class="ltx_tr">
<th id="S4.T3.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Models</th>
<th id="S4.T3.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Original</th>
<th id="S4.T3.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">+SPA</th>
<th id="S4.T3.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Computational cost increase (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.1.2.1" class="ltx_tr">
<td id="S4.T3.4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Sun et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a></cite>
</td>
<td id="S4.T3.4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">13.097</td>
<td id="S4.T3.4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">13.101</td>
<td id="S4.T3.4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.031%</td>
</tr>
<tr id="S4.T3.4.1.3.2" class="ltx_tr">
<td id="S4.T3.4.1.3.2.1" class="ltx_td ltx_align_center ltx_border_b">Zhou et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">zhou2017towards </a></cite>
</td>
<td id="S4.T3.4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b">12.003</td>
<td id="S4.T3.4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b">12.007</td>
<td id="S4.T3.4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b">0.033%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.3.2" class="ltx_text" style="font-size:90%;">Comparison with the state-of-the-art based on the MPJPE metric tested on Human3.6M dataset using Protocol#2. Please note that unlike AHuP, all of these models have used some real 3D human pose data in their training process. For our method, PA stands for when the PA rigid alignment is applied. The last two rows stands for Martinez approach <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">martinez2017simple </a></cite> trained on ScanAva+ dataset.</span></figcaption>
<div id="S4.T4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:179.2pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-220.1pt,90.7pt) scale(0.496287993636386,0.496287993636386) ;">
<table id="S4.T4.4.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.4.1.1.1" class="ltx_tr">
<td id="S4.T4.4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Methods</span></td>
<td id="S4.T4.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Dir</span></td>
<td id="S4.T4.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Dis</span></td>
<td id="S4.T4.4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Eat</span></td>
<td id="S4.T4.4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Gre</span></td>
<td id="S4.T4.4.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.6.1" class="ltx_text" style="font-size:90%;">Phon.</span></td>
<td id="S4.T4.4.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.7.1" class="ltx_text" style="font-size:90%;">Pose</span></td>
<td id="S4.T4.4.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.8.1" class="ltx_text" style="font-size:90%;">Pur.</span></td>
<td id="S4.T4.4.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.9.1" class="ltx_text" style="font-size:90%;">Sit</span></td>
<td id="S4.T4.4.1.1.1.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.10.1" class="ltx_text" style="font-size:90%;">SitD</span></td>
<td id="S4.T4.4.1.1.1.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.11.1" class="ltx_text" style="font-size:90%;">Smo.</span></td>
<td id="S4.T4.4.1.1.1.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.12.1" class="ltx_text" style="font-size:90%;">Phot.</span></td>
<td id="S4.T4.4.1.1.1.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.13.1" class="ltx_text" style="font-size:90%;">Wait</span></td>
<td id="S4.T4.4.1.1.1.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.14.1" class="ltx_text" style="font-size:90%;">Walk</span></td>
<td id="S4.T4.4.1.1.1.15" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.15.1" class="ltx_text" style="font-size:90%;">WalkD.</span></td>
<td id="S4.T4.4.1.1.1.16" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.16.1" class="ltx_text" style="font-size:90%;">WalkP.</span></td>
<td id="S4.T4.4.1.1.1.17" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.1.1.17.1" class="ltx_text" style="font-size:90%;">Avg</span></td>
</tr>
<tr id="S4.T4.4.1.2.2" class="ltx_tr">
<td id="S4.T4.4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T4.4.1.2.2.1.1" class="ltx_text" style="font-size:90%;">Akhter &amp; Black </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">akhter2015pose </a></cite>
</td>
<td id="S4.T4.4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.2.1" class="ltx_text" style="font-size:90%;">199.2</span></td>
<td id="S4.T4.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.3.1" class="ltx_text" style="font-size:90%;">177.6</span></td>
<td id="S4.T4.4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.4.1" class="ltx_text" style="font-size:90%;">161.8</span></td>
<td id="S4.T4.4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.5.1" class="ltx_text" style="font-size:90%;">197.8</span></td>
<td id="S4.T4.4.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.6.1" class="ltx_text" style="font-size:90%;">176.2</span></td>
<td id="S4.T4.4.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.7.1" class="ltx_text" style="font-size:90%;">186.5</span></td>
<td id="S4.T4.4.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.8.1" class="ltx_text" style="font-size:90%;">195.4</span></td>
<td id="S4.T4.4.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.9.1" class="ltx_text" style="font-size:90%;">167.3</span></td>
<td id="S4.T4.4.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.10.1" class="ltx_text" style="font-size:90%;">160.7</span></td>
<td id="S4.T4.4.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.11.1" class="ltx_text" style="font-size:90%;">173.7</span></td>
<td id="S4.T4.4.1.2.2.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.12.1" class="ltx_text" style="font-size:90%;">177.8</span></td>
<td id="S4.T4.4.1.2.2.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.13.1" class="ltx_text" style="font-size:90%;">181.9</span></td>
<td id="S4.T4.4.1.2.2.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.14.1" class="ltx_text" style="font-size:90%;">176.2</span></td>
<td id="S4.T4.4.1.2.2.15" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.15.1" class="ltx_text" style="font-size:90%;">198.6</span></td>
<td id="S4.T4.4.1.2.2.16" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.16.1" class="ltx_text" style="font-size:90%;">192.7</span></td>
<td id="S4.T4.4.1.2.2.17" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.17.1" class="ltx_text" style="font-size:90%;">181.1</span></td>
</tr>
<tr id="S4.T4.4.1.3.3" class="ltx_tr">
<td id="S4.T4.4.1.3.3.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.3.3.1.1" class="ltx_text" style="font-size:90%;">Ramakrishna </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib49" title="" class="ltx_ref">ramakrishna2012reconstructing </a></cite>
</td>
<td id="S4.T4.4.1.3.3.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.2.1" class="ltx_text" style="font-size:90%;">137.4</span></td>
<td id="S4.T4.4.1.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.3.1" class="ltx_text" style="font-size:90%;">149.3</span></td>
<td id="S4.T4.4.1.3.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.4.1" class="ltx_text" style="font-size:90%;">141.6</span></td>
<td id="S4.T4.4.1.3.3.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.5.1" class="ltx_text" style="font-size:90%;">154.3</span></td>
<td id="S4.T4.4.1.3.3.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.6.1" class="ltx_text" style="font-size:90%;">157.7</span></td>
<td id="S4.T4.4.1.3.3.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.7.1" class="ltx_text" style="font-size:90%;">158.9</span></td>
<td id="S4.T4.4.1.3.3.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.8.1" class="ltx_text" style="font-size:90%;">141.8</span></td>
<td id="S4.T4.4.1.3.3.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.9.1" class="ltx_text" style="font-size:90%;">158.1</span></td>
<td id="S4.T4.4.1.3.3.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.10.1" class="ltx_text" style="font-size:90%;">168.6</span></td>
<td id="S4.T4.4.1.3.3.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.11.1" class="ltx_text" style="font-size:90%;">175.6</span></td>
<td id="S4.T4.4.1.3.3.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.12.1" class="ltx_text" style="font-size:90%;">160.4</span></td>
<td id="S4.T4.4.1.3.3.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.13.1" class="ltx_text" style="font-size:90%;">161.7</span></td>
<td id="S4.T4.4.1.3.3.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.14.1" class="ltx_text" style="font-size:90%;">150.0</span></td>
<td id="S4.T4.4.1.3.3.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.15.1" class="ltx_text" style="font-size:90%;">174.8</span></td>
<td id="S4.T4.4.1.3.3.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.16.1" class="ltx_text" style="font-size:90%;">150.2</span></td>
<td id="S4.T4.4.1.3.3.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.3.3.17.1" class="ltx_text" style="font-size:90%;">157.3</span></td>
</tr>
<tr id="S4.T4.4.1.4.4" class="ltx_tr">
<td id="S4.T4.4.1.4.4.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.4.4.1.1" class="ltx_text" style="font-size:90%;">Zhou </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib70" title="" class="ltx_ref">zhou2016sparse </a></cite>
</td>
<td id="S4.T4.4.1.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.2.1" class="ltx_text" style="font-size:90%;">99.7</span></td>
<td id="S4.T4.4.1.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.3.1" class="ltx_text" style="font-size:90%;">95.8</span></td>
<td id="S4.T4.4.1.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.4.1" class="ltx_text" style="font-size:90%;">87.9</span></td>
<td id="S4.T4.4.1.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.5.1" class="ltx_text" style="font-size:90%;">116.8</span></td>
<td id="S4.T4.4.1.4.4.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.6.1" class="ltx_text" style="font-size:90%;">108.3</span></td>
<td id="S4.T4.4.1.4.4.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.7.1" class="ltx_text" style="font-size:90%;">107.3</span></td>
<td id="S4.T4.4.1.4.4.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.8.1" class="ltx_text" style="font-size:90%;">93.5</span></td>
<td id="S4.T4.4.1.4.4.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.9.1" class="ltx_text" style="font-size:90%;">95.3</span></td>
<td id="S4.T4.4.1.4.4.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.10.1" class="ltx_text" style="font-size:90%;">109.1</span></td>
<td id="S4.T4.4.1.4.4.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.11.1" class="ltx_text" style="font-size:90%;">137.5</span></td>
<td id="S4.T4.4.1.4.4.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.12.1" class="ltx_text" style="font-size:90%;">106.0</span></td>
<td id="S4.T4.4.1.4.4.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.13.1" class="ltx_text" style="font-size:90%;">102.2</span></td>
<td id="S4.T4.4.1.4.4.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.14.1" class="ltx_text" style="font-size:90%;">106.5</span></td>
<td id="S4.T4.4.1.4.4.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.15.1" class="ltx_text" style="font-size:90%;">110.4</span></td>
<td id="S4.T4.4.1.4.4.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.16.1" class="ltx_text" style="font-size:90%;">115.2</span></td>
<td id="S4.T4.4.1.4.4.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.4.17.1" class="ltx_text" style="font-size:90%;">106.7</span></td>
</tr>
<tr id="S4.T4.4.1.5.5" class="ltx_tr">
<td id="S4.T4.4.1.5.5.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.5.5.1.1" class="ltx_text" style="font-size:90%;">SMPLify </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">bogo2016keep </a></cite>
</td>
<td id="S4.T4.4.1.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.2.1" class="ltx_text" style="font-size:90%;">62.0</span></td>
<td id="S4.T4.4.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.3.1" class="ltx_text" style="font-size:90%;">60.2</span></td>
<td id="S4.T4.4.1.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.4.1" class="ltx_text" style="font-size:90%;">67.8</span></td>
<td id="S4.T4.4.1.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.5.1" class="ltx_text" style="font-size:90%;">76.5</span></td>
<td id="S4.T4.4.1.5.5.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.6.1" class="ltx_text" style="font-size:90%;">92.1</span></td>
<td id="S4.T4.4.1.5.5.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.7.1" class="ltx_text" style="font-size:90%;">77.0</span></td>
<td id="S4.T4.4.1.5.5.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.8.1" class="ltx_text" style="font-size:90%;">73.0</span></td>
<td id="S4.T4.4.1.5.5.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.9.1" class="ltx_text" style="font-size:90%;">75.3</span></td>
<td id="S4.T4.4.1.5.5.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.10.1" class="ltx_text" style="font-size:90%;">100.3</span></td>
<td id="S4.T4.4.1.5.5.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.11.1" class="ltx_text" style="font-size:90%;">137.3</span></td>
<td id="S4.T4.4.1.5.5.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.12.1" class="ltx_text" style="font-size:90%;">83.4</span></td>
<td id="S4.T4.4.1.5.5.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.13.1" class="ltx_text" style="font-size:90%;">77.3</span></td>
<td id="S4.T4.4.1.5.5.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.14.1" class="ltx_text" style="font-size:90%;">79.7</span></td>
<td id="S4.T4.4.1.5.5.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.15.1" class="ltx_text" style="font-size:90%;">86.8</span></td>
<td id="S4.T4.4.1.5.5.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.16.1" class="ltx_text" style="font-size:90%;">81.7</span></td>
<td id="S4.T4.4.1.5.5.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.5.17.1" class="ltx_text" style="font-size:90%;">82.3</span></td>
</tr>
<tr id="S4.T4.4.1.6.6" class="ltx_tr">
<td id="S4.T4.4.1.6.6.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.6.6.1.1" class="ltx_text" style="font-size:90%;">Chen</span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">chen20173d </a></cite>
</td>
<td id="S4.T4.4.1.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.2.1" class="ltx_text" style="font-size:90%;">89.9</span></td>
<td id="S4.T4.4.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.3.1" class="ltx_text" style="font-size:90%;">97.6</span></td>
<td id="S4.T4.4.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.4.1" class="ltx_text" style="font-size:90%;">90.0</span></td>
<td id="S4.T4.4.1.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.5.1" class="ltx_text" style="font-size:90%;">107.9</span></td>
<td id="S4.T4.4.1.6.6.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.6.1" class="ltx_text" style="font-size:90%;">107.3</span></td>
<td id="S4.T4.4.1.6.6.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.7.1" class="ltx_text" style="font-size:90%;">93.6</span></td>
<td id="S4.T4.4.1.6.6.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.8.1" class="ltx_text" style="font-size:90%;">136.1</span></td>
<td id="S4.T4.4.1.6.6.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.9.1" class="ltx_text" style="font-size:90%;">133.1</span></td>
<td id="S4.T4.4.1.6.6.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.10.1" class="ltx_text" style="font-size:90%;">240.1</span></td>
<td id="S4.T4.4.1.6.6.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.11.1" class="ltx_text" style="font-size:90%;">106.7</span></td>
<td id="S4.T4.4.1.6.6.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.12.1" class="ltx_text" style="font-size:90%;">139.2</span></td>
<td id="S4.T4.4.1.6.6.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.13.1" class="ltx_text" style="font-size:90%;">106.2</span></td>
<td id="S4.T4.4.1.6.6.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.14.1" class="ltx_text" style="font-size:90%;">87.0</span></td>
<td id="S4.T4.4.1.6.6.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.15.1" class="ltx_text" style="font-size:90%;">114.1</span></td>
<td id="S4.T4.4.1.6.6.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.16.1" class="ltx_text" style="font-size:90%;">90.6</span></td>
<td id="S4.T4.4.1.6.6.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.6.17.1" class="ltx_text" style="font-size:90%;">114.2</span></td>
</tr>
<tr id="S4.T4.4.1.7.7" class="ltx_tr">
<td id="S4.T4.4.1.7.7.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.7.7.1.1" class="ltx_text" style="font-size:90%;">Tome </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib57" title="" class="ltx_ref">tome2017lifting </a></cite>
</td>
<td id="S4.T4.4.1.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.2.1" class="ltx_text" style="font-size:90%;">65.0</span></td>
<td id="S4.T4.4.1.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.3.1" class="ltx_text" style="font-size:90%;">73.5</span></td>
<td id="S4.T4.4.1.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.4.1" class="ltx_text" style="font-size:90%;">76.8</span></td>
<td id="S4.T4.4.1.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.5.1" class="ltx_text" style="font-size:90%;">86.4</span></td>
<td id="S4.T4.4.1.7.7.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.6.1" class="ltx_text" style="font-size:90%;">86.3</span></td>
<td id="S4.T4.4.1.7.7.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.7.1" class="ltx_text" style="font-size:90%;">68.9</span></td>
<td id="S4.T4.4.1.7.7.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.8.1" class="ltx_text" style="font-size:90%;">74.8</span></td>
<td id="S4.T4.4.1.7.7.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.9.1" class="ltx_text" style="font-size:90%;">110.2</span></td>
<td id="S4.T4.4.1.7.7.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.10.1" class="ltx_text" style="font-size:90%;">173.9</span></td>
<td id="S4.T4.4.1.7.7.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.11.1" class="ltx_text" style="font-size:90%;">85.0</span></td>
<td id="S4.T4.4.1.7.7.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.12.1" class="ltx_text" style="font-size:90%;">110.7</span></td>
<td id="S4.T4.4.1.7.7.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.13.1" class="ltx_text" style="font-size:90%;">85.8</span></td>
<td id="S4.T4.4.1.7.7.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.14.1" class="ltx_text" style="font-size:90%;">71.4</span></td>
<td id="S4.T4.4.1.7.7.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.15.1" class="ltx_text" style="font-size:90%;">86.3</span></td>
<td id="S4.T4.4.1.7.7.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.16.1" class="ltx_text" style="font-size:90%;">73.1</span></td>
<td id="S4.T4.4.1.7.7.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.7.7.17.1" class="ltx_text" style="font-size:90%;">88.4</span></td>
</tr>
<tr id="S4.T4.4.1.8.8" class="ltx_tr">
<td id="S4.T4.4.1.8.8.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.8.8.1.1" class="ltx_text" style="font-size:90%;">Moreno </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib40" title="" class="ltx_ref">moreno20173d </a></cite>
</td>
<td id="S4.T4.4.1.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.2.1" class="ltx_text" style="font-size:90%;">69.5</span></td>
<td id="S4.T4.4.1.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.3.1" class="ltx_text" style="font-size:90%;">80.2</span></td>
<td id="S4.T4.4.1.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.4.1" class="ltx_text" style="font-size:90%;">78.2</span></td>
<td id="S4.T4.4.1.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.5.1" class="ltx_text" style="font-size:90%;">87.0</span></td>
<td id="S4.T4.4.1.8.8.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.6.1" class="ltx_text" style="font-size:90%;">100.8</span></td>
<td id="S4.T4.4.1.8.8.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.7.1" class="ltx_text" style="font-size:90%;">76.0</span></td>
<td id="S4.T4.4.1.8.8.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.8.1" class="ltx_text" style="font-size:90%;">69.7</span></td>
<td id="S4.T4.4.1.8.8.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.9.1" class="ltx_text" style="font-size:90%;">104.7</span></td>
<td id="S4.T4.4.1.8.8.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.10.1" class="ltx_text" style="font-size:90%;">113.9</span></td>
<td id="S4.T4.4.1.8.8.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.11.1" class="ltx_text" style="font-size:90%;">89.7</span></td>
<td id="S4.T4.4.1.8.8.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.12.1" class="ltx_text" style="font-size:90%;">102.7</span></td>
<td id="S4.T4.4.1.8.8.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.13.1" class="ltx_text" style="font-size:90%;">98.5</span></td>
<td id="S4.T4.4.1.8.8.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.14.1" class="ltx_text" style="font-size:90%;">79.2</span></td>
<td id="S4.T4.4.1.8.8.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.15.1" class="ltx_text" style="font-size:90%;">82.4</span></td>
<td id="S4.T4.4.1.8.8.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.16.1" class="ltx_text" style="font-size:90%;">77.2</span></td>
<td id="S4.T4.4.1.8.8.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.8.8.17.1" class="ltx_text" style="font-size:90%;">87.3</span></td>
</tr>
<tr id="S4.T4.4.1.9.9" class="ltx_tr">
<td id="S4.T4.4.1.9.9.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.9.9.1.1" class="ltx_text" style="font-size:90%;">Zhou </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref">zhou2018monocap </a></cite>
</td>
<td id="S4.T4.4.1.9.9.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.2.1" class="ltx_text" style="font-size:90%;">68.7</span></td>
<td id="S4.T4.4.1.9.9.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.3.1" class="ltx_text" style="font-size:90%;">74.8</span></td>
<td id="S4.T4.4.1.9.9.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.4.1" class="ltx_text" style="font-size:90%;">67.8</span></td>
<td id="S4.T4.4.1.9.9.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.5.1" class="ltx_text" style="font-size:90%;">76.4</span></td>
<td id="S4.T4.4.1.9.9.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.6.1" class="ltx_text" style="font-size:90%;">76.3</span></td>
<td id="S4.T4.4.1.9.9.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.7.1" class="ltx_text" style="font-size:90%;">84.0</span></td>
<td id="S4.T4.4.1.9.9.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.8.1" class="ltx_text" style="font-size:90%;">70.2</span></td>
<td id="S4.T4.4.1.9.9.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.9.1" class="ltx_text" style="font-size:90%;">88.0</span></td>
<td id="S4.T4.4.1.9.9.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.10.1" class="ltx_text" style="font-size:90%;">113.8</span></td>
<td id="S4.T4.4.1.9.9.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.11.1" class="ltx_text" style="font-size:90%;">78.0</span></td>
<td id="S4.T4.4.1.9.9.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.12.1" class="ltx_text" style="font-size:90%;">98.4</span></td>
<td id="S4.T4.4.1.9.9.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.13.1" class="ltx_text" style="font-size:90%;">90.1</span></td>
<td id="S4.T4.4.1.9.9.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.14.1" class="ltx_text" style="font-size:90%;">62.6</span></td>
<td id="S4.T4.4.1.9.9.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.15.1" class="ltx_text" style="font-size:90%;">75.1</span></td>
<td id="S4.T4.4.1.9.9.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.16.1" class="ltx_text" style="font-size:90%;">73.6</span></td>
<td id="S4.T4.4.1.9.9.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.9.9.17.1" class="ltx_text" style="font-size:90%;">79.9</span></td>
</tr>
<tr id="S4.T4.4.1.10.10" class="ltx_tr">
<td id="S4.T4.4.1.10.10.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.10.10.1.1" class="ltx_text" style="font-size:90%;">Jahangiri </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib22" title="" class="ltx_ref">jahangiri2017generating </a></cite>
</td>
<td id="S4.T4.4.1.10.10.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.2.1" class="ltx_text" style="font-size:90%;">74.4</span></td>
<td id="S4.T4.4.1.10.10.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.3.1" class="ltx_text" style="font-size:90%;">66.7</span></td>
<td id="S4.T4.4.1.10.10.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.4.1" class="ltx_text" style="font-size:90%;">67.9</span></td>
<td id="S4.T4.4.1.10.10.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.5.1" class="ltx_text" style="font-size:90%;">75.2</span></td>
<td id="S4.T4.4.1.10.10.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.6.1" class="ltx_text" style="font-size:90%;">77.3</span></td>
<td id="S4.T4.4.1.10.10.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.7.1" class="ltx_text" style="font-size:90%;">70.6</span></td>
<td id="S4.T4.4.1.10.10.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.8.1" class="ltx_text" style="font-size:90%;">64.5</span></td>
<td id="S4.T4.4.1.10.10.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.9.1" class="ltx_text" style="font-size:90%;">95.6</span></td>
<td id="S4.T4.4.1.10.10.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.10.1" class="ltx_text" style="font-size:90%;">127.3</span></td>
<td id="S4.T4.4.1.10.10.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.11.1" class="ltx_text" style="font-size:90%;">79.6</span></td>
<td id="S4.T4.4.1.10.10.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.12.1" class="ltx_text" style="font-size:90%;">79.1</span></td>
<td id="S4.T4.4.1.10.10.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.13.1" class="ltx_text" style="font-size:90%;">73.4</span></td>
<td id="S4.T4.4.1.10.10.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.14.1" class="ltx_text" style="font-size:90%;">67.4</span></td>
<td id="S4.T4.4.1.10.10.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.15.1" class="ltx_text" style="font-size:90%;">71.8</span></td>
<td id="S4.T4.4.1.10.10.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.16.1" class="ltx_text" style="font-size:90%;">72.8</span></td>
<td id="S4.T4.4.1.10.10.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.10.10.17.1" class="ltx_text" style="font-size:90%;">77.6</span></td>
</tr>
<tr id="S4.T4.4.1.11.11" class="ltx_tr">
<td id="S4.T4.4.1.11.11.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.11.11.1.1" class="ltx_text" style="font-size:90%;">Mehta </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib36" title="" class="ltx_ref">mehta2017monocular </a></cite>
</td>
<td id="S4.T4.4.1.11.11.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.2.1" class="ltx_text" style="font-size:90%;">57.5</span></td>
<td id="S4.T4.4.1.11.11.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.3.1" class="ltx_text" style="font-size:90%;">68.6</span></td>
<td id="S4.T4.4.1.11.11.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.4.1" class="ltx_text" style="font-size:90%;">59.6</span></td>
<td id="S4.T4.4.1.11.11.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.5.1" class="ltx_text" style="font-size:90%;">67.3</span></td>
<td id="S4.T4.4.1.11.11.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.6.1" class="ltx_text" style="font-size:90%;">78.1</span></td>
<td id="S4.T4.4.1.11.11.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.7.1" class="ltx_text" style="font-size:90%;">56.9</span></td>
<td id="S4.T4.4.1.11.11.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.8.1" class="ltx_text" style="font-size:90%;">69.1</span></td>
<td id="S4.T4.4.1.11.11.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.9.1" class="ltx_text" style="font-size:90%;">98.0</span></td>
<td id="S4.T4.4.1.11.11.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.10.1" class="ltx_text" style="font-size:90%;">117.5</span></td>
<td id="S4.T4.4.1.11.11.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.11.1" class="ltx_text" style="font-size:90%;">69.5</span></td>
<td id="S4.T4.4.1.11.11.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.12.1" class="ltx_text" style="font-size:90%;">82.4</span></td>
<td id="S4.T4.4.1.11.11.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.13.1" class="ltx_text" style="font-size:90%;">68.0</span></td>
<td id="S4.T4.4.1.11.11.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.14.1" class="ltx_text" style="font-size:90%;">55.3</span></td>
<td id="S4.T4.4.1.11.11.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.15.1" class="ltx_text" style="font-size:90%;">76.5</span></td>
<td id="S4.T4.4.1.11.11.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.16.1" class="ltx_text" style="font-size:90%;">61.4</span></td>
<td id="S4.T4.4.1.11.11.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.11.11.17.1" class="ltx_text" style="font-size:90%;">72.9</span></td>
</tr>
<tr id="S4.T4.4.1.12.12" class="ltx_tr">
<td id="S4.T4.4.1.12.12.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.12.12.1.1" class="ltx_text" style="font-size:90%;">Martinez </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">martinez2017simple </a></cite>
</td>
<td id="S4.T4.4.1.12.12.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.2.1" class="ltx_text" style="font-size:90%;">51.8</span></td>
<td id="S4.T4.4.1.12.12.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.3.1" class="ltx_text" style="font-size:90%;">56.2</span></td>
<td id="S4.T4.4.1.12.12.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.4.1" class="ltx_text" style="font-size:90%;">58.1</span></td>
<td id="S4.T4.4.1.12.12.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.5.1" class="ltx_text" style="font-size:90%;">59.0</span></td>
<td id="S4.T4.4.1.12.12.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.6.1" class="ltx_text" style="font-size:90%;">69.5</span></td>
<td id="S4.T4.4.1.12.12.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.7.1" class="ltx_text" style="font-size:90%;">55.2</span></td>
<td id="S4.T4.4.1.12.12.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.8.1" class="ltx_text" style="font-size:90%;">58.1</span></td>
<td id="S4.T4.4.1.12.12.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.9.1" class="ltx_text" style="font-size:90%;">74.0</span></td>
<td id="S4.T4.4.1.12.12.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.10.1" class="ltx_text" style="font-size:90%;">94.6</span></td>
<td id="S4.T4.4.1.12.12.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.11.1" class="ltx_text" style="font-size:90%;">62.3</span></td>
<td id="S4.T4.4.1.12.12.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.12.1" class="ltx_text" style="font-size:90%;">78.4</span></td>
<td id="S4.T4.4.1.12.12.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.13.1" class="ltx_text" style="font-size:90%;">59.1</span></td>
<td id="S4.T4.4.1.12.12.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.14.1" class="ltx_text" style="font-size:90%;">49.5</span></td>
<td id="S4.T4.4.1.12.12.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.15.1" class="ltx_text" style="font-size:90%;">65.1</span></td>
<td id="S4.T4.4.1.12.12.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.16.1" class="ltx_text" style="font-size:90%;">52.4</span></td>
<td id="S4.T4.4.1.12.12.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.12.12.17.1" class="ltx_text" style="font-size:90%;">62.9</span></td>
</tr>
<tr id="S4.T4.4.1.13.13" class="ltx_tr">
<td id="S4.T4.4.1.13.13.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.13.13.1.1" class="ltx_text" style="font-size:90%;">Fang </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">fang2018learning </a></cite>
</td>
<td id="S4.T4.4.1.13.13.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.2.1" class="ltx_text" style="font-size:90%;">50.1</span></td>
<td id="S4.T4.4.1.13.13.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.3.1" class="ltx_text" style="font-size:90%;">54.3</span></td>
<td id="S4.T4.4.1.13.13.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.4.1" class="ltx_text" style="font-size:90%;">57.0</span></td>
<td id="S4.T4.4.1.13.13.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.5.1" class="ltx_text" style="font-size:90%;">57.1</span></td>
<td id="S4.T4.4.1.13.13.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.6.1" class="ltx_text" style="font-size:90%;">66.6</span></td>
<td id="S4.T4.4.1.13.13.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.7.1" class="ltx_text" style="font-size:90%;">53.4</span></td>
<td id="S4.T4.4.1.13.13.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.8.1" class="ltx_text" style="font-size:90%;">55.7</span></td>
<td id="S4.T4.4.1.13.13.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.9.1" class="ltx_text" style="font-size:90%;">72.8</span></td>
<td id="S4.T4.4.1.13.13.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.10.1" class="ltx_text" style="font-size:90%;">88.6</span></td>
<td id="S4.T4.4.1.13.13.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.11.1" class="ltx_text" style="font-size:90%;">60.3</span></td>
<td id="S4.T4.4.1.13.13.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.12.1" class="ltx_text" style="font-size:90%;">73.3</span></td>
<td id="S4.T4.4.1.13.13.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.13.1" class="ltx_text" style="font-size:90%;">57.7</span></td>
<td id="S4.T4.4.1.13.13.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.14.1" class="ltx_text" style="font-size:90%;">47.5</span></td>
<td id="S4.T4.4.1.13.13.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.15.1" class="ltx_text" style="font-size:90%;">62.7</span></td>
<td id="S4.T4.4.1.13.13.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.16.1" class="ltx_text" style="font-size:90%;">50.6</span></td>
<td id="S4.T4.4.1.13.13.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.13.13.17.1" class="ltx_text" style="font-size:90%;">60.4</span></td>
</tr>
<tr id="S4.T4.4.1.14.14" class="ltx_tr">
<td id="S4.T4.4.1.14.14.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.14.14.1.1" class="ltx_text" style="font-size:90%;">Sun </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref">sun2017compositional </a></cite>
</td>
<td id="S4.T4.4.1.14.14.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.2.1" class="ltx_text" style="font-size:90%;">52.8</span></td>
<td id="S4.T4.4.1.14.14.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.3.1" class="ltx_text" style="font-size:90%;">54.8</span></td>
<td id="S4.T4.4.1.14.14.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.4.1" class="ltx_text" style="font-size:90%;">54.2</span></td>
<td id="S4.T4.4.1.14.14.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.5.1" class="ltx_text" style="font-size:90%;">54.3</span></td>
<td id="S4.T4.4.1.14.14.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.6.1" class="ltx_text" style="font-size:90%;">61.8</span></td>
<td id="S4.T4.4.1.14.14.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.7.1" class="ltx_text" style="font-size:90%;">53.1</span></td>
<td id="S4.T4.4.1.14.14.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.8.1" class="ltx_text" style="font-size:90%;">53.6</span></td>
<td id="S4.T4.4.1.14.14.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.9.1" class="ltx_text" style="font-size:90%;">71.7</span></td>
<td id="S4.T4.4.1.14.14.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.10.1" class="ltx_text" style="font-size:90%;">86.7</span></td>
<td id="S4.T4.4.1.14.14.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.11.1" class="ltx_text" style="font-size:90%;">61.5</span></td>
<td id="S4.T4.4.1.14.14.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.12.1" class="ltx_text" style="font-size:90%;">67.2</span></td>
<td id="S4.T4.4.1.14.14.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.13.1" class="ltx_text" style="font-size:90%;">53.4</span></td>
<td id="S4.T4.4.1.14.14.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.14.1" class="ltx_text" style="font-size:90%;">47.1</span></td>
<td id="S4.T4.4.1.14.14.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.15.1" class="ltx_text" style="font-size:90%;">61.6</span></td>
<td id="S4.T4.4.1.14.14.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.16.1" class="ltx_text" style="font-size:90%;">63.4</span></td>
<td id="S4.T4.4.1.14.14.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.14.14.17.1" class="ltx_text" style="font-size:90%;">59.1</span></td>
</tr>
<tr id="S4.T4.4.1.15.15" class="ltx_tr">
<td id="S4.T4.4.1.15.15.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.15.15.1.1" class="ltx_text" style="font-size:90%;">Sun </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">fang2018learning </a></cite>
</td>
<td id="S4.T4.4.1.15.15.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.2.1" class="ltx_text" style="font-size:90%;">47.5</span></td>
<td id="S4.T4.4.1.15.15.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.3.1" class="ltx_text" style="font-size:90%;">47.7</span></td>
<td id="S4.T4.4.1.15.15.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.4.1" class="ltx_text" style="font-size:90%;">49.5</span></td>
<td id="S4.T4.4.1.15.15.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.5.1" class="ltx_text" style="font-size:90%;">50.2</span></td>
<td id="S4.T4.4.1.15.15.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.6.1" class="ltx_text" style="font-size:90%;">51.4</span></td>
<td id="S4.T4.4.1.15.15.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.7.1" class="ltx_text" style="font-size:90%;">43.8</span></td>
<td id="S4.T4.4.1.15.15.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.8.1" class="ltx_text" style="font-size:90%;">46.4</span></td>
<td id="S4.T4.4.1.15.15.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.9.1" class="ltx_text" style="font-size:90%;">58.9</span></td>
<td id="S4.T4.4.1.15.15.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.10.1" class="ltx_text" style="font-size:90%;">65.7</span></td>
<td id="S4.T4.4.1.15.15.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.11.1" class="ltx_text" style="font-size:90%;">49.4</span></td>
<td id="S4.T4.4.1.15.15.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.12.1" class="ltx_text" style="font-size:90%;">55.8</span></td>
<td id="S4.T4.4.1.15.15.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.13.1" class="ltx_text" style="font-size:90%;">47.8</span></td>
<td id="S4.T4.4.1.15.15.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.14.1" class="ltx_text" style="font-size:90%;">38.9</span></td>
<td id="S4.T4.4.1.15.15.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.15.1" class="ltx_text" style="font-size:90%;">49.0</span></td>
<td id="S4.T4.4.1.15.15.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.16.1" class="ltx_text" style="font-size:90%;">43.8</span></td>
<td id="S4.T4.4.1.15.15.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.15.15.17.1" class="ltx_text ltx_font_bold" style="font-size:90%;">49.6</span></td>
</tr>
<tr id="S4.T4.4.1.16.16" class="ltx_tr">
<td id="S4.T4.4.1.16.16.1" class="ltx_td ltx_align_center">
<span id="S4.T4.4.1.16.16.1.1" class="ltx_text" style="font-size:90%;">Moon </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">moon2019camera </a></cite>
</td>
<td id="S4.T4.4.1.16.16.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.2.1" class="ltx_text" style="font-size:90%;">50.5</span></td>
<td id="S4.T4.4.1.16.16.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.3.1" class="ltx_text" style="font-size:90%;">55.7</span></td>
<td id="S4.T4.4.1.16.16.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.4.1" class="ltx_text" style="font-size:90%;">50.1</span></td>
<td id="S4.T4.4.1.16.16.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.5.1" class="ltx_text" style="font-size:90%;">51.7</span></td>
<td id="S4.T4.4.1.16.16.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.6.1" class="ltx_text" style="font-size:90%;">53.9</span></td>
<td id="S4.T4.4.1.16.16.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.7.1" class="ltx_text" style="font-size:90%;">46.8</span></td>
<td id="S4.T4.4.1.16.16.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.8.1" class="ltx_text" style="font-size:90%;">50.0</span></td>
<td id="S4.T4.4.1.16.16.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.9.1" class="ltx_text" style="font-size:90%;">61.9</span></td>
<td id="S4.T4.4.1.16.16.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.10.1" class="ltx_text" style="font-size:90%;">68.0</span></td>
<td id="S4.T4.4.1.16.16.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.11.1" class="ltx_text" style="font-size:90%;">52.5</span></td>
<td id="S4.T4.4.1.16.16.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.12.1" class="ltx_text" style="font-size:90%;">55.9</span></td>
<td id="S4.T4.4.1.16.16.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.13.1" class="ltx_text" style="font-size:90%;">49.9</span></td>
<td id="S4.T4.4.1.16.16.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.14.1" class="ltx_text" style="font-size:90%;">41.8</span></td>
<td id="S4.T4.4.1.16.16.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.15.1" class="ltx_text" style="font-size:90%;">56.1</span></td>
<td id="S4.T4.4.1.16.16.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.16.1" class="ltx_text" style="font-size:90%;">46.9</span></td>
<td id="S4.T4.4.1.16.16.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.16.16.17.1" class="ltx_text" style="font-size:90%;">53.3</span></td>
</tr>
<tr id="S4.T4.4.1.17.17" class="ltx_tr">
<td id="S4.T4.4.1.17.17.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.1.1" class="ltx_text" style="font-size:90%;">ScanAva-AHuP</span></td>
<td id="S4.T4.4.1.17.17.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.2.1" class="ltx_text" style="font-size:90%;">135.9</span></td>
<td id="S4.T4.4.1.17.17.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.3.1" class="ltx_text" style="font-size:90%;">137.2</span></td>
<td id="S4.T4.4.1.17.17.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.4.1" class="ltx_text" style="font-size:90%;">104.0</span></td>
<td id="S4.T4.4.1.17.17.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.5.1" class="ltx_text" style="font-size:90%;">137.1</span></td>
<td id="S4.T4.4.1.17.17.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.6.1" class="ltx_text" style="font-size:90%;">139.4</span></td>
<td id="S4.T4.4.1.17.17.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.7.1" class="ltx_text" style="font-size:90%;">133.6</span></td>
<td id="S4.T4.4.1.17.17.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.8.1" class="ltx_text" style="font-size:90%;">140.8</span></td>
<td id="S4.T4.4.1.17.17.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.9.1" class="ltx_text" style="font-size:90%;">133.7</span></td>
<td id="S4.T4.4.1.17.17.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.10.1" class="ltx_text" style="font-size:90%;">163.3</span></td>
<td id="S4.T4.4.1.17.17.11" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.11.1" class="ltx_text" style="font-size:90%;">129.5</span></td>
<td id="S4.T4.4.1.17.17.12" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.12.1" class="ltx_text" style="font-size:90%;">137.9</span></td>
<td id="S4.T4.4.1.17.17.13" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.13.1" class="ltx_text" style="font-size:90%;">139.5</span></td>
<td id="S4.T4.4.1.17.17.14" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.14.1" class="ltx_text" style="font-size:90%;">123.2</span></td>
<td id="S4.T4.4.1.17.17.15" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.15.1" class="ltx_text" style="font-size:90%;">135.1</span></td>
<td id="S4.T4.4.1.17.17.16" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.16.1" class="ltx_text" style="font-size:90%;">130.2</span></td>
<td id="S4.T4.4.1.17.17.17" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.4.1.17.17.17.1" class="ltx_text" style="font-size:90%;">134.5</span></td>
</tr>
<tr id="S4.T4.4.1.18.18" class="ltx_tr">
<td id="S4.T4.4.1.18.18.1" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.1.1" class="ltx_text" style="font-size:90%;">ScanAva-AHuP PA</span></td>
<td id="S4.T4.4.1.18.18.2" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.2.1" class="ltx_text" style="font-size:90%;">75.2</span></td>
<td id="S4.T4.4.1.18.18.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.3.1" class="ltx_text" style="font-size:90%;">79.1</span></td>
<td id="S4.T4.4.1.18.18.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.4.1" class="ltx_text" style="font-size:90%;">68.0</span></td>
<td id="S4.T4.4.1.18.18.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.5.1" class="ltx_text" style="font-size:90%;">79.1</span></td>
<td id="S4.T4.4.1.18.18.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.6.1" class="ltx_text" style="font-size:90%;">91.7</span></td>
<td id="S4.T4.4.1.18.18.7" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.7.1" class="ltx_text" style="font-size:90%;">75.8</span></td>
<td id="S4.T4.4.1.18.18.8" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.8.1" class="ltx_text" style="font-size:90%;">82.3</span></td>
<td id="S4.T4.4.1.18.18.9" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.9.1" class="ltx_text" style="font-size:90%;">100.9</span></td>
<td id="S4.T4.4.1.18.18.10" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.10.1" class="ltx_text" style="font-size:90%;">128.0</span></td>
<td id="S4.T4.4.1.18.18.11" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.11.1" class="ltx_text" style="font-size:90%;">87.4</span></td>
<td id="S4.T4.4.1.18.18.12" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.12.1" class="ltx_text" style="font-size:90%;">83.3</span></td>
<td id="S4.T4.4.1.18.18.13" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.13.1" class="ltx_text" style="font-size:90%;">81.8</span></td>
<td id="S4.T4.4.1.18.18.14" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.14.1" class="ltx_text" style="font-size:90%;">76.8</span></td>
<td id="S4.T4.4.1.18.18.15" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.15.1" class="ltx_text" style="font-size:90%;">82.2</span></td>
<td id="S4.T4.4.1.18.18.16" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.16.1" class="ltx_text" style="font-size:90%;">78.3</span></td>
<td id="S4.T4.4.1.18.18.17" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.18.18.17.1" class="ltx_text" style="font-size:90%;">85.1</span></td>
</tr>
<tr id="S4.T4.4.1.19.19" class="ltx_tr">
<td id="S4.T4.4.1.19.19.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T4.4.1.19.19.1.1" class="ltx_text" style="font-size:90%;">Martinez (ScanAva+) </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">martinez2017simple </a></cite>
</td>
<td id="S4.T4.4.1.19.19.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.2.1" class="ltx_text" style="font-size:90%;">153.2</span></td>
<td id="S4.T4.4.1.19.19.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.3.1" class="ltx_text" style="font-size:90%;">152.6</span></td>
<td id="S4.T4.4.1.19.19.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.4.1" class="ltx_text" style="font-size:90%;">129.7</span></td>
<td id="S4.T4.4.1.19.19.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.5.1" class="ltx_text" style="font-size:90%;">153.8</span></td>
<td id="S4.T4.4.1.19.19.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.6.1" class="ltx_text" style="font-size:90%;">151.9</span></td>
<td id="S4.T4.4.1.19.19.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.7.1" class="ltx_text" style="font-size:90%;">149.9</span></td>
<td id="S4.T4.4.1.19.19.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.8.1" class="ltx_text" style="font-size:90%;">144.0</span></td>
<td id="S4.T4.4.1.19.19.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.9.1" class="ltx_text" style="font-size:90%;">159.8</span></td>
<td id="S4.T4.4.1.19.19.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.10.1" class="ltx_text" style="font-size:90%;">191.0</span></td>
<td id="S4.T4.4.1.19.19.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.11.1" class="ltx_text" style="font-size:90%;">146.2</span></td>
<td id="S4.T4.4.1.19.19.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.12.1" class="ltx_text" style="font-size:90%;">147.9</span></td>
<td id="S4.T4.4.1.19.19.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.13.1" class="ltx_text" style="font-size:90%;">158.7</span></td>
<td id="S4.T4.4.1.19.19.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.14.1" class="ltx_text" style="font-size:90%;">148.5</span></td>
<td id="S4.T4.4.1.19.19.15" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.15.1" class="ltx_text" style="font-size:90%;">140.8</span></td>
<td id="S4.T4.4.1.19.19.16" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.16.1" class="ltx_text" style="font-size:90%;">139.8</span></td>
<td id="S4.T4.4.1.19.19.17" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.19.19.17.1" class="ltx_text" style="font-size:90%;">151.5</span></td>
</tr>
<tr id="S4.T4.4.1.20.20" class="ltx_tr">
<td id="S4.T4.4.1.20.20.1" class="ltx_td ltx_align_center ltx_border_b">
<span id="S4.T4.4.1.20.20.1.1" class="ltx_text" style="font-size:90%;">Martinez (ScanAva+) </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">martinez2017simple </a></cite><span id="S4.T4.4.1.20.20.1.4" class="ltx_text" style="font-size:90%;"> PA</span>
</td>
<td id="S4.T4.4.1.20.20.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.2.1" class="ltx_text" style="font-size:90%;">97.8</span></td>
<td id="S4.T4.4.1.20.20.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.3.1" class="ltx_text" style="font-size:90%;">97.9</span></td>
<td id="S4.T4.4.1.20.20.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.4.1" class="ltx_text" style="font-size:90%;">93.1</span></td>
<td id="S4.T4.4.1.20.20.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.5.1" class="ltx_text" style="font-size:90%;">99.5</span></td>
<td id="S4.T4.4.1.20.20.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.6.1" class="ltx_text" style="font-size:90%;">105.7</span></td>
<td id="S4.T4.4.1.20.20.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.7.1" class="ltx_text" style="font-size:90%;">91.3</span></td>
<td id="S4.T4.4.1.20.20.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.8.1" class="ltx_text" style="font-size:90%;">91.0</span></td>
<td id="S4.T4.4.1.20.20.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.9.1" class="ltx_text" style="font-size:90%;">121.4</span></td>
<td id="S4.T4.4.1.20.20.10" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.10.1" class="ltx_text" style="font-size:90%;">139.7</span></td>
<td id="S4.T4.4.1.20.20.11" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.11.1" class="ltx_text" style="font-size:90%;">104.6</span></td>
<td id="S4.T4.4.1.20.20.12" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.12.1" class="ltx_text" style="font-size:90%;">96.6</span></td>
<td id="S4.T4.4.1.20.20.13" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.13.1" class="ltx_text" style="font-size:90%;">99.5</span></td>
<td id="S4.T4.4.1.20.20.14" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.14.1" class="ltx_text" style="font-size:90%;">105.2</span></td>
<td id="S4.T4.4.1.20.20.15" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.15.1" class="ltx_text" style="font-size:90%;">98.2</span></td>
<td id="S4.T4.4.1.20.20.16" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.16.1" class="ltx_text" style="font-size:90%;">98.0</span></td>
<td id="S4.T4.4.1.20.20.17" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.1.20.20.17.1" class="ltx_text" style="font-size:90%;">103.3</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.3.2" class="ltx_text" style="font-size:90%;">3DPCK comparison with the state-of-the-art tested on the MuPoTS dataset. 16 out of 20 subjects are listed due to space limitation. </span></figcaption>
<div id="S4.T5.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:88.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-50.7pt,10.2pt) scale(0.810483743891116,0.810483743891116) ;">
<table id="S4.T5.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.4.1.1.1" class="ltx_tr">
<th id="S4.T5.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.1.1" class="ltx_text" style="font-size:50%;">Methods</span></th>
<th id="S4.T5.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.2.1" class="ltx_text" style="font-size:50%;">S1</span></th>
<th id="S4.T5.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.3.1" class="ltx_text" style="font-size:50%;">S2</span></th>
<th id="S4.T5.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.4.1" class="ltx_text" style="font-size:50%;">S3</span></th>
<th id="S4.T5.4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.5.1" class="ltx_text" style="font-size:50%;">S4</span></th>
<th id="S4.T5.4.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.6.1" class="ltx_text" style="font-size:50%;">S5</span></th>
<th id="S4.T5.4.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.7.1" class="ltx_text" style="font-size:50%;">S6</span></th>
<th id="S4.T5.4.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.8.1" class="ltx_text" style="font-size:50%;">S7</span></th>
<th id="S4.T5.4.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.9.1" class="ltx_text" style="font-size:50%;">S8</span></th>
<th id="S4.T5.4.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.10.1" class="ltx_text" style="font-size:50%;">S9</span></th>
<th id="S4.T5.4.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.11.1" class="ltx_text" style="font-size:50%;">S10</span></th>
<th id="S4.T5.4.1.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.12.1" class="ltx_text" style="font-size:50%;">S11</span></th>
<th id="S4.T5.4.1.1.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.13.1" class="ltx_text" style="font-size:50%;">S12</span></th>
<th id="S4.T5.4.1.1.1.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.14.1" class="ltx_text" style="font-size:50%;">S13</span></th>
<th id="S4.T5.4.1.1.1.15" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.15.1" class="ltx_text" style="font-size:50%;">S14</span></th>
<th id="S4.T5.4.1.1.1.16" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.16.1" class="ltx_text" style="font-size:50%;">S15</span></th>
<th id="S4.T5.4.1.1.1.17" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.17.1" class="ltx_text" style="font-size:50%;">S16</span></th>
<th id="S4.T5.4.1.1.1.18" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.18.1" class="ltx_text" style="font-size:50%;">‚Ä¶</span></th>
<th id="S4.T5.4.1.1.1.19" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.19.1" class="ltx_text" style="font-size:50%;">Avg</span></th>
<th id="S4.T5.4.1.1.1.20" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.1.1.1.20.1" class="ltx_text" style="font-size:50%;">AUC</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.4.1.2.1" class="ltx_tr">
<td id="S4.T5.4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T5.4.1.2.1.1.1" class="ltx_text" style="font-size:50%;">Rogez </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib51" title="" class="ltx_ref">rogez2017lcr </a></cite>
</td>
<td id="S4.T5.4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.2.1" class="ltx_text" style="font-size:50%;">67.7</span></td>
<td id="S4.T5.4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.3.1" class="ltx_text" style="font-size:50%;">49.8</span></td>
<td id="S4.T5.4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.4.1" class="ltx_text" style="font-size:50%;">53.4</span></td>
<td id="S4.T5.4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.5.1" class="ltx_text" style="font-size:50%;">59.1</span></td>
<td id="S4.T5.4.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.6.1" class="ltx_text" style="font-size:50%;">67.5</span></td>
<td id="S4.T5.4.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.7.1" class="ltx_text" style="font-size:50%;">22.8</span></td>
<td id="S4.T5.4.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.8.1" class="ltx_text" style="font-size:50%;">43.7</span></td>
<td id="S4.T5.4.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.9.1" class="ltx_text" style="font-size:50%;">49.9</span></td>
<td id="S4.T5.4.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.10.1" class="ltx_text" style="font-size:50%;">31.1</span></td>
<td id="S4.T5.4.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.11.1" class="ltx_text" style="font-size:50%;">78.1</span></td>
<td id="S4.T5.4.1.2.1.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.12.1" class="ltx_text" style="font-size:50%;">50.2</span></td>
<td id="S4.T5.4.1.2.1.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.13.1" class="ltx_text" style="font-size:50%;">51.0</span></td>
<td id="S4.T5.4.1.2.1.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.14.1" class="ltx_text" style="font-size:50%;">51.6</span></td>
<td id="S4.T5.4.1.2.1.15" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.15.1" class="ltx_text" style="font-size:50%;">49.3</span></td>
<td id="S4.T5.4.1.2.1.16" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.16.1" class="ltx_text" style="font-size:50%;">56.2</span></td>
<td id="S4.T5.4.1.2.1.17" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.17.1" class="ltx_text" style="font-size:50%;">66.5</span></td>
<td id="S4.T5.4.1.2.1.18" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.18.1" class="ltx_text" style="font-size:50%;">‚Ä¶</span></td>
<td id="S4.T5.4.1.2.1.19" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.19.1" class="ltx_text" style="font-size:50%;">53.8</span></td>
<td id="S4.T5.4.1.2.1.20" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.4.1.2.1.20.1" class="ltx_text" style="font-size:50%;">27.6</span></td>
</tr>
<tr id="S4.T5.4.1.3.2" class="ltx_tr">
<td id="S4.T5.4.1.3.2.1" class="ltx_td ltx_align_center">
<span id="S4.T5.4.1.3.2.1.1" class="ltx_text" style="font-size:50%;">Mehta </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib38" title="" class="ltx_ref">mehta2018single </a></cite>
</td>
<td id="S4.T5.4.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.2.1" class="ltx_text" style="font-size:50%;">81.0</span></td>
<td id="S4.T5.4.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.3.1" class="ltx_text" style="font-size:50%;">60.9</span></td>
<td id="S4.T5.4.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.4.1" class="ltx_text" style="font-size:50%;">64.4</span></td>
<td id="S4.T5.4.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.5.1" class="ltx_text" style="font-size:50%;">63.0</span></td>
<td id="S4.T5.4.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.6.1" class="ltx_text" style="font-size:50%;">69.1</span></td>
<td id="S4.T5.4.1.3.2.7" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.7.1" class="ltx_text" style="font-size:50%;">30.3</span></td>
<td id="S4.T5.4.1.3.2.8" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.8.1" class="ltx_text" style="font-size:50%;">65.0</span></td>
<td id="S4.T5.4.1.3.2.9" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.9.1" class="ltx_text" style="font-size:50%;">59.6</span></td>
<td id="S4.T5.4.1.3.2.10" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.10.1" class="ltx_text" style="font-size:50%;">64.1</span></td>
<td id="S4.T5.4.1.3.2.11" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.11.1" class="ltx_text" style="font-size:50%;">83.9</span></td>
<td id="S4.T5.4.1.3.2.12" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.12.1" class="ltx_text" style="font-size:50%;">68.0</span></td>
<td id="S4.T5.4.1.3.2.13" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.13.1" class="ltx_text" style="font-size:50%;">68.6</span></td>
<td id="S4.T5.4.1.3.2.14" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.14.1" class="ltx_text" style="font-size:50%;">62.3</span></td>
<td id="S4.T5.4.1.3.2.15" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.15.1" class="ltx_text" style="font-size:50%;">59.2</span></td>
<td id="S4.T5.4.1.3.2.16" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.16.1" class="ltx_text" style="font-size:50%;">70.1</span></td>
<td id="S4.T5.4.1.3.2.17" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.17.1" class="ltx_text" style="font-size:50%;">80.0</span></td>
<td id="S4.T5.4.1.3.2.18" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.18.1" class="ltx_text" style="font-size:50%;">‚Ä¶</span></td>
<td id="S4.T5.4.1.3.2.19" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.19.1" class="ltx_text" style="font-size:50%;">66.0</span></td>
<td id="S4.T5.4.1.3.2.20" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.3.2.20.1" class="ltx_text" style="font-size:50%;">37.8</span></td>
</tr>
<tr id="S4.T5.4.1.4.3" class="ltx_tr">
<td id="S4.T5.4.1.4.3.1" class="ltx_td ltx_align_center">
<span id="S4.T5.4.1.4.3.1.1" class="ltx_text" style="font-size:50%;">Rogez </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib52" title="" class="ltx_ref">rogez2019lcr </a></cite>
</td>
<td id="S4.T5.4.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.2.1" class="ltx_text" style="font-size:50%;">87.3</span></td>
<td id="S4.T5.4.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.3.1" class="ltx_text" style="font-size:50%;">61.9</span></td>
<td id="S4.T5.4.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.4.1" class="ltx_text" style="font-size:50%;">67.9</span></td>
<td id="S4.T5.4.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.5.1" class="ltx_text" style="font-size:50%;">74.6</span></td>
<td id="S4.T5.4.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.6.1" class="ltx_text" style="font-size:50%;">78.8</span></td>
<td id="S4.T5.4.1.4.3.7" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.7.1" class="ltx_text" style="font-size:50%;">48.9</span></td>
<td id="S4.T5.4.1.4.3.8" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.8.1" class="ltx_text" style="font-size:50%;">58.3</span></td>
<td id="S4.T5.4.1.4.3.9" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.9.1" class="ltx_text" style="font-size:50%;">59.7</span></td>
<td id="S4.T5.4.1.4.3.10" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.10.1" class="ltx_text" style="font-size:50%;">78.1</span></td>
<td id="S4.T5.4.1.4.3.11" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.11.1" class="ltx_text" style="font-size:50%;">89.5</span></td>
<td id="S4.T5.4.1.4.3.12" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.12.1" class="ltx_text" style="font-size:50%;">69.2</span></td>
<td id="S4.T5.4.1.4.3.13" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.13.1" class="ltx_text" style="font-size:50%;">73.8</span></td>
<td id="S4.T5.4.1.4.3.14" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.14.1" class="ltx_text" style="font-size:50%;">66.2</span></td>
<td id="S4.T5.4.1.4.3.15" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.15.1" class="ltx_text" style="font-size:50%;">56.0</span></td>
<td id="S4.T5.4.1.4.3.16" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.16.1" class="ltx_text" style="font-size:50%;">74.1</span></td>
<td id="S4.T5.4.1.4.3.17" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.17.1" class="ltx_text" style="font-size:50%;">82.1</span></td>
<td id="S4.T5.4.1.4.3.18" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.18.1" class="ltx_text" style="font-size:50%;">‚Ä¶</span></td>
<td id="S4.T5.4.1.4.3.19" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.19.1" class="ltx_text" style="font-size:50%;">70.6</span></td>
<td id="S4.T5.4.1.4.3.20" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.4.3.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T5.4.1.5.4" class="ltx_tr">
<td id="S4.T5.4.1.5.4.1" class="ltx_td ltx_align_center">
<span id="S4.T5.4.1.5.4.1.1" class="ltx_text" style="font-size:50%;">Moon </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">moon2019camera </a></cite>
</td>
<td id="S4.T5.4.1.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.2.1" class="ltx_text" style="font-size:50%;">94.4</span></td>
<td id="S4.T5.4.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.3.1" class="ltx_text" style="font-size:50%;">77.5</span></td>
<td id="S4.T5.4.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.4.1" class="ltx_text" style="font-size:50%;">79.0</span></td>
<td id="S4.T5.4.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.5.1" class="ltx_text" style="font-size:50%;">81.9</span></td>
<td id="S4.T5.4.1.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.6.1" class="ltx_text" style="font-size:50%;">85.3</span></td>
<td id="S4.T5.4.1.5.4.7" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.7.1" class="ltx_text" style="font-size:50%;">72.8</span></td>
<td id="S4.T5.4.1.5.4.8" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.8.1" class="ltx_text" style="font-size:50%;">81.9</span></td>
<td id="S4.T5.4.1.5.4.9" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.9.1" class="ltx_text" style="font-size:50%;">75.7</span></td>
<td id="S4.T5.4.1.5.4.10" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.10.1" class="ltx_text" style="font-size:50%;">90.2</span></td>
<td id="S4.T5.4.1.5.4.11" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.11.1" class="ltx_text" style="font-size:50%;">90.4</span></td>
<td id="S4.T5.4.1.5.4.12" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.12.1" class="ltx_text" style="font-size:50%;">79.2</span></td>
<td id="S4.T5.4.1.5.4.13" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.13.1" class="ltx_text" style="font-size:50%;">79.9</span></td>
<td id="S4.T5.4.1.5.4.14" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.14.1" class="ltx_text" style="font-size:50%;">75.1</span></td>
<td id="S4.T5.4.1.5.4.15" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.15.1" class="ltx_text" style="font-size:50%;">72.7</span></td>
<td id="S4.T5.4.1.5.4.16" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.16.1" class="ltx_text" style="font-size:50%;">81.1</span></td>
<td id="S4.T5.4.1.5.4.17" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.17.1" class="ltx_text" style="font-size:50%;">89.9</span></td>
<td id="S4.T5.4.1.5.4.18" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.18.1" class="ltx_text" style="font-size:50%;">‚Ä¶</span></td>
<td id="S4.T5.4.1.5.4.19" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.19.1" class="ltx_text ltx_font_bold" style="font-size:50%;">81.8</span></td>
<td id="S4.T5.4.1.5.4.20" class="ltx_td ltx_align_center"><span id="S4.T5.4.1.5.4.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T5.4.1.6.5" class="ltx_tr">
<td id="S4.T5.4.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.1.1" class="ltx_text" style="font-size:50%;">ScanAva-AHuP</span></td>
<td id="S4.T5.4.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.2.1" class="ltx_text" style="font-size:50%;">90.6</span></td>
<td id="S4.T5.4.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.3.1" class="ltx_text" style="font-size:50%;">75.0</span></td>
<td id="S4.T5.4.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.4.1" class="ltx_text" style="font-size:50%;">66.0</span></td>
<td id="S4.T5.4.1.6.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.5.1" class="ltx_text" style="font-size:50%;">73.5</span></td>
<td id="S4.T5.4.1.6.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.6.1" class="ltx_text" style="font-size:50%;">87.6</span></td>
<td id="S4.T5.4.1.6.5.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.7.1" class="ltx_text" style="font-size:50%;">73.5</span></td>
<td id="S4.T5.4.1.6.5.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.8.1" class="ltx_text" style="font-size:50%;">90.7</span></td>
<td id="S4.T5.4.1.6.5.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.9.1" class="ltx_text" style="font-size:50%;">71.1</span></td>
<td id="S4.T5.4.1.6.5.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.10.1" class="ltx_text" style="font-size:50%;">91.0</span></td>
<td id="S4.T5.4.1.6.5.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.11.1" class="ltx_text" style="font-size:50%;">95.0</span></td>
<td id="S4.T5.4.1.6.5.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.12.1" class="ltx_text" style="font-size:50%;">87.1</span></td>
<td id="S4.T5.4.1.6.5.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.13.1" class="ltx_text" style="font-size:50%;">87.6</span></td>
<td id="S4.T5.4.1.6.5.14" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.14.1" class="ltx_text" style="font-size:50%;">74.2</span></td>
<td id="S4.T5.4.1.6.5.15" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.15.1" class="ltx_text" style="font-size:50%;">68.6</span></td>
<td id="S4.T5.4.1.6.5.16" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.16.1" class="ltx_text" style="font-size:50%;">85.9</span></td>
<td id="S4.T5.4.1.6.5.17" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.17.1" class="ltx_text" style="font-size:50%;">72.0</span></td>
<td id="S4.T5.4.1.6.5.18" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.18.1" class="ltx_text" style="font-size:50%;">‚Ä¶</span></td>
<td id="S4.T5.4.1.6.5.19" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.19.1" class="ltx_text" style="font-size:50%;">81.6</span></td>
<td id="S4.T5.4.1.6.5.20" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.4.1.6.5.20.1" class="ltx_text ltx_font_bold" style="font-size:50%;">39.0</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2105.10837/assets/x4.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="248" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Qualitative recovery results of AHuP trained on ScanAva+ tested across domains and datasets.</span></figcaption>
</figure>
</section>
<section id="S4.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.8 </span>Comparing 3D Pose Estimation Performance of AHuP with the SOTA</h3>

<div id="S4.SS8.p1" class="ltx_para">
<p id="S4.SS8.p1.1" class="ltx_p">Fundamentally, our AHuP design focuses on performance improvement under a cross-set evaluation scenario. Nonetheless, to show AHuP performance in a larger picture, we directly compare AHuP model performance trained on (ScanAva+ + MSCOCO + MPII) with the SOTA models on the reported metrics as shown in Tab.¬†<a href="#S4.T4" title="Table 4 ‚Ä£ 4.7 Practical Values of AHuP ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Please note that this comparison is between AHuP without using any real 3D human pose data, and other models which directly benefit from using the training and test split from the same benchmarks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">h36m_pami </a>; <a href="#bib.bib37" title="" class="ltx_ref">singleshotmultiperson2018 </a></cite>, with no domain shift to overcome at all <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">sun2018integral </a>; <a href="#bib.bib66" title="" class="ltx_ref">yasin2016dual </a>; <a href="#bib.bib39" title="" class="ltx_ref">moon2019camera </a></cite>.
Following the convention in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">moon2019camera </a></cite>, we reported our performance under Human3.6M Protocol#2 as shown in Tab.¬†<a href="#S4.T4" title="Table 4 ‚Ä£ 4.7 Practical Values of AHuP ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. As 3D pose estimation is a scale-uncertain process, different datasets have different camera poses and parameters,
which will directly affect the regression results. So we also included the rigid Procrustes analysis (PA) alignment <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">gower1975generalized </a></cite> result for a more just comparison.
Though it cannot match the best performing models, we demonstrate that our model can already rival some approaches that learn directly from real 3D human pose data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib66" title="" class="ltx_ref">yasin2016dual </a>; <a href="#bib.bib7" title="" class="ltx_ref">chen20173d </a></cite>.
Furthermore, the difference in pose definition could introduce additional estimation errors.
For example, in ScanAva+ the ‚Äúhead‚Äù joint lies at the top point of the head, yet in Human3.6M, this joint is biased towards the head center. The ‚Äúankle joint‚Äù in Human 3.6M lies at the back of the heel, yet ScanAva+ and SURREAL place them closer to the center of the ankle above the foot.
</p>
</div>
<div id="S4.SS8.p2" class="ltx_para">
<p id="S4.SS8.p2.1" class="ltx_p">One question is whether or not we really benefit from the learned features that are extracted from the synthetic data with the SAA, or does it only constitute a 2D-to-3D lifting. To investigate this question, we specifically trained a 2D-to-3D lifting SOTA model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">martinez2017simple </a></cite> under the same setting of AHuP with synthetic ScanAva+ data. Its performance is reported as Martinez (ScanAva+) in the Tab.¬†<a href="#S4.T4" title="Table 4 ‚Ä£ 4.7 Practical Values of AHuP ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, which shows that AHuP still performs noticeably better than 2D-to-3D lifting version when employed under the same setting.</p>
</div>
<div id="S4.SS8.p3" class="ltx_para">
<p id="S4.SS8.p3.1" class="ltx_p">Another set of real 3D pose data evaluations is conducted on MuPoTS <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">singleshotmultiperson2018 </a></cite> with results shown in Tab.¬†<a href="#S4.T5" title="Table 5 ‚Ä£ 4.7 Practical Values of AHuP ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. It is quite surprising that despite the fact that we do not use a single frame of real 3D human data, AHuP shows competitive performance among SOTA models that are trained on the real 3D human pose data.
This mainly stems from the fact that MuPoTS is collected in the wild with multiple people, so the images are in a more natural setting. In comparison, Human3.6M is collected in a studio environment with a limited number of subjects.
For a fixed lab setting such as Human3.6M, many context factors, such as camera pose and background, can be inherently well-studied from its corresponding training set, but it is not the case for MuPoTS, since its data seems to have higher variations in these factors.</p>
</div>
</section>
<section id="S4.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.9 </span>Qualitative Comparison</h3>

<div id="S4.SS9.p1" class="ltx_para">
<p id="S4.SS9.p1.1" class="ltx_p">We also visualized the recovered 3D pose results when AHuP trained on ScanAva+ is used (see Fig.¬†<a href="#S4.F6" title="Figure 6 ‚Ä£ 4.7 Practical Values of AHuP ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). Despite the lower performance compared to the top rank 3D pose estimation models, the recovered skeletons via our method agree well with human perception.
In fact, when recovered joints are within a tolerable error threshold, from a human perspective, our prediction is ‚Äúsemantically‚Äù correct.
We also tested AHuP on synthetic data in the last two columns of Fig.¬†<a href="#S4.F6" title="Figure 6 ‚Ä£ 4.7 Practical Values of AHuP ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, which performs equivalently well.</p>
</div>
</section>
<section id="S4.SS10" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.10 </span>Evaluation in 3D Multi-Person Pose Estimation</h3>

<div id="S4.SS10.p1" class="ltx_para">
<p id="S4.SS10.p1.1" class="ltx_p">Our approach also shows compatibility with other works. Combining with the proposed approach in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">moon2019camera </a></cite> by employing Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref">he2017mask </a></cite> as detect-net (for human detection), our model can be well integrated as a pose-net (for 3D human pose estimation) for multi-person pose estimation. As multi-person performance is also affected by root localization, we only report qualitative results, as shown in Fig.¬†<a href="#S4.F7" title="Figure 7 ‚Ä£ 4.10 Evaluation in 3D Multi-Person Pose Estimation ‚Ä£ 4 Performance Evaluation ‚Ä£ Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> without taking credits from <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">moon2019camera </a></cite>.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2105.10837/assets/x5.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Qualitative recovery results of ScanAva-AHuP on MSCOCO dataset.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The ultimate goal of training an inference model is to make the model ready to perform in some real world applications. Training and testing under different contexts potentially introduces the domain gap and influences the model performance negatively. This issue is especially magnified in the 3D human pose problem, where the majority of the 3D human pose benchmarks are collected under controlled lab settings.
To mitigate this effect, we presented our adapted human pose (AHuP) approach that incorporates a semantic awareness adaptation (SAA) technique as well as a skeletal pose adaptation (SPA) algorithm and illustrated how AHuP improves 3D pose estimation model performance both quantitatively and qualitatively.
For a better illustration of an application with a significant context shift, we chose the synthetic human data to train our inference model without using any real 3D human pose data. We then tested AHuP on the well-known 3D human pose benchmarks, in which it showed comparable performance with many of the state-of-the-art (SOTA) models, which have full access to the real 3D pose data.
For existing SOTA models, our approach can also be added as a light-weighted adaptation head which showed consistent improvement for all the candidate models, over all training and testing combinations in our study.
Admittedly, without having access to the target real 3D human data, AHuP has a challenge in beating the best performers. However, in a real-life problem, the solution we care most about is often not what we can achieve under an ideal condition (such as a controlled lab setting) but how well we can get a solution when given limited access to target data under the practical constraints.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
Akhter, I., Black, M.J.: Pose-conditioned joint angle limits for 3d human pose
reconstruction.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 1446‚Äì1455 (2015)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
Andriluka, M., Pishchulin, L., Gehler, P., Schiele, B.: 2d human pose
estimation: New benchmark and state of the art analysis.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on computer Vision and Pattern
Recognition, pp. 3686‚Äì3693 (2014)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.:
Scape: shape completion and animation of people.

</span>
<span class="ltx_bibblock">In: ACM transactions on graphics (TOG), vol.¬†24, pp. 408‚Äì416. ACM
(2005)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(4)</span>
<span class="ltx_bibblock">
Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black, M.J.: Keep
it smpl: Automatic estimation of 3d human pose and shape from a single image.

</span>
<span class="ltx_bibblock">In: European Conference on Computer Vision, pp. 561‚Äì578. Springer
(2016)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(5)</span>
<span class="ltx_bibblock">
Butler, D.J., Wulff, J., Stanley, G.B., Black, M.J.: A naturalistic open source
movie for optical flow evaluation.

</span>
<span class="ltx_bibblock">In: European conference on computer vision, pp. 611‚Äì625. Springer
(2012)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(6)</span>
<span class="ltx_bibblock">
Cao, M., Zhou, X., Xu, Y., Pang, Y., Yao, B.: Adversarial domain adaptation
with semantic consistency for cross-domain image classification.

</span>
<span class="ltx_bibblock">In: Proceedings of the 28th ACM International Conference on
Information and Knowledge Management, CIKM ‚Äô19, p. 259‚Äì268. Association for
Computing Machinery, New York, NY, USA (2019).

</span>
<span class="ltx_bibblock">DOI¬†<span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3357384.3357918</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi-org.ezproxy.neu.edu/10.1145/3357384.3357918" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi-org.ezproxy.neu.edu/10.1145/3357384.3357918</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
Chen, C.H., Ramanan, D.: 3d human pose estimation= 2d pose estimation+
matching.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 7035‚Äì7043 (2017)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(8)</span>
<span class="ltx_bibblock">
Chen, C.H., Tyagi, A., Agrawal, A., Drover, D., Stojanov, S., Rehg, J.M.:
Unsupervised 3d pose estimation with geometric self-supervision.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5714‚Äì5724 (2019)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
Chen, W., Wang, H., Li, Y., Su, H., Wang, Z., Tu, C., Lischinski, D., Cohen-Or,
D., Chen, B.: Synthesizing training images for boosting human 3d pose
estimation.

</span>
<span class="ltx_bibblock">In: 2016 Fourth International Conference on 3D Vision (3DV), pp.
479‚Äì488. IEEE (2016)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(10)</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A
large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In: 2009 IEEE conference on computer vision and pattern recognition,
pp. 248‚Äì255. Ieee (2009)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
Fang, H.S., Xu, Y., Wang, W., Liu, X., Zhu, S.C.: Learning pose grammar to
encode human body configuration for 3d pose estimation.

</span>
<span class="ltx_bibblock">In: Thirty-Second AAAI Conference on Artificial Intelligence (2018)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(12)</span>
<span class="ltx_bibblock">
Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,
F., Marchand, M., Lempitsky, V.: Domain-adversarial training of neural
networks.

</span>
<span class="ltx_bibblock">The Journal of Machine Learning Research <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">17</span>(1), 2096‚Äì2030
(2016)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(13)</span>
<span class="ltx_bibblock">
Glorot, X., Bengio, Y.: Understanding the difficulty of training deep
feedforward neural networks.

</span>
<span class="ltx_bibblock">In: Proceedings of the thirteenth international conference on
artificial intelligence and statistics, pp. 249‚Äì256 (2010)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(14)</span>
<span class="ltx_bibblock">
Gower, J.C.: Generalized procrustes analysis.

</span>
<span class="ltx_bibblock">Psychometrika <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">40</span>(1), 33‚Äì51 (1975)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(15)</span>
<span class="ltx_bibblock">
He, K., Gkioxari, G., Doll√°r, P., Girshick, R.: Mask r-cnn.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE international conference on computer
vision, pp. 2961‚Äì2969 (2017)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 770‚Äì778 (2016)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
Hoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A.,
Darrell, T.: Cycada: Cycle-consistent adversarial domain adaptation.

</span>
<span class="ltx_bibblock">In: Proceedings of the 35th International Conference on Machine
Learning (2018)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(18)</span>
<span class="ltx_bibblock">
Hong, Z.W., Yu-Ming, C., Su, S.Y., Shann, T.Y., Chang, Y.H., Yang, H.K., Ho,
B.H.L., Tu, C.C., Chang, Y.C., Hsiao, T.C., et¬†al.: Virtual-to-real: Learning
to control in visual semantic segmentation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1802.00285 (2018)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(19)</span>
<span class="ltx_bibblock">
Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6m: Large scale
datasets and predictive methods for 3d human sensing in natural environments.

</span>
<span class="ltx_bibblock">IEEE Transactions on Pattern Analysis and Machine Intelligence
<span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">36</span>(7), 1325‚Äì1339 (2014)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(20)</span>
<span class="ltx_bibblock">
Iqbal, U., Molchanov, P., Kautz, J.: Weakly-supervised 3d human pose learning
via multi-view images in the wild.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) (2020)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(21)</span>
<span class="ltx_bibblock">
Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with
conditional adversarial networks.

</span>
<span class="ltx_bibblock">In: Computer Vision and Pattern Recognition (CVPR), 2017 IEEE
Conference on (2017)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(22)</span>
<span class="ltx_bibblock">
Jahangiri, E., Yuille, A.L.: Generating multiple diverse hypotheses for human
3d pose consistent with 2d joint detections.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 805‚Äì814 (2017)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(23)</span>
<span class="ltx_bibblock">
Kocabas, M., Karagoz, S., Akbas, E.: Self-supervised learning of 3d human pose
using multi-view geometry.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1903.02330 (2019)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(24)</span>
<span class="ltx_bibblock">
Lassner, C., Romero, J., Kiefel, M., Bogo, F., Black, M.J., Gehler, P.V.: Unite
the people: Closing the loop between 3d and 2d human representations.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 6050‚Äì6059 (2017)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(25)</span>
<span class="ltx_bibblock">
Li, S., Chan, A.B.: 3d human pose estimation from monocular images with deep
convolutional neural network.

</span>
<span class="ltx_bibblock">In: Asian Conference on Computer Vision, pp. 332‚Äì347. Springer
(2014)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(26)</span>
<span class="ltx_bibblock">
Li, S., Ke, L., Pratama, K., Tai, Y.W., Tang, C.K., Cheng, K.T.: Cascaded deep
monocular 3d human pose estimation with evolutionary training data.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) (2020)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(27)</span>
<span class="ltx_bibblock">
Liebelt, J., Schmid, C.: Multi-view object class detection with a 3d geometric
model.

</span>
<span class="ltx_bibblock">In: 2010 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, pp. 1688‚Äì1695. IEEE (2010)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(28)</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
Doll√°r, P., Zitnick, C.L.: Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In: European conference on computer vision, pp. 740‚Äì755. Springer
(2014)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(29)</span>
<span class="ltx_bibblock">
Liu, S., Ostadabbas, S.: A semi-supervised data augmentation approach using 3d
graphical engines.

</span>
<span class="ltx_bibblock">In: Proceedings of the European Conference on Computer Vision (ECCV),
pp. 0‚Äì0 (2018)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(30)</span>
<span class="ltx_bibblock">
Long, M., Cao, Y., Wang, J., Jordan, M.I.: Learning transferable features with
deep adaptation networks.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1502.02791 (2015)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(31)</span>
<span class="ltx_bibblock">
Loper, M., Mahmood, N., Black, M.J.: Mosh: Motion and shape capture from sparse
markers.

</span>
<span class="ltx_bibblock">ACM Transactions on Graphics (TOG) <span id="bib.bib31.1.1" class="ltx_text ltx_font_bold">33</span>(6), 1‚Äì13 (2014)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(32)</span>
<span class="ltx_bibblock">
Luo, Y., Zheng, L., Guan, T., Yu, J., Yang, Y.: Taking a closer look at domain
shift: Category-level adversaries for semantics consistent domain adaptation.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(33)</span>
<span class="ltx_bibblock">
Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne.

</span>
<span class="ltx_bibblock">Journal of machine learning research <span id="bib.bib33.1.1" class="ltx_text ltx_font_bold">9</span>(Nov), 2579‚Äì2605
(2008)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(34)</span>
<span class="ltx_bibblock">
Marin, J., V√°zquez, D., Ger√≥nimo, D., L√≥pez, A.M.: Learning
appearance in virtual scenarios for pedestrian detection.

</span>
<span class="ltx_bibblock">In: 2010 IEEE computer society conference on computer vision and
pattern recognition, pp. 137‚Äì144. IEEE (2010)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(35)</span>
<span class="ltx_bibblock">
Martinez, J., Hossain, R., Romero, J., Little, J.J.: A simple yet effective
baseline for 3d human pose estimation.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 2640‚Äì2649 (2017)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(36)</span>
<span class="ltx_bibblock">
Mehta, D., Rhodin, H., Casas, D., Fua, P., Sotnychenko, O., Xu, W., Theobalt,
C.: Monocular 3d human pose estimation in the wild using improved cnn
supervision.

</span>
<span class="ltx_bibblock">In: 2017 International Conference on 3D Vision (3DV), pp. 506‚Äì516
(2017)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(37)</span>
<span class="ltx_bibblock">
Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Sridhar, S., Pons-Moll, G.,
Theobalt, C.: Single-shot multi-person 3d pose estimation from monocular rgb.

</span>
<span class="ltx_bibblock">In: 3D Vision (3DV), 2018 Sixth International Conference on. IEEE
(2018).

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson</a>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(38)</span>
<span class="ltx_bibblock">
Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Sridhar, S., Pons-Moll, G.,
Theobalt, C.: Single-shot multi-person 3d pose estimation from monocular rgb.

</span>
<span class="ltx_bibblock">In: 2018 International Conference on 3D Vision (3DV), pp. 120‚Äì130.
IEEE (2018)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(39)</span>
<span class="ltx_bibblock">
Moon, G., Chang, J.Y., Lee, K.M.: Camera distance-aware top-down approach for
3d multi-person pose estimation from a single rgb image.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 10133‚Äì10142 (2019)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(40)</span>
<span class="ltx_bibblock">
Moreno-Noguer, F.: 3d human pose estimation from a single image via distance
matrix regression.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2823‚Äì2832 (2017)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(41)</span>
<span class="ltx_bibblock">
Morgado, P., Vasconcelos, N.: Semantically consistent regularization for
zero-shot recognition.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (2017)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(42)</span>
<span class="ltx_bibblock">
Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose
estimation.

</span>
<span class="ltx_bibblock">European Conference on Computer Vision pp. 483‚Äì499 (2016)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(43)</span>
<span class="ltx_bibblock">
Pavllo, D., Feichtenhofer, C., Grangier, D., Auli, M.: 3d human pose estimation
in video with temporal convolutions and semi-supervised training.

</span>
<span class="ltx_bibblock">In: Conference on Computer Vision and Pattern Recognition (CVPR)
(2019)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(44)</span>
<span class="ltx_bibblock">
Peng, X., Sun, B., Ali, K., Saenko, K.: Learning deep object detectors from 3d
models.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 1278‚Äì1286 (2015)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(45)</span>
<span class="ltx_bibblock">
Peris, M., Martull, S., Maki, A., Ohkawa, Y., Fukui, K.: Towards a simulation
driven stereo vision system.

</span>
<span class="ltx_bibblock">In: Proceedings of the 21st International Conference on Pattern
Recognition (ICPR2012), pp. 1038‚Äì1042. IEEE (2012)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(46)</span>
<span class="ltx_bibblock">
Popa, A.I., Zanfir, M., Sminchisescu, C.: Deep multitask architecture for
integrated 2d and 3d human sensing.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 6289‚Äì6298 (2017)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(47)</span>
<span class="ltx_bibblock">
Qui√±onero-Candela, J., Sugiyama, M., Schwaighofer, A., Lawrence, N.:
Covariate shift and local learning by distribution matching (2008)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(48)</span>
<span class="ltx_bibblock">
Ragheb, H., Velastin, S., Remagnino, P., Ellis, T.: Vihasi: virtual human
action silhouette data for the performance evaluation of silhouette-based
action recognition methods.

</span>
<span class="ltx_bibblock">In: 2008 Second ACM/IEEE International Conference on Distributed
Smart Cameras, pp. 1‚Äì10. IEEE (2008)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(49)</span>
<span class="ltx_bibblock">
Ramakrishna, V., Kanade, T., Sheikh, Y.: Reconstructing 3d human pose from 2d
image landmarks.

</span>
<span class="ltx_bibblock">In: European Conference on Computer Vision, pp. 573‚Äì586. Springer
(2012)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(50)</span>
<span class="ltx_bibblock">
Remelli, E., Han, S., Honari, S., Fua, P., Wang, R.: Lightweight multi-view 3d
pose estimation through camera-disentangled representation.

</span>
<span class="ltx_bibblock">In: IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) (2020)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(51)</span>
<span class="ltx_bibblock">
Rogez, G., Weinzaepfel, P., Schmid, C.: Lcr-net:
Localization-classification-regression for human pose.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3433‚Äì3441 (2017)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(52)</span>
<span class="ltx_bibblock">
Rogez, G., Weinzaepfel, P., Schmid, C.: Lcr-net++: Multi-person 2d and 3d pose
detection in natural images.

</span>
<span class="ltx_bibblock">IEEE transactions on pattern analysis and machine intelligence
(2019)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(53)</span>
<span class="ltx_bibblock">
Sigal, L., Balan, A.O., Black, M.J.: Humaneva: Synchronized video and motion
capture dataset and baseline algorithm for evaluation of articulated human
motion.

</span>
<span class="ltx_bibblock">International journal of computer vision <span id="bib.bib53.1.1" class="ltx_text ltx_font_bold">87</span>(1-2), 4 (2010)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(54)</span>
<span class="ltx_bibblock">
Sun, B., Saenko, K.: Deep coral: Correlation alignment for deep domain
adaptation.

</span>
<span class="ltx_bibblock">In: European Conference on Computer Vision, pp. 443‚Äì450. Springer
(2016)

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(55)</span>
<span class="ltx_bibblock">
Sun, X., Shang, J., Liang, S., Wei, Y.: Compositional human pose regression.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 2602‚Äì2611 (2017)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(56)</span>
<span class="ltx_bibblock">
Sun, X., Xiao, B., Wei, F., Liang, S., Wei, Y.: Integral human pose regression.

</span>
<span class="ltx_bibblock">In: Proceedings of the European Conference on Computer Vision (ECCV),
pp. 529‚Äì545 (2018)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(57)</span>
<span class="ltx_bibblock">
Tome, D., Russell, C., Agapito, L.: Lifting from the deep: Convolutional 3d
pose estimation from a single image.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2500‚Äì2509 (2017)

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(58)</span>
<span class="ltx_bibblock">
Tzeng, E., Hoffman, J., Darrell, T., Saenko, K.: Simultaneous deep transfer
across domains and tasks.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 4068‚Äì4076 (2015)

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(59)</span>
<span class="ltx_bibblock">
Tzeng, E., Hoffman, J., Saenko, K., Darrell, T.: Adversarial discriminative
domain adaptation.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 7167‚Äì7176 (2017)

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(60)</span>
<span class="ltx_bibblock">
Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T.: Deep domain
confusion: Maximizing for domain invariance.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1412.3474 (2014)

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(61)</span>
<span class="ltx_bibblock">
Varol, G., Romero, J., Martin, X., Mahmood, N., Black, M.J., Laptev, I.,
Schmid, C.: Learning from synthetic humans.

</span>
<span class="ltx_bibblock">In: CVPR (2017)

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(62)</span>
<span class="ltx_bibblock">
Wang, M., Yang, G.Y., Li, R., Liang, R.Z., Zhang, S.H., Hall, P.M., Hu, S.M.:
Example-guided style-consistent image synthesis from semantic labeling.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) (2019)

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(63)</span>
<span class="ltx_bibblock">
Wang, Q., Gao, J., Lin, W., Yuan, Y.: Learning from synthetic data for crowd
counting in the wild.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 8198‚Äì8207 (2019)

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(64)</span>
<span class="ltx_bibblock">
Xu, J., Yu, Z., Ni, B., Yang, J., Yang, X., Zhang, W.: Deep kinematics analysis
for monocular 3d human pose estimation.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) (2020)

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(65)</span>
<span class="ltx_bibblock">
Yang, W., Ouyang, W., Wang, X., Ren, J., Li, H., Wang, X.: 3d human pose
estimation in the wild by adversarial learning.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5255‚Äì5264 (2018)

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(66)</span>
<span class="ltx_bibblock">
Yasin, H., Iqbal, U., Kruger, B., Weber, A., Gall, J.: A dual-source approach
for 3d pose estimation from a single image.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4948‚Äì4956 (2016)

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(67)</span>
<span class="ltx_bibblock">
Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y.: Random erasing data
augmentation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1708.04896 (2017)

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(68)</span>
<span class="ltx_bibblock">
Zhou, X., Huang, Q., Sun, X., Xue, X., Wei, Y.: Towards 3d human pose
estimation in the wild: a weakly-supervised approach.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 398‚Äì407 (2017)

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(69)</span>
<span class="ltx_bibblock">
Zhou, X., Leonardos, S., Hu, X., Daniilidis, K.: 3d shape estimation from 2d
landmarks: A convex relaxation approach.

</span>
<span class="ltx_bibblock">In: proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4447‚Äì4455 (2015)

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(70)</span>
<span class="ltx_bibblock">
Zhou, X., Zhu, M., Leonardos, S., Daniilidis, K.: Sparse representation for 3d
shape estimation: A convex relaxation approach.

</span>
<span class="ltx_bibblock">IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib70.1.1" class="ltx_text ltx_font_bold">39</span>(8), 1648‚Äì1661 (2016)

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(71)</span>
<span class="ltx_bibblock">
Zhou, X., Zhu, M., Pavlakos, G., Leonardos, S., Derpanis, K.G., Daniilidis, K.:
Monocap: Monocular human motion capture using a cnn coupled with a geometric
prior.

</span>
<span class="ltx_bibblock">IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib71.1.1" class="ltx_text ltx_font_bold">41</span>(4), 901‚Äì914 (2018)

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(72)</span>
<span class="ltx_bibblock">
Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image
translation using cycle-consistent adversarial networks.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE international conference on computer
vision, pp. 2223‚Äì2232 (2017)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2105.10836" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2105.10837" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2105.10837">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2105.10837" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2105.10838" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 05:23:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
