<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia</title>
<!--Generated on Sat Oct  5 18:16:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04254v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S1" title="In Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S2" title="In Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S3" title="In Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Task formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="In Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5" title="In Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Entity insertion with <span class="ltx_text ltx_font_smallcaps">LocEI</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS1" title="In 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS2" title="In 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Two-stage training pipeline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS3" title="In 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Knowledge injection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS4" title="In 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Incorporating multilinguality (<span class="ltx_text ltx_font_smallcaps">xLocEI</span>)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6" title="In Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS1" title="In 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS2" title="In 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS3" title="In 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS4" title="In 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Main results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS4.SSSx1" title="In 6.4 Main results ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title">Zero-shot vs. Fine-tuned</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS5" title="In 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Ablation analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S7" title="In Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S7.SS1" title="In 7 Discussions ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Summary of findings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S7.SS2" title="In 7 Discussions ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Implications and broader impact</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A1" title="In Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional related work</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A1.SS1" title="In Appendix A Additional related work ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Pre-trained language models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A1.SS2" title="In Appendix A Additional related work ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Ranking tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A1.SS3" title="In Appendix A Additional related work ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Domain adaption</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2" title="In Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional dataset processing details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.SS1" title="In Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Data preparation steps</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.SS2" title="In Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Dataset statistics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.SS3" title="In Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>Entity insertion categories</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.SS4" title="In Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>Dynamic context removal</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.SS5" title="In Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.5 </span>Rules for sampling negative candidates</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3" title="In Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional experiments</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS1" title="In Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS2" title="In Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Multilingual entity insertion stratified by language</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS3" title="In Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Zero-shot entity insertion stratified by language</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS4" title="In Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.4 </span>Impact of the starting model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS5" title="In Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.5 </span>Impact of the model size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS6" title="In Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.6 </span>Impact of the size of training data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS7" title="In Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.7 </span>Training stages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS8" title="In Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.8 </span>RoBERTa vs XLM-RoBERTa</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS9" title="In Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.9 </span>Single Encoder vs Triple Encoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS10" title="In Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.10 </span>Pointwise Loss vs Ranking Loss</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Tomás Feith, 
Akhil Arora,<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotemark: </span></span></span></span>  
Martin Gerlach,
Debjit Paul,
Robert West  
<br class="ltx_break"/>EPFL   
Aarhus University   
Wikimedia Foundation 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.9.id1">tsfeith@gmail.com</span>,
<span class="ltx_text ltx_font_typewriter" id="id10.10.id2">akhil.arora@cs.au.dk</span>,
<span class="ltx_text ltx_font_typewriter" id="id11.11.id3">mgerlach@wikimedia.org</span>, 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id12.12.id4">{debjit.paul, robert.west}@epfl.ch</span>
</span><span class="ltx_author_notes">  Equal contribution, contact: <a class="ltx_ref ltx_href" href="mailto:akhil.arora@cs.au.dk" title="">akhil.arora@cs.au.dk</a>.  Work done at EPFL.  R. West is a Wikimedia Foundation Research Fellow.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id13.id1">Links are a fundamental part of information networks, turning isolated pieces of knowledge into a network of information richer than the sum of its parts. However, adding a new link to the network is not trivial: it requires not only the identification of a suitable pair of source and target entities but also the understanding of the content of the source to locate a suitable position for the link in the text. The latter problem has not been addressed effectively, particularly in the absence of text spans in the source that could serve as anchors to insert a link to the target entity. To bridge this gap, we introduce and operationalize the task of <em class="ltx_emph ltx_font_italic" id="id13.id1.1">entity insertion</em> in information networks. Focusing on the case of Wikipedia, we empirically show that this problem is, both, relevant and challenging for editors. We compile a benchmark dataset in 105 languages and develop a framework for entity insertion called <span class="ltx_text ltx_font_smallcaps" id="id13.id1.2">LocEI</span> (Localized Entity Insertion) and its multilingual variant <span class="ltx_text ltx_font_smallcaps" id="id13.id1.3">xLocEI</span>. We show that <span class="ltx_text ltx_font_smallcaps" id="id13.id1.4">xLocEI</span> outperforms all baseline models (including state-of-the-art prompt-based ranking with LLMs such as GPT-4) and that it can be applied in a zero-shot manner on languages not seen during training with minimal performance drop. These findings are important for applying entity insertion models in practice, e.g., to support editors in adding links across the more than 300 language versions of Wikipedia.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="304" id="S1.F1.g1" src="extracted/5903880/images/entity_insertion_example.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><em class="ltx_emph ltx_font_italic" id="S1.F1.8.1" style="color:#0000FF;">Entity linking</em>: insert a link to the entity <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.F1.9.2" style="color:#000000;">Margaret “Peggy” Woolley</span> by identifying a suitable mention from the existent text in the version <span class="ltx_text ltx_font_bold" id="S1.F1.10.3">before</span> insertion, vs. <em class="ltx_emph ltx_font_italic" id="S1.F1.11.4" style="color:#FF0000;">Entity insertion</em>: no mention existent yet, identify the most suitable span <span class="ltx_text ltx_framed ltx_framed_rectangle" id="S1.F1.12.5" style="color:#FFFFFF;background-color:#FFFFFF;border-color: #FF0000;">ab</span> in the version <span class="ltx_text ltx_font_bold" id="S1.F1.13.6">before</span> to insert the entity <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.F1.14.7" style="color:#000000;">Private school</span>.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">From digital encyclopedias and blogs to knowledge graphs, knowledge on the Web is organized as a network of interlinked entities and their descriptions.
However, online knowledge is not static: new webpages are created, and existing pages are updated almost every day.
While there exists substantial support for content creation (e.g. via translation <cite class="ltx_cite ltx_citemacro_citet">Wulczyn et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib51" title="">2016</a>)</cite> or generative AI tools  <cite class="ltx_cite ltx_citemacro_citet">Shao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib39" title="">2024</a>)</cite>), adding new knowledge not only requires creating content but also integrating it into the existing knowledge structure. The latter usually leaves editors with the time-consuming task of reading lengthy webpages to identify a relevant text span for inserting an entity that is not yet mentioned on the page. Thus, to support editors in effectively integrating entities in multilingual linked corpora on the Web, we introduce the task of <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">entity insertion</em>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text ltx_font_bold" id="S1.p2.1.1">Entity insertion.</span> We consider Wikipedia as the primary use case and focus on the task of adding links.
Specifically, given a source and target entity, the goal of <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">entity insertion</em> is to identify the most suitable text span in the article describing the source entity for inserting a link to the target entity. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">1</span></a> portrays a real example of the entity insertion task with the eventual goal of adding a link from the source entity <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.3">June Spencer</span>, a former English actress, to the target entity <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.4">Private school</span>.
Most importantly, entity insertion is a different and much more challenging task when compared to <em class="ltx_emph ltx_font_italic" id="S1.p2.1.5">entity linking</em>, as no existent text span in the version of the source article (<span class="ltx_text ltx_font_typewriter" id="S1.p2.1.6">June Spencer</span>) at edit time could be used to link to the target entity (<span class="ltx_text ltx_font_typewriter" id="S1.p2.1.7">Private school</span>). Rather, a new text span–<em class="ltx_emph ltx_font_italic" id="S1.p2.1.8">“She also worked at a private school.”</em>–was added along with the to-be-inserted target entity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text ltx_font_bold" id="S1.p3.1.1">Challenges.</span> Entity insertion is not only an interesting and challenging language understanding task, but it is also the most common scenario faced by editors when adding links in practice.
In fact, we find that for <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">60-70%</em> of all the links added to Wikipedia, <em class="ltx_emph ltx_font_italic" id="S1.p3.1.3">none of the existing text</em> is suitable to insert the corresponding entities, and new text needs to be added along with the entity by the editor (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">2</span></a>).
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">2</span></a> also shows that entity insertion is associated with a <em class="ltx_emph ltx_font_italic" id="S1.p3.1.4">high cognitive load</em>, as the task requires, on average, an editor to select the most suitable sentence from a pool of <math alttext="\sim" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><csymbol cd="latexml" id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">∼</annotation></semantics></math>100 candidate sentences.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Therefore, it is vital to operationalize and develop new methods for entity insertion in order to support editors in adding links to Wikipedia and other information networks.
To this end, we make the following key contributions in this paper.</p>
</div>
<figure class="ltx_figure" id="S1.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_square" height="332" id="S1.F2.g1" src="x1.png" width="402"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="300" id="S1.F2.g2" src="x2.png" width="403"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Challenges of entity insertion. (Left) Micro (weighted by the number of data points in a language) and macro (equal weight to each language) aggregates of insertion types over the 105 languages considered in this study. (Right) Complementary cumulative distribution function (CCDF) of the number of candidate sentences (<math alttext="N" class="ltx_Math" display="inline" id="S1.F2.2.m1.1"><semantics id="S1.F2.2.m1.1b"><mi id="S1.F2.2.m1.1.1" xref="S1.F2.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S1.F2.2.m1.1c"><ci id="S1.F2.2.m1.1.1.cmml" xref="S1.F2.2.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.2.m1.1d">N</annotation><annotation encoding="application/x-llamapun" id="S1.F2.2.m1.1e">italic_N</annotation></semantics></math>) in a Wikipedia article (log x-axis).</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Contributions.</span>
We introduce the <em class="ltx_emph ltx_font_italic" id="S1.p5.1.2">novel task of entity insertion</em> (§ <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S3" title="3 Task formulation ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">3</span></a>).
We release a <em class="ltx_emph ltx_font_italic" id="S1.p5.1.3">large dataset in 105 languages</em> of links from Wikipedia articles to enable further research into entity insertion (§ <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>).
We introduce <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.4">LocEI</span>, a framework for entity insertion, and its multilingual variant <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.5">xLocEI</span> (§ <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5" title="5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5</span></a>).
We show the benefit of multilingual knowledge in downstream performance and highlight the <em class="ltx_emph ltx_font_italic" id="S1.p5.1.6">zero-shot</em> capabilities of <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.7">xLocEI</span> (§ <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6" title="6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we review works that overlap closely with our study (cf. Appx. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A1" title="Appendix A Additional related work ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">A</span></a> for details).</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Entity linking.</span>

Previous work has framed entity insertion as an entity linking problem <cite class="ltx_cite ltx_citemacro_citep">(Gerlach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib17" title="">2021</a>; Milne and Witten, <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib28" title="">2008</a>; West et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib45" title="">2009</a>; Arora et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib1" title="">2021</a>; Čuljak et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib8" title="">2022</a>; West et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib46" title="">2010</a>)</cite>, where the goal is to assign a unique identity to entities mentioned in the text. The task of entity linking is composed of two sub-tasks: Named Entity Recognition (NER) and Named Entity Disambiguation (NED). Most research <cite class="ltx_cite ltx_citemacro_citep">(Hoffart et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib21" title="">2011</a>; Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib14" title="">2020</a>; van Hulst et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib43" title="">2020</a>)</cite> into entity linking solves first the NER problem, in which the task is to find candidate mentions for named entities in the source article.
However, there is recent work <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib53" title="">2022</a>)</cite> exploring the problem in reverse order, first solving NED by finding target entities related to the source article and then NER searching only for mentions for the found targets.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">When the mention is present, the task of entity insertion is similar to NER <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib53" title="">2022</a>)</cite>, as both tasks can be solved by searching for mentions in the text.
However, entity insertion is a more general task as it allows for the mention of the target entity to not yet be present in the text. In this case, the goal is to exploit the context information to find the text span most related to the target entity. NER modules <cite class="ltx_cite ltx_citemacro_citep">(Finkel et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib13" title="">2005</a>; Nothman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib33" title="">2013</a>)</cite> are designed to search for the most related mentions, and thus, they are not applicable in scenarios where the mentions are not yet available.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Entity tagging.</span>
<cite class="ltx_cite ltx_citemacro_citet">Du et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib11" title="">2022</a>)</cite> introduced this task as a relaxed form of entity linking.
An entity tagging model is only tasked with determining the entities present in the text and does not need to find the exact mentions of the entities.
However, even though the model is not tasked with extracting an entity’s mention, the task of entity tagging still assumes that the text contains some mention of the entity, which distinguishes it from entity insertion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.p5.1.1">Link the Wiki.</span>
<cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib22" title="">2008</a>)</cite> ran a track at INEX 2008 with two tasks: file-to-file link discovery and mention-to-BEP (best entry point) link discovery.
File-to-file link discovery is a document-level task that can be framed as a link prediction task in networks, where the Wikipedia articles act as nodes and the links act as edges.
The mention-to-BEP task is an entity linking task with anchor prediction, where the two-part goal is to find mentions in the source article pointing to other articles, and finding the best point of entry (the anchor) in the target file.
This task has more recently resurfaced as an anchor prediction task <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib26" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="308" id="S2.F3.g1" src="x3.png" width="772"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Data processing pipeline. Obtain added links <math alttext="L" class="ltx_Math" display="inline" id="S2.F3.6.m1.1"><semantics id="S2.F3.6.m1.1b"><mi id="S2.F3.6.m1.1.1" xref="S2.F3.6.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.F3.6.m1.1c"><ci id="S2.F3.6.m1.1.1.cmml" xref="S2.F3.6.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.6.m1.1d">L</annotation><annotation encoding="application/x-llamapun" id="S2.F3.6.m1.1e">italic_L</annotation></semantics></math> by taking a set difference of the links existent in consecutive months. For each added link <math alttext="L_{i}" class="ltx_Math" display="inline" id="S2.F3.7.m2.1"><semantics id="S2.F3.7.m2.1b"><msub id="S2.F3.7.m2.1.1" xref="S2.F3.7.m2.1.1.cmml"><mi id="S2.F3.7.m2.1.1.2" xref="S2.F3.7.m2.1.1.2.cmml">L</mi><mi id="S2.F3.7.m2.1.1.3" xref="S2.F3.7.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F3.7.m2.1c"><apply id="S2.F3.7.m2.1.1.cmml" xref="S2.F3.7.m2.1.1"><csymbol cd="ambiguous" id="S2.F3.7.m2.1.1.1.cmml" xref="S2.F3.7.m2.1.1">subscript</csymbol><ci id="S2.F3.7.m2.1.1.2.cmml" xref="S2.F3.7.m2.1.1.2">𝐿</ci><ci id="S2.F3.7.m2.1.1.3.cmml" xref="S2.F3.7.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.7.m2.1d">L_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.F3.7.m2.1e">italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, scan all <math alttext="M" class="ltx_Math" display="inline" id="S2.F3.8.m3.1"><semantics id="S2.F3.8.m3.1b"><mi id="S2.F3.8.m3.1.1" xref="S2.F3.8.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.F3.8.m3.1c"><ci id="S2.F3.8.m3.1.1.cmml" xref="S2.F3.8.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.8.m3.1d">M</annotation><annotation encoding="application/x-llamapun" id="S2.F3.8.m3.1e">italic_M</annotation></semantics></math> versions in the full revision history <math alttext="v_{0}^{i}" class="ltx_Math" display="inline" id="S2.F3.9.m4.1"><semantics id="S2.F3.9.m4.1b"><msubsup id="S2.F3.9.m4.1.1" xref="S2.F3.9.m4.1.1.cmml"><mi id="S2.F3.9.m4.1.1.2.2" xref="S2.F3.9.m4.1.1.2.2.cmml">v</mi><mn id="S2.F3.9.m4.1.1.2.3" xref="S2.F3.9.m4.1.1.2.3.cmml">0</mn><mi id="S2.F3.9.m4.1.1.3" xref="S2.F3.9.m4.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F3.9.m4.1c"><apply id="S2.F3.9.m4.1.1.cmml" xref="S2.F3.9.m4.1.1"><csymbol cd="ambiguous" id="S2.F3.9.m4.1.1.1.cmml" xref="S2.F3.9.m4.1.1">superscript</csymbol><apply id="S2.F3.9.m4.1.1.2.cmml" xref="S2.F3.9.m4.1.1"><csymbol cd="ambiguous" id="S2.F3.9.m4.1.1.2.1.cmml" xref="S2.F3.9.m4.1.1">subscript</csymbol><ci id="S2.F3.9.m4.1.1.2.2.cmml" xref="S2.F3.9.m4.1.1.2.2">𝑣</ci><cn id="S2.F3.9.m4.1.1.2.3.cmml" type="integer" xref="S2.F3.9.m4.1.1.2.3">0</cn></apply><ci id="S2.F3.9.m4.1.1.3.cmml" xref="S2.F3.9.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.9.m4.1d">v_{0}^{i}</annotation><annotation encoding="application/x-llamapun" id="S2.F3.9.m4.1e">italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="v_{M}^{i}" class="ltx_Math" display="inline" id="S2.F3.10.m5.1"><semantics id="S2.F3.10.m5.1b"><msubsup id="S2.F3.10.m5.1.1" xref="S2.F3.10.m5.1.1.cmml"><mi id="S2.F3.10.m5.1.1.2.2" xref="S2.F3.10.m5.1.1.2.2.cmml">v</mi><mi id="S2.F3.10.m5.1.1.2.3" xref="S2.F3.10.m5.1.1.2.3.cmml">M</mi><mi id="S2.F3.10.m5.1.1.3" xref="S2.F3.10.m5.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F3.10.m5.1c"><apply id="S2.F3.10.m5.1.1.cmml" xref="S2.F3.10.m5.1.1"><csymbol cd="ambiguous" id="S2.F3.10.m5.1.1.1.cmml" xref="S2.F3.10.m5.1.1">superscript</csymbol><apply id="S2.F3.10.m5.1.1.2.cmml" xref="S2.F3.10.m5.1.1"><csymbol cd="ambiguous" id="S2.F3.10.m5.1.1.2.1.cmml" xref="S2.F3.10.m5.1.1">subscript</csymbol><ci id="S2.F3.10.m5.1.1.2.2.cmml" xref="S2.F3.10.m5.1.1.2.2">𝑣</ci><ci id="S2.F3.10.m5.1.1.2.3.cmml" xref="S2.F3.10.m5.1.1.2.3">𝑀</ci></apply><ci id="S2.F3.10.m5.1.1.3.cmml" xref="S2.F3.10.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.10.m5.1d">v_{M}^{i}</annotation><annotation encoding="application/x-llamapun" id="S2.F3.10.m5.1e">italic_v start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> to identify the article version in which the link was added and compute the difference between the before and after versions to extract the exact entity insertion scenario.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p6">
<p class="ltx_p" id="S2.p6.1"><span class="ltx_text ltx_font_bold" id="S2.p6.1.1">Passage ranking.</span>
Transformer-based models have revolutionized passage ranking by enhancing semantic understanding beyond traditional lexical methods like BM25 <cite class="ltx_cite ltx_citemacro_cite">Robertson and Zaragoza (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib37" title="">2009</a>)</cite>. BERT demonstrated early success by leveraging contextualized embeddings for re-ranking <cite class="ltx_cite ltx_citemacro_cite">Nogueira and Cho (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib30" title="">2019</a>)</cite>, leading to innovations like ColBERT <cite class="ltx_cite ltx_citemacro_cite">Khattab and Zaharia (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib24" title="">2020</a>)</cite>, which uses a dual-encoder architecture for more efficient retrieval. Recent models such as T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib35" title="">2020</a>)</cite> and ELECTRA <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib6" title="">2020</a>)</cite> further refine ranking by employing advanced pre-training techniques. Building on top of this work, <cite class="ltx_cite ltx_citemacro_cite">Fang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib12" title="">2023</a>); Dong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib10" title="">2022</a>)</cite> employ knowledge graphs to exploit background information to better rank passages. However, such graph-based approaches are not suited for large-scale, highly dynamic graphs (such as Wikipedia), as the cost of recomputing all the embeddings associated with the graph is too high. Finally, while large language models have been shown to be the state of the art for passage ranking <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib34" title="">2024</a>)</cite>, despite their performance they are impractical at the Web-scale owing to exorbitantly high computational costs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p7">
<p class="ltx_p" id="S2.p7.1"><span class="ltx_text ltx_font_bold" id="S2.p7.1.1">Key differences.</span>
Entity insertion is fundamentally different from all the aforementioned tasks and possesses novel downstream applications.
First, entity insertion does not assume that a mention to the target entity is present in the text at inference time.
Second, the optimization objective of entity insertion, which involves identifying the text span most related to a target entity, could be seen as the dual of tasks such as NED and entity tagging, which aim instead to find the most relevant target entity for a given text span.
Finally, entity insertion aims to find the best text span in the source article to insert the target entity.
In contrast, anchor prediction performs the reverse task by trying to find the best text span for grounding the source entity in the target article.
Moreover, anchor prediction is an unnatural task as humans find the vast majority of links to be unanchorable <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib26" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Task formulation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.8">Let <math alttext="E_{\text{src}}" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><msub id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">E</mi><mtext id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3a.cmml">src</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">𝐸</ci><ci id="S3.p1.1.m1.1.1.3a.cmml" xref="S3.p1.1.m1.1.1.3"><mtext id="S3.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.p1.1.m1.1.1.3">src</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">E_{\text{src}}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_E start_POSTSUBSCRIPT src end_POSTSUBSCRIPT</annotation></semantics></math> be a source entity and <math alttext="E_{\text{tgt}}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">E</mi><mtext id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3a.cmml">tgt</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">𝐸</ci><ci id="S3.p1.2.m2.1.1.3a.cmml" xref="S3.p1.2.m2.1.1.3"><mtext id="S3.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.p1.2.m2.1.1.3">tgt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">E_{\text{tgt}}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_E start_POSTSUBSCRIPT tgt end_POSTSUBSCRIPT</annotation></semantics></math> be a target entity. Let <math alttext="X_{src}" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><msub id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">X</mi><mrow id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml"><mi id="S3.p1.3.m3.1.1.3.2" xref="S3.p1.3.m3.1.1.3.2.cmml">s</mi><mo id="S3.p1.3.m3.1.1.3.1" xref="S3.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p1.3.m3.1.1.3.3" xref="S3.p1.3.m3.1.1.3.3.cmml">r</mi><mo id="S3.p1.3.m3.1.1.3.1a" xref="S3.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p1.3.m3.1.1.3.4" xref="S3.p1.3.m3.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">𝑋</ci><apply id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3"><times id="S3.p1.3.m3.1.1.3.1.cmml" xref="S3.p1.3.m3.1.1.3.1"></times><ci id="S3.p1.3.m3.1.1.3.2.cmml" xref="S3.p1.3.m3.1.1.3.2">𝑠</ci><ci id="S3.p1.3.m3.1.1.3.3.cmml" xref="S3.p1.3.m3.1.1.3.3">𝑟</ci><ci id="S3.p1.3.m3.1.1.3.4.cmml" xref="S3.p1.3.m3.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">X_{src}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_X start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT</annotation></semantics></math> be the textual content of the article corresponding to <math alttext="E_{\text{src}}" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><msub id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">E</mi><mtext id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3a.cmml">src</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">𝐸</ci><ci id="S3.p1.4.m4.1.1.3a.cmml" xref="S3.p1.4.m4.1.1.3"><mtext id="S3.p1.4.m4.1.1.3.cmml" mathsize="70%" xref="S3.p1.4.m4.1.1.3">src</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">E_{\text{src}}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">italic_E start_POSTSUBSCRIPT src end_POSTSUBSCRIPT</annotation></semantics></math>. The text can be partitioned into a set of (potentially overlapping) text spans, <math alttext="\mathcal{X}_{src}=\left\{x_{1},...,x_{M}\right\}" class="ltx_Math" display="inline" id="S3.p1.5.m5.3"><semantics id="S3.p1.5.m5.3a"><mrow id="S3.p1.5.m5.3.3" xref="S3.p1.5.m5.3.3.cmml"><msub id="S3.p1.5.m5.3.3.4" xref="S3.p1.5.m5.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.5.m5.3.3.4.2" xref="S3.p1.5.m5.3.3.4.2.cmml">𝒳</mi><mrow id="S3.p1.5.m5.3.3.4.3" xref="S3.p1.5.m5.3.3.4.3.cmml"><mi id="S3.p1.5.m5.3.3.4.3.2" xref="S3.p1.5.m5.3.3.4.3.2.cmml">s</mi><mo id="S3.p1.5.m5.3.3.4.3.1" xref="S3.p1.5.m5.3.3.4.3.1.cmml">⁢</mo><mi id="S3.p1.5.m5.3.3.4.3.3" xref="S3.p1.5.m5.3.3.4.3.3.cmml">r</mi><mo id="S3.p1.5.m5.3.3.4.3.1a" xref="S3.p1.5.m5.3.3.4.3.1.cmml">⁢</mo><mi id="S3.p1.5.m5.3.3.4.3.4" xref="S3.p1.5.m5.3.3.4.3.4.cmml">c</mi></mrow></msub><mo id="S3.p1.5.m5.3.3.3" xref="S3.p1.5.m5.3.3.3.cmml">=</mo><mrow id="S3.p1.5.m5.3.3.2.2" xref="S3.p1.5.m5.3.3.2.3.cmml"><mo id="S3.p1.5.m5.3.3.2.2.3" xref="S3.p1.5.m5.3.3.2.3.cmml">{</mo><msub id="S3.p1.5.m5.2.2.1.1.1" xref="S3.p1.5.m5.2.2.1.1.1.cmml"><mi id="S3.p1.5.m5.2.2.1.1.1.2" xref="S3.p1.5.m5.2.2.1.1.1.2.cmml">x</mi><mn id="S3.p1.5.m5.2.2.1.1.1.3" xref="S3.p1.5.m5.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.p1.5.m5.3.3.2.2.4" xref="S3.p1.5.m5.3.3.2.3.cmml">,</mo><mi id="S3.p1.5.m5.1.1" mathvariant="normal" xref="S3.p1.5.m5.1.1.cmml">…</mi><mo id="S3.p1.5.m5.3.3.2.2.5" xref="S3.p1.5.m5.3.3.2.3.cmml">,</mo><msub id="S3.p1.5.m5.3.3.2.2.2" xref="S3.p1.5.m5.3.3.2.2.2.cmml"><mi id="S3.p1.5.m5.3.3.2.2.2.2" xref="S3.p1.5.m5.3.3.2.2.2.2.cmml">x</mi><mi id="S3.p1.5.m5.3.3.2.2.2.3" xref="S3.p1.5.m5.3.3.2.2.2.3.cmml">M</mi></msub><mo id="S3.p1.5.m5.3.3.2.2.6" xref="S3.p1.5.m5.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.3b"><apply id="S3.p1.5.m5.3.3.cmml" xref="S3.p1.5.m5.3.3"><eq id="S3.p1.5.m5.3.3.3.cmml" xref="S3.p1.5.m5.3.3.3"></eq><apply id="S3.p1.5.m5.3.3.4.cmml" xref="S3.p1.5.m5.3.3.4"><csymbol cd="ambiguous" id="S3.p1.5.m5.3.3.4.1.cmml" xref="S3.p1.5.m5.3.3.4">subscript</csymbol><ci id="S3.p1.5.m5.3.3.4.2.cmml" xref="S3.p1.5.m5.3.3.4.2">𝒳</ci><apply id="S3.p1.5.m5.3.3.4.3.cmml" xref="S3.p1.5.m5.3.3.4.3"><times id="S3.p1.5.m5.3.3.4.3.1.cmml" xref="S3.p1.5.m5.3.3.4.3.1"></times><ci id="S3.p1.5.m5.3.3.4.3.2.cmml" xref="S3.p1.5.m5.3.3.4.3.2">𝑠</ci><ci id="S3.p1.5.m5.3.3.4.3.3.cmml" xref="S3.p1.5.m5.3.3.4.3.3">𝑟</ci><ci id="S3.p1.5.m5.3.3.4.3.4.cmml" xref="S3.p1.5.m5.3.3.4.3.4">𝑐</ci></apply></apply><set id="S3.p1.5.m5.3.3.2.3.cmml" xref="S3.p1.5.m5.3.3.2.2"><apply id="S3.p1.5.m5.2.2.1.1.1.cmml" xref="S3.p1.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.2.2.1.1.1.1.cmml" xref="S3.p1.5.m5.2.2.1.1.1">subscript</csymbol><ci id="S3.p1.5.m5.2.2.1.1.1.2.cmml" xref="S3.p1.5.m5.2.2.1.1.1.2">𝑥</ci><cn id="S3.p1.5.m5.2.2.1.1.1.3.cmml" type="integer" xref="S3.p1.5.m5.2.2.1.1.1.3">1</cn></apply><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">…</ci><apply id="S3.p1.5.m5.3.3.2.2.2.cmml" xref="S3.p1.5.m5.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p1.5.m5.3.3.2.2.2.1.cmml" xref="S3.p1.5.m5.3.3.2.2.2">subscript</csymbol><ci id="S3.p1.5.m5.3.3.2.2.2.2.cmml" xref="S3.p1.5.m5.3.3.2.2.2.2">𝑥</ci><ci id="S3.p1.5.m5.3.3.2.2.2.3.cmml" xref="S3.p1.5.m5.3.3.2.2.2.3">𝑀</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.3c">\mathcal{X}_{src}=\left\{x_{1},...,x_{M}\right\}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.3d">caligraphic_X start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT = { italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT }</annotation></semantics></math>, where <math alttext="M" class="ltx_Math" display="inline" id="S3.p1.6.m6.1"><semantics id="S3.p1.6.m6.1a"><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.1d">italic_M</annotation></semantics></math> is the number of text spans in the article.
Entity insertion is the task of selecting the most relevant span <math alttext="x^{*}" class="ltx_Math" display="inline" id="S3.p1.7.m7.1"><semantics id="S3.p1.7.m7.1a"><msup id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml"><mi id="S3.p1.7.m7.1.1.2" xref="S3.p1.7.m7.1.1.2.cmml">x</mi><mo id="S3.p1.7.m7.1.1.3" xref="S3.p1.7.m7.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><apply id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p1.7.m7.1.1.1.cmml" xref="S3.p1.7.m7.1.1">superscript</csymbol><ci id="S3.p1.7.m7.1.1.2.cmml" xref="S3.p1.7.m7.1.1.2">𝑥</ci><times id="S3.p1.7.m7.1.1.3.cmml" xref="S3.p1.7.m7.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">x^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.7.m7.1d">italic_x start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> to insert the target entity <math alttext="E_{\text{tgt}}" class="ltx_Math" display="inline" id="S3.p1.8.m8.1"><semantics id="S3.p1.8.m8.1a"><msub id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml"><mi id="S3.p1.8.m8.1.1.2" xref="S3.p1.8.m8.1.1.2.cmml">E</mi><mtext id="S3.p1.8.m8.1.1.3" xref="S3.p1.8.m8.1.1.3a.cmml">tgt</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><apply id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p1.8.m8.1.1.1.cmml" xref="S3.p1.8.m8.1.1">subscript</csymbol><ci id="S3.p1.8.m8.1.1.2.cmml" xref="S3.p1.8.m8.1.1.2">𝐸</ci><ci id="S3.p1.8.m8.1.1.3a.cmml" xref="S3.p1.8.m8.1.1.3"><mtext id="S3.p1.8.m8.1.1.3.cmml" mathsize="70%" xref="S3.p1.8.m8.1.1.3">tgt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">E_{\text{tgt}}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.8.m8.1d">italic_E start_POSTSUBSCRIPT tgt end_POSTSUBSCRIPT</annotation></semantics></math>. Formally,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{*}=\arg\max_{x\in\mathcal{X}_{src}}\mathcal{R}(x,E_{\text{tgt}})\vspace*{-2mm}" class="ltx_Math" display="block" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><msup id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml"><mi id="S3.E1.m1.2.2.3.2" xref="S3.E1.m1.2.2.3.2.cmml">x</mi><mo id="S3.E1.m1.2.2.3.3" xref="S3.E1.m1.2.2.3.3.cmml">∗</mo></msup><mo id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml"><mrow id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.2.2.1.3.cmml"><mi id="S3.E1.m1.2.2.1.3.1" xref="S3.E1.m1.2.2.1.3.1.cmml">arg</mi><mo id="S3.E1.m1.2.2.1.3a" lspace="0.167em" xref="S3.E1.m1.2.2.1.3.cmml">⁡</mo><mrow id="S3.E1.m1.2.2.1.3.2" xref="S3.E1.m1.2.2.1.3.2.cmml"><munder id="S3.E1.m1.2.2.1.3.2.1" xref="S3.E1.m1.2.2.1.3.2.1.cmml"><mi id="S3.E1.m1.2.2.1.3.2.1.2" xref="S3.E1.m1.2.2.1.3.2.1.2.cmml">max</mi><mrow id="S3.E1.m1.2.2.1.3.2.1.3" xref="S3.E1.m1.2.2.1.3.2.1.3.cmml"><mi id="S3.E1.m1.2.2.1.3.2.1.3.2" xref="S3.E1.m1.2.2.1.3.2.1.3.2.cmml">x</mi><mo id="S3.E1.m1.2.2.1.3.2.1.3.1" xref="S3.E1.m1.2.2.1.3.2.1.3.1.cmml">∈</mo><msub id="S3.E1.m1.2.2.1.3.2.1.3.3" xref="S3.E1.m1.2.2.1.3.2.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.3.2.1.3.3.2" xref="S3.E1.m1.2.2.1.3.2.1.3.3.2.cmml">𝒳</mi><mrow id="S3.E1.m1.2.2.1.3.2.1.3.3.3" xref="S3.E1.m1.2.2.1.3.2.1.3.3.3.cmml"><mi id="S3.E1.m1.2.2.1.3.2.1.3.3.3.2" xref="S3.E1.m1.2.2.1.3.2.1.3.3.3.2.cmml">s</mi><mo id="S3.E1.m1.2.2.1.3.2.1.3.3.3.1" xref="S3.E1.m1.2.2.1.3.2.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.2.2.1.3.2.1.3.3.3.3" xref="S3.E1.m1.2.2.1.3.2.1.3.3.3.3.cmml">r</mi><mo id="S3.E1.m1.2.2.1.3.2.1.3.3.3.1a" xref="S3.E1.m1.2.2.1.3.2.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.2.2.1.3.2.1.3.3.3.4" xref="S3.E1.m1.2.2.1.3.2.1.3.3.3.4.cmml">c</mi></mrow></msub></mrow></munder><mo id="S3.E1.m1.2.2.1.3.2a" lspace="0.167em" xref="S3.E1.m1.2.2.1.3.2.cmml">⁡</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.3.2.2" xref="S3.E1.m1.2.2.1.3.2.2.cmml">ℛ</mi></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.2.cmml"><mo id="S3.E1.m1.2.2.1.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.1.1.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">x</mi><mo id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.2.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">E</mi><mtext id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.3a.cmml">tgt</mtext></msub><mo id="S3.E1.m1.2.2.1.1.1.4" stretchy="false" xref="S3.E1.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"></eq><apply id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.3.1.cmml" xref="S3.E1.m1.2.2.3">superscript</csymbol><ci id="S3.E1.m1.2.2.3.2.cmml" xref="S3.E1.m1.2.2.3.2">𝑥</ci><times id="S3.E1.m1.2.2.3.3.cmml" xref="S3.E1.m1.2.2.3.3"></times></apply><apply id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1"><times id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.2"></times><apply id="S3.E1.m1.2.2.1.3.cmml" xref="S3.E1.m1.2.2.1.3"><arg id="S3.E1.m1.2.2.1.3.1.cmml" xref="S3.E1.m1.2.2.1.3.1"></arg><apply id="S3.E1.m1.2.2.1.3.2.cmml" xref="S3.E1.m1.2.2.1.3.2"><apply id="S3.E1.m1.2.2.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.3.2.1.1.cmml" xref="S3.E1.m1.2.2.1.3.2.1">subscript</csymbol><max id="S3.E1.m1.2.2.1.3.2.1.2.cmml" xref="S3.E1.m1.2.2.1.3.2.1.2"></max><apply id="S3.E1.m1.2.2.1.3.2.1.3.cmml" xref="S3.E1.m1.2.2.1.3.2.1.3"><in id="S3.E1.m1.2.2.1.3.2.1.3.1.cmml" xref="S3.E1.m1.2.2.1.3.2.1.3.1"></in><ci id="S3.E1.m1.2.2.1.3.2.1.3.2.cmml" xref="S3.E1.m1.2.2.1.3.2.1.3.2">𝑥</ci><apply id="S3.E1.m1.2.2.1.3.2.1.3.3.cmml" xref="S3.E1.m1.2.2.1.3.2.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.3.2.1.3.3.1.cmml" xref="S3.E1.m1.2.2.1.3.2.1.3.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.3.2.1.3.3.2.cmml" xref="S3.E1.m1.2.2.1.3.2.1.3.3.2">𝒳</ci><apply id="S3.E1.m1.2.2.1.3.2.1.3.3.3.cmml" xref="S3.E1.m1.2.2.1.3.2.1.3.3.3"><times id="S3.E1.m1.2.2.1.3.2.1.3.3.3.1.cmml" xref="S3.E1.m1.2.2.1.3.2.1.3.3.3.1"></times><ci id="S3.E1.m1.2.2.1.3.2.1.3.3.3.2.cmml" xref="S3.E1.m1.2.2.1.3.2.1.3.3.3.2">𝑠</ci><ci id="S3.E1.m1.2.2.1.3.2.1.3.3.3.3.cmml" xref="S3.E1.m1.2.2.1.3.2.1.3.3.3.3">𝑟</ci><ci id="S3.E1.m1.2.2.1.3.2.1.3.3.3.4.cmml" xref="S3.E1.m1.2.2.1.3.2.1.3.3.3.4">𝑐</ci></apply></apply></apply></apply><ci id="S3.E1.m1.2.2.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.3.2.2">ℛ</ci></apply></apply><interval closure="open" id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑥</ci><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2">𝐸</ci><ci id="S3.E1.m1.2.2.1.1.1.1.3a.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3"><mtext id="S3.E1.m1.2.2.1.1.1.1.3.cmml" mathsize="70%" xref="S3.E1.m1.2.2.1.1.1.1.3">tgt</mtext></ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">x^{*}=\arg\max_{x\in\mathcal{X}_{src}}\mathcal{R}(x,E_{\text{tgt}})\vspace*{-2mm}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">italic_x start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_x ∈ caligraphic_X start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_R ( italic_x , italic_E start_POSTSUBSCRIPT tgt end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p1.12">where <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S3.p1.9.m1.1"><semantics id="S3.p1.9.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.9.m1.1.1" xref="S3.p1.9.m1.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.9.m1.1b"><ci id="S3.p1.9.m1.1.1.cmml" xref="S3.p1.9.m1.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m1.1c">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.9.m1.1d">caligraphic_R</annotation></semantics></math> is an arbitrary relevance function quantifying the relevance of <math alttext="E_{\text{tgt}}" class="ltx_Math" display="inline" id="S3.p1.10.m2.1"><semantics id="S3.p1.10.m2.1a"><msub id="S3.p1.10.m2.1.1" xref="S3.p1.10.m2.1.1.cmml"><mi id="S3.p1.10.m2.1.1.2" xref="S3.p1.10.m2.1.1.2.cmml">E</mi><mtext id="S3.p1.10.m2.1.1.3" xref="S3.p1.10.m2.1.1.3a.cmml">tgt</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p1.10.m2.1b"><apply id="S3.p1.10.m2.1.1.cmml" xref="S3.p1.10.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.10.m2.1.1.1.cmml" xref="S3.p1.10.m2.1.1">subscript</csymbol><ci id="S3.p1.10.m2.1.1.2.cmml" xref="S3.p1.10.m2.1.1.2">𝐸</ci><ci id="S3.p1.10.m2.1.1.3a.cmml" xref="S3.p1.10.m2.1.1.3"><mtext id="S3.p1.10.m2.1.1.3.cmml" mathsize="70%" xref="S3.p1.10.m2.1.1.3">tgt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m2.1c">E_{\text{tgt}}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.10.m2.1d">italic_E start_POSTSUBSCRIPT tgt end_POSTSUBSCRIPT</annotation></semantics></math> to each text span <math alttext="x\in\mathcal{X}_{src}" class="ltx_Math" display="inline" id="S3.p1.11.m3.1"><semantics id="S3.p1.11.m3.1a"><mrow id="S3.p1.11.m3.1.1" xref="S3.p1.11.m3.1.1.cmml"><mi id="S3.p1.11.m3.1.1.2" xref="S3.p1.11.m3.1.1.2.cmml">x</mi><mo id="S3.p1.11.m3.1.1.1" xref="S3.p1.11.m3.1.1.1.cmml">∈</mo><msub id="S3.p1.11.m3.1.1.3" xref="S3.p1.11.m3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.11.m3.1.1.3.2" xref="S3.p1.11.m3.1.1.3.2.cmml">𝒳</mi><mrow id="S3.p1.11.m3.1.1.3.3" xref="S3.p1.11.m3.1.1.3.3.cmml"><mi id="S3.p1.11.m3.1.1.3.3.2" xref="S3.p1.11.m3.1.1.3.3.2.cmml">s</mi><mo id="S3.p1.11.m3.1.1.3.3.1" xref="S3.p1.11.m3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.p1.11.m3.1.1.3.3.3" xref="S3.p1.11.m3.1.1.3.3.3.cmml">r</mi><mo id="S3.p1.11.m3.1.1.3.3.1a" xref="S3.p1.11.m3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.p1.11.m3.1.1.3.3.4" xref="S3.p1.11.m3.1.1.3.3.4.cmml">c</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.11.m3.1b"><apply id="S3.p1.11.m3.1.1.cmml" xref="S3.p1.11.m3.1.1"><in id="S3.p1.11.m3.1.1.1.cmml" xref="S3.p1.11.m3.1.1.1"></in><ci id="S3.p1.11.m3.1.1.2.cmml" xref="S3.p1.11.m3.1.1.2">𝑥</ci><apply id="S3.p1.11.m3.1.1.3.cmml" xref="S3.p1.11.m3.1.1.3"><csymbol cd="ambiguous" id="S3.p1.11.m3.1.1.3.1.cmml" xref="S3.p1.11.m3.1.1.3">subscript</csymbol><ci id="S3.p1.11.m3.1.1.3.2.cmml" xref="S3.p1.11.m3.1.1.3.2">𝒳</ci><apply id="S3.p1.11.m3.1.1.3.3.cmml" xref="S3.p1.11.m3.1.1.3.3"><times id="S3.p1.11.m3.1.1.3.3.1.cmml" xref="S3.p1.11.m3.1.1.3.3.1"></times><ci id="S3.p1.11.m3.1.1.3.3.2.cmml" xref="S3.p1.11.m3.1.1.3.3.2">𝑠</ci><ci id="S3.p1.11.m3.1.1.3.3.3.cmml" xref="S3.p1.11.m3.1.1.3.3.3">𝑟</ci><ci id="S3.p1.11.m3.1.1.3.3.4.cmml" xref="S3.p1.11.m3.1.1.3.3.4">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m3.1c">x\in\mathcal{X}_{src}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.11.m3.1d">italic_x ∈ caligraphic_X start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT</annotation></semantics></math>.
We frame entity insertion as a <em class="ltx_emph ltx_font_italic" id="S3.p1.12.1">ranking task</em>, where the goal is to rank all the candidate text spans <math alttext="\mathcal{X}_{src}" class="ltx_Math" display="inline" id="S3.p1.12.m4.1"><semantics id="S3.p1.12.m4.1a"><msub id="S3.p1.12.m4.1.1" xref="S3.p1.12.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.12.m4.1.1.2" xref="S3.p1.12.m4.1.1.2.cmml">𝒳</mi><mrow id="S3.p1.12.m4.1.1.3" xref="S3.p1.12.m4.1.1.3.cmml"><mi id="S3.p1.12.m4.1.1.3.2" xref="S3.p1.12.m4.1.1.3.2.cmml">s</mi><mo id="S3.p1.12.m4.1.1.3.1" xref="S3.p1.12.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.p1.12.m4.1.1.3.3" xref="S3.p1.12.m4.1.1.3.3.cmml">r</mi><mo id="S3.p1.12.m4.1.1.3.1a" xref="S3.p1.12.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.p1.12.m4.1.1.3.4" xref="S3.p1.12.m4.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.12.m4.1b"><apply id="S3.p1.12.m4.1.1.cmml" xref="S3.p1.12.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.12.m4.1.1.1.cmml" xref="S3.p1.12.m4.1.1">subscript</csymbol><ci id="S3.p1.12.m4.1.1.2.cmml" xref="S3.p1.12.m4.1.1.2">𝒳</ci><apply id="S3.p1.12.m4.1.1.3.cmml" xref="S3.p1.12.m4.1.1.3"><times id="S3.p1.12.m4.1.1.3.1.cmml" xref="S3.p1.12.m4.1.1.3.1"></times><ci id="S3.p1.12.m4.1.1.3.2.cmml" xref="S3.p1.12.m4.1.1.3.2">𝑠</ci><ci id="S3.p1.12.m4.1.1.3.3.cmml" xref="S3.p1.12.m4.1.1.3.3">𝑟</ci><ci id="S3.p1.12.m4.1.1.3.4.cmml" xref="S3.p1.12.m4.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.12.m4.1c">\mathcal{X}_{src}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.12.m4.1d">caligraphic_X start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT</annotation></semantics></math> based on their relevance to the target entity.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Data</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We constructed a new multilingual dataset for studying entity insertion in Wikipedia.
The dataset consists of links extracted from all Wikipedia articles, each link’s surrounding context, and additional article-level meta-data (such as titles, Wikidata QIDs, and lead paragraphs).
Overall, the dataset contains 958M links from 49M articles in 105 languages.
The largest language is English (en), with 166.7M links from 6.7M articles, and the smallest language is Xhosa (xh), with 2.8K links from 1.6K articles
(cf. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2" title="Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">B</span></a> for details).</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S2.F3" title="Figure 3 ‣ 2 Related work ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">3</span></a> provides an overview of our data processing pipeline. The data processing was done in two steps.
We first extracted all the links from the 2023-10-01 snapshot.
Next, we found all the links added in the time between 2023-10-01 and 2023-11-01.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="221" id="S4.F4.g1" src="extracted/5903880/images/model_architecture.png" width="527"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Architectural overview of <span class="ltx_text ltx_font_smallcaps" id="S4.F4.8.1">LocEI</span>. The target entity <math alttext="E_{tgt}" class="ltx_Math" display="inline" id="S4.F4.4.m1.1"><semantics id="S4.F4.4.m1.1b"><msub id="S4.F4.4.m1.1.1" xref="S4.F4.4.m1.1.1.cmml"><mi id="S4.F4.4.m1.1.1.2" xref="S4.F4.4.m1.1.1.2.cmml">E</mi><mrow id="S4.F4.4.m1.1.1.3" xref="S4.F4.4.m1.1.1.3.cmml"><mi id="S4.F4.4.m1.1.1.3.2" xref="S4.F4.4.m1.1.1.3.2.cmml">t</mi><mo id="S4.F4.4.m1.1.1.3.1" xref="S4.F4.4.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.F4.4.m1.1.1.3.3" xref="S4.F4.4.m1.1.1.3.3.cmml">g</mi><mo id="S4.F4.4.m1.1.1.3.1b" xref="S4.F4.4.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.F4.4.m1.1.1.3.4" xref="S4.F4.4.m1.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.F4.4.m1.1c"><apply id="S4.F4.4.m1.1.1.cmml" xref="S4.F4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.F4.4.m1.1.1.1.cmml" xref="S4.F4.4.m1.1.1">subscript</csymbol><ci id="S4.F4.4.m1.1.1.2.cmml" xref="S4.F4.4.m1.1.1.2">𝐸</ci><apply id="S4.F4.4.m1.1.1.3.cmml" xref="S4.F4.4.m1.1.1.3"><times id="S4.F4.4.m1.1.1.3.1.cmml" xref="S4.F4.4.m1.1.1.3.1"></times><ci id="S4.F4.4.m1.1.1.3.2.cmml" xref="S4.F4.4.m1.1.1.3.2">𝑡</ci><ci id="S4.F4.4.m1.1.1.3.3.cmml" xref="S4.F4.4.m1.1.1.3.3">𝑔</ci><ci id="S4.F4.4.m1.1.1.3.4.cmml" xref="S4.F4.4.m1.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.4.m1.1d">E_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S4.F4.4.m1.1e">italic_E start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and each candidate text span <math alttext="x\in\mathcal{X}_{src}" class="ltx_Math" display="inline" id="S4.F4.5.m2.1"><semantics id="S4.F4.5.m2.1b"><mrow id="S4.F4.5.m2.1.1" xref="S4.F4.5.m2.1.1.cmml"><mi id="S4.F4.5.m2.1.1.2" xref="S4.F4.5.m2.1.1.2.cmml">x</mi><mo id="S4.F4.5.m2.1.1.1" xref="S4.F4.5.m2.1.1.1.cmml">∈</mo><msub id="S4.F4.5.m2.1.1.3" xref="S4.F4.5.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.F4.5.m2.1.1.3.2" xref="S4.F4.5.m2.1.1.3.2.cmml">𝒳</mi><mrow id="S4.F4.5.m2.1.1.3.3" xref="S4.F4.5.m2.1.1.3.3.cmml"><mi id="S4.F4.5.m2.1.1.3.3.2" xref="S4.F4.5.m2.1.1.3.3.2.cmml">s</mi><mo id="S4.F4.5.m2.1.1.3.3.1" xref="S4.F4.5.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.3.3.3" xref="S4.F4.5.m2.1.1.3.3.3.cmml">r</mi><mo id="S4.F4.5.m2.1.1.3.3.1b" xref="S4.F4.5.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.3.3.4" xref="S4.F4.5.m2.1.1.3.3.4.cmml">c</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.5.m2.1c"><apply id="S4.F4.5.m2.1.1.cmml" xref="S4.F4.5.m2.1.1"><in id="S4.F4.5.m2.1.1.1.cmml" xref="S4.F4.5.m2.1.1.1"></in><ci id="S4.F4.5.m2.1.1.2.cmml" xref="S4.F4.5.m2.1.1.2">𝑥</ci><apply id="S4.F4.5.m2.1.1.3.cmml" xref="S4.F4.5.m2.1.1.3"><csymbol cd="ambiguous" id="S4.F4.5.m2.1.1.3.1.cmml" xref="S4.F4.5.m2.1.1.3">subscript</csymbol><ci id="S4.F4.5.m2.1.1.3.2.cmml" xref="S4.F4.5.m2.1.1.3.2">𝒳</ci><apply id="S4.F4.5.m2.1.1.3.3.cmml" xref="S4.F4.5.m2.1.1.3.3"><times id="S4.F4.5.m2.1.1.3.3.1.cmml" xref="S4.F4.5.m2.1.1.3.3.1"></times><ci id="S4.F4.5.m2.1.1.3.3.2.cmml" xref="S4.F4.5.m2.1.1.3.3.2">𝑠</ci><ci id="S4.F4.5.m2.1.1.3.3.3.cmml" xref="S4.F4.5.m2.1.1.3.3.3">𝑟</ci><ci id="S4.F4.5.m2.1.1.3.3.4.cmml" xref="S4.F4.5.m2.1.1.3.3.4">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.5.m2.1d">x\in\mathcal{X}_{src}</annotation><annotation encoding="application/x-llamapun" id="S4.F4.5.m2.1e">italic_x ∈ caligraphic_X start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT</annotation></semantics></math> of the source entity <math alttext="E_{src}" class="ltx_Math" display="inline" id="S4.F4.6.m3.1"><semantics id="S4.F4.6.m3.1b"><msub id="S4.F4.6.m3.1.1" xref="S4.F4.6.m3.1.1.cmml"><mi id="S4.F4.6.m3.1.1.2" xref="S4.F4.6.m3.1.1.2.cmml">E</mi><mrow id="S4.F4.6.m3.1.1.3" xref="S4.F4.6.m3.1.1.3.cmml"><mi id="S4.F4.6.m3.1.1.3.2" xref="S4.F4.6.m3.1.1.3.2.cmml">s</mi><mo id="S4.F4.6.m3.1.1.3.1" xref="S4.F4.6.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.3.3" xref="S4.F4.6.m3.1.1.3.3.cmml">r</mi><mo id="S4.F4.6.m3.1.1.3.1b" xref="S4.F4.6.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.3.4" xref="S4.F4.6.m3.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.F4.6.m3.1c"><apply id="S4.F4.6.m3.1.1.cmml" xref="S4.F4.6.m3.1.1"><csymbol cd="ambiguous" id="S4.F4.6.m3.1.1.1.cmml" xref="S4.F4.6.m3.1.1">subscript</csymbol><ci id="S4.F4.6.m3.1.1.2.cmml" xref="S4.F4.6.m3.1.1.2">𝐸</ci><apply id="S4.F4.6.m3.1.1.3.cmml" xref="S4.F4.6.m3.1.1.3"><times id="S4.F4.6.m3.1.1.3.1.cmml" xref="S4.F4.6.m3.1.1.3.1"></times><ci id="S4.F4.6.m3.1.1.3.2.cmml" xref="S4.F4.6.m3.1.1.3.2">𝑠</ci><ci id="S4.F4.6.m3.1.1.3.3.cmml" xref="S4.F4.6.m3.1.1.3.3">𝑟</ci><ci id="S4.F4.6.m3.1.1.3.4.cmml" xref="S4.F4.6.m3.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.6.m3.1d">E_{src}</annotation><annotation encoding="application/x-llamapun" id="S4.F4.6.m3.1e">italic_E start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT</annotation></semantics></math> are concatenated together and encoded jointly using a transformer encoder. The relevance scores of candidate text spans are computed using an MLP trained via a list-wise ranking objective.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Existing links.</span>

We extract the content of all articles from their HTML version using the corresponding snapshot of the Enterprise HTML dumps <cite class="ltx_cite ltx_citemacro_cite">WMF (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib48" title="">2010b</a>)</cite>.
We removed articles without a lead paragraph and a Wikidata QID.
For each article, we consider all internal links in the main article body (ignoring figures, tables, notes, and captions) together with their surrounding context.
We removed all the links where either the source or the target article was one of the removed articles and we dropped all the self-links.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Added links.</span>
We extract the set of added links by comparing existing links in snapshots from consecutive months.
We apply the same procedure as above to each snapshot, respectively, and take the difference of the two sets to identify the links that exist in the second month but not in the first.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">To identify the article version in which the link was added, we go through the articles’ full <em class="ltx_emph ltx_font_italic" id="S4.p5.1.1">revision history</em> available in the Wikimedia XML dumps <cite class="ltx_cite ltx_citemacro_cite">WMF (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib47" title="">2010a</a>)</cite>.
Next, we identify the two versions of an article before and after the link addition and download the corresponding HTML.
Comparing the two HTML versions, we extract the content modifications made by the editor when adding the link, and categorize them into <em class="ltx_emph ltx_font_italic" id="S4.p5.1.2">five entity insertion scenarios</em>.
(1) <span class="ltx_text ltx_font_typewriter" id="S4.p5.1.3">text_present</span>: the link was added by hyperlinking an existing mention; (2) <span class="ltx_text ltx_font_typewriter" id="S4.p5.1.4">missing_mention</span>: the link was added by adding the mention for a new entity (and potentially some additional content) into an existent sentence; (3) <span class="ltx_text ltx_font_typewriter" id="S4.p5.1.5">missing_sentence</span>: the link was added by writing a new sentence to complement the already existing text and hyperlinking part of the sentence at the same time; (4) <span class="ltx_text ltx_font_typewriter" id="S4.p5.1.6">missing_span</span>: an extension of the previous category, where the editors added a span of multiple sentences; and (5) <span class="ltx_text ltx_font_typewriter" id="S4.p5.1.7">missing_section</span>: the link was added in a section that did not exist in the previous version of the article.
We provide examples (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.T9" title="Table 9 ‣ B.3 Entity insertion categories ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">9</span></a>) and frequency of occurrence (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.F5" title="Figure 5 ‣ B.3 Entity insertion categories ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5</span></a>) of these cases in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.SS3" title="B.3 Entity insertion categories ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">B.3</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.p6.1.1">Data release.</span> The dataset is made publicly available on Zenodo under an open license (CC-BY-SA 4.0) at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/13888211" title="">https://zenodo.org/records/13888211</a>.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Entity insertion with <span class="ltx_text ltx_font_smallcaps" id="S5.1.1">LocEI</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4.F4" title="Figure 4 ‣ 4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a> presents an overview of <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.1">LocEI</span>. Our model (§ <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS1" title="5.1 Model ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5.1</span></a>) is composed of a transformer-based encoder that jointly encodes the target entity as well as the candidate spans in the source entity, and a multilayer perceptrion (MLP) trained via a list-wise objective capable of ranking candidates based on their relevance to the target. We introduce a novel data augmentation strategy that closely mimics real-world entity insertion scenarios (§ <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS2" title="5.2 Two-stage training pipeline ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5.2</span></a>), a knowledge injection module to incorporate external knowledge about the entities (§ <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS3" title="5.3 Knowledge injection ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5.3</span></a>), and the multilingual variant <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.2">xLocEI</span> (§ <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS4" title="5.4 Incorporating multilinguality (xLocEI) ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5.4</span></a>).</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Model</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.9"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.9.1">Architecture.</span> We use a transformer-based encoder <math alttext="\Gamma" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" mathvariant="normal" xref="S5.SS1.p1.1.m1.1.1.cmml">Γ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">Γ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\Gamma</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">roman_Γ</annotation></semantics></math> to jointly encode each candidate text span <math alttext="x\in\mathcal{X}_{src}" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">x</mi><mo id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">∈</mo><msub id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p1.2.m2.1.1.3.2" xref="S5.SS1.p1.2.m2.1.1.3.2.cmml">𝒳</mi><mrow id="S5.SS1.p1.2.m2.1.1.3.3" xref="S5.SS1.p1.2.m2.1.1.3.3.cmml"><mi id="S5.SS1.p1.2.m2.1.1.3.3.2" xref="S5.SS1.p1.2.m2.1.1.3.3.2.cmml">s</mi><mo id="S5.SS1.p1.2.m2.1.1.3.3.1" xref="S5.SS1.p1.2.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.2.m2.1.1.3.3.3" xref="S5.SS1.p1.2.m2.1.1.3.3.3.cmml">r</mi><mo id="S5.SS1.p1.2.m2.1.1.3.3.1a" xref="S5.SS1.p1.2.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.2.m2.1.1.3.3.4" xref="S5.SS1.p1.2.m2.1.1.3.3.4.cmml">c</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><in id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1"></in><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">𝑥</ci><apply id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p1.2.m2.1.1.3.1.cmml" xref="S5.SS1.p1.2.m2.1.1.3">subscript</csymbol><ci id="S5.SS1.p1.2.m2.1.1.3.2.cmml" xref="S5.SS1.p1.2.m2.1.1.3.2">𝒳</ci><apply id="S5.SS1.p1.2.m2.1.1.3.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3"><times id="S5.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3.1"></times><ci id="S5.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3.2">𝑠</ci><ci id="S5.SS1.p1.2.m2.1.1.3.3.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3.3">𝑟</ci><ci id="S5.SS1.p1.2.m2.1.1.3.3.4.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3.4">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">x\in\mathcal{X}_{src}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">italic_x ∈ caligraphic_X start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT</annotation></semantics></math> and the target entity <math alttext="E_{tgt}" class="ltx_Math" display="inline" id="S5.SS1.p1.3.m3.1"><semantics id="S5.SS1.p1.3.m3.1a"><msub id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><mi id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2.cmml">E</mi><mrow id="S5.SS1.p1.3.m3.1.1.3" xref="S5.SS1.p1.3.m3.1.1.3.cmml"><mi id="S5.SS1.p1.3.m3.1.1.3.2" xref="S5.SS1.p1.3.m3.1.1.3.2.cmml">t</mi><mo id="S5.SS1.p1.3.m3.1.1.3.1" xref="S5.SS1.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.3.m3.1.1.3.3" xref="S5.SS1.p1.3.m3.1.1.3.3.cmml">g</mi><mo id="S5.SS1.p1.3.m3.1.1.3.1a" xref="S5.SS1.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.3.m3.1.1.3.4" xref="S5.SS1.p1.3.m3.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2">𝐸</ci><apply id="S5.SS1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3"><times id="S5.SS1.p1.3.m3.1.1.3.1.cmml" xref="S5.SS1.p1.3.m3.1.1.3.1"></times><ci id="S5.SS1.p1.3.m3.1.1.3.2.cmml" xref="S5.SS1.p1.3.m3.1.1.3.2">𝑡</ci><ci id="S5.SS1.p1.3.m3.1.1.3.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3.3">𝑔</ci><ci id="S5.SS1.p1.3.m3.1.1.3.4.cmml" xref="S5.SS1.p1.3.m3.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">E_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.3.m3.1d">italic_E start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> into a sequence of vectors. To reduce this sequence into a single vector, we use the embedding of the <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.9.2">[CLS]</span> token <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib9" title="">2019</a>)</cite>, which measures how related the candidate <math alttext="x" class="ltx_Math" display="inline" id="S5.SS1.p1.4.m4.1"><semantics id="S5.SS1.p1.4.m4.1a"><mi id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><ci id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.4.m4.1d">italic_x</annotation></semantics></math> is to the entity <math alttext="E_{tgt}" class="ltx_Math" display="inline" id="S5.SS1.p1.5.m5.1"><semantics id="S5.SS1.p1.5.m5.1a"><msub id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml"><mi id="S5.SS1.p1.5.m5.1.1.2" xref="S5.SS1.p1.5.m5.1.1.2.cmml">E</mi><mrow id="S5.SS1.p1.5.m5.1.1.3" xref="S5.SS1.p1.5.m5.1.1.3.cmml"><mi id="S5.SS1.p1.5.m5.1.1.3.2" xref="S5.SS1.p1.5.m5.1.1.3.2.cmml">t</mi><mo id="S5.SS1.p1.5.m5.1.1.3.1" xref="S5.SS1.p1.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.5.m5.1.1.3.3" xref="S5.SS1.p1.5.m5.1.1.3.3.cmml">g</mi><mo id="S5.SS1.p1.5.m5.1.1.3.1a" xref="S5.SS1.p1.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.5.m5.1.1.3.4" xref="S5.SS1.p1.5.m5.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.1b"><apply id="S5.SS1.p1.5.m5.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.5.m5.1.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S5.SS1.p1.5.m5.1.1.2.cmml" xref="S5.SS1.p1.5.m5.1.1.2">𝐸</ci><apply id="S5.SS1.p1.5.m5.1.1.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3"><times id="S5.SS1.p1.5.m5.1.1.3.1.cmml" xref="S5.SS1.p1.5.m5.1.1.3.1"></times><ci id="S5.SS1.p1.5.m5.1.1.3.2.cmml" xref="S5.SS1.p1.5.m5.1.1.3.2">𝑡</ci><ci id="S5.SS1.p1.5.m5.1.1.3.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3">𝑔</ci><ci id="S5.SS1.p1.5.m5.1.1.3.4.cmml" xref="S5.SS1.p1.5.m5.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.1c">E_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.5.m5.1d">italic_E start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. An MLP <math alttext="\Lambda" class="ltx_Math" display="inline" id="S5.SS1.p1.6.m6.1"><semantics id="S5.SS1.p1.6.m6.1a"><mi id="S5.SS1.p1.6.m6.1.1" mathvariant="normal" xref="S5.SS1.p1.6.m6.1.1.cmml">Λ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.6.m6.1b"><ci id="S5.SS1.p1.6.m6.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1">Λ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.6.m6.1c">\Lambda</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.6.m6.1d">roman_Λ</annotation></semantics></math> produces a scalar relevance score between the candidate <math alttext="x" class="ltx_Math" display="inline" id="S5.SS1.p1.7.m7.1"><semantics id="S5.SS1.p1.7.m7.1a"><mi id="S5.SS1.p1.7.m7.1.1" xref="S5.SS1.p1.7.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.7.m7.1b"><ci id="S5.SS1.p1.7.m7.1.1.cmml" xref="S5.SS1.p1.7.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.7.m7.1c">x</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.7.m7.1d">italic_x</annotation></semantics></math> and the entity <math alttext="E_{tgt}" class="ltx_Math" display="inline" id="S5.SS1.p1.8.m8.1"><semantics id="S5.SS1.p1.8.m8.1a"><msub id="S5.SS1.p1.8.m8.1.1" xref="S5.SS1.p1.8.m8.1.1.cmml"><mi id="S5.SS1.p1.8.m8.1.1.2" xref="S5.SS1.p1.8.m8.1.1.2.cmml">E</mi><mrow id="S5.SS1.p1.8.m8.1.1.3" xref="S5.SS1.p1.8.m8.1.1.3.cmml"><mi id="S5.SS1.p1.8.m8.1.1.3.2" xref="S5.SS1.p1.8.m8.1.1.3.2.cmml">t</mi><mo id="S5.SS1.p1.8.m8.1.1.3.1" xref="S5.SS1.p1.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.8.m8.1.1.3.3" xref="S5.SS1.p1.8.m8.1.1.3.3.cmml">g</mi><mo id="S5.SS1.p1.8.m8.1.1.3.1a" xref="S5.SS1.p1.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.8.m8.1.1.3.4" xref="S5.SS1.p1.8.m8.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.8.m8.1b"><apply id="S5.SS1.p1.8.m8.1.1.cmml" xref="S5.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.8.m8.1.1.1.cmml" xref="S5.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S5.SS1.p1.8.m8.1.1.2.cmml" xref="S5.SS1.p1.8.m8.1.1.2">𝐸</ci><apply id="S5.SS1.p1.8.m8.1.1.3.cmml" xref="S5.SS1.p1.8.m8.1.1.3"><times id="S5.SS1.p1.8.m8.1.1.3.1.cmml" xref="S5.SS1.p1.8.m8.1.1.3.1"></times><ci id="S5.SS1.p1.8.m8.1.1.3.2.cmml" xref="S5.SS1.p1.8.m8.1.1.3.2">𝑡</ci><ci id="S5.SS1.p1.8.m8.1.1.3.3.cmml" xref="S5.SS1.p1.8.m8.1.1.3.3">𝑔</ci><ci id="S5.SS1.p1.8.m8.1.1.3.4.cmml" xref="S5.SS1.p1.8.m8.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.8.m8.1c">E_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.8.m8.1d">italic_E start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> using the relevance embedding produced by the encoder <math alttext="\Gamma" class="ltx_Math" display="inline" id="S5.SS1.p1.9.m9.1"><semantics id="S5.SS1.p1.9.m9.1a"><mi id="S5.SS1.p1.9.m9.1.1" mathvariant="normal" xref="S5.SS1.p1.9.m9.1.1.cmml">Γ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.9.m9.1b"><ci id="S5.SS1.p1.9.m9.1.1.cmml" xref="S5.SS1.p1.9.m9.1.1">Γ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.9.m9.1c">\Gamma</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.9.m9.1d">roman_Γ</annotation></semantics></math> defined as</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\vspace*{-1mm}R=\Gamma\left(\phi;\theta_{\Gamma}\right)\vspace*{-2mm}" class="ltx_Math" display="block" id="S5.E2.m1.2"><semantics id="S5.E2.m1.2a"><mrow id="S5.E2.m1.2.2" xref="S5.E2.m1.2.2.cmml"><mi id="S5.E2.m1.2.2.3" xref="S5.E2.m1.2.2.3.cmml">R</mi><mo id="S5.E2.m1.2.2.2" xref="S5.E2.m1.2.2.2.cmml">=</mo><mrow id="S5.E2.m1.2.2.1" xref="S5.E2.m1.2.2.1.cmml"><mi id="S5.E2.m1.2.2.1.3" mathvariant="normal" xref="S5.E2.m1.2.2.1.3.cmml">Γ</mi><mo id="S5.E2.m1.2.2.1.2" xref="S5.E2.m1.2.2.1.2.cmml">⁢</mo><mrow id="S5.E2.m1.2.2.1.1.1" xref="S5.E2.m1.2.2.1.1.2.cmml"><mo id="S5.E2.m1.2.2.1.1.1.2" xref="S5.E2.m1.2.2.1.1.2.cmml">(</mo><mi id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml">ϕ</mi><mo id="S5.E2.m1.2.2.1.1.1.3" xref="S5.E2.m1.2.2.1.1.2.cmml">;</mo><msub id="S5.E2.m1.2.2.1.1.1.1" xref="S5.E2.m1.2.2.1.1.1.1.cmml"><mi id="S5.E2.m1.2.2.1.1.1.1.2" xref="S5.E2.m1.2.2.1.1.1.1.2.cmml">θ</mi><mi id="S5.E2.m1.2.2.1.1.1.1.3" mathvariant="normal" xref="S5.E2.m1.2.2.1.1.1.1.3.cmml">Γ</mi></msub><mo id="S5.E2.m1.2.2.1.1.1.4" xref="S5.E2.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.2b"><apply id="S5.E2.m1.2.2.cmml" xref="S5.E2.m1.2.2"><eq id="S5.E2.m1.2.2.2.cmml" xref="S5.E2.m1.2.2.2"></eq><ci id="S5.E2.m1.2.2.3.cmml" xref="S5.E2.m1.2.2.3">𝑅</ci><apply id="S5.E2.m1.2.2.1.cmml" xref="S5.E2.m1.2.2.1"><times id="S5.E2.m1.2.2.1.2.cmml" xref="S5.E2.m1.2.2.1.2"></times><ci id="S5.E2.m1.2.2.1.3.cmml" xref="S5.E2.m1.2.2.1.3">Γ</ci><list id="S5.E2.m1.2.2.1.1.2.cmml" xref="S5.E2.m1.2.2.1.1.1"><ci id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1">italic-ϕ</ci><apply id="S5.E2.m1.2.2.1.1.1.1.cmml" xref="S5.E2.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.E2.m1.2.2.1.1.1.1.1.cmml" xref="S5.E2.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S5.E2.m1.2.2.1.1.1.1.2.cmml" xref="S5.E2.m1.2.2.1.1.1.1.2">𝜃</ci><ci id="S5.E2.m1.2.2.1.1.1.1.3.cmml" xref="S5.E2.m1.2.2.1.1.1.1.3">Γ</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.2c">\vspace*{-1mm}R=\Gamma\left(\phi;\theta_{\Gamma}\right)\vspace*{-2mm}</annotation><annotation encoding="application/x-llamapun" id="S5.E2.m1.2d">italic_R = roman_Γ ( italic_ϕ ; italic_θ start_POSTSUBSCRIPT roman_Γ end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S5.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\vspace*{-1mm}r=\Lambda\left(R_{\texttt{[CLS]}};\theta_{\Lambda}\right)" class="ltx_Math" display="block" id="S5.E3.m1.2"><semantics id="S5.E3.m1.2a"><mrow id="S5.E3.m1.2.2" xref="S5.E3.m1.2.2.cmml"><mi id="S5.E3.m1.2.2.4" xref="S5.E3.m1.2.2.4.cmml">r</mi><mo id="S5.E3.m1.2.2.3" xref="S5.E3.m1.2.2.3.cmml">=</mo><mrow id="S5.E3.m1.2.2.2" xref="S5.E3.m1.2.2.2.cmml"><mi id="S5.E3.m1.2.2.2.4" mathvariant="normal" xref="S5.E3.m1.2.2.2.4.cmml">Λ</mi><mo id="S5.E3.m1.2.2.2.3" xref="S5.E3.m1.2.2.2.3.cmml">⁢</mo><mrow id="S5.E3.m1.2.2.2.2.2" xref="S5.E3.m1.2.2.2.2.3.cmml"><mo id="S5.E3.m1.2.2.2.2.2.3" xref="S5.E3.m1.2.2.2.2.3.cmml">(</mo><msub id="S5.E3.m1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.2.cmml">R</mi><mtext class="ltx_mathvariant_monospace" id="S5.E3.m1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.3a.cmml">[CLS]</mtext></msub><mo id="S5.E3.m1.2.2.2.2.2.4" xref="S5.E3.m1.2.2.2.2.3.cmml">;</mo><msub id="S5.E3.m1.2.2.2.2.2.2" xref="S5.E3.m1.2.2.2.2.2.2.cmml"><mi id="S5.E3.m1.2.2.2.2.2.2.2" xref="S5.E3.m1.2.2.2.2.2.2.2.cmml">θ</mi><mi id="S5.E3.m1.2.2.2.2.2.2.3" mathvariant="normal" xref="S5.E3.m1.2.2.2.2.2.2.3.cmml">Λ</mi></msub><mo id="S5.E3.m1.2.2.2.2.2.5" xref="S5.E3.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.2b"><apply id="S5.E3.m1.2.2.cmml" xref="S5.E3.m1.2.2"><eq id="S5.E3.m1.2.2.3.cmml" xref="S5.E3.m1.2.2.3"></eq><ci id="S5.E3.m1.2.2.4.cmml" xref="S5.E3.m1.2.2.4">𝑟</ci><apply id="S5.E3.m1.2.2.2.cmml" xref="S5.E3.m1.2.2.2"><times id="S5.E3.m1.2.2.2.3.cmml" xref="S5.E3.m1.2.2.2.3"></times><ci id="S5.E3.m1.2.2.2.4.cmml" xref="S5.E3.m1.2.2.2.4">Λ</ci><list id="S5.E3.m1.2.2.2.2.3.cmml" xref="S5.E3.m1.2.2.2.2.2"><apply id="S5.E3.m1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E3.m1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2">𝑅</ci><ci id="S5.E3.m1.1.1.1.1.1.1.3a.cmml" xref="S5.E3.m1.1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S5.E3.m1.1.1.1.1.1.1.3.cmml" mathsize="70%" xref="S5.E3.m1.1.1.1.1.1.1.3">[CLS]</mtext></ci></apply><apply id="S5.E3.m1.2.2.2.2.2.2.cmml" xref="S5.E3.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.2.2.2.2.1.cmml" xref="S5.E3.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S5.E3.m1.2.2.2.2.2.2.2.cmml" xref="S5.E3.m1.2.2.2.2.2.2.2">𝜃</ci><ci id="S5.E3.m1.2.2.2.2.2.2.3.cmml" xref="S5.E3.m1.2.2.2.2.2.2.3">Λ</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.2c">\vspace*{-1mm}r=\Lambda\left(R_{\texttt{[CLS]}};\theta_{\Lambda}\right)</annotation><annotation encoding="application/x-llamapun" id="S5.E3.m1.2d">italic_r = roman_Λ ( italic_R start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT ; italic_θ start_POSTSUBSCRIPT roman_Λ end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.p1.22">where <math alttext="\Gamma" class="ltx_Math" display="inline" id="S5.SS1.p1.10.m1.1"><semantics id="S5.SS1.p1.10.m1.1a"><mi id="S5.SS1.p1.10.m1.1.1" mathvariant="normal" xref="S5.SS1.p1.10.m1.1.1.cmml">Γ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.10.m1.1b"><ci id="S5.SS1.p1.10.m1.1.1.cmml" xref="S5.SS1.p1.10.m1.1.1">Γ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.10.m1.1c">\Gamma</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.10.m1.1d">roman_Γ</annotation></semantics></math> is an encoder and <math alttext="\Lambda" class="ltx_Math" display="inline" id="S5.SS1.p1.11.m2.1"><semantics id="S5.SS1.p1.11.m2.1a"><mi id="S5.SS1.p1.11.m2.1.1" mathvariant="normal" xref="S5.SS1.p1.11.m2.1.1.cmml">Λ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.11.m2.1b"><ci id="S5.SS1.p1.11.m2.1.1.cmml" xref="S5.SS1.p1.11.m2.1.1">Λ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.11.m2.1c">\Lambda</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.11.m2.1d">roman_Λ</annotation></semantics></math> is an MLP with <math alttext="\theta_{\Gamma}" class="ltx_Math" display="inline" id="S5.SS1.p1.12.m3.1"><semantics id="S5.SS1.p1.12.m3.1a"><msub id="S5.SS1.p1.12.m3.1.1" xref="S5.SS1.p1.12.m3.1.1.cmml"><mi id="S5.SS1.p1.12.m3.1.1.2" xref="S5.SS1.p1.12.m3.1.1.2.cmml">θ</mi><mi id="S5.SS1.p1.12.m3.1.1.3" mathvariant="normal" xref="S5.SS1.p1.12.m3.1.1.3.cmml">Γ</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.12.m3.1b"><apply id="S5.SS1.p1.12.m3.1.1.cmml" xref="S5.SS1.p1.12.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.12.m3.1.1.1.cmml" xref="S5.SS1.p1.12.m3.1.1">subscript</csymbol><ci id="S5.SS1.p1.12.m3.1.1.2.cmml" xref="S5.SS1.p1.12.m3.1.1.2">𝜃</ci><ci id="S5.SS1.p1.12.m3.1.1.3.cmml" xref="S5.SS1.p1.12.m3.1.1.3">Γ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.12.m3.1c">\theta_{\Gamma}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.12.m3.1d">italic_θ start_POSTSUBSCRIPT roman_Γ end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\theta_{\Lambda}" class="ltx_Math" display="inline" id="S5.SS1.p1.13.m4.1"><semantics id="S5.SS1.p1.13.m4.1a"><msub id="S5.SS1.p1.13.m4.1.1" xref="S5.SS1.p1.13.m4.1.1.cmml"><mi id="S5.SS1.p1.13.m4.1.1.2" xref="S5.SS1.p1.13.m4.1.1.2.cmml">θ</mi><mi id="S5.SS1.p1.13.m4.1.1.3" mathvariant="normal" xref="S5.SS1.p1.13.m4.1.1.3.cmml">Λ</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.13.m4.1b"><apply id="S5.SS1.p1.13.m4.1.1.cmml" xref="S5.SS1.p1.13.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.13.m4.1.1.1.cmml" xref="S5.SS1.p1.13.m4.1.1">subscript</csymbol><ci id="S5.SS1.p1.13.m4.1.1.2.cmml" xref="S5.SS1.p1.13.m4.1.1.2">𝜃</ci><ci id="S5.SS1.p1.13.m4.1.1.3.cmml" xref="S5.SS1.p1.13.m4.1.1.3">Λ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.13.m4.1c">\theta_{\Lambda}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.13.m4.1d">italic_θ start_POSTSUBSCRIPT roman_Λ end_POSTSUBSCRIPT</annotation></semantics></math> as the learnable parameter spaces, respectively, <math alttext="\phi" class="ltx_Math" display="inline" id="S5.SS1.p1.14.m5.1"><semantics id="S5.SS1.p1.14.m5.1a"><mi id="S5.SS1.p1.14.m5.1.1" xref="S5.SS1.p1.14.m5.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.14.m5.1b"><ci id="S5.SS1.p1.14.m5.1.1.cmml" xref="S5.SS1.p1.14.m5.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.14.m5.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.14.m5.1d">italic_ϕ</annotation></semantics></math> is obtained by concatenating the input representations of <math alttext="E_{tgt}" class="ltx_Math" display="inline" id="S5.SS1.p1.15.m6.1"><semantics id="S5.SS1.p1.15.m6.1a"><msub id="S5.SS1.p1.15.m6.1.1" xref="S5.SS1.p1.15.m6.1.1.cmml"><mi id="S5.SS1.p1.15.m6.1.1.2" xref="S5.SS1.p1.15.m6.1.1.2.cmml">E</mi><mrow id="S5.SS1.p1.15.m6.1.1.3" xref="S5.SS1.p1.15.m6.1.1.3.cmml"><mi id="S5.SS1.p1.15.m6.1.1.3.2" xref="S5.SS1.p1.15.m6.1.1.3.2.cmml">t</mi><mo id="S5.SS1.p1.15.m6.1.1.3.1" xref="S5.SS1.p1.15.m6.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.15.m6.1.1.3.3" xref="S5.SS1.p1.15.m6.1.1.3.3.cmml">g</mi><mo id="S5.SS1.p1.15.m6.1.1.3.1a" xref="S5.SS1.p1.15.m6.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p1.15.m6.1.1.3.4" xref="S5.SS1.p1.15.m6.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.15.m6.1b"><apply id="S5.SS1.p1.15.m6.1.1.cmml" xref="S5.SS1.p1.15.m6.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.15.m6.1.1.1.cmml" xref="S5.SS1.p1.15.m6.1.1">subscript</csymbol><ci id="S5.SS1.p1.15.m6.1.1.2.cmml" xref="S5.SS1.p1.15.m6.1.1.2">𝐸</ci><apply id="S5.SS1.p1.15.m6.1.1.3.cmml" xref="S5.SS1.p1.15.m6.1.1.3"><times id="S5.SS1.p1.15.m6.1.1.3.1.cmml" xref="S5.SS1.p1.15.m6.1.1.3.1"></times><ci id="S5.SS1.p1.15.m6.1.1.3.2.cmml" xref="S5.SS1.p1.15.m6.1.1.3.2">𝑡</ci><ci id="S5.SS1.p1.15.m6.1.1.3.3.cmml" xref="S5.SS1.p1.15.m6.1.1.3.3">𝑔</ci><ci id="S5.SS1.p1.15.m6.1.1.3.4.cmml" xref="S5.SS1.p1.15.m6.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.15.m6.1c">E_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.15.m6.1d">italic_E start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="x" class="ltx_Math" display="inline" id="S5.SS1.p1.16.m7.1"><semantics id="S5.SS1.p1.16.m7.1a"><mi id="S5.SS1.p1.16.m7.1.1" xref="S5.SS1.p1.16.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.16.m7.1b"><ci id="S5.SS1.p1.16.m7.1.1.cmml" xref="S5.SS1.p1.16.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.16.m7.1c">x</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.16.m7.1d">italic_x</annotation></semantics></math>, <math alttext="R" class="ltx_Math" display="inline" id="S5.SS1.p1.17.m8.1"><semantics id="S5.SS1.p1.17.m8.1a"><mi id="S5.SS1.p1.17.m8.1.1" xref="S5.SS1.p1.17.m8.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.17.m8.1b"><ci id="S5.SS1.p1.17.m8.1.1.cmml" xref="S5.SS1.p1.17.m8.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.17.m8.1c">R</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.17.m8.1d">italic_R</annotation></semantics></math> is the sequence of d-dimensional contextualized embeddings produced by <math alttext="\Gamma" class="ltx_Math" display="inline" id="S5.SS1.p1.18.m9.1"><semantics id="S5.SS1.p1.18.m9.1a"><mi id="S5.SS1.p1.18.m9.1.1" mathvariant="normal" xref="S5.SS1.p1.18.m9.1.1.cmml">Γ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.18.m9.1b"><ci id="S5.SS1.p1.18.m9.1.1.cmml" xref="S5.SS1.p1.18.m9.1.1">Γ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.18.m9.1c">\Gamma</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.18.m9.1d">roman_Γ</annotation></semantics></math>, <math alttext="R_{\texttt{[CLS]}}" class="ltx_Math" display="inline" id="S5.SS1.p1.19.m10.1"><semantics id="S5.SS1.p1.19.m10.1a"><msub id="S5.SS1.p1.19.m10.1.1" xref="S5.SS1.p1.19.m10.1.1.cmml"><mi id="S5.SS1.p1.19.m10.1.1.2" xref="S5.SS1.p1.19.m10.1.1.2.cmml">R</mi><mtext class="ltx_mathvariant_monospace" id="S5.SS1.p1.19.m10.1.1.3" xref="S5.SS1.p1.19.m10.1.1.3a.cmml">[CLS]</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.19.m10.1b"><apply id="S5.SS1.p1.19.m10.1.1.cmml" xref="S5.SS1.p1.19.m10.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.19.m10.1.1.1.cmml" xref="S5.SS1.p1.19.m10.1.1">subscript</csymbol><ci id="S5.SS1.p1.19.m10.1.1.2.cmml" xref="S5.SS1.p1.19.m10.1.1.2">𝑅</ci><ci id="S5.SS1.p1.19.m10.1.1.3a.cmml" xref="S5.SS1.p1.19.m10.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S5.SS1.p1.19.m10.1.1.3.cmml" mathsize="70%" xref="S5.SS1.p1.19.m10.1.1.3">[CLS]</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.19.m10.1c">R_{\texttt{[CLS]}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.19.m10.1d">italic_R start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT</annotation></semantics></math> is the <math alttext="d" class="ltx_Math" display="inline" id="S5.SS1.p1.20.m11.1"><semantics id="S5.SS1.p1.20.m11.1a"><mi id="S5.SS1.p1.20.m11.1.1" xref="S5.SS1.p1.20.m11.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.20.m11.1b"><ci id="S5.SS1.p1.20.m11.1.1.cmml" xref="S5.SS1.p1.20.m11.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.20.m11.1c">d</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.20.m11.1d">italic_d</annotation></semantics></math>-dimensional relevance embedding, and <math alttext="r" class="ltx_Math" display="inline" id="S5.SS1.p1.21.m12.1"><semantics id="S5.SS1.p1.21.m12.1a"><mi id="S5.SS1.p1.21.m12.1.1" xref="S5.SS1.p1.21.m12.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.21.m12.1b"><ci id="S5.SS1.p1.21.m12.1.1.cmml" xref="S5.SS1.p1.21.m12.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.21.m12.1c">r</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.21.m12.1d">italic_r</annotation></semantics></math> is the relevance scalar produced by <math alttext="\Lambda" class="ltx_Math" display="inline" id="S5.SS1.p1.22.m13.1"><semantics id="S5.SS1.p1.22.m13.1a"><mi id="S5.SS1.p1.22.m13.1.1" mathvariant="normal" xref="S5.SS1.p1.22.m13.1.1.cmml">Λ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.22.m13.1b"><ci id="S5.SS1.p1.22.m13.1.1.cmml" xref="S5.SS1.p1.22.m13.1.1">Λ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.22.m13.1c">\Lambda</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.22.m13.1d">roman_Λ</annotation></semantics></math> to rank the candidates.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.7"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.7.1">Entity and candidate span modeling.</span>
We represent the target entity <math alttext="E_{tgt}" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><msub id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><mi id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml">E</mi><mrow id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml"><mi id="S5.SS1.p2.1.m1.1.1.3.2" xref="S5.SS1.p2.1.m1.1.1.3.2.cmml">t</mi><mo id="S5.SS1.p2.1.m1.1.1.3.1" xref="S5.SS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p2.1.m1.1.1.3.3" xref="S5.SS1.p2.1.m1.1.1.3.3.cmml">g</mi><mo id="S5.SS1.p2.1.m1.1.1.3.1a" xref="S5.SS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p2.1.m1.1.1.3.4" xref="S5.SS1.p2.1.m1.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2">𝐸</ci><apply id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3"><times id="S5.SS1.p2.1.m1.1.1.3.1.cmml" xref="S5.SS1.p2.1.m1.1.1.3.1"></times><ci id="S5.SS1.p2.1.m1.1.1.3.2.cmml" xref="S5.SS1.p2.1.m1.1.1.3.2">𝑡</ci><ci id="S5.SS1.p2.1.m1.1.1.3.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3.3">𝑔</ci><ci id="S5.SS1.p2.1.m1.1.1.3.4.cmml" xref="S5.SS1.p2.1.m1.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">E_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">italic_E start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> via two textual features, the title <math alttext="T_{tgt}" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><msub id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><mi id="S5.SS1.p2.2.m2.1.1.2" xref="S5.SS1.p2.2.m2.1.1.2.cmml">T</mi><mrow id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml"><mi id="S5.SS1.p2.2.m2.1.1.3.2" xref="S5.SS1.p2.2.m2.1.1.3.2.cmml">t</mi><mo id="S5.SS1.p2.2.m2.1.1.3.1" xref="S5.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p2.2.m2.1.1.3.3" xref="S5.SS1.p2.2.m2.1.1.3.3.cmml">g</mi><mo id="S5.SS1.p2.2.m2.1.1.3.1a" xref="S5.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p2.2.m2.1.1.3.4" xref="S5.SS1.p2.2.m2.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2">𝑇</ci><apply id="S5.SS1.p2.2.m2.1.1.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3"><times id="S5.SS1.p2.2.m2.1.1.3.1.cmml" xref="S5.SS1.p2.2.m2.1.1.3.1"></times><ci id="S5.SS1.p2.2.m2.1.1.3.2.cmml" xref="S5.SS1.p2.2.m2.1.1.3.2">𝑡</ci><ci id="S5.SS1.p2.2.m2.1.1.3.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3">𝑔</ci><ci id="S5.SS1.p2.2.m2.1.1.3.4.cmml" xref="S5.SS1.p2.2.m2.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">T_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">italic_T start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and the lead text <math alttext="L_{tgt}" class="ltx_Math" display="inline" id="S5.SS1.p2.3.m3.1"><semantics id="S5.SS1.p2.3.m3.1a"><msub id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml"><mi id="S5.SS1.p2.3.m3.1.1.2" xref="S5.SS1.p2.3.m3.1.1.2.cmml">L</mi><mrow id="S5.SS1.p2.3.m3.1.1.3" xref="S5.SS1.p2.3.m3.1.1.3.cmml"><mi id="S5.SS1.p2.3.m3.1.1.3.2" xref="S5.SS1.p2.3.m3.1.1.3.2.cmml">t</mi><mo id="S5.SS1.p2.3.m3.1.1.3.1" xref="S5.SS1.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p2.3.m3.1.1.3.3" xref="S5.SS1.p2.3.m3.1.1.3.3.cmml">g</mi><mo id="S5.SS1.p2.3.m3.1.1.3.1a" xref="S5.SS1.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p2.3.m3.1.1.3.4" xref="S5.SS1.p2.3.m3.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2">𝐿</ci><apply id="S5.SS1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3"><times id="S5.SS1.p2.3.m3.1.1.3.1.cmml" xref="S5.SS1.p2.3.m3.1.1.3.1"></times><ci id="S5.SS1.p2.3.m3.1.1.3.2.cmml" xref="S5.SS1.p2.3.m3.1.1.3.2">𝑡</ci><ci id="S5.SS1.p2.3.m3.1.1.3.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3.3">𝑔</ci><ci id="S5.SS1.p2.3.m3.1.1.3.4.cmml" xref="S5.SS1.p2.3.m3.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">L_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.3.m3.1d">italic_L start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> which is a short paragraph present in most Wikipedia articles. Each candidate span <math alttext="x" class="ltx_Math" display="inline" id="S5.SS1.p2.4.m4.1"><semantics id="S5.SS1.p2.4.m4.1a"><mi id="S5.SS1.p2.4.m4.1.1" xref="S5.SS1.p2.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.1b"><ci id="S5.SS1.p2.4.m4.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.4.m4.1d">italic_x</annotation></semantics></math> is represented via the text <math alttext="t" class="ltx_Math" display="inline" id="S5.SS1.p2.5.m5.1"><semantics id="S5.SS1.p2.5.m5.1a"><mi id="S5.SS1.p2.5.m5.1.1" xref="S5.SS1.p2.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.5.m5.1b"><ci id="S5.SS1.p2.5.m5.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.5.m5.1c">t</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.5.m5.1d">italic_t</annotation></semantics></math> contained in <math alttext="x" class="ltx_Math" display="inline" id="S5.SS1.p2.6.m6.1"><semantics id="S5.SS1.p2.6.m6.1a"><mi id="S5.SS1.p2.6.m6.1.1" xref="S5.SS1.p2.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.6.m6.1b"><ci id="S5.SS1.p2.6.m6.1.1.cmml" xref="S5.SS1.p2.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.6.m6.1c">x</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.6.m6.1d">italic_x</annotation></semantics></math>. These textual features are concatenated together into a single textual input <math alttext="\phi" class="ltx_Math" display="inline" id="S5.SS1.p2.7.m7.1"><semantics id="S5.SS1.p2.7.m7.1a"><mi id="S5.SS1.p2.7.m7.1.1" xref="S5.SS1.p2.7.m7.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.7.m7.1b"><ci id="S5.SS1.p2.7.m7.1.1.cmml" xref="S5.SS1.p2.7.m7.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.7.m7.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.7.m7.1d">italic_ϕ</annotation></semantics></math> as,</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\vspace*{-1mm}\phi=\mathcal{T}(\texttt{[CLS]}T_{tgt}\texttt{[SEP]}L_{tgt}%
\texttt{[SEP]}t\texttt{[SEP]})\vspace*{-2mm}" class="ltx_Math" display="block" id="S5.E4.m1.1"><semantics id="S5.E4.m1.1a"><mrow id="S5.E4.m1.1.1" xref="S5.E4.m1.1.1.cmml"><mi id="S5.E4.m1.1.1.3" xref="S5.E4.m1.1.1.3.cmml">ϕ</mi><mo id="S5.E4.m1.1.1.2" xref="S5.E4.m1.1.1.2.cmml">=</mo><mrow id="S5.E4.m1.1.1.1" xref="S5.E4.m1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E4.m1.1.1.1.3" xref="S5.E4.m1.1.1.1.3.cmml">𝒯</mi><mo id="S5.E4.m1.1.1.1.2" xref="S5.E4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S5.E4.m1.1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.1.1.cmml"><mo id="S5.E4.m1.1.1.1.1.1.2" stretchy="false" xref="S5.E4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E4.m1.1.1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S5.E4.m1.1.1.1.1.1.1.2" xref="S5.E4.m1.1.1.1.1.1.1.2a.cmml">[CLS]</mtext><mo id="S5.E4.m1.1.1.1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.1.1.1.cmml">⁢</mo><msub id="S5.E4.m1.1.1.1.1.1.1.3" xref="S5.E4.m1.1.1.1.1.1.1.3.cmml"><mi id="S5.E4.m1.1.1.1.1.1.1.3.2" xref="S5.E4.m1.1.1.1.1.1.1.3.2.cmml">T</mi><mrow id="S5.E4.m1.1.1.1.1.1.1.3.3" xref="S5.E4.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S5.E4.m1.1.1.1.1.1.1.3.3.2" xref="S5.E4.m1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mo id="S5.E4.m1.1.1.1.1.1.1.3.3.1" xref="S5.E4.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.E4.m1.1.1.1.1.1.1.3.3.3" xref="S5.E4.m1.1.1.1.1.1.1.3.3.3.cmml">g</mi><mo id="S5.E4.m1.1.1.1.1.1.1.3.3.1a" xref="S5.E4.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.E4.m1.1.1.1.1.1.1.3.3.4" xref="S5.E4.m1.1.1.1.1.1.1.3.3.4.cmml">t</mi></mrow></msub><mo id="S5.E4.m1.1.1.1.1.1.1.1a" xref="S5.E4.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mtext class="ltx_mathvariant_monospace" id="S5.E4.m1.1.1.1.1.1.1.4" xref="S5.E4.m1.1.1.1.1.1.1.4a.cmml">[SEP]</mtext><mo id="S5.E4.m1.1.1.1.1.1.1.1b" xref="S5.E4.m1.1.1.1.1.1.1.1.cmml">⁢</mo><msub id="S5.E4.m1.1.1.1.1.1.1.5" xref="S5.E4.m1.1.1.1.1.1.1.5.cmml"><mi id="S5.E4.m1.1.1.1.1.1.1.5.2" xref="S5.E4.m1.1.1.1.1.1.1.5.2.cmml">L</mi><mrow id="S5.E4.m1.1.1.1.1.1.1.5.3" xref="S5.E4.m1.1.1.1.1.1.1.5.3.cmml"><mi id="S5.E4.m1.1.1.1.1.1.1.5.3.2" xref="S5.E4.m1.1.1.1.1.1.1.5.3.2.cmml">t</mi><mo id="S5.E4.m1.1.1.1.1.1.1.5.3.1" xref="S5.E4.m1.1.1.1.1.1.1.5.3.1.cmml">⁢</mo><mi id="S5.E4.m1.1.1.1.1.1.1.5.3.3" xref="S5.E4.m1.1.1.1.1.1.1.5.3.3.cmml">g</mi><mo id="S5.E4.m1.1.1.1.1.1.1.5.3.1a" xref="S5.E4.m1.1.1.1.1.1.1.5.3.1.cmml">⁢</mo><mi id="S5.E4.m1.1.1.1.1.1.1.5.3.4" xref="S5.E4.m1.1.1.1.1.1.1.5.3.4.cmml">t</mi></mrow></msub><mo id="S5.E4.m1.1.1.1.1.1.1.1c" xref="S5.E4.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mtext class="ltx_mathvariant_monospace" id="S5.E4.m1.1.1.1.1.1.1.6" xref="S5.E4.m1.1.1.1.1.1.1.6a.cmml">[SEP]</mtext><mo id="S5.E4.m1.1.1.1.1.1.1.1d" xref="S5.E4.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.E4.m1.1.1.1.1.1.1.7" xref="S5.E4.m1.1.1.1.1.1.1.7.cmml">t</mi><mo id="S5.E4.m1.1.1.1.1.1.1.1e" xref="S5.E4.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mtext class="ltx_mathvariant_monospace" id="S5.E4.m1.1.1.1.1.1.1.8" xref="S5.E4.m1.1.1.1.1.1.1.8a.cmml">[SEP]</mtext></mrow><mo id="S5.E4.m1.1.1.1.1.1.3" stretchy="false" xref="S5.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E4.m1.1b"><apply id="S5.E4.m1.1.1.cmml" xref="S5.E4.m1.1.1"><eq id="S5.E4.m1.1.1.2.cmml" xref="S5.E4.m1.1.1.2"></eq><ci id="S5.E4.m1.1.1.3.cmml" xref="S5.E4.m1.1.1.3">italic-ϕ</ci><apply id="S5.E4.m1.1.1.1.cmml" xref="S5.E4.m1.1.1.1"><times id="S5.E4.m1.1.1.1.2.cmml" xref="S5.E4.m1.1.1.1.2"></times><ci id="S5.E4.m1.1.1.1.3.cmml" xref="S5.E4.m1.1.1.1.3">𝒯</ci><apply id="S5.E4.m1.1.1.1.1.1.1.cmml" xref="S5.E4.m1.1.1.1.1.1"><times id="S5.E4.m1.1.1.1.1.1.1.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1.1"></times><ci id="S5.E4.m1.1.1.1.1.1.1.2a.cmml" xref="S5.E4.m1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S5.E4.m1.1.1.1.1.1.1.2.cmml" xref="S5.E4.m1.1.1.1.1.1.1.2">[CLS]</mtext></ci><apply id="S5.E4.m1.1.1.1.1.1.1.3.cmml" xref="S5.E4.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E4.m1.1.1.1.1.1.1.3.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E4.m1.1.1.1.1.1.1.3.2.cmml" xref="S5.E4.m1.1.1.1.1.1.1.3.2">𝑇</ci><apply id="S5.E4.m1.1.1.1.1.1.1.3.3.cmml" xref="S5.E4.m1.1.1.1.1.1.1.3.3"><times id="S5.E4.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1.3.3.1"></times><ci id="S5.E4.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S5.E4.m1.1.1.1.1.1.1.3.3.2">𝑡</ci><ci id="S5.E4.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S5.E4.m1.1.1.1.1.1.1.3.3.3">𝑔</ci><ci id="S5.E4.m1.1.1.1.1.1.1.3.3.4.cmml" xref="S5.E4.m1.1.1.1.1.1.1.3.3.4">𝑡</ci></apply></apply><ci id="S5.E4.m1.1.1.1.1.1.1.4a.cmml" xref="S5.E4.m1.1.1.1.1.1.1.4"><mtext class="ltx_mathvariant_monospace" id="S5.E4.m1.1.1.1.1.1.1.4.cmml" xref="S5.E4.m1.1.1.1.1.1.1.4">[SEP]</mtext></ci><apply id="S5.E4.m1.1.1.1.1.1.1.5.cmml" xref="S5.E4.m1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S5.E4.m1.1.1.1.1.1.1.5.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S5.E4.m1.1.1.1.1.1.1.5.2.cmml" xref="S5.E4.m1.1.1.1.1.1.1.5.2">𝐿</ci><apply id="S5.E4.m1.1.1.1.1.1.1.5.3.cmml" xref="S5.E4.m1.1.1.1.1.1.1.5.3"><times id="S5.E4.m1.1.1.1.1.1.1.5.3.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1.5.3.1"></times><ci id="S5.E4.m1.1.1.1.1.1.1.5.3.2.cmml" xref="S5.E4.m1.1.1.1.1.1.1.5.3.2">𝑡</ci><ci id="S5.E4.m1.1.1.1.1.1.1.5.3.3.cmml" xref="S5.E4.m1.1.1.1.1.1.1.5.3.3">𝑔</ci><ci id="S5.E4.m1.1.1.1.1.1.1.5.3.4.cmml" xref="S5.E4.m1.1.1.1.1.1.1.5.3.4">𝑡</ci></apply></apply><ci id="S5.E4.m1.1.1.1.1.1.1.6a.cmml" xref="S5.E4.m1.1.1.1.1.1.1.6"><mtext class="ltx_mathvariant_monospace" id="S5.E4.m1.1.1.1.1.1.1.6.cmml" xref="S5.E4.m1.1.1.1.1.1.1.6">[SEP]</mtext></ci><ci id="S5.E4.m1.1.1.1.1.1.1.7.cmml" xref="S5.E4.m1.1.1.1.1.1.1.7">𝑡</ci><ci id="S5.E4.m1.1.1.1.1.1.1.8a.cmml" xref="S5.E4.m1.1.1.1.1.1.1.8"><mtext class="ltx_mathvariant_monospace" id="S5.E4.m1.1.1.1.1.1.1.8.cmml" xref="S5.E4.m1.1.1.1.1.1.1.8">[SEP]</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E4.m1.1c">\vspace*{-1mm}\phi=\mathcal{T}(\texttt{[CLS]}T_{tgt}\texttt{[SEP]}L_{tgt}%
\texttt{[SEP]}t\texttt{[SEP]})\vspace*{-2mm}</annotation><annotation encoding="application/x-llamapun" id="S5.E4.m1.1d">italic_ϕ = caligraphic_T ( [CLS] italic_T start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT [SEP] italic_L start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT [SEP] italic_t [SEP] )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.p2.9">where <math alttext="\mathcal{T}(\cdot)" class="ltx_Math" display="inline" id="S5.SS1.p2.8.m1.1"><semantics id="S5.SS1.p2.8.m1.1a"><mrow id="S5.SS1.p2.8.m1.1.2" xref="S5.SS1.p2.8.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p2.8.m1.1.2.2" xref="S5.SS1.p2.8.m1.1.2.2.cmml">𝒯</mi><mo id="S5.SS1.p2.8.m1.1.2.1" xref="S5.SS1.p2.8.m1.1.2.1.cmml">⁢</mo><mrow id="S5.SS1.p2.8.m1.1.2.3.2" xref="S5.SS1.p2.8.m1.1.2.cmml"><mo id="S5.SS1.p2.8.m1.1.2.3.2.1" stretchy="false" xref="S5.SS1.p2.8.m1.1.2.cmml">(</mo><mo id="S5.SS1.p2.8.m1.1.1" lspace="0em" rspace="0em" xref="S5.SS1.p2.8.m1.1.1.cmml">⋅</mo><mo id="S5.SS1.p2.8.m1.1.2.3.2.2" stretchy="false" xref="S5.SS1.p2.8.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.8.m1.1b"><apply id="S5.SS1.p2.8.m1.1.2.cmml" xref="S5.SS1.p2.8.m1.1.2"><times id="S5.SS1.p2.8.m1.1.2.1.cmml" xref="S5.SS1.p2.8.m1.1.2.1"></times><ci id="S5.SS1.p2.8.m1.1.2.2.cmml" xref="S5.SS1.p2.8.m1.1.2.2">𝒯</ci><ci id="S5.SS1.p2.8.m1.1.1.cmml" xref="S5.SS1.p2.8.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.8.m1.1c">\mathcal{T}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.8.m1.1d">caligraphic_T ( ⋅ )</annotation></semantics></math> is the tokenizer operator that produces a sequence of <math alttext="T" class="ltx_Math" display="inline" id="S5.SS1.p2.9.m2.1"><semantics id="S5.SS1.p2.9.m2.1a"><mi id="S5.SS1.p2.9.m2.1.1" xref="S5.SS1.p2.9.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.9.m2.1b"><ci id="S5.SS1.p2.9.m2.1.1.cmml" xref="S5.SS1.p2.9.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.9.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.9.m2.1d">italic_T</annotation></semantics></math> tokens.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.4"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.4.1">Optimization.</span>
Given that entity insertion is a ranking task, we use an objective function that introduces the notion of ranking into the model. Specifically, we train the relevance scoring module using a cross-entropy loss over a <em class="ltx_emph ltx_font_italic" id="S5.SS1.p3.4.2">list</em> of candidates. Given a target entity <math alttext="E_{tgt}" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><msub id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">E</mi><mrow id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml"><mi id="S5.SS1.p3.1.m1.1.1.3.2" xref="S5.SS1.p3.1.m1.1.1.3.2.cmml">t</mi><mo id="S5.SS1.p3.1.m1.1.1.3.1" xref="S5.SS1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p3.1.m1.1.1.3.3" xref="S5.SS1.p3.1.m1.1.1.3.3.cmml">g</mi><mo id="S5.SS1.p3.1.m1.1.1.3.1a" xref="S5.SS1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p3.1.m1.1.1.3.4" xref="S5.SS1.p3.1.m1.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">𝐸</ci><apply id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3"><times id="S5.SS1.p3.1.m1.1.1.3.1.cmml" xref="S5.SS1.p3.1.m1.1.1.3.1"></times><ci id="S5.SS1.p3.1.m1.1.1.3.2.cmml" xref="S5.SS1.p3.1.m1.1.1.3.2">𝑡</ci><ci id="S5.SS1.p3.1.m1.1.1.3.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3.3">𝑔</ci><ci id="S5.SS1.p3.1.m1.1.1.3.4.cmml" xref="S5.SS1.p3.1.m1.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">E_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">italic_E start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, a list of <math alttext="N" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mi id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><ci id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m2.1d">italic_N</annotation></semantics></math> candidate text spans <math alttext="\mathcal{X}_{N}=[x_{1},\dots,x_{N}]" class="ltx_Math" display="inline" id="S5.SS1.p3.3.m3.3"><semantics id="S5.SS1.p3.3.m3.3a"><mrow id="S5.SS1.p3.3.m3.3.3" xref="S5.SS1.p3.3.m3.3.3.cmml"><msub id="S5.SS1.p3.3.m3.3.3.4" xref="S5.SS1.p3.3.m3.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p3.3.m3.3.3.4.2" xref="S5.SS1.p3.3.m3.3.3.4.2.cmml">𝒳</mi><mi id="S5.SS1.p3.3.m3.3.3.4.3" xref="S5.SS1.p3.3.m3.3.3.4.3.cmml">N</mi></msub><mo id="S5.SS1.p3.3.m3.3.3.3" xref="S5.SS1.p3.3.m3.3.3.3.cmml">=</mo><mrow id="S5.SS1.p3.3.m3.3.3.2.2" xref="S5.SS1.p3.3.m3.3.3.2.3.cmml"><mo id="S5.SS1.p3.3.m3.3.3.2.2.3" stretchy="false" xref="S5.SS1.p3.3.m3.3.3.2.3.cmml">[</mo><msub id="S5.SS1.p3.3.m3.2.2.1.1.1" xref="S5.SS1.p3.3.m3.2.2.1.1.1.cmml"><mi id="S5.SS1.p3.3.m3.2.2.1.1.1.2" xref="S5.SS1.p3.3.m3.2.2.1.1.1.2.cmml">x</mi><mn id="S5.SS1.p3.3.m3.2.2.1.1.1.3" xref="S5.SS1.p3.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S5.SS1.p3.3.m3.3.3.2.2.4" xref="S5.SS1.p3.3.m3.3.3.2.3.cmml">,</mo><mi id="S5.SS1.p3.3.m3.1.1" mathvariant="normal" xref="S5.SS1.p3.3.m3.1.1.cmml">…</mi><mo id="S5.SS1.p3.3.m3.3.3.2.2.5" xref="S5.SS1.p3.3.m3.3.3.2.3.cmml">,</mo><msub id="S5.SS1.p3.3.m3.3.3.2.2.2" xref="S5.SS1.p3.3.m3.3.3.2.2.2.cmml"><mi id="S5.SS1.p3.3.m3.3.3.2.2.2.2" xref="S5.SS1.p3.3.m3.3.3.2.2.2.2.cmml">x</mi><mi id="S5.SS1.p3.3.m3.3.3.2.2.2.3" xref="S5.SS1.p3.3.m3.3.3.2.2.2.3.cmml">N</mi></msub><mo id="S5.SS1.p3.3.m3.3.3.2.2.6" stretchy="false" xref="S5.SS1.p3.3.m3.3.3.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.3b"><apply id="S5.SS1.p3.3.m3.3.3.cmml" xref="S5.SS1.p3.3.m3.3.3"><eq id="S5.SS1.p3.3.m3.3.3.3.cmml" xref="S5.SS1.p3.3.m3.3.3.3"></eq><apply id="S5.SS1.p3.3.m3.3.3.4.cmml" xref="S5.SS1.p3.3.m3.3.3.4"><csymbol cd="ambiguous" id="S5.SS1.p3.3.m3.3.3.4.1.cmml" xref="S5.SS1.p3.3.m3.3.3.4">subscript</csymbol><ci id="S5.SS1.p3.3.m3.3.3.4.2.cmml" xref="S5.SS1.p3.3.m3.3.3.4.2">𝒳</ci><ci id="S5.SS1.p3.3.m3.3.3.4.3.cmml" xref="S5.SS1.p3.3.m3.3.3.4.3">𝑁</ci></apply><list id="S5.SS1.p3.3.m3.3.3.2.3.cmml" xref="S5.SS1.p3.3.m3.3.3.2.2"><apply id="S5.SS1.p3.3.m3.2.2.1.1.1.cmml" xref="S5.SS1.p3.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.3.m3.2.2.1.1.1.1.cmml" xref="S5.SS1.p3.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S5.SS1.p3.3.m3.2.2.1.1.1.2.cmml" xref="S5.SS1.p3.3.m3.2.2.1.1.1.2">𝑥</ci><cn id="S5.SS1.p3.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S5.SS1.p3.3.m3.2.2.1.1.1.3">1</cn></apply><ci id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">…</ci><apply id="S5.SS1.p3.3.m3.3.3.2.2.2.cmml" xref="S5.SS1.p3.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S5.SS1.p3.3.m3.3.3.2.2.2.1.cmml" xref="S5.SS1.p3.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S5.SS1.p3.3.m3.3.3.2.2.2.2.cmml" xref="S5.SS1.p3.3.m3.3.3.2.2.2.2">𝑥</ci><ci id="S5.SS1.p3.3.m3.3.3.2.2.2.3.cmml" xref="S5.SS1.p3.3.m3.3.3.2.2.2.3">𝑁</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.3c">\mathcal{X}_{N}=[x_{1},\dots,x_{N}]</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.3.m3.3d">caligraphic_X start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math>, and <math alttext="i^{\prime}" class="ltx_Math" display="inline" id="S5.SS1.p3.4.m4.1"><semantics id="S5.SS1.p3.4.m4.1a"><msup id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml"><mi id="S5.SS1.p3.4.m4.1.1.2" xref="S5.SS1.p3.4.m4.1.1.2.cmml">i</mi><mo id="S5.SS1.p3.4.m4.1.1.3" xref="S5.SS1.p3.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><apply id="S5.SS1.p3.4.m4.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.4.m4.1.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1">superscript</csymbol><ci id="S5.SS1.p3.4.m4.1.1.2.cmml" xref="S5.SS1.p3.4.m4.1.1.2">𝑖</ci><ci id="S5.SS1.p3.4.m4.1.1.3.cmml" xref="S5.SS1.p3.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">i^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.4.m4.1d">italic_i start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> as the index of the correct candidate, we use the following list-wise objective,</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\vspace*{-1mm}\max_{\theta}\frac{\exp\left(\operatorname{score}\left(x_{i^{%
\prime}},E_{tgt};\theta\right)\right)}{\sum_{i=1}^{N}\exp\left(\operatorname{%
score}\left(x_{i},E_{tgt};\theta\right)\right)}" class="ltx_Math" display="block" id="S5.Ex1.m1.8"><semantics id="S5.Ex1.m1.8a"><mrow id="S5.Ex1.m1.8.9" xref="S5.Ex1.m1.8.9.cmml"><munder id="S5.Ex1.m1.8.9.1" xref="S5.Ex1.m1.8.9.1.cmml"><mi id="S5.Ex1.m1.8.9.1.2" xref="S5.Ex1.m1.8.9.1.2.cmml">max</mi><mi id="S5.Ex1.m1.8.9.1.3" xref="S5.Ex1.m1.8.9.1.3.cmml">θ</mi></munder><mo id="S5.Ex1.m1.8.9a" lspace="0.167em" xref="S5.Ex1.m1.8.9.cmml">⁡</mo><mfrac id="S5.Ex1.m1.8.8" xref="S5.Ex1.m1.8.8.cmml"><mrow id="S5.Ex1.m1.4.4.4.4" xref="S5.Ex1.m1.4.4.4.5.cmml"><mi id="S5.Ex1.m1.3.3.3.3" xref="S5.Ex1.m1.3.3.3.3.cmml">exp</mi><mo id="S5.Ex1.m1.4.4.4.4a" xref="S5.Ex1.m1.4.4.4.5.cmml">⁡</mo><mrow id="S5.Ex1.m1.4.4.4.4.1" xref="S5.Ex1.m1.4.4.4.5.cmml"><mo id="S5.Ex1.m1.4.4.4.4.1.2" xref="S5.Ex1.m1.4.4.4.5.cmml">(</mo><mrow id="S5.Ex1.m1.4.4.4.4.1.1.2" xref="S5.Ex1.m1.4.4.4.4.1.1.3.cmml"><mi id="S5.Ex1.m1.1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.cmml">score</mi><mo id="S5.Ex1.m1.4.4.4.4.1.1.2a" xref="S5.Ex1.m1.4.4.4.4.1.1.3.cmml">⁡</mo><mrow id="S5.Ex1.m1.4.4.4.4.1.1.2.2" xref="S5.Ex1.m1.4.4.4.4.1.1.3.cmml"><mo id="S5.Ex1.m1.4.4.4.4.1.1.2.2.3" xref="S5.Ex1.m1.4.4.4.4.1.1.3.cmml">(</mo><msub id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.cmml"><mi id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.2" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.2.cmml">x</mi><msup id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3.cmml"><mi id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3.2" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3.2.cmml">i</mi><mo id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3.3" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3.3.cmml">′</mo></msup></msub><mo id="S5.Ex1.m1.4.4.4.4.1.1.2.2.4" xref="S5.Ex1.m1.4.4.4.4.1.1.3.cmml">,</mo><msub id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.cmml"><mi id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.2" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.2.cmml">E</mi><mrow id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.cmml"><mi id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.2" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.2.cmml">t</mi><mo id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.1" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.3" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.3.cmml">g</mi><mo id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.1a" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.4" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.4.cmml">t</mi></mrow></msub><mo id="S5.Ex1.m1.4.4.4.4.1.1.2.2.5" xref="S5.Ex1.m1.4.4.4.4.1.1.3.cmml">;</mo><mi id="S5.Ex1.m1.2.2.2.2" xref="S5.Ex1.m1.2.2.2.2.cmml">θ</mi><mo id="S5.Ex1.m1.4.4.4.4.1.1.2.2.6" xref="S5.Ex1.m1.4.4.4.4.1.1.3.cmml">)</mo></mrow></mrow><mo id="S5.Ex1.m1.4.4.4.4.1.3" xref="S5.Ex1.m1.4.4.4.5.cmml">)</mo></mrow></mrow><mrow id="S5.Ex1.m1.8.8.8" xref="S5.Ex1.m1.8.8.8.cmml"><msubsup id="S5.Ex1.m1.8.8.8.5" xref="S5.Ex1.m1.8.8.8.5.cmml"><mo id="S5.Ex1.m1.8.8.8.5.2.2" xref="S5.Ex1.m1.8.8.8.5.2.2.cmml">∑</mo><mrow id="S5.Ex1.m1.8.8.8.5.2.3" xref="S5.Ex1.m1.8.8.8.5.2.3.cmml"><mi id="S5.Ex1.m1.8.8.8.5.2.3.2" xref="S5.Ex1.m1.8.8.8.5.2.3.2.cmml">i</mi><mo id="S5.Ex1.m1.8.8.8.5.2.3.1" xref="S5.Ex1.m1.8.8.8.5.2.3.1.cmml">=</mo><mn id="S5.Ex1.m1.8.8.8.5.2.3.3" xref="S5.Ex1.m1.8.8.8.5.2.3.3.cmml">1</mn></mrow><mi id="S5.Ex1.m1.8.8.8.5.3" xref="S5.Ex1.m1.8.8.8.5.3.cmml">N</mi></msubsup><mrow id="S5.Ex1.m1.8.8.8.4.1" xref="S5.Ex1.m1.8.8.8.4.2.cmml"><mi id="S5.Ex1.m1.7.7.7.3" xref="S5.Ex1.m1.7.7.7.3.cmml">exp</mi><mo id="S5.Ex1.m1.8.8.8.4.1a" xref="S5.Ex1.m1.8.8.8.4.2.cmml">⁡</mo><mrow id="S5.Ex1.m1.8.8.8.4.1.1" xref="S5.Ex1.m1.8.8.8.4.2.cmml"><mo id="S5.Ex1.m1.8.8.8.4.1.1.2" xref="S5.Ex1.m1.8.8.8.4.2.cmml">(</mo><mrow id="S5.Ex1.m1.8.8.8.4.1.1.1.2" xref="S5.Ex1.m1.8.8.8.4.1.1.1.3.cmml"><mi id="S5.Ex1.m1.5.5.5.1" xref="S5.Ex1.m1.5.5.5.1.cmml">score</mi><mo id="S5.Ex1.m1.8.8.8.4.1.1.1.2a" xref="S5.Ex1.m1.8.8.8.4.1.1.1.3.cmml">⁡</mo><mrow id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2" xref="S5.Ex1.m1.8.8.8.4.1.1.1.3.cmml"><mo id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.3" xref="S5.Ex1.m1.8.8.8.4.1.1.1.3.cmml">(</mo><msub id="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1" xref="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1.cmml"><mi id="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1.2" xref="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1.2.cmml">x</mi><mi id="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1.3" xref="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.4" xref="S5.Ex1.m1.8.8.8.4.1.1.1.3.cmml">,</mo><msub id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.cmml"><mi id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.2" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.2.cmml">E</mi><mrow id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.cmml"><mi id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.2" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.2.cmml">t</mi><mo id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.1" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.3" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.3.cmml">g</mi><mo id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.1a" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.4" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.4.cmml">t</mi></mrow></msub><mo id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.5" xref="S5.Ex1.m1.8.8.8.4.1.1.1.3.cmml">;</mo><mi id="S5.Ex1.m1.6.6.6.2" xref="S5.Ex1.m1.6.6.6.2.cmml">θ</mi><mo id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.6" xref="S5.Ex1.m1.8.8.8.4.1.1.1.3.cmml">)</mo></mrow></mrow><mo id="S5.Ex1.m1.8.8.8.4.1.1.3" xref="S5.Ex1.m1.8.8.8.4.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex1.m1.8b"><apply id="S5.Ex1.m1.8.9.cmml" xref="S5.Ex1.m1.8.9"><apply id="S5.Ex1.m1.8.9.1.cmml" xref="S5.Ex1.m1.8.9.1"><csymbol cd="ambiguous" id="S5.Ex1.m1.8.9.1.1.cmml" xref="S5.Ex1.m1.8.9.1">subscript</csymbol><max id="S5.Ex1.m1.8.9.1.2.cmml" xref="S5.Ex1.m1.8.9.1.2"></max><ci id="S5.Ex1.m1.8.9.1.3.cmml" xref="S5.Ex1.m1.8.9.1.3">𝜃</ci></apply><apply id="S5.Ex1.m1.8.8.cmml" xref="S5.Ex1.m1.8.8"><divide id="S5.Ex1.m1.8.8.9.cmml" xref="S5.Ex1.m1.8.8"></divide><apply id="S5.Ex1.m1.4.4.4.5.cmml" xref="S5.Ex1.m1.4.4.4.4"><exp id="S5.Ex1.m1.3.3.3.3.cmml" xref="S5.Ex1.m1.3.3.3.3"></exp><apply id="S5.Ex1.m1.4.4.4.4.1.1.3.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.2"><ci id="S5.Ex1.m1.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1.1">score</ci><apply id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1">subscript</csymbol><ci id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.2">𝑥</ci><apply id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3.1.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3">superscript</csymbol><ci id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3.2.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3.2">𝑖</ci><ci id="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3.3.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.1.1.1.3.3">′</ci></apply></apply><apply id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2"><csymbol cd="ambiguous" id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.1.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2">subscript</csymbol><ci id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.2.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.2">𝐸</ci><apply id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3"><times id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.1.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.1"></times><ci id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.2.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.2">𝑡</ci><ci id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.3.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.3">𝑔</ci><ci id="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.4.cmml" xref="S5.Ex1.m1.4.4.4.4.1.1.2.2.2.3.4">𝑡</ci></apply></apply><ci id="S5.Ex1.m1.2.2.2.2.cmml" xref="S5.Ex1.m1.2.2.2.2">𝜃</ci></apply></apply><apply id="S5.Ex1.m1.8.8.8.cmml" xref="S5.Ex1.m1.8.8.8"><apply id="S5.Ex1.m1.8.8.8.5.cmml" xref="S5.Ex1.m1.8.8.8.5"><csymbol cd="ambiguous" id="S5.Ex1.m1.8.8.8.5.1.cmml" xref="S5.Ex1.m1.8.8.8.5">superscript</csymbol><apply id="S5.Ex1.m1.8.8.8.5.2.cmml" xref="S5.Ex1.m1.8.8.8.5"><csymbol cd="ambiguous" id="S5.Ex1.m1.8.8.8.5.2.1.cmml" xref="S5.Ex1.m1.8.8.8.5">subscript</csymbol><sum id="S5.Ex1.m1.8.8.8.5.2.2.cmml" xref="S5.Ex1.m1.8.8.8.5.2.2"></sum><apply id="S5.Ex1.m1.8.8.8.5.2.3.cmml" xref="S5.Ex1.m1.8.8.8.5.2.3"><eq id="S5.Ex1.m1.8.8.8.5.2.3.1.cmml" xref="S5.Ex1.m1.8.8.8.5.2.3.1"></eq><ci id="S5.Ex1.m1.8.8.8.5.2.3.2.cmml" xref="S5.Ex1.m1.8.8.8.5.2.3.2">𝑖</ci><cn id="S5.Ex1.m1.8.8.8.5.2.3.3.cmml" type="integer" xref="S5.Ex1.m1.8.8.8.5.2.3.3">1</cn></apply></apply><ci id="S5.Ex1.m1.8.8.8.5.3.cmml" xref="S5.Ex1.m1.8.8.8.5.3">𝑁</ci></apply><apply id="S5.Ex1.m1.8.8.8.4.2.cmml" xref="S5.Ex1.m1.8.8.8.4.1"><exp id="S5.Ex1.m1.7.7.7.3.cmml" xref="S5.Ex1.m1.7.7.7.3"></exp><apply id="S5.Ex1.m1.8.8.8.4.1.1.1.3.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2"><ci id="S5.Ex1.m1.5.5.5.1.cmml" xref="S5.Ex1.m1.5.5.5.1">score</ci><apply id="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1">subscript</csymbol><ci id="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1.2">𝑥</ci><ci id="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.1.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2">subscript</csymbol><ci id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.2.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.2">𝐸</ci><apply id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3"><times id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.1.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.1"></times><ci id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.2.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.2">𝑡</ci><ci id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.3.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.3">𝑔</ci><ci id="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.4.cmml" xref="S5.Ex1.m1.8.8.8.4.1.1.1.2.2.2.3.4">𝑡</ci></apply></apply><ci id="S5.Ex1.m1.6.6.6.2.cmml" xref="S5.Ex1.m1.6.6.6.2">𝜃</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex1.m1.8c">\vspace*{-1mm}\max_{\theta}\frac{\exp\left(\operatorname{score}\left(x_{i^{%
\prime}},E_{tgt};\theta\right)\right)}{\sum_{i=1}^{N}\exp\left(\operatorname{%
score}\left(x_{i},E_{tgt};\theta\right)\right)}</annotation><annotation encoding="application/x-llamapun" id="S5.Ex1.m1.8d">roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT divide start_ARG roman_exp ( roman_score ( italic_x start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT ; italic_θ ) ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_exp ( roman_score ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT ; italic_θ ) ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.p3.5">where <math alttext="\operatorname{score}" class="ltx_Math" display="inline" id="S5.SS1.p3.5.m1.1"><semantics id="S5.SS1.p3.5.m1.1a"><mi id="S5.SS1.p3.5.m1.1.1" xref="S5.SS1.p3.5.m1.1.1.cmml">score</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.5.m1.1b"><ci id="S5.SS1.p3.5.m1.1.1.cmml" xref="S5.SS1.p3.5.m1.1.1">score</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.5.m1.1c">\operatorname{score}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.5.m1.1d">roman_score</annotation></semantics></math> is an operator chaining the operations from Equations <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.E2" title="In 5.1 Model ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.E3" title="In 5.1 Model ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.5"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.5.1">Inference.</span>
The document <math alttext="X_{src}" class="ltx_Math" display="inline" id="S5.SS1.p4.1.m1.1"><semantics id="S5.SS1.p4.1.m1.1a"><msub id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml"><mi id="S5.SS1.p4.1.m1.1.1.2" xref="S5.SS1.p4.1.m1.1.1.2.cmml">X</mi><mrow id="S5.SS1.p4.1.m1.1.1.3" xref="S5.SS1.p4.1.m1.1.1.3.cmml"><mi id="S5.SS1.p4.1.m1.1.1.3.2" xref="S5.SS1.p4.1.m1.1.1.3.2.cmml">s</mi><mo id="S5.SS1.p4.1.m1.1.1.3.1" xref="S5.SS1.p4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p4.1.m1.1.1.3.3" xref="S5.SS1.p4.1.m1.1.1.3.3.cmml">r</mi><mo id="S5.SS1.p4.1.m1.1.1.3.1a" xref="S5.SS1.p4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p4.1.m1.1.1.3.4" xref="S5.SS1.p4.1.m1.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><apply id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p4.1.m1.1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p4.1.m1.1.1.2.cmml" xref="S5.SS1.p4.1.m1.1.1.2">𝑋</ci><apply id="S5.SS1.p4.1.m1.1.1.3.cmml" xref="S5.SS1.p4.1.m1.1.1.3"><times id="S5.SS1.p4.1.m1.1.1.3.1.cmml" xref="S5.SS1.p4.1.m1.1.1.3.1"></times><ci id="S5.SS1.p4.1.m1.1.1.3.2.cmml" xref="S5.SS1.p4.1.m1.1.1.3.2">𝑠</ci><ci id="S5.SS1.p4.1.m1.1.1.3.3.cmml" xref="S5.SS1.p4.1.m1.1.1.3.3">𝑟</ci><ci id="S5.SS1.p4.1.m1.1.1.3.4.cmml" xref="S5.SS1.p4.1.m1.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">X_{src}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.1.m1.1d">italic_X start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT</annotation></semantics></math> in which to insert the entity <math alttext="E_{tgt}" class="ltx_Math" display="inline" id="S5.SS1.p4.2.m2.1"><semantics id="S5.SS1.p4.2.m2.1a"><msub id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml"><mi id="S5.SS1.p4.2.m2.1.1.2" xref="S5.SS1.p4.2.m2.1.1.2.cmml">E</mi><mrow id="S5.SS1.p4.2.m2.1.1.3" xref="S5.SS1.p4.2.m2.1.1.3.cmml"><mi id="S5.SS1.p4.2.m2.1.1.3.2" xref="S5.SS1.p4.2.m2.1.1.3.2.cmml">t</mi><mo id="S5.SS1.p4.2.m2.1.1.3.1" xref="S5.SS1.p4.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p4.2.m2.1.1.3.3" xref="S5.SS1.p4.2.m2.1.1.3.3.cmml">g</mi><mo id="S5.SS1.p4.2.m2.1.1.3.1a" xref="S5.SS1.p4.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p4.2.m2.1.1.3.4" xref="S5.SS1.p4.2.m2.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.1b"><apply id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p4.2.m2.1.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p4.2.m2.1.1.2.cmml" xref="S5.SS1.p4.2.m2.1.1.2">𝐸</ci><apply id="S5.SS1.p4.2.m2.1.1.3.cmml" xref="S5.SS1.p4.2.m2.1.1.3"><times id="S5.SS1.p4.2.m2.1.1.3.1.cmml" xref="S5.SS1.p4.2.m2.1.1.3.1"></times><ci id="S5.SS1.p4.2.m2.1.1.3.2.cmml" xref="S5.SS1.p4.2.m2.1.1.3.2">𝑡</ci><ci id="S5.SS1.p4.2.m2.1.1.3.3.cmml" xref="S5.SS1.p4.2.m2.1.1.3.3">𝑔</ci><ci id="S5.SS1.p4.2.m2.1.1.3.4.cmml" xref="S5.SS1.p4.2.m2.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">E_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.2.m2.1d">italic_E start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> may contain a number <math alttext="D" class="ltx_Math" display="inline" id="S5.SS1.p4.3.m3.1"><semantics id="S5.SS1.p4.3.m3.1a"><mi id="S5.SS1.p4.3.m3.1.1" xref="S5.SS1.p4.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.3.m3.1b"><ci id="S5.SS1.p4.3.m3.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.3.m3.1c">D</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.3.m3.1d">italic_D</annotation></semantics></math> of potentially overlapping text spans <math alttext="\mathcal{X}_{src}=[x_{1},...,x_{D}]" class="ltx_Math" display="inline" id="S5.SS1.p4.4.m4.3"><semantics id="S5.SS1.p4.4.m4.3a"><mrow id="S5.SS1.p4.4.m4.3.3" xref="S5.SS1.p4.4.m4.3.3.cmml"><msub id="S5.SS1.p4.4.m4.3.3.4" xref="S5.SS1.p4.4.m4.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p4.4.m4.3.3.4.2" xref="S5.SS1.p4.4.m4.3.3.4.2.cmml">𝒳</mi><mrow id="S5.SS1.p4.4.m4.3.3.4.3" xref="S5.SS1.p4.4.m4.3.3.4.3.cmml"><mi id="S5.SS1.p4.4.m4.3.3.4.3.2" xref="S5.SS1.p4.4.m4.3.3.4.3.2.cmml">s</mi><mo id="S5.SS1.p4.4.m4.3.3.4.3.1" xref="S5.SS1.p4.4.m4.3.3.4.3.1.cmml">⁢</mo><mi id="S5.SS1.p4.4.m4.3.3.4.3.3" xref="S5.SS1.p4.4.m4.3.3.4.3.3.cmml">r</mi><mo id="S5.SS1.p4.4.m4.3.3.4.3.1a" xref="S5.SS1.p4.4.m4.3.3.4.3.1.cmml">⁢</mo><mi id="S5.SS1.p4.4.m4.3.3.4.3.4" xref="S5.SS1.p4.4.m4.3.3.4.3.4.cmml">c</mi></mrow></msub><mo id="S5.SS1.p4.4.m4.3.3.3" xref="S5.SS1.p4.4.m4.3.3.3.cmml">=</mo><mrow id="S5.SS1.p4.4.m4.3.3.2.2" xref="S5.SS1.p4.4.m4.3.3.2.3.cmml"><mo id="S5.SS1.p4.4.m4.3.3.2.2.3" stretchy="false" xref="S5.SS1.p4.4.m4.3.3.2.3.cmml">[</mo><msub id="S5.SS1.p4.4.m4.2.2.1.1.1" xref="S5.SS1.p4.4.m4.2.2.1.1.1.cmml"><mi id="S5.SS1.p4.4.m4.2.2.1.1.1.2" xref="S5.SS1.p4.4.m4.2.2.1.1.1.2.cmml">x</mi><mn id="S5.SS1.p4.4.m4.2.2.1.1.1.3" xref="S5.SS1.p4.4.m4.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S5.SS1.p4.4.m4.3.3.2.2.4" xref="S5.SS1.p4.4.m4.3.3.2.3.cmml">,</mo><mi id="S5.SS1.p4.4.m4.1.1" mathvariant="normal" xref="S5.SS1.p4.4.m4.1.1.cmml">…</mi><mo id="S5.SS1.p4.4.m4.3.3.2.2.5" xref="S5.SS1.p4.4.m4.3.3.2.3.cmml">,</mo><msub id="S5.SS1.p4.4.m4.3.3.2.2.2" xref="S5.SS1.p4.4.m4.3.3.2.2.2.cmml"><mi id="S5.SS1.p4.4.m4.3.3.2.2.2.2" xref="S5.SS1.p4.4.m4.3.3.2.2.2.2.cmml">x</mi><mi id="S5.SS1.p4.4.m4.3.3.2.2.2.3" xref="S5.SS1.p4.4.m4.3.3.2.2.2.3.cmml">D</mi></msub><mo id="S5.SS1.p4.4.m4.3.3.2.2.6" stretchy="false" xref="S5.SS1.p4.4.m4.3.3.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.4.m4.3b"><apply id="S5.SS1.p4.4.m4.3.3.cmml" xref="S5.SS1.p4.4.m4.3.3"><eq id="S5.SS1.p4.4.m4.3.3.3.cmml" xref="S5.SS1.p4.4.m4.3.3.3"></eq><apply id="S5.SS1.p4.4.m4.3.3.4.cmml" xref="S5.SS1.p4.4.m4.3.3.4"><csymbol cd="ambiguous" id="S5.SS1.p4.4.m4.3.3.4.1.cmml" xref="S5.SS1.p4.4.m4.3.3.4">subscript</csymbol><ci id="S5.SS1.p4.4.m4.3.3.4.2.cmml" xref="S5.SS1.p4.4.m4.3.3.4.2">𝒳</ci><apply id="S5.SS1.p4.4.m4.3.3.4.3.cmml" xref="S5.SS1.p4.4.m4.3.3.4.3"><times id="S5.SS1.p4.4.m4.3.3.4.3.1.cmml" xref="S5.SS1.p4.4.m4.3.3.4.3.1"></times><ci id="S5.SS1.p4.4.m4.3.3.4.3.2.cmml" xref="S5.SS1.p4.4.m4.3.3.4.3.2">𝑠</ci><ci id="S5.SS1.p4.4.m4.3.3.4.3.3.cmml" xref="S5.SS1.p4.4.m4.3.3.4.3.3">𝑟</ci><ci id="S5.SS1.p4.4.m4.3.3.4.3.4.cmml" xref="S5.SS1.p4.4.m4.3.3.4.3.4">𝑐</ci></apply></apply><list id="S5.SS1.p4.4.m4.3.3.2.3.cmml" xref="S5.SS1.p4.4.m4.3.3.2.2"><apply id="S5.SS1.p4.4.m4.2.2.1.1.1.cmml" xref="S5.SS1.p4.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p4.4.m4.2.2.1.1.1.1.cmml" xref="S5.SS1.p4.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S5.SS1.p4.4.m4.2.2.1.1.1.2.cmml" xref="S5.SS1.p4.4.m4.2.2.1.1.1.2">𝑥</ci><cn id="S5.SS1.p4.4.m4.2.2.1.1.1.3.cmml" type="integer" xref="S5.SS1.p4.4.m4.2.2.1.1.1.3">1</cn></apply><ci id="S5.SS1.p4.4.m4.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1">…</ci><apply id="S5.SS1.p4.4.m4.3.3.2.2.2.cmml" xref="S5.SS1.p4.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S5.SS1.p4.4.m4.3.3.2.2.2.1.cmml" xref="S5.SS1.p4.4.m4.3.3.2.2.2">subscript</csymbol><ci id="S5.SS1.p4.4.m4.3.3.2.2.2.2.cmml" xref="S5.SS1.p4.4.m4.3.3.2.2.2.2">𝑥</ci><ci id="S5.SS1.p4.4.m4.3.3.2.2.2.3.cmml" xref="S5.SS1.p4.4.m4.3.3.2.2.2.3">𝐷</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.4.m4.3c">\mathcal{X}_{src}=[x_{1},...,x_{D}]</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.4.m4.3d">caligraphic_X start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ]</annotation></semantics></math>. At inference time, the procedure described above is applied to all the <math alttext="D" class="ltx_Math" display="inline" id="S5.SS1.p4.5.m5.1"><semantics id="S5.SS1.p4.5.m5.1a"><mi id="S5.SS1.p4.5.m5.1.1" xref="S5.SS1.p4.5.m5.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.5.m5.1b"><ci id="S5.SS1.p4.5.m5.1.1.cmml" xref="S5.SS1.p4.5.m5.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.5.m5.1c">D</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.5.m5.1d">italic_D</annotation></semantics></math> candidate text spans, and a relevance score is obtained for each candidate.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Two-stage training pipeline</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We extract two types of links for studying entity insertion: existing and added links (§ <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>). While added links reflect the entity insertion scenarios observed in the real world, we found that the number of added links is low for most languages (cf. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.T8" title="Table 8 ‣ B.2 Dataset statistics ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">8</span></a> in the Appendix), thereby not being sufficient for training our model. To circumvent this challenge, we develop a <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">two-stage training pipeline</span> that uses both existing and added links.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Dynamic context removal.</span>
A key challenge with existing links is that they only reflect the <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.2">text_present</span> category of entity insertion, as the mention of the target entity is always present in the article containing the link. We mitigate this challenge by introducing a novel <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.3">data augmentation</em> strategy to simulate all other real-world entity insertion scenarios that are missing in the existing links. <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.4">Dynamic context removal</em> modifies the context associated with each existing link during training to simulate editors’ edits of adding links under different scenarios of entity insertion discussed in §<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>. Specifically, to simulate the <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.5">missing_mention</span>, <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.6">missing_sentence</span>, and <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.7">missing_span</span> scenarios, we randomly remove a word (<span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.8">rm_mention</span>), a sentence (<span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.9">rm_sent</span>), or a span of sentences (<span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.10">rm_span</span>), respectively.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.T1" title="Table 1 ‣ 5.2 Two-stage training pipeline ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the strategies (cf. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.T10" title="Table 10 ‣ B.4 Dynamic context removal ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">10</span></a> in Appx. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.SS4" title="B.4 Dynamic context removal ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">B.4</span></a> for details with examples).</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Note that dynamic context removal may generate structural and linguistic patterns that would not occur in the text written by human editors. For example, applying the <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p3.1.1">rm_mention</span> strategy on the sentence “<em class="ltx_emph ltx_font_italic" id="S5.SS2.p3.1.2">Laika was a <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS2.p3.1.2.1">Soviet space dog</span> who was one of the first animals in space to orbit the Earth.</em>”, would produce the sentence “<em class="ltx_emph ltx_font_italic" id="S5.SS2.p3.1.3">Laika was a who was one of the first animals in space to orbit the Earth.</em>”. Such a sentence is unlikely to be found in natural text articles, and thus, there is a distribution shift from the augmented training data to the test data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Expansion.</span> To reduce the impact of this shift, we introduce a second stage of training where we use the added links containing real-world entity insertion scenarios. Note that unlike the first stage, which uses existing links, the second stage does not require dynamic context removal, as we have access to the real contexts used by editors covering all the entity insertion scenarios.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Dynamic context removal strategies.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.1" style="width:173.4pt;height:63.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.5pt,13.3pt) scale(0.704027716864877,0.704027716864877) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T1.1.1">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">Strategy</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.2" style="padding-top:1pt;padding-bottom:1pt;">Text Removed</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T1.1.1.2.1.1">rm_nth</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.2" style="padding-top:1pt;padding-bottom:1pt;">None</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.3.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T1.1.1.3.1.1">rm_mention</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.3.2" style="padding-top:1pt;padding-bottom:1pt;">Mention</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T1.1.1.4.1.1">rm_sent</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.2" style="padding-top:1pt;padding-bottom:1pt;">Sentence containing mention</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.1.1.5.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T1.1.1.5.1.1">rm_span</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.1.1.5.2" style="padding-top:1pt;padding-bottom:1pt;">Span of sentences containing mention</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Knowledge injection</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">While the representation presented in Eq. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.E4" title="In 5.1 Model ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a> (§ <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS1" title="5.1 Model ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5.1</span></a>) already allows <span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p1.1.1">LocEI</span> to measure the target entity’s relevance to the candidate text span, we inject external knowledge about the target entity and knowledge about the structural organization of the source article to produce better relevance embeddings.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.2">Since section titles provide additional ‘local’ knowledge in the form of a summarized conceptualization of a candidate span, we first add the title of the section <math alttext="s" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><mi id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><ci id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">italic_s</annotation></semantics></math> in which a span appears to its input representation.
Next, we add the list of mentions <math alttext="M_{tgt}" class="ltx_Math" display="inline" id="S5.SS3.p2.2.m2.1"><semantics id="S5.SS3.p2.2.m2.1a"><msub id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml"><mi id="S5.SS3.p2.2.m2.1.1.2" xref="S5.SS3.p2.2.m2.1.1.2.cmml">M</mi><mrow id="S5.SS3.p2.2.m2.1.1.3" xref="S5.SS3.p2.2.m2.1.1.3.cmml"><mi id="S5.SS3.p2.2.m2.1.1.3.2" xref="S5.SS3.p2.2.m2.1.1.3.2.cmml">t</mi><mo id="S5.SS3.p2.2.m2.1.1.3.1" xref="S5.SS3.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS3.p2.2.m2.1.1.3.3" xref="S5.SS3.p2.2.m2.1.1.3.3.cmml">g</mi><mo id="S5.SS3.p2.2.m2.1.1.3.1a" xref="S5.SS3.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS3.p2.2.m2.1.1.3.4" xref="S5.SS3.p2.2.m2.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><apply id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.2.m2.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p2.2.m2.1.1.2.cmml" xref="S5.SS3.p2.2.m2.1.1.2">𝑀</ci><apply id="S5.SS3.p2.2.m2.1.1.3.cmml" xref="S5.SS3.p2.2.m2.1.1.3"><times id="S5.SS3.p2.2.m2.1.1.3.1.cmml" xref="S5.SS3.p2.2.m2.1.1.3.1"></times><ci id="S5.SS3.p2.2.m2.1.1.3.2.cmml" xref="S5.SS3.p2.2.m2.1.1.3.2">𝑡</ci><ci id="S5.SS3.p2.2.m2.1.1.3.3.cmml" xref="S5.SS3.p2.2.m2.1.1.3.3">𝑔</ci><ci id="S5.SS3.p2.2.m2.1.1.3.4.cmml" xref="S5.SS3.p2.2.m2.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">M_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.2.m2.1d">italic_M start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> previously associated with the target entity. This list provides a strong signal of how the entity is typically referenced in the text, thereby facilitating the model to better attend to these mentions when computing the relevance embedding.
The final input format after knowledge injections is:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center">
<div class="ltx_inline-block ltx_markedasmath ltx_transformed_outer" id="S5.Ex2.m1.1.1.1" style="width:429.3pt;height:21.2pt;vertical-align:-5.8pt;"><span class="ltx_transformed_inner" style="transform:translate(110.2pt,-4.0pt) scale(2.05533704083081,2.05533704083081) ;">
<p class="ltx_p" id="S5.Ex2.m1.1.1.1.1"><math alttext="\phi=\mathcal{T}(\texttt{[CLS]}T_{tgt}M_{tgt}\texttt{[SEP]}L_{tgt}\texttt{[SEP%
]}s\texttt{[SEP]}t\texttt{[SEP]})" class="ltx_Math" display="inline" id="S5.Ex2.m1.1.1.1.1.m1.1"><semantics id="S5.Ex2.m1.1.1.1.1.m1.1a"><mrow id="S5.Ex2.m1.1.1.1.1.m1.1.1" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.cmml"><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.3.cmml">ϕ</mi><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.2.cmml">=</mo><mrow id="S5.Ex2.m1.1.1.1.1.m1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.3.cmml">𝒯</mi><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml"><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.2" stretchy="false" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.2a.cmml">[CLS]</mtext><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><msub id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.2" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.2.cmml">T</mi><mrow id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.2" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.1" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.3" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.3.cmml">g</mi><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.1a" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.4" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.4.cmml">t</mi></mrow></msub><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1a" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><msub id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.cmml"><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.2" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.2.cmml">M</mi><mrow id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.cmml"><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.2" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.2.cmml">t</mi><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.1" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.3" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.3.cmml">g</mi><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.1a" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.4" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.4.cmml">t</mi></mrow></msub><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1b" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mtext class="ltx_mathvariant_monospace" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.5" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.5a.cmml">[SEP]</mtext><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1c" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><msub id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.cmml"><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.2" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.2.cmml">L</mi><mrow id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.cmml"><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.2" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.2.cmml">t</mi><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.1" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.3" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.3.cmml">g</mi><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.1a" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.4" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.4.cmml">t</mi></mrow></msub><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1d" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mtext class="ltx_mathvariant_monospace" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.7" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.7a.cmml">[SEP]</mtext><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1e" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.8" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.8.cmml">s</mi><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1f" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mtext class="ltx_mathvariant_monospace" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.9" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.9a.cmml">[SEP]</mtext><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1g" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.10" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.10.cmml">t</mi><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1h" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mtext class="ltx_mathvariant_monospace" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.11" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.11a.cmml">[SEP]</mtext></mrow><mo id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.3" stretchy="false" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex2.m1.1.1.1.1.m1.1b"><apply id="S5.Ex2.m1.1.1.1.1.m1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1"><eq id="S5.Ex2.m1.1.1.1.1.m1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.2"></eq><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.3">italic-ϕ</ci><apply id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1"><times id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.2"></times><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.3">𝒯</ci><apply id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1"><times id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.1"></times><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.2a.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.2">[CLS]</mtext></ci><apply id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.2">𝑇</ci><apply id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3"><times id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.1"></times><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.2">𝑡</ci><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.3">𝑔</ci><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.4.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.3.4">𝑡</ci></apply></apply><apply id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.1.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.2.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.2">𝑀</ci><apply id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3"><times id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.1.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.1"></times><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.2.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.2">𝑡</ci><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.3.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.3">𝑔</ci><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.4.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.4.3.4">𝑡</ci></apply></apply><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.5a.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.5"><mtext class="ltx_mathvariant_monospace" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.5.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.5">[SEP]</mtext></ci><apply id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.1.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6">subscript</csymbol><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.2.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.2">𝐿</ci><apply id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3"><times id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.1.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.1"></times><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.2.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.2">𝑡</ci><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.3.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.3">𝑔</ci><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.4.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.6.3.4">𝑡</ci></apply></apply><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.7a.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.7"><mtext class="ltx_mathvariant_monospace" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.7.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.7">[SEP]</mtext></ci><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.8.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.8">𝑠</ci><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.9a.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.9"><mtext class="ltx_mathvariant_monospace" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.9.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.9">[SEP]</mtext></ci><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.10.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.10">𝑡</ci><ci id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.11a.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.11"><mtext class="ltx_mathvariant_monospace" id="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.11.cmml" xref="S5.Ex2.m1.1.1.1.1.m1.1.1.1.1.1.1.11">[SEP]</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex2.m1.1.1.1.1.m1.1c">\phi=\mathcal{T}(\texttt{[CLS]}T_{tgt}M_{tgt}\texttt{[SEP]}L_{tgt}\texttt{[SEP%
]}s\texttt{[SEP]}t\texttt{[SEP]})</annotation><annotation encoding="application/x-llamapun" id="S5.Ex2.m1.1.1.1.1.m1.1d">italic_ϕ = caligraphic_T ( [CLS] italic_T start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT italic_M start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT [SEP] italic_L start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT [SEP] italic_s [SEP] italic_t [SEP] )</annotation></semantics></math></p>
</span></div>
</td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Incorporating multilinguality (<span class="ltx_text ltx_font_smallcaps" id="S5.SS4.1.1">xLocEI</span>)</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">To enable the encoder to better model the relationship between an entity target and candidate text spans, we leverage the patterns existent in multiple languages.
For this, we train a single model by jointly considering entity insertion examples in multiple languages.
This enables cross-lingual transfer, empowering, especially, low-resource languages with lesser and lower quality training data.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Entity insertion performance obtained by macro-averaging over 20 Wikipedia language versions used for training the benchmarked methods. <span class="ltx_text ltx_font_smallcaps" id="S5.T2.8.1">xLocEI</span> trains a single model jointly on all 20 languages, whereas other methods train a separate model for each language. The categorization of entity insertion types into ‘Overall’, ‘Missing’, and ‘Present’ is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS3" title="6.3 Setup ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6.3</span></a>. Note that EntQA and GET work only for English (results in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.T3" title="Table 3 ‣ 5.4 Incorporating multilinguality (xLocEI) ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">3</span></a>), whereas PRP-Allpair was only used for zero-shot analysis (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.T4" title="Table 4 ‣ 6.3 Setup ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>) and English (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.T3" title="Table 3 ‣ 5.4 Incorporating multilinguality (xLocEI) ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">3</span></a>).</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S5.T2.6" style="width:346.9pt;height:119.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.9pt,12.3pt) scale(0.82868509195787,0.82868509195787) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T2.6.6">
<tr class="ltx_tr" id="S5.T2.6.6.7">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S5.T2.6.6.7.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.6.6.7.2" rowspan="2"><span class="ltx_text" id="S5.T2.6.6.7.2.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S5.T2.6.6.7.3">Hits@1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T2.6.6.7.4">MRR</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.8">
<td class="ltx_td ltx_border_r" id="S5.T2.6.6.8.1"></td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.8.2">Overall</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.8.3">Present</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.8.4">Missing</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.8.5">Overall</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.8.6">Present</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.8.7">Missing</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.9.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.9.2">Random</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.9.3">0.107</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.9.4">0.115</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.9.5">0.103</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.9.6">0.243</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.9.7">0.259</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.9.8">0.236</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.10.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.10.2">String Match</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.3">0.459</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.4">0.708</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.10.5">0.270</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.6">0.557</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.7">0.774</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.8">0.395</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.11.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.11.2">BM25</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.3">0.508</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.4">0.799</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.11.5">0.280</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.6">0.612</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.7">0.866</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.8">0.421</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.12.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.12.2">Simple fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.12.3">0.584</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.12.4">0.883</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.12.5">0.350</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.12.6">0.649</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.12.7">0.907</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.12.8">0.451</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.13.1">Proposed</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.13.2"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.6.6.13.2.1">LocEI</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.13.3">0.672</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.13.4">0.877</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.13.5">0.509</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.13.6">0.744</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.13.7">0.906</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.13.8">0.617</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.6.6.6.7">Proposed</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.6.6.6.8"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.6.6.6.8.1">xLocEI</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1">0.726<sup class="ltx_sup" id="S5.T2.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T2.1.1.1.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.1">0.909<sup class="ltx_sup" id="S5.T2.2.2.2.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T2.2.2.2.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.3.3.1">0.579<sup class="ltx_sup" id="S5.T2.3.3.3.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T2.3.3.3.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S5.T2.4.4.4.4.1">0.789<sup class="ltx_sup" id="S5.T2.4.4.4.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T2.4.4.4.4.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.5.5.5.5"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.5.5.1">0.929<sup class="ltx_sup" id="S5.T2.5.5.5.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T2.5.5.5.5.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.6.6.6.6"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.6.6.1">0.678<sup class="ltx_sup" id="S5.T2.6.6.6.6.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T2.6.6.6.6.1.1.1">†</span></sup></span></td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="S5.I1">
<li class="ltx_item" id="S5.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">†</span>
<div class="ltx_para" id="S5.I1.ix1.p1">
<p class="ltx_p" id="S5.I1.ix1.p1.1">Indicates statistical significance (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S5.I1.ix1.p1.1.m1.1"><semantics id="S5.I1.ix1.p1.1.m1.1a"><mrow id="S5.I1.ix1.p1.1.m1.1.1" xref="S5.I1.ix1.p1.1.m1.1.1.cmml"><mi id="S5.I1.ix1.p1.1.m1.1.1.2" xref="S5.I1.ix1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S5.I1.ix1.p1.1.m1.1.1.1" xref="S5.I1.ix1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.I1.ix1.p1.1.m1.1.1.3" xref="S5.I1.ix1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.ix1.p1.1.m1.1b"><apply id="S5.I1.ix1.p1.1.m1.1.1.cmml" xref="S5.I1.ix1.p1.1.m1.1.1"><lt id="S5.I1.ix1.p1.1.m1.1.1.1.cmml" xref="S5.I1.ix1.p1.1.m1.1.1.1"></lt><ci id="S5.I1.ix1.p1.1.m1.1.1.2.cmml" xref="S5.I1.ix1.p1.1.m1.1.1.2">𝑝</ci><cn id="S5.I1.ix1.p1.1.m1.1.1.3.cmml" type="float" xref="S5.I1.ix1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix1.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S5.I1.ix1.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>) between the best and the second-best scores.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Entity insertion performance obtained for English.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S5.T3.5" style="width:346.9pt;height:105.1pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-154.8pt,46.7pt) scale(0.528394840783784,0.528394840783784) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.5.5">
<tr class="ltx_tr" id="S5.T3.5.5.6">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S5.T3.5.5.6.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.5.5.6.2" rowspan="2"><span class="ltx_text" id="S5.T3.5.5.6.2.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S5.T3.5.5.6.3">Hits@1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T3.5.5.6.4">MRR</td>
</tr>
<tr class="ltx_tr" id="S5.T3.5.5.7">
<td class="ltx_td ltx_border_r" id="S5.T3.5.5.7.1"></td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.7.2">Overall</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.7.3">Present</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.7.4">Missing</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.7.5">Overall</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.7.6">Present</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.7.7">Missing</td>
</tr>
<tr class="ltx_tr" id="S5.T3.5.5.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.5.5.8.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.5.5.8.2">Random</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.5.8.3">0.079</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.5.8.4">0.110</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.5.5.8.5">0.067</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.5.8.6">0.202</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.5.8.7">0.240</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.5.8.8">0.187</td>
</tr>
<tr class="ltx_tr" id="S5.T3.5.5.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.9.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.9.2">String Match</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.9.3">0.391</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.9.4">0.732</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.9.5">0.264</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.9.6">0.489</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.9.7">0.796</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.9.8">0.374</td>
</tr>
<tr class="ltx_tr" id="S5.T3.5.5.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.10.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.10.2">BM25</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.10.3">0.439</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.10.4">0.838</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.10.5">0.290</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.10.6">0.538</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.10.7">0.894</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.10.8">0.404</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.1.1.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.1.1.1">EntQA<math alttext="{}_{\text{RET}}" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.m1.1a"><msub id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml"><mi id="S5.T3.1.1.1.1.m1.1.1a" xref="S5.T3.1.1.1.1.m1.1.1.cmml"></mi><mtext id="S5.T3.1.1.1.1.m1.1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.1a.cmml">RET</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1"><ci id="S5.T3.1.1.1.1.m1.1.1.1a.cmml" xref="S5.T3.1.1.1.1.m1.1.1.1"><mtext id="S5.T3.1.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="S5.T3.1.1.1.1.m1.1.1.1">RET</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">{}_{\text{RET}}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.m1.1d">start_FLOATSUBSCRIPT RET end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.1.3">0.099</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.1.4">0.136</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.1.1.5">0.085</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.1.6">0.234</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.1.7">0.278</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.1.8">0.217</td>
</tr>
<tr class="ltx_tr" id="S5.T3.5.5.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.11.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.11.2">GET</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.11.3">0.391</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.11.4">0.827</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.11.5">0.228</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.11.6">0.469</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.11.7">0.851</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.11.8">0.326</td>
</tr>
<tr class="ltx_tr" id="S5.T3.5.5.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.12.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.12.2">PRP-Allpair (GPT-3.5) <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib34" title="">2024</a>)</cite> *</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.12.3">0.160</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.12.4">0.375</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.12.5">0.092</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.12.6">0.322</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.12.7">0.536</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.12.8">0.255</td>
</tr>
<tr class="ltx_tr" id="S5.T3.5.5.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.13.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.13.2">PRP-Allpair (GPT-4) <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib34" title="">2024</a>)</cite> *</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.13.3">0.370</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.13.4">0.833</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.13.5">0.224</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.13.6">0.499</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.13.7">0.877</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.13.8">0.380</td>
</tr>
<tr class="ltx_tr" id="S5.T3.5.5.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.14.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.14.2">Simple fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.14.3">0.443</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.14.4">0.860</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.5.5.14.5">0.287</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.14.6">0.522</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.14.7">0.888</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.14.8">0.385</td>
</tr>
<tr class="ltx_tr" id="S5.T3.5.5.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.5.5.5.5">Proposed</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.5.5.5.6"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.5.5.5.6.1">LocEI</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T3.2.2.2.1"><span class="ltx_text ltx_font_bold" id="S5.T3.2.2.2.1.1">0.677<sup class="ltx_sup" id="S5.T3.2.2.2.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T3.2.2.2.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T3.5.5.5.7"><span class="ltx_text ltx_font_bold" id="S5.T3.5.5.5.7.1">0.879</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.3.3.3.2"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.3.2.1">0.602<sup class="ltx_sup" id="S5.T3.3.3.3.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T3.3.3.3.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T3.4.4.4.3"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.4.3.1">0.741<sup class="ltx_sup" id="S5.T3.4.4.4.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T3.4.4.4.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T3.5.5.5.8"><span class="ltx_text ltx_font_bold" id="S5.T3.5.5.5.8.1">0.902</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T3.5.5.5.4"><span class="ltx_text ltx_font_bold" id="S5.T3.5.5.5.4.1">0.681<sup class="ltx_sup" id="S5.T3.5.5.5.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S5.T3.5.5.5.4.1.1.1">†</span></sup></span></td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="S5.I2">
<li class="ltx_item" id="S5.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">†</span>
<div class="ltx_para" id="S5.I2.ix1.p1">
<p class="ltx_p" id="S5.I2.ix1.p1.1">Indicates statistical significance (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S5.I2.ix1.p1.1.m1.1"><semantics id="S5.I2.ix1.p1.1.m1.1a"><mrow id="S5.I2.ix1.p1.1.m1.1.1" xref="S5.I2.ix1.p1.1.m1.1.1.cmml"><mi id="S5.I2.ix1.p1.1.m1.1.1.2" xref="S5.I2.ix1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S5.I2.ix1.p1.1.m1.1.1.1" xref="S5.I2.ix1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.I2.ix1.p1.1.m1.1.1.3" xref="S5.I2.ix1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.ix1.p1.1.m1.1b"><apply id="S5.I2.ix1.p1.1.m1.1.1.cmml" xref="S5.I2.ix1.p1.1.m1.1.1"><lt id="S5.I2.ix1.p1.1.m1.1.1.1.cmml" xref="S5.I2.ix1.p1.1.m1.1.1.1"></lt><ci id="S5.I2.ix1.p1.1.m1.1.1.2.cmml" xref="S5.I2.ix1.p1.1.m1.1.1.2">𝑝</ci><cn id="S5.I2.ix1.p1.1.m1.1.1.3.cmml" type="float" xref="S5.I2.ix1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.ix1.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S5.I2.ix1.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>) between the best and the second-best scores.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">*</span>
<div class="ltx_para" id="S5.I2.ix2.p1">
<p class="ltx_p" id="S5.I2.ix2.p1.1">Evaluation on a sample of 100 test instances.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">All the resources required to reproduce the experiments in this paper are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/epfl-dlab/multilingual-entity-insertion" title="">https://github.com/epfl-dlab/multilingual-entity-insertion</a>.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Data</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We study entity insertion in 105 language versions of Wikipedia. We use a judicious mix (based on size, script, geographic coverage, etc.) of 20 languages for training the benchmarked methods, however, for evaluation, we consider all 105 languages. For dataset statistics, cf. Tables <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.T7" title="Table 7 ‣ B.2 Dataset statistics ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">7</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.T8" title="Table 8 ‣ B.2 Dataset statistics ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">8</span></a> of Appx. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2" title="Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.1">Training set.</span>
We train <span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p2.1.2">LocEI</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p2.1.3">xLocEI</span> using a two-stage training pipeline (§ <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS2" title="5.2 Two-stage training pipeline ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5.2</span></a>). While the data for the first stage is based on the existing links extracted from the 2023-10-01 snapshot, the second stage data is built using the links added between the 2023-09-01 and 2023-10-01 snapshots.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.5"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.5.1">Negative candidates.</span>

During training, we extract <math alttext="N" class="ltx_Math" display="inline" id="S6.SS1.p3.1.m1.1"><semantics id="S6.SS1.p3.1.m1.1a"><mi id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><ci id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.1.m1.1d">italic_N</annotation></semantics></math> negative candidates for each positive candidate. Negative candidates are text spans in the source <math alttext="X_{src}" class="ltx_Math" display="inline" id="S6.SS1.p3.2.m2.1"><semantics id="S6.SS1.p3.2.m2.1a"><msub id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml"><mi id="S6.SS1.p3.2.m2.1.1.2" xref="S6.SS1.p3.2.m2.1.1.2.cmml">X</mi><mrow id="S6.SS1.p3.2.m2.1.1.3" xref="S6.SS1.p3.2.m2.1.1.3.cmml"><mi id="S6.SS1.p3.2.m2.1.1.3.2" xref="S6.SS1.p3.2.m2.1.1.3.2.cmml">s</mi><mo id="S6.SS1.p3.2.m2.1.1.3.1" xref="S6.SS1.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S6.SS1.p3.2.m2.1.1.3.3" xref="S6.SS1.p3.2.m2.1.1.3.3.cmml">r</mi><mo id="S6.SS1.p3.2.m2.1.1.3.1a" xref="S6.SS1.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S6.SS1.p3.2.m2.1.1.3.4" xref="S6.SS1.p3.2.m2.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><apply id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS1.p3.2.m2.1.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S6.SS1.p3.2.m2.1.1.2.cmml" xref="S6.SS1.p3.2.m2.1.1.2">𝑋</ci><apply id="S6.SS1.p3.2.m2.1.1.3.cmml" xref="S6.SS1.p3.2.m2.1.1.3"><times id="S6.SS1.p3.2.m2.1.1.3.1.cmml" xref="S6.SS1.p3.2.m2.1.1.3.1"></times><ci id="S6.SS1.p3.2.m2.1.1.3.2.cmml" xref="S6.SS1.p3.2.m2.1.1.3.2">𝑠</ci><ci id="S6.SS1.p3.2.m2.1.1.3.3.cmml" xref="S6.SS1.p3.2.m2.1.1.3.3">𝑟</ci><ci id="S6.SS1.p3.2.m2.1.1.3.4.cmml" xref="S6.SS1.p3.2.m2.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">X_{src}</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.2.m2.1d">italic_X start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT</annotation></semantics></math> where the target entity <math alttext="E_{tgt}" class="ltx_Math" display="inline" id="S6.SS1.p3.3.m3.1"><semantics id="S6.SS1.p3.3.m3.1a"><msub id="S6.SS1.p3.3.m3.1.1" xref="S6.SS1.p3.3.m3.1.1.cmml"><mi id="S6.SS1.p3.3.m3.1.1.2" xref="S6.SS1.p3.3.m3.1.1.2.cmml">E</mi><mrow id="S6.SS1.p3.3.m3.1.1.3" xref="S6.SS1.p3.3.m3.1.1.3.cmml"><mi id="S6.SS1.p3.3.m3.1.1.3.2" xref="S6.SS1.p3.3.m3.1.1.3.2.cmml">t</mi><mo id="S6.SS1.p3.3.m3.1.1.3.1" xref="S6.SS1.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S6.SS1.p3.3.m3.1.1.3.3" xref="S6.SS1.p3.3.m3.1.1.3.3.cmml">g</mi><mo id="S6.SS1.p3.3.m3.1.1.3.1a" xref="S6.SS1.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S6.SS1.p3.3.m3.1.1.3.4" xref="S6.SS1.p3.3.m3.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.3.m3.1b"><apply id="S6.SS1.p3.3.m3.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS1.p3.3.m3.1.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S6.SS1.p3.3.m3.1.1.2.cmml" xref="S6.SS1.p3.3.m3.1.1.2">𝐸</ci><apply id="S6.SS1.p3.3.m3.1.1.3.cmml" xref="S6.SS1.p3.3.m3.1.1.3"><times id="S6.SS1.p3.3.m3.1.1.3.1.cmml" xref="S6.SS1.p3.3.m3.1.1.3.1"></times><ci id="S6.SS1.p3.3.m3.1.1.3.2.cmml" xref="S6.SS1.p3.3.m3.1.1.3.2">𝑡</ci><ci id="S6.SS1.p3.3.m3.1.1.3.3.cmml" xref="S6.SS1.p3.3.m3.1.1.3.3">𝑔</ci><ci id="S6.SS1.p3.3.m3.1.1.3.4.cmml" xref="S6.SS1.p3.3.m3.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.3.m3.1c">E_{tgt}</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.3.m3.1d">italic_E start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT</annotation></semantics></math> was not inserted. Whenever possible, we select <math alttext="N" class="ltx_Math" display="inline" id="S6.SS1.p3.4.m4.1"><semantics id="S6.SS1.p3.4.m4.1a"><mi id="S6.SS1.p3.4.m4.1.1" xref="S6.SS1.p3.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.4.m4.1b"><ci id="S6.SS1.p3.4.m4.1.1.cmml" xref="S6.SS1.p3.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.4.m4.1d">italic_N</annotation></semantics></math> negative candidates (“hard negatives”) from the same source article as the positive candidate. However, when articles are too small to be able to select <math alttext="N" class="ltx_Math" display="inline" id="S6.SS1.p3.5.m5.1"><semantics id="S6.SS1.p3.5.m5.1a"><mi id="S6.SS1.p3.5.m5.1.1" xref="S6.SS1.p3.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.5.m5.1b"><ci id="S6.SS1.p3.5.m5.1.1.cmml" xref="S6.SS1.p3.5.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.5.m5.1c">N</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.5.m5.1d">italic_N</annotation></semantics></math> negatives, we sample the remaining negative candidates randomly from other articles (“easy negatives”). Details pertaining to the implementation of negative candidate extraction are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.SS5" title="B.5 Rules for sampling negative candidates ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">B.5</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p4.1.1">Test set.</span> For evaluation, we use the links added between the 2023-10-01 and 2023-11-01 snapshots. This ensures no overlap between the training and test sets and is therefore advantageous in mitigating data leakages.
Unlike training, we use all the <math alttext="D" class="ltx_Math" display="inline" id="S6.SS1.p4.1.m1.1"><semantics id="S6.SS1.p4.1.m1.1a"><mi id="S6.SS1.p4.1.m1.1.1" xref="S6.SS1.p4.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.1.m1.1b"><ci id="S6.SS1.p4.1.m1.1.1.cmml" xref="S6.SS1.p4.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p4.1.m1.1d">italic_D</annotation></semantics></math> available candidates in an article for evaluation.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Baselines</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S6.SS2.p1.1.m1.1"><semantics id="S6.SS2.p1.1.m1.1a"><mo id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><ci id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S6.SS2.p1.1.1">Random:</span> ranks candidates uniformly at random.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S6.SS2.p2.1.m1.1"><semantics id="S6.SS2.p2.1.m1.1a"><mo id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><ci id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S6.SS2.p2.1.1">String Match:</span> searches for previously used mentions in the candidate text spans.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S6.SS2.p3.1.m1.1"><semantics id="S6.SS2.p3.1.m1.1a"><mo id="S6.SS2.p3.1.m1.1.1" xref="S6.SS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.1.m1.1b"><ci id="S6.SS2.p3.1.m1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S6.SS2.p3.1.1">BM25 <cite class="ltx_cite ltx_citemacro_cite">Robertson and Zaragoza (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib37" title="">2009</a>)</cite>:</span> applies the Okapi-BM25 implementation <cite class="ltx_cite ltx_citemacro_cite">Trotman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib42" title="">2014</a>)</cite> on keywords extracted from the target lead paragraph and the candidate text spans.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S6.SS2.p4.1.m1.1"><semantics id="S6.SS2.p4.1.m1.1a"><mo id="S6.SS2.p4.1.m1.1.1" xref="S6.SS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.1.m1.1b"><ci id="S6.SS2.p4.1.m1.1.1.cmml" xref="S6.SS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S6.SS2.p4.1.1">EntQA <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib53" title="">2022</a>)</cite> (English only):</span> for independently encoding the candidate text spans and target entity. We then use the retriever model of EntQA to rank text spans based on the cosine similarity between the embeddings.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p5">
<p class="ltx_p" id="S6.SS2.p5.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S6.SS2.p5.1.m1.1"><semantics id="S6.SS2.p5.1.m1.1a"><mo id="S6.SS2.p5.1.m1.1.1" xref="S6.SS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p5.1.m1.1b"><ci id="S6.SS2.p5.1.m1.1.1.cmml" xref="S6.SS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p5.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S6.SS2.p5.1.1">GET <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib11" title="">2022</a>)</cite> (English only):</span> use the generative ability of GET to generate the target entity name for each candidate text span. We then rank the text spans based on their likelihood of generating the target entity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p6">
<p class="ltx_p" id="S6.SS2.p6.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S6.SS2.p6.1.m1.1"><semantics id="S6.SS2.p6.1.m1.1a"><mo id="S6.SS2.p6.1.m1.1.1" xref="S6.SS2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p6.1.m1.1b"><ci id="S6.SS2.p6.1.m1.1.1.cmml" xref="S6.SS2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p6.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S6.SS2.p6.1.1">PRP-Allpair <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib34" title="">2024</a>)</cite> (Zero-shot only):</span> to assess the relevance of candidate text spans to the target entity in a pairwise manner using GPT-3.5 and GPT-4, and then uncover the ranking from all pairwise comparisons.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Setup</h3>
<div class="ltx_para ltx_noindent" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p1.1.1">Model.</span> We present results for <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.p1.1.2">LocEI</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.p1.1.3">xLocEI</span> by fine-tuning the pre-trained <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p1.1.4">xlm-roberta-base</span> model <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib7" title="">2020</a>)</cite> as the encoder. The MLP is a 2-layer network with ReLU activations. We also explored different model sizes (e.g. Large and XL) and other pre-trained models (BERT and T5): results in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3" title="Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Entity insertion performance in the zero-shot setting: results obtained by macro-averaging over 9 Wikipedia language versions that were not used for fine-tuning <span class="ltx_text ltx_font_smallcaps" id="S6.T4.3.1">xLocEI<sub class="ltx_sub" id="S6.T4.3.1.1"><span class="ltx_text ltx_font_italic" id="S6.T4.3.1.1.1">11</span></sub></span>. <span class="ltx_text ltx_font_smallcaps" id="S6.T4.4.2">xLocEI<sub class="ltx_sub" id="S6.T4.4.2.1"><span class="ltx_text ltx_font_italic" id="S6.T4.4.2.1.1">20</span></sub></span> was trained jointly on all 20 languages, whereas <span class="ltx_text ltx_font_smallcaps" id="S6.T4.16.3">LocEI</span> trains a separate model for each language. The categorization of entity insertion types into ‘Overall’, ‘Missing’, and ‘Present’ is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS3" title="6.3 Setup ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6.3</span></a>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S6.T4.14" style="width:346.9pt;height:66.1pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-159.6pt,30.2pt) scale(0.52074921420092,0.52074921420092) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T4.14.10">
<tr class="ltx_tr" id="S6.T4.14.10.11">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T4.14.10.11.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T4.14.10.11.2" rowspan="2"><span class="ltx_text" id="S6.T4.14.10.11.2.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" colspan="3" id="S6.T4.14.10.11.3">Hits@1</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_tt" colspan="3" id="S6.T4.14.10.11.4">MRR</td>
</tr>
<tr class="ltx_tr" id="S6.T4.14.10.12">
<td class="ltx_td ltx_border_r" id="S6.T4.14.10.12.1"></td>
<td class="ltx_td ltx_align_center" id="S6.T4.14.10.12.2">Overall</td>
<td class="ltx_td ltx_align_center" id="S6.T4.14.10.12.3">Present</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T4.14.10.12.4">Missing</td>
<td class="ltx_td ltx_align_center" id="S6.T4.14.10.12.5">Overall</td>
<td class="ltx_td ltx_align_center" id="S6.T4.14.10.12.6">Present</td>
<td class="ltx_td ltx_align_center" id="S6.T4.14.10.12.7">Missing</td>
</tr>
<tr class="ltx_tr" id="S6.T4.14.10.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T4.14.10.13.1">Fine-tuned</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T4.14.10.13.2"><span class="ltx_text ltx_font_smallcaps" id="S6.T4.14.10.13.2.1">LocEI</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T4.14.10.13.3">0.647</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T4.14.10.13.4">0.873</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T4.14.10.13.5">0.486</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T4.14.10.13.6">0.718</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T4.14.10.13.7">0.902</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T4.14.10.13.8">0.588</td>
</tr>
<tr class="ltx_tr" id="S6.T4.9.5.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T4.9.5.5.6">Fine-Tuned</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T4.5.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="S6.T4.5.1.1.1.1">xLocEI<sub class="ltx_sub" id="S6.T4.5.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S6.T4.5.1.1.1.1.1.1">20</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.6.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T4.6.2.2.2.1">0.709<sup class="ltx_sup" id="S6.T4.6.2.2.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T4.6.2.2.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.9.5.5.7"><span class="ltx_text ltx_font_bold" id="S6.T4.9.5.5.7.1">0.901</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T4.7.3.3.3"><span class="ltx_text ltx_font_bold" id="S6.T4.7.3.3.3.1">0.570<sup class="ltx_sup" id="S6.T4.7.3.3.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T4.7.3.3.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.8.4.4.4"><span class="ltx_text ltx_font_bold" id="S6.T4.8.4.4.4.1">0.772<sup class="ltx_sup" id="S6.T4.8.4.4.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T4.8.4.4.4.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.9.5.5.8"><span class="ltx_text ltx_font_bold" id="S6.T4.9.5.5.8.1">0.923</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.9.5.5.5"><span class="ltx_text ltx_font_bold" id="S6.T4.9.5.5.5.1">0.662<sup class="ltx_sup" id="S6.T4.9.5.5.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T4.9.5.5.5.1.1.1">†</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.14.10.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.14.10.14.1">Zero-shot</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.14.10.14.2">PRP-Allpair (GPT-3.5) <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib34" title="">2024</a>)</cite> *</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.14.10.14.3">0.289</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.14.10.14.4">0.423</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.14.10.14.5">0.210</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.14.10.14.6">0.433</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.14.10.14.7">0.563</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.14.10.14.8">0.353</td>
</tr>
<tr class="ltx_tr" id="S6.T4.14.10.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T4.14.10.15.1">Zero-shot</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T4.14.10.15.2">PRP-Allpair (GPT-4) <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib34" title="">2024</a>)</cite> *</td>
<td class="ltx_td ltx_align_center" id="S6.T4.14.10.15.3">0.571</td>
<td class="ltx_td ltx_align_center" id="S6.T4.14.10.15.4">0.859</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T4.14.10.15.5">0.344</td>
<td class="ltx_td ltx_align_center" id="S6.T4.14.10.15.6">0.656</td>
<td class="ltx_td ltx_align_center" id="S6.T4.14.10.15.7">0.897</td>
<td class="ltx_td ltx_align_center" id="S6.T4.14.10.15.8">0.468</td>
</tr>
<tr class="ltx_tr" id="S6.T4.14.10.10">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T4.14.10.10.6">Zero-Shot</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T4.10.6.6.1"><span class="ltx_text ltx_font_smallcaps" id="S6.T4.10.6.6.1.1">xLocEI<sub class="ltx_sub" id="S6.T4.10.6.6.1.1.1"><span class="ltx_text ltx_font_italic" id="S6.T4.10.6.6.1.1.1.1">11</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.11.7.7.2"><span class="ltx_text ltx_font_bold" id="S6.T4.11.7.7.2.1">0.690<sup class="ltx_sup" id="S6.T4.11.7.7.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T4.11.7.7.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.14.10.10.7"><span class="ltx_text ltx_font_bold" id="S6.T4.14.10.10.7.1">0.887</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T4.12.8.8.3"><span class="ltx_text ltx_font_bold" id="S6.T4.12.8.8.3.1">0.541<sup class="ltx_sup" id="S6.T4.12.8.8.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T4.12.8.8.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.13.9.9.4"><span class="ltx_text ltx_font_bold" id="S6.T4.13.9.9.4.1">0.755<sup class="ltx_sup" id="S6.T4.13.9.9.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T4.13.9.9.4.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.14.10.10.8"><span class="ltx_text ltx_font_bold" id="S6.T4.14.10.10.8.1">0.913</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.14.10.10.5"><span class="ltx_text ltx_font_bold" id="S6.T4.14.10.10.5.1">0.636<sup class="ltx_sup" id="S6.T4.14.10.10.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T4.14.10.10.5.1.1.1">†</span></sup></span></td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="S6.I1">
<li class="ltx_item" id="S6.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">†</span>
<div class="ltx_para" id="S6.I1.ix1.p1">
<p class="ltx_p" id="S6.I1.ix1.p1.1">Indicates statistical significance (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S6.I1.ix1.p1.1.m1.1"><semantics id="S6.I1.ix1.p1.1.m1.1a"><mrow id="S6.I1.ix1.p1.1.m1.1.1" xref="S6.I1.ix1.p1.1.m1.1.1.cmml"><mi id="S6.I1.ix1.p1.1.m1.1.1.2" xref="S6.I1.ix1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S6.I1.ix1.p1.1.m1.1.1.1" xref="S6.I1.ix1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S6.I1.ix1.p1.1.m1.1.1.3" xref="S6.I1.ix1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.I1.ix1.p1.1.m1.1b"><apply id="S6.I1.ix1.p1.1.m1.1.1.cmml" xref="S6.I1.ix1.p1.1.m1.1.1"><lt id="S6.I1.ix1.p1.1.m1.1.1.1.cmml" xref="S6.I1.ix1.p1.1.m1.1.1.1"></lt><ci id="S6.I1.ix1.p1.1.m1.1.1.2.cmml" xref="S6.I1.ix1.p1.1.m1.1.1.2">𝑝</ci><cn id="S6.I1.ix1.p1.1.m1.1.1.3.cmml" type="float" xref="S6.I1.ix1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I1.ix1.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S6.I1.ix1.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>) from fine-tuned <span class="ltx_text ltx_font_smallcaps" id="S6.I1.ix1.p1.1.1">LocEI</span>.
</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">*</span>
<div class="ltx_para" id="S6.I1.ix2.p1">
<p class="ltx_p" id="S6.I1.ix2.p1.1">Evaluation on a sample of 100 test instances.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<figure class="ltx_table" id="S6.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Entity insertion performance in the full zero-shot setting: results obtained by macro-averaging over 85 held-out Wikipedia language versions that were not used for fine-tuning the benchmarked methods.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S6.T5.4" style="width:429.3pt;height:157.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(42.8pt,-15.7pt) scale(1.249234561426,1.249234561426) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T5.4.4">
<tr class="ltx_tr" id="S6.T5.4.4.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T5.4.4.5.1" rowspan="2"><span class="ltx_text" id="S6.T5.4.4.5.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S6.T5.4.4.5.2">Hits@1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T5.4.4.5.3">MRR</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.4.6">
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.6.1">Overall</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.6.2">Present</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.4.4.6.3">Missing</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.6.4">Overall</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.6.5">Present</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.6.6">Missing</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.4.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.4.7.1">Random</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.4.4.7.2">0.148</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.4.4.7.3">0.132</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.4.4.7.4">0.148</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.4.4.7.5">0.288</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.4.4.7.6">0.287</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.4.4.7.7">0.281</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.4.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T5.4.4.8.1">String Match</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.8.2">0.442</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.8.3">0.717</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.4.4.8.4">0.273</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.8.5">0.549</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.8.6">0.786</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.8.7">0.406</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.4.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T5.4.4.9.1">BM25</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.9.2">0.456</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.9.3">0.733</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.4.4.9.4">0.294</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.9.5">0.580</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.9.6">0.823</td>
<td class="ltx_td ltx_align_center" id="S6.T5.4.4.9.7">0.435</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="S6.T5.1.1.1.1.1">xLocEI<sub class="ltx_sub" id="S6.T5.1.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S6.T5.1.1.1.1.1.1.1">11</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1.2">0.683</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1.3">0.853</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.1.1.1.4">0.585</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1.5">0.754</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1.6">0.886</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1.7">0.676</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S6.T5.2.2.2.1"><span class="ltx_text ltx_font_smallcaps" id="S6.T5.2.2.2.1.1">xLocEI<sub class="ltx_sub" id="S6.T5.2.2.2.1.1.1"><span class="ltx_text ltx_font_italic" id="S6.T5.2.2.2.1.1.1.1">20</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.3.3.3.2"><span class="ltx_text ltx_font_bold" id="S6.T5.3.3.3.2.1">0.706<sup class="ltx_sup" id="S6.T5.3.3.3.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T5.3.3.3.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.4.4.4.3"><span class="ltx_text ltx_font_bold" id="S6.T5.4.4.4.3.1">0.873<sup class="ltx_sup" id="S6.T5.4.4.4.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T5.4.4.4.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T5.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S6.T5.4.4.4.4.1">0.602</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.4.4.4.5"><span class="ltx_text ltx_font_bold" id="S6.T5.4.4.4.5.1">0.769</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.4.4.4.6"><span class="ltx_text ltx_font_bold" id="S6.T5.4.4.4.6.1">0.901</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.4.4.4.7"><span class="ltx_text ltx_font_bold" id="S6.T5.4.4.4.7.1">0.685</span></td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="S6.I2">
<li class="ltx_item" id="S6.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">†</span>
<div class="ltx_para" id="S6.I2.ix1.p1">
<p class="ltx_p" id="S6.I2.ix1.p1.1">Indicates statistical significance (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S6.I2.ix1.p1.1.m1.1"><semantics id="S6.I2.ix1.p1.1.m1.1a"><mrow id="S6.I2.ix1.p1.1.m1.1.1" xref="S6.I2.ix1.p1.1.m1.1.1.cmml"><mi id="S6.I2.ix1.p1.1.m1.1.1.2" xref="S6.I2.ix1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S6.I2.ix1.p1.1.m1.1.1.1" xref="S6.I2.ix1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S6.I2.ix1.p1.1.m1.1.1.3" xref="S6.I2.ix1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.I2.ix1.p1.1.m1.1b"><apply id="S6.I2.ix1.p1.1.m1.1.1.cmml" xref="S6.I2.ix1.p1.1.m1.1.1"><lt id="S6.I2.ix1.p1.1.m1.1.1.1.cmml" xref="S6.I2.ix1.p1.1.m1.1.1.1"></lt><ci id="S6.I2.ix1.p1.1.m1.1.1.2.cmml" xref="S6.I2.ix1.p1.1.m1.1.1.2">𝑝</ci><cn id="S6.I2.ix1.p1.1.m1.1.1.3.cmml" type="float" xref="S6.I2.ix1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I2.ix1.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S6.I2.ix1.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>) between 
<br class="ltx_break"/>the best and the second-best scores.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p2.1.1">Evaluation metrics.</span> We use (1) Hits@1, and (2) mean reciprocal rank (MRR) to evaluate the quality of the benchmarked methods. For each language, we compute the micro aggregates of the metrics over all added links in the test set.
Moreover, we present results grouped into three categories: (1) Overall: considering the entire test set, (2) Present: considering links corresponding to the <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.2">text_present</span> entity insertion scenario, and (3) Missing: considering links corresponding to all the other scenarios, namely, <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.3">missing_mention</span>, <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.4">missing_sentence</span>, and <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.5">missing_span</span>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">Additional details about the experimental setup and hyperparameter tuning (impact of pre-trained models, model sizes, training stages, pointwise vs. ranking loss, etc.) are present in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3" title="Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Main results</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">We evaluate three variants of our entity insertion model: i) <span class="ltx_text ltx_font_italic" id="S6.SS4.p1.1.1">simple fine-tuning</span>: a family of monolingual models fine-tuned in each language without the extensions (data augmentation, knowledge injection, two-stage training) introduced in <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p1.1.2">LocEI</span>; ii) <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p1.1.3">LocEI</span>: a family of monolingual models fine-tuned using the full <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p1.1.4">LocEI</span> framework; and iii) <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p1.1.5">xLocEI</span>, a single multilingual model fine-tuned jointly on all the languages using the full <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p1.1.6">LocEI</span> framework.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.T2" title="Table 2 ‣ 5.4 Incorporating multilinguality (xLocEI) ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">2</span></a> shows the models’ performance metrics (Hits@1 and MRR) aggregated (macro-average) over the 20 considered languages.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.3"><span class="ltx_text ltx_font_bold" id="S6.SS4.p2.3.1">Overall performance.</span>
We see that <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p2.3.2">xLocEI</span> achieves the best overall quality and statistically significantly outperforms all other models for all cases considered. The key highlights are as follows: (1) <em class="ltx_emph ltx_font_italic" id="S6.SS4.p2.3.3">BM25</em>, a hard-to-beat baseline for ranking tasks, is around <math alttext="20" class="ltx_Math" display="inline" id="S6.SS4.p2.1.m1.1"><semantics id="S6.SS4.p2.1.m1.1a"><mn id="S6.SS4.p2.1.m1.1.1" xref="S6.SS4.p2.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p2.1.m1.1b"><cn id="S6.SS4.p2.1.m1.1.1.cmml" type="integer" xref="S6.SS4.p2.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p2.1.m1.1c">20</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p2.1.m1.1d">20</annotation></semantics></math> percentage points inferior to <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p2.3.4">xLocEI</span>, (2) <em class="ltx_emph ltx_font_italic" id="S6.SS4.p2.3.5">simple fine-tuning</em>, a baseline that we introduce in this work, substantially outperforms all the other considered methods, but is inferior to <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p2.3.6">LocEI</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p2.3.7">xLocEI</span> by being about <math alttext="10" class="ltx_Math" display="inline" id="S6.SS4.p2.2.m2.1"><semantics id="S6.SS4.p2.2.m2.1a"><mn id="S6.SS4.p2.2.m2.1.1" xref="S6.SS4.p2.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p2.2.m2.1b"><cn id="S6.SS4.p2.2.m2.1.1.cmml" type="integer" xref="S6.SS4.p2.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p2.2.m2.1c">10</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p2.2.m2.1d">10</annotation></semantics></math> and <math alttext="15" class="ltx_Math" display="inline" id="S6.SS4.p2.3.m3.1"><semantics id="S6.SS4.p2.3.m3.1a"><mn id="S6.SS4.p2.3.m3.1.1" xref="S6.SS4.p2.3.m3.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p2.3.m3.1b"><cn id="S6.SS4.p2.3.m3.1.1.cmml" type="integer" xref="S6.SS4.p2.3.m3.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p2.3.m3.1c">15</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p2.3.m3.1d">15</annotation></semantics></math> percentage points worse, respectively, and (3) <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p2.3.8">xLocEI</span> consistently yields better scores than the language-specific <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p2.3.9">LocEI</span> models, demonstrating that the multilingual model is capable of transferring knowledge across languages to improve overall performance. In fact, by looking at the performance for the individual languages in Figs. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.F6" title="Figure 6 ‣ C.2 Multilingual entity insertion stratified by language ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.F7" title="Figure 7 ‣ C.2 Multilingual entity insertion stratified by language ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">7</span></a> (Appx. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS2" title="C.2 Multilingual entity insertion stratified by language ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">C.2</span></a>), we see that the improvement from <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p2.3.10">xLocEI</span> over <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p2.3.11">LocEI</span> is larger in low-resource languages (languages with less training data) such as Afrikaans (af), Welsh (cy), Uzbek (uz).</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS4.p3">
<p class="ltx_p" id="S6.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS4.p3.1.1">Performance on ‘Missing’ and ‘Present’ categories.</span>
The key finding is that the baselines lack robustness to the variation in entity insertion types, which is substantiated by the huge disparity of entity insertion performance (around <math alttext="50" class="ltx_Math" display="inline" id="S6.SS4.p3.1.m1.1"><semantics id="S6.SS4.p3.1.m1.1a"><mn id="S6.SS4.p3.1.m1.1.1" xref="S6.SS4.p3.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.1.m1.1b"><cn id="S6.SS4.p3.1.m1.1.1.cmml" type="integer" xref="S6.SS4.p3.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.1.m1.1c">50</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.1.m1.1d">50</annotation></semantics></math> percentage points) of all the baselines in the ‘Present’ and ‘Missing’ categories. This result further highlights the key limitation of the baselines: they cannot address the challenging scenarios of entity insertion. The key reason behind this disparity is that all the existing baselines rely on the existence of a suitable text span to insert a link to the target entity. On the contrary, both <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p3.1.2">LocEI</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p3.1.3">xLocEI</span> effectively utilize the signals manifested in the context due to the introduced extensions (e.g. data augmentation) and are therefore robust to different entity insertion scenarios. Consequently, we observe that both <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p3.1.4">LocEI</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p3.1.5">xLocEI</span> obtain substantial improvements over all the baseline models in the <span class="ltx_text ltx_font_typewriter" id="S6.SS4.p3.1.6">missing</span> category.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS4.p4">
<p class="ltx_p" id="S6.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS4.p4.1.1">Performance on English.</span> <span class="ltx_text" id="S6.SS4.p4.1.2" style="color:#000000;">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.T3" title="Table 3 ‣ 5.4 Incorporating multilinguality (xLocEI) ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">3</span></a> shows that even in English (a high-resource language), <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.p4.1.2.1">xLocEI</span> outperforms all baselines. Once again, this gap is pronounced in the <span class="ltx_text ltx_font_typewriter" id="S6.SS4.p4.1.2.2">missing</span> case, further highlighting the difficulty and novelty of the task.</span></p>
</div>
<section class="ltx_subsubsection" id="S6.SS4.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Zero-shot vs. Fine-tuned</h4>
<div class="ltx_para" id="S6.SS4.SSSx1.p1">
<p class="ltx_p" id="S6.SS4.SSSx1.p1.6">We further study the performance of <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p1.6.7">xLocEI</span> in the zero-shot scenario, i.e., evaluating the model in languages that were not explicitly contained in the data for fine-tuning.
This is relevant to assess the potential to support languages for which there is little or no training data available.
We consider <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p1.1.1">xLocEI<sub class="ltx_sub" id="S6.SS4.SSSx1.p1.1.1.1"><span class="ltx_text ltx_font_italic" id="S6.SS4.SSSx1.p1.1.1.1.1">11</span></sub></span>, a variant of the multilingual <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p1.6.8">xLocEI</span> which is trained on only 11 out of the 20 languages (cf. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.T11" title="Table 11 ‣ C.3 Zero-shot entity insertion stratified by language ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">11</span></a> in Appx. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.SS3" title="C.3 Zero-shot entity insertion stratified by language ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">C.3</span></a> for details).
We then evaluate the zero-shot performance of <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p1.2.2">xLocEI<sub class="ltx_sub" id="S6.SS4.SSSx1.p1.2.2.1"><span class="ltx_text ltx_font_italic" id="S6.SS4.SSSx1.p1.2.2.1.1">11</span></sub></span> in the remaining 9 languages not considered for training.
For comparison, we also show the non-zero shot performance of the models considered in the previous subsection: i) <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p1.6.9">LocEI</span>, the family of monolingual models fine-tuned in each language; and ii) <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p1.3.3">xLocEI<sub class="ltx_sub" id="S6.SS4.SSSx1.p1.3.3.1"><span class="ltx_text ltx_font_italic" id="S6.SS4.SSSx1.p1.3.3.1.1">20</span></sub></span>, the single multilingual model trained on all 20 languages.
The main result, shown in (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.T4" title="Table 4 ‣ 6.3 Setup ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>), is that <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p1.4.4">xLocEI<sub class="ltx_sub" id="S6.SS4.SSSx1.p1.4.4.1"><span class="ltx_text ltx_font_italic" id="S6.SS4.SSSx1.p1.4.4.1.1">11</span></sub></span> retains over 95% performance in the zero-shot scenario in comparison to the results of the best model, <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p1.5.5">xLocEI<sub class="ltx_sub" id="S6.SS4.SSSx1.p1.5.5.1"><span class="ltx_text ltx_font_italic" id="S6.SS4.SSSx1.p1.5.5.1.1">20</span></sub></span>, which was fine-tuned on these languages.
Nevertheless, <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p1.6.6">xLocEI<sub class="ltx_sub" id="S6.SS4.SSSx1.p1.6.6.1"><span class="ltx_text ltx_font_italic" id="S6.SS4.SSSx1.p1.6.6.1.1">11</span></sub></span> still outperforms the language-specific <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p1.6.10">LocEI</span> models fine-tuned on each language individually.
We expand the robustness of these results by considering two additional scenarios.</p>
</div>
<figure class="ltx_table" id="S6.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Analyzing the impact of the extensions introduced in the <span class="ltx_text ltx_font_smallcaps" id="S6.T6.17.1">LocEI</span> framework on the entity insertion performance for only English and the macro-average over 20 Wikipedia language versions. The categorization of entity insertion types into ‘Overall’, ‘Missing’, and ‘Present’ is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS3" title="6.3 Setup ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6.3</span></a>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S6.T6.15" style="width:411.9pt;height:84.1pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-149.3pt,30.3pt) scale(0.579768453381161,0.579768453381161) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T6.15.15">
<tr class="ltx_tr" id="S6.T6.15.15.16">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T6.15.15.16.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="6" id="S6.T6.15.15.16.2">English</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S6.T6.15.15.16.3">All 20 Languages</td>
</tr>
<tr class="ltx_tr" id="S6.T6.15.15.17">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T6.15.15.17.1" rowspan="2"><span class="ltx_text" id="S6.T6.15.15.17.1.1">Model Variant</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="S6.T6.15.15.17.2">Hits@1</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="S6.T6.15.15.17.3">MRR</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="S6.T6.15.15.17.4">Hits@1</td>
<td class="ltx_td ltx_align_center" colspan="3" id="S6.T6.15.15.17.5">MRR</td>
</tr>
<tr class="ltx_tr" id="S6.T6.15.15.18">
<td class="ltx_td ltx_align_center" id="S6.T6.15.15.18.1">Overall</td>
<td class="ltx_td ltx_align_center" id="S6.T6.15.15.18.2">Present</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.15.15.18.3">Missing</td>
<td class="ltx_td ltx_align_center" id="S6.T6.15.15.18.4">Overall</td>
<td class="ltx_td ltx_align_center" id="S6.T6.15.15.18.5">Present</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.15.15.18.6">Missing</td>
<td class="ltx_td ltx_align_center" id="S6.T6.15.15.18.7">Overall</td>
<td class="ltx_td ltx_align_center" id="S6.T6.15.15.18.8">Present</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.15.15.18.9">Missing</td>
<td class="ltx_td ltx_align_center" id="S6.T6.15.15.18.10">Overall</td>
<td class="ltx_td ltx_align_center" id="S6.T6.15.15.18.11">Present</td>
<td class="ltx_td ltx_align_center" id="S6.T6.15.15.18.12">Missing</td>
</tr>
<tr class="ltx_tr" id="S6.T6.15.15.19">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.15.15.19.1">simple fine-tuning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.15.19.2">0.443</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.15.19.3">0.860</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T6.15.15.19.4">0.287</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.15.19.5">0.522</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.15.19.6">0.888</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T6.15.15.19.7">0.385</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.15.19.8">0.584</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.15.19.9"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.19.9.1">0.883</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T6.15.15.19.10">0.350</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.15.19.11">0.649</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.15.19.12"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.19.12.1">0.907</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.15.19.13">0.451</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T6.1.1.1.2">      +dynamic ctxt removal (w/o neg)</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.1.1.3">0.440</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.1.1.4">0.805</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.1.1.1.5">0.304</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.1.1.6">0.532</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.1.1.7">0.842</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.1.1.1.8">0.415</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.1.1.9">0.541</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.1.1.10">0.782</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.1.1.1.11">0.372</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.1.1.12">0.626</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.1.1.13">0.828</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1">0.487<sup class="ltx_sup" id="S6.T6.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S6.T6.1.1.1.1.1.1">†</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S6.T6.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T6.6.6.6.6">      +dynamic ctxt removal</td>
<td class="ltx_td ltx_align_center" id="S6.T6.6.6.6.7">0.473</td>
<td class="ltx_td ltx_align_center" id="S6.T6.6.6.6.8">0.846</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.6.6.6.9">0.334</td>
<td class="ltx_td ltx_align_center" id="S6.T6.6.6.6.10">0.547</td>
<td class="ltx_td ltx_align_center" id="S6.T6.6.6.6.11">0.875</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.6.6.6.12">0.424</td>
<td class="ltx_td ltx_align_center" id="S6.T6.2.2.2.1">0.574<sup class="ltx_sup" id="S6.T6.2.2.2.1.1"><span class="ltx_text ltx_font_italic" id="S6.T6.2.2.2.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.3.3.2">0.838<sup class="ltx_sup" id="S6.T6.3.3.3.2.1"><span class="ltx_text ltx_font_italic" id="S6.T6.3.3.3.2.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.6.6.6.13">0.376</td>
<td class="ltx_td ltx_align_center" id="S6.T6.4.4.4.3">0.649<sup class="ltx_sup" id="S6.T6.4.4.4.3.1"><span class="ltx_text ltx_font_italic" id="S6.T6.4.4.4.3.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.5.5.5.4">0.873<sup class="ltx_sup" id="S6.T6.5.5.5.4.1"><span class="ltx_text ltx_font_italic" id="S6.T6.5.5.5.4.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.6.6.6.5">0.486<sup class="ltx_sup" id="S6.T6.6.6.6.5.1"><span class="ltx_text ltx_font_italic" id="S6.T6.6.6.6.5.1.1">†</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S6.T6.14.14.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T6.14.14.14.9">      +expansion</td>
<td class="ltx_td ltx_align_center" id="S6.T6.7.7.7.1">0.648<sup class="ltx_sup" id="S6.T6.7.7.7.1.1"><span class="ltx_text ltx_font_italic" id="S6.T6.7.7.7.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.14.14.14.10">0.875</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.8.8.8.2">0.563<sup class="ltx_sup" id="S6.T6.8.8.8.2.1"><span class="ltx_text ltx_font_italic" id="S6.T6.8.8.8.2.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.9.9.9.3">0.719<sup class="ltx_sup" id="S6.T6.9.9.9.3.1"><span class="ltx_text ltx_font_italic" id="S6.T6.9.9.9.3.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.14.14.14.11"><span class="ltx_text ltx_font_bold" id="S6.T6.14.14.14.11.1">0.902</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.10.10.10.4">0.651<sup class="ltx_sup" id="S6.T6.10.10.10.4.1"><span class="ltx_text ltx_font_italic" id="S6.T6.10.10.10.4.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.11.11.11.5">0.657<sup class="ltx_sup" id="S6.T6.11.11.11.5.1"><span class="ltx_text ltx_font_italic" id="S6.T6.11.11.11.5.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.14.14.14.12">0.850</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T6.12.12.12.6">0.500<sup class="ltx_sup" id="S6.T6.12.12.12.6.1"><span class="ltx_text ltx_font_italic" id="S6.T6.12.12.12.6.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.13.13.13.7">0.733<sup class="ltx_sup" id="S6.T6.13.13.13.7.1"><span class="ltx_text ltx_font_italic" id="S6.T6.13.13.13.7.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S6.T6.14.14.14.13">0.889</td>
<td class="ltx_td ltx_align_center" id="S6.T6.14.14.14.8">0.609<sup class="ltx_sup" id="S6.T6.14.14.14.8.1"><span class="ltx_text ltx_font_italic" id="S6.T6.14.14.14.8.1.1">†</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S6.T6.15.15.15">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S6.T6.15.15.15.2">      +knowledge injection</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.15.15.15.3"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.15.3.1">0.677</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.15.15.15.4"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.15.4.1">0.879</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T6.15.15.15.5"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.15.5.1">0.602</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.15.15.15.6"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.15.6.1">0.741</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.15.15.15.7"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.15.7.1">0.902</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T6.15.15.15.8"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.15.8.1">0.681</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.15.15.15.9"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.15.9.1">0.672</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.15.15.15.1">0.877<sup class="ltx_sup" id="S6.T6.15.15.15.1.1"><span class="ltx_text ltx_font_italic" id="S6.T6.15.15.15.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T6.15.15.15.10"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.15.10.1">0.509</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.15.15.15.11"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.15.11.1">0.744</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.15.15.15.12">0.906</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.15.15.15.13"><span class="ltx_text ltx_font_bold" id="S6.T6.15.15.15.13.1">0.617</span></td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="S6.I3">
<li class="ltx_item" id="S6.I3.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">†</span>
<div class="ltx_para" id="S6.I3.ix1.p1">
<p class="ltx_p" id="S6.I3.ix1.p1.1">Indicates statistical significance (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S6.I3.ix1.p1.1.m1.1"><semantics id="S6.I3.ix1.p1.1.m1.1a"><mrow id="S6.I3.ix1.p1.1.m1.1.1" xref="S6.I3.ix1.p1.1.m1.1.1.cmml"><mi id="S6.I3.ix1.p1.1.m1.1.1.2" xref="S6.I3.ix1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S6.I3.ix1.p1.1.m1.1.1.1" xref="S6.I3.ix1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S6.I3.ix1.p1.1.m1.1.1.3" xref="S6.I3.ix1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.I3.ix1.p1.1.m1.1b"><apply id="S6.I3.ix1.p1.1.m1.1.1.cmml" xref="S6.I3.ix1.p1.1.m1.1.1"><lt id="S6.I3.ix1.p1.1.m1.1.1.1.cmml" xref="S6.I3.ix1.p1.1.m1.1.1.1"></lt><ci id="S6.I3.ix1.p1.1.m1.1.1.2.cmml" xref="S6.I3.ix1.p1.1.m1.1.1.2">𝑝</ci><cn id="S6.I3.ix1.p1.1.m1.1.1.3.cmml" type="float" xref="S6.I3.ix1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I3.ix1.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S6.I3.ix1.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>) between the variant and the previous variant.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<div class="ltx_para" id="S6.SS4.SSSx1.p2">
<p class="ltx_p" id="S6.SS4.SSSx1.p2.2">First, we compare the performance of <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p2.1.1">xLocEI<sub class="ltx_sub" id="S6.SS4.SSSx1.p2.1.1.1"><span class="ltx_text ltx_font_italic" id="S6.SS4.SSSx1.p2.1.1.1.1">11</span></sub></span> with PRP-Allpair, the state-of-the-art framework for ranking tasks using LLMs (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.T4" title="Table 4 ‣ 6.3 Setup ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>).
We find that <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p2.2.2">xLocEI<sub class="ltx_sub" id="S6.SS4.SSSx1.p2.2.2.1"><span class="ltx_text ltx_font_italic" id="S6.SS4.SSSx1.p2.2.2.1.1">11</span></sub></span> substantially outperforms PRP-Allpair, both when using GPT-3.5 and GPT-4, particularly for the cases when the mention that is linked is not yet present in the text (<span class="ltx_text ltx_font_typewriter" id="S6.SS4.SSSx1.p2.2.3">missing</span>).</p>
</div>
<div class="ltx_para" id="S6.SS4.SSSx1.p3">
<p class="ltx_p" id="S6.SS4.SSSx1.p3.2">Second, we evaluate our models on held-out data of the remaining 85 languages in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.T5" title="Table 5 ‣ 6.3 Setup ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5</span></a>. We reproduce a high zero-shot performance of <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p3.1.1">xLocEI<sub class="ltx_sub" id="S6.SS4.SSSx1.p3.1.1.1"><span class="ltx_text ltx_font_italic" id="S6.SS4.SSSx1.p3.1.1.1.1">11</span></sub></span>, in comparison to results in the <math alttext="9" class="ltx_Math" display="inline" id="S6.SS4.SSSx1.p3.2.m1.1"><semantics id="S6.SS4.SSSx1.p3.2.m1.1a"><mn id="S6.SS4.SSSx1.p3.2.m1.1.1" xref="S6.SS4.SSSx1.p3.2.m1.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.SSSx1.p3.2.m1.1b"><cn id="S6.SS4.SSSx1.p3.2.m1.1.1.cmml" type="integer" xref="S6.SS4.SSSx1.p3.2.m1.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSSx1.p3.2.m1.1c">9</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.SSSx1.p3.2.m1.1d">9</annotation></semantics></math> languages considered in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.T4" title="Table 4 ‣ 6.3 Setup ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>. In comparison, other baseline models yield substantially lower performance.</p>
</div>
<div class="ltx_para" id="S6.SS4.SSSx1.p4">
<p class="ltx_p" id="S6.SS4.SSSx1.p4.1">Overall, these findings show that <span class="ltx_text ltx_font_smallcaps" id="S6.SS4.SSSx1.p4.1.1">xLocEI</span> is capable of transferring the knowledge acquired during fine-tuning to unseen languages while maintaining a similar level of performance.
This demonstrates that our entity insertion model can be scaled to many languages even if little or no additional training data is available for those languages.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Ablation analysis</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">Finally, we investigate in more detail the effect of the extensions, namely, data augmentation, knowledge injection, and two-stage training that we introduce in the training pipeline of our model in comparison to a standard fine-tuning approach.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.T6" title="Table 6 ‣ Zero-shot vs. Fine-tuned ‣ 6.4 Main results ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6</span></a> portrays the improvement in performance on account for each extension introduced in this work.</p>
</div>
<div class="ltx_para" id="S6.SS5.p2">
<p class="ltx_p" id="S6.SS5.p2.1">Overall, we see that each extension has an overall positive impact on performance.
First, introducing the dynamic context removal for data augmentation is only effective when including negative examples.
In that case, it improves the performance on the <span class="ltx_text ltx_font_typewriter" id="S6.SS5.p2.1.1">missing</span> cases, but at the cost of performance in the <span class="ltx_text ltx_font_typewriter" id="S6.SS5.p2.1.2">present</span> case.
This is expected because context removal leads to the model seeing fewer training samples in the <span class="ltx_text ltx_font_typewriter" id="S6.SS5.p2.1.3">present</span> case.
Second, introducing expansion as a second stage in the training led to a large boost in performance in all scenarios, showing the benefit of using the smaller but high-quality dataset of added links for the training.
Third, the knowledge injection further improved the performance in both scenarios, indicating that the additional knowledge helps the model produce better relevance embeddings.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussions</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Summary of findings</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">We introduced the novel task of entity insertion in information networks.
Considering the case of Wikipedia, we justified the relevance and need for solving this task by demonstrating empirically that existing methods such as entity linking are often not suitable in practice.
In fact, we showed that in 65% of edits in which links were inserted by editors, none of the existing text is suitable to insert the entity, i.e. new text has to be inserted <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.1">somewhere in the article</span> along with the inserted entity.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">We developed a multilingual model (<span class="ltx_text ltx_font_smallcaps" id="S7.SS1.p2.1.1">xLocEI</span>) to effectively solve the entity insertion task across 20 Wikipedia languages outperforming all other models.
First, our model substantially outperforms strong baseline approaches based on string matching or BM25, especially in the case when the linked mention was missing. We demonstrate how each of the introduced novelties (data augmentation, knowledge injection, two-stage training pipeline) contribute to improve the downstream performance.
Second, the multilingual model yields consistently better results than language-specific models. This shows that our model is capable of collating the knowledge acquired from each language to improve performance over all languages.
Third, our model works well in a zero-shot scenario, i.e. not only retaining over <math alttext="95\%" class="ltx_Math" display="inline" id="S7.SS1.p2.1.m1.1"><semantics id="S7.SS1.p2.1.m1.1a"><mrow id="S7.SS1.p2.1.m1.1.1" xref="S7.SS1.p2.1.m1.1.1.cmml"><mn id="S7.SS1.p2.1.m1.1.1.2" xref="S7.SS1.p2.1.m1.1.1.2.cmml">95</mn><mo id="S7.SS1.p2.1.m1.1.1.1" xref="S7.SS1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p2.1.m1.1b"><apply id="S7.SS1.p2.1.m1.1.1.cmml" xref="S7.SS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S7.SS1.p2.1.m1.1.1.1.cmml" xref="S7.SS1.p2.1.m1.1.1.1">percent</csymbol><cn id="S7.SS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S7.SS1.p2.1.m1.1.1.2">95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p2.1.m1.1c">95\%</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p2.1.m1.1d">95 %</annotation></semantics></math> of the hypothetical best performance if the language was included but even outperforming the much larger GPT-3.5 and GPT-4.
This demonstrates that the model is capable of transferring knowledge to languages unseen during fine-tuning which is crucial for the practical application across the more than 300 languages in Wikipedia, for which often there is little or no training data available.
We compiled a new benchmark dataset for entity insertion in Wikipedia covering 105 languages.
We make the dataset publicly available to enable future research in entity insertion.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Implications and broader impact</h3>
<div class="ltx_para ltx_noindent" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p1.1.1">A new benchmark for NLP tasks.</span>
The problems of link recommendations and entity linking have been well-studied and many excellent solutions have been brought forward, some of which are denoted even near-optimal <cite class="ltx_cite ltx_citemacro_cite">Ghasemian et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib18" title="">2020</a>)</cite>.
The problem of entity insertion constitutes a new relevant and challenging task in the domain of NLP.
Our multilingual dataset provides a resource for researchers for development and evaluation of new models to solve this task.
This will help improve the overall capabilities of large language models when applied in the context of networks that are crucial for organizing textual information.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p2.1.1">Supporting editors to bridge knowledge gaps.</span>
Many articles in Wikipedia lack visibility in the hyperlink network capturing a specific aspect of the general problem of knowledge gaps <cite class="ltx_cite ltx_citemacro_cite">Redi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib36" title="">2020</a>)</cite>.
For example, there are more than 8.8M so-called orphan articles <cite class="ltx_cite ltx_citemacro_cite">Arora et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib3" title="">2024</a>)</cite>, i.e., articles without any incoming links, which are de-facto invisible to readers navigating Wikipedia.
Even if suitable link targets are identified, a remaining challenge for editors is to identify a relevant position in the text where the link can be inserted.
At the current rate of “de-orphanization”, it would take editors more than 20 years to work through the backlog of orphan articles, suggesting that existing tools do not support editors in addressing this issue effectively.
Our model can support editors in this task, complementing existing approached based on entity linking such as the add-a-link tool for newcomer editors <cite class="ltx_cite ltx_citemacro_cite">Gerlach et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib17" title="">2021</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We tried different pre-trained language models for our experiments with RoBERTa outperforming BERT and T5 by a large margin.
The use of larger models with more parameters could further improve performance.
While differences between RoBERTa-base and -large in English were marginal, we noticed a substantial drop when using the multilingual XLM-RoBERTa instead RoBERTa.
This suggests that larger model architectures could be especially beneficial in the multilingual setting in order to improve support for low-resource languages, where performance is typically lower in comparison <cite class="ltx_cite ltx_citemacro_cite">Wu and Dredze (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib50" title="">2020</a>)</cite>.
While multilingual models based on transformer architectures support many languages (e.g., XLM-RoBERTa was pre-trained on 100 languages), many of the more than 300 languages in Wikipedia are still not explicitly represented in the training data of these models. Thus, if unaddressed, the use of such models could lead to a language gap
constituting a substantial barrier towards knowledge equity <cite class="ltx_cite ltx_citemacro_cite">Redi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib36" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">One practical limitation of the model is that the ranking of all text spans can become expensive if the article is very long and, thus, contains many candidates.
This constitutes challenge for deploying the model in the future as a ready-to-use-tools for editors in practice.
This requires the integration of potential solutions for improving inference such as via hierarchical searching.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Further improvements to the model could come from integrating of additional information from the local Wikipedia graph structure or the candidate context.
For example, a very strong signal are the links already existing in the candidate context, as these indicate entities related to the context.
Providing these as additional features to the model might help generate better representations of the candidate <cite class="ltx_cite ltx_citemacro_cite">Arora et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib2" title="">2022</a>)</cite> and, as a result, better relevance embeddings.
Furthermore, one could take advantage of the multilingual nature of Wikipedia with more than 300 language versions, each having a surprising amount of information not contained in any other languages <cite class="ltx_cite ltx_citemacro_cite">Bao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib4" title="">2012</a>)</cite>.
Thus, existing content about a target entity from other languages could provide relevant context <cite class="ltx_cite ltx_citemacro_cite">García-Durán et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib16" title="">2022</a>)</cite>, which could be made available through automatic translation, such as the already available section translation tool in Wikipedia <cite class="ltx_cite ltx_citemacro_cite">WMF (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib49" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">In our operationalization of entity insertion, we assume that the link to be inserted consisting of the pair of the source- and target entity is known.
This assumption holds in the specific use-case of article “de-orphanization” <cite class="ltx_cite ltx_citemacro_cite">Arora et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib3" title="">2024</a>)</cite> serving as the motivation for formulating the task of entity insertion.
However, when this is not the case, our model requires an additional step to generate a specific link, e.g., via existing link recommendation models.</p>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">Our modeling framework is not suitable for the scenario where links are added in a section that did not exist in the previous version of the article (<span class="ltx_text ltx_font_typewriter" id="Sx1.p5.1.1">missing_section</span>).
The text from the surrounding sections are not a good indicator for the insertion of a new entity, because they typically cover different subjects.
The <span class="ltx_text ltx_font_typewriter" id="Sx1.p5.1.2">missing_section</span> scenario could be addressed through complementary approaches based on generative models that produce a draft for new section when none of the existing candidates leads to a high relevance score.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics statement</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">We have assessed the ethics of conducting this research and the output from the research and we have not identified ethical risks associated with the research.
All the datasets and resources used in this work are publicly available and do not contain private or sensitive information about Wikipedia readers.
While the dataset comes from individual edits (e.g., links added to Wikipedia articles), it does not contain any details about the author(s) of those edits.
All the findings are based on analyses conducted at an aggregate level, and thus, no individual-level inferences can be drawn. And lastly, the experiments are done on data already collected and no human subject has been involved as part of them. We confirm that we have read and abide by the ACL code of conduct.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">We thank Leila Zia, Alberto García-Durán, and Marija Šakota for insightful discussions and for reviewing an initial draft of this paper. West’s lab is partly supported by grants from the Swiss National Science Foundation (200021_185043 and 211379), Swiss Data Science Center (P22_08), H2020 (952215), Microsoft Swiss JRC, and Google. We also gratefully acknowledge generous gifts from Facebook, Google, and Microsoft.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora et al. (2021)</span>
<span class="ltx_bibblock">
Akhil Arora, Alberto Garcia-Duran, and Robert West. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.emnlp-main.634" title="">Low-Rank
Subspaces for Unsupervised Entity Linking</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">EMNLP</em>, pages 8037–8054.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora et al. (2022)</span>
<span class="ltx_bibblock">
Akhil Arora, Martin Gerlach, Tiziano Piccardi, Alberto García-Durán,
and Robert West. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3488560.3498496" title="">Wikipedia reader
navigation: When synthetic data is enough</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">WSDM</em>, page 16–26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora et al. (2024)</span>
<span class="ltx_bibblock">
Akhil Arora, Robert West, and Martin Gerlach. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/icwsm.v18i1.31300" title="">Orphan articles:
The dark matter of wikipedia</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">ICWSM</em>, pages 100–112.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al. (2012)</span>
<span class="ltx_bibblock">
Patti Bao, Brent Hecht, Samuel Carton, Mahmood Quaderi, Michael Horn, and
Darren Gergle. 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/2207676.2208553" title="">Omnipedia: bridging
the wikipedia language gap</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">SIGCHI</em>, pages 1075–1084.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brutzkus and Globerson (2019)</span>
<span class="ltx_bibblock">
Alon Brutzkus and Amir Globerson. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://proceedings.mlr.press/v97/brutzkus19b.html" title="">Why do
Larger Models Generalize Better? A Theoretical Perspective via the XOR
Problem</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">ICML</em>, pages 822–830.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2020)</span>
<span class="ltx_bibblock">
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=r1xMH1BtvB" title="">ELECTRA:
pre-training text encoders as discriminators rather than generators</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">ICLR</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,
and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.747" title="">Unsupervised
Cross-lingual Representation Learning at Scale</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">ACL</em>, pages 8440–8451.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Čuljak et al. (2022)</span>
<span class="ltx_bibblock">
Marko Čuljak, Andreas Spitz, Robert West, and Akhil Arora. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.naacl-srw.30" title="">Strong
Heuristics for Named Entity Linking</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">NAACL-SRW</em>, pages 235–246.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/n19-1423" title="">BERT: Pre-training
of Deep Bidirectional Transformers for Language Understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">NAACL</em>, pages 4171–4186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2022)</span>
<span class="ltx_bibblock">
Qian Dong, Yiding Liu, Suqi Cheng, Shuaiqiang Wang, Zhicong Cheng, Shuzi Niu,
and Dawei Yin. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3477495.3531997" title="">Incorporating
explicit knowledge in pre-trained language models for passage re-ranking</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">SIGIR</em>, pages 1490–1501.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2022)</span>
<span class="ltx_bibblock">
Christina Du, Kashyap Popat, Louis Martin, and Fabio Petroni. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2209.06148" title="">Entity Tagging:
Extracting Entities in Text Without Mention Supervision</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">CoRR</em>, abs/2209.06148.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2023)</span>
<span class="ltx_bibblock">
Jinyuan Fang, Zaiqiao Meng, and Craig Macdonald. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3583780.3615252" title="">KGPR: Knowledge
Graph Enhanced Passage Ranking</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CIKM</em>, page 3880–3885.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finkel et al. (2005)</span>
<span class="ltx_bibblock">
Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P05-1045/" title="">Incorporating non-local
information into information extraction systems by gibbs sampling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">ACL</em>, pages 363–370.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2020)</span>
<span class="ltx_bibblock">
Xingyu Fu, Weijia Shi, Xiaodong Yu, Zian Zhao, and Dan Roth. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.emnlp-main.521" title="">Design
challenges in low-resource cross-lingual entity linking</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">EMNLP</em>, pages 6418–6432.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2021)</span>
<span class="ltx_bibblock">
Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-030-72240-1_26" title="">Rethink
Training of BERT Rerankers in Multi-stage Retrieval Pipeline</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">ECIR</em>, pages 280–286.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">García-Durán et al. (2022)</span>
<span class="ltx_bibblock">
Alberto García-Durán, Akhil Arora, and Robert West. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.lrec-1.690" title="">Efficient Entity
Candidate Generation for Low-Resource Languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">LREC</em>, pages 6429–6438.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerlach et al. (2021)</span>
<span class="ltx_bibblock">
Martin Gerlach, Marshall Miller, Rita Ho, Kosta Harlan, and Djellel Eddine
Difallah. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3459637.3481939" title="">Multilingual Entity
Linking System for Wikipedia with a Machine-in-the-Loop Approach</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">CIKM</em>, pages 3818–3827.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghasemian et al. (2020)</span>
<span class="ltx_bibblock">
Amir Ghasemian, Homa Hosseinmardi, Aram Galstyan, Edoardo M Airoldi, and Aaron
Clauset. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1073/pnas.1914950117" title="">Stacking models for
nearly optimal link prediction in complex networks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">PNAS</em>, 117(38):23393–23400.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et al. (2020)</span>
<span class="ltx_bibblock">
Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug
Downey, and Noah A. Smith. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.740" title="">Don’t Stop
Pretraining: Adapt Language Models to Domains and Tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ACL</em>, pages 8342–8360.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2020)</span>
<span class="ltx_bibblock">
Shuguang Han, Xuanhui Wang, Mike Bendersky, and Marc Najork. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2004.08476" title="">Learning-to-Rank with
BERT in TF-Ranking</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">CoRR</em>, abs/2004.08476.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffart et al. (2011)</span>
<span class="ltx_bibblock">
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau,
Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/D11-1072/" title="">Robust Disambiguation of
Named Entities in Text</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">EMNLP</em>, pages 782–792.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2008)</span>
<span class="ltx_bibblock">
Darren Wei Che Huang, Shlomo Geva, and Andrew Trotman. 2008.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-642-03761-0_32" title="">Overview of
the INEX 2008 Link the Wiki Track</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">INEX Workshop</em>, volume 5631, pages 314–325.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ju et al. (2021)</span>
<span class="ltx_bibblock">
Jia-Huei Ju, Jheng-Hong Yang, and Chuan-Ju Wang. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3404835.3463048" title="">Text-to-Text
Multi-view Learning for Passage Re-ranking</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">SIGIR</em>, pages 1803–1807.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab and Zaharia (2020)</span>
<span class="ltx_bibblock">
Omar Khattab and Matei Zaharia. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3397271.3401075" title="">ColBERT: Efficient
and Effective Passage Search via Contextualized Late Interaction over
BERT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">SIGIR</em>, pages 39–48.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2021)</span>
<span class="ltx_bibblock">
Jimmy Lin, Rodrigo Frassetto Nogueira, and Andrew Yates. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.2200/S01123ED1V01Y202108HLT053" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1.1">Pretrained Transformers for Text Ranking: BERT and Beyond</em></a>.

</span>
<span class="ltx_bibblock">Synthesis Lectures on Human Language Technologies. Morgan &amp;
Claypool Publishers.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Nelson F. Liu, Kenton Lee, and Kristina Toutanova. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.14337" title="">Anchor
Prediction: Automatic Refinement of Internet Links</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">CoRR</em>, abs/2305.14337.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1907.11692" title="">RoBERTa: A Robustly
Optimized BERT Pretraining Approach</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">CoRR</em>, abs/1907.11692.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milne and Witten (2008)</span>
<span class="ltx_bibblock">
David N. Milne and Ian H. Witten. 2008.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/1458082.1458150" title="">Learning to Link
with Wikipedia</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">CIKM</em>, pages 509–518.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Müller et al. (2020)</span>
<span class="ltx_bibblock">
Louis Martin Benjamin Müller, Pedro Javier Ortiz Suárez, Yoann
Dupont, Laurent Romary, Éric de la Clergerie, Djamé Seddah, and
Benoît Sagot. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.645" title="">CamemBERT: a
Tasty French Language Model</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">ACL</em>, pages 7203–7219.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nogueira and Cho (2019)</span>
<span class="ltx_bibblock">
Rodrigo Frassetto Nogueira and Kyunghyun Cho. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1901.04085" title="">Passage re-ranking with
BERT</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">CoRR</em>, abs/1901.04085.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nogueira et al. (2020)</span>
<span class="ltx_bibblock">
Rodrigo Frassetto Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.findings-emnlp.63" title="">Document
Ranking with a Pretrained Sequence-to-Sequence Model</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">EMNLP (Findings)</em>, pages 708–718.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nogueira et al. (2019)</span>
<span class="ltx_bibblock">
Rodrigo Frassetto Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1910.14424" title="">Multi-Stage Document
Ranking with BERT</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">CoRR</em>, abs/1910.14424.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nothman et al. (2013)</span>
<span class="ltx_bibblock">
Joel Nothman, Nicky Ringland, Will Radford, Tara Murphy, and James R. Curran.
2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/J.ARTINT.2012.03.006" title="">Learning
multilingual named entity recognition from Wikipedia</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Artif. Intell.</em>, 194:151–175.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2024)</span>
<span class="ltx_bibblock">
Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming
Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael
Bendersky. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.findings-naacl.97" title="">Large
language models are effective text rankers with pairwise ranking prompting</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">NAACL (Findings)</em>, pages 1504–1518.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v21/20-074.html" title="">Exploring the Limits
of Transfer Learning with a Unified Text-to-Text Transformer</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">J. Mach. Learn. Res.</em>, 21:140:1–140:67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Redi et al. (2020)</span>
<span class="ltx_bibblock">
Miriam Redi, Martin Gerlach, Isaac Johnson, Jonathan Morgan, and Leila Zia.
2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2008.12314" title="">A Taxonomy of Knowledge
Gaps for Wikimedia Projects (Second Draft)</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Zaragoza (2009)</span>
<span class="ltx_bibblock">
Stephen E. Robertson and Hugo Zaragoza. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1561/1500000019" title="">The Probabilistic
Relevance Framework: BM25 and Beyond</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Found. Trends Inf. Retr.</em>, 3(4):333–389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rybak et al. (2020)</span>
<span class="ltx_bibblock">
Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and Ireneusz Gawlik. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.111" title="">KLEJ:
Comprehensive Benchmark for Polish Language Understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">ACL</em>, pages 1191–1201.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2024)</span>
<span class="ltx_bibblock">
Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica
Lam. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.naacl-long.347" title="">Assisting in
writing Wikipedia-like articles from scratch with large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">NAACL</em>, pages 6252–6278.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simon et al. (2024)</span>
<span class="ltx_bibblock">
James B Simon, Dhruva Karkada, Nikhil Ghosh, and Mikhail Belkin. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=OdpIjS0vkO" title="">More is better:
when infinite overparameterization is optimal and overfitting is obligatory</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">ICLR</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soltanolkotabi et al. (2019)</span>
<span class="ltx_bibblock">
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TIT.2018.2854560" title="">Theoretical
Insights Into the Optimization Landscape of Over-Parameterized Shallow Neural
Networks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">IEEE Trans. Inf. Theory</em>, 65(2):742–769.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trotman et al. (2014)</span>
<span class="ltx_bibblock">
Andrew Trotman, Antti Puurula, and Blake Burgess. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/2682862.2682863" title="">Improvements to
BM25 and Language Models Examined</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">ADCS</em>, page 58.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Hulst et al. (2020)</span>
<span class="ltx_bibblock">
Johannes M. van Hulst, Faegheh Hasibi, Koen Dercksen, Krisztian Balog, and
Arjen P. de Vries. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3397271.3401416" title="">REL: An Entity
Linker Standing on the Shoulders of Giants</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">SIGIR</em>, pages 2197–2200.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" title="">Attention is All you Need</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">NeurIPS</em>, pages 5998–6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">West et al. (2009)</span>
<span class="ltx_bibblock">
Robert West, Doina Precup, and Joelle Pineau. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/1645953.1646093" title="">Completing
Wikipedia’s Hyperlink Structure through Dimensionality Reduction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CIKM</em>, pages 1097–1106.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">West et al. (2010)</span>
<span class="ltx_bibblock">
Robert West, Doina Precup, and Joelle Pineau. 2010.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/1871437.1871556" title="">Automatically
suggesting topics for augmenting text documents</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">CIKM</em>, page 929–938.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WMF (2010a)</span>
<span class="ltx_bibblock">
WMF. 2010a.

</span>
<span class="ltx_bibblock">Wikimedia downloads.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dumps.wikimedia.org/backup-index.html" title="">https://dumps.wikimedia.org/backup-index.html</a>.

</span>
<span class="ltx_bibblock">Accessed: 2023-05-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WMF (2010b)</span>
<span class="ltx_bibblock">
WMF. 2010b.

</span>
<span class="ltx_bibblock">Wikimedia enterprise html dumps.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dumps.wikimedia.org/other/enterprise_html/" title="">https://dumps.wikimedia.org/other/enterprise_html/</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-02-09.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WMF (2019)</span>
<span class="ltx_bibblock">
WMF. 2019.

</span>
<span class="ltx_bibblock">Content translation: Section translation.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mediawiki.org/wiki/Content_translation/Section_translation" title="">https://www.mediawiki.org/wiki/Content_translation/Section_translation</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-04-09.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Dredze (2020)</span>
<span class="ltx_bibblock">
Shijie Wu and Mark Dredze. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.repl4nlp-1.16" title="">Are All
Languages Created Equal in Multilingual BERT?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">RepL4NLP Workshop</em>, pages 120–130.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wulczyn et al. (2016)</span>
<span class="ltx_bibblock">
Ellery Wulczyn, Robert West, Leila Zia, and Jure Leskovec. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/2872427.2883077" title="">Growing wikipedia
across languages via recommendation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">WWW</em>, pages 975–985.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2021)</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, and Colin Raffel. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.41" title="">mT5: A
Massively Multilingual Pre-trained Text-to-Text Transformer</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">NAACL</em>, pages 483–498.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Wenzheng Zhang, Wenyue Hua, and Karl Stratos. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=US2rTP5nm_" title="">EntQA: Entity
linking as question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">ICLR</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al. (2023)</span>
<span class="ltx_bibblock">
Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni,
Xuanhui Wang, and Michael Bendersky. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3539618.3592047" title="">RankT5: Fine-Tuning
T5 for Text Ranking with Ranking Losses</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">SIGIR</em>, pages 2308–2313.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional related work</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Pre-trained language models</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">The transformer architecture, introduced by <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib44" title="">2017</a>)</cite>, has become the <span class="ltx_text ltx_font_italic" id="A1.SS1.p1.1.1">de facto</span> architecture for most Natural Language Processing (NLP) applications. A transformer-based pre-trained language model takes as input a text sequence and computes a vector embedding that captures the semantic and structural information contained in the text sequence, which can then be used in downstream applications.</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">Pre-training is an expensive process. For example, the base variant of BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib9" title="">2019</a>)</cite> took four days to train with 16 TPUs and RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib27" title="">2019</a>)</cite> took one day to train with 1024 GPUs. However, pre-trained models can be leveraged to novel downstream tasks by fine-tuning them on task-specific datasets. As a comparison, the authors of BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib9" title="">2019</a>)</cite> introduced several fine-tuned variants of BERT, all of which were fine-tuned in one hour using one TPU, which is much cheaper than pre-training the model for each task. This paradigm of pre-training language models on large amounts of data and then fine-tuning on much smaller amounts can reduce the cost of model training while retaining the knowledge from the pre-trained model and transferring it to the downstream task. Popular pre-trained models for multilingual tasks are mBERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib9" title="">2019</a>)</cite>, XLM-RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib7" title="">2020</a>)</cite>, and mT5 <cite class="ltx_cite ltx_citemacro_citep">(Xue et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib52" title="">2021</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Ranking tasks</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">Since entity insertion is a ranking task, in this section, we provide a short review of literature focusing on document retrieval and ranking.</p>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1">Classical approaches for ranking tasks, such as BM25 <cite class="ltx_cite ltx_citemacro_cite">Robertson and Zaragoza (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib37" title="">2009</a>)</cite>, mainly rely on probabilistic methods that attempt to match keywords between a query and a candidate document. However, these methods cannot capture complex semantic and structural patterns. For example, the sentences “The hero defeated the dragon and saved the damsel” and “The knight slayed the beast and rescued the princess” are semantically equivalent, but classical methods would to match them due to the small vocabulary overlap.</p>
</div>
<div class="ltx_para" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1">That said, pre-trained language models have become state-of-the-art for text ranking <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib25" title="">2021</a>)</cite>. A popular design for transformer-based ranking tasks is the cross-attention model, in which the query and the candidate document are concatenated into a sequence and then processed by the model. Since transformer models employ attention mechanisms, this strategy allows the model to capture the interactions between the query and the document.</p>
</div>
<div class="ltx_para" id="A1.SS2.p4">
<p class="ltx_p" id="A1.SS2.p4.1">This approach has been explored for encoder-only models <cite class="ltx_cite ltx_citemacro_cite">Han et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib20" title="">2020</a>); Nogueira et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib32" title="">2019</a>); Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib15" title="">2021</a>)</cite>, outperforming classical methods. There has also been previous research <cite class="ltx_cite ltx_citemacro_cite">Nogueira et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib31" title="">2020</a>); Ju et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib23" title="">2021</a>)</cite> in exploring encoder-decoder models, such as T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib35" title="">2020</a>)</cite>. However, even though encoder-decoder models are typically larger than encoder-only models, RankT5 <cite class="ltx_cite ltx_citemacro_cite">Zhuang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib54" title="">2023</a>)</cite> has shown that there is no consistent winner between encoder-decoder and encoder-only models.</p>
</div>
<div class="ltx_para" id="A1.SS2.p5">
<p class="ltx_p" id="A1.SS2.p5.1">Given its recent success in document retrieval, the training objective of <span class="ltx_text ltx_font_smallcaps" id="A1.SS2.p5.1.1">LocEI</span> is inspired by the ranking loss proposed in RankT5 <cite class="ltx_cite ltx_citemacro_cite">Zhuang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib54" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Domain adaption</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1"><cite class="ltx_cite ltx_citemacro_cite">Gururangan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib19" title="">2020</a>)</cite> have shown that a second phase of pre-training using domain-specific knowledge can improve the performance of language models. Their experiments started with a pre-trained RoBERTa model and continued pre-training it using unlabelled data from a large corpus of domain-specific text.</p>
</div>
<div class="ltx_para" id="A1.SS3.p2">
<p class="ltx_p" id="A1.SS3.p2.1">In our work, we propose a similar approach for fine-tuning, where we apply a first stage of domain-shifted data and then a second stage of domain-specific data to improve the performance further.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional dataset processing details</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Data preparation steps</h3>
<div class="ltx_para ltx_noindent" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p1.1.1">Existing links.</span> For the existing links, we store the following data: source and target titles, Wikidata QIDs, lead paragraphs, the name of the section containing the link, and a context surrounding the link. The context is defined as the sentence containing the link and the five sentences before and after (or until we reach the end of the section). We additionally keep positional information about the mention and the sentence containing the mention relative to the context (i.e., the start and end indices of the mention and the sentence in the context). The positional information is relevant to the data augmentation strategy we introduced (see § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.SS4" title="B.4 Dynamic context removal ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">B.4</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p2.1.1">Added links.</span> For the added links, we store the same information as in the existing links, except for the positional information. This is because positional information is required primarily for performing data augmentations, which are required only for processing existing links.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Dataset statistics</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.T7" title="Table 7 ‣ B.2 Dataset statistics ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">7</span></a> shows the summary statistics of the entity insertion dataset for each of the 105 considered language versions of Wikipedia, in particular the number of articles, the number of existing links, and the number of added links.</p>
</div>
<figure class="ltx_table" id="A2.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Summary statistics of the full entity insertion dataset collected from 105 different Wikipedia language versions.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T7.1" style="width:429.3pt;height:669.4pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-97.3pt,151.6pt) scale(0.687988019381936,0.687988019381936) ;">
<table class="ltx_tabular ltx_align_middle" id="A2.T7.1.1">
<tr class="ltx_tr" id="A2.T7.1.1.1">
<td class="ltx_td" id="A2.T7.1.1.1.1"></td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.1.2">Language</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.1.3">Articles</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.1.4">Existing Links</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.1.5">Added Links</td>
<td class="ltx_td" id="A2.T7.1.1.1.6"></td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.1.7">Language</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.1.8">Articles</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.1.9">Existing Links</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.1.10">Added Links</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.2.1">en</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.2.2">English</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.2.3">6.7M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.2.4">166M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T7.1.1.2.5">368K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.2.6">de</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.2.7">German</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.2.8">2.8M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.2.9">78.3M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.2.10">94.3K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.3">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.3.1">sv</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.3.2">Swedish</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.3.3">2.5M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.3.4">29.9M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.3.5">10.7K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.3.6">fr</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.3.7">French</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.3.8">2.5M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.3.9">85.1M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.3.10">64.5K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.4">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.4.1">nl</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.4.2">Dutch</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.4.3">2.1M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.4.4">24.7M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.4.5">23.6K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.4.6">ru</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.4.7">Russian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.4.8">1.9M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.4.9">47.6M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.4.10">33.8K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.5">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.5.1">es</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.5.2">Spanish</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.5.3">1.8M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.5.4">47.9M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.5.5">66.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.5.6">it</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.5.7">Italian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.5.8">1.7M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.5.9">51.1M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.5.10">45.6K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.6">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.6.1">pl</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.6.2">Polish</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.6.3">1.5M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.6.4">30.1M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.6.5">27.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.6.6">ja</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.6.7">Japanese</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.6.8">1.3M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.6.9">60.6M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.6.10">79.0K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.7">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.7.1">zh</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.7.2">Chinese</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.7.3">1.3M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.7.4">23.1M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.7.5">28.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.7.6">vi</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.7.7">Vietnamese</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.7.8">1.2M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.7.9">10.3M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.7.10">11.9K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.8">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.8.1">ar</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.8.2">Arabic</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.8.3">1.2M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.8.4">16.3M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.8.5">17.8K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.8.6">pt</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.8.7">Portuguese</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.8.8">1.1M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.8.9">21.9M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.8.10">24.2K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.9">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.9.1">fa</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.9.2">Persian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.9.3">971K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.9.4">9.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.9.5">18.1K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.9.6">ca</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.9.7">Catalan</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.9.8">732K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.9.9">14.6M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.9.10">18.4K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.10">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.10.1">sr</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.10.2">Serbian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.10.3">671K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.10.4">8.3M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.10.5">5.4K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.10.6">id</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.10.7">Indonesian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.10.8">650K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.10.9">8.5M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.10.10">13.7K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.11">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.11.1">ko</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.11.2">Korean</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.11.3">634K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.11.4">11.2M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.11.5">21.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.11.6">no</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.11.7">Norwegian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.11.8">611K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.11.9">11.3M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.11.10">7.2K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.12">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.12.1">ce</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.12.2">Chechen</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.12.3">599K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.12.4">3.0M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.12.5">48</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.12.6">fi</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.12.7">Finnish</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.12.8">554K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.12.9">9.7M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.12.10">13.7K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.13">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.13.1">cs</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.13.2">Czech</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.13.3">531K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.13.4">14.4M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.13.5">12.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.13.6">tr</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.13.7">Turkish</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.13.8">531K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.13.9">6.7M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.13.10">14.9K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.14">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.14.1">hu</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.14.2">Hungarian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.14.3">527K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.14.4">10.6M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.14.5">7.8K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.14.6">tt</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.14.7">Tatar</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.14.8">496K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.14.9">3.1M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.14.10">94</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.15">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.15.1">sh</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.15.2">Serbo-Croatian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.15.3">456K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.15.4">8.3M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.15.5">807</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.15.6">ro</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.15.7">Romanian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.15.8">439K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.15.9">6.9M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.15.10">4.2K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.16">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.16.1">eu</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.16.2">Basque</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.16.3">412K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.16.4">4.4M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.16.5">5.1K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.16.6">ms</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.16.7">Malay</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.16.8">363K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.16.9">2.9M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.16.10">2.7K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.17">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.17.1">he</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.17.2">Hebrew</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.17.3">341K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.17.4">14.7M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.17.5">36.7K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.17.6">eo</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.17.7">Esperanto</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.17.8">340K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.17.9">6.7M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.17.10">5.8K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.18">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.18.1">hy</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.18.2">Armenian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.18.3">296K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.18.4">4.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.18.5">3.7K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.18.6">da</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.18.7">Danish</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.18.8">294K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.18.9">5.7M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.18.10">2.3K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.19">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.19.1">bg</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.19.2">Bulgarian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.19.3">288K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.19.4">5.2M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.19.5">4.8K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.19.6">cy</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.19.7">Welsh</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.19.8">270K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.19.9">2.6M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.19.10">386</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.20">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.20.1">sk</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.20.2">Slovak</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.20.3">242K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.20.4">3.4M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.20.5">3.1K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.20.6">azb</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.20.7">South Azerbaijani</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.20.8">242K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.20.9">1.0M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.20.10">22</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.21">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.21.1">simple</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.21.2">Simple English</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.21.3">240K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.21.4">2.6M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.21.5">3.8K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.21.6">et</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.21.7">Estonian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.21.8">235K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.21.9">4.4M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.21.10">4.7K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.22">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.22.1">kk</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.22.2">Kazakh</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.22.3">233K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.22.4">1.6M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.22.5">1.5K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.22.6">be</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.22.7">Belarusian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.22.8">232K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.22.9">3.1M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.22.10">3.0K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.23">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.23.1">uz</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.23.2">Uzbek</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.23.3">230K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.23.4">1.3M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.23.5">4.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.23.6">min</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.23.7">Minangkabau</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.23.8">226K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.23.9">644K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.23.10">21</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.24">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.24.1">el</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.24.2">Greek</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.24.3">224K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.24.4">4.8M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.24.5">6.7K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.24.6">lt</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.24.7">Lithuanian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.24.8">210K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.24.9">3.8M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.24.10">2.4K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.25">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.25.1">gl</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.25.2">Galician</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.25.3">196K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.25.4">3.9M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.25.5">4.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.25.6">hr</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.25.7">Croatian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.25.8">194K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.25.9">3.2M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.25.10">3.4K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.26">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.26.1">ur</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.26.2">Urdu</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.26.3">190K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.26.4">1.4M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.26.5">5.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.26.6">az</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.26.7">Azerbaijani</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.26.8">188K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.26.9">2.4M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.26.10">6.3K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.27">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.27.1">sl</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.27.2">Slovenian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.27.3">182K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.27.4">3.1M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.27.5">1.5K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.27.6">ka</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.27.7">Georgian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.27.8">163K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.27.9">2.3M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.27.10">1.7K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.28">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.28.1">ta</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.28.2">Tamil</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.28.3">157K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.28.4">1.6M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.28.5">1.1K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.28.6">hi</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.28.7">Hindi</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.28.8">157K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.28.9">1.2M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.28.10">2.3K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.29">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.29.1">la</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.29.2">Latin</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.29.3">138K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.29.4">2.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.29.5">1.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.29.6">mk</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.29.7">Macedonian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.29.8">136K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.29.9">2.3M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.29.10">923</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.30">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.30.1">ast</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.30.2">Asturian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.30.3">128K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.30.4">2.6M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.30.5">49</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.30.6">lv</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.30.7">Latvian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.30.8">121K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.30.9">2.0M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.30.10">1.9K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.31">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.31.1">af</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.31.2">Afrikaans</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.31.3">111K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.31.4">1.3M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.31.5">1.4K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.31.6">tg</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.31.7">Tajik</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.31.8">108K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.31.9">567K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.31.10">123</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.32">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.32.1">sq</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.32.2">Albanian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.32.3">97.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.32.4">873K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.32.5">523</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.32.6">mg</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.32.7">Malagasy</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.32.8">95.7K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.32.9">495K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.32.10">1.2K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.33">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.33.1">bs</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.33.2">Bosnian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.33.3">89.7K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.33.4">1.6M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.33.5">969</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.33.6">oc</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.33.7">Occitan</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.33.8">88.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.33.9">1.1M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.33.10">1.9K</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.34">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.34.1">te</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.34.2">Telugu</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.34.3">82.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.34.4">934K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.34.5">1.1K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.34.6">sw</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.34.7">Swahili</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.34.8">74.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.34.9">1.0M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.34.10">558</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.35">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.35.1">lmo</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.35.2">Lombard</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.35.3">71.9K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.35.4">380K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.35.5">26</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.35.6">jv</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.35.7">Javanese</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.35.8">70.5K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.35.9">513K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.35.10">161</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.36">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.36.1">ba</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.36.2">Bashkir</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.36.3">62.4K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.36.4">960K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.36.5">649</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.36.6">lb</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.36.7">Luxembourgish</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.36.8">61.7K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.36.9">930K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.36.10">754</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.37">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.37.1">mr</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.37.2">Marathi</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.37.3">60.9K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.37.4">409K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.37.5">67</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.37.6">su</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.37.7">Sundanese</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.37.8">60.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.37.9">470K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.37.10">6</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.38">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.38.1">is</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.38.2">Icelandic</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.38.3">56.4K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.38.4">725K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.38.5">1.0K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.38.6">ga</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.38.7">Irish</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.38.8">56.0K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.38.9">387K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.38.10">204</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.39">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.39.1">ku</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.39.2">Kurdish</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.39.3">54.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.39.4">252K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.39.5">614</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.39.6">fy</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.39.7">Western Frisian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.39.8">51.0K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.39.9">1.3M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.39.10">579</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.40">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.40.1">pa</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.40.2">Punjabi</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.40.3">49.4K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.40.4">282K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.40.5">139</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.40.6">cv</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.40.7">Chuvash</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.40.8">48.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.40.9">213K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.40.10">304</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.41">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.41.1">br</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.41.2">Breton</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.41.3">46.5K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.41.4">326K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.41.5">852</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.41.6">tl</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.41.7">Tagalog</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.41.8">43.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.41.9">435K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.41.10">512</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.42">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.42.1">an</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.42.2">Aragonese</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.42.3">40.8K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.42.4">620K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.42.5">70</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.42.6">io</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.42.7">Ido</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.42.8">40.7K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.42.9">422K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.42.10">230</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.43">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.43.1">sco</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.43.2">Scots</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.43.3">35.5K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.43.4">251K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.43.5">40</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.43.6">vo</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.43.7">Volapük</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.43.8">34.6K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.43.9">134K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.43.10">7</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.44">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.44.1">ne</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.44.2">Nepali</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.44.3">32.1K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.44.4">168K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.44.5">250</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.44.6">ha</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.44.7">Hausa</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.44.8">30.6K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.44.9">129K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.44.10">262</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.45">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.45.1">gu</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.45.2">Gujarati</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.45.3">30.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.45.4">411K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.45.5">29</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.45.6">kn</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.45.7">Kannada</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.45.8">28.0K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.45.9">253K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.45.10">514</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.46">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.46.1">bar</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.46.2">Bavarian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.46.3">27.0K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.46.4">207K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.46.5">21</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.46.6">scn</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.46.7">Sicilian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.46.8">23.8K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.46.9">132K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.46.10">5</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.47">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.47.1">mn</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.47.2">Mongolian</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.47.3">22.5K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.47.4">187K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.47.5">467</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.47.6">si</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.47.7">Sinhala</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.47.8">20.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.47.9">81.7K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.47.10">36</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.48">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.48.1">ps</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.48.2">Pashto</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.48.3">16.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.48.4">49.7K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.48.5">10</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.48.6">gd</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.48.7">Scottish Gaelic</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.48.8">15.8K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.48.9">207K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.48.10">14</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.49">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.49.1">yi</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.49.2">Yiddish</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.49.3">15.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.49.4">185K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.49.5">21</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.49.6">sd</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.49.7">Sindhi</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.49.8">13.4K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.49.9">49.5K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.49.10">14</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.50">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.50.1">am</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.50.2">Amharic</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.50.3">12.9K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.50.4">69.1K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.50.5">12</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.50.6">as</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.50.7">Assamese</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.50.8">11.9K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.50.9">104K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.50.10">459</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.51">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.51.1">sa</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.51.2">Sanskrit</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.51.3">10.5K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.51.4">65.2K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.51.5">18</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.51.6">km</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.51.7">Khmer</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.51.8">9.8K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.51.9">52.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.51.10">95</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.52">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.52.1">ary</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.52.2">Moroccan Arabic</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.52.3">8.0K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.52.4">50.5K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.52.5">129</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.52.6">so</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.52.7">Somali</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.52.8">7.4K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.52.9">64.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.52.10">60</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.53">
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.53.1">ug</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.53.2">Uyghur</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.53.3">5.9K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.53.4">9.7K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T7.1.1.53.5">1</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.53.6">lo</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.53.7">Lao</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.53.8">4.7K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.53.9">14.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.1.53.10">11</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.54">
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T7.1.1.54.1">om</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T7.1.1.54.2">Oromo</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T7.1.1.54.3">1.7K</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T7.1.1.54.4">5.0K</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T7.1.1.54.5">18</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T7.1.1.54.6">xh</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T7.1.1.54.7">Xhosa</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T7.1.1.54.8">1.6K</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T7.1.1.54.9">2.8K</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T7.1.1.54.10">1</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="A2.SS2.p2">
<p class="ltx_p" id="A2.SS2.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.T8" title="Table 8 ‣ B.2 Dataset statistics ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">8</span></a> shows the number of samples contained in the training and test splits, respectively, for each of the 20 Wikipedia language versions considered in the experiments.</p>
</div>
<figure class="ltx_table" id="A2.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Summary statistics of the train and test sets for 20 Wikipedia language versions considered in the experiments.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T8.1" style="width:303.5pt;height:385.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.1pt,5.2pt) scale(0.973986809377387,0.973986809377387) ;">
<table class="ltx_tabular ltx_align_middle" id="A2.T8.1.1">
<tr class="ltx_tr" id="A2.T8.1.1.1">
<td class="ltx_td" id="A2.T8.1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.1.2" rowspan="2"><span class="ltx_text" id="A2.T8.1.1.1.2.1">Language</span></td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.1.3" rowspan="2"><span class="ltx_text" id="A2.T8.1.1.1.3.1">Articles</span></td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.1.4" rowspan="2"><span class="ltx_text" id="A2.T8.1.1.1.4.1">Existing Links</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="A2.T8.1.1.1.5">Added Links</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.2">
<td class="ltx_td" id="A2.T8.1.1.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.2.2">Train</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.2.3">Test</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.1">en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T8.1.1.3.2">English</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.3">6.7M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.4">166M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.5">552K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.6">416K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.4">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.4.1">fr</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.4.2">French</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.4.3">2.5M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.4.4">85M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.4.5">130K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.4.6">76K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.5">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.5.1">it</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.5.2">Italian</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.5.3">1.8M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.5.4">51M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.5.5">101K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.5.6">56K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.6">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.6.1">ja</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.6.2">Japanese</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.6.3">1.4M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.6.4">61M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.6.5">150K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.6.6">111K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.7">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.7.1">pt</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.7.2">Portuguese</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.7.3">1.1M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.7.4">22M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.7.5">54K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.7.6">32K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.8">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.8.1">cs</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.8.2">Czech</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.8.3">526K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.8.4">14M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.8.5">27K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.8.6">15K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.9">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.9.1">ms</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.9.2">Malay</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.9.3">362K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.9.4">2.9M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.9.5">6K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.9.6">3K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.10">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.10.1">cy</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.10.2">Welsh</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.10.3">269K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.10.4">2.7M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.10.5">1K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.10.6">455</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.11">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.11.1">sk</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.11.2">Slovak</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.11.3">240K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.11.4">3.4M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.11.5">7K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.11.6">4.3K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.12">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.12.1">simple</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.12.2">Simple English</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.12.3">238K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.12.4">2.6M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.12.5">9.4K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.12.6">4.8K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.13">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.13.1">kk</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.13.2">Kazakh</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.13.3">232K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.13.4">1.6M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.13.5">2.7K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.13.6">2.0K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.14">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.14.1">uz</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.14.2">Uzbek</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.14.3">224K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.14.4">1.3M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.14.5">12K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.14.6">5.9K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.15">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.15.1">ur</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.15.2">Urdu</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.15.3">188K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.15.4">1.4M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.15.5">14K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.15.6">7.5K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.16">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.16.1">hi</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.16.2">Hindi</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.16.3">155K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.16.4">1.2M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.16.5">3.2K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.16.6">3.2K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.17">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.17.1">af</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.17.2">Afrikaans</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.17.3">111K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.17.4">1.4M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.17.5">3.3K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.17.6">1.7K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.18">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.18.1">sw</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.18.2">Swahili</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.18.3">73K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.18.4">1.0M</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.18.5">1.1K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.18.6">616</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.19">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.19.1">ga</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.19.2">Irish</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.19.3">56K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.19.4">380K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.19.5">849</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.19.6">256</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.20">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.20.1">is</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.20.2">Icelandic</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.20.3">51K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.20.4">610K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.20.5">1.6K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.20.6">1.2K</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.21">
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.21.1">gu</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T8.1.1.21.2">Gujarati</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.21.3">30K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.21.4">410K</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.21.5">197</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.21.6">48</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.22">
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T8.1.1.22.1">kn</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T8.1.1.22.2">Kannada</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T8.1.1.22.3">27K</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T8.1.1.22.4">250K</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T8.1.1.22.5">1.1K</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T8.1.1.22.6">609</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Entity insertion categories</h3>
<div class="ltx_para" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.T9" title="Table 9 ‣ B.3 Entity insertion categories ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">9</span></a> shows an example for each of the entity insertion categories, except for the category <span class="ltx_text ltx_font_typewriter" id="A2.SS3.p1.1.1">missing_section</span>, demonstrating that the problem of entity insertion grows in complexity as more text is missing.</p>
</div>
<figure class="ltx_table" id="A2.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Examples of different entity insertion categories observed when adding links in Wikipedia. The added link is marked in <span class="ltx_text" id="A2.T9.2.1" style="color:#0000FF;">blue</span>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A2.T9.3">
<tr class="ltx_tr" id="A2.T9.3.1">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T9.3.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Strategy</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T9.3.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T9.3.1.2.1">
<span class="ltx_p" id="A2.T9.3.1.2.1.1" style="width:169.8pt;">First Version Text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T9.3.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T9.3.1.3.1">
<span class="ltx_p" id="A2.T9.3.1.3.1.1" style="width:169.8pt;">Second Version Text</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T9.3.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T9.3.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Text Present</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T9.3.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T9.3.2.2.1">
<span class="ltx_p" id="A2.T9.3.2.2.1.1" style="width:169.8pt;">It is best eaten when it is somewhat below normal room temperature. In most countries, brie-style cheeses are made with Pasteurized milk.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T9.3.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T9.3.2.3.1">
<span class="ltx_p" id="A2.T9.3.2.3.1.1" style="width:169.8pt;">It is best eaten when it is somewhat below normal <span class="ltx_text" id="A2.T9.3.2.3.1.1.1" style="color:#0000FF;">room temperature</span>. In most countries, brie-style cheeses are made with Pasteurized milk.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T9.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T9.3.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Missing Mention</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T9.3.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T9.3.3.2.1">
<span class="ltx_p" id="A2.T9.3.3.2.1.1" style="width:169.8pt;">Vercetti Regular, also known as Vercetti, is a free font that can be used for both commercial and personal purposes. It became available in 2022 under the Licence Amicale, which allows users to share the font files with friends and colleagues.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T9.3.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T9.3.3.3.1">
<span class="ltx_p" id="A2.T9.3.3.3.1.1" style="width:169.8pt;">Vercetti Regular, also known as Vercetti, is a free font (<span class="ltx_text" id="A2.T9.3.3.3.1.1.1" style="color:#0000FF;">freeware</span>) that can be used for both commercial and personal purposes. It became available in 2022 under the Licence Amicale, which allows users to share the font files with friends and colleagues.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T9.3.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T9.3.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Missing Sentence</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T9.3.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T9.3.4.2.1">
<span class="ltx_p" id="A2.T9.3.4.2.1.1" style="width:169.8pt;"><span class="ltx_text" id="A2.T9.3.4.2.1.1.1" style="color:#FFFFFF;">Kivi was born in Nurmijärvi.</span> Kivi lived in time when all educated people in Finland spoke Swedish. He was the first professional writer who published his works in Finnish. Kivi, Mikael Agricola and Elias Lönnrot are regarded fathers of a national literature in Finnish.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T9.3.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T9.3.4.3.1">
<span class="ltx_p" id="A2.T9.3.4.3.1.1" style="width:169.8pt;">Kivi was born in <span class="ltx_text" id="A2.T9.3.4.3.1.1.1" style="color:#0000FF;">Nurmijärvi</span>. He lived in time when all educated people in Finland spoke Swedish. He was the first professional writer who published his works in Finnish. Kivi, Mikael Agricola and Elias Lönnrot are regarded fathers of a national literature in Finnish.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T9.3.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A2.T9.3.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Missing Span</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A2.T9.3.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T9.3.5.2.1">
<span class="ltx_p" id="A2.T9.3.5.2.1.1" style="width:169.8pt;">The game will be released for Windows PC, Mac and Linux, with Nintendo Switch being the only console to receive the game at launch. 
<br class="ltx_break"/><span class="ltx_text" id="A2.T9.3.5.2.1.1.1" style="color:#FFFFFF;">During the Xbox &amp; Bethesda Games Showcase, it was revealed that the game would be coming to Xbox Game Pass through PC and Xbox Series X/S. It was also revealed that the game would be coming to PlayStation 4 and PlayStation 5.</span>
<br class="ltx_break"/>Originally, Hornet was planned as a second playable character to be included in a downloadable content pack (DLC) for Hollow Knight, funded as a stretch goal in the game’s Kickstarter campaign.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A2.T9.3.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T9.3.5.3.1">
<span class="ltx_p" id="A2.T9.3.5.3.1.1" style="width:169.8pt;">The game will be released for Windows PC, Mac and Linux, with Nintendo Switch being the only console to receive the game at launch. 
<br class="ltx_break"/>During the Xbox &amp; Bethesda Games Showcase, it was revealed that the game would be coming to Xbox Game Pass through PC and Xbox Series X/S. It was also revealed that the game would be coming to PlayStation 4 and <span class="ltx_text" id="A2.T9.3.5.3.1.1.1" style="color:#0000FF;">PlayStation 5</span>. 
<br class="ltx_break"/>Originally, Hornet was planned as a second playable character to be included in a downloadable content pack (DLC) for Hollow Knight, funded as a stretch goal in the game’s Kickstarter campaign.</span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="A2.SS3.p2">
<p class="ltx_p" id="A2.SS3.p2.1">Additionally, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.F5" title="Figure 5 ‣ B.3 Entity insertion categories ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5</span></a> shows the distribution of entity insertion categories for 20 Wikipedia language versions considered in the experiments.</p>
</div>
<figure class="ltx_figure" id="A2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="A2.F5.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The distribution of entity insertion categories across the 20 considered Wikipedia language versions from October to November 2023. The x-axis shows the language code and the number of links added in each language.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Dynamic context removal</h3>
<div class="ltx_para" id="A2.SS4.p1">
<p class="ltx_p" id="A2.SS4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A2.T10" title="Table 10 ‣ B.4 Dynamic context removal ‣ Appendix B Additional dataset processing details ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">10</span></a> shows examples of the different types of dynamic context removal. Specifically, we randomly remove a word (<span class="ltx_text ltx_font_typewriter" id="A2.SS4.p1.1.1">rm_mention</span> simulation), a sentence (<span class="ltx_text ltx_font_typewriter" id="A2.SS4.p1.1.2">rm_sent</span> simulation), or a span of sentences (<span class="ltx_text ltx_font_typewriter" id="A2.SS4.p1.1.3">rm_span</span> simulation) during training. Before sending the input to the model, we randomly select one of the masking strategies mentioned above (as well as no masking) to modify the input accordingly. However, before applying the strategy, we verify if the selected strategy does not produce an empty input. This may happen when, for example, the context is a single sentence, in which case simulating the <span class="ltx_text ltx_font_typewriter" id="A2.SS4.p1.1.4">rm_sent</span> strategy would lead to an empty input. If the sampled strategy would produce an empty input, we re-sample a less aggressive strategy.</p>
</div>
<div class="ltx_para" id="A2.SS4.p2">
<p class="ltx_p" id="A2.SS4.p2.2">While performing the <span class="ltx_text ltx_font_typewriter" id="A2.SS4.p2.2.1">rm_span</span> simulation, the number of sentences to remove is chosen randomly between <math alttext="2" class="ltx_Math" display="inline" id="A2.SS4.p2.1.m1.1"><semantics id="A2.SS4.p2.1.m1.1a"><mn id="A2.SS4.p2.1.m1.1.1" xref="A2.SS4.p2.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A2.SS4.p2.1.m1.1b"><cn id="A2.SS4.p2.1.m1.1.1.cmml" type="integer" xref="A2.SS4.p2.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS4.p2.1.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="A2.SS4.p2.1.m1.1d">2</annotation></semantics></math> and <math alttext="5" class="ltx_Math" display="inline" id="A2.SS4.p2.2.m2.1"><semantics id="A2.SS4.p2.2.m2.1a"><mn id="A2.SS4.p2.2.m2.1.1" xref="A2.SS4.p2.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="A2.SS4.p2.2.m2.1b"><cn id="A2.SS4.p2.2.m2.1.1.cmml" type="integer" xref="A2.SS4.p2.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS4.p2.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="A2.SS4.p2.2.m2.1d">5</annotation></semantics></math>. Note that we used a space-based splitting for ease of implementation, and we acknowledge that this could be an issue for certain languages, such as Japanese or Chinese, which we intend to fix in the future.</p>
</div>
<figure class="ltx_table" id="A2.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Examples of different strategies for dynamic context removal. The mention of the target link is marked in <span class="ltx_text" id="A2.T10.2.1" style="color:#0000FF;">blue</span>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A2.T10.3">
<tr class="ltx_tr" id="A2.T10.3.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A2.T10.3.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.1.1.1">
<span class="ltx_p" id="A2.T10.3.1.1.1.1" style="width:86.7pt;">Strategy</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T10.3.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.1.2.1">
<span class="ltx_p" id="A2.T10.3.1.2.1.1" style="width:166.2pt;">Original Text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T10.3.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.1.3.1">
<span class="ltx_p" id="A2.T10.3.1.3.1.1" style="width:166.2pt;">Modified Text</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T10.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T10.3.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.2.1.1">
<span class="ltx_p" id="A2.T10.3.2.1.1.1" style="width:86.7pt;">No removal
<br class="ltx_break"/>(<span class="ltx_text ltx_font_typewriter" id="A2.T10.3.2.1.1.1.1">rm_nth</span>)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T10.3.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.2.2.1">
<span class="ltx_p" id="A2.T10.3.2.2.1.1" style="width:166.2pt;">Pulaski County is a county located in the central portion of the U.S. state of Georgia. As of the 2020 census, the population was 9,855. The <span class="ltx_text" id="A2.T10.3.2.2.1.1.1" style="color:#0000FF;">county seat</span> is Hawkinsville.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T10.3.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.2.3.1">
<span class="ltx_p" id="A2.T10.3.2.3.1.1" style="width:166.2pt;">Pulaski County is a county located in the central portion of the U.S. state of Georgia. As of the 2020 census, the population was 9,855. The county seat is Hawkinsville.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T10.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A2.T10.3.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.3.1.1">
<span class="ltx_p" id="A2.T10.3.3.1.1.1" style="width:86.7pt;">Mention removal
<br class="ltx_break"/>(<span class="ltx_text ltx_font_typewriter" id="A2.T10.3.3.1.1.1.1">rm_mention</span>)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T10.3.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.3.2.1">
<span class="ltx_p" id="A2.T10.3.3.2.1.1" style="width:166.2pt;">Perthes-lès-Brienne is a commune of the Aube <span class="ltx_text" id="A2.T10.3.3.2.1.1.1" style="color:#0000FF;">département</span> in the north-central part of France.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T10.3.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.3.3.1">
<span class="ltx_p" id="A2.T10.3.3.3.1.1" style="width:166.2pt;">Perthes-lès-Brienne is a commune of the Aube    in the north-central part of France.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T10.3.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A2.T10.3.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.4.1.1">
<span class="ltx_p" id="A2.T10.3.4.1.1.1" style="width:86.7pt;">Sentence removal
<br class="ltx_break"/>(<span class="ltx_text ltx_font_typewriter" id="A2.T10.3.4.1.1.1.1">rm_sent</span>)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T10.3.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.4.2.1">
<span class="ltx_p" id="A2.T10.3.4.2.1.1" style="width:166.2pt;">In this Japanese name, the family name is Fujita. Yoshiaki Fujita (born <span class="ltx_text" id="A2.T10.3.4.2.1.1.1" style="color:#0000FF;">12 January</span> 1983) is a Japanese football player. He plays for Oita Trinita.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T10.3.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.4.3.1">
<span class="ltx_p" id="A2.T10.3.4.3.1.1" style="width:166.2pt;">In this Japanese name, the family name is Fujita. 
<br class="ltx_break"/>
<br class="ltx_break"/>     He plays for Oita Trinita.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T10.3.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="A2.T10.3.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.5.1.1">
<span class="ltx_p" id="A2.T10.3.5.1.1.1" style="width:86.7pt;">Span removal
<br class="ltx_break"/>(<span class="ltx_text ltx_font_typewriter" id="A2.T10.3.5.1.1.1.1">rm_span</span>)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="A2.T10.3.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.5.2.1">
<span class="ltx_p" id="A2.T10.3.5.2.1.1" style="width:166.2pt;">Administration
<br class="ltx_break"/>The department of French Guiana is managed by the Collectivité territorial de la Guyane in Cayenne. There are 2 <span class="ltx_text" id="A2.T10.3.5.2.1.1.1" style="color:#0000FF;">arrondissements</span> (districts) and 22 communes (municipalities) in French Guiana. The cantons of the department were eliminated on 31 December 2015 by the Law 2011-884 of 27 July 2011.
<br class="ltx_break"/>The 22 communes in the department are:</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="A2.T10.3.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T10.3.5.3.1">
<span class="ltx_p" id="A2.T10.3.5.3.1.1" style="width:166.2pt;">Administration
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/>The 22 communes in the department are:</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.5 </span>Rules for sampling negative candidates</h3>
<div class="ltx_para" id="A2.SS5.p1">
<p class="ltx_p" id="A2.SS5.p1.1">We employ the following rules when constructing the negative candidates, both for training and validation.</p>
<ol class="ltx_enumerate" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1">A candidate’s context should not span over two different sections.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.1">A candidate’s context should not contain any of the mentions previously used to link to the target entity.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="A2.SS5.p2">
<p class="ltx_p" id="A2.SS5.p2.1">The first rule keeps the content of each context consistent, as two distinct sections can cover very different topics. The second rule ensures that all the candidates used to evaluate the module are correctly classified as either positive candidates or negative candidates. For example, if the goal is to insert the entity “1984” (the book - <code class="ltx_verbatim ltx_font_typewriter" id="A2.SS5.p2.1.1">Q208460</code>) and there is a sentence in the article with the word “1984” not linked to the target article, there could be three reasons for the link to be missing. First, the mention “1984” could be related to a different entity (e.g., the year - <code class="ltx_verbatim ltx_font_typewriter" id="A2.SS5.p2.1.2">Q2432</code>), in which case the sentence should belong to a negative candidate. Second, the mention is supposed to be for the target entity but it is not yet linked, in which case the sentence should belong to an additional positive candidate. Finally, the mention is supposed to be for the target entity but it should not be linked because of Wikipedia’s editing guidelines, in which case it is not clear whether the sentence should belong to a negative or a positive candidate. Due to this unclear categorization, we choose to remove any sentences containing mentions previously associated with the target entity to be inserted.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional experiments</h2>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Hyperparameters</h3>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.8">We train the encoder and MLP with learning rates of <math alttext="1e-5" class="ltx_Math" display="inline" id="A3.SS1.p1.1.m1.1"><semantics id="A3.SS1.p1.1.m1.1a"><mrow id="A3.SS1.p1.1.m1.1.1" xref="A3.SS1.p1.1.m1.1.1.cmml"><mrow id="A3.SS1.p1.1.m1.1.1.2" xref="A3.SS1.p1.1.m1.1.1.2.cmml"><mn id="A3.SS1.p1.1.m1.1.1.2.2" xref="A3.SS1.p1.1.m1.1.1.2.2.cmml">1</mn><mo id="A3.SS1.p1.1.m1.1.1.2.1" xref="A3.SS1.p1.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A3.SS1.p1.1.m1.1.1.2.3" xref="A3.SS1.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A3.SS1.p1.1.m1.1.1.1" xref="A3.SS1.p1.1.m1.1.1.1.cmml">−</mo><mn id="A3.SS1.p1.1.m1.1.1.3" xref="A3.SS1.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.1.m1.1b"><apply id="A3.SS1.p1.1.m1.1.1.cmml" xref="A3.SS1.p1.1.m1.1.1"><minus id="A3.SS1.p1.1.m1.1.1.1.cmml" xref="A3.SS1.p1.1.m1.1.1.1"></minus><apply id="A3.SS1.p1.1.m1.1.1.2.cmml" xref="A3.SS1.p1.1.m1.1.1.2"><times id="A3.SS1.p1.1.m1.1.1.2.1.cmml" xref="A3.SS1.p1.1.m1.1.1.2.1"></times><cn id="A3.SS1.p1.1.m1.1.1.2.2.cmml" type="integer" xref="A3.SS1.p1.1.m1.1.1.2.2">1</cn><ci id="A3.SS1.p1.1.m1.1.1.2.3.cmml" xref="A3.SS1.p1.1.m1.1.1.2.3">𝑒</ci></apply><cn id="A3.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="A3.SS1.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.1.m1.1c">1e-5</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.1.m1.1d">1 italic_e - 5</annotation></semantics></math> and <math alttext="1e-4" class="ltx_Math" display="inline" id="A3.SS1.p1.2.m2.1"><semantics id="A3.SS1.p1.2.m2.1a"><mrow id="A3.SS1.p1.2.m2.1.1" xref="A3.SS1.p1.2.m2.1.1.cmml"><mrow id="A3.SS1.p1.2.m2.1.1.2" xref="A3.SS1.p1.2.m2.1.1.2.cmml"><mn id="A3.SS1.p1.2.m2.1.1.2.2" xref="A3.SS1.p1.2.m2.1.1.2.2.cmml">1</mn><mo id="A3.SS1.p1.2.m2.1.1.2.1" xref="A3.SS1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="A3.SS1.p1.2.m2.1.1.2.3" xref="A3.SS1.p1.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="A3.SS1.p1.2.m2.1.1.1" xref="A3.SS1.p1.2.m2.1.1.1.cmml">−</mo><mn id="A3.SS1.p1.2.m2.1.1.3" xref="A3.SS1.p1.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.2.m2.1b"><apply id="A3.SS1.p1.2.m2.1.1.cmml" xref="A3.SS1.p1.2.m2.1.1"><minus id="A3.SS1.p1.2.m2.1.1.1.cmml" xref="A3.SS1.p1.2.m2.1.1.1"></minus><apply id="A3.SS1.p1.2.m2.1.1.2.cmml" xref="A3.SS1.p1.2.m2.1.1.2"><times id="A3.SS1.p1.2.m2.1.1.2.1.cmml" xref="A3.SS1.p1.2.m2.1.1.2.1"></times><cn id="A3.SS1.p1.2.m2.1.1.2.2.cmml" type="integer" xref="A3.SS1.p1.2.m2.1.1.2.2">1</cn><ci id="A3.SS1.p1.2.m2.1.1.2.3.cmml" xref="A3.SS1.p1.2.m2.1.1.2.3">𝑒</ci></apply><cn id="A3.SS1.p1.2.m2.1.1.3.cmml" type="integer" xref="A3.SS1.p1.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.2.m2.1c">1e-4</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.2.m2.1d">1 italic_e - 4</annotation></semantics></math>, respectively, using <math alttext="N=9" class="ltx_Math" display="inline" id="A3.SS1.p1.3.m3.1"><semantics id="A3.SS1.p1.3.m3.1a"><mrow id="A3.SS1.p1.3.m3.1.1" xref="A3.SS1.p1.3.m3.1.1.cmml"><mi id="A3.SS1.p1.3.m3.1.1.2" xref="A3.SS1.p1.3.m3.1.1.2.cmml">N</mi><mo id="A3.SS1.p1.3.m3.1.1.1" xref="A3.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="A3.SS1.p1.3.m3.1.1.3" xref="A3.SS1.p1.3.m3.1.1.3.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.3.m3.1b"><apply id="A3.SS1.p1.3.m3.1.1.cmml" xref="A3.SS1.p1.3.m3.1.1"><eq id="A3.SS1.p1.3.m3.1.1.1.cmml" xref="A3.SS1.p1.3.m3.1.1.1"></eq><ci id="A3.SS1.p1.3.m3.1.1.2.cmml" xref="A3.SS1.p1.3.m3.1.1.2">𝑁</ci><cn id="A3.SS1.p1.3.m3.1.1.3.cmml" type="integer" xref="A3.SS1.p1.3.m3.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.3.m3.1c">N=9</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.3.m3.1d">italic_N = 9</annotation></semantics></math> negative candidates. Moreover, we use <math alttext="5" class="ltx_Math" display="inline" id="A3.SS1.p1.4.m4.1"><semantics id="A3.SS1.p1.4.m4.1a"><mn id="A3.SS1.p1.4.m4.1.1" xref="A3.SS1.p1.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.4.m4.1b"><cn id="A3.SS1.p1.4.m4.1.1.cmml" type="integer" xref="A3.SS1.p1.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.4.m4.1c">5</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.4.m4.1d">5</annotation></semantics></math> sentences on either side as context for each candidate text span and set <math alttext="|M_{tgt}|=10" class="ltx_Math" display="inline" id="A3.SS1.p1.5.m5.1"><semantics id="A3.SS1.p1.5.m5.1a"><mrow id="A3.SS1.p1.5.m5.1.1" xref="A3.SS1.p1.5.m5.1.1.cmml"><mrow id="A3.SS1.p1.5.m5.1.1.1.1" xref="A3.SS1.p1.5.m5.1.1.1.2.cmml"><mo id="A3.SS1.p1.5.m5.1.1.1.1.2" stretchy="false" xref="A3.SS1.p1.5.m5.1.1.1.2.1.cmml">|</mo><msub id="A3.SS1.p1.5.m5.1.1.1.1.1" xref="A3.SS1.p1.5.m5.1.1.1.1.1.cmml"><mi id="A3.SS1.p1.5.m5.1.1.1.1.1.2" xref="A3.SS1.p1.5.m5.1.1.1.1.1.2.cmml">M</mi><mrow id="A3.SS1.p1.5.m5.1.1.1.1.1.3" xref="A3.SS1.p1.5.m5.1.1.1.1.1.3.cmml"><mi id="A3.SS1.p1.5.m5.1.1.1.1.1.3.2" xref="A3.SS1.p1.5.m5.1.1.1.1.1.3.2.cmml">t</mi><mo id="A3.SS1.p1.5.m5.1.1.1.1.1.3.1" xref="A3.SS1.p1.5.m5.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p1.5.m5.1.1.1.1.1.3.3" xref="A3.SS1.p1.5.m5.1.1.1.1.1.3.3.cmml">g</mi><mo id="A3.SS1.p1.5.m5.1.1.1.1.1.3.1a" xref="A3.SS1.p1.5.m5.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p1.5.m5.1.1.1.1.1.3.4" xref="A3.SS1.p1.5.m5.1.1.1.1.1.3.4.cmml">t</mi></mrow></msub><mo id="A3.SS1.p1.5.m5.1.1.1.1.3" stretchy="false" xref="A3.SS1.p1.5.m5.1.1.1.2.1.cmml">|</mo></mrow><mo id="A3.SS1.p1.5.m5.1.1.2" xref="A3.SS1.p1.5.m5.1.1.2.cmml">=</mo><mn id="A3.SS1.p1.5.m5.1.1.3" xref="A3.SS1.p1.5.m5.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.5.m5.1b"><apply id="A3.SS1.p1.5.m5.1.1.cmml" xref="A3.SS1.p1.5.m5.1.1"><eq id="A3.SS1.p1.5.m5.1.1.2.cmml" xref="A3.SS1.p1.5.m5.1.1.2"></eq><apply id="A3.SS1.p1.5.m5.1.1.1.2.cmml" xref="A3.SS1.p1.5.m5.1.1.1.1"><abs id="A3.SS1.p1.5.m5.1.1.1.2.1.cmml" xref="A3.SS1.p1.5.m5.1.1.1.1.2"></abs><apply id="A3.SS1.p1.5.m5.1.1.1.1.1.cmml" xref="A3.SS1.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="A3.SS1.p1.5.m5.1.1.1.1.1.1.cmml" xref="A3.SS1.p1.5.m5.1.1.1.1.1">subscript</csymbol><ci id="A3.SS1.p1.5.m5.1.1.1.1.1.2.cmml" xref="A3.SS1.p1.5.m5.1.1.1.1.1.2">𝑀</ci><apply id="A3.SS1.p1.5.m5.1.1.1.1.1.3.cmml" xref="A3.SS1.p1.5.m5.1.1.1.1.1.3"><times id="A3.SS1.p1.5.m5.1.1.1.1.1.3.1.cmml" xref="A3.SS1.p1.5.m5.1.1.1.1.1.3.1"></times><ci id="A3.SS1.p1.5.m5.1.1.1.1.1.3.2.cmml" xref="A3.SS1.p1.5.m5.1.1.1.1.1.3.2">𝑡</ci><ci id="A3.SS1.p1.5.m5.1.1.1.1.1.3.3.cmml" xref="A3.SS1.p1.5.m5.1.1.1.1.1.3.3">𝑔</ci><ci id="A3.SS1.p1.5.m5.1.1.1.1.1.3.4.cmml" xref="A3.SS1.p1.5.m5.1.1.1.1.1.3.4">𝑡</ci></apply></apply></apply><cn id="A3.SS1.p1.5.m5.1.1.3.cmml" type="integer" xref="A3.SS1.p1.5.m5.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.5.m5.1c">|M_{tgt}|=10</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.5.m5.1d">| italic_M start_POSTSUBSCRIPT italic_t italic_g italic_t end_POSTSUBSCRIPT | = 10</annotation></semantics></math>. The first stage of training uses <math alttext="20K" class="ltx_Math" display="inline" id="A3.SS1.p1.6.m6.1"><semantics id="A3.SS1.p1.6.m6.1a"><mrow id="A3.SS1.p1.6.m6.1.1" xref="A3.SS1.p1.6.m6.1.1.cmml"><mn id="A3.SS1.p1.6.m6.1.1.2" xref="A3.SS1.p1.6.m6.1.1.2.cmml">20</mn><mo id="A3.SS1.p1.6.m6.1.1.1" xref="A3.SS1.p1.6.m6.1.1.1.cmml">⁢</mo><mi id="A3.SS1.p1.6.m6.1.1.3" xref="A3.SS1.p1.6.m6.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.6.m6.1b"><apply id="A3.SS1.p1.6.m6.1.1.cmml" xref="A3.SS1.p1.6.m6.1.1"><times id="A3.SS1.p1.6.m6.1.1.1.cmml" xref="A3.SS1.p1.6.m6.1.1.1"></times><cn id="A3.SS1.p1.6.m6.1.1.2.cmml" type="integer" xref="A3.SS1.p1.6.m6.1.1.2">20</cn><ci id="A3.SS1.p1.6.m6.1.1.3.cmml" xref="A3.SS1.p1.6.m6.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.6.m6.1c">20K</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.6.m6.1d">20 italic_K</annotation></semantics></math> data points and is trained for <math alttext="4" class="ltx_Math" display="inline" id="A3.SS1.p1.7.m7.1"><semantics id="A3.SS1.p1.7.m7.1a"><mn id="A3.SS1.p1.7.m7.1.1" xref="A3.SS1.p1.7.m7.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.7.m7.1b"><cn id="A3.SS1.p1.7.m7.1.1.cmml" type="integer" xref="A3.SS1.p1.7.m7.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.7.m7.1c">4</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.7.m7.1d">4</annotation></semantics></math> epochs, whereas the second stage uses all the available data for <math alttext="2" class="ltx_Math" display="inline" id="A3.SS1.p1.8.m8.1"><semantics id="A3.SS1.p1.8.m8.1a"><mn id="A3.SS1.p1.8.m8.1.1" xref="A3.SS1.p1.8.m8.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.8.m8.1b"><cn id="A3.SS1.p1.8.m8.1.1.cmml" type="integer" xref="A3.SS1.p1.8.m8.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.8.m8.1c">2</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.8.m8.1d">2</annotation></semantics></math> epochs. Mimicking the real-world entity insertion scenarios, we set <span class="ltx_text ltx_font_typewriter" id="A3.SS1.p1.8.1">rm_nth</span>=40%, <span class="ltx_text ltx_font_typewriter" id="A3.SS1.p1.8.2">rm_mention</span>=20%, <span class="ltx_text ltx_font_typewriter" id="A3.SS1.p1.8.3">rm_sentence</span>=30%, and <span class="ltx_text ltx_font_typewriter" id="A3.SS1.p1.8.4">rm_span</span>=10%.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Multilingual entity insertion stratified by language</h3>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">Figs. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.F6" title="Figure 6 ‣ C.2 Multilingual entity insertion stratified by language ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.F7" title="Figure 7 ‣ C.2 Multilingual entity insertion stratified by language ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">7</span></a> portray the entity insertion performance stratified by language of all the benchmarked methods using hits@1 and MRR, respectively. <span class="ltx_text" id="A3.SS2.p1.1.1" style="color:#000000;">The results clearly show that, as entity insertion becomes more complex, the baselines start to decrease in performance, being significantly outperformed by <span class="ltx_text ltx_font_smallcaps" id="A3.SS2.p1.1.1.1">LocEI</span> and <span class="ltx_text ltx_font_smallcaps" id="A3.SS2.p1.1.1.2">xLocEI</span>.</span></p>
</div>
<figure class="ltx_figure" id="A3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1013" id="A3.F6.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Entity insertion performance across all 20 Wikipedia language versions measured using hits@1. <span class="ltx_text ltx_font_smallcaps" id="A3.F6.2.1">xLocEI</span> trains a single model jointly on all 20 languages, whereas other methods train a separate model for each language. The categorization of entity insertion types is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1013" id="A3.F7.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Entity insertion performance across all 20 Wikipedia language versions measured using MRR. <span class="ltx_text ltx_font_smallcaps" id="A3.F7.2.1">xLocEI</span> trains a single model jointly on all 20 languages, whereas other methods train a separate model for each language. The categorization of entity insertion types is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Zero-shot entity insertion stratified by language</h3>
<div class="ltx_para" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.T11" title="Table 11 ‣ C.3 Zero-shot entity insertion stratified by language ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">11</span></a> provides additional details about the data such as the languages and the size of the datasets, used to train the different variants of the multilingual models employed in the zero-shot setting.</p>
</div>
<figure class="ltx_table" id="A3.T11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Details about the languages and size of the dataset used to train the two <span class="ltx_text ltx_font_smallcaps" id="A3.T11.10.3">xLocEI</span> model variants, i.e., <span class="ltx_text ltx_font_smallcaps" id="A3.T11.3.1">xLocEI<sub class="ltx_sub" id="A3.T11.3.1.1"><span class="ltx_text ltx_font_italic" id="A3.T11.3.1.1.1">20</span></sub></span> and <span class="ltx_text ltx_font_smallcaps" id="A3.T11.4.2">xLocEI<sub class="ltx_sub" id="A3.T11.4.2.1"><span class="ltx_text ltx_font_italic" id="A3.T11.4.2.1.1">11</span></sub></span>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T11.8" style="width:424.9pt;height:106.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(14.2pt,-3.6pt) scale(1.07179818827707,1.07179818827707) ;">
<table class="ltx_tabular ltx_align_middle" id="A3.T11.8.4">
<tr class="ltx_tr" id="A3.T11.8.4.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T11.8.4.5.1" rowspan="2"><span class="ltx_text" id="A3.T11.8.4.5.1.1">Model</span></td>
<td class="ltx_td ltx_align_center" id="A3.T11.8.4.5.2" rowspan="2"><span class="ltx_text" id="A3.T11.8.4.5.2.1">Starting Model</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T11.8.4.5.3" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="A3.T11.8.4.5.3.1">
<span class="ltx_p" id="A3.T11.8.4.5.3.1.1" style="width:144.5pt;"><span class="ltx_text" id="A3.T11.8.4.5.3.1.1.1">Fine-Tuned Languages</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" colspan="2" id="A3.T11.8.4.5.4">Training Data Size</td>
</tr>
<tr class="ltx_tr" id="A3.T11.8.4.6">
<td class="ltx_td ltx_align_center" id="A3.T11.8.4.6.1">Stage 1</td>
<td class="ltx_td ltx_align_center" id="A3.T11.8.4.6.2">Stage 2</td>
</tr>
<tr class="ltx_tr" id="A3.T11.6.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T11.5.1.1.1">xLocEI<sub class="ltx_sub" id="A3.T11.5.1.1.1.1"><span class="ltx_text ltx_font_italic" id="A3.T11.5.1.1.1.1.1">20</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.6.2.2.2">XLM-RoBERTa<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.T11.6.2.2.2.m1.1"><semantics id="A3.T11.6.2.2.2.m1.1a"><msub id="A3.T11.6.2.2.2.m1.1.1" xref="A3.T11.6.2.2.2.m1.1.1.cmml"><mi id="A3.T11.6.2.2.2.m1.1.1a" xref="A3.T11.6.2.2.2.m1.1.1.cmml"></mi><mtext id="A3.T11.6.2.2.2.m1.1.1.1" xref="A3.T11.6.2.2.2.m1.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T11.6.2.2.2.m1.1b"><apply id="A3.T11.6.2.2.2.m1.1.1.cmml" xref="A3.T11.6.2.2.2.m1.1.1"><ci id="A3.T11.6.2.2.2.m1.1.1.1a.cmml" xref="A3.T11.6.2.2.2.m1.1.1.1"><mtext id="A3.T11.6.2.2.2.m1.1.1.1.cmml" mathsize="70%" xref="A3.T11.6.2.2.2.m1.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T11.6.2.2.2.m1.1c">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.T11.6.2.2.2.m1.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T11.6.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="A3.T11.6.2.2.3.1">
<span class="ltx_p" id="A3.T11.6.2.2.3.1.1" style="width:144.5pt;">en, fr, it, ja, pt, cs, ms, cy, sk, uz, simple, kk, ur, hi, af, sw, ga, is, kn, gu</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.6.2.2.4">20K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.6.2.2.5">503K</td>
</tr>
<tr class="ltx_tr" id="A3.T11.8.4.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A3.T11.7.3.3.1">xLocEI<sub class="ltx_sub" id="A3.T11.7.3.3.1.1"><span class="ltx_text ltx_font_italic" id="A3.T11.7.3.3.1.1.1">11</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T11.8.4.4.2">XLM-RoBERTa<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.T11.8.4.4.2.m1.1"><semantics id="A3.T11.8.4.4.2.m1.1a"><msub id="A3.T11.8.4.4.2.m1.1.1" xref="A3.T11.8.4.4.2.m1.1.1.cmml"><mi id="A3.T11.8.4.4.2.m1.1.1a" xref="A3.T11.8.4.4.2.m1.1.1.cmml"></mi><mtext id="A3.T11.8.4.4.2.m1.1.1.1" xref="A3.T11.8.4.4.2.m1.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T11.8.4.4.2.m1.1b"><apply id="A3.T11.8.4.4.2.m1.1.1.cmml" xref="A3.T11.8.4.4.2.m1.1.1"><ci id="A3.T11.8.4.4.2.m1.1.1.1a.cmml" xref="A3.T11.8.4.4.2.m1.1.1.1"><mtext id="A3.T11.8.4.4.2.m1.1.1.1.cmml" mathsize="70%" xref="A3.T11.8.4.4.2.m1.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T11.8.4.4.2.m1.1c">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.T11.8.4.4.2.m1.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="A3.T11.8.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="A3.T11.8.4.4.3.1">
<span class="ltx_p" id="A3.T11.8.4.4.3.1.1" style="width:144.5pt;">en, it, ja, cs, cy, uz, ur, hi, sw, is, kn</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T11.8.4.4.4">20K</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T11.8.4.4.5">348K</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="A3.SS3.p2">
<p class="ltx_p" id="A3.SS3.p2.1">Figs. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.F8" title="Figure 8 ‣ C.3 Zero-shot entity insertion stratified by language ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">8</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.F9" title="Figure 9 ‣ C.3 Zero-shot entity insertion stratified by language ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">9</span></a> portray the zero-shot entity insertion performance stratified by language of all the benchmarked methods using hits@1 and MRR, respectively.</p>
</div>
<figure class="ltx_figure" id="A3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="A3.F8.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Entity insertion performance measured using hits@1 in the zero-shot setting: results across 9 Wikipedia language versions that were not used for fine-tuning <span class="ltx_text ltx_font_smallcaps" id="A3.F8.3.1">xLocEI<sub class="ltx_sub" id="A3.F8.3.1.1"><span class="ltx_text ltx_font_italic" id="A3.F8.3.1.1.1">11</span></sub></span>. <span class="ltx_text ltx_font_smallcaps" id="A3.F8.4.2">xLocEI<sub class="ltx_sub" id="A3.F8.4.2.1"><span class="ltx_text ltx_font_italic" id="A3.F8.4.2.1.1">20</span></sub></span> was trained jointly on all 20 languages, whereas <span class="ltx_text ltx_font_smallcaps" id="A3.F8.6.3">LocEI</span> trains a separate model for each language. The categorization of entity insertion types is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="A3.F9.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Entity insertion performance measured using MRR in the zero-shot setting: results across 9 Wikipedia language versions that were not used for fine-tuning <span class="ltx_text ltx_font_smallcaps" id="A3.F9.3.1">xLocEI<sub class="ltx_sub" id="A3.F9.3.1.1"><span class="ltx_text ltx_font_italic" id="A3.F9.3.1.1.1">11</span></sub></span>. <span class="ltx_text ltx_font_smallcaps" id="A3.F9.4.2">xLocEI<sub class="ltx_sub" id="A3.F9.4.2.1"><span class="ltx_text ltx_font_italic" id="A3.F9.4.2.1.1">20</span></sub></span> was trained jointly on all 20 languages, whereas <span class="ltx_text ltx_font_smallcaps" id="A3.F9.6.3">LocEI</span> trains a separate model for each language. The categorization of entity insertion types is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4 </span>Impact of the starting model</h3>
<div class="ltx_para" id="A3.SS4.p1">
<p class="ltx_p" id="A3.SS4.p1.5">Since our approach is based on fine-tuning pre-trained models, the starting pre-trained model may have an impact on the eventual model performance. We studied this dependence using three pre-trained models: BERT<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.SS4.p1.1.m1.1"><semantics id="A3.SS4.p1.1.m1.1a"><msub id="A3.SS4.p1.1.m1.1.1" xref="A3.SS4.p1.1.m1.1.1.cmml"><mi id="A3.SS4.p1.1.m1.1.1a" xref="A3.SS4.p1.1.m1.1.1.cmml"></mi><mtext id="A3.SS4.p1.1.m1.1.1.1" xref="A3.SS4.p1.1.m1.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS4.p1.1.m1.1b"><apply id="A3.SS4.p1.1.m1.1.1.cmml" xref="A3.SS4.p1.1.m1.1.1"><ci id="A3.SS4.p1.1.m1.1.1.1a.cmml" xref="A3.SS4.p1.1.m1.1.1.1"><mtext id="A3.SS4.p1.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.SS4.p1.1.m1.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p1.1.m1.1c">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p1.1.m1.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math>, RoBERTa<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.SS4.p1.2.m2.1"><semantics id="A3.SS4.p1.2.m2.1a"><msub id="A3.SS4.p1.2.m2.1.1" xref="A3.SS4.p1.2.m2.1.1.cmml"><mi id="A3.SS4.p1.2.m2.1.1a" xref="A3.SS4.p1.2.m2.1.1.cmml"></mi><mtext id="A3.SS4.p1.2.m2.1.1.1" xref="A3.SS4.p1.2.m2.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS4.p1.2.m2.1b"><apply id="A3.SS4.p1.2.m2.1.1.cmml" xref="A3.SS4.p1.2.m2.1.1"><ci id="A3.SS4.p1.2.m2.1.1.1a.cmml" xref="A3.SS4.p1.2.m2.1.1.1"><mtext id="A3.SS4.p1.2.m2.1.1.1.cmml" mathsize="70%" xref="A3.SS4.p1.2.m2.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p1.2.m2.1c">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p1.2.m2.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math> and the encoder portion of T5<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.SS4.p1.3.m3.1"><semantics id="A3.SS4.p1.3.m3.1a"><msub id="A3.SS4.p1.3.m3.1.1" xref="A3.SS4.p1.3.m3.1.1.cmml"><mi id="A3.SS4.p1.3.m3.1.1a" xref="A3.SS4.p1.3.m3.1.1.cmml"></mi><mtext id="A3.SS4.p1.3.m3.1.1.1" xref="A3.SS4.p1.3.m3.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS4.p1.3.m3.1b"><apply id="A3.SS4.p1.3.m3.1.1.cmml" xref="A3.SS4.p1.3.m3.1.1"><ci id="A3.SS4.p1.3.m3.1.1.1a.cmml" xref="A3.SS4.p1.3.m3.1.1.1"><mtext id="A3.SS4.p1.3.m3.1.1.1.cmml" mathsize="70%" xref="A3.SS4.p1.3.m3.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p1.3.m3.1c">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p1.3.m3.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math>, which we call T5<math alttext="{}_{\text{BASE}}^{\text{enc}}" class="ltx_Math" display="inline" id="A3.SS4.p1.4.m4.1"><semantics id="A3.SS4.p1.4.m4.1a"><mmultiscripts id="A3.SS4.p1.4.m4.1.1" xref="A3.SS4.p1.4.m4.1.1.cmml"><mi id="A3.SS4.p1.4.m4.1.1.2.2" xref="A3.SS4.p1.4.m4.1.1.2.2.cmml"></mi><mprescripts id="A3.SS4.p1.4.m4.1.1a" xref="A3.SS4.p1.4.m4.1.1.cmml"></mprescripts><mrow id="A3.SS4.p1.4.m4.1.1b" xref="A3.SS4.p1.4.m4.1.1.cmml"></mrow><mtext id="A3.SS4.p1.4.m4.1.1.3" xref="A3.SS4.p1.4.m4.1.1.3a.cmml">enc</mtext><mtext id="A3.SS4.p1.4.m4.1.1.2.3" xref="A3.SS4.p1.4.m4.1.1.2.3a.cmml">BASE</mtext><mrow id="A3.SS4.p1.4.m4.1.1c" xref="A3.SS4.p1.4.m4.1.1.cmml"></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="A3.SS4.p1.4.m4.1b"><apply id="A3.SS4.p1.4.m4.1.1.cmml" xref="A3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A3.SS4.p1.4.m4.1.1.1.cmml" xref="A3.SS4.p1.4.m4.1.1">superscript</csymbol><apply id="A3.SS4.p1.4.m4.1.1.2.cmml" xref="A3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A3.SS4.p1.4.m4.1.1.2.1.cmml" xref="A3.SS4.p1.4.m4.1.1">subscript</csymbol><csymbol cd="latexml" id="A3.SS4.p1.4.m4.1.1.2.2.cmml" xref="A3.SS4.p1.4.m4.1.1.2.2">absent</csymbol><ci id="A3.SS4.p1.4.m4.1.1.2.3a.cmml" xref="A3.SS4.p1.4.m4.1.1.2.3"><mtext id="A3.SS4.p1.4.m4.1.1.2.3.cmml" mathsize="70%" xref="A3.SS4.p1.4.m4.1.1.2.3">BASE</mtext></ci></apply><ci id="A3.SS4.p1.4.m4.1.1.3a.cmml" xref="A3.SS4.p1.4.m4.1.1.3"><mtext id="A3.SS4.p1.4.m4.1.1.3.cmml" mathsize="70%" xref="A3.SS4.p1.4.m4.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p1.4.m4.1c">{}_{\text{BASE}}^{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p1.4.m4.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT enc end_POSTSUPERSCRIPT</annotation></semantics></math>. We considered BERT and RoBERTa because they are amongst the most popular transformer encoder models. We additionally included T5 to see how encoder-decoder models perform in the entity insertion task. However, as RankT5 <cite class="ltx_cite ltx_citemacro_cite">Zhuang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib54" title="">2023</a>)</cite> showed there was no clear benefit in using the full encoder-decoder architecture, as opposed to encoder-only architecture, and thus, for computational reasons we decided to use the encoder-only variant of T5, T5<math alttext="{}^{\text{enc}}" class="ltx_Math" display="inline" id="A3.SS4.p1.5.m5.1"><semantics id="A3.SS4.p1.5.m5.1a"><msup id="A3.SS4.p1.5.m5.1.1" xref="A3.SS4.p1.5.m5.1.1.cmml"><mi id="A3.SS4.p1.5.m5.1.1a" xref="A3.SS4.p1.5.m5.1.1.cmml"></mi><mtext id="A3.SS4.p1.5.m5.1.1.1" xref="A3.SS4.p1.5.m5.1.1.1a.cmml">enc</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.SS4.p1.5.m5.1b"><apply id="A3.SS4.p1.5.m5.1.1.cmml" xref="A3.SS4.p1.5.m5.1.1"><ci id="A3.SS4.p1.5.m5.1.1.1a.cmml" xref="A3.SS4.p1.5.m5.1.1.1"><mtext id="A3.SS4.p1.5.m5.1.1.1.cmml" mathsize="70%" xref="A3.SS4.p1.5.m5.1.1.1">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p1.5.m5.1c">{}^{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p1.5.m5.1d">start_FLOATSUPERSCRIPT enc end_FLOATSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="A3.T12">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Comparing the entity insertion performance obtained for Simple English with different starting models. The categorization of entity insertion types into ‘Overall’, ‘Missing’, and ‘Present’ is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS3" title="6.3 Setup ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6.3</span></a>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="A3.T12.6">
<tr class="ltx_tr" id="A3.T12.6.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T12.6.7.1" rowspan="2"><span class="ltx_text" id="A3.T12.6.7.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="A3.T12.6.7.2">Hits@1</td>
<td class="ltx_td ltx_align_center" colspan="3" id="A3.T12.6.7.3">MRR</td>
</tr>
<tr class="ltx_tr" id="A3.T12.6.8">
<td class="ltx_td ltx_align_center" id="A3.T12.6.8.1">Overall</td>
<td class="ltx_td ltx_align_center" id="A3.T12.6.8.2">Present</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T12.6.8.3">Missing</td>
<td class="ltx_td ltx_align_center" id="A3.T12.6.8.4">Overall</td>
<td class="ltx_td ltx_align_center" id="A3.T12.6.8.5">Present</td>
<td class="ltx_td ltx_align_center" id="A3.T12.6.8.6">Missing</td>
</tr>
<tr class="ltx_tr" id="A3.T12.6.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T12.6.9.1">BERT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T12.6.9.2">0.666</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T12.6.9.3">0.916</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T12.6.9.4">0.492</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T12.6.9.5">0.738</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T12.6.9.6">0.940</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T12.6.9.7">0.598</td>
</tr>
<tr class="ltx_tr" id="A3.T12.1.1">
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T12.1.1.1">T5<math alttext="{}^{\text{enc}}" class="ltx_Math" display="inline" id="A3.T12.1.1.1.m1.1"><semantics id="A3.T12.1.1.1.m1.1a"><msup id="A3.T12.1.1.1.m1.1.1" xref="A3.T12.1.1.1.m1.1.1.cmml"><mi id="A3.T12.1.1.1.m1.1.1a" xref="A3.T12.1.1.1.m1.1.1.cmml"></mi><mtext id="A3.T12.1.1.1.m1.1.1.1" xref="A3.T12.1.1.1.m1.1.1.1a.cmml">enc</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.T12.1.1.1.m1.1b"><apply id="A3.T12.1.1.1.m1.1.1.cmml" xref="A3.T12.1.1.1.m1.1.1"><ci id="A3.T12.1.1.1.m1.1.1.1a.cmml" xref="A3.T12.1.1.1.m1.1.1.1"><mtext id="A3.T12.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.T12.1.1.1.m1.1.1.1">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T12.1.1.1.m1.1c">{}^{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="A3.T12.1.1.1.m1.1d">start_FLOATSUPERSCRIPT enc end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A3.T12.1.1.2">0.710</td>
<td class="ltx_td ltx_align_center" id="A3.T12.1.1.3">0.929</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T12.1.1.4">0.558</td>
<td class="ltx_td ltx_align_center" id="A3.T12.1.1.5">0.774</td>
<td class="ltx_td ltx_align_center" id="A3.T12.1.1.6">0.952</td>
<td class="ltx_td ltx_align_center" id="A3.T12.1.1.7">0.650</td>
</tr>
<tr class="ltx_tr" id="A3.T12.6.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A3.T12.6.6.6">RoBERTa</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T12.2.2.1"><span class="ltx_text ltx_font_bold" id="A3.T12.2.2.1.1">0.851<sup class="ltx_sup" id="A3.T12.2.2.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T12.2.2.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T12.3.3.2"><span class="ltx_text ltx_font_bold" id="A3.T12.3.3.2.1">0.957<sup class="ltx_sup" id="A3.T12.3.3.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T12.3.3.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A3.T12.4.4.3"><span class="ltx_text ltx_font_bold" id="A3.T12.4.4.3.1">0.777<sup class="ltx_sup" id="A3.T12.4.4.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T12.4.4.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T12.5.5.4"><span class="ltx_text ltx_font_bold" id="A3.T12.5.5.4.1">0.890<sup class="ltx_sup" id="A3.T12.5.5.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T12.5.5.4.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T12.6.6.7"><span class="ltx_text ltx_font_bold" id="A3.T12.6.6.7.1">0.968</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T12.6.6.5"><span class="ltx_text ltx_font_bold" id="A3.T12.6.6.5.1">0.835<sup class="ltx_sup" id="A3.T12.6.6.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T12.6.6.5.1.1.1">†</span></sup></span></td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="A3.I1">
<li class="ltx_item" id="A3.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">†</span>
<div class="ltx_para" id="A3.I1.ix1.p1">
<p class="ltx_p" id="A3.I1.ix1.p1.1">Indicates statistical significance (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="A3.I1.ix1.p1.1.m1.1"><semantics id="A3.I1.ix1.p1.1.m1.1a"><mrow id="A3.I1.ix1.p1.1.m1.1.1" xref="A3.I1.ix1.p1.1.m1.1.1.cmml"><mi id="A3.I1.ix1.p1.1.m1.1.1.2" xref="A3.I1.ix1.p1.1.m1.1.1.2.cmml">p</mi><mo id="A3.I1.ix1.p1.1.m1.1.1.1" xref="A3.I1.ix1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="A3.I1.ix1.p1.1.m1.1.1.3" xref="A3.I1.ix1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.I1.ix1.p1.1.m1.1b"><apply id="A3.I1.ix1.p1.1.m1.1.1.cmml" xref="A3.I1.ix1.p1.1.m1.1.1"><lt id="A3.I1.ix1.p1.1.m1.1.1.1.cmml" xref="A3.I1.ix1.p1.1.m1.1.1.1"></lt><ci id="A3.I1.ix1.p1.1.m1.1.1.2.cmml" xref="A3.I1.ix1.p1.1.m1.1.1.2">𝑝</ci><cn id="A3.I1.ix1.p1.1.m1.1.1.3.cmml" type="float" xref="A3.I1.ix1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.I1.ix1.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="A3.I1.ix1.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>) between the best and the second-best scores.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<div class="ltx_para" id="A3.SS4.p2">
<p class="ltx_p" id="A3.SS4.p2.5">We trained each model on the Simple English dataset, and we measured their performance on the test data. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.T12" title="Table 12 ‣ C.4 Impact of the starting model ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">12</span></a> shows that the RoBERTa model outperformed both BERT and T5<math alttext="{}^{\text{enc}}" class="ltx_Math" display="inline" id="A3.SS4.p2.1.m1.1"><semantics id="A3.SS4.p2.1.m1.1a"><msup id="A3.SS4.p2.1.m1.1.1" xref="A3.SS4.p2.1.m1.1.1.cmml"><mi id="A3.SS4.p2.1.m1.1.1a" xref="A3.SS4.p2.1.m1.1.1.cmml"></mi><mtext id="A3.SS4.p2.1.m1.1.1.1" xref="A3.SS4.p2.1.m1.1.1.1a.cmml">enc</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.SS4.p2.1.m1.1b"><apply id="A3.SS4.p2.1.m1.1.1.cmml" xref="A3.SS4.p2.1.m1.1.1"><ci id="A3.SS4.p2.1.m1.1.1.1a.cmml" xref="A3.SS4.p2.1.m1.1.1.1"><mtext id="A3.SS4.p2.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.SS4.p2.1.m1.1.1.1">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p2.1.m1.1c">{}^{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p2.1.m1.1d">start_FLOATSUPERSCRIPT enc end_FLOATSUPERSCRIPT</annotation></semantics></math> in all entity insertion categories by a large margin. BERT and T5<math alttext="{}^{\text{enc}}" class="ltx_Math" display="inline" id="A3.SS4.p2.2.m2.1"><semantics id="A3.SS4.p2.2.m2.1a"><msup id="A3.SS4.p2.2.m2.1.1" xref="A3.SS4.p2.2.m2.1.1.cmml"><mi id="A3.SS4.p2.2.m2.1.1a" xref="A3.SS4.p2.2.m2.1.1.cmml"></mi><mtext id="A3.SS4.p2.2.m2.1.1.1" xref="A3.SS4.p2.2.m2.1.1.1a.cmml">enc</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.SS4.p2.2.m2.1b"><apply id="A3.SS4.p2.2.m2.1.1.cmml" xref="A3.SS4.p2.2.m2.1.1"><ci id="A3.SS4.p2.2.m2.1.1.1a.cmml" xref="A3.SS4.p2.2.m2.1.1.1"><mtext id="A3.SS4.p2.2.m2.1.1.1.cmml" mathsize="70%" xref="A3.SS4.p2.2.m2.1.1.1">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p2.2.m2.1c">{}^{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p2.2.m2.1d">start_FLOATSUPERSCRIPT enc end_FLOATSUPERSCRIPT</annotation></semantics></math> performed similarly, with T5<math alttext="{}^{\text{enc}}" class="ltx_Math" display="inline" id="A3.SS4.p2.3.m3.1"><semantics id="A3.SS4.p2.3.m3.1a"><msup id="A3.SS4.p2.3.m3.1.1" xref="A3.SS4.p2.3.m3.1.1.cmml"><mi id="A3.SS4.p2.3.m3.1.1a" xref="A3.SS4.p2.3.m3.1.1.cmml"></mi><mtext id="A3.SS4.p2.3.m3.1.1.1" xref="A3.SS4.p2.3.m3.1.1.1a.cmml">enc</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.SS4.p2.3.m3.1b"><apply id="A3.SS4.p2.3.m3.1.1.cmml" xref="A3.SS4.p2.3.m3.1.1"><ci id="A3.SS4.p2.3.m3.1.1.1a.cmml" xref="A3.SS4.p2.3.m3.1.1.1"><mtext id="A3.SS4.p2.3.m3.1.1.1.cmml" mathsize="70%" xref="A3.SS4.p2.3.m3.1.1.1">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p2.3.m3.1c">{}^{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p2.3.m3.1d">start_FLOATSUPERSCRIPT enc end_FLOATSUPERSCRIPT</annotation></semantics></math> doing slightly better. These results may be explained by the fact that the RoBERTa tokenizer has a much larger vocabulary than the tokenizers for BERT or T5<math alttext="{}^{\text{enc}}" class="ltx_Math" display="inline" id="A3.SS4.p2.4.m4.1"><semantics id="A3.SS4.p2.4.m4.1a"><msup id="A3.SS4.p2.4.m4.1.1" xref="A3.SS4.p2.4.m4.1.1.cmml"><mi id="A3.SS4.p2.4.m4.1.1a" xref="A3.SS4.p2.4.m4.1.1.cmml"></mi><mtext id="A3.SS4.p2.4.m4.1.1.1" xref="A3.SS4.p2.4.m4.1.1.1a.cmml">enc</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.SS4.p2.4.m4.1b"><apply id="A3.SS4.p2.4.m4.1.1.cmml" xref="A3.SS4.p2.4.m4.1.1"><ci id="A3.SS4.p2.4.m4.1.1.1a.cmml" xref="A3.SS4.p2.4.m4.1.1.1"><mtext id="A3.SS4.p2.4.m4.1.1.1.cmml" mathsize="70%" xref="A3.SS4.p2.4.m4.1.1.1">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p2.4.m4.1c">{}^{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p2.4.m4.1d">start_FLOATSUPERSCRIPT enc end_FLOATSUPERSCRIPT</annotation></semantics></math>. A larger vocabulary might make it possible for the model to capture more fine-grained linguistic and structural patterns in the candidate text spans, enabling the model to exploit patterns that neither T5<math alttext="{}^{\text{enc}}" class="ltx_Math" display="inline" id="A3.SS4.p2.5.m5.1"><semantics id="A3.SS4.p2.5.m5.1a"><msup id="A3.SS4.p2.5.m5.1.1" xref="A3.SS4.p2.5.m5.1.1.cmml"><mi id="A3.SS4.p2.5.m5.1.1a" xref="A3.SS4.p2.5.m5.1.1.cmml"></mi><mtext id="A3.SS4.p2.5.m5.1.1.1" xref="A3.SS4.p2.5.m5.1.1.1a.cmml">enc</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.SS4.p2.5.m5.1b"><apply id="A3.SS4.p2.5.m5.1.1.cmml" xref="A3.SS4.p2.5.m5.1.1"><ci id="A3.SS4.p2.5.m5.1.1.1a.cmml" xref="A3.SS4.p2.5.m5.1.1.1"><mtext id="A3.SS4.p2.5.m5.1.1.1.cmml" mathsize="70%" xref="A3.SS4.p2.5.m5.1.1.1">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p2.5.m5.1c">{}^{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p2.5.m5.1d">start_FLOATSUPERSCRIPT enc end_FLOATSUPERSCRIPT</annotation></semantics></math> nor BERT can capture.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.5 </span>Impact of the model size</h3>
<div class="ltx_para" id="A3.SS5.p1">
<p class="ltx_p" id="A3.SS5.p1.2">There is a widely known trend in the deep learning community that bigger models tend to perform better than smaller models <cite class="ltx_cite ltx_citemacro_cite">Soltanolkotabi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib41" title="">2019</a>); Brutzkus and Globerson (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib5" title="">2019</a>); Simon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib40" title="">2024</a>)</cite>. To this end, we studied how the model size impacts the entity insertion performance by comparing RoBERTa<math alttext="{}_{\text{LARGE}}" class="ltx_Math" display="inline" id="A3.SS5.p1.1.m1.1"><semantics id="A3.SS5.p1.1.m1.1a"><msub id="A3.SS5.p1.1.m1.1.1" xref="A3.SS5.p1.1.m1.1.1.cmml"><mi id="A3.SS5.p1.1.m1.1.1a" xref="A3.SS5.p1.1.m1.1.1.cmml"></mi><mtext id="A3.SS5.p1.1.m1.1.1.1" xref="A3.SS5.p1.1.m1.1.1.1a.cmml">LARGE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS5.p1.1.m1.1b"><apply id="A3.SS5.p1.1.m1.1.1.cmml" xref="A3.SS5.p1.1.m1.1.1"><ci id="A3.SS5.p1.1.m1.1.1.1a.cmml" xref="A3.SS5.p1.1.m1.1.1.1"><mtext id="A3.SS5.p1.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.SS5.p1.1.m1.1.1.1">LARGE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS5.p1.1.m1.1c">{}_{\text{LARGE}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS5.p1.1.m1.1d">start_FLOATSUBSCRIPT LARGE end_FLOATSUBSCRIPT</annotation></semantics></math> with RoBERTa<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.SS5.p1.2.m2.1"><semantics id="A3.SS5.p1.2.m2.1a"><msub id="A3.SS5.p1.2.m2.1.1" xref="A3.SS5.p1.2.m2.1.1.cmml"><mi id="A3.SS5.p1.2.m2.1.1a" xref="A3.SS5.p1.2.m2.1.1.cmml"></mi><mtext id="A3.SS5.p1.2.m2.1.1.1" xref="A3.SS5.p1.2.m2.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS5.p1.2.m2.1b"><apply id="A3.SS5.p1.2.m2.1.1.cmml" xref="A3.SS5.p1.2.m2.1.1"><ci id="A3.SS5.p1.2.m2.1.1.1a.cmml" xref="A3.SS5.p1.2.m2.1.1.1"><mtext id="A3.SS5.p1.2.m2.1.1.1.cmml" mathsize="70%" xref="A3.SS5.p1.2.m2.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS5.p1.2.m2.1c">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS5.p1.2.m2.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math> on the Simple English dataset.</p>
</div>
<div class="ltx_para" id="A3.SS5.p2">
<p class="ltx_p" id="A3.SS5.p2.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.T13" title="Table 13 ‣ C.5 Impact of the model size ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">13</span></a> shows that there is no statistically significant difference between the performance of RoBERTa<math alttext="{}_{\text{LARGE}}" class="ltx_Math" display="inline" id="A3.SS5.p2.1.m1.1"><semantics id="A3.SS5.p2.1.m1.1a"><msub id="A3.SS5.p2.1.m1.1.1" xref="A3.SS5.p2.1.m1.1.1.cmml"><mi id="A3.SS5.p2.1.m1.1.1a" xref="A3.SS5.p2.1.m1.1.1.cmml"></mi><mtext id="A3.SS5.p2.1.m1.1.1.1" xref="A3.SS5.p2.1.m1.1.1.1a.cmml">LARGE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS5.p2.1.m1.1b"><apply id="A3.SS5.p2.1.m1.1.1.cmml" xref="A3.SS5.p2.1.m1.1.1"><ci id="A3.SS5.p2.1.m1.1.1.1a.cmml" xref="A3.SS5.p2.1.m1.1.1.1"><mtext id="A3.SS5.p2.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.SS5.p2.1.m1.1.1.1">LARGE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS5.p2.1.m1.1c">{}_{\text{LARGE}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS5.p2.1.m1.1d">start_FLOATSUBSCRIPT LARGE end_FLOATSUBSCRIPT</annotation></semantics></math> and RoBERTa<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.SS5.p2.2.m2.1"><semantics id="A3.SS5.p2.2.m2.1a"><msub id="A3.SS5.p2.2.m2.1.1" xref="A3.SS5.p2.2.m2.1.1.cmml"><mi id="A3.SS5.p2.2.m2.1.1a" xref="A3.SS5.p2.2.m2.1.1.cmml"></mi><mtext id="A3.SS5.p2.2.m2.1.1.1" xref="A3.SS5.p2.2.m2.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS5.p2.2.m2.1b"><apply id="A3.SS5.p2.2.m2.1.1.cmml" xref="A3.SS5.p2.2.m2.1.1"><ci id="A3.SS5.p2.2.m2.1.1.1a.cmml" xref="A3.SS5.p2.2.m2.1.1.1"><mtext id="A3.SS5.p2.2.m2.1.1.1.cmml" mathsize="70%" xref="A3.SS5.p2.2.m2.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS5.p2.2.m2.1c">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS5.p2.2.m2.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math>. These results point to the fact that the increased model complexity is not sufficient to improve model performance. It is worth noting that these results were obtained for Simple English. The multilingual problem is much harder and it might benefit from the increased complexity and larger parameter space of the larger model. We leave this study for future work.</p>
</div>
<div class="ltx_para" id="A3.SS5.p3">
<p class="ltx_p" id="A3.SS5.p3.1">Additionally, these findings give more strength to the hypothesis that the reason why RoBERTa is significantly better than BERT and T5<math alttext="{}^{\text{enc}}" class="ltx_Math" display="inline" id="A3.SS5.p3.1.m1.1"><semantics id="A3.SS5.p3.1.m1.1a"><msup id="A3.SS5.p3.1.m1.1.1" xref="A3.SS5.p3.1.m1.1.1.cmml"><mi id="A3.SS5.p3.1.m1.1.1a" xref="A3.SS5.p3.1.m1.1.1.cmml"></mi><mtext id="A3.SS5.p3.1.m1.1.1.1" xref="A3.SS5.p3.1.m1.1.1.1a.cmml">enc</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.SS5.p3.1.m1.1b"><apply id="A3.SS5.p3.1.m1.1.1.cmml" xref="A3.SS5.p3.1.m1.1.1"><ci id="A3.SS5.p3.1.m1.1.1.1a.cmml" xref="A3.SS5.p3.1.m1.1.1.1"><mtext id="A3.SS5.p3.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.SS5.p3.1.m1.1.1.1">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS5.p3.1.m1.1c">{}^{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS5.p3.1.m1.1d">start_FLOATSUPERSCRIPT enc end_FLOATSUPERSCRIPT</annotation></semantics></math> is because RoBERTa’s larger tokenizer allows the model to capture more fine-grained linguistic and structural patterns in the candidate. This increased input representation space seems to be vital for entity insertion.</p>
</div>
<figure class="ltx_table" id="A3.T13">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 13: </span>Comparing the entity insertion performance obtained for Simple English with varying model sizes. The categorization of entity insertion types is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T13.2">
<tr class="ltx_tr" id="A3.T13.2.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T13.2.3.1" rowspan="2"><span class="ltx_text" id="A3.T13.2.3.1.1">Model</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="A3.T13.2.3.2">Text Present</td>
<td class="ltx_td ltx_align_center" colspan="2" id="A3.T13.2.3.3">Missing Mention</td>
<td class="ltx_td ltx_align_center" colspan="2" id="A3.T13.2.3.4">Missing Sentence</td>
<td class="ltx_td ltx_align_center" colspan="2" id="A3.T13.2.3.5">Missing Span</td>
</tr>
<tr class="ltx_tr" id="A3.T13.2.4">
<td class="ltx_td ltx_align_center" id="A3.T13.2.4.1">Hits@1</td>
<td class="ltx_td ltx_align_center" id="A3.T13.2.4.2">MRR</td>
<td class="ltx_td ltx_align_center" id="A3.T13.2.4.3">Hits@1</td>
<td class="ltx_td ltx_align_center" id="A3.T13.2.4.4">MRR</td>
<td class="ltx_td ltx_align_center" id="A3.T13.2.4.5">Hits@1</td>
<td class="ltx_td ltx_align_center" id="A3.T13.2.4.6">MRR</td>
<td class="ltx_td ltx_align_center" id="A3.T13.2.4.7">Hits@1</td>
<td class="ltx_td ltx_align_center" id="A3.T13.2.4.8">MRR</td>
</tr>
<tr class="ltx_tr" id="A3.T13.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T13.1.1.1">RoBERTa<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.T13.1.1.1.m1.1"><semantics id="A3.T13.1.1.1.m1.1a"><msub id="A3.T13.1.1.1.m1.1.1" xref="A3.T13.1.1.1.m1.1.1.cmml"><mi id="A3.T13.1.1.1.m1.1.1a" xref="A3.T13.1.1.1.m1.1.1.cmml"></mi><mtext id="A3.T13.1.1.1.m1.1.1.1" xref="A3.T13.1.1.1.m1.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T13.1.1.1.m1.1b"><apply id="A3.T13.1.1.1.m1.1.1.cmml" xref="A3.T13.1.1.1.m1.1.1"><ci id="A3.T13.1.1.1.m1.1.1.1a.cmml" xref="A3.T13.1.1.1.m1.1.1.1"><mtext id="A3.T13.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.T13.1.1.1.m1.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T13.1.1.1.m1.1c">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.T13.1.1.1.m1.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T13.1.1.2">0.956</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T13.1.1.3">0.968</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T13.1.1.4"><span class="ltx_text ltx_font_bold" id="A3.T13.1.1.4.1">0.696</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T13.1.1.5"><span class="ltx_text ltx_font_bold" id="A3.T13.1.1.5.1">0.760</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T13.1.1.6">0.834</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T13.1.1.7">0.884</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T13.1.1.8">0.799</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T13.1.1.9">0.859</td>
</tr>
<tr class="ltx_tr" id="A3.T13.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A3.T13.2.2.1">RoBERTa<math alttext="{}_{\text{LARGE}}" class="ltx_Math" display="inline" id="A3.T13.2.2.1.m1.1"><semantics id="A3.T13.2.2.1.m1.1a"><msub id="A3.T13.2.2.1.m1.1.1" xref="A3.T13.2.2.1.m1.1.1.cmml"><mi id="A3.T13.2.2.1.m1.1.1a" xref="A3.T13.2.2.1.m1.1.1.cmml"></mi><mtext id="A3.T13.2.2.1.m1.1.1.1" xref="A3.T13.2.2.1.m1.1.1.1a.cmml">LARGE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T13.2.2.1.m1.1b"><apply id="A3.T13.2.2.1.m1.1.1.cmml" xref="A3.T13.2.2.1.m1.1.1"><ci id="A3.T13.2.2.1.m1.1.1.1a.cmml" xref="A3.T13.2.2.1.m1.1.1.1"><mtext id="A3.T13.2.2.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.T13.2.2.1.m1.1.1.1">LARGE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T13.2.2.1.m1.1c">{}_{\text{LARGE}}</annotation><annotation encoding="application/x-llamapun" id="A3.T13.2.2.1.m1.1d">start_FLOATSUBSCRIPT LARGE end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T13.2.2.2"><span class="ltx_text ltx_font_bold" id="A3.T13.2.2.2.1">0.964</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T13.2.2.3"><span class="ltx_text ltx_font_bold" id="A3.T13.2.2.3.1">0.975</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T13.2.2.4">0.670</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T13.2.2.5">0.744</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T13.2.2.6"><span class="ltx_text ltx_font_bold" id="A3.T13.2.2.6.1">0.856</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T13.2.2.7"><span class="ltx_text ltx_font_bold" id="A3.T13.2.2.7.1">0.895</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T13.2.2.8"><span class="ltx_text ltx_font_bold" id="A3.T13.2.2.8.1">0.822</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T13.2.2.9"><span class="ltx_text ltx_font_bold" id="A3.T13.2.2.9.1">0.873</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.6 </span>Impact of the size of training data</h3>
<div class="ltx_para" id="A3.SS6.p1">
<p class="ltx_p" id="A3.SS6.p1.1">As discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS2" title="5.2 Two-stage training pipeline ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5.2</span></a>, we use the existing and added links data during the first and second stages of our training pipeline, respectively. In this analysis, we studied how much data is needed for each stage. To study the impact of the training data size on the downstream entity insertion performance, we trained a RoBERTa<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.SS6.p1.1.m1.1"><semantics id="A3.SS6.p1.1.m1.1a"><msub id="A3.SS6.p1.1.m1.1.1" xref="A3.SS6.p1.1.m1.1.1.cmml"><mi id="A3.SS6.p1.1.m1.1.1a" xref="A3.SS6.p1.1.m1.1.1.cmml"></mi><mtext id="A3.SS6.p1.1.m1.1.1.1" xref="A3.SS6.p1.1.m1.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS6.p1.1.m1.1b"><apply id="A3.SS6.p1.1.m1.1.1.cmml" xref="A3.SS6.p1.1.m1.1.1"><ci id="A3.SS6.p1.1.m1.1.1.1a.cmml" xref="A3.SS6.p1.1.m1.1.1.1"><mtext id="A3.SS6.p1.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.SS6.p1.1.m1.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS6.p1.1.m1.1c">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS6.p1.1.m1.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math> model with varying portions of the full English dataset.</p>
</div>
<div class="ltx_para" id="A3.SS6.p2">
<p class="ltx_p" id="A3.SS6.p2.2">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.F10" title="Figure 10 ‣ C.6 Impact of the size of training data ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">10</span></a> shows the performance of <span class="ltx_text ltx_font_smallcaps" id="A3.SS6.p2.2.1">LocEI</span> for different entity insertion categories with varying training data sizes og <math alttext="\{10^{3},10^{4},10^{5},10^{6}\}" class="ltx_Math" display="inline" id="A3.SS6.p2.1.m1.4"><semantics id="A3.SS6.p2.1.m1.4a"><mrow id="A3.SS6.p2.1.m1.4.4.4" xref="A3.SS6.p2.1.m1.4.4.5.cmml"><mo id="A3.SS6.p2.1.m1.4.4.4.5" stretchy="false" xref="A3.SS6.p2.1.m1.4.4.5.cmml">{</mo><msup id="A3.SS6.p2.1.m1.1.1.1.1" xref="A3.SS6.p2.1.m1.1.1.1.1.cmml"><mn id="A3.SS6.p2.1.m1.1.1.1.1.2" xref="A3.SS6.p2.1.m1.1.1.1.1.2.cmml">10</mn><mn id="A3.SS6.p2.1.m1.1.1.1.1.3" xref="A3.SS6.p2.1.m1.1.1.1.1.3.cmml">3</mn></msup><mo id="A3.SS6.p2.1.m1.4.4.4.6" xref="A3.SS6.p2.1.m1.4.4.5.cmml">,</mo><msup id="A3.SS6.p2.1.m1.2.2.2.2" xref="A3.SS6.p2.1.m1.2.2.2.2.cmml"><mn id="A3.SS6.p2.1.m1.2.2.2.2.2" xref="A3.SS6.p2.1.m1.2.2.2.2.2.cmml">10</mn><mn id="A3.SS6.p2.1.m1.2.2.2.2.3" xref="A3.SS6.p2.1.m1.2.2.2.2.3.cmml">4</mn></msup><mo id="A3.SS6.p2.1.m1.4.4.4.7" xref="A3.SS6.p2.1.m1.4.4.5.cmml">,</mo><msup id="A3.SS6.p2.1.m1.3.3.3.3" xref="A3.SS6.p2.1.m1.3.3.3.3.cmml"><mn id="A3.SS6.p2.1.m1.3.3.3.3.2" xref="A3.SS6.p2.1.m1.3.3.3.3.2.cmml">10</mn><mn id="A3.SS6.p2.1.m1.3.3.3.3.3" xref="A3.SS6.p2.1.m1.3.3.3.3.3.cmml">5</mn></msup><mo id="A3.SS6.p2.1.m1.4.4.4.8" xref="A3.SS6.p2.1.m1.4.4.5.cmml">,</mo><msup id="A3.SS6.p2.1.m1.4.4.4.4" xref="A3.SS6.p2.1.m1.4.4.4.4.cmml"><mn id="A3.SS6.p2.1.m1.4.4.4.4.2" xref="A3.SS6.p2.1.m1.4.4.4.4.2.cmml">10</mn><mn id="A3.SS6.p2.1.m1.4.4.4.4.3" xref="A3.SS6.p2.1.m1.4.4.4.4.3.cmml">6</mn></msup><mo id="A3.SS6.p2.1.m1.4.4.4.9" stretchy="false" xref="A3.SS6.p2.1.m1.4.4.5.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.SS6.p2.1.m1.4b"><set id="A3.SS6.p2.1.m1.4.4.5.cmml" xref="A3.SS6.p2.1.m1.4.4.4"><apply id="A3.SS6.p2.1.m1.1.1.1.1.cmml" xref="A3.SS6.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="A3.SS6.p2.1.m1.1.1.1.1.1.cmml" xref="A3.SS6.p2.1.m1.1.1.1.1">superscript</csymbol><cn id="A3.SS6.p2.1.m1.1.1.1.1.2.cmml" type="integer" xref="A3.SS6.p2.1.m1.1.1.1.1.2">10</cn><cn id="A3.SS6.p2.1.m1.1.1.1.1.3.cmml" type="integer" xref="A3.SS6.p2.1.m1.1.1.1.1.3">3</cn></apply><apply id="A3.SS6.p2.1.m1.2.2.2.2.cmml" xref="A3.SS6.p2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="A3.SS6.p2.1.m1.2.2.2.2.1.cmml" xref="A3.SS6.p2.1.m1.2.2.2.2">superscript</csymbol><cn id="A3.SS6.p2.1.m1.2.2.2.2.2.cmml" type="integer" xref="A3.SS6.p2.1.m1.2.2.2.2.2">10</cn><cn id="A3.SS6.p2.1.m1.2.2.2.2.3.cmml" type="integer" xref="A3.SS6.p2.1.m1.2.2.2.2.3">4</cn></apply><apply id="A3.SS6.p2.1.m1.3.3.3.3.cmml" xref="A3.SS6.p2.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="A3.SS6.p2.1.m1.3.3.3.3.1.cmml" xref="A3.SS6.p2.1.m1.3.3.3.3">superscript</csymbol><cn id="A3.SS6.p2.1.m1.3.3.3.3.2.cmml" type="integer" xref="A3.SS6.p2.1.m1.3.3.3.3.2">10</cn><cn id="A3.SS6.p2.1.m1.3.3.3.3.3.cmml" type="integer" xref="A3.SS6.p2.1.m1.3.3.3.3.3">5</cn></apply><apply id="A3.SS6.p2.1.m1.4.4.4.4.cmml" xref="A3.SS6.p2.1.m1.4.4.4.4"><csymbol cd="ambiguous" id="A3.SS6.p2.1.m1.4.4.4.4.1.cmml" xref="A3.SS6.p2.1.m1.4.4.4.4">superscript</csymbol><cn id="A3.SS6.p2.1.m1.4.4.4.4.2.cmml" type="integer" xref="A3.SS6.p2.1.m1.4.4.4.4.2">10</cn><cn id="A3.SS6.p2.1.m1.4.4.4.4.3.cmml" type="integer" xref="A3.SS6.p2.1.m1.4.4.4.4.3">6</cn></apply></set></annotation-xml><annotation encoding="application/x-tex" id="A3.SS6.p2.1.m1.4c">\{10^{3},10^{4},10^{5},10^{6}\}</annotation><annotation encoding="application/x-llamapun" id="A3.SS6.p2.1.m1.4d">{ 10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT }</annotation></semantics></math> in the first stage of the training pipeline. Note that <span class="ltx_text ltx_font_smallcaps" id="A3.SS6.p2.2.2">LocEI</span> was trained using only the first stage for this analysis. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.F11" title="Figure 11 ‣ C.6 Impact of the size of training data ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">11</span></a> shows an analogous plot for the second stage with varying training data sizes of <math alttext="\{10^{2},10^{3},10^{4},10^{5}\}" class="ltx_Math" display="inline" id="A3.SS6.p2.2.m2.4"><semantics id="A3.SS6.p2.2.m2.4a"><mrow id="A3.SS6.p2.2.m2.4.4.4" xref="A3.SS6.p2.2.m2.4.4.5.cmml"><mo id="A3.SS6.p2.2.m2.4.4.4.5" stretchy="false" xref="A3.SS6.p2.2.m2.4.4.5.cmml">{</mo><msup id="A3.SS6.p2.2.m2.1.1.1.1" xref="A3.SS6.p2.2.m2.1.1.1.1.cmml"><mn id="A3.SS6.p2.2.m2.1.1.1.1.2" xref="A3.SS6.p2.2.m2.1.1.1.1.2.cmml">10</mn><mn id="A3.SS6.p2.2.m2.1.1.1.1.3" xref="A3.SS6.p2.2.m2.1.1.1.1.3.cmml">2</mn></msup><mo id="A3.SS6.p2.2.m2.4.4.4.6" xref="A3.SS6.p2.2.m2.4.4.5.cmml">,</mo><msup id="A3.SS6.p2.2.m2.2.2.2.2" xref="A3.SS6.p2.2.m2.2.2.2.2.cmml"><mn id="A3.SS6.p2.2.m2.2.2.2.2.2" xref="A3.SS6.p2.2.m2.2.2.2.2.2.cmml">10</mn><mn id="A3.SS6.p2.2.m2.2.2.2.2.3" xref="A3.SS6.p2.2.m2.2.2.2.2.3.cmml">3</mn></msup><mo id="A3.SS6.p2.2.m2.4.4.4.7" xref="A3.SS6.p2.2.m2.4.4.5.cmml">,</mo><msup id="A3.SS6.p2.2.m2.3.3.3.3" xref="A3.SS6.p2.2.m2.3.3.3.3.cmml"><mn id="A3.SS6.p2.2.m2.3.3.3.3.2" xref="A3.SS6.p2.2.m2.3.3.3.3.2.cmml">10</mn><mn id="A3.SS6.p2.2.m2.3.3.3.3.3" xref="A3.SS6.p2.2.m2.3.3.3.3.3.cmml">4</mn></msup><mo id="A3.SS6.p2.2.m2.4.4.4.8" xref="A3.SS6.p2.2.m2.4.4.5.cmml">,</mo><msup id="A3.SS6.p2.2.m2.4.4.4.4" xref="A3.SS6.p2.2.m2.4.4.4.4.cmml"><mn id="A3.SS6.p2.2.m2.4.4.4.4.2" xref="A3.SS6.p2.2.m2.4.4.4.4.2.cmml">10</mn><mn id="A3.SS6.p2.2.m2.4.4.4.4.3" xref="A3.SS6.p2.2.m2.4.4.4.4.3.cmml">5</mn></msup><mo id="A3.SS6.p2.2.m2.4.4.4.9" stretchy="false" xref="A3.SS6.p2.2.m2.4.4.5.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.SS6.p2.2.m2.4b"><set id="A3.SS6.p2.2.m2.4.4.5.cmml" xref="A3.SS6.p2.2.m2.4.4.4"><apply id="A3.SS6.p2.2.m2.1.1.1.1.cmml" xref="A3.SS6.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="A3.SS6.p2.2.m2.1.1.1.1.1.cmml" xref="A3.SS6.p2.2.m2.1.1.1.1">superscript</csymbol><cn id="A3.SS6.p2.2.m2.1.1.1.1.2.cmml" type="integer" xref="A3.SS6.p2.2.m2.1.1.1.1.2">10</cn><cn id="A3.SS6.p2.2.m2.1.1.1.1.3.cmml" type="integer" xref="A3.SS6.p2.2.m2.1.1.1.1.3">2</cn></apply><apply id="A3.SS6.p2.2.m2.2.2.2.2.cmml" xref="A3.SS6.p2.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="A3.SS6.p2.2.m2.2.2.2.2.1.cmml" xref="A3.SS6.p2.2.m2.2.2.2.2">superscript</csymbol><cn id="A3.SS6.p2.2.m2.2.2.2.2.2.cmml" type="integer" xref="A3.SS6.p2.2.m2.2.2.2.2.2">10</cn><cn id="A3.SS6.p2.2.m2.2.2.2.2.3.cmml" type="integer" xref="A3.SS6.p2.2.m2.2.2.2.2.3">3</cn></apply><apply id="A3.SS6.p2.2.m2.3.3.3.3.cmml" xref="A3.SS6.p2.2.m2.3.3.3.3"><csymbol cd="ambiguous" id="A3.SS6.p2.2.m2.3.3.3.3.1.cmml" xref="A3.SS6.p2.2.m2.3.3.3.3">superscript</csymbol><cn id="A3.SS6.p2.2.m2.3.3.3.3.2.cmml" type="integer" xref="A3.SS6.p2.2.m2.3.3.3.3.2">10</cn><cn id="A3.SS6.p2.2.m2.3.3.3.3.3.cmml" type="integer" xref="A3.SS6.p2.2.m2.3.3.3.3.3">4</cn></apply><apply id="A3.SS6.p2.2.m2.4.4.4.4.cmml" xref="A3.SS6.p2.2.m2.4.4.4.4"><csymbol cd="ambiguous" id="A3.SS6.p2.2.m2.4.4.4.4.1.cmml" xref="A3.SS6.p2.2.m2.4.4.4.4">superscript</csymbol><cn id="A3.SS6.p2.2.m2.4.4.4.4.2.cmml" type="integer" xref="A3.SS6.p2.2.m2.4.4.4.4.2">10</cn><cn id="A3.SS6.p2.2.m2.4.4.4.4.3.cmml" type="integer" xref="A3.SS6.p2.2.m2.4.4.4.4.3">5</cn></apply></set></annotation-xml><annotation encoding="application/x-tex" id="A3.SS6.p2.2.m2.4c">\{10^{2},10^{3},10^{4},10^{5}\}</annotation><annotation encoding="application/x-llamapun" id="A3.SS6.p2.2.m2.4d">{ 10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT }</annotation></semantics></math>. For this analysis, <span class="ltx_text ltx_font_smallcaps" id="A3.SS6.p2.2.3">LocEI</span> was trained using only the second stage.</p>
</div>
<div class="ltx_para" id="A3.SS6.p3">
<p class="ltx_p" id="A3.SS6.p3.1">These results show that it is much more important to have more data in the second stage when compared to the first stage. The performance did not visibly improve over the data range considered for the first stage, indicating no benefit in training on a lot of existing links. On the other hand, the model performance improved drastically as the data size increased for the second stage, with no sign of plateauing. Based on these results, the optimal training schedule for an entity insertion model using our data seems to be a short first stage, followed by a second stage using as much data as possible.</p>
</div>
<figure class="ltx_figure" id="A3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="306" id="A3.F10.g1" src="x9.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Impact of the amount of data used in the first stage on the downstream entity insertion performance. Note that the model is trained solely using the first stage. The categorization of entity insertion types is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="306" id="A3.F11.g1" src="x10.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Impact of the amount of data used in the second stage on the downstream entity insertion performance. Note that the model is trained solely using the second stage. The categorization of entity insertion types is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.7 </span>Training stages</h3>
<div class="ltx_para" id="A3.SS7.p1">
<p class="ltx_p" id="A3.SS7.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.T14" title="Table 14 ‣ C.7 Training stages ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">14</span></a> shows the impact of different training strategies: (1) Warm start (only using the first stage), (2) Expansion (only using the second stage), and (3) Warm start + Expansion (using both stages), on the downstream entity insertion performance of <span class="ltx_text ltx_font_smallcaps" id="A3.SS7.p1.1.1">LocEI</span> using data extracted from English Wikipedia.</p>
</div>
<figure class="ltx_table" id="A3.T14">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 14: </span>Comparison of the impact of different stages of the training pipeline on the downstream entity insertion performance. The categorization of entity insertion types into ‘Overall’, ‘Missing’, and ‘Present’ is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS3" title="6.3 Setup ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6.3</span></a>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="A3.T14.6">
<tr class="ltx_tr" id="A3.T14.6.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T14.6.7.1" rowspan="2"><span class="ltx_text" id="A3.T14.6.7.1.1">Training Stages</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="A3.T14.6.7.2">Hits@1</td>
<td class="ltx_td ltx_align_center" colspan="3" id="A3.T14.6.7.3">MRR</td>
</tr>
<tr class="ltx_tr" id="A3.T14.6.8">
<td class="ltx_td ltx_align_center" id="A3.T14.6.8.1">Overall</td>
<td class="ltx_td ltx_align_center" id="A3.T14.6.8.2">Present</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T14.6.8.3">Missing</td>
<td class="ltx_td ltx_align_center" id="A3.T14.6.8.4">Overall</td>
<td class="ltx_td ltx_align_center" id="A3.T14.6.8.5">Present</td>
<td class="ltx_td ltx_align_center" id="A3.T14.6.8.6">Missing</td>
</tr>
<tr class="ltx_tr" id="A3.T14.6.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T14.6.9.1">Warm start</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T14.6.9.2">0.584</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T14.6.9.3"><span class="ltx_text ltx_font_bold" id="A3.T14.6.9.3.1">0.883</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T14.6.9.4">0.350</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T14.6.9.5">0.649</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T14.6.9.6"><span class="ltx_text ltx_font_bold" id="A3.T14.6.9.6.1">0.907</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T14.6.9.7">0.451</td>
</tr>
<tr class="ltx_tr" id="A3.T14.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T14.2.2.3">Expansion</td>
<td class="ltx_td ltx_align_center" id="A3.T14.2.2.4">0.604</td>
<td class="ltx_td ltx_align_center" id="A3.T14.2.2.5">0.738</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T14.1.1.1">0.494<sup class="ltx_sup" id="A3.T14.1.1.1.1"><span class="ltx_text ltx_font_italic" id="A3.T14.1.1.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="A3.T14.2.2.6">0.689</td>
<td class="ltx_td ltx_align_center" id="A3.T14.2.2.7">0.801</td>
<td class="ltx_td ltx_align_center" id="A3.T14.2.2.2">0.603<sup class="ltx_sup" id="A3.T14.2.2.2.1"><span class="ltx_text ltx_font_italic" id="A3.T14.2.2.2.1.1">†</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="A3.T14.6.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A3.T14.6.6.5">Warm start + Expansion</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T14.3.3.1"><span class="ltx_text ltx_font_bold" id="A3.T14.3.3.1.1">0.672<sup class="ltx_sup" id="A3.T14.3.3.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T14.3.3.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T14.4.4.2">0.877<sup class="ltx_sup" id="A3.T14.4.4.2.1"><span class="ltx_text ltx_font_italic" id="A3.T14.4.4.2.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A3.T14.6.6.6"><span class="ltx_text ltx_font_bold" id="A3.T14.6.6.6.1">0.509</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T14.5.5.3"><span class="ltx_text ltx_font_bold" id="A3.T14.5.5.3.1">0.744<sup class="ltx_sup" id="A3.T14.5.5.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T14.5.5.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T14.6.6.4">0.906<sup class="ltx_sup" id="A3.T14.6.6.4.1"><span class="ltx_text ltx_font_italic" id="A3.T14.6.6.4.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T14.6.6.7"><span class="ltx_text ltx_font_bold" id="A3.T14.6.6.7.1">0.617</span></td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="A3.I2">
<li class="ltx_item" id="A3.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">†</span>
<div class="ltx_para" id="A3.I2.ix1.p1">
<p class="ltx_p" id="A3.I2.ix1.p1.1">Indicates statistical significance (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="A3.I2.ix1.p1.1.m1.1"><semantics id="A3.I2.ix1.p1.1.m1.1a"><mrow id="A3.I2.ix1.p1.1.m1.1.1" xref="A3.I2.ix1.p1.1.m1.1.1.cmml"><mi id="A3.I2.ix1.p1.1.m1.1.1.2" xref="A3.I2.ix1.p1.1.m1.1.1.2.cmml">p</mi><mo id="A3.I2.ix1.p1.1.m1.1.1.1" xref="A3.I2.ix1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="A3.I2.ix1.p1.1.m1.1.1.3" xref="A3.I2.ix1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.I2.ix1.p1.1.m1.1b"><apply id="A3.I2.ix1.p1.1.m1.1.1.cmml" xref="A3.I2.ix1.p1.1.m1.1.1"><lt id="A3.I2.ix1.p1.1.m1.1.1.1.cmml" xref="A3.I2.ix1.p1.1.m1.1.1.1"></lt><ci id="A3.I2.ix1.p1.1.m1.1.1.2.cmml" xref="A3.I2.ix1.p1.1.m1.1.1.2">𝑝</ci><cn id="A3.I2.ix1.p1.1.m1.1.1.3.cmml" type="float" xref="A3.I2.ix1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.I2.ix1.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="A3.I2.ix1.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>) between the variant and the previous variant.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.8 </span>RoBERTa vs XLM-RoBERTa</h3>
<div class="ltx_para" id="A3.SS8.p1">
<p class="ltx_p" id="A3.SS8.p1.1">We found the scores obtained with RoBERTa on Simple English to be significantly higher than the scores achieved by the multilingual XLM-RoBERTa. In this analysis, we compare the performance of these two models on the full English dataset, with both models having been fine-tuned on the same English dataset. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.T15" title="Table 15 ‣ C.8 RoBERTa vs XLM-RoBERTa ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">15</span></a> shows a statistically significant difference in the performance of RoBERTa and XLM-RoBERTa, with RoBERTa scoring higher in all entity insertion strategies, with gaps up to 25%. We draw two conclusions from these results.</p>
</div>
<figure class="ltx_table" id="A3.T15">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 15: </span>Comparing the entity insertion performance of our model fine-tuned using the monolingual RoBERTa<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.T15.3.m1.1"><semantics id="A3.T15.3.m1.1b"><msub id="A3.T15.3.m1.1.1" xref="A3.T15.3.m1.1.1.cmml"><mi id="A3.T15.3.m1.1.1b" xref="A3.T15.3.m1.1.1.cmml"></mi><mtext id="A3.T15.3.m1.1.1.1" xref="A3.T15.3.m1.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T15.3.m1.1c"><apply id="A3.T15.3.m1.1.1.cmml" xref="A3.T15.3.m1.1.1"><ci id="A3.T15.3.m1.1.1.1a.cmml" xref="A3.T15.3.m1.1.1.1"><mtext id="A3.T15.3.m1.1.1.1.cmml" mathsize="70%" xref="A3.T15.3.m1.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T15.3.m1.1d">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.T15.3.m1.1e">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math> and the multilingual XLM-RoBERTa<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.T15.4.m2.1"><semantics id="A3.T15.4.m2.1b"><msub id="A3.T15.4.m2.1.1" xref="A3.T15.4.m2.1.1.cmml"><mi id="A3.T15.4.m2.1.1b" xref="A3.T15.4.m2.1.1.cmml"></mi><mtext id="A3.T15.4.m2.1.1.1" xref="A3.T15.4.m2.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T15.4.m2.1c"><apply id="A3.T15.4.m2.1.1.cmml" xref="A3.T15.4.m2.1.1"><ci id="A3.T15.4.m2.1.1.1a.cmml" xref="A3.T15.4.m2.1.1.1"><mtext id="A3.T15.4.m2.1.1.1.cmml" mathsize="70%" xref="A3.T15.4.m2.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T15.4.m2.1d">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.T15.4.m2.1e">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math> on the data extracted from English Wikipedia. The categorization of entity insertion types is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S4" title="4 Data ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">4</span></a>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="A3.T15.14" style="width:433.6pt;height:73.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(5.2pt,-0.9pt) scale(1.02479503356455,1.02479503356455) ;">
<table class="ltx_tabular ltx_align_middle" id="A3.T15.14.10">
<tr class="ltx_tr" id="A3.T15.14.10.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T15.14.10.11.1" rowspan="2"><span class="ltx_text" id="A3.T15.14.10.11.1.1">Model</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="A3.T15.14.10.11.2">Text Present</td>
<td class="ltx_td ltx_align_center" colspan="2" id="A3.T15.14.10.11.3">Missing Mention</td>
<td class="ltx_td ltx_align_center" colspan="2" id="A3.T15.14.10.11.4">Missing Sentence</td>
<td class="ltx_td ltx_align_center" colspan="2" id="A3.T15.14.10.11.5">Missing Span</td>
</tr>
<tr class="ltx_tr" id="A3.T15.14.10.12">
<td class="ltx_td ltx_align_center" id="A3.T15.14.10.12.1">Hits@1</td>
<td class="ltx_td ltx_align_center" id="A3.T15.14.10.12.2">MRR</td>
<td class="ltx_td ltx_align_center" id="A3.T15.14.10.12.3">Hits@1</td>
<td class="ltx_td ltx_align_center" id="A3.T15.14.10.12.4">MRR</td>
<td class="ltx_td ltx_align_center" id="A3.T15.14.10.12.5">Hits@1</td>
<td class="ltx_td ltx_align_center" id="A3.T15.14.10.12.6">MRR</td>
<td class="ltx_td ltx_align_center" id="A3.T15.14.10.12.7">Hits@1</td>
<td class="ltx_td ltx_align_center" id="A3.T15.14.10.12.8">MRR</td>
</tr>
<tr class="ltx_tr" id="A3.T15.13.9.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T15.5.1.1.1">RoBERTa<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.T15.5.1.1.1.m1.1"><semantics id="A3.T15.5.1.1.1.m1.1a"><msub id="A3.T15.5.1.1.1.m1.1.1" xref="A3.T15.5.1.1.1.m1.1.1.cmml"><mi id="A3.T15.5.1.1.1.m1.1.1a" xref="A3.T15.5.1.1.1.m1.1.1.cmml"></mi><mtext id="A3.T15.5.1.1.1.m1.1.1.1" xref="A3.T15.5.1.1.1.m1.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T15.5.1.1.1.m1.1b"><apply id="A3.T15.5.1.1.1.m1.1.1.cmml" xref="A3.T15.5.1.1.1.m1.1.1"><ci id="A3.T15.5.1.1.1.m1.1.1.1a.cmml" xref="A3.T15.5.1.1.1.m1.1.1.1"><mtext id="A3.T15.5.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.T15.5.1.1.1.m1.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T15.5.1.1.1.m1.1c">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.T15.5.1.1.1.m1.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.6.2.2.2"><span class="ltx_text ltx_font_bold" id="A3.T15.6.2.2.2.1">0.923<sup class="ltx_sup" id="A3.T15.6.2.2.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T15.6.2.2.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.7.3.3.3"><span class="ltx_text ltx_font_bold" id="A3.T15.7.3.3.3.1">0.936<sup class="ltx_sup" id="A3.T15.7.3.3.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T15.7.3.3.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.8.4.4.4"><span class="ltx_text ltx_font_bold" id="A3.T15.8.4.4.4.1">0.737<sup class="ltx_sup" id="A3.T15.8.4.4.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T15.8.4.4.4.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.9.5.5.5"><span class="ltx_text ltx_font_bold" id="A3.T15.9.5.5.5.1">0.797<sup class="ltx_sup" id="A3.T15.9.5.5.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T15.9.5.5.5.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.10.6.6.6"><span class="ltx_text ltx_font_bold" id="A3.T15.10.6.6.6.1">0.850<sup class="ltx_sup" id="A3.T15.10.6.6.6.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T15.10.6.6.6.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.11.7.7.7"><span class="ltx_text ltx_font_bold" id="A3.T15.11.7.7.7.1">0.898<sup class="ltx_sup" id="A3.T15.11.7.7.7.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T15.11.7.7.7.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.12.8.8.8"><span class="ltx_text ltx_font_bold" id="A3.T15.12.8.8.8.1">0.787<sup class="ltx_sup" id="A3.T15.12.8.8.8.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T15.12.8.8.8.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.13.9.9.9"><span class="ltx_text ltx_font_bold" id="A3.T15.13.9.9.9.1">0.848<sup class="ltx_sup" id="A3.T15.13.9.9.9.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A3.T15.13.9.9.9.1.1.1">†</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="A3.T15.14.10.10">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A3.T15.14.10.10.1">XLM-RoBERTa<math alttext="{}_{\text{BASE}}" class="ltx_Math" display="inline" id="A3.T15.14.10.10.1.m1.1"><semantics id="A3.T15.14.10.10.1.m1.1a"><msub id="A3.T15.14.10.10.1.m1.1.1" xref="A3.T15.14.10.10.1.m1.1.1.cmml"><mi id="A3.T15.14.10.10.1.m1.1.1a" xref="A3.T15.14.10.10.1.m1.1.1.cmml"></mi><mtext id="A3.T15.14.10.10.1.m1.1.1.1" xref="A3.T15.14.10.10.1.m1.1.1.1a.cmml">BASE</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T15.14.10.10.1.m1.1b"><apply id="A3.T15.14.10.10.1.m1.1.1.cmml" xref="A3.T15.14.10.10.1.m1.1.1"><ci id="A3.T15.14.10.10.1.m1.1.1.1a.cmml" xref="A3.T15.14.10.10.1.m1.1.1.1"><mtext id="A3.T15.14.10.10.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.T15.14.10.10.1.m1.1.1.1">BASE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T15.14.10.10.1.m1.1c">{}_{\text{BASE}}</annotation><annotation encoding="application/x-llamapun" id="A3.T15.14.10.10.1.m1.1d">start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T15.14.10.10.2">0.863</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T15.14.10.10.3">0.892</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T15.14.10.10.4">0.543</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T15.14.10.10.5">0.630</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T15.14.10.10.6">0.595</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T15.14.10.10.7">0.662</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T15.14.10.10.8">0.697</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T15.14.10.10.9">0.615</td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="A3.I3">
<li class="ltx_item" id="A3.I3.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">†</span>
<div class="ltx_para" id="A3.I3.ix1.p1">
<p class="ltx_p" id="A3.I3.ix1.p1.1">Indicates statistical significance (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="A3.I3.ix1.p1.1.m1.1"><semantics id="A3.I3.ix1.p1.1.m1.1a"><mrow id="A3.I3.ix1.p1.1.m1.1.1" xref="A3.I3.ix1.p1.1.m1.1.1.cmml"><mi id="A3.I3.ix1.p1.1.m1.1.1.2" xref="A3.I3.ix1.p1.1.m1.1.1.2.cmml">p</mi><mo id="A3.I3.ix1.p1.1.m1.1.1.1" xref="A3.I3.ix1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="A3.I3.ix1.p1.1.m1.1.1.3" xref="A3.I3.ix1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.I3.ix1.p1.1.m1.1b"><apply id="A3.I3.ix1.p1.1.m1.1.1.cmml" xref="A3.I3.ix1.p1.1.m1.1.1"><lt id="A3.I3.ix1.p1.1.m1.1.1.1.cmml" xref="A3.I3.ix1.p1.1.m1.1.1.1"></lt><ci id="A3.I3.ix1.p1.1.m1.1.1.2.cmml" xref="A3.I3.ix1.p1.1.m1.1.1.2">𝑝</ci><cn id="A3.I3.ix1.p1.1.m1.1.1.3.cmml" type="float" xref="A3.I3.ix1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.I3.ix1.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="A3.I3.ix1.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>).</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<div class="ltx_para" id="A3.SS8.p2">
<p class="ltx_p" id="A3.SS8.p2.1">In our ablations, we found that RoBERTa outperformed BERT and T5<math alttext="{}^{\text{enc}}" class="ltx_Math" display="inline" id="A3.SS8.p2.1.m1.1"><semantics id="A3.SS8.p2.1.m1.1a"><msup id="A3.SS8.p2.1.m1.1.1" xref="A3.SS8.p2.1.m1.1.1.cmml"><mi id="A3.SS8.p2.1.m1.1.1a" xref="A3.SS8.p2.1.m1.1.1.cmml"></mi><mtext id="A3.SS8.p2.1.m1.1.1.1" xref="A3.SS8.p2.1.m1.1.1.1a.cmml">enc</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.SS8.p2.1.m1.1b"><apply id="A3.SS8.p2.1.m1.1.1.cmml" xref="A3.SS8.p2.1.m1.1.1"><ci id="A3.SS8.p2.1.m1.1.1.1a.cmml" xref="A3.SS8.p2.1.m1.1.1.1"><mtext id="A3.SS8.p2.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.SS8.p2.1.m1.1.1.1">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS8.p2.1.m1.1c">{}^{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS8.p2.1.m1.1d">start_FLOATSUPERSCRIPT enc end_FLOATSUPERSCRIPT</annotation></semantics></math> by a large margin, which leads us to select XLM-RoBERTa as the best candidate for the multilingual model to use in our experiments. However, the performance of RoBERTa does not seem to directly correlate with the performance of XLM-RoBERTa, as seen by the large drop in English when moving from RoBERTa to XLM-RoBERTa. This finding casts some doubt on the decision of the best multilingual model and opens the doors to models like multilingual BERT and mT5 <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib52" title="">2021</a>)</cite>. In the future, it would be interesting to consider other multilingual models and see if they can outperform XLM-RoBERTa.</p>
</div>
<div class="ltx_para" id="A3.SS8.p3">
<p class="ltx_p" id="A3.SS8.p3.1">As shown in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS4" title="6.4 Main results ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6.4</span></a>, XLM-RoBERTa fine-tuned on the multilingual dataset generally outperformed XLM-RoBERTa fine-tuned on a single language. However, the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.T15" title="Table 15 ‣ C.8 RoBERTa vs XLM-RoBERTa ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">15</span></a> point to the fact that a model pre-trained on a single language (RoBERTa) outperforms a model pre-trained on multiple languages (XLM-RoBERTa). The dominance of the monolingual model is not surprising as a model pre-trained on a single language had a much smaller domain to learn than a multilingual model, and thus, might have been able to learn linguistic and structural patterns that the multilingual model failed to capture. So, for the languages where a pre-trained model does exist (for example, BERT for English, CamemBERT <cite class="ltx_cite ltx_citemacro_cite">Müller et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib29" title="">2020</a>)</cite> for French, HerBERT <cite class="ltx_cite ltx_citemacro_cite">Rybak et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#bib.bib38" title="">2020</a>)</cite> for Polish), that model may outperform the multilingual variant. However, it is unrealistic to assume that there can be a pre-trained model for each of the 300+ languages of Wikipedia. The multilingual model becomes essential for the languages for which there is no pre-trained model. As we saw in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS4" title="6.4 Main results ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6.4</span></a> and § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS4.SSSx1" title="Zero-shot vs. Fine-tuned ‣ 6.4 Main results ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6.4</span></a>, the multilingual model is capable of transferring knowledge to unseen languages, which proves its potential for low-resource languages for which a full pre-trained model is not realistic.</p>
</div>
<figure class="ltx_table" id="A3.T16">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 16: </span>Comparing the entity insertion performance obtained for Simple English with different loss functions: pointwise vs. ranking loss. The categorization of entity insertion types into ‘Overall’, ‘Missing’, and ‘Present’ is discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S6.SS3" title="6.3 Setup ‣ 6 Experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">6.3</span></a>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T16.1">
<tr class="ltx_tr" id="A3.T16.1.1">
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T16.1.1.1" rowspan="2"><span class="ltx_text" id="A3.T16.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="A3.T16.1.1.2">Hits@1</td>
<td class="ltx_td ltx_align_center" colspan="3" id="A3.T16.1.1.3">MRR</td>
</tr>
<tr class="ltx_tr" id="A3.T16.1.2">
<td class="ltx_td ltx_align_center" id="A3.T16.1.2.1">Overall</td>
<td class="ltx_td ltx_align_center" id="A3.T16.1.2.2">Present</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T16.1.2.3">Missing</td>
<td class="ltx_td ltx_align_center" id="A3.T16.1.2.4">Overall</td>
<td class="ltx_td ltx_align_center" id="A3.T16.1.2.5">Present</td>
<td class="ltx_td ltx_align_center" id="A3.T16.1.2.6">Missing</td>
</tr>
<tr class="ltx_tr" id="A3.T16.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T16.1.3.1">Pointwise Loss</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T16.1.3.2">0.641</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T16.1.3.3">0.891</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T16.1.3.4">0.477</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T16.1.3.5">0.712</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T16.1.3.6">0.922</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T16.1.3.7">0.574</td>
</tr>
<tr class="ltx_tr" id="A3.T16.1.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A3.T16.1.4.1">Ranking Loss</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T16.1.4.2"><span class="ltx_text ltx_font_bold" id="A3.T16.1.4.2.1">0.658</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T16.1.4.3"><span class="ltx_text ltx_font_bold" id="A3.T16.1.4.3.1">0.907</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A3.T16.1.4.4"><span class="ltx_text ltx_font_bold" id="A3.T16.1.4.4.1">0.495</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T16.1.4.5"><span class="ltx_text ltx_font_bold" id="A3.T16.1.4.5.1">0.731</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T16.1.4.6"><span class="ltx_text ltx_font_bold" id="A3.T16.1.4.6.1">0.930</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T16.1.4.7"><span class="ltx_text ltx_font_bold" id="A3.T16.1.4.7.1">0.601</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.9 </span>Single Encoder vs Triple Encoder</h3>
<div class="ltx_para" id="A3.SS9.p1">
<p class="ltx_p" id="A3.SS9.p1.1">In early iterations of our work, we explored a different model architecture. This architecture used the additional knowledge of the source article. Given the amount of text that needed to be encoded, and considering that most transformers have a limited number of tokens they can process, we chose to encode each of the three components separately. We had the following input representations:</p>
<ul class="ltx_itemize" id="A3.I4">
<li class="ltx_item" id="A3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I4.i1.p1">
<p class="ltx_p" id="A3.I4.i1.p1.1">Source Article: [CLS]&lt;Src Title&gt;[SEP]&lt;Src Lead&gt;</p>
</div>
</li>
<li class="ltx_item" id="A3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I4.i2.p1">
<p class="ltx_p" id="A3.I4.i2.p1.1">Candidate: [CLS]&lt;Src Section&gt;[SEP]&lt;Tgt Mention&gt;[SEP]&lt;Context&gt;</p>
</div>
</li>
<li class="ltx_item" id="A3.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I4.i3.p1">
<p class="ltx_p" id="A3.I4.i3.p1.1">Target Title: [CLS]&lt;Tgt Title&gt;[SEP]&lt;Tgt Lead&gt;</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A3.SS9.p2">
<p class="ltx_p" id="A3.SS9.p2.1">Each of the components of the triplet was encoded independently, and then stacked together. Finally, an MLP capturing the interactions between the three embeddings was used to produce a relevance score.</p>
</div>
<div class="ltx_para" id="A3.SS9.p3">
<p class="ltx_p" id="A3.SS9.p3.1">The key intuition behind this architecture was to represent a link as a knowledge triplet <span class="ltx_text ltx_font_typewriter" id="A3.SS9.p3.1.1">&lt;src, text, tgt&gt;</span>, and the overall architecture was supposed to predict whether the triplet was correct. However, we found that such an architecture decayed into a state where the target and source embeddings were independent of the input, always producing the same embedding. We believe that the model relied exclusively on the semantic knowledge contained in the list of target mentions to identify whether the entity should be inserted in the candidate text span, and the source and target article embeddings decayed into a global average optimum that maximized the performance of the MLP for the candidate embedding. Nevertheless, this meant that all the knowledge about the target entity contained in the target lead was being ignored.</p>
</div>
<div class="ltx_para" id="A3.SS9.p4">
<p class="ltx_p" id="A3.SS9.p4.1">To take advantage of the total available information, we moved to the architecture described in § <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#S5.SS1" title="5.1 Model ‣ 5 Entity insertion with LocEI ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">5.1</span></a>. We removed the source title and the source lead, driven by the token limit of the transformer architecture. We believed that this knowledge provided the least marginal gain from the three components of the triplet, at a cost of token space for the candidate and the target, as the source article knowledge only gave additional context to the candidate text span.</p>
</div>
<div class="ltx_para" id="A3.SS9.p5">
<p class="ltx_p" id="A3.SS9.p5.1">We additionally moved to a single encoder for two reasons. First, the transformer architecture is more expressive than an MLP, and thus, it was better suited to capture the interactions between the candidate and the target. With only two knowledge sources instead of three, we felt we had sufficient token space for each source to capture enough semantic information for each input. Second, by relying on one single embedding, the embedding couldn’t decay into a global average optimum which provided no information about the input, because the relevance score was entirely dependent on the representation power of that single embedding.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS10">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.10 </span>Pointwise Loss vs Ranking Loss</h3>
<div class="ltx_para" id="A3.SS10.p1">
<p class="ltx_p" id="A3.SS10.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04254v1#A3.T16" title="Table 16 ‣ C.8 RoBERTa vs XLM-RoBERTa ‣ Appendix C Additional experiments ‣ Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia"><span class="ltx_text ltx_ref_tag">16</span></a> shows how the choice of different loss functions (pointwise vs. ranking) impacts the downstream entity insertion performance of our models evaluated on the Simple English dataset.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Oct  5 18:16:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
