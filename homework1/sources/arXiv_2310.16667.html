<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.16667] CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection</title><meta property="og:description" content="Deriving reliable region-word alignment from image-text pairs is critical to learn object-level vision-language representations for open-vocabulary object detection. Existing methods typically rely on pre-trained or se…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.16667">

<!--Generated on Tue Feb 27 23:57:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Chuofan Ma<sup id="id12.8.id1" class="ltx_sup"><span id="id12.8.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>   
Yi Jiang<sup id="id13.9.id2" class="ltx_sup"><span id="id13.9.id2.1" class="ltx_text ltx_font_italic">2</span></sup>   
Xin Wen<sup id="id14.10.id3" class="ltx_sup"><span id="id14.10.id3.1" class="ltx_text ltx_font_italic">1∗</span></sup>   
Zehuan Yuan<sup id="id15.11.id4" class="ltx_sup"><span id="id15.11.id4.1" class="ltx_text ltx_font_italic">2</span></sup>   
Xiaojuan Qi<sup id="id16.12.id5" class="ltx_sup"><span id="id16.12.id5.1" class="ltx_text ltx_font_italic">1</span></sup>    
<br class="ltx_break"><sup id="id17.13.id6" class="ltx_sup">1</sup>The University of Hong Kong    <sup id="id18.14.id7" class="ltx_sup">2</sup>ByteDance Inc.    
<br class="ltx_break">
</span><span class="ltx_author_notes">This work was performed when Chuofan Ma worked as an intern at ByteDance.Equal contribution.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.4" class="ltx_p">Deriving reliable region-word alignment from image-text pairs is critical to learn object-level vision-language representations for open-vocabulary object detection. Existing methods typically rely on pre-trained or self-trained vision-language models for alignment, which are prone to limitations in localization accuracy or generalization capabilities. In this paper, we propose CoDet, a novel approach that overcomes the reliance on pre-aligned vision-language space by reformulating region-word alignment as a co-occurring object discovery problem.
Intuitively, by grouping images that mention a shared concept in their captions, objects corresponding to the shared concept shall exhibit high co-occurrence among the group. CoDet then leverages visual similarities to discover the co-occurring objects and align them with the shared concept. Extensive experiments demonstrate that CoDet has superior performances and compelling scalability in open-vocabulary detection, <span id="id11.4.1" class="ltx_ERROR undefined">\eg</span>, by scaling up the visual backbone, CoDet achieves 37.0 <math id="id8.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{novel}}^{\text{m}}" display="inline"><semantics id="id8.1.m1.1a"><msubsup id="id8.1.m1.1.1" xref="id8.1.m1.1.1.cmml"><mtext id="id8.1.m1.1.1.2.2" xref="id8.1.m1.1.1.2.2a.cmml">AP</mtext><mtext id="id8.1.m1.1.1.2.3" xref="id8.1.m1.1.1.2.3a.cmml">novel</mtext><mtext id="id8.1.m1.1.1.3" xref="id8.1.m1.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="id8.1.m1.1b"><apply id="id8.1.m1.1.1.cmml" xref="id8.1.m1.1.1"><csymbol cd="ambiguous" id="id8.1.m1.1.1.1.cmml" xref="id8.1.m1.1.1">superscript</csymbol><apply id="id8.1.m1.1.1.2.cmml" xref="id8.1.m1.1.1"><csymbol cd="ambiguous" id="id8.1.m1.1.1.2.1.cmml" xref="id8.1.m1.1.1">subscript</csymbol><ci id="id8.1.m1.1.1.2.2a.cmml" xref="id8.1.m1.1.1.2.2"><mtext id="id8.1.m1.1.1.2.2.cmml" xref="id8.1.m1.1.1.2.2">AP</mtext></ci><ci id="id8.1.m1.1.1.2.3a.cmml" xref="id8.1.m1.1.1.2.3"><mtext mathsize="70%" id="id8.1.m1.1.1.2.3.cmml" xref="id8.1.m1.1.1.2.3">novel</mtext></ci></apply><ci id="id8.1.m1.1.1.3a.cmml" xref="id8.1.m1.1.1.3"><mtext mathsize="70%" id="id8.1.m1.1.1.3.cmml" xref="id8.1.m1.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.1.m1.1c">\text{AP}_{\text{novel}}^{\text{m}}</annotation></semantics></math> and 44.7 <math id="id9.2.m2.1" class="ltx_Math" alttext="\text{AP}_{\text{all}}^{\text{m}}" display="inline"><semantics id="id9.2.m2.1a"><msubsup id="id9.2.m2.1.1" xref="id9.2.m2.1.1.cmml"><mtext id="id9.2.m2.1.1.2.2" xref="id9.2.m2.1.1.2.2a.cmml">AP</mtext><mtext id="id9.2.m2.1.1.2.3" xref="id9.2.m2.1.1.2.3a.cmml">all</mtext><mtext id="id9.2.m2.1.1.3" xref="id9.2.m2.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="id9.2.m2.1b"><apply id="id9.2.m2.1.1.cmml" xref="id9.2.m2.1.1"><csymbol cd="ambiguous" id="id9.2.m2.1.1.1.cmml" xref="id9.2.m2.1.1">superscript</csymbol><apply id="id9.2.m2.1.1.2.cmml" xref="id9.2.m2.1.1"><csymbol cd="ambiguous" id="id9.2.m2.1.1.2.1.cmml" xref="id9.2.m2.1.1">subscript</csymbol><ci id="id9.2.m2.1.1.2.2a.cmml" xref="id9.2.m2.1.1.2.2"><mtext id="id9.2.m2.1.1.2.2.cmml" xref="id9.2.m2.1.1.2.2">AP</mtext></ci><ci id="id9.2.m2.1.1.2.3a.cmml" xref="id9.2.m2.1.1.2.3"><mtext mathsize="70%" id="id9.2.m2.1.1.2.3.cmml" xref="id9.2.m2.1.1.2.3">all</mtext></ci></apply><ci id="id9.2.m2.1.1.3a.cmml" xref="id9.2.m2.1.1.3"><mtext mathsize="70%" id="id9.2.m2.1.1.3.cmml" xref="id9.2.m2.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.2.m2.1c">\text{AP}_{\text{all}}^{\text{m}}</annotation></semantics></math> on OV-LVIS, surpassing the previous SoTA by 4.2 <math id="id10.3.m3.1" class="ltx_Math" alttext="\text{AP}_{\text{novel}}^{\text{m}}" display="inline"><semantics id="id10.3.m3.1a"><msubsup id="id10.3.m3.1.1" xref="id10.3.m3.1.1.cmml"><mtext id="id10.3.m3.1.1.2.2" xref="id10.3.m3.1.1.2.2a.cmml">AP</mtext><mtext id="id10.3.m3.1.1.2.3" xref="id10.3.m3.1.1.2.3a.cmml">novel</mtext><mtext id="id10.3.m3.1.1.3" xref="id10.3.m3.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="id10.3.m3.1b"><apply id="id10.3.m3.1.1.cmml" xref="id10.3.m3.1.1"><csymbol cd="ambiguous" id="id10.3.m3.1.1.1.cmml" xref="id10.3.m3.1.1">superscript</csymbol><apply id="id10.3.m3.1.1.2.cmml" xref="id10.3.m3.1.1"><csymbol cd="ambiguous" id="id10.3.m3.1.1.2.1.cmml" xref="id10.3.m3.1.1">subscript</csymbol><ci id="id10.3.m3.1.1.2.2a.cmml" xref="id10.3.m3.1.1.2.2"><mtext id="id10.3.m3.1.1.2.2.cmml" xref="id10.3.m3.1.1.2.2">AP</mtext></ci><ci id="id10.3.m3.1.1.2.3a.cmml" xref="id10.3.m3.1.1.2.3"><mtext mathsize="70%" id="id10.3.m3.1.1.2.3.cmml" xref="id10.3.m3.1.1.2.3">novel</mtext></ci></apply><ci id="id10.3.m3.1.1.3a.cmml" xref="id10.3.m3.1.1.3"><mtext mathsize="70%" id="id10.3.m3.1.1.3.cmml" xref="id10.3.m3.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.3.m3.1c">\text{AP}_{\text{novel}}^{\text{m}}</annotation></semantics></math> and 9.8 <math id="id11.4.m4.1" class="ltx_Math" alttext="\text{AP}_{\text{all}}^{\text{m}}" display="inline"><semantics id="id11.4.m4.1a"><msubsup id="id11.4.m4.1.1" xref="id11.4.m4.1.1.cmml"><mtext id="id11.4.m4.1.1.2.2" xref="id11.4.m4.1.1.2.2a.cmml">AP</mtext><mtext id="id11.4.m4.1.1.2.3" xref="id11.4.m4.1.1.2.3a.cmml">all</mtext><mtext id="id11.4.m4.1.1.3" xref="id11.4.m4.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="id11.4.m4.1b"><apply id="id11.4.m4.1.1.cmml" xref="id11.4.m4.1.1"><csymbol cd="ambiguous" id="id11.4.m4.1.1.1.cmml" xref="id11.4.m4.1.1">superscript</csymbol><apply id="id11.4.m4.1.1.2.cmml" xref="id11.4.m4.1.1"><csymbol cd="ambiguous" id="id11.4.m4.1.1.2.1.cmml" xref="id11.4.m4.1.1">subscript</csymbol><ci id="id11.4.m4.1.1.2.2a.cmml" xref="id11.4.m4.1.1.2.2"><mtext id="id11.4.m4.1.1.2.2.cmml" xref="id11.4.m4.1.1.2.2">AP</mtext></ci><ci id="id11.4.m4.1.1.2.3a.cmml" xref="id11.4.m4.1.1.2.3"><mtext mathsize="70%" id="id11.4.m4.1.1.2.3.cmml" xref="id11.4.m4.1.1.2.3">all</mtext></ci></apply><ci id="id11.4.m4.1.1.3a.cmml" xref="id11.4.m4.1.1.3"><mtext mathsize="70%" id="id11.4.m4.1.1.3.cmml" xref="id11.4.m4.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.4.m4.1c">\text{AP}_{\text{all}}^{\text{m}}</annotation></semantics></math>. Code is available at <a target="_blank" href="https://github.com/CVMI-Lab/CoDet" title="" class="ltx_ref ltx_href">https://github.com/CVMI-Lab/CoDet</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Object detection is a fundamental vision task that offers object-centric comprehension of visual scenes for various downstream applications. While remarkable progress has been made in terms of detection accuracy and speed, traditional detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> are mostly constrained to a fixed vocabulary defined by training data, <span id="S1.p1.1.1" class="ltx_ERROR undefined">\eg</span>, 80 categories in COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>. This accounts for a major gap compared to human visual intelligence, which can perceive a diverse range of visual concepts in the open world. To address such limitations, this paper focuses on the open-vocabulary setting of object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>, where the detector is trained to recognize objects of arbitrary categories.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recently, vision-language pretraining on web-scale image-text pairs has demonstrated impressive open-vocabulary capability in image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>. It inspires the community to adapt this paradigm to object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, specifically by training an open-vocabulary detector using region-text pairs in detection or grounding annotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>.
However, unlike free-form image-text pairs, human-annotated region-text pairs are limited and difficult to scale. Consequently, a growing body of research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> aims to mine additional region-text pairs from image-text pairs, which raises a new question: <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">how to find the alignments between regions and words?</em> (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.p2.1.2" class="ltx_text" style="color:#FF0000;">a</span>)</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recent studies typically rely on vision-language models (VLMs) to determine region-word alignments, for example, by estimating region-word similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>. Despite its simplicity, the quality of generated pseudo region-text pairs is subject to limitations of VLMs. As illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.p3.1.1" class="ltx_text" style="color:#FF0000;">b</span>, VLMs pre-trained with image-level supervision, such as CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>, are largely unaware of localization quality of pseudo labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>. Although detector-based VLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite> mitigate this issue to some extent, they are initially pre-trained with a limited number of detection or grounding concepts, resulting in inaccurate alignments for novel concepts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite>. Furthermore, this approach essentially faces a chicken-and-egg problem: obtaining high-quality region-word pairs requires a VLM with object-level vision-language knowledge, yet training such a VLM, in turn, depends on a large number of aligned region-word pairs.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, instead of directly aligning regions and words with VLMs, we propose leveraging region correspondences across images for co-occurring concept discovery and alignment, which we call CoDet. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.p4.1.1" class="ltx_text" style="color:#FF0000;">c</span> illustrates the idea. Our key motivation is that objects corresponding to the same concept should exhibit consistent visual similarity across images, which provides visual clues to identity region-word correspondences. Based on this intuition, we construct semantic groups by sampling images that mention a shared concept in their captions, from which we can infer that a common object corresponding to the shared concept exists across images. Subsequently, we leverage cross-image region similarity to identify regions potentially containing the common object, and construct a prototype from them. The prototype and the shared concept form a natural region-text pair, which is then adopted to supervise the training of an open-vocabulary object detector.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Unlike previous works, our method avoids the dependence on a pre-aligned vision-language space and <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">solely relies on the vision space</span> to discover region-word correspondences. However, there could exist multiple co-occurring concepts in the same group, and even the same concept may still exhibit high intra-category variation in appearance, in which case general visual similarity would fail to distinguish the object of interest. To address this issue, we introduce text guidance into similarity estimation between region proposals, making it concept-aware and more accurately reflecting the closeness of objects concerning the shared semantic concept.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.16667/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.9.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.10.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustration of different region-text alignment paradigms.<span id="S1.F1.10.2.1" class="ltx_text ltx_font_medium">
(a): example image-text pair, and region proposals generated by a pre-trained region proposal network;
(b): a pre-trained VLM (<span id="S1.F1.10.2.1.1" class="ltx_ERROR undefined">\eg</span>, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>) is used to retrieve the box with the highest region-word similarity concerning the query text, which yet exhibits poor localization quality;
(c) our method overcomes the reliance on VLMs by exploring visual clues, <span id="S1.F1.10.2.1.2" class="ltx_ERROR undefined">\ie</span>, object co-occurrence, within a group of image-text pairs containing the same concept (<span id="S1.F1.10.2.1.3" class="ltx_ERROR undefined">\eg</span>, <span id="S1.F1.10.2.1.4" class="ltx_text" style="color:#FFC000;">frisbee</span> <span id="S1.F1.10.2.1.5" class="ltx_ERROR undefined">\twemoji</span>flying disc). <span id="S1.F1.10.2.1.6" class="ltx_text ltx_font_italic">Best viewed in color.</span></span></span></figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The main contributions of this paper can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a novel perspective in discovering region-word correspondences from image-text pairs, which bypasses the dependence on a pre-aligned vision-language space by reformulating region-word alignment as a co-occurring object discovery problem.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Building on this insight, we propose CoDet, an open-vocabulary detection framework that learns object-level vision-language alignment directly from web-crawled image-text pairs.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">CoDet consistently outperforms existing methods on the challenging OV-LVIS benchmark and demonstrates superior performances in cross-dataset detection on COCO and Objects365.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.4" class="ltx_p">CoDet exhibits strong scalability with visual backbones - it achieves <math id="S1.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="23.4/29.4/37.0" display="inline"><semantics id="S1.I1.i4.p1.1.m1.1a"><mrow id="S1.I1.i4.p1.1.m1.1.1" xref="S1.I1.i4.p1.1.m1.1.1.cmml"><mn id="S1.I1.i4.p1.1.m1.1.1.2" xref="S1.I1.i4.p1.1.m1.1.1.2.cmml">23.4</mn><mo id="S1.I1.i4.p1.1.m1.1.1.1" xref="S1.I1.i4.p1.1.m1.1.1.1.cmml">/</mo><mn id="S1.I1.i4.p1.1.m1.1.1.3" xref="S1.I1.i4.p1.1.m1.1.1.3.cmml">29.4</mn><mo id="S1.I1.i4.p1.1.m1.1.1.1a" xref="S1.I1.i4.p1.1.m1.1.1.1.cmml">/</mo><mn id="S1.I1.i4.p1.1.m1.1.1.4" xref="S1.I1.i4.p1.1.m1.1.1.4.cmml">37.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i4.p1.1.m1.1b"><apply id="S1.I1.i4.p1.1.m1.1.1.cmml" xref="S1.I1.i4.p1.1.m1.1.1"><divide id="S1.I1.i4.p1.1.m1.1.1.1.cmml" xref="S1.I1.i4.p1.1.m1.1.1.1"></divide><cn type="float" id="S1.I1.i4.p1.1.m1.1.1.2.cmml" xref="S1.I1.i4.p1.1.m1.1.1.2">23.4</cn><cn type="float" id="S1.I1.i4.p1.1.m1.1.1.3.cmml" xref="S1.I1.i4.p1.1.m1.1.1.3">29.4</cn><cn type="float" id="S1.I1.i4.p1.1.m1.1.1.4.cmml" xref="S1.I1.i4.p1.1.m1.1.1.4">37.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i4.p1.1.m1.1c">23.4/29.4/37.0</annotation></semantics></math> mask <math id="S1.I1.i4.p1.2.m2.1" class="ltx_Math" alttext="\text{AP}_{\text{novel}}" display="inline"><semantics id="S1.I1.i4.p1.2.m2.1a"><msub id="S1.I1.i4.p1.2.m2.1.1" xref="S1.I1.i4.p1.2.m2.1.1.cmml"><mtext id="S1.I1.i4.p1.2.m2.1.1.2" xref="S1.I1.i4.p1.2.m2.1.1.2a.cmml">AP</mtext><mtext id="S1.I1.i4.p1.2.m2.1.1.3" xref="S1.I1.i4.p1.2.m2.1.1.3a.cmml">novel</mtext></msub><annotation-xml encoding="MathML-Content" id="S1.I1.i4.p1.2.m2.1b"><apply id="S1.I1.i4.p1.2.m2.1.1.cmml" xref="S1.I1.i4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S1.I1.i4.p1.2.m2.1.1.1.cmml" xref="S1.I1.i4.p1.2.m2.1.1">subscript</csymbol><ci id="S1.I1.i4.p1.2.m2.1.1.2a.cmml" xref="S1.I1.i4.p1.2.m2.1.1.2"><mtext id="S1.I1.i4.p1.2.m2.1.1.2.cmml" xref="S1.I1.i4.p1.2.m2.1.1.2">AP</mtext></ci><ci id="S1.I1.i4.p1.2.m2.1.1.3a.cmml" xref="S1.I1.i4.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S1.I1.i4.p1.2.m2.1.1.3.cmml" xref="S1.I1.i4.p1.2.m2.1.1.3">novel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i4.p1.2.m2.1c">\text{AP}_{\text{novel}}</annotation></semantics></math> with ResNet50/Swin-B/EVA02-L backbone, outperforming previous SoTA at a comparable model size by <math id="S1.I1.i4.p1.3.m3.1" class="ltx_Math" alttext="0.8/3.1/4.2" display="inline"><semantics id="S1.I1.i4.p1.3.m3.1a"><mrow id="S1.I1.i4.p1.3.m3.1.1" xref="S1.I1.i4.p1.3.m3.1.1.cmml"><mn id="S1.I1.i4.p1.3.m3.1.1.2" xref="S1.I1.i4.p1.3.m3.1.1.2.cmml">0.8</mn><mo id="S1.I1.i4.p1.3.m3.1.1.1" xref="S1.I1.i4.p1.3.m3.1.1.1.cmml">/</mo><mn id="S1.I1.i4.p1.3.m3.1.1.3" xref="S1.I1.i4.p1.3.m3.1.1.3.cmml">3.1</mn><mo id="S1.I1.i4.p1.3.m3.1.1.1a" xref="S1.I1.i4.p1.3.m3.1.1.1.cmml">/</mo><mn id="S1.I1.i4.p1.3.m3.1.1.4" xref="S1.I1.i4.p1.3.m3.1.1.4.cmml">4.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i4.p1.3.m3.1b"><apply id="S1.I1.i4.p1.3.m3.1.1.cmml" xref="S1.I1.i4.p1.3.m3.1.1"><divide id="S1.I1.i4.p1.3.m3.1.1.1.cmml" xref="S1.I1.i4.p1.3.m3.1.1.1"></divide><cn type="float" id="S1.I1.i4.p1.3.m3.1.1.2.cmml" xref="S1.I1.i4.p1.3.m3.1.1.2">0.8</cn><cn type="float" id="S1.I1.i4.p1.3.m3.1.1.3.cmml" xref="S1.I1.i4.p1.3.m3.1.1.3">3.1</cn><cn type="float" id="S1.I1.i4.p1.3.m3.1.1.4.cmml" xref="S1.I1.i4.p1.3.m3.1.1.4">4.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i4.p1.3.m3.1c">0.8/3.1/4.2</annotation></semantics></math> mask <math id="S1.I1.i4.p1.4.m4.1" class="ltx_Math" alttext="\text{AP}_{\text{novel}}" display="inline"><semantics id="S1.I1.i4.p1.4.m4.1a"><msub id="S1.I1.i4.p1.4.m4.1.1" xref="S1.I1.i4.p1.4.m4.1.1.cmml"><mtext id="S1.I1.i4.p1.4.m4.1.1.2" xref="S1.I1.i4.p1.4.m4.1.1.2a.cmml">AP</mtext><mtext id="S1.I1.i4.p1.4.m4.1.1.3" xref="S1.I1.i4.p1.4.m4.1.1.3a.cmml">novel</mtext></msub><annotation-xml encoding="MathML-Content" id="S1.I1.i4.p1.4.m4.1b"><apply id="S1.I1.i4.p1.4.m4.1.1.cmml" xref="S1.I1.i4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S1.I1.i4.p1.4.m4.1.1.1.cmml" xref="S1.I1.i4.p1.4.m4.1.1">subscript</csymbol><ci id="S1.I1.i4.p1.4.m4.1.1.2a.cmml" xref="S1.I1.i4.p1.4.m4.1.1.2"><mtext id="S1.I1.i4.p1.4.m4.1.1.2.cmml" xref="S1.I1.i4.p1.4.m4.1.1.2">AP</mtext></ci><ci id="S1.I1.i4.p1.4.m4.1.1.3a.cmml" xref="S1.I1.i4.p1.4.m4.1.1.3"><mtext mathsize="70%" id="S1.I1.i4.p1.4.m4.1.1.3.cmml" xref="S1.I1.i4.p1.4.m4.1.1.3">novel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i4.p1.4.m4.1c">\text{AP}_{\text{novel}}</annotation></semantics></math>, respectively.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Zero-shot object detection (ZSD)</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">leverages language feature space for generalization to unseen objects. The basic idea is to project region features to the pre-computed text embedding space (<span id="S2.SS0.SSS0.Px1.p1.1.1" class="ltx_ERROR undefined">\eg</span>, GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>) and use word embeddings as the classifier weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>. This presents ZSD with the flexibility of recognizing unseen objects given its name during inference. Nevertheless, ZSD settings restrict training samples to come from a limited number of seen classes, which is not sufficient to align the feature space of vision and language. Although some works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> try to overcome this limitation by hallucinating novel classes using Generative Adversarial Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, there is still a large performance gap between ZSD and its supervised counterparts.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Weakly supervised object detection (WSD)</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">exploits data with image-level labels to train an object detector. It typically treats an image as a bag of proposals, and assigns the image label to these proposals through multiple instance learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>. By relieving object detection from costly instance-level annotations, WSD is able to scale detection vocabulary with cheaper classification data. For instance, recent work Detic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite> greatly expands the vocabulary of detectors to twenty-thousand classes by leveraging image-level supervision from ImageNet-21K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>. However, WSD still requires non-trivial annotation efforts and has a closed vocabulary during inference.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Open-vocabulary object detection (OVD)</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">is built upon the framework of ZSD, but relaxes the stringent definition of novel classes from ‘not seen’ to ‘not known in advance’, which leads to a more practical setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>. Particularly, with recent advancement of vision-language pre-training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>, a widely adopted approach of OVD is to transfer the knowledge of pre-trained vision-language models (VLMs), <span id="S2.SS0.SSS0.Px3.p1.1.1" class="ltx_ERROR undefined">\eg</span>, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>, to object detectors through distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> or weight transfer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>. Despite its utility, performances of these methods are arguably restricted by the teacher VLM, which is shown to be largely unaware of fine-grained region-word alignment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. Alternatively, another group of works utilize large-scale image-text pairs to expand detection vocabulary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>, sharing a similar idea as WSD. Due to the absence of regional annotations in image-caption data, these methods typically rely on pre-trained or self-trained VLMs to find region-word correspondences, which are prone to limitations in localization accuracy or generalization capabilities. Our method is orthogonal to all the aforementioned approaches in the sense that it does not explicitly model region-word correspondences, but leverage region correspondences across images to bridge regions and words, which greatly simplifies the task.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Cross-image Region Correspondence</h4>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">is widely explored to discover semantically related regions among a collection of images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>. Based on the observation that modern visual backbones provide consistent semantic correspondences in the feature space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, many works take a heuristic approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> or use clustering algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> for common region discovery. Our method takes inspiration from these works to discover co-occurring objects across image-text pairs, with newly proposed text guidance.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present CoDet, an end-to-end framework exploiting image-text pairs for open-vocabulary object detection. Figure <a href="#S3.F2" title="Figure 2 ‣ Task formulation. ‣ 3.1 Preliminaries ‣ 3 Method ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives an overview of CoDet. We first provide a brief introduction to the OVD setup (Sec. <a href="#S3.SS1" title="3.1 Preliminaries ‣ 3 Method ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). Then we discuss how to reformulate region-word alignment as a co-occurring object discovery problem (Sec. <a href="#S3.SS2" title="3.2 Aligning Regions and Words by Co-occurrence ‣ 3 Method ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>), which is subsequently addressed by CoDet (Sec. <a href="#S3.SS3" title="3.3 Discovering Co-occurring Objects across Images ‣ 3 Method ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>). Finally, we summarize the overall training objectives and inference pipelines of CoDet (Sec. <a href="#S3.SS4" title="3.4 Training and Inference ‣ 3 Method ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminaries</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Task formulation.</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.5" class="ltx_p">In our study, we adopt the classical OVD problem setup as in OVR-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>. Specifically, box annotations are only provided for a predetermined set of base categories <math id="S3.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{C}^{\text{base}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><msup id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">𝒞</mi><mtext id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3a.cmml">base</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝒞</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">\mathcal{C}^{\text{base}}</annotation></semantics></math> during training. While at the test phase, the object detector is required to generalize beyond <math id="S3.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{C}^{\text{base}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><msup id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">𝒞</mi><mtext id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3a.cmml">base</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2">𝒞</ci><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">\mathcal{C}^{\text{base}}</annotation></semantics></math> to detect objects from novel categories <math id="S3.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{C}^{\text{novel}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.3.m3.1a"><msup id="S3.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml">𝒞</mi><mtext id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3a.cmml">novel</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2">𝒞</ci><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3">novel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.3.m3.1c">\mathcal{C}^{\text{novel}}</annotation></semantics></math>. Notably, <math id="S3.SS1.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{C}^{\text{novel}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.4.m4.1a"><msup id="S3.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml">𝒞</mi><mtext id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3a.cmml">novel</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2">𝒞</ci><ci id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3">novel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.4.m4.1c">\mathcal{C}^{\text{novel}}</annotation></semantics></math> is not known in advance to simulate open-world scenarios. This implies the object detector needs to possess the capability to recognize potentially any object based on its name. To achieve this goal, we additionally leverage image-text pairs with an unbounded vocabulary <math id="S3.SS1.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{C}^{\text{open}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.5.m5.1a"><msup id="S3.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml">𝒞</mi><mtext id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3a.cmml">open</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.5.m5.1b"><apply id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2">𝒞</ci><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3">open</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.5.m5.1c">\mathcal{C}^{\text{open}}</annotation></semantics></math> to extend the lexicon of the object detector.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2310.16667/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.5.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Overview of CoDet.<span id="S3.F2.6.2.1" class="ltx_text ltx_font_medium"> Our method learns to jointly discover region-word pairs from a group of image-text pairs that mention a shared concept in their captions (<span id="S3.F2.6.2.1.1" class="ltx_ERROR undefined">\eg</span>, dog in the figure). We identify the co-occurring objects and the shared concept as natural region-word pairs. Then we leverage inter-image region correspondences, <span id="S3.F2.6.2.1.2" class="ltx_ERROR undefined">\ie</span>, region-region similarity, with text guidance to locate the co-occurring objects for region-word alignment.
</span></span></figcaption>
</figure>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">OVD framework.</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">Our method is built on the two-stage detection framework Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>. To adapt it for the open-vocabulary setting, we follow the common practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite> to decouple localization from classification, by replacing the class-specific localization heads with class-agnostic ones that produce a single box or mask prediction for each region proposal. Besides, to enable open-vocabulary classification of objects, the fixed classifier weights are substituted with dynamic text embeddings of category names, which are generated by the pre-trained text encoder of CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Aligning Regions and Words by Co-occurrence</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.11" class="ltx_p">Due to the absence of box annotations, a core challenge of utilizing image-text pairs for detection training is to figure out the fine-grained alignments between regions and words. To put it formally, for an image-text pair <math id="S3.SS2.p1.1.m1.2" class="ltx_Math" alttext="\left\langle I,T\right\rangle" display="inline"><semantics id="S3.SS2.p1.1.m1.2a"><mrow id="S3.SS2.p1.1.m1.2.3.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml"><mo id="S3.SS2.p1.1.m1.2.3.2.1" xref="S3.SS2.p1.1.m1.2.3.1.cmml">⟨</mo><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">I</mi><mo id="S3.SS2.p1.1.m1.2.3.2.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml">T</mi><mo id="S3.SS2.p1.1.m1.2.3.2.3" xref="S3.SS2.p1.1.m1.2.3.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.2b"><list id="S3.SS2.p1.1.m1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3.2"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝐼</ci><ci id="S3.SS2.p1.1.m1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2">𝑇</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">\left\langle I,T\right\rangle</annotation></semantics></math>, <math id="S3.SS2.p1.2.m2.5" class="ltx_Math" alttext="R=\left\{r_{1},r_{2},...,r_{\left|R\right|}\right\}" display="inline"><semantics id="S3.SS2.p1.2.m2.5a"><mrow id="S3.SS2.p1.2.m2.5.5" xref="S3.SS2.p1.2.m2.5.5.cmml"><mi id="S3.SS2.p1.2.m2.5.5.5" xref="S3.SS2.p1.2.m2.5.5.5.cmml">R</mi><mo id="S3.SS2.p1.2.m2.5.5.4" xref="S3.SS2.p1.2.m2.5.5.4.cmml">=</mo><mrow id="S3.SS2.p1.2.m2.5.5.3.3" xref="S3.SS2.p1.2.m2.5.5.3.4.cmml"><mo id="S3.SS2.p1.2.m2.5.5.3.3.4" xref="S3.SS2.p1.2.m2.5.5.3.4.cmml">{</mo><msub id="S3.SS2.p1.2.m2.3.3.1.1.1" xref="S3.SS2.p1.2.m2.3.3.1.1.1.cmml"><mi id="S3.SS2.p1.2.m2.3.3.1.1.1.2" xref="S3.SS2.p1.2.m2.3.3.1.1.1.2.cmml">r</mi><mn id="S3.SS2.p1.2.m2.3.3.1.1.1.3" xref="S3.SS2.p1.2.m2.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p1.2.m2.5.5.3.3.5" xref="S3.SS2.p1.2.m2.5.5.3.4.cmml">,</mo><msub id="S3.SS2.p1.2.m2.4.4.2.2.2" xref="S3.SS2.p1.2.m2.4.4.2.2.2.cmml"><mi id="S3.SS2.p1.2.m2.4.4.2.2.2.2" xref="S3.SS2.p1.2.m2.4.4.2.2.2.2.cmml">r</mi><mn id="S3.SS2.p1.2.m2.4.4.2.2.2.3" xref="S3.SS2.p1.2.m2.4.4.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.p1.2.m2.5.5.3.3.6" xref="S3.SS2.p1.2.m2.5.5.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p1.2.m2.2.2" xref="S3.SS2.p1.2.m2.2.2.cmml">…</mi><mo id="S3.SS2.p1.2.m2.5.5.3.3.7" xref="S3.SS2.p1.2.m2.5.5.3.4.cmml">,</mo><msub id="S3.SS2.p1.2.m2.5.5.3.3.3" xref="S3.SS2.p1.2.m2.5.5.3.3.3.cmml"><mi id="S3.SS2.p1.2.m2.5.5.3.3.3.2" xref="S3.SS2.p1.2.m2.5.5.3.3.3.2.cmml">r</mi><mrow id="S3.SS2.p1.2.m2.1.1.1.3" xref="S3.SS2.p1.2.m2.1.1.1.2.cmml"><mo id="S3.SS2.p1.2.m2.1.1.1.3.1" xref="S3.SS2.p1.2.m2.1.1.1.2.1.cmml">|</mo><mi id="S3.SS2.p1.2.m2.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.1.cmml">R</mi><mo id="S3.SS2.p1.2.m2.1.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.1.2.1.cmml">|</mo></mrow></msub><mo id="S3.SS2.p1.2.m2.5.5.3.3.8" xref="S3.SS2.p1.2.m2.5.5.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.5b"><apply id="S3.SS2.p1.2.m2.5.5.cmml" xref="S3.SS2.p1.2.m2.5.5"><eq id="S3.SS2.p1.2.m2.5.5.4.cmml" xref="S3.SS2.p1.2.m2.5.5.4"></eq><ci id="S3.SS2.p1.2.m2.5.5.5.cmml" xref="S3.SS2.p1.2.m2.5.5.5">𝑅</ci><set id="S3.SS2.p1.2.m2.5.5.3.4.cmml" xref="S3.SS2.p1.2.m2.5.5.3.3"><apply id="S3.SS2.p1.2.m2.3.3.1.1.1.cmml" xref="S3.SS2.p1.2.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.3.3.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.3.3.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.3.3.1.1.1.2.cmml" xref="S3.SS2.p1.2.m2.3.3.1.1.1.2">𝑟</ci><cn type="integer" id="S3.SS2.p1.2.m2.3.3.1.1.1.3.cmml" xref="S3.SS2.p1.2.m2.3.3.1.1.1.3">1</cn></apply><apply id="S3.SS2.p1.2.m2.4.4.2.2.2.cmml" xref="S3.SS2.p1.2.m2.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.4.4.2.2.2.1.cmml" xref="S3.SS2.p1.2.m2.4.4.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.4.4.2.2.2.2.cmml" xref="S3.SS2.p1.2.m2.4.4.2.2.2.2">𝑟</ci><cn type="integer" id="S3.SS2.p1.2.m2.4.4.2.2.2.3.cmml" xref="S3.SS2.p1.2.m2.4.4.2.2.2.3">2</cn></apply><ci id="S3.SS2.p1.2.m2.2.2.cmml" xref="S3.SS2.p1.2.m2.2.2">…</ci><apply id="S3.SS2.p1.2.m2.5.5.3.3.3.cmml" xref="S3.SS2.p1.2.m2.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.5.5.3.3.3.1.cmml" xref="S3.SS2.p1.2.m2.5.5.3.3.3">subscript</csymbol><ci id="S3.SS2.p1.2.m2.5.5.3.3.3.2.cmml" xref="S3.SS2.p1.2.m2.5.5.3.3.3.2">𝑟</ci><apply id="S3.SS2.p1.2.m2.1.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.3"><abs id="S3.SS2.p1.2.m2.1.1.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.3.1"></abs><ci id="S3.SS2.p1.2.m2.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1">𝑅</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.5c">R=\left\{r_{1},r_{2},...,r_{\left|R\right|}\right\}</annotation></semantics></math> denotes the set of regions proposals extracted from image <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">I</annotation></semantics></math>, and <math id="S3.SS2.p1.4.m4.5" class="ltx_Math" alttext="C=\left\{c_{1},c_{2},...,c_{\left|C\right|}\right\}" display="inline"><semantics id="S3.SS2.p1.4.m4.5a"><mrow id="S3.SS2.p1.4.m4.5.5" xref="S3.SS2.p1.4.m4.5.5.cmml"><mi id="S3.SS2.p1.4.m4.5.5.5" xref="S3.SS2.p1.4.m4.5.5.5.cmml">C</mi><mo id="S3.SS2.p1.4.m4.5.5.4" xref="S3.SS2.p1.4.m4.5.5.4.cmml">=</mo><mrow id="S3.SS2.p1.4.m4.5.5.3.3" xref="S3.SS2.p1.4.m4.5.5.3.4.cmml"><mo id="S3.SS2.p1.4.m4.5.5.3.3.4" xref="S3.SS2.p1.4.m4.5.5.3.4.cmml">{</mo><msub id="S3.SS2.p1.4.m4.3.3.1.1.1" xref="S3.SS2.p1.4.m4.3.3.1.1.1.cmml"><mi id="S3.SS2.p1.4.m4.3.3.1.1.1.2" xref="S3.SS2.p1.4.m4.3.3.1.1.1.2.cmml">c</mi><mn id="S3.SS2.p1.4.m4.3.3.1.1.1.3" xref="S3.SS2.p1.4.m4.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p1.4.m4.5.5.3.3.5" xref="S3.SS2.p1.4.m4.5.5.3.4.cmml">,</mo><msub id="S3.SS2.p1.4.m4.4.4.2.2.2" xref="S3.SS2.p1.4.m4.4.4.2.2.2.cmml"><mi id="S3.SS2.p1.4.m4.4.4.2.2.2.2" xref="S3.SS2.p1.4.m4.4.4.2.2.2.2.cmml">c</mi><mn id="S3.SS2.p1.4.m4.4.4.2.2.2.3" xref="S3.SS2.p1.4.m4.4.4.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.p1.4.m4.5.5.3.3.6" xref="S3.SS2.p1.4.m4.5.5.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p1.4.m4.2.2" xref="S3.SS2.p1.4.m4.2.2.cmml">…</mi><mo id="S3.SS2.p1.4.m4.5.5.3.3.7" xref="S3.SS2.p1.4.m4.5.5.3.4.cmml">,</mo><msub id="S3.SS2.p1.4.m4.5.5.3.3.3" xref="S3.SS2.p1.4.m4.5.5.3.3.3.cmml"><mi id="S3.SS2.p1.4.m4.5.5.3.3.3.2" xref="S3.SS2.p1.4.m4.5.5.3.3.3.2.cmml">c</mi><mrow id="S3.SS2.p1.4.m4.1.1.1.3" xref="S3.SS2.p1.4.m4.1.1.1.2.cmml"><mo id="S3.SS2.p1.4.m4.1.1.1.3.1" xref="S3.SS2.p1.4.m4.1.1.1.2.1.cmml">|</mo><mi id="S3.SS2.p1.4.m4.1.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.1.cmml">C</mi><mo id="S3.SS2.p1.4.m4.1.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.1.2.1.cmml">|</mo></mrow></msub><mo id="S3.SS2.p1.4.m4.5.5.3.3.8" xref="S3.SS2.p1.4.m4.5.5.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.5b"><apply id="S3.SS2.p1.4.m4.5.5.cmml" xref="S3.SS2.p1.4.m4.5.5"><eq id="S3.SS2.p1.4.m4.5.5.4.cmml" xref="S3.SS2.p1.4.m4.5.5.4"></eq><ci id="S3.SS2.p1.4.m4.5.5.5.cmml" xref="S3.SS2.p1.4.m4.5.5.5">𝐶</ci><set id="S3.SS2.p1.4.m4.5.5.3.4.cmml" xref="S3.SS2.p1.4.m4.5.5.3.3"><apply id="S3.SS2.p1.4.m4.3.3.1.1.1.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.3.3.1.1.1.1.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.3.3.1.1.1.2.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.2">𝑐</ci><cn type="integer" id="S3.SS2.p1.4.m4.3.3.1.1.1.3.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.3">1</cn></apply><apply id="S3.SS2.p1.4.m4.4.4.2.2.2.cmml" xref="S3.SS2.p1.4.m4.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.4.4.2.2.2.1.cmml" xref="S3.SS2.p1.4.m4.4.4.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.4.m4.4.4.2.2.2.2.cmml" xref="S3.SS2.p1.4.m4.4.4.2.2.2.2">𝑐</ci><cn type="integer" id="S3.SS2.p1.4.m4.4.4.2.2.2.3.cmml" xref="S3.SS2.p1.4.m4.4.4.2.2.2.3">2</cn></apply><ci id="S3.SS2.p1.4.m4.2.2.cmml" xref="S3.SS2.p1.4.m4.2.2">…</ci><apply id="S3.SS2.p1.4.m4.5.5.3.3.3.cmml" xref="S3.SS2.p1.4.m4.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.5.5.3.3.3.1.cmml" xref="S3.SS2.p1.4.m4.5.5.3.3.3">subscript</csymbol><ci id="S3.SS2.p1.4.m4.5.5.3.3.3.2.cmml" xref="S3.SS2.p1.4.m4.5.5.3.3.3.2">𝑐</ci><apply id="S3.SS2.p1.4.m4.1.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.1.3"><abs id="S3.SS2.p1.4.m4.1.1.1.2.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1.3.1"></abs><ci id="S3.SS2.p1.4.m4.1.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1">𝐶</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.5c">C=\left\{c_{1},c_{2},...,c_{\left|C\right|}\right\}</annotation></semantics></math> denotes the set of concept words extracted from caption <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">T</annotation></semantics></math>. Under weak caption supervision, we can assume the presence of a concept <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">c</annotation></semantics></math> in <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">T</annotation></semantics></math> indicates the existence of at least one region <math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><mi id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><ci id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">r</annotation></semantics></math> in <math id="S3.SS2.p1.9.m9.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS2.p1.9.m9.1a"><mi id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><ci id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">I</annotation></semantics></math> containing the corresponding object. However, the concrete correspondence between <math id="S3.SS2.p1.10.m10.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p1.10.m10.1a"><mi id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><ci id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">c</annotation></semantics></math> and <math id="S3.SS2.p1.11.m11.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p1.11.m11.1a"><mi id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b"><ci id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">r</annotation></semantics></math> remains unknown.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.9" class="ltx_p">To solve the problem, we propose to explore the global context of caption data and align regions and words via co-occurrence. Specifically, for a given concept <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">c</annotation></semantics></math>, we construct a concept group <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">G</annotation></semantics></math> comprising all image-text pairs that mention <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">c</annotation></semantics></math> in their captions. This grouping effectively clusters images that contain objects corresponding to concept <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">c</annotation></semantics></math>. Consequently, if <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">G</annotation></semantics></math> is large enough, we can induce that the objects corresponding to concept <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">c</annotation></semantics></math> will be the most common objects among images in <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mi id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><ci id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">G</annotation></semantics></math>. This natural correlation automatically aligns the co-occurring objects in <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mi id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><ci id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">G</annotation></semantics></math> to concept <math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><mi id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><ci id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">c</annotation></semantics></math>. We thus reduce the problem of modeling cross-modality correspondence (region-word) to in-modality correspondence (region-region), which we address in the next section.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Discovering Co-occurring Objects across Images</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Intuitively, candidate region proposals containing the co-occurring object should exhibit consistent and similar visual patterns across the images. We thus take a similarity-driven approach to discover these proposals. As illustrated in Figure <a href="#S3.F2" title="Figure 2 ‣ Task formulation. ‣ 3.1 Preliminaries ‣ 3 Method ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, during training, we only sample a mini-group of images from the concept group as inputs in consideration of efficiency. In the mini-group, we <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">iteratively</em> choose one image as query image, and leave the rest as support images. Note that the regional proposals for each image are cached to avoid re-computation when swapping query and support images. The basic idea is to discover co-occurring objects in the query image from region proposals that have close neighbors across the support images. To fulfill this purpose, we introduce text-guided similarity estimation and similarity-based prototype discovery in the following paragraphs.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Similarity-based prototype discovery.</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.10" class="ltx_p">Since modern visual backbones provide consistent feature correspondences for visually similar regions across images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, a straightforward way to identify co-occurring objects is to measure the cosine similarity between features of region proposals. Concretely, we calculate the pairwise similarity of region proposals between the query image and the support images. This yields a similarity matrix <math id="S3.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{S}\in\mathbb{R}^{n\times mn}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml">𝐒</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.cmml"><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.1.cmml">×</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.3.cmml">m</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1"><in id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1"></in><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2">𝐒</ci><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3"><times id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.1"></times><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2"><times id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.1"></times><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.2">𝑛</ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.2.3">𝑚</ci></apply><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">\mathbf{S}\in\mathbb{R}^{n\times mn}</annotation></semantics></math>, where <math id="S3.SS3.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.1c">n</annotation></semantics></math> stands for the number of proposals per image, and <math id="S3.SS3.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.3.m3.1c">m</annotation></semantics></math> stands for the number of support images. Intuitively, co-occurring regions should exhibit high responses (similarities) in the last dimension of <math id="S3.SS3.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{S}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.4.m4.1a"><mi id="S3.SS3.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px1.p1.4.m4.1.1.cmml">𝐒</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.4.m4.1b"><ci id="S3.SS3.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.1.1">𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.4.m4.1c">\mathbf{S}</annotation></semantics></math>. But instead of using hand-crafted rules as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, we employ a two-layer MLP, denoted as <math id="S3.SS3.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="\Phi" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.5.m5.1a"><mi mathvariant="normal" id="S3.SS3.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px1.p1.5.m5.1.1.cmml">Φ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.5.m5.1b"><ci id="S3.SS3.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.1.1">Φ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.5.m5.1c">\Phi</annotation></semantics></math>, to derive co-occurrence from <math id="S3.SS3.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{S}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.6.m6.1a"><mi id="S3.SS3.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.cmml">𝐒</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.6.m6.1b"><ci id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1">𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.6.m6.1c">\mathbf{S}</annotation></semantics></math>. <math id="S3.SS3.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="\Phi" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.7.m7.1a"><mi mathvariant="normal" id="S3.SS3.SSS0.Px1.p1.7.m7.1.1" xref="S3.SS3.SSS0.Px1.p1.7.m7.1.1.cmml">Φ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.7.m7.1b"><ci id="S3.SS3.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.7.m7.1.1">Φ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.7.m7.1c">\Phi</annotation></semantics></math> is trained to estimate the probability of each region proposal in the query images as a co-occurring region, solely conditioned on <math id="S3.SS3.SSS0.Px1.p1.8.m8.1" class="ltx_Math" alttext="\mathbf{S}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.8.m8.1a"><mi id="S3.SS3.SSS0.Px1.p1.8.m8.1.1" xref="S3.SS3.SSS0.Px1.p1.8.m8.1.1.cmml">𝐒</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.8.m8.1b"><ci id="S3.SS3.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.8.m8.1.1">𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.8.m8.1c">\mathbf{S}</annotation></semantics></math>. Here, we do not explicitly supervise the output probability since there is no ground-truth annotation, but <math id="S3.SS3.SSS0.Px1.p1.9.m9.1" class="ltx_Math" alttext="\Phi" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.9.m9.1a"><mi mathvariant="normal" id="S3.SS3.SSS0.Px1.p1.9.m9.1.1" xref="S3.SS3.SSS0.Px1.p1.9.m9.1.1.cmml">Φ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.9.m9.1b"><ci id="S3.SS3.SSS0.Px1.p1.9.m9.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.9.m9.1.1">Φ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.9.m9.1c">\Phi</annotation></semantics></math> is encouraged to assign high probabilities for co-occurring regions to minimize the overall region-word alignment loss. Based on the estimated probability vector <math id="S3.SS3.SSS0.Px1.p1.10.m10.1" class="ltx_Math" alttext="\mathbf{p}\in\mathbb{R}^{n}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.10.m10.1a"><mrow id="S3.SS3.SSS0.Px1.p1.10.m10.1.1" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.2" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.2.cmml">𝐩</mi><mo id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.1" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.1.cmml">∈</mo><msup id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3.2" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3.3" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.10.m10.1b"><apply id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1"><in id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.1"></in><ci id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.2">𝐩</ci><apply id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3">superscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3.2">ℝ</ci><ci id="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px1.p1.10.m10.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.10.m10.1c">\mathbf{p}\in\mathbb{R}^{n}</annotation></semantics></math>, we obtain the prototypical region features for the co-occurring object via simple weighted sum:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\mathbf{f}_{p}=\sum_{i=1}^{N}\mathbf{p}_{i}\cdot\mathbf{f}_{i},\quad\mathrm{where}\;\mathbf{p}=\mathop{\operatorname{softmax}}_{N}\left(\operatorname{\Phi}\left(\mathbf{S}\right)\right)\,." display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1"><mrow id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.3.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2.2" xref="S3.E1.m1.3.3.1.1.1.1.2.2.cmml">𝐟</mi><mi id="S3.E1.m1.3.3.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.1.1.2.3.cmml">p</mi></msub><mo rspace="0.111em" id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml"><munderover id="S3.E1.m1.3.3.1.1.1.1.3.1" xref="S3.E1.m1.3.3.1.1.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E1.m1.3.3.1.1.1.1.3.1.2.2" xref="S3.E1.m1.3.3.1.1.1.1.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.3.1.2.3" xref="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.2" xref="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.1" xref="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.3" xref="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.3.3.1.1.1.1.3.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.1.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.3.3.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.3.2.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.3.2.2" xref="S3.E1.m1.3.3.1.1.1.1.3.2.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.3.2.2.2" xref="S3.E1.m1.3.3.1.1.1.1.3.2.2.2.cmml">𝐩</mi><mi id="S3.E1.m1.3.3.1.1.1.1.3.2.2.3" xref="S3.E1.m1.3.3.1.1.1.1.3.2.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.3.3.1.1.1.1.3.2.1" xref="S3.E1.m1.3.3.1.1.1.1.3.2.1.cmml">⋅</mo><msub id="S3.E1.m1.3.3.1.1.1.1.3.2.3" xref="S3.E1.m1.3.3.1.1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.3.2.3.2" xref="S3.E1.m1.3.3.1.1.1.1.3.2.3.2.cmml">𝐟</mi><mi id="S3.E1.m1.3.3.1.1.1.1.3.2.3.3" xref="S3.E1.m1.3.3.1.1.1.1.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><mo rspace="1.167em" id="S3.E1.m1.3.3.1.1.2.3" xref="S3.E1.m1.3.3.1.1.3a.cmml">,</mo><mrow id="S3.E1.m1.3.3.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.2.cmml"><mrow id="S3.E1.m1.3.3.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.3.2" xref="S3.E1.m1.3.3.1.1.2.2.3.2.cmml">where</mi><mo lspace="0.280em" rspace="0em" id="S3.E1.m1.3.3.1.1.2.2.3.1" xref="S3.E1.m1.3.3.1.1.2.2.3.1.cmml">​</mo><mi id="S3.E1.m1.3.3.1.1.2.2.3.3" xref="S3.E1.m1.3.3.1.1.2.2.3.3.cmml">𝐩</mi></mrow><mo rspace="0.1389em" id="S3.E1.m1.3.3.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1" xref="S3.E1.m1.3.3.1.1.2.2.1.cmml"><munder id="S3.E1.m1.3.3.1.1.2.2.1.2" xref="S3.E1.m1.3.3.1.1.2.2.1.2.cmml"><mo lspace="0.1389em" movablelimits="false" rspace="0em" id="S3.E1.m1.3.3.1.1.2.2.1.2.2" xref="S3.E1.m1.3.3.1.1.2.2.1.2.2.cmml">softmax</mo><mi id="S3.E1.m1.3.3.1.1.2.2.1.2.3" xref="S3.E1.m1.3.3.1.1.2.2.1.2.3.cmml">N</mi></munder><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.cmml"><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.2.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">Φ</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2a" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.cmml">⁡</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">𝐒</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.1.2" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.2.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0.170em" id="S3.E1.m1.3.3.1.2">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3a.cmml" xref="S3.E1.m1.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><eq id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"></eq><apply id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2">𝐟</ci><ci id="S3.E1.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.3">𝑝</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3"><apply id="S3.E1.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1">superscript</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.3.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.3.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1">subscript</csymbol><sum id="S3.E1.m1.3.3.1.1.1.1.3.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1.2.2"></sum><apply id="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1.2.3"><eq id="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.1"></eq><ci id="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.3.3.1.1.1.1.3.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1.3">𝑁</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2"><ci id="S3.E1.m1.3.3.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.1">⋅</ci><apply id="S3.E1.m1.3.3.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.2.2">𝐩</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.2.3">𝑖</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.3.2">𝐟</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2"><eq id="S3.E1.m1.3.3.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2"></eq><apply id="S3.E1.m1.3.3.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3"><times id="S3.E1.m1.3.3.1.1.2.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.1"></times><ci id="S3.E1.m1.3.3.1.1.2.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2">where</ci><ci id="S3.E1.m1.3.3.1.1.2.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.3">𝐩</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1"><apply id="S3.E1.m1.3.3.1.1.2.2.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.2.2">softmax</ci><ci id="S3.E1.m1.3.3.1.1.2.2.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.2.3">𝑁</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">Φ</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝐒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\mathbf{f}_{p}=\sum_{i=1}^{N}\mathbf{p}_{i}\cdot\mathbf{f}_{i},\quad\mathrm{where}\;\mathbf{p}=\mathop{\operatorname{softmax}}_{N}\left(\operatorname{\Phi}\left(\mathbf{S}\right)\right)\,.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.SSS0.Px1.p1.11" class="ltx_p">As the text label for this prototype naturally corresponds to the shared concept <math id="S3.SS3.SSS0.Px1.p1.11.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.11.m1.1a"><mi id="S3.SS3.SSS0.Px1.p1.11.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.11.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.11.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.11.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.11.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.11.m1.1c">c</annotation></semantics></math> in the mini-group.
We can thus learn region-word alignment with a binary cross-entropy (BCE) classification loss:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.7" class="ltx_Math" alttext="\mathcal{L}_{\text{region-word}}=\mathcal{L}_{\text{BCE}}(\mathbf{W}\mathbf{f}_{p},c),\quad\mathrm{where}\;\mathcal{L}_{\text{BCE}}(\mathbf{s},c)=-\operatorname{log}\operatorname{\sigma}\left(\mathbf{s}_{c}\right)-\sum_{k\neq c}\operatorname{log}\left(1-\operatorname{\sigma}\left(\mathbf{s}_{k}\right)\right)\,," display="block"><semantics id="S3.E2.m1.7a"><mrow id="S3.E2.m1.7.7.1"><mrow id="S3.E2.m1.7.7.1.1.2" xref="S3.E2.m1.7.7.1.1.3.cmml"><mrow id="S3.E2.m1.7.7.1.1.1.1" xref="S3.E2.m1.7.7.1.1.1.1.cmml"><msub id="S3.E2.m1.7.7.1.1.1.1.3" xref="S3.E2.m1.7.7.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.7.7.1.1.1.1.3.2" xref="S3.E2.m1.7.7.1.1.1.1.3.2.cmml">ℒ</mi><mtext id="S3.E2.m1.7.7.1.1.1.1.3.3" xref="S3.E2.m1.7.7.1.1.1.1.3.3a.cmml">region-word</mtext></msub><mo id="S3.E2.m1.7.7.1.1.1.1.2" xref="S3.E2.m1.7.7.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.7.7.1.1.1.1.1" xref="S3.E2.m1.7.7.1.1.1.1.1.cmml"><msub id="S3.E2.m1.7.7.1.1.1.1.1.3" xref="S3.E2.m1.7.7.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.7.7.1.1.1.1.1.3.2" xref="S3.E2.m1.7.7.1.1.1.1.1.3.2.cmml">ℒ</mi><mtext id="S3.E2.m1.7.7.1.1.1.1.1.3.3" xref="S3.E2.m1.7.7.1.1.1.1.1.3.3a.cmml">BCE</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.1.1.1.1.1.2" xref="S3.E2.m1.7.7.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.7.7.1.1.1.1.1.1.1" xref="S3.E2.m1.7.7.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.7.7.1.1.1.1.1.1.1.2" xref="S3.E2.m1.7.7.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E2.m1.7.7.1.1.1.1.1.1.1.1" xref="S3.E2.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.7.7.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.7.7.1.1.1.1.1.1.1.1.2.cmml">𝐖𝐟</mi><mi id="S3.E2.m1.7.7.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.7.7.1.1.1.1.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E2.m1.7.7.1.1.1.1.1.1.1.3" xref="S3.E2.m1.7.7.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">c</mi><mo stretchy="false" id="S3.E2.m1.7.7.1.1.1.1.1.1.1.4" xref="S3.E2.m1.7.7.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo rspace="1.167em" id="S3.E2.m1.7.7.1.1.2.3" xref="S3.E2.m1.7.7.1.1.3a.cmml">,</mo><mrow id="S3.E2.m1.7.7.1.1.2.2" xref="S3.E2.m1.7.7.1.1.2.2.cmml"><mrow id="S3.E2.m1.7.7.1.1.2.2.4" xref="S3.E2.m1.7.7.1.1.2.2.4.cmml"><mi id="S3.E2.m1.7.7.1.1.2.2.4.2" xref="S3.E2.m1.7.7.1.1.2.2.4.2.cmml">where</mi><mo lspace="0.280em" rspace="0em" id="S3.E2.m1.7.7.1.1.2.2.4.1" xref="S3.E2.m1.7.7.1.1.2.2.4.1.cmml">​</mo><msub id="S3.E2.m1.7.7.1.1.2.2.4.3" xref="S3.E2.m1.7.7.1.1.2.2.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.7.7.1.1.2.2.4.3.2" xref="S3.E2.m1.7.7.1.1.2.2.4.3.2.cmml">ℒ</mi><mtext id="S3.E2.m1.7.7.1.1.2.2.4.3.3" xref="S3.E2.m1.7.7.1.1.2.2.4.3.3a.cmml">BCE</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.1.1.2.2.4.1a" xref="S3.E2.m1.7.7.1.1.2.2.4.1.cmml">​</mo><mrow id="S3.E2.m1.7.7.1.1.2.2.4.4.2" xref="S3.E2.m1.7.7.1.1.2.2.4.4.1.cmml"><mo stretchy="false" id="S3.E2.m1.7.7.1.1.2.2.4.4.2.1" xref="S3.E2.m1.7.7.1.1.2.2.4.4.1.cmml">(</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">𝐬</mi><mo id="S3.E2.m1.7.7.1.1.2.2.4.4.2.2" xref="S3.E2.m1.7.7.1.1.2.2.4.4.1.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">c</mi><mo stretchy="false" id="S3.E2.m1.7.7.1.1.2.2.4.4.2.3" xref="S3.E2.m1.7.7.1.1.2.2.4.4.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.7.7.1.1.2.2.3" xref="S3.E2.m1.7.7.1.1.2.2.3.cmml">=</mo><mrow id="S3.E2.m1.7.7.1.1.2.2.2" xref="S3.E2.m1.7.7.1.1.2.2.2.cmml"><mrow id="S3.E2.m1.7.7.1.1.2.2.1.1" xref="S3.E2.m1.7.7.1.1.2.2.1.1.cmml"><mo rspace="0.167em" id="S3.E2.m1.7.7.1.1.2.2.1.1a" xref="S3.E2.m1.7.7.1.1.2.2.1.1.cmml">−</mo><mrow id="S3.E2.m1.7.7.1.1.2.2.1.1.1" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.cmml"><mi id="S3.E2.m1.7.7.1.1.2.2.1.1.1.2" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.2.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.7.7.1.1.2.2.1.1.1a" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.cmml">⁡</mo><mrow id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.2.cmml"><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">σ</mi><mo id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1a" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.2.cmml"><mo id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.2.cmml">(</mo><msub id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.2.cmml">𝐬</mi><mi id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.3.cmml">c</mi></msub><mo id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo rspace="0.055em" id="S3.E2.m1.7.7.1.1.2.2.2.3" xref="S3.E2.m1.7.7.1.1.2.2.2.3.cmml">−</mo><mrow id="S3.E2.m1.7.7.1.1.2.2.2.2" xref="S3.E2.m1.7.7.1.1.2.2.2.2.cmml"><munder id="S3.E2.m1.7.7.1.1.2.2.2.2.2" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2.cmml"><mo movablelimits="false" id="S3.E2.m1.7.7.1.1.2.2.2.2.2.2" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.7.7.1.1.2.2.2.2.2.3" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.cmml"><mi id="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.2" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.2.cmml">k</mi><mo id="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.1" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.1.cmml">≠</mo><mi id="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.3" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.3.cmml">c</mi></mrow></munder><mrow id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.2.cmml"><mi id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml">log</mi><mo id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1a" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.2.cmml"><mo id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.2" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.2.cmml">(</mo><mrow id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.cmml"><mn id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.3" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.3.cmml">1</mn><mo id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.2" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.2.cmml">−</mo><mrow id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">σ</mi><mo id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1a" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1.2.cmml">𝐬</mi><mi id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.170em" id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.3" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.7.7.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.7b"><apply id="S3.E2.m1.7.7.1.1.3.cmml" xref="S3.E2.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.3a.cmml" xref="S3.E2.m1.7.7.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E2.m1.7.7.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.1.1"><eq id="S3.E2.m1.7.7.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.1.1.2"></eq><apply id="S3.E2.m1.7.7.1.1.1.1.3.cmml" xref="S3.E2.m1.7.7.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.1.1.3.1.cmml" xref="S3.E2.m1.7.7.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.7.7.1.1.1.1.3.2.cmml" xref="S3.E2.m1.7.7.1.1.1.1.3.2">ℒ</ci><ci id="S3.E2.m1.7.7.1.1.1.1.3.3a.cmml" xref="S3.E2.m1.7.7.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E2.m1.7.7.1.1.1.1.3.3.cmml" xref="S3.E2.m1.7.7.1.1.1.1.3.3">region-word</mtext></ci></apply><apply id="S3.E2.m1.7.7.1.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1"><times id="S3.E2.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.2"></times><apply id="S3.E2.m1.7.7.1.1.1.1.1.3.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.7.7.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.3.2">ℒ</ci><ci id="S3.E2.m1.7.7.1.1.1.1.1.3.3a.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E2.m1.7.7.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.3.3">BCE</mtext></ci></apply><interval closure="open" id="S3.E2.m1.7.7.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.1.1"><apply id="S3.E2.m1.7.7.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.7.7.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.1.1.1.2">𝐖𝐟</ci><ci id="S3.E2.m1.7.7.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.1.1.1.3">𝑝</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑐</ci></interval></apply></apply><apply id="S3.E2.m1.7.7.1.1.2.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2"><eq id="S3.E2.m1.7.7.1.1.2.2.3.cmml" xref="S3.E2.m1.7.7.1.1.2.2.3"></eq><apply id="S3.E2.m1.7.7.1.1.2.2.4.cmml" xref="S3.E2.m1.7.7.1.1.2.2.4"><times id="S3.E2.m1.7.7.1.1.2.2.4.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.4.1"></times><ci id="S3.E2.m1.7.7.1.1.2.2.4.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.4.2">where</ci><apply id="S3.E2.m1.7.7.1.1.2.2.4.3.cmml" xref="S3.E2.m1.7.7.1.1.2.2.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.2.2.4.3.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.4.3">subscript</csymbol><ci id="S3.E2.m1.7.7.1.1.2.2.4.3.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.4.3.2">ℒ</ci><ci id="S3.E2.m1.7.7.1.1.2.2.4.3.3a.cmml" xref="S3.E2.m1.7.7.1.1.2.2.4.3.3"><mtext mathsize="70%" id="S3.E2.m1.7.7.1.1.2.2.4.3.3.cmml" xref="S3.E2.m1.7.7.1.1.2.2.4.3.3">BCE</mtext></ci></apply><interval closure="open" id="S3.E2.m1.7.7.1.1.2.2.4.4.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.4.4.2"><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝐬</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑐</ci></interval></apply><apply id="S3.E2.m1.7.7.1.1.2.2.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2"><minus id="S3.E2.m1.7.7.1.1.2.2.2.3.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.3"></minus><apply id="S3.E2.m1.7.7.1.1.2.2.1.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.1.1"><minus id="S3.E2.m1.7.7.1.1.2.2.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.1.1"></minus><apply id="S3.E2.m1.7.7.1.1.2.2.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1"><ci id="S3.E2.m1.7.7.1.1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.2">log</ci><apply id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1"><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝜎</ci><apply id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.2">𝐬</ci><ci id="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.3">𝑐</ci></apply></apply></apply></apply><apply id="S3.E2.m1.7.7.1.1.2.2.2.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2"><apply id="S3.E2.m1.7.7.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2">subscript</csymbol><sum id="S3.E2.m1.7.7.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2.2"></sum><apply id="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2.3"><neq id="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.1"></neq><ci id="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.2">𝑘</ci><ci id="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.3.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.2.3.3">𝑐</ci></apply></apply><apply id="S3.E2.m1.7.7.1.1.2.2.2.2.1.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1"><ci id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6">log</ci><apply id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1"><minus id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.2"></minus><cn type="integer" id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.3">1</cn><apply id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1"><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">𝜎</ci><apply id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1.2">𝐬</ci><ci id="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.7.7.1.1.2.2.2.2.1.1.1.1.1.1.1.1.3">𝑘</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.7c">\mathcal{L}_{\text{region-word}}=\mathcal{L}_{\text{BCE}}(\mathbf{W}\mathbf{f}_{p},c),\quad\mathrm{where}\;\mathcal{L}_{\text{BCE}}(\mathbf{s},c)=-\operatorname{log}\operatorname{\sigma}\left(\mathbf{s}_{c}\right)-\sum_{k\neq c}\operatorname{log}\left(1-\operatorname{\sigma}\left(\mathbf{s}_{k}\right)\right)\,,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.SSS0.Px1.p1.14" class="ltx_p">where <math id="S3.SS3.SSS0.Px1.p1.12.m1.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.12.m1.1a"><mi id="S3.SS3.SSS0.Px1.p1.12.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.12.m1.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.12.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.12.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.12.m1.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.12.m1.1c">\mathbf{W}</annotation></semantics></math> the classifier weight derived from <math id="S3.SS3.SSS0.Px1.p1.13.m2.1" class="ltx_Math" alttext="\mathcal{C}^{\text{open}}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.13.m2.1a"><msup id="S3.SS3.SSS0.Px1.p1.13.m2.1.1" xref="S3.SS3.SSS0.Px1.p1.13.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px1.p1.13.m2.1.1.2" xref="S3.SS3.SSS0.Px1.p1.13.m2.1.1.2.cmml">𝒞</mi><mtext id="S3.SS3.SSS0.Px1.p1.13.m2.1.1.3" xref="S3.SS3.SSS0.Px1.p1.13.m2.1.1.3a.cmml">open</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.13.m2.1b"><apply id="S3.SS3.SSS0.Px1.p1.13.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.13.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.13.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.13.m2.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.13.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.13.m2.1.1.2">𝒞</ci><ci id="S3.SS3.SSS0.Px1.p1.13.m2.1.1.3a.cmml" xref="S3.SS3.SSS0.Px1.p1.13.m2.1.1.3"><mtext mathsize="70%" id="S3.SS3.SSS0.Px1.p1.13.m2.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.13.m2.1.1.3">open</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.13.m2.1c">\mathcal{C}^{\text{open}}</annotation></semantics></math>, and <math id="S3.SS3.SSS0.Px1.p1.14.m3.2" class="ltx_Math" alttext="\operatorname{\sigma}(\cdot)" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.14.m3.2a"><mrow id="S3.SS3.SSS0.Px1.p1.14.m3.2.3.2" xref="S3.SS3.SSS0.Px1.p1.14.m3.2.3.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.14.m3.1.1" xref="S3.SS3.SSS0.Px1.p1.14.m3.1.1.cmml">σ</mi><mo id="S3.SS3.SSS0.Px1.p1.14.m3.2.3.2a" xref="S3.SS3.SSS0.Px1.p1.14.m3.2.3.1.cmml">⁡</mo><mrow id="S3.SS3.SSS0.Px1.p1.14.m3.2.3.2.1" xref="S3.SS3.SSS0.Px1.p1.14.m3.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.SSS0.Px1.p1.14.m3.2.3.2.1.1" xref="S3.SS3.SSS0.Px1.p1.14.m3.2.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p1.14.m3.2.2" xref="S3.SS3.SSS0.Px1.p1.14.m3.2.2.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.SSS0.Px1.p1.14.m3.2.3.2.1.2" xref="S3.SS3.SSS0.Px1.p1.14.m3.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.14.m3.2b"><apply id="S3.SS3.SSS0.Px1.p1.14.m3.2.3.1.cmml" xref="S3.SS3.SSS0.Px1.p1.14.m3.2.3.2"><ci id="S3.SS3.SSS0.Px1.p1.14.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.14.m3.1.1">𝜎</ci><ci id="S3.SS3.SSS0.Px1.p1.14.m3.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.14.m3.2.2">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.14.m3.2c">\operatorname{\sigma}(\cdot)</annotation></semantics></math> stands for the sigmoid function.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Text-guided region-region similarity estimation.</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.9" class="ltx_p">However, general cosine similarity may not always truly reflect closeness of objects in the semantic space, as objects of the same category may exhibit significant variance in appearance. Moreover, there could exist multiple co-occurring concepts among the sampled images, which incurs ambiguity in identifying co-occurrence. To address the problems, we introduce text guidance into similarity estimation to make it concept-aware. Concretely, given features of two region proposals <math id="S3.SS3.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{f}_{i}" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.1.m1.1a"><msub id="S3.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">𝐟</mi><mi id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2">𝐟</ci><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.1.m1.1c">\mathbf{f}_{i}</annotation></semantics></math>, <math id="S3.SS3.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{f}_{j}" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.2.m2.1a"><msub id="S3.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml">𝐟</mi><mi id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.2">𝐟</ci><ci id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.2.m2.1c">\mathbf{f}_{j}</annotation></semantics></math> <math id="S3.SS3.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.3.m3.1a"><mo id="S3.SS3.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.3.m3.1b"><in id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.3.m3.1c">\in</annotation></semantics></math> <math id="S3.SS3.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="\mathbb{R}^{d}" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.4.m4.1a"><msup id="S3.SS3.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.2.cmml">ℝ</mi><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3.cmml">d</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.2">ℝ</ci><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.4.m4.1c">\mathbb{R}^{d}</annotation></semantics></math>,
where <math id="S3.SS3.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.5.m5.1a"><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.5.m5.1b"><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.5.m5.1c">d</annotation></semantics></math> is the dimension of feature vectors, we additionally introduce <math id="S3.SS3.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{w}_{c}" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.6.m6.1a"><msub id="S3.SS3.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2.cmml">𝐰</mi><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2">𝐰</ci><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.6.m6.1c">\mathbf{w}_{c}</annotation></semantics></math> <math id="S3.SS3.SSS0.Px2.p1.7.m7.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.7.m7.1a"><mo id="S3.SS3.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.7.m7.1b"><in id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.7.m7.1c">\in</annotation></semantics></math> <math id="S3.SS3.SSS0.Px2.p1.8.m8.1" class="ltx_Math" alttext="\mathbb{R}^{d}" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.8.m8.1a"><msup id="S3.SS3.SSS0.Px2.p1.8.m8.1.1" xref="S3.SS3.SSS0.Px2.p1.8.m8.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.8.m8.1.1.2" xref="S3.SS3.SSS0.Px2.p1.8.m8.1.1.2.cmml">ℝ</mi><mi id="S3.SS3.SSS0.Px2.p1.8.m8.1.1.3" xref="S3.SS3.SSS0.Px2.p1.8.m8.1.1.3.cmml">d</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.8.m8.1b"><apply id="S3.SS3.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.8.m8.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.8.m8.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.8.m8.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.8.m8.1.1.2">ℝ</ci><ci id="S3.SS3.SSS0.Px2.p1.8.m8.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.8.m8.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.8.m8.1c">\mathbb{R}^{d}</annotation></semantics></math>, the text embedding of concept <math id="S3.SS3.SSS0.Px2.p1.9.m9.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.9.m9.1a"><mi id="S3.SS3.SSS0.Px2.p1.9.m9.1.1" xref="S3.SS3.SSS0.Px2.p1.9.m9.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.9.m9.1b"><ci id="S3.SS3.SSS0.Px2.p1.9.m9.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.9.m9.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.9.m9.1c">c</annotation></semantics></math> (the shared concept in the mini-group), to re-weight similarity calculation:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.5" class="ltx_Math" alttext="s_{ij}=\bar{\mathbf{w}}^{\top}_{c}\cdot\left(\frac{\mathbf{f}_{i}}{\left\|\mathbf{f}_{i}\right\|}\circ\frac{\mathbf{f}_{j}}{\left\|\mathbf{f}_{j}\right\|}\right),\quad\mathrm{where}\;\bar{\mathbf{w}}_{c}=\sqrt{d}\frac{\left|\mathbf{w}_{c}\right|}{\left\|\mathbf{w}_{c}\right\|}\,," display="block"><semantics id="S3.E3.m1.5a"><mrow id="S3.E3.m1.5.5.1"><mrow id="S3.E3.m1.5.5.1.1.2" xref="S3.E3.m1.5.5.1.1.3.cmml"><mrow id="S3.E3.m1.5.5.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.cmml"><msub id="S3.E3.m1.5.5.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.3.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.3.2" xref="S3.E3.m1.5.5.1.1.1.1.3.2.cmml">s</mi><mrow id="S3.E3.m1.5.5.1.1.1.1.3.3" xref="S3.E3.m1.5.5.1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.3.3.2" xref="S3.E3.m1.5.5.1.1.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.1.1.3.3.1" xref="S3.E3.m1.5.5.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E3.m1.5.5.1.1.1.1.3.3.3" xref="S3.E3.m1.5.5.1.1.1.1.3.3.3.cmml">j</mi></mrow></msub><mo id="S3.E3.m1.5.5.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.5.5.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.cmml"><msubsup id="S3.E3.m1.5.5.1.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E3.m1.5.5.1.1.1.1.1.3.2.2" xref="S3.E3.m1.5.5.1.1.1.1.1.3.2.2.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.1.3.2.2.2" xref="S3.E3.m1.5.5.1.1.1.1.1.3.2.2.2.cmml">𝐰</mi><mo id="S3.E3.m1.5.5.1.1.1.1.1.3.2.2.1" xref="S3.E3.m1.5.5.1.1.1.1.1.3.2.2.1.cmml">¯</mo></mover><mi id="S3.E3.m1.5.5.1.1.1.1.1.3.3" xref="S3.E3.m1.5.5.1.1.1.1.1.3.3.cmml">c</mi><mo id="S3.E3.m1.5.5.1.1.1.1.1.3.2.3" xref="S3.E3.m1.5.5.1.1.1.1.1.3.2.3.cmml">⊤</mo></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.5.5.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.1.2.cmml">⋅</mo><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mfrac id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">𝐟</mi><mi id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">i</mi></msub><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.2.cmml"><mo id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.1.cmml">‖</mo><msub id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">𝐟</mi><mi id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.2.1.cmml">‖</mo></mrow></mfrac><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">∘</mo><mfrac id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><msub id="S3.E3.m1.2.2.3" xref="S3.E3.m1.2.2.3.cmml"><mi id="S3.E3.m1.2.2.3.2" xref="S3.E3.m1.2.2.3.2.cmml">𝐟</mi><mi id="S3.E3.m1.2.2.3.3" xref="S3.E3.m1.2.2.3.3.cmml">j</mi></msub><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.2.cmml"><mo id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.2.1.cmml">‖</mo><msub id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.2.cmml">𝐟</mi><mi id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.3.cmml">j</mi></msub><mo id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.2.1.cmml">‖</mo></mrow></mfrac></mrow><mo id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo rspace="1.167em" id="S3.E3.m1.5.5.1.1.2.3" xref="S3.E3.m1.5.5.1.1.3a.cmml">,</mo><mrow id="S3.E3.m1.5.5.1.1.2.2" xref="S3.E3.m1.5.5.1.1.2.2.cmml"><mrow id="S3.E3.m1.5.5.1.1.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.2.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.2.2.cmml">where</mi><mo lspace="0.280em" rspace="0em" id="S3.E3.m1.5.5.1.1.2.2.2.1" xref="S3.E3.m1.5.5.1.1.2.2.2.1.cmml">​</mo><msub id="S3.E3.m1.5.5.1.1.2.2.2.3" xref="S3.E3.m1.5.5.1.1.2.2.2.3.cmml"><mover accent="true" id="S3.E3.m1.5.5.1.1.2.2.2.3.2" xref="S3.E3.m1.5.5.1.1.2.2.2.3.2.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.2.3.2.2" xref="S3.E3.m1.5.5.1.1.2.2.2.3.2.2.cmml">𝐰</mi><mo id="S3.E3.m1.5.5.1.1.2.2.2.3.2.1" xref="S3.E3.m1.5.5.1.1.2.2.2.3.2.1.cmml">¯</mo></mover><mi id="S3.E3.m1.5.5.1.1.2.2.2.3.3" xref="S3.E3.m1.5.5.1.1.2.2.2.3.3.cmml">c</mi></msub></mrow><mo id="S3.E3.m1.5.5.1.1.2.2.1" xref="S3.E3.m1.5.5.1.1.2.2.1.cmml">=</mo><mrow id="S3.E3.m1.5.5.1.1.2.2.3" xref="S3.E3.m1.5.5.1.1.2.2.3.cmml"><msqrt id="S3.E3.m1.5.5.1.1.2.2.3.2" xref="S3.E3.m1.5.5.1.1.2.2.3.2.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.3.2.2" xref="S3.E3.m1.5.5.1.1.2.2.3.2.2.cmml">d</mi></msqrt><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.2.2.3.1" xref="S3.E3.m1.5.5.1.1.2.2.3.1.cmml">​</mo><mfrac id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml"><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.2.cmml"><mo id="S3.E3.m1.3.3.1.1.2" xref="S3.E3.m1.3.3.1.2.1.cmml">|</mo><msub id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.2.cmml">𝐰</mi><mi id="S3.E3.m1.3.3.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.3.cmml">c</mi></msub><mo id="S3.E3.m1.3.3.1.1.3" xref="S3.E3.m1.3.3.1.2.1.cmml">|</mo></mrow><mrow id="S3.E3.m1.4.4.2.1" xref="S3.E3.m1.4.4.2.2.cmml"><mo id="S3.E3.m1.4.4.2.1.2" xref="S3.E3.m1.4.4.2.2.1.cmml">‖</mo><msub id="S3.E3.m1.4.4.2.1.1" xref="S3.E3.m1.4.4.2.1.1.cmml"><mi id="S3.E3.m1.4.4.2.1.1.2" xref="S3.E3.m1.4.4.2.1.1.2.cmml">𝐰</mi><mi id="S3.E3.m1.4.4.2.1.1.3" xref="S3.E3.m1.4.4.2.1.1.3.cmml">c</mi></msub><mo id="S3.E3.m1.4.4.2.1.3" xref="S3.E3.m1.4.4.2.2.1.cmml">‖</mo></mrow></mfrac></mrow></mrow></mrow><mo lspace="0.170em" id="S3.E3.m1.5.5.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.5b"><apply id="S3.E3.m1.5.5.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.3a.cmml" xref="S3.E3.m1.5.5.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E3.m1.5.5.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1"><eq id="S3.E3.m1.5.5.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2"></eq><apply id="S3.E3.m1.5.5.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.5.5.1.1.1.1.3.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.3.2">𝑠</ci><apply id="S3.E3.m1.5.5.1.1.1.1.3.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.3.3"><times id="S3.E3.m1.5.5.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.3.3.1"></times><ci id="S3.E3.m1.5.5.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.3.3.2">𝑖</ci><ci id="S3.E3.m1.5.5.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.3.3.3">𝑗</ci></apply></apply><apply id="S3.E3.m1.5.5.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1"><ci id="S3.E3.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.2">⋅</ci><apply id="S3.E3.m1.5.5.1.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E3.m1.5.5.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E3.m1.5.5.1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3.2.2"><ci id="S3.E3.m1.5.5.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3.2.2.1">¯</ci><ci id="S3.E3.m1.5.5.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3.2.2.2">𝐰</ci></apply><csymbol cd="latexml" id="S3.E3.m1.5.5.1.1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3.2.3">top</csymbol></apply><ci id="S3.E3.m1.5.5.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3.3">𝑐</ci></apply><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1"><compose id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.1"></compose><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><divide id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1"></divide><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝐟</ci><ci id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">𝑖</ci></apply><apply id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">norm</csymbol><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2">𝐟</ci><ci id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><divide id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2"></divide><apply id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.3.1.cmml" xref="S3.E3.m1.2.2.3">subscript</csymbol><ci id="S3.E3.m1.2.2.3.2.cmml" xref="S3.E3.m1.2.2.3.2">𝐟</ci><ci id="S3.E3.m1.2.2.3.3.cmml" xref="S3.E3.m1.2.2.3.3">𝑗</ci></apply><apply id="S3.E3.m1.2.2.1.2.cmml" xref="S3.E3.m1.2.2.1.1"><csymbol cd="latexml" id="S3.E3.m1.2.2.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.2">norm</csymbol><apply id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2">𝐟</ci><ci id="S3.E3.m1.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3">𝑗</ci></apply></apply></apply></apply></apply></apply><apply id="S3.E3.m1.5.5.1.1.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2"><eq id="S3.E3.m1.5.5.1.1.2.2.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1"></eq><apply id="S3.E3.m1.5.5.1.1.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2"><times id="S3.E3.m1.5.5.1.1.2.2.2.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.1"></times><ci id="S3.E3.m1.5.5.1.1.2.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.2">where</ci><apply id="S3.E3.m1.5.5.1.1.2.2.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.2.2.2.3.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.3">subscript</csymbol><apply id="S3.E3.m1.5.5.1.1.2.2.2.3.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.3.2"><ci id="S3.E3.m1.5.5.1.1.2.2.2.3.2.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.3.2.1">¯</ci><ci id="S3.E3.m1.5.5.1.1.2.2.2.3.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.3.2.2">𝐰</ci></apply><ci id="S3.E3.m1.5.5.1.1.2.2.2.3.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.3.3">𝑐</ci></apply></apply><apply id="S3.E3.m1.5.5.1.1.2.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.3"><times id="S3.E3.m1.5.5.1.1.2.2.3.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.3.1"></times><apply id="S3.E3.m1.5.5.1.1.2.2.3.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.3.2"><root id="S3.E3.m1.5.5.1.1.2.2.3.2a.cmml" xref="S3.E3.m1.5.5.1.1.2.2.3.2"></root><ci id="S3.E3.m1.5.5.1.1.2.2.3.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.3.2.2">𝑑</ci></apply><apply id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4"><divide id="S3.E3.m1.4.4.3.cmml" xref="S3.E3.m1.4.4"></divide><apply id="S3.E3.m1.3.3.1.2.cmml" xref="S3.E3.m1.3.3.1.1"><abs id="S3.E3.m1.3.3.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.2"></abs><apply id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2">𝐰</ci><ci id="S3.E3.m1.3.3.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.3">𝑐</ci></apply></apply><apply id="S3.E3.m1.4.4.2.2.cmml" xref="S3.E3.m1.4.4.2.1"><csymbol cd="latexml" id="S3.E3.m1.4.4.2.2.1.cmml" xref="S3.E3.m1.4.4.2.1.2">norm</csymbol><apply id="S3.E3.m1.4.4.2.1.1.cmml" xref="S3.E3.m1.4.4.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.1.1.1.cmml" xref="S3.E3.m1.4.4.2.1.1">subscript</csymbol><ci id="S3.E3.m1.4.4.2.1.1.2.cmml" xref="S3.E3.m1.4.4.2.1.1.2">𝐰</ci><ci id="S3.E3.m1.4.4.2.1.1.3.cmml" xref="S3.E3.m1.4.4.2.1.1.3">𝑐</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.5c">s_{ij}=\bar{\mathbf{w}}^{\top}_{c}\cdot\left(\frac{\mathbf{f}_{i}}{\left\|\mathbf{f}_{i}\right\|}\circ\frac{\mathbf{f}_{j}}{\left\|\mathbf{f}_{j}\right\|}\right),\quad\mathrm{where}\;\bar{\mathbf{w}}_{c}=\sqrt{d}\frac{\left|\mathbf{w}_{c}\right|}{\left\|\mathbf{w}_{c}\right\|}\,,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.SSS0.Px2.p1.14" class="ltx_p">where “<math id="S3.SS3.SSS0.Px2.p1.10.m1.1" class="ltx_Math" alttext="\circ" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.10.m1.1a"><mo id="S3.SS3.SSS0.Px2.p1.10.m1.1.1" xref="S3.SS3.SSS0.Px2.p1.10.m1.1.1.cmml">∘</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.10.m1.1b"><compose id="S3.SS3.SSS0.Px2.p1.10.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.10.m1.1.1"></compose></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.10.m1.1c">\circ</annotation></semantics></math>” denotes Hadamard product, “<math id="S3.SS3.SSS0.Px2.p1.11.m2.1" class="ltx_math_unparsed" alttext="\left|\cdot\right|" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.11.m2.1a"><mrow id="S3.SS3.SSS0.Px2.p1.11.m2.1b"><mo fence="false" id="S3.SS3.SSS0.Px2.p1.11.m2.1.1">|</mo><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px2.p1.11.m2.1.2">⋅</mo><mo fence="false" id="S3.SS3.SSS0.Px2.p1.11.m2.1.3">|</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.11.m2.1c">\left|\cdot\right|</annotation></semantics></math>” denotes absolute value operation, and “<math id="S3.SS3.SSS0.Px2.p1.12.m3.1" class="ltx_math_unparsed" alttext="\left\|\cdot\right\|" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.12.m3.1a"><mrow id="S3.SS3.SSS0.Px2.p1.12.m3.1b"><mo rspace="0em" stretchy="true" id="S3.SS3.SSS0.Px2.p1.12.m3.1.1">∥</mo><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px2.p1.12.m3.1.2">⋅</mo><mo lspace="0em" stretchy="true" id="S3.SS3.SSS0.Px2.p1.12.m3.1.3">∥</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.12.m3.1c">\left\|\cdot\right\|</annotation></semantics></math>” denotes <math id="S3.SS3.SSS0.Px2.p1.13.m4.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.13.m4.1a"><msub id="S3.SS3.SSS0.Px2.p1.13.m4.1.1" xref="S3.SS3.SSS0.Px2.p1.13.m4.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.SSS0.Px2.p1.13.m4.1.1.2" xref="S3.SS3.SSS0.Px2.p1.13.m4.1.1.2.cmml">ℓ</mi><mn id="S3.SS3.SSS0.Px2.p1.13.m4.1.1.3" xref="S3.SS3.SSS0.Px2.p1.13.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.13.m4.1b"><apply id="S3.SS3.SSS0.Px2.p1.13.m4.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.13.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.13.m4.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.13.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.13.m4.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.13.m4.1.1.2">ℓ</ci><cn type="integer" id="S3.SS3.SSS0.Px2.p1.13.m4.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.13.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.13.m4.1c">\ell_{2}</annotation></semantics></math>-normalization, respectively.
Here, the rationale is that the relative magnitude of text features at different dimensions indicates their relative importance in the classification of concept <math id="S3.SS3.SSS0.Px2.p1.14.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.14.m5.1a"><mi id="S3.SS3.SSS0.Px2.p1.14.m5.1.1" xref="S3.SS3.SSS0.Px2.p1.14.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.14.m5.1b"><ci id="S3.SS3.SSS0.Px2.p1.14.m5.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.14.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.14.m5.1c">c</annotation></semantics></math>. By weighting the image feature similarities with the text feature magnitudes, the similarity measurement can put more emphasis on feature dimensions that reflect more of the text features. Therefore, the re-weighted similarity metric provides a more nuanced and tailored measure of the proximity between objects in the context of a particular concept. It is noteworthy that we choose regional features from the output of the penultimate layer of the classification head (the last layer is the text embedding) so that they naturally reside in the shared feature space as text embeddings.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Training and Inference</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, we train the model simultaneously on detection data and image-text pairs to acquire localization capability and knowledge of vision-language alignments. In addition to learning region-level alignments from region-word pairs discovered in Sec. <a href="#S3.SS3" title="3.3 Discovering Co-occurring Objects across Images ‣ 3 Method ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we treat image-text pairs as a generalized form of region-word pairs to learn image-level alignments. Particularly, we use a region proposal covering the entire image to extract image features, and encode the entire caption into language embeddings.
Similar to CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>, we consider each image and its original caption as a positive pair and other captions in the same batch as negative pairs. We then use a BCE loss similar to Eq. (<a href="#S3.E2" title="In Similarity-based prototype discovery. ‣ 3.3 Discovering Co-occurring Objects across Images ‣ 3 Method ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) to calculate the image-text matching loss <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{image-text}}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3a.cmml">image-text</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">ℒ</ci><ci id="S3.SS4.p1.1.m1.1.1.3a.cmml" xref="S3.SS4.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">image-text</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\mathcal{L}_{\text{image-text}}</annotation></semantics></math>. The overall training objective for this framework is:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.6" class="ltx_Math" alttext="\mathcal{L}(I)=\begin{cases}\mathcal{L}_{\mathrm{rpn}}+\mathcal{L}_{\mathrm{reg}}+\mathcal{L}_{\mathrm{cls}},&amp;\text{ if }I\in\mathcal{D}^{\mathrm{det}}\\
\mathcal{L}_{\text{region-word}}+\mathcal{L}_{\text{image-text}},&amp;\text{ if }I\in\mathcal{D}^{\mathrm{cap}}\end{cases}\,," display="block"><semantics id="S3.E4.m1.6a"><mrow id="S3.E4.m1.6.6.1" xref="S3.E4.m1.6.6.1.1.cmml"><mrow id="S3.E4.m1.6.6.1.1" xref="S3.E4.m1.6.6.1.1.cmml"><mrow id="S3.E4.m1.6.6.1.1.2" xref="S3.E4.m1.6.6.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.6.6.1.1.2.2" xref="S3.E4.m1.6.6.1.1.2.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.2.1" xref="S3.E4.m1.6.6.1.1.2.1.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.2.3.2" xref="S3.E4.m1.6.6.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.6.6.1.1.2.3.2.1" xref="S3.E4.m1.6.6.1.1.2.cmml">(</mo><mi id="S3.E4.m1.5.5" xref="S3.E4.m1.5.5.cmml">I</mi><mo stretchy="false" id="S3.E4.m1.6.6.1.1.2.3.2.2" xref="S3.E4.m1.6.6.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.6.6.1.1.1" xref="S3.E4.m1.6.6.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.4.4" xref="S3.E4.m1.6.6.1.1.3.1.cmml"><mo id="S3.E4.m1.4.4.5" xref="S3.E4.m1.6.6.1.1.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E4.m1.4.4.4" xref="S3.E4.m1.6.6.1.1.3.1.cmml"><mtr id="S3.E4.m1.4.4.4a" xref="S3.E4.m1.6.6.1.1.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.4.4.4b" xref="S3.E4.m1.6.6.1.1.3.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.cmml">ℒ</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.3.cmml">rpn</mi></msub><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml">ℒ</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml">reg</mi></msub><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1a" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.1.1.1.1.1.1.4.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.4.2.cmml">ℒ</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.4.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.4.3.cmml">cls</mi></msub></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.4.4.4c" xref="S3.E4.m1.6.6.1.1.3.1.cmml"><mrow id="S3.E4.m1.2.2.2.2.2.1" xref="S3.E4.m1.2.2.2.2.2.1.cmml"><mrow id="S3.E4.m1.2.2.2.2.2.1.2" xref="S3.E4.m1.2.2.2.2.2.1.2.cmml"><mtext id="S3.E4.m1.2.2.2.2.2.1.2.2" xref="S3.E4.m1.2.2.2.2.2.1.2.2a.cmml"> if </mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.2.2.2.1.2.1" xref="S3.E4.m1.2.2.2.2.2.1.2.1.cmml">​</mo><mi id="S3.E4.m1.2.2.2.2.2.1.2.3" xref="S3.E4.m1.2.2.2.2.2.1.2.3.cmml">I</mi></mrow><mo id="S3.E4.m1.2.2.2.2.2.1.1" xref="S3.E4.m1.2.2.2.2.2.1.1.cmml">∈</mo><msup id="S3.E4.m1.2.2.2.2.2.1.3" xref="S3.E4.m1.2.2.2.2.2.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.2.2.2.1.3.2" xref="S3.E4.m1.2.2.2.2.2.1.3.2.cmml">𝒟</mi><mi id="S3.E4.m1.2.2.2.2.2.1.3.3" xref="S3.E4.m1.2.2.2.2.2.1.3.3.cmml">det</mi></msup></mrow></mtd></mtr><mtr id="S3.E4.m1.4.4.4d" xref="S3.E4.m1.6.6.1.1.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.4.4.4e" xref="S3.E4.m1.6.6.1.1.3.1.cmml"><mrow id="S3.E4.m1.3.3.3.3.1.1.1" xref="S3.E4.m1.3.3.3.3.1.1.1.1.cmml"><mrow id="S3.E4.m1.3.3.3.3.1.1.1.1" xref="S3.E4.m1.3.3.3.3.1.1.1.1.cmml"><msub id="S3.E4.m1.3.3.3.3.1.1.1.1.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.3.3.3.3.1.1.1.1.2.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.2.cmml">ℒ</mi><mtext id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3a.cmml">region-word</mtext></msub><mo id="S3.E4.m1.3.3.3.3.1.1.1.1.1" xref="S3.E4.m1.3.3.3.3.1.1.1.1.1.cmml">+</mo><msub id="S3.E4.m1.3.3.3.3.1.1.1.1.3" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.3.3.3.3.1.1.1.1.3.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.2.cmml">ℒ</mi><mtext id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3a.cmml">image-text</mtext></msub></mrow><mo id="S3.E4.m1.3.3.3.3.1.1.1.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.4.4.4f" xref="S3.E4.m1.6.6.1.1.3.1.cmml"><mrow id="S3.E4.m1.4.4.4.4.2.1" xref="S3.E4.m1.4.4.4.4.2.1.cmml"><mrow id="S3.E4.m1.4.4.4.4.2.1.2" xref="S3.E4.m1.4.4.4.4.2.1.2.cmml"><mtext id="S3.E4.m1.4.4.4.4.2.1.2.2" xref="S3.E4.m1.4.4.4.4.2.1.2.2a.cmml"> if </mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.4.4.2.1.2.1" xref="S3.E4.m1.4.4.4.4.2.1.2.1.cmml">​</mo><mi id="S3.E4.m1.4.4.4.4.2.1.2.3" xref="S3.E4.m1.4.4.4.4.2.1.2.3.cmml">I</mi></mrow><mo id="S3.E4.m1.4.4.4.4.2.1.1" xref="S3.E4.m1.4.4.4.4.2.1.1.cmml">∈</mo><msup id="S3.E4.m1.4.4.4.4.2.1.3" xref="S3.E4.m1.4.4.4.4.2.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.4.4.4.4.2.1.3.2" xref="S3.E4.m1.4.4.4.4.2.1.3.2.cmml">𝒟</mi><mi id="S3.E4.m1.4.4.4.4.2.1.3.3" xref="S3.E4.m1.4.4.4.4.2.1.3.3.cmml">cap</mi></msup></mrow></mtd></mtr></mtable></mrow></mrow><mo id="S3.E4.m1.6.6.1.2" xref="S3.E4.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.6b"><apply id="S3.E4.m1.6.6.1.1.cmml" xref="S3.E4.m1.6.6.1"><eq id="S3.E4.m1.6.6.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1"></eq><apply id="S3.E4.m1.6.6.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.2"><times id="S3.E4.m1.6.6.1.1.2.1.cmml" xref="S3.E4.m1.6.6.1.1.2.1"></times><ci id="S3.E4.m1.6.6.1.1.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2">ℒ</ci><ci id="S3.E4.m1.5.5.cmml" xref="S3.E4.m1.5.5">𝐼</ci></apply><apply id="S3.E4.m1.6.6.1.1.3.1.cmml" xref="S3.E4.m1.4.4"><csymbol cd="latexml" id="S3.E4.m1.6.6.1.1.3.1.1.cmml" xref="S3.E4.m1.4.4.5">cases</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><plus id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.2">ℒ</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.3">rpn</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2">ℒ</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.3">reg</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.4.2">ℒ</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.4.3">cls</ci></apply></apply><apply id="S3.E4.m1.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.1"><in id="S3.E4.m1.2.2.2.2.2.1.1.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1"></in><apply id="S3.E4.m1.2.2.2.2.2.1.2.cmml" xref="S3.E4.m1.2.2.2.2.2.1.2"><times id="S3.E4.m1.2.2.2.2.2.1.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.1.2.1"></times><ci id="S3.E4.m1.2.2.2.2.2.1.2.2a.cmml" xref="S3.E4.m1.2.2.2.2.2.1.2.2"><mtext id="S3.E4.m1.2.2.2.2.2.1.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.1.2.2"> if </mtext></ci><ci id="S3.E4.m1.2.2.2.2.2.1.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2.1.2.3">𝐼</ci></apply><apply id="S3.E4.m1.2.2.2.2.2.1.3.cmml" xref="S3.E4.m1.2.2.2.2.2.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.2.1.3.1.cmml" xref="S3.E4.m1.2.2.2.2.2.1.3">superscript</csymbol><ci id="S3.E4.m1.2.2.2.2.2.1.3.2.cmml" xref="S3.E4.m1.2.2.2.2.2.1.3.2">𝒟</ci><ci id="S3.E4.m1.2.2.2.2.2.1.3.3.cmml" xref="S3.E4.m1.2.2.2.2.2.1.3.3">det</ci></apply></apply><apply id="S3.E4.m1.3.3.3.3.1.1.1.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1"><plus id="S3.E4.m1.3.3.3.3.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.1"></plus><apply id="S3.E4.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.3.1.1.1.1.2.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.2.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.2">ℒ</ci><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3a.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3">region-word</mtext></ci></apply><apply id="S3.E4.m1.3.3.3.3.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.3.1.1.1.1.3.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.3.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.2">ℒ</ci><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3a.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3">image-text</mtext></ci></apply></apply><apply id="S3.E4.m1.4.4.4.4.2.1.cmml" xref="S3.E4.m1.4.4.4.4.2.1"><in id="S3.E4.m1.4.4.4.4.2.1.1.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1"></in><apply id="S3.E4.m1.4.4.4.4.2.1.2.cmml" xref="S3.E4.m1.4.4.4.4.2.1.2"><times id="S3.E4.m1.4.4.4.4.2.1.2.1.cmml" xref="S3.E4.m1.4.4.4.4.2.1.2.1"></times><ci id="S3.E4.m1.4.4.4.4.2.1.2.2a.cmml" xref="S3.E4.m1.4.4.4.4.2.1.2.2"><mtext id="S3.E4.m1.4.4.4.4.2.1.2.2.cmml" xref="S3.E4.m1.4.4.4.4.2.1.2.2"> if </mtext></ci><ci id="S3.E4.m1.4.4.4.4.2.1.2.3.cmml" xref="S3.E4.m1.4.4.4.4.2.1.2.3">𝐼</ci></apply><apply id="S3.E4.m1.4.4.4.4.2.1.3.cmml" xref="S3.E4.m1.4.4.4.4.2.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.4.4.2.1.3.1.cmml" xref="S3.E4.m1.4.4.4.4.2.1.3">superscript</csymbol><ci id="S3.E4.m1.4.4.4.4.2.1.3.2.cmml" xref="S3.E4.m1.4.4.4.4.2.1.3.2">𝒟</ci><ci id="S3.E4.m1.4.4.4.4.2.1.3.3.cmml" xref="S3.E4.m1.4.4.4.4.2.1.3.3">cap</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.6c">\mathcal{L}(I)=\begin{cases}\mathcal{L}_{\mathrm{rpn}}+\mathcal{L}_{\mathrm{reg}}+\mathcal{L}_{\mathrm{cls}},&amp;\text{ if }I\in\mathcal{D}^{\mathrm{det}}\\
\mathcal{L}_{\text{region-word}}+\mathcal{L}_{\text{image-text}},&amp;\text{ if }I\in\mathcal{D}^{\mathrm{cap}}\end{cases}\,,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p1.2" class="ltx_p">where <math id="S3.SS4.p1.2.m1.3" class="ltx_Math" alttext="\mathcal{L}_{\mathrm{rpn}},\mathcal{L}_{\mathrm{reg}},\mathcal{L}_{\mathrm{cls}}" display="inline"><semantics id="S3.SS4.p1.2.m1.3a"><mrow id="S3.SS4.p1.2.m1.3.3.3" xref="S3.SS4.p1.2.m1.3.3.4.cmml"><msub id="S3.SS4.p1.2.m1.1.1.1.1" xref="S3.SS4.p1.2.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.2.m1.1.1.1.1.2" xref="S3.SS4.p1.2.m1.1.1.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p1.2.m1.1.1.1.1.3" xref="S3.SS4.p1.2.m1.1.1.1.1.3.cmml">rpn</mi></msub><mo id="S3.SS4.p1.2.m1.3.3.3.4" xref="S3.SS4.p1.2.m1.3.3.4.cmml">,</mo><msub id="S3.SS4.p1.2.m1.2.2.2.2" xref="S3.SS4.p1.2.m1.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.2.m1.2.2.2.2.2" xref="S3.SS4.p1.2.m1.2.2.2.2.2.cmml">ℒ</mi><mi id="S3.SS4.p1.2.m1.2.2.2.2.3" xref="S3.SS4.p1.2.m1.2.2.2.2.3.cmml">reg</mi></msub><mo id="S3.SS4.p1.2.m1.3.3.3.5" xref="S3.SS4.p1.2.m1.3.3.4.cmml">,</mo><msub id="S3.SS4.p1.2.m1.3.3.3.3" xref="S3.SS4.p1.2.m1.3.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.2.m1.3.3.3.3.2" xref="S3.SS4.p1.2.m1.3.3.3.3.2.cmml">ℒ</mi><mi id="S3.SS4.p1.2.m1.3.3.3.3.3" xref="S3.SS4.p1.2.m1.3.3.3.3.3.cmml">cls</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m1.3b"><list id="S3.SS4.p1.2.m1.3.3.4.cmml" xref="S3.SS4.p1.2.m1.3.3.3"><apply id="S3.SS4.p1.2.m1.1.1.1.1.cmml" xref="S3.SS4.p1.2.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m1.1.1.1.1.1.cmml" xref="S3.SS4.p1.2.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m1.1.1.1.1.2.cmml" xref="S3.SS4.p1.2.m1.1.1.1.1.2">ℒ</ci><ci id="S3.SS4.p1.2.m1.1.1.1.1.3.cmml" xref="S3.SS4.p1.2.m1.1.1.1.1.3">rpn</ci></apply><apply id="S3.SS4.p1.2.m1.2.2.2.2.cmml" xref="S3.SS4.p1.2.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m1.2.2.2.2.1.cmml" xref="S3.SS4.p1.2.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS4.p1.2.m1.2.2.2.2.2.cmml" xref="S3.SS4.p1.2.m1.2.2.2.2.2">ℒ</ci><ci id="S3.SS4.p1.2.m1.2.2.2.2.3.cmml" xref="S3.SS4.p1.2.m1.2.2.2.2.3">reg</ci></apply><apply id="S3.SS4.p1.2.m1.3.3.3.3.cmml" xref="S3.SS4.p1.2.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m1.3.3.3.3.1.cmml" xref="S3.SS4.p1.2.m1.3.3.3.3">subscript</csymbol><ci id="S3.SS4.p1.2.m1.3.3.3.3.2.cmml" xref="S3.SS4.p1.2.m1.3.3.3.3.2">ℒ</ci><ci id="S3.SS4.p1.2.m1.3.3.3.3.3.cmml" xref="S3.SS4.p1.2.m1.3.3.3.3.3">cls</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m1.3c">\mathcal{L}_{\mathrm{rpn}},\mathcal{L}_{\mathrm{reg}},\mathcal{L}_{\mathrm{cls}}</annotation></semantics></math> are standard losses in the two-stage detector.
For inference, CoDet does not require cross-image correspondence modeling as in training. It behaves like a normal two-stage object detector by forming the classifier with arbitrary language embeddings.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Benchmark Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">OV-LVIS</span>    is a general benchmark for open-vocabulary object detection, built upon LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> dataset which contains a diverse set of 1203 categories of objects with a long-tail distribution. Following standard practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>, we set the 866 common and frequent categories in LVIS as base categories, and leave the 337 rare categories as novel categories. Besides, we choose CC3M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> which contains 2.8 million free-from image-text pairs crawled from the web, as the source of image-text pairs. The main evaluation metric on OV-LVIS is the mask AP of novel (rare) categories.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">OV-COCO</span>    is derived from the popular COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> benchmark for evaluation of zero-shot and open-vocabulary object detection methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>. It splits the categories of COCO into 48 base categories and 17 novel categories, while removing the 15 categories without a synset in the WordNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>. As for image-caption data, following existing works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>, <a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, we use COCO Caption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> training set which provides 5 human-generated captions for each image for experiments on OV-COCO. The main evaluation metric on OV-COCO is the box <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\text{AP}_{50}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><msub id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mtext id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2a.cmml">AP</mtext><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2a.cmml" xref="S4.SS1.p2.1.m1.1.1.2"><mtext id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">AP</mtext></ci><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\text{AP}_{50}</annotation></semantics></math> of novel categories.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We extract object concepts from the text corpus of COCO Caption/CC3M using an off-the-shelf language parser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>. Remarkably, we filter out concepts without a synset in WordNet or outside the scope of the ‘object’ definition (<span id="S4.SS2.p1.1.1" class="ltx_ERROR undefined">\ie</span>, not under the hierarchy of ‘object’ synset in WordNet) to clean the extracted concepts. For phrases of more than one word, we simply apply the filtering logic to the last word in the phrase. Subsequently, we remove concepts with a frequency lower than 100/20 in COCO-Caption/CC3M. This leaves 634/4706 concepts for COCO/CC3M.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For experiments on OV-LVIS, unless otherwise specified, we use CenterNet2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite> with ResNet50 as the backbone, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>. For OV-COCO, Faster R-CNN with a ResNet50-C4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> backbone is adopted. To achieve faster convergence, we initialize the model with parameters from the base class detection pre-training as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>.
The batch size on a single GPU is set to 2/8 for COCO/LVIS detection data and 8/32 for COCO Caption/CC3M caption data. The ratio between the detection batch and caption batch is set to 1:1 during co-training. Notably, a caption batch by default contains four mini-groups, where each mini-group is constructed by sampling 2/8 image-text pairs from the same concept group in COCO Caption/CC3M. We train the model for 90k iterations on 8 GPUs.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.7.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.8.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison with state-of-the-art open-vocabulary object detection methods on OV-LVIS<span id="S4.T1.8.2.1" class="ltx_text ltx_font_medium">. Caption supervision means the method learns vision-language alignment from image-text pairs, while CLIP supervision indicates transferring knowledge from pre-trained CLIP. The column ‘Strict’ indicates whether the method follows a strict open-vocabulary setting.</span></span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.4" class="ltx_tr">
<th id="S4.T1.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.4.5.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<th id="S4.T1.4.4.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.4.6.1" class="ltx_text" style="font-size:80%;">Backbone</span></th>
<th id="S4.T1.4.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.4.7.1" class="ltx_text" style="font-size:80%;">Supervision</span></th>
<th id="S4.T1.4.4.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.4.8.1" class="ltx_text" style="font-size:80%;">Strict</span></th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{novel}}^{\text{m}}" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><msubsup id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T1.1.1.1.m1.1.1.2.2" xref="S4.T1.1.1.1.m1.1.1.2.2a.cmml">AP</mtext><mtext mathsize="80%" id="S4.T1.1.1.1.m1.1.1.2.3" xref="S4.T1.1.1.1.m1.1.1.2.3a.cmml">novel</mtext><mtext mathsize="80%" id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">superscript</csymbol><apply id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.1.1.1.m1.1.1.2.2a.cmml" xref="S4.T1.1.1.1.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2.2">AP</mtext></ci><ci id="S4.T1.1.1.1.m1.1.1.2.3a.cmml" xref="S4.T1.1.1.1.m1.1.1.2.3"><mtext mathsize="56%" id="S4.T1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T1.1.1.1.m1.1.1.2.3">novel</mtext></ci></apply><ci id="S4.T1.1.1.1.m1.1.1.3a.cmml" xref="S4.T1.1.1.1.m1.1.1.3"><mtext mathsize="56%" id="S4.T1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\text{AP}_{\text{novel}}^{\text{m}}</annotation></semantics></math></th>
<th id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{c}}^{\text{m}}" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><msubsup id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T1.2.2.2.m1.1.1.2.2" xref="S4.T1.2.2.2.m1.1.1.2.2a.cmml">AP</mtext><mtext mathsize="80%" id="S4.T1.2.2.2.m1.1.1.2.3" xref="S4.T1.2.2.2.m1.1.1.2.3a.cmml">c</mtext><mtext mathsize="80%" id="S4.T1.2.2.2.m1.1.1.3" xref="S4.T1.2.2.2.m1.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><apply id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.2.2.2.m1.1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">superscript</csymbol><apply id="S4.T1.2.2.2.m1.1.1.2.cmml" xref="S4.T1.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.2.2.2.m1.1.1.2.1.cmml" xref="S4.T1.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T1.2.2.2.m1.1.1.2.2a.cmml" xref="S4.T1.2.2.2.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T1.2.2.2.m1.1.1.2.2.cmml" xref="S4.T1.2.2.2.m1.1.1.2.2">AP</mtext></ci><ci id="S4.T1.2.2.2.m1.1.1.2.3a.cmml" xref="S4.T1.2.2.2.m1.1.1.2.3"><mtext mathsize="56%" id="S4.T1.2.2.2.m1.1.1.2.3.cmml" xref="S4.T1.2.2.2.m1.1.1.2.3">c</mtext></ci></apply><ci id="S4.T1.2.2.2.m1.1.1.3a.cmml" xref="S4.T1.2.2.2.m1.1.1.3"><mtext mathsize="56%" id="S4.T1.2.2.2.m1.1.1.3.cmml" xref="S4.T1.2.2.2.m1.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\text{AP}_{\text{c}}^{\text{m}}</annotation></semantics></math></th>
<th id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><math id="S4.T1.3.3.3.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{f}}^{\text{m}}" display="inline"><semantics id="S4.T1.3.3.3.m1.1a"><msubsup id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T1.3.3.3.m1.1.1.2.2" xref="S4.T1.3.3.3.m1.1.1.2.2a.cmml">AP</mtext><mtext mathsize="80%" id="S4.T1.3.3.3.m1.1.1.2.3" xref="S4.T1.3.3.3.m1.1.1.2.3a.cmml">f</mtext><mtext mathsize="80%" id="S4.T1.3.3.3.m1.1.1.3" xref="S4.T1.3.3.3.m1.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><apply id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.3.m1.1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">superscript</csymbol><apply id="S4.T1.3.3.3.m1.1.1.2.cmml" xref="S4.T1.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.3.m1.1.1.2.1.cmml" xref="S4.T1.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.T1.3.3.3.m1.1.1.2.2a.cmml" xref="S4.T1.3.3.3.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T1.3.3.3.m1.1.1.2.2.cmml" xref="S4.T1.3.3.3.m1.1.1.2.2">AP</mtext></ci><ci id="S4.T1.3.3.3.m1.1.1.2.3a.cmml" xref="S4.T1.3.3.3.m1.1.1.2.3"><mtext mathsize="56%" id="S4.T1.3.3.3.m1.1.1.2.3.cmml" xref="S4.T1.3.3.3.m1.1.1.2.3">f</mtext></ci></apply><ci id="S4.T1.3.3.3.m1.1.1.3a.cmml" xref="S4.T1.3.3.3.m1.1.1.3"><mtext mathsize="56%" id="S4.T1.3.3.3.m1.1.1.3.cmml" xref="S4.T1.3.3.3.m1.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\text{AP}_{\text{f}}^{\text{m}}</annotation></semantics></math></th>
<th id="S4.T1.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><math id="S4.T1.4.4.4.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{all}}^{\text{m}}" display="inline"><semantics id="S4.T1.4.4.4.m1.1a"><msubsup id="S4.T1.4.4.4.m1.1.1" xref="S4.T1.4.4.4.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T1.4.4.4.m1.1.1.2.2" xref="S4.T1.4.4.4.m1.1.1.2.2a.cmml">AP</mtext><mtext mathsize="80%" id="S4.T1.4.4.4.m1.1.1.2.3" xref="S4.T1.4.4.4.m1.1.1.2.3a.cmml">all</mtext><mtext mathsize="80%" id="S4.T1.4.4.4.m1.1.1.3" xref="S4.T1.4.4.4.m1.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.m1.1b"><apply id="S4.T1.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.4.4.4.m1.1.1.1.cmml" xref="S4.T1.4.4.4.m1.1.1">superscript</csymbol><apply id="S4.T1.4.4.4.m1.1.1.2.cmml" xref="S4.T1.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.4.4.4.m1.1.1.2.1.cmml" xref="S4.T1.4.4.4.m1.1.1">subscript</csymbol><ci id="S4.T1.4.4.4.m1.1.1.2.2a.cmml" xref="S4.T1.4.4.4.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T1.4.4.4.m1.1.1.2.2.cmml" xref="S4.T1.4.4.4.m1.1.1.2.2">AP</mtext></ci><ci id="S4.T1.4.4.4.m1.1.1.2.3a.cmml" xref="S4.T1.4.4.4.m1.1.1.2.3"><mtext mathsize="56%" id="S4.T1.4.4.4.m1.1.1.2.3.cmml" xref="S4.T1.4.4.4.m1.1.1.2.3">all</mtext></ci></apply><ci id="S4.T1.4.4.4.m1.1.1.3a.cmml" xref="S4.T1.4.4.4.m1.1.1.3"><mtext mathsize="56%" id="S4.T1.4.4.4.m1.1.1.3.cmml" xref="S4.T1.4.4.4.m1.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.m1.1c">\text{AP}_{\text{all}}^{\text{m}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.5.1" class="ltx_tr">
<th id="S4.T1.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.5.1.1.1" class="ltx_text" style="font-size:80%;">ViLD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.5.1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="S4.T1.4.5.1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.5.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.5.1.2.1" class="ltx_text" style="font-size:80%;">RN50-FPN</span></th>
<td id="S4.T1.4.5.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.5.1.3.1" class="ltx_text" style="font-size:80%;">CLIP</span></td>
<td id="S4.T1.4.5.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.5.1.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.5.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.5.1.5.1" class="ltx_text" style="font-size:80%;">16.6</span></td>
<td id="S4.T1.4.5.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.5.1.6.1" class="ltx_text" style="font-size:80%;">24.6</span></td>
<td id="S4.T1.4.5.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.5.1.7.1" class="ltx_text" style="font-size:80%;">30.3</span></td>
<td id="S4.T1.4.5.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.5.1.8.1" class="ltx_text" style="font-size:80%;">25.5</span></td>
</tr>
<tr id="S4.T1.4.6.2" class="ltx_tr">
<th id="S4.T1.4.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.6.2.1.1" class="ltx_text" style="font-size:80%;">RegionCLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.6.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a><span id="S4.T1.4.6.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.6.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.6.2.2.1" class="ltx_text" style="font-size:80%;">RN50-C4</span></th>
<td id="S4.T1.4.6.2.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.6.2.3.1" class="ltx_text" style="font-size:80%;">Caption</span></td>
<td id="S4.T1.4.6.2.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.6.2.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.6.2.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.6.2.5.1" class="ltx_text" style="font-size:80%;">17.1</span></td>
<td id="S4.T1.4.6.2.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.6.2.6.1" class="ltx_text" style="font-size:80%;">27.4</span></td>
<td id="S4.T1.4.6.2.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.6.2.7.1" class="ltx_text" style="font-size:80%;">34.0</span></td>
<td id="S4.T1.4.6.2.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.6.2.8.1" class="ltx_text" style="font-size:80%;">28.2</span></td>
</tr>
<tr id="S4.T1.4.7.3" class="ltx_tr">
<th id="S4.T1.4.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.7.3.1.1" class="ltx_text" style="font-size:80%;">DetPro </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.7.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a><span id="S4.T1.4.7.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.7.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.7.3.2.1" class="ltx_text" style="font-size:80%;">RN50-FPN</span></th>
<td id="S4.T1.4.7.3.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.7.3.3.1" class="ltx_text" style="font-size:80%;">CLIP</span></td>
<td id="S4.T1.4.7.3.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.7.3.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.7.3.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.7.3.5.1" class="ltx_text" style="font-size:80%;">19.8</span></td>
<td id="S4.T1.4.7.3.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.7.3.6.1" class="ltx_text" style="font-size:80%;">25.6</span></td>
<td id="S4.T1.4.7.3.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.7.3.7.1" class="ltx_text" style="font-size:80%;">28.9</span></td>
<td id="S4.T1.4.7.3.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.7.3.8.1" class="ltx_text" style="font-size:80%;">25.9</span></td>
</tr>
<tr id="S4.T1.4.8.4" class="ltx_tr">
<th id="S4.T1.4.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.8.4.1.1" class="ltx_text" style="font-size:80%;">OV-DETR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.8.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a><span id="S4.T1.4.8.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.8.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.8.4.2.1" class="ltx_text" style="font-size:80%;">RN50-C4</span></th>
<td id="S4.T1.4.8.4.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.8.4.3.1" class="ltx_text" style="font-size:80%;">Caption</span></td>
<td id="S4.T1.4.8.4.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.8.4.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T1.4.8.4.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.8.4.5.1" class="ltx_text" style="font-size:80%;">17.4</span></td>
<td id="S4.T1.4.8.4.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.8.4.6.1" class="ltx_text" style="font-size:80%;">25.0</span></td>
<td id="S4.T1.4.8.4.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.8.4.7.1" class="ltx_text" style="font-size:80%;">32.5</span></td>
<td id="S4.T1.4.8.4.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.8.4.8.1" class="ltx_text" style="font-size:80%;">26.6</span></td>
</tr>
<tr id="S4.T1.4.9.5" class="ltx_tr">
<th id="S4.T1.4.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.9.5.1.1" class="ltx_text" style="font-size:80%;">PromptDet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.9.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a><span id="S4.T1.4.9.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.9.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.9.5.2.1" class="ltx_text" style="font-size:80%;">RN50-FPN</span></th>
<td id="S4.T1.4.9.5.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.9.5.3.1" class="ltx_text" style="font-size:80%;">Caption</span></td>
<td id="S4.T1.4.9.5.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.9.5.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T1.4.9.5.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.9.5.5.1" class="ltx_text" style="font-size:80%;">19.0</span></td>
<td id="S4.T1.4.9.5.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.9.5.6.1" class="ltx_text" style="font-size:80%;">18.5</span></td>
<td id="S4.T1.4.9.5.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.9.5.7.1" class="ltx_text" style="font-size:80%;">25.8</span></td>
<td id="S4.T1.4.9.5.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.9.5.8.1" class="ltx_text" style="font-size:80%;">21.4</span></td>
</tr>
<tr id="S4.T1.4.10.6" class="ltx_tr">
<th id="S4.T1.4.10.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.10.6.1.1" class="ltx_text" style="font-size:80%;">Detic </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.10.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a><span id="S4.T1.4.10.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.10.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.10.6.2.1" class="ltx_text" style="font-size:80%;">RN50</span></th>
<td id="S4.T1.4.10.6.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.10.6.3.1" class="ltx_text" style="font-size:80%;">Caption</span></td>
<td id="S4.T1.4.10.6.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.10.6.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T1.4.10.6.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.10.6.5.1" class="ltx_text" style="font-size:80%;">19.5</span></td>
<td id="S4.T1.4.10.6.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.10.6.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.4.10.6.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.10.6.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.4.10.6.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.10.6.8.1" class="ltx_text" style="font-size:80%;">30.9</span></td>
</tr>
<tr id="S4.T1.4.11.7" class="ltx_tr">
<th id="S4.T1.4.11.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.11.7.1.1" class="ltx_text" style="font-size:80%;">F-VLM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.11.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a><span id="S4.T1.4.11.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.11.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.11.7.2.1" class="ltx_text" style="font-size:80%;">RN50-FPN</span></th>
<td id="S4.T1.4.11.7.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.11.7.3.1" class="ltx_text" style="font-size:80%;">CLIP</span></td>
<td id="S4.T1.4.11.7.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.11.7.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.11.7.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.11.7.5.1" class="ltx_text" style="font-size:80%;">18.6</span></td>
<td id="S4.T1.4.11.7.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.11.7.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.4.11.7.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.11.7.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.4.11.7.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.11.7.8.1" class="ltx_text" style="font-size:80%;">24.2</span></td>
</tr>
<tr id="S4.T1.4.12.8" class="ltx_tr">
<th id="S4.T1.4.12.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.12.8.1.1" class="ltx_text" style="font-size:80%;">VLDet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.12.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a><span id="S4.T1.4.12.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.12.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.12.8.2.1" class="ltx_text" style="font-size:80%;">RN50</span></th>
<td id="S4.T1.4.12.8.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.12.8.3.1" class="ltx_text" style="font-size:80%;">Caption</span></td>
<td id="S4.T1.4.12.8.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.12.8.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.12.8.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.12.8.5.1" class="ltx_text" style="font-size:80%;">21.7</span></td>
<td id="S4.T1.4.12.8.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.12.8.6.1" class="ltx_text" style="font-size:80%;">29.8</span></td>
<td id="S4.T1.4.12.8.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.12.8.7.1" class="ltx_text" style="font-size:80%;">34.3</span></td>
<td id="S4.T1.4.12.8.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.12.8.8.1" class="ltx_text" style="font-size:80%;">30.1</span></td>
</tr>
<tr id="S4.T1.4.13.9" class="ltx_tr">
<th id="S4.T1.4.13.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.13.9.1.1" class="ltx_text" style="font-size:80%;">BARON </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.13.9.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a><span id="S4.T1.4.13.9.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.13.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.13.9.2.1" class="ltx_text" style="font-size:80%;">RN50-FPN</span></th>
<td id="S4.T1.4.13.9.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.13.9.3.1" class="ltx_text" style="font-size:80%;">CLIP</span></td>
<td id="S4.T1.4.13.9.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.13.9.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.13.9.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.13.9.5.1" class="ltx_text" style="font-size:80%;">22.6</span></td>
<td id="S4.T1.4.13.9.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.13.9.6.1" class="ltx_text" style="font-size:80%;">27.6</span></td>
<td id="S4.T1.4.13.9.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.13.9.7.1" class="ltx_text" style="font-size:80%;">29.8</span></td>
<td id="S4.T1.4.13.9.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.13.9.8.1" class="ltx_text" style="font-size:80%;">27.6</span></td>
</tr>
<tr id="S4.T1.4.14.10" class="ltx_tr">
<th id="S4.T1.4.14.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.14.10.1.1" class="ltx_text" style="font-size:80%;">CoDet (Ours)</span></th>
<th id="S4.T1.4.14.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.14.10.2.1" class="ltx_text" style="font-size:80%;">RN50</span></th>
<td id="S4.T1.4.14.10.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.14.10.3.1" class="ltx_text" style="font-size:80%;">Caption</span></td>
<td id="S4.T1.4.14.10.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.14.10.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.14.10.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.14.10.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">23.4</span></td>
<td id="S4.T1.4.14.10.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.14.10.6.1" class="ltx_text" style="font-size:80%;">30.0</span></td>
<td id="S4.T1.4.14.10.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.14.10.7.1" class="ltx_text" style="font-size:80%;">34.6</span></td>
<td id="S4.T1.4.14.10.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.14.10.8.1" class="ltx_text" style="font-size:80%;">30.7</span></td>
</tr>
<tr id="S4.T1.4.15.11" class="ltx_tr">
<th id="S4.T1.4.15.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.15.11.1.1" class="ltx_text" style="font-size:80%;">RegionCLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.15.11.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a><span id="S4.T1.4.15.11.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.15.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.15.11.2.1" class="ltx_text" style="font-size:80%;">R50x4 (87M)</span></th>
<td id="S4.T1.4.15.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.15.11.3.1" class="ltx_text" style="font-size:80%;">Caption</span></td>
<td id="S4.T1.4.15.11.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.15.11.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.15.11.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.15.11.5.1" class="ltx_text" style="font-size:80%;">22.0</span></td>
<td id="S4.T1.4.15.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.15.11.6.1" class="ltx_text" style="font-size:80%;">32.1</span></td>
<td id="S4.T1.4.15.11.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.15.11.7.1" class="ltx_text" style="font-size:80%;">36.9</span></td>
<td id="S4.T1.4.15.11.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.15.11.8.1" class="ltx_text" style="font-size:80%;">32.3</span></td>
</tr>
<tr id="S4.T1.4.16.12" class="ltx_tr">
<th id="S4.T1.4.16.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.16.12.1.1" class="ltx_text" style="font-size:80%;">Detic </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.16.12.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a><span id="S4.T1.4.16.12.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.16.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.16.12.2.1" class="ltx_text" style="font-size:80%;">SwinB (88M)</span></th>
<td id="S4.T1.4.16.12.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.16.12.3.1" class="ltx_text" style="font-size:80%;">Caption</span></td>
<td id="S4.T1.4.16.12.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.16.12.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T1.4.16.12.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.16.12.5.1" class="ltx_text" style="font-size:80%;">23.9</span></td>
<td id="S4.T1.4.16.12.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.16.12.6.1" class="ltx_text" style="font-size:80%;">40.2</span></td>
<td id="S4.T1.4.16.12.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.16.12.7.1" class="ltx_text" style="font-size:80%;">42.8</span></td>
<td id="S4.T1.4.16.12.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.16.12.8.1" class="ltx_text" style="font-size:80%;">38.4</span></td>
</tr>
<tr id="S4.T1.4.17.13" class="ltx_tr">
<th id="S4.T1.4.17.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.17.13.1.1" class="ltx_text" style="font-size:80%;">F-VLM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.17.13.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a><span id="S4.T1.4.17.13.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.17.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.17.13.2.1" class="ltx_text" style="font-size:80%;">R50x4 (87M)</span></th>
<td id="S4.T1.4.17.13.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.17.13.3.1" class="ltx_text" style="font-size:80%;">CLIP</span></td>
<td id="S4.T1.4.17.13.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.17.13.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.17.13.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.17.13.5.1" class="ltx_text" style="font-size:80%;">26.3</span></td>
<td id="S4.T1.4.17.13.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.17.13.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.4.17.13.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.17.13.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.4.17.13.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.17.13.8.1" class="ltx_text" style="font-size:80%;">28.5</span></td>
</tr>
<tr id="S4.T1.4.18.14" class="ltx_tr">
<th id="S4.T1.4.18.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.18.14.1.1" class="ltx_text" style="font-size:80%;">VLDet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.18.14.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a><span id="S4.T1.4.18.14.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.18.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.18.14.2.1" class="ltx_text" style="font-size:80%;">SwinB (88M)</span></th>
<td id="S4.T1.4.18.14.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.18.14.3.1" class="ltx_text" style="font-size:80%;">Caption</span></td>
<td id="S4.T1.4.18.14.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.18.14.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.18.14.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.18.14.5.1" class="ltx_text" style="font-size:80%;">26.3</span></td>
<td id="S4.T1.4.18.14.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.18.14.6.1" class="ltx_text" style="font-size:80%;">39.4</span></td>
<td id="S4.T1.4.18.14.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.18.14.7.1" class="ltx_text" style="font-size:80%;">41.9</span></td>
<td id="S4.T1.4.18.14.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.18.14.8.1" class="ltx_text" style="font-size:80%;">38.1</span></td>
</tr>
<tr id="S4.T1.4.19.15" class="ltx_tr">
<th id="S4.T1.4.19.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.19.15.1.1" class="ltx_text" style="font-size:80%;">CoDet (Ours)</span></th>
<th id="S4.T1.4.19.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.19.15.2.1" class="ltx_text" style="font-size:80%;">SwinB (88M)</span></th>
<td id="S4.T1.4.19.15.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.19.15.3.1" class="ltx_text" style="font-size:80%;">Caption</span></td>
<td id="S4.T1.4.19.15.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.19.15.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.19.15.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.19.15.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">29.4</span></td>
<td id="S4.T1.4.19.15.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.19.15.6.1" class="ltx_text" style="font-size:80%;">39.5</span></td>
<td id="S4.T1.4.19.15.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.19.15.7.1" class="ltx_text" style="font-size:80%;">43.0</span></td>
<td id="S4.T1.4.19.15.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.19.15.8.1" class="ltx_text" style="font-size:80%;">39.2</span></td>
</tr>
<tr id="S4.T1.4.20.16" class="ltx_tr">
<th id="S4.T1.4.20.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 8.0pt;">
<span id="S4.T1.4.20.16.1.1" class="ltx_text" style="font-size:80%;">F-VLM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.20.16.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a><span id="S4.T1.4.20.16.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T1.4.20.16.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.20.16.2.1" class="ltx_text" style="font-size:80%;">R50x64 (420M)</span></th>
<td id="S4.T1.4.20.16.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.20.16.3.1" class="ltx_text" style="font-size:80%;">CLIP</span></td>
<td id="S4.T1.4.20.16.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.20.16.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.20.16.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.20.16.5.1" class="ltx_text" style="font-size:80%;">32.8</span></td>
<td id="S4.T1.4.20.16.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.20.16.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.4.20.16.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.20.16.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.4.20.16.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.20.16.8.1" class="ltx_text" style="font-size:80%;">34.9</span></td>
</tr>
<tr id="S4.T1.4.21.17" class="ltx_tr">
<th id="S4.T1.4.21.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.21.17.1.1" class="ltx_text" style="font-size:80%;">CoDet (Ours)</span></th>
<th id="S4.T1.4.21.17.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.21.17.2.1" class="ltx_text" style="font-size:80%;">EVA02-L (304M)</span></th>
<td id="S4.T1.4.21.17.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.21.17.3.1" class="ltx_text" style="font-size:80%;">Caption</span></td>
<td id="S4.T1.4.21.17.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.21.17.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T1.4.21.17.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.21.17.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">37.0</span></td>
<td id="S4.T1.4.21.17.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.21.17.6.1" class="ltx_text" style="font-size:80%;">46.3</span></td>
<td id="S4.T1.4.21.17.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.21.17.7.1" class="ltx_text" style="font-size:80%;">46.3</span></td>
<td id="S4.T1.4.21.17.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T1.4.21.17.8.1" class="ltx_text" style="font-size:80%;">44.7</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Benchmark Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents our results on OV-LVIS. We follow a strict open-vocabulary setting where novel categories are kept unknown during training, to ensure we obtain a generic open-vocabulary detector not biased towards specific novel categories. It can be seen that CoDet consistently outperforms SoTA methods in novel object detection. Especially, among the group of methods learning from caption supervision, CoDet surpasses all alternatives which rely on CLIP (RegionCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>, OV-DETR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>), max-size prior (Detic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite>), or self-trained VLM (PromptDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, VLDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>) to generate pseudo region-text pairs, demonstrating the superiority of visual guidance in region-text alignment.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">In addition, we validate the scalability of CoDet by testing with more powerful visual backbones, <span id="S4.SS3.p2.1.1" class="ltx_ERROR undefined">\ie</span>, Swin-B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> and EVA02-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>. It turns out that our method scales up surprisingly well with model capacity - it leads to a +6.0/13.6 <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{novel}}^{\text{m}}" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><msubsup id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mtext id="S4.SS3.p2.1.m1.1.1.2.2" xref="S4.SS3.p2.1.m1.1.1.2.2a.cmml">AP</mtext><mtext id="S4.SS3.p2.1.m1.1.1.2.3" xref="S4.SS3.p2.1.m1.1.1.2.3a.cmml">novel</mtext><mtext id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">superscript</csymbol><apply id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.2.1.cmml" xref="S4.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.2a.cmml" xref="S4.SS3.p2.1.m1.1.1.2.2"><mtext id="S4.SS3.p2.1.m1.1.1.2.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2.2">AP</mtext></ci><ci id="S4.SS3.p2.1.m1.1.1.2.3a.cmml" xref="S4.SS3.p2.1.m1.1.1.2.3"><mtext mathsize="70%" id="S4.SS3.p2.1.m1.1.1.2.3.cmml" xref="S4.SS3.p2.1.m1.1.1.2.3">novel</mtext></ci></apply><ci id="S4.SS3.p2.1.m1.1.1.3a.cmml" xref="S4.SS3.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\text{AP}_{\text{novel}}^{\text{m}}</annotation></semantics></math> performance boost by switching from ResNet50 to Swin-B/EVA02-L, and continuously enlarges the performance gains over the second best method with a comparable model size. We believe this is because stronger visual representations provide more consistent semantic correspondences across images, which are critical for discovering co-occurring objects among the concept group.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Benchmark Results ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents our results on OV-COCO, where CoDet achieves the second best performance among existing methods. Compared with the leading method VLDet, the underperformance of CoDet can be mainly attributed to the human-curated bias in COCO Caption data distribution. That is, images in COCO Caption contain at least one of the 80 categories in COCO, which leads to highly concentrated concepts. For instance, roughly 1/2 of the images contain ‘people’, and 1/10 of the images contain ‘car’. This unavoidably incurs many hard negatives for identifying co-occurring objects of interest. Ablation studies on the concept group size of CoDet in Table <a href="#S4.T5" title="Table 5 ‣ Size of concept group. ‣ 4.6 Ablation Study ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> reveals the same problem. But we believe this would not harm the generality of our method as we show CoDet works well on web-crawled data (<span id="S4.SS3.p3.1.1" class="ltx_ERROR undefined">\eg</span>, CC3M), which is a more practical setting and can easily be scaled up.</p>
</div>
<div id="S4.SS3.5" class="ltx_logical-block ltx_minipage ltx_align_top" style="width:182.1pt;">
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.6.2.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison with state-of-the-art methods on OV-COCO<span id="S4.T2.2.1.1" class="ltx_text ltx_font_medium">. <sup id="S4.T2.2.1.1.1" class="ltx_sup"><span id="S4.T2.2.1.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>: implemented with Deformable DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>.</span></span></figcaption>
</figure>
<div id="S4.SS3.5.p1" class="ltx_para">
<table id="S4.SS3.4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS3.3.3.3" class="ltx_tr">
<th id="S4.SS3.3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.3.3.3.4.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<th id="S4.SS3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 4.0pt;"><math id="S4.SS3.1.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{novel}}" display="inline"><semantics id="S4.SS3.1.1.1.1.m1.1a"><msubsup id="S4.SS3.1.1.1.1.m1.1.1" xref="S4.SS3.1.1.1.1.m1.1.1.cmml"><mtext mathsize="80%" id="S4.SS3.1.1.1.1.m1.1.1.2.2" xref="S4.SS3.1.1.1.1.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.SS3.1.1.1.1.m1.1.1.2.3" xref="S4.SS3.1.1.1.1.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.SS3.1.1.1.1.m1.1.1.3" xref="S4.SS3.1.1.1.1.m1.1.1.3a.cmml">novel</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.1.1.1.1.m1.1b"><apply id="S4.SS3.1.1.1.1.m1.1.1.cmml" xref="S4.SS3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.1.1.1.1.m1.1.1.1.cmml" xref="S4.SS3.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S4.SS3.1.1.1.1.m1.1.1.2.cmml" xref="S4.SS3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.SS3.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.1.1.1.1.m1.1.1.2.2a.cmml" xref="S4.SS3.1.1.1.1.m1.1.1.2.2"><mtext mathsize="80%" id="S4.SS3.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.SS3.1.1.1.1.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.SS3.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.SS3.1.1.1.1.m1.1.1.2.3">50</cn></apply><ci id="S4.SS3.1.1.1.1.m1.1.1.3a.cmml" xref="S4.SS3.1.1.1.1.m1.1.1.3"><mtext mathsize="56%" id="S4.SS3.1.1.1.1.m1.1.1.3.cmml" xref="S4.SS3.1.1.1.1.m1.1.1.3">novel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.1.1.1.1.m1.1c">\text{AP}_{50}^{\text{novel}}</annotation></semantics></math></th>
<th id="S4.SS3.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 4.0pt;"><math id="S4.SS3.2.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{base}}" display="inline"><semantics id="S4.SS3.2.2.2.2.m1.1a"><msubsup id="S4.SS3.2.2.2.2.m1.1.1" xref="S4.SS3.2.2.2.2.m1.1.1.cmml"><mtext mathsize="80%" id="S4.SS3.2.2.2.2.m1.1.1.2.2" xref="S4.SS3.2.2.2.2.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.SS3.2.2.2.2.m1.1.1.2.3" xref="S4.SS3.2.2.2.2.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.SS3.2.2.2.2.m1.1.1.3" xref="S4.SS3.2.2.2.2.m1.1.1.3a.cmml">base</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.2.2.2.2.m1.1b"><apply id="S4.SS3.2.2.2.2.m1.1.1.cmml" xref="S4.SS3.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.2.2.2.2.m1.1.1.1.cmml" xref="S4.SS3.2.2.2.2.m1.1.1">superscript</csymbol><apply id="S4.SS3.2.2.2.2.m1.1.1.2.cmml" xref="S4.SS3.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.2.2.2.2.m1.1.1.2.1.cmml" xref="S4.SS3.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.SS3.2.2.2.2.m1.1.1.2.2a.cmml" xref="S4.SS3.2.2.2.2.m1.1.1.2.2"><mtext mathsize="80%" id="S4.SS3.2.2.2.2.m1.1.1.2.2.cmml" xref="S4.SS3.2.2.2.2.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.SS3.2.2.2.2.m1.1.1.2.3.cmml" xref="S4.SS3.2.2.2.2.m1.1.1.2.3">50</cn></apply><ci id="S4.SS3.2.2.2.2.m1.1.1.3a.cmml" xref="S4.SS3.2.2.2.2.m1.1.1.3"><mtext mathsize="56%" id="S4.SS3.2.2.2.2.m1.1.1.3.cmml" xref="S4.SS3.2.2.2.2.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.2.2.2.2.m1.1c">\text{AP}_{50}^{\text{base}}</annotation></semantics></math></th>
<th id="S4.SS3.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 4.0pt;"><math id="S4.SS3.3.3.3.3.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{all}}" display="inline"><semantics id="S4.SS3.3.3.3.3.m1.1a"><msubsup id="S4.SS3.3.3.3.3.m1.1.1" xref="S4.SS3.3.3.3.3.m1.1.1.cmml"><mtext mathsize="80%" id="S4.SS3.3.3.3.3.m1.1.1.2.2" xref="S4.SS3.3.3.3.3.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.SS3.3.3.3.3.m1.1.1.2.3" xref="S4.SS3.3.3.3.3.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.SS3.3.3.3.3.m1.1.1.3" xref="S4.SS3.3.3.3.3.m1.1.1.3a.cmml">all</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.3.3.3.3.m1.1b"><apply id="S4.SS3.3.3.3.3.m1.1.1.cmml" xref="S4.SS3.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.3.3.3.3.m1.1.1.1.cmml" xref="S4.SS3.3.3.3.3.m1.1.1">superscript</csymbol><apply id="S4.SS3.3.3.3.3.m1.1.1.2.cmml" xref="S4.SS3.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.3.3.3.3.m1.1.1.2.1.cmml" xref="S4.SS3.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.SS3.3.3.3.3.m1.1.1.2.2a.cmml" xref="S4.SS3.3.3.3.3.m1.1.1.2.2"><mtext mathsize="80%" id="S4.SS3.3.3.3.3.m1.1.1.2.2.cmml" xref="S4.SS3.3.3.3.3.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.SS3.3.3.3.3.m1.1.1.2.3.cmml" xref="S4.SS3.3.3.3.3.m1.1.1.2.3">50</cn></apply><ci id="S4.SS3.3.3.3.3.m1.1.1.3a.cmml" xref="S4.SS3.3.3.3.3.m1.1.1.3"><mtext mathsize="56%" id="S4.SS3.3.3.3.3.m1.1.1.3.cmml" xref="S4.SS3.3.3.3.3.m1.1.1.3">all</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.3.3.3.3.m1.1c">\text{AP}_{50}^{\text{all}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS3.4.4.5.1" class="ltx_tr">
<th id="S4.SS3.4.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.4.4.5.1.1.1" class="ltx_text" style="font-size:80%;">OVR-CNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.4.4.5.1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a><span id="S4.SS3.4.4.5.1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.SS3.4.4.5.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.5.1.2.1" class="ltx_text" style="font-size:80%;">22.8</span></td>
<td id="S4.SS3.4.4.5.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.5.1.3.1" class="ltx_text" style="font-size:80%;">46.0</span></td>
<td id="S4.SS3.4.4.5.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.5.1.4.1" class="ltx_text" style="font-size:80%;">39.9</span></td>
</tr>
<tr id="S4.SS3.4.4.6.2" class="ltx_tr">
<th id="S4.SS3.4.4.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.4.4.6.2.1.1" class="ltx_text" style="font-size:80%;">ViLD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.4.4.6.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="S4.SS3.4.4.6.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.SS3.4.4.6.2.2" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.6.2.2.1" class="ltx_text" style="font-size:80%;">27.6</span></td>
<td id="S4.SS3.4.4.6.2.3" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.6.2.3.1" class="ltx_text" style="font-size:80%;">59.5</span></td>
<td id="S4.SS3.4.4.6.2.4" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.6.2.4.1" class="ltx_text" style="font-size:80%;">51.3</span></td>
</tr>
<tr id="S4.SS3.4.4.7.3" class="ltx_tr">
<th id="S4.SS3.4.4.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.4.4.7.3.1.1" class="ltx_text" style="font-size:80%;">RegionCLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.4.4.7.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a><span id="S4.SS3.4.4.7.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.SS3.4.4.7.3.2" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.7.3.2.1" class="ltx_text" style="font-size:80%;">26.8</span></td>
<td id="S4.SS3.4.4.7.3.3" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.7.3.3.1" class="ltx_text" style="font-size:80%;">54.8</span></td>
<td id="S4.SS3.4.4.7.3.4" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.7.3.4.1" class="ltx_text" style="font-size:80%;">47.5</span></td>
</tr>
<tr id="S4.SS3.4.4.8.4" class="ltx_tr">
<th id="S4.SS3.4.4.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.4.4.8.4.1.1" class="ltx_text" style="font-size:80%;">Detic </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.4.4.8.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a><span id="S4.SS3.4.4.8.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.SS3.4.4.8.4.2" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.8.4.2.1" class="ltx_text" style="font-size:80%;">27.8</span></td>
<td id="S4.SS3.4.4.8.4.3" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.8.4.3.1" class="ltx_text" style="font-size:80%;">47.1</span></td>
<td id="S4.SS3.4.4.8.4.4" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.8.4.4.1" class="ltx_text" style="font-size:80%;">42.0</span></td>
</tr>
<tr id="S4.SS3.4.4.4" class="ltx_tr">
<th id="S4.SS3.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.4.4.4.1.1" class="ltx_text" style="font-size:80%;">OV-DETR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.4.4.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a><span id="S4.SS3.4.4.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><sup id="S4.SS3.4.4.4.1.4" class="ltx_sup"><span id="S4.SS3.4.4.4.1.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">†</span></sup>
</th>
<td id="S4.SS3.4.4.4.2" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.4.2.1" class="ltx_text" style="font-size:80%;">29.4</span></td>
<td id="S4.SS3.4.4.4.3" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.4.3.1" class="ltx_text" style="font-size:80%;">61.0</span></td>
<td id="S4.SS3.4.4.4.4" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.4.4.1" class="ltx_text" style="font-size:80%;">52.7</span></td>
</tr>
<tr id="S4.SS3.4.4.9.5" class="ltx_tr">
<th id="S4.SS3.4.4.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.4.4.9.5.1.1" class="ltx_text" style="font-size:80%;">PB-OVD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.4.4.9.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a><span id="S4.SS3.4.4.9.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.SS3.4.4.9.5.2" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.9.5.2.1" class="ltx_text" style="font-size:80%;">29.1</span></td>
<td id="S4.SS3.4.4.9.5.3" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.9.5.3.1" class="ltx_text" style="font-size:80%;">44.4</span></td>
<td id="S4.SS3.4.4.9.5.4" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.9.5.4.1" class="ltx_text" style="font-size:80%;">40.4</span></td>
</tr>
<tr id="S4.SS3.4.4.10.6" class="ltx_tr">
<th id="S4.SS3.4.4.10.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.10.6.1.1" class="ltx_text" style="font-size:80%;">VLDet</span></th>
<td id="S4.SS3.4.4.10.6.2" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.10.6.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">32.0</span></td>
<td id="S4.SS3.4.4.10.6.3" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.10.6.3.1" class="ltx_text" style="font-size:80%;">50.6</span></td>
<td id="S4.SS3.4.4.10.6.4" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.10.6.4.1" class="ltx_text" style="font-size:80%;">45.8</span></td>
</tr>
<tr id="S4.SS3.4.4.11.7" class="ltx_tr">
<th id="S4.SS3.4.4.11.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.11.7.1.1" class="ltx_text" style="font-size:80%;">CoDet (Ours)</span></th>
<td id="S4.SS3.4.4.11.7.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.11.7.2.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">30.6</span></td>
<td id="S4.SS3.4.4.11.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.11.7.3.1" class="ltx_text" style="font-size:80%;">52.3</span></td>
<td id="S4.SS3.4.4.11.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.4.4.11.7.4.1" class="ltx_text" style="font-size:80%;">46.6</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="S4.SS3.11" class="ltx_logical-block ltx_minipage ltx_align_top" style="width:238.5pt;">
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.6.2.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Cross-datasets transfer detection from OV-LVIS to COCO and Objects365.<span id="S4.T3.2.1.1" class="ltx_text ltx_font_medium"> <sup id="S4.T3.2.1.1.1" class="ltx_sup"><span id="S4.T3.2.1.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>: Detection-specialized pre-training with SoCo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>.</span></span></figcaption>
</figure>
<div id="S4.SS3.11.p1" class="ltx_para">
<table id="S4.SS3.10.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.SS3.10.5.6.1" class="ltx_tr">
<th id="S4.SS3.10.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding:0.4pt 4.0pt;" rowspan="2"><span id="S4.SS3.10.5.6.1.1.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<td id="S4.SS3.10.5.6.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.4pt 4.0pt;" colspan="3"><span id="S4.SS3.10.5.6.1.2.1" class="ltx_text" style="font-size:80%;">COCO</span></td>
<td id="S4.SS3.10.5.6.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.4pt 4.0pt;" colspan="3"><span id="S4.SS3.10.5.6.1.3.1" class="ltx_text" style="font-size:80%;">Objects365</span></td>
</tr>
<tr id="S4.SS3.9.4.4" class="ltx_tr">
<td id="S4.SS3.9.4.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.9.4.4.5.1" class="ltx_text" style="font-size:80%;">AP</span></td>
<td id="S4.SS3.6.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.6.1.1.1.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.SS3.6.1.1.1.2" class="ltx_sub"><span id="S4.SS3.6.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">50</span></sub>
</td>
<td id="S4.SS3.7.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.7.2.2.2.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.SS3.7.2.2.2.2" class="ltx_sub"><span id="S4.SS3.7.2.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">75</span></sub>
</td>
<td id="S4.SS3.9.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.9.4.4.6.1" class="ltx_text" style="font-size:80%;">AP</span></td>
<td id="S4.SS3.8.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.8.3.3.3.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.SS3.8.3.3.3.2" class="ltx_sub"><span id="S4.SS3.8.3.3.3.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">50</span></sub>
</td>
<td id="S4.SS3.9.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.9.4.4.4.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.SS3.9.4.4.4.2" class="ltx_sub"><span id="S4.SS3.9.4.4.4.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">75</span></sub>
</td>
</tr>
<tr id="S4.SS3.10.5.7.2" class="ltx_tr">
<th id="S4.SS3.10.5.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.10.5.7.2.1.1" class="ltx_text" style="font-size:80%;">Supervised </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.10.5.7.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="S4.SS3.10.5.7.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.SS3.10.5.7.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.7.2.2.1" class="ltx_text" style="font-size:80%;">46.5</span></td>
<td id="S4.SS3.10.5.7.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.7.2.3.1" class="ltx_text" style="font-size:80%;">67.6</span></td>
<td id="S4.SS3.10.5.7.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.7.2.4.1" class="ltx_text" style="font-size:80%;">50.9</span></td>
<td id="S4.SS3.10.5.7.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.7.2.5.1" class="ltx_text" style="font-size:80%;">25.6</span></td>
<td id="S4.SS3.10.5.7.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.7.2.6.1" class="ltx_text" style="font-size:80%;">38.6</span></td>
<td id="S4.SS3.10.5.7.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.7.2.7.1" class="ltx_text" style="font-size:80%;">28.0</span></td>
</tr>
<tr id="S4.SS3.10.5.8.3" class="ltx_tr">
<th id="S4.SS3.10.5.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.10.5.8.3.1.1" class="ltx_text" style="font-size:80%;">ViLD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.10.5.8.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="S4.SS3.10.5.8.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.SS3.10.5.8.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.8.3.2.1" class="ltx_text" style="font-size:80%;">36.6</span></td>
<td id="S4.SS3.10.5.8.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.8.3.3.1" class="ltx_text" style="font-size:80%;">55.6</span></td>
<td id="S4.SS3.10.5.8.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.8.3.4.1" class="ltx_text" style="font-size:80%;">39.8</span></td>
<td id="S4.SS3.10.5.8.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.8.3.5.1" class="ltx_text" style="font-size:80%;">11.8</span></td>
<td id="S4.SS3.10.5.8.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.8.3.6.1" class="ltx_text" style="font-size:80%;">18.2</span></td>
<td id="S4.SS3.10.5.8.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.8.3.7.1" class="ltx_text" style="font-size:80%;">12.6</span></td>
</tr>
<tr id="S4.SS3.10.5.5" class="ltx_tr">
<th id="S4.SS3.10.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.10.5.5.1.1" class="ltx_text" style="font-size:80%;">DetPro </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.10.5.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a><span id="S4.SS3.10.5.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><sup id="S4.SS3.10.5.5.1.4" class="ltx_sup"><span id="S4.SS3.10.5.5.1.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">†</span></sup>
</th>
<td id="S4.SS3.10.5.5.2" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.5.2.1" class="ltx_text" style="font-size:80%;">34.9</span></td>
<td id="S4.SS3.10.5.5.3" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.5.3.1" class="ltx_text" style="font-size:80%;">53.8</span></td>
<td id="S4.SS3.10.5.5.4" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.5.4.1" class="ltx_text" style="font-size:80%;">37.4</span></td>
<td id="S4.SS3.10.5.5.5" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.5.5.1" class="ltx_text" style="font-size:80%;">12.1</span></td>
<td id="S4.SS3.10.5.5.6" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.5.6.1" class="ltx_text" style="font-size:80%;">18.8</span></td>
<td id="S4.SS3.10.5.5.7" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.5.7.1" class="ltx_text" style="font-size:80%;">12.9</span></td>
</tr>
<tr id="S4.SS3.10.5.9.4" class="ltx_tr">
<th id="S4.SS3.10.5.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.10.5.9.4.1.1" class="ltx_text" style="font-size:80%;">F-VLM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.10.5.9.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a><span id="S4.SS3.10.5.9.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.SS3.10.5.9.4.2" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.9.4.2.1" class="ltx_text" style="font-size:80%;">32.5</span></td>
<td id="S4.SS3.10.5.9.4.3" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.9.4.3.1" class="ltx_text" style="font-size:80%;">53.1</span></td>
<td id="S4.SS3.10.5.9.4.4" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.9.4.4.1" class="ltx_text" style="font-size:80%;">34.6</span></td>
<td id="S4.SS3.10.5.9.4.5" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.9.4.5.1" class="ltx_text" style="font-size:80%;">11.9</span></td>
<td id="S4.SS3.10.5.9.4.6" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.9.4.6.1" class="ltx_text" style="font-size:80%;">19.2</span></td>
<td id="S4.SS3.10.5.9.4.7" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.9.4.7.1" class="ltx_text" style="font-size:80%;">12.6</span></td>
</tr>
<tr id="S4.SS3.10.5.10.5" class="ltx_tr">
<th id="S4.SS3.10.5.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.4pt 4.0pt;">
<span id="S4.SS3.10.5.10.5.1.1" class="ltx_text" style="font-size:80%;">BARON </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.10.5.10.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a><span id="S4.SS3.10.5.10.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.SS3.10.5.10.5.2" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.10.5.2.1" class="ltx_text" style="font-size:80%;">36.2</span></td>
<td id="S4.SS3.10.5.10.5.3" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.10.5.3.1" class="ltx_text" style="font-size:80%;">55.7</span></td>
<td id="S4.SS3.10.5.10.5.4" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.10.5.4.1" class="ltx_text" style="font-size:80%;">39.1</span></td>
<td id="S4.SS3.10.5.10.5.5" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.10.5.5.1" class="ltx_text" style="font-size:80%;">13.6</span></td>
<td id="S4.SS3.10.5.10.5.6" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.10.5.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">21.0</span></td>
<td id="S4.SS3.10.5.10.5.7" class="ltx_td ltx_align_center" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.10.5.7.1" class="ltx_text" style="font-size:80%;">14.5</span></td>
</tr>
<tr id="S4.SS3.10.5.11.6" class="ltx_tr">
<th id="S4.SS3.10.5.11.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.11.6.1.1" class="ltx_text" style="font-size:80%;">CoDet (Ours)</span></th>
<td id="S4.SS3.10.5.11.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.11.6.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">39.1</span></td>
<td id="S4.SS3.10.5.11.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.11.6.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">57.0</span></td>
<td id="S4.SS3.10.5.11.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.11.6.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">42.3</span></td>
<td id="S4.SS3.10.5.11.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.11.6.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">14.2</span></td>
<td id="S4.SS3.10.5.11.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.11.6.6.1" class="ltx_text" style="font-size:80%;">20.5</span></td>
<td id="S4.SS3.10.5.11.6.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 4.0pt;"><span id="S4.SS3.10.5.11.6.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">15.3</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Transfer to Other Datasets</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In simulation to detection in the open world, where test data may come from different domains, we conduct cross-dataset transfer detection experiments in Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Benchmark Results ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Specifically, we transfer the open-vocabulary detector trained on OV-LVIS (LVIS base + CC3M) to COCO and Objects365 v1, by plugging in the vocabulary of test datasets without further model fine-tuning. In comparison with existing works, CoDet outperforms the second-best method ViLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> which uses a 32<math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mo id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><times id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">\times</annotation></semantics></math> training schedule by 2.0% and 2.1% AP on COCO and Objects365, validating the generalization capability of CoDet across image domains and vocabularies.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Visualization and Analysis</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Discovering reliable region-word alignments is critical to learn object-level vision-language representations from image-text pairs. In this section, we investigate different types of alignment strategies that are primarily based on: 1) region-word similarity, <span id="S4.SS5.p1.1.1" class="ltx_ERROR undefined">\ie</span>, assigning words to regions of the highest similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>; 2) hand-crafted prior, <span id="S4.SS5.p1.1.2" class="ltx_ERROR undefined">\ie</span>, assigning words to regions of the maximum size <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite>; and 3) region-region similarity, <span id="S4.SS5.p1.1.3" class="ltx_ERROR undefined">\ie</span>, assigning words to regions of the maximum weight, derived by CoDet from region-region similarity matrix (one may refer to Figure <a href="#S3.F2" title="Figure 2 ‣ Task formulation. ‣ 3.1 Preliminaries ‣ 3 Method ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for clarity).</p>
</div>
<figure id="S4.F3" class="ltx_figure ltx_align_floatleft"><img src="/html/2310.16667/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">A comparison of different alignment strategies on OV-COCO.<span id="S4.F3.4.2.1" class="ltx_text ltx_font_medium"> Cover rate is the ratio of assigned proposal covering the ground-truth box.</span></span></figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ 4.5 Visualization and Analysis ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows a comparison of these strategies. Specifically, we employ the model trained with different strategies on OV-COCO benchmark to generate pseudo-labels for novel categories in COCO validation set. Note that the pseudo-labels are generated with the same strategy as in training, not by prediction results. We evaluate the quality of pseudo-labels by cover rate, which is defined as the ratio of pseudo-labels whose assigned region has a mIoU &gt; 0.5 with the closest ground-truth box. At the end of training, we can see that alignments based on region-region similarity produce much more accurate pseudo labels compared with the other two strategies, manifesting the reliability of visual guidance. Another intriguing finding is that, our method can benefit from self-training which leads to steadily increasing pseudo-label quality, while such pattern is not observed in similarly self-trained model relying on region-word similarity for alignment. We conjecture this is because the model gets stuck in the aforementioned chicken-and-egg problem, which is reflected in its unstable training curve (see Appendix <a href="#A2" title="Appendix B Further Analysis on Different Alignment Strategies ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>). This also aligns with the finding in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite> <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Detic shows that matching regions and words based on highest region-word similarity produces highly inconsistent pseudo labels at different training stages.</span></span></span>. Further qualitative comparisons are presented in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.5 Visualization and Analysis ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2310.16667/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="248" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.5.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.6.2" class="ltx_text" style="font-size:90%;">Visualization of pseudo bounding box labels generated by different region-word alignment strategies. From top to bottom, each row shows results of strategies based on <span id="S4.F4.6.2.1" class="ltx_text" style="color:#7EAC55;">region-region similarity</span>, <span id="S4.F4.6.2.2" class="ltx_text" style="color:#F5C243;">region-word similarity</span>, and <span id="S4.F4.6.2.3" class="ltx_text" style="color:#4E70BE;">hand-crafted prior</span>, respectively. Zoom in for a better view.</span></figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Ablation Study</h3>

<section id="S4.SS6.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Text-guidance.</h4>

<div id="S4.SS6.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS6.SSS0.Px1.p1.1" class="ltx_p">We ablate the impact of text guidance in estimating inter-region similarity on OV-COCO. As shown in Table <a href="#S4.T4.st2" title="In Table 4 ‣ Heuristic vs. Prototype-based co-occurring object discovery. ‣ 4.6 Ablation Study ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a><span id="S4.SS6.SSS0.Px1.p1.1.1" class="ltx_text" style="color:#FF0000;">a</span>, introducing text guidance leads to a significant performance boost on novel AP. This finding aligns with our intuition that putting region similarity estimation in a semantic context can provide better measurements of semantic closeness. Figure <a href="#S4.F5" title="Figure 5 ‣ Heuristic vs. Prototype-based co-occurring object discovery. ‣ 4.6 Ablation Study ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> further demonstrates how text guidance facilitates mitigating interference from irrelevant concepts.</p>
</div>
</section>
<section id="S4.SS6.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Heuristic vs. Prototype-based co-occurring object discovery.</h4>

<div id="S4.SS6.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS6.SSS0.Px2.p1.1" class="ltx_p">To investigate the effectiveness of prototype-based strategy in co-occurring object discovery, we adopt a heuristic strategy in image co-segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> for comparison. Results are presented in Table <a href="#S4.T4.st2" title="In Table 4 ‣ Heuristic vs. Prototype-based co-occurring object discovery. ‣ 4.6 Ablation Study ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a><span id="S4.SS6.SSS0.Px2.p1.1.1" class="ltx_text" style="color:#FF0000;">b</span>. The heuristic strategy works to select a single region proposal that has close neighbors across support images following hand-crafted rules (please refer to Appendix <a href="#A1" title="Appendix A A Heuristic Baseline for Co-occurrence Discovery ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> for more details). Our prototype-based strategy makes a substantial improvement over this simple baseline by 3.7 AP on novel categories. We speculate the gains mainly come from two aspects: 1) robustness to noisy similarity estimations; We notice that some region proposals in the background may be estimated with high similarity in the early stage, which disturbs the selection by the heuristic strategy. While our prototype-based strategy avoids hard selection by assigning soft weights to each region proposal to construct a prototype, thus is more robust to such noises. 2) the ability to harness multiple instances; considering that there may be multiple instances corresponding to the shared concept in an image, prototype-based strategy can effectively make use of region proposals of different instances to construct a prototype, which we show in Appendix <a href="#A3" title="Appendix C Visualization on OV-LVIS and OV-COCO ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>).</p>
</div>
<figure id="S4.T4" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation study on effective components.<span id="S4.T4.4.2.1" class="ltx_text ltx_font_medium"> We show that both text guidance and prototype-based strategy substantially facilitate co-occurring object discovery.</span></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T4.st1" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.st1.6.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.T4.st1.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Text guidance</span></figcaption>
<table id="S4.T4.st1.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_top">
<thead class="ltx_thead">
<tr id="S4.T4.st1.3.3.3" class="ltx_tr">
<th id="S4.T4.st1.3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st1.3.3.3.4.1" class="ltx_text" style="font-size:80%;">Text guide</span></th>
<th id="S4.T4.st1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><math id="S4.T4.st1.1.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{novel}}" display="inline"><semantics id="S4.T4.st1.1.1.1.1.m1.1a"><msubsup id="S4.T4.st1.1.1.1.1.m1.1.1" xref="S4.T4.st1.1.1.1.1.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T4.st1.1.1.1.1.m1.1.1.2.2" xref="S4.T4.st1.1.1.1.1.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.T4.st1.1.1.1.1.m1.1.1.2.3" xref="S4.T4.st1.1.1.1.1.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.T4.st1.1.1.1.1.m1.1.1.3" xref="S4.T4.st1.1.1.1.1.m1.1.1.3a.cmml">novel</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T4.st1.1.1.1.1.m1.1b"><apply id="S4.T4.st1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.st1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.st1.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S4.T4.st1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T4.st1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st1.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T4.st1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T4.st1.1.1.1.1.m1.1.1.2.2a.cmml" xref="S4.T4.st1.1.1.1.1.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T4.st1.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T4.st1.1.1.1.1.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.T4.st1.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T4.st1.1.1.1.1.m1.1.1.2.3">50</cn></apply><ci id="S4.T4.st1.1.1.1.1.m1.1.1.3a.cmml" xref="S4.T4.st1.1.1.1.1.m1.1.1.3"><mtext mathsize="56%" id="S4.T4.st1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T4.st1.1.1.1.1.m1.1.1.3">novel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.st1.1.1.1.1.m1.1c">\text{AP}_{50}^{\text{novel}}</annotation></semantics></math></th>
<th id="S4.T4.st1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><math id="S4.T4.st1.2.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{base}}" display="inline"><semantics id="S4.T4.st1.2.2.2.2.m1.1a"><msubsup id="S4.T4.st1.2.2.2.2.m1.1.1" xref="S4.T4.st1.2.2.2.2.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T4.st1.2.2.2.2.m1.1.1.2.2" xref="S4.T4.st1.2.2.2.2.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.T4.st1.2.2.2.2.m1.1.1.2.3" xref="S4.T4.st1.2.2.2.2.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.T4.st1.2.2.2.2.m1.1.1.3" xref="S4.T4.st1.2.2.2.2.m1.1.1.3a.cmml">base</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T4.st1.2.2.2.2.m1.1b"><apply id="S4.T4.st1.2.2.2.2.m1.1.1.cmml" xref="S4.T4.st1.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st1.2.2.2.2.m1.1.1.1.cmml" xref="S4.T4.st1.2.2.2.2.m1.1.1">superscript</csymbol><apply id="S4.T4.st1.2.2.2.2.m1.1.1.2.cmml" xref="S4.T4.st1.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st1.2.2.2.2.m1.1.1.2.1.cmml" xref="S4.T4.st1.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T4.st1.2.2.2.2.m1.1.1.2.2a.cmml" xref="S4.T4.st1.2.2.2.2.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T4.st1.2.2.2.2.m1.1.1.2.2.cmml" xref="S4.T4.st1.2.2.2.2.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.T4.st1.2.2.2.2.m1.1.1.2.3.cmml" xref="S4.T4.st1.2.2.2.2.m1.1.1.2.3">50</cn></apply><ci id="S4.T4.st1.2.2.2.2.m1.1.1.3a.cmml" xref="S4.T4.st1.2.2.2.2.m1.1.1.3"><mtext mathsize="56%" id="S4.T4.st1.2.2.2.2.m1.1.1.3.cmml" xref="S4.T4.st1.2.2.2.2.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.st1.2.2.2.2.m1.1c">\text{AP}_{50}^{\text{base}}</annotation></semantics></math></th>
<th id="S4.T4.st1.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><math id="S4.T4.st1.3.3.3.3.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{all}}" display="inline"><semantics id="S4.T4.st1.3.3.3.3.m1.1a"><msubsup id="S4.T4.st1.3.3.3.3.m1.1.1" xref="S4.T4.st1.3.3.3.3.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T4.st1.3.3.3.3.m1.1.1.2.2" xref="S4.T4.st1.3.3.3.3.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.T4.st1.3.3.3.3.m1.1.1.2.3" xref="S4.T4.st1.3.3.3.3.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.T4.st1.3.3.3.3.m1.1.1.3" xref="S4.T4.st1.3.3.3.3.m1.1.1.3a.cmml">all</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T4.st1.3.3.3.3.m1.1b"><apply id="S4.T4.st1.3.3.3.3.m1.1.1.cmml" xref="S4.T4.st1.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st1.3.3.3.3.m1.1.1.1.cmml" xref="S4.T4.st1.3.3.3.3.m1.1.1">superscript</csymbol><apply id="S4.T4.st1.3.3.3.3.m1.1.1.2.cmml" xref="S4.T4.st1.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st1.3.3.3.3.m1.1.1.2.1.cmml" xref="S4.T4.st1.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.T4.st1.3.3.3.3.m1.1.1.2.2a.cmml" xref="S4.T4.st1.3.3.3.3.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T4.st1.3.3.3.3.m1.1.1.2.2.cmml" xref="S4.T4.st1.3.3.3.3.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.T4.st1.3.3.3.3.m1.1.1.2.3.cmml" xref="S4.T4.st1.3.3.3.3.m1.1.1.2.3">50</cn></apply><ci id="S4.T4.st1.3.3.3.3.m1.1.1.3a.cmml" xref="S4.T4.st1.3.3.3.3.m1.1.1.3"><mtext mathsize="56%" id="S4.T4.st1.3.3.3.3.m1.1.1.3.cmml" xref="S4.T4.st1.3.3.3.3.m1.1.1.3">all</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.st1.3.3.3.3.m1.1c">\text{AP}_{50}^{\text{all}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.st1.3.3.4.1" class="ltx_tr">
<th id="S4.T4.st1.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st1.3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">✗</span></th>
<td id="S4.T4.st1.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st1.3.3.4.1.2.1" class="ltx_text" style="font-size:80%;">26.6</span></td>
<td id="S4.T4.st1.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st1.3.3.4.1.3.1" class="ltx_text" style="font-size:80%;">52.4</span></td>
<td id="S4.T4.st1.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st1.3.3.4.1.4.1" class="ltx_text" style="font-size:80%;">45.7</span></td>
</tr>
<tr id="S4.T4.st1.3.3.5.2" class="ltx_tr">
<th id="S4.T4.st1.3.3.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st1.3.3.5.2.1.1" class="ltx_text" style="font-size:80%;">✓</span></th>
<td id="S4.T4.st1.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st1.3.3.5.2.2.1" class="ltx_text" style="font-size:80%;">30.6</span></td>
<td id="S4.T4.st1.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st1.3.3.5.2.3.1" class="ltx_text" style="font-size:80%;">52.3</span></td>
<td id="S4.T4.st1.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st1.3.3.5.2.4.1" class="ltx_text" style="font-size:80%;">46.6</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T4.st2" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.st2.6.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.T4.st2.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Strategy for co-occurring object discovery </span></figcaption>
<table id="S4.T4.st2.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_top">
<thead class="ltx_thead">
<tr id="S4.T4.st2.3.3.3" class="ltx_tr">
<th id="S4.T4.st2.3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st2.3.3.3.4.1" class="ltx_text" style="font-size:80%;">Strategy</span></th>
<th id="S4.T4.st2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><math id="S4.T4.st2.1.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{novel}}" display="inline"><semantics id="S4.T4.st2.1.1.1.1.m1.1a"><msubsup id="S4.T4.st2.1.1.1.1.m1.1.1" xref="S4.T4.st2.1.1.1.1.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T4.st2.1.1.1.1.m1.1.1.2.2" xref="S4.T4.st2.1.1.1.1.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.T4.st2.1.1.1.1.m1.1.1.2.3" xref="S4.T4.st2.1.1.1.1.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.T4.st2.1.1.1.1.m1.1.1.3" xref="S4.T4.st2.1.1.1.1.m1.1.1.3a.cmml">novel</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T4.st2.1.1.1.1.m1.1b"><apply id="S4.T4.st2.1.1.1.1.m1.1.1.cmml" xref="S4.T4.st2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.st2.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S4.T4.st2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T4.st2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st2.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T4.st2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T4.st2.1.1.1.1.m1.1.1.2.2a.cmml" xref="S4.T4.st2.1.1.1.1.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T4.st2.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T4.st2.1.1.1.1.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.T4.st2.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T4.st2.1.1.1.1.m1.1.1.2.3">50</cn></apply><ci id="S4.T4.st2.1.1.1.1.m1.1.1.3a.cmml" xref="S4.T4.st2.1.1.1.1.m1.1.1.3"><mtext mathsize="56%" id="S4.T4.st2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T4.st2.1.1.1.1.m1.1.1.3">novel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.st2.1.1.1.1.m1.1c">\text{AP}_{50}^{\text{novel}}</annotation></semantics></math></th>
<th id="S4.T4.st2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><math id="S4.T4.st2.2.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{base}}" display="inline"><semantics id="S4.T4.st2.2.2.2.2.m1.1a"><msubsup id="S4.T4.st2.2.2.2.2.m1.1.1" xref="S4.T4.st2.2.2.2.2.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T4.st2.2.2.2.2.m1.1.1.2.2" xref="S4.T4.st2.2.2.2.2.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.T4.st2.2.2.2.2.m1.1.1.2.3" xref="S4.T4.st2.2.2.2.2.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.T4.st2.2.2.2.2.m1.1.1.3" xref="S4.T4.st2.2.2.2.2.m1.1.1.3a.cmml">base</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T4.st2.2.2.2.2.m1.1b"><apply id="S4.T4.st2.2.2.2.2.m1.1.1.cmml" xref="S4.T4.st2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T4.st2.2.2.2.2.m1.1.1">superscript</csymbol><apply id="S4.T4.st2.2.2.2.2.m1.1.1.2.cmml" xref="S4.T4.st2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st2.2.2.2.2.m1.1.1.2.1.cmml" xref="S4.T4.st2.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T4.st2.2.2.2.2.m1.1.1.2.2a.cmml" xref="S4.T4.st2.2.2.2.2.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T4.st2.2.2.2.2.m1.1.1.2.2.cmml" xref="S4.T4.st2.2.2.2.2.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.T4.st2.2.2.2.2.m1.1.1.2.3.cmml" xref="S4.T4.st2.2.2.2.2.m1.1.1.2.3">50</cn></apply><ci id="S4.T4.st2.2.2.2.2.m1.1.1.3a.cmml" xref="S4.T4.st2.2.2.2.2.m1.1.1.3"><mtext mathsize="56%" id="S4.T4.st2.2.2.2.2.m1.1.1.3.cmml" xref="S4.T4.st2.2.2.2.2.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.st2.2.2.2.2.m1.1c">\text{AP}_{50}^{\text{base}}</annotation></semantics></math></th>
<th id="S4.T4.st2.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;"><math id="S4.T4.st2.3.3.3.3.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{all}}" display="inline"><semantics id="S4.T4.st2.3.3.3.3.m1.1a"><msubsup id="S4.T4.st2.3.3.3.3.m1.1.1" xref="S4.T4.st2.3.3.3.3.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T4.st2.3.3.3.3.m1.1.1.2.2" xref="S4.T4.st2.3.3.3.3.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.T4.st2.3.3.3.3.m1.1.1.2.3" xref="S4.T4.st2.3.3.3.3.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.T4.st2.3.3.3.3.m1.1.1.3" xref="S4.T4.st2.3.3.3.3.m1.1.1.3a.cmml">all</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T4.st2.3.3.3.3.m1.1b"><apply id="S4.T4.st2.3.3.3.3.m1.1.1.cmml" xref="S4.T4.st2.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st2.3.3.3.3.m1.1.1.1.cmml" xref="S4.T4.st2.3.3.3.3.m1.1.1">superscript</csymbol><apply id="S4.T4.st2.3.3.3.3.m1.1.1.2.cmml" xref="S4.T4.st2.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.st2.3.3.3.3.m1.1.1.2.1.cmml" xref="S4.T4.st2.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.T4.st2.3.3.3.3.m1.1.1.2.2a.cmml" xref="S4.T4.st2.3.3.3.3.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T4.st2.3.3.3.3.m1.1.1.2.2.cmml" xref="S4.T4.st2.3.3.3.3.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.T4.st2.3.3.3.3.m1.1.1.2.3.cmml" xref="S4.T4.st2.3.3.3.3.m1.1.1.2.3">50</cn></apply><ci id="S4.T4.st2.3.3.3.3.m1.1.1.3a.cmml" xref="S4.T4.st2.3.3.3.3.m1.1.1.3"><mtext mathsize="56%" id="S4.T4.st2.3.3.3.3.m1.1.1.3.cmml" xref="S4.T4.st2.3.3.3.3.m1.1.1.3">all</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.st2.3.3.3.3.m1.1c">\text{AP}_{50}^{\text{all}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.st2.3.3.4.1" class="ltx_tr">
<th id="S4.T4.st2.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 8.0pt;">
<span id="S4.T4.st2.3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">Heuristic </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.st2.3.3.4.1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a><span id="S4.T4.st2.3.3.4.1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T4.st2.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st2.3.3.4.1.2.1" class="ltx_text" style="font-size:80%;">26.9</span></td>
<td id="S4.T4.st2.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st2.3.3.4.1.3.1" class="ltx_text" style="font-size:80%;">52.4</span></td>
<td id="S4.T4.st2.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st2.3.3.4.1.4.1" class="ltx_text" style="font-size:80%;">45.7</span></td>
</tr>
<tr id="S4.T4.st2.3.3.5.2" class="ltx_tr">
<th id="S4.T4.st2.3.3.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st2.3.3.5.2.1.1" class="ltx_text" style="font-size:80%;">Prototype-based</span></th>
<td id="S4.T4.st2.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st2.3.3.5.2.2.1" class="ltx_text" style="font-size:80%;">30.6</span></td>
<td id="S4.T4.st2.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st2.3.3.5.2.3.1" class="ltx_text" style="font-size:80%;">52.3</span></td>
<td id="S4.T4.st2.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T4.st2.3.3.5.2.4.1" class="ltx_text" style="font-size:80%;">46.6</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2310.16667/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">There can be more than one co-occurring concept among sampled images. Text guidance helps filter out the distracting concept (chair legs) and focus on the concept of interest (pigeons).</span></figcaption>
</figure>
</section>
<section id="S4.SS6.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Size of concept group.</h4>

<div id="S4.SS6.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS6.SSS0.Px3.p1.1" class="ltx_p">In CoDet, co-occurring object discovery is based on a mini-group of images sampled from the same concept group during training. Since the size of the mini-group is generally small, sometimes there could be more than one co-occurring concept in a group, in which case the model will be confused about which concept to discover, as illustrated in Figure <a href="#S4.F5" title="Figure 5 ‣ Heuristic vs. Prototype-based co-occurring object discovery. ‣ 4.6 Ablation Study ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Besides introducing text guidance, intuitively, increasing the group size can also effectively reduce this ambiguity, as verified by results on OV-LVIS in Table <a href="#S4.T5" title="Table 5 ‣ Size of concept group. ‣ 4.6 Ablation Study ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. However, contrary results are observed in experiments on OV-COCO. We speculate these abnormal results are probably caused by the aforementioned human-curated bias in COCO Caption (See Sec. <a href="#S4.SS3" title="4.3 Benchmark Results ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>). Due to the highly concentrated concepts, increasing the group size will undesirably introduce more concurrent concepts that harm the model performances.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.10.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.11.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation study on concept group size.<span id="S4.T5.11.2.1" class="ltx_text ltx_font_medium"> CoDet shows different preferences of concept group size on human-curated caption data (OV-COCO) and web-crawled image-text pairs (OV-LVIS).</span></span></figcaption>
<table id="S4.T5.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.7.8.1" class="ltx_tr">
<th id="S4.T5.7.8.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:0.4pt 8.0pt;" rowspan="2"><span id="S4.T5.7.8.1.1.1" class="ltx_text" style="font-size:80%;">Group Size</span></th>
<th id="S4.T5.7.8.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;" colspan="3"><span id="S4.T5.7.8.1.2.1" class="ltx_text" style="font-size:80%;">OV-COCO</span></th>
<th id="S4.T5.7.8.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.4pt 8.0pt;" colspan="4"><span id="S4.T5.7.8.1.3.1" class="ltx_text" style="font-size:80%;">OV-LVIS</span></th>
</tr>
<tr id="S4.T5.7.7" class="ltx_tr">
<th id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.4pt 8.0pt;"><math id="S4.T5.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{novel}}" display="inline"><semantics id="S4.T5.1.1.1.m1.1a"><msubsup id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T5.1.1.1.m1.1.1.2.2" xref="S4.T5.1.1.1.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.T5.1.1.1.m1.1.1.2.3" xref="S4.T5.1.1.1.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.T5.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.m1.1.1.3a.cmml">novel</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">superscript</csymbol><apply id="S4.T5.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.1.1.1.m1.1.1.2.1.cmml" xref="S4.T5.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T5.1.1.1.m1.1.1.2.2a.cmml" xref="S4.T5.1.1.1.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T5.1.1.1.m1.1.1.2.2.cmml" xref="S4.T5.1.1.1.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.T5.1.1.1.m1.1.1.2.3.cmml" xref="S4.T5.1.1.1.m1.1.1.2.3">50</cn></apply><ci id="S4.T5.1.1.1.m1.1.1.3a.cmml" xref="S4.T5.1.1.1.m1.1.1.3"><mtext mathsize="56%" id="S4.T5.1.1.1.m1.1.1.3.cmml" xref="S4.T5.1.1.1.m1.1.1.3">novel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\text{AP}_{50}^{\text{novel}}</annotation></semantics></math></th>
<th id="S4.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.4pt 8.0pt;"><math id="S4.T5.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{base}}" display="inline"><semantics id="S4.T5.2.2.2.m1.1a"><msubsup id="S4.T5.2.2.2.m1.1.1" xref="S4.T5.2.2.2.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T5.2.2.2.m1.1.1.2.2" xref="S4.T5.2.2.2.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.T5.2.2.2.m1.1.1.2.3" xref="S4.T5.2.2.2.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.T5.2.2.2.m1.1.1.3" xref="S4.T5.2.2.2.m1.1.1.3a.cmml">base</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.m1.1b"><apply id="S4.T5.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.2.2.2.m1.1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1">superscript</csymbol><apply id="S4.T5.2.2.2.m1.1.1.2.cmml" xref="S4.T5.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.2.2.2.m1.1.1.2.1.cmml" xref="S4.T5.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T5.2.2.2.m1.1.1.2.2a.cmml" xref="S4.T5.2.2.2.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T5.2.2.2.m1.1.1.2.2.cmml" xref="S4.T5.2.2.2.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.T5.2.2.2.m1.1.1.2.3.cmml" xref="S4.T5.2.2.2.m1.1.1.2.3">50</cn></apply><ci id="S4.T5.2.2.2.m1.1.1.3a.cmml" xref="S4.T5.2.2.2.m1.1.1.3"><mtext mathsize="56%" id="S4.T5.2.2.2.m1.1.1.3.cmml" xref="S4.T5.2.2.2.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.m1.1c">\text{AP}_{50}^{\text{base}}</annotation></semantics></math></th>
<th id="S4.T5.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.4pt 8.0pt;"><math id="S4.T5.3.3.3.m1.1" class="ltx_Math" alttext="\text{AP}_{50}^{\text{all}}" display="inline"><semantics id="S4.T5.3.3.3.m1.1a"><msubsup id="S4.T5.3.3.3.m1.1.1" xref="S4.T5.3.3.3.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T5.3.3.3.m1.1.1.2.2" xref="S4.T5.3.3.3.m1.1.1.2.2a.cmml">AP</mtext><mn mathsize="80%" id="S4.T5.3.3.3.m1.1.1.2.3" xref="S4.T5.3.3.3.m1.1.1.2.3.cmml">50</mn><mtext mathsize="80%" id="S4.T5.3.3.3.m1.1.1.3" xref="S4.T5.3.3.3.m1.1.1.3a.cmml">all</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.m1.1b"><apply id="S4.T5.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.3.3.3.m1.1.1.1.cmml" xref="S4.T5.3.3.3.m1.1.1">superscript</csymbol><apply id="S4.T5.3.3.3.m1.1.1.2.cmml" xref="S4.T5.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.3.3.3.m1.1.1.2.1.cmml" xref="S4.T5.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.T5.3.3.3.m1.1.1.2.2a.cmml" xref="S4.T5.3.3.3.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T5.3.3.3.m1.1.1.2.2.cmml" xref="S4.T5.3.3.3.m1.1.1.2.2">AP</mtext></ci><cn type="integer" id="S4.T5.3.3.3.m1.1.1.2.3.cmml" xref="S4.T5.3.3.3.m1.1.1.2.3">50</cn></apply><ci id="S4.T5.3.3.3.m1.1.1.3a.cmml" xref="S4.T5.3.3.3.m1.1.1.3"><mtext mathsize="56%" id="S4.T5.3.3.3.m1.1.1.3.cmml" xref="S4.T5.3.3.3.m1.1.1.3">all</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.m1.1c">\text{AP}_{50}^{\text{all}}</annotation></semantics></math></th>
<th id="S4.T5.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.4pt 8.0pt;"><math id="S4.T5.4.4.4.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{novel}}^{\text{m}}" display="inline"><semantics id="S4.T5.4.4.4.m1.1a"><msubsup id="S4.T5.4.4.4.m1.1.1" xref="S4.T5.4.4.4.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T5.4.4.4.m1.1.1.2.2" xref="S4.T5.4.4.4.m1.1.1.2.2a.cmml">AP</mtext><mtext mathsize="80%" id="S4.T5.4.4.4.m1.1.1.2.3" xref="S4.T5.4.4.4.m1.1.1.2.3a.cmml">novel</mtext><mtext mathsize="80%" id="S4.T5.4.4.4.m1.1.1.3" xref="S4.T5.4.4.4.m1.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.4.m1.1b"><apply id="S4.T5.4.4.4.m1.1.1.cmml" xref="S4.T5.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.4.4.4.m1.1.1.1.cmml" xref="S4.T5.4.4.4.m1.1.1">superscript</csymbol><apply id="S4.T5.4.4.4.m1.1.1.2.cmml" xref="S4.T5.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.4.4.4.m1.1.1.2.1.cmml" xref="S4.T5.4.4.4.m1.1.1">subscript</csymbol><ci id="S4.T5.4.4.4.m1.1.1.2.2a.cmml" xref="S4.T5.4.4.4.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T5.4.4.4.m1.1.1.2.2.cmml" xref="S4.T5.4.4.4.m1.1.1.2.2">AP</mtext></ci><ci id="S4.T5.4.4.4.m1.1.1.2.3a.cmml" xref="S4.T5.4.4.4.m1.1.1.2.3"><mtext mathsize="56%" id="S4.T5.4.4.4.m1.1.1.2.3.cmml" xref="S4.T5.4.4.4.m1.1.1.2.3">novel</mtext></ci></apply><ci id="S4.T5.4.4.4.m1.1.1.3a.cmml" xref="S4.T5.4.4.4.m1.1.1.3"><mtext mathsize="56%" id="S4.T5.4.4.4.m1.1.1.3.cmml" xref="S4.T5.4.4.4.m1.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.4.m1.1c">\text{AP}_{\text{novel}}^{\text{m}}</annotation></semantics></math></th>
<th id="S4.T5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.4pt 8.0pt;"><math id="S4.T5.5.5.5.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{c}}^{\text{m}}" display="inline"><semantics id="S4.T5.5.5.5.m1.1a"><msubsup id="S4.T5.5.5.5.m1.1.1" xref="S4.T5.5.5.5.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T5.5.5.5.m1.1.1.2.2" xref="S4.T5.5.5.5.m1.1.1.2.2a.cmml">AP</mtext><mtext mathsize="80%" id="S4.T5.5.5.5.m1.1.1.2.3" xref="S4.T5.5.5.5.m1.1.1.2.3a.cmml">c</mtext><mtext mathsize="80%" id="S4.T5.5.5.5.m1.1.1.3" xref="S4.T5.5.5.5.m1.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.5.m1.1b"><apply id="S4.T5.5.5.5.m1.1.1.cmml" xref="S4.T5.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.5.5.5.m1.1.1.1.cmml" xref="S4.T5.5.5.5.m1.1.1">superscript</csymbol><apply id="S4.T5.5.5.5.m1.1.1.2.cmml" xref="S4.T5.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.5.5.5.m1.1.1.2.1.cmml" xref="S4.T5.5.5.5.m1.1.1">subscript</csymbol><ci id="S4.T5.5.5.5.m1.1.1.2.2a.cmml" xref="S4.T5.5.5.5.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T5.5.5.5.m1.1.1.2.2.cmml" xref="S4.T5.5.5.5.m1.1.1.2.2">AP</mtext></ci><ci id="S4.T5.5.5.5.m1.1.1.2.3a.cmml" xref="S4.T5.5.5.5.m1.1.1.2.3"><mtext mathsize="56%" id="S4.T5.5.5.5.m1.1.1.2.3.cmml" xref="S4.T5.5.5.5.m1.1.1.2.3">c</mtext></ci></apply><ci id="S4.T5.5.5.5.m1.1.1.3a.cmml" xref="S4.T5.5.5.5.m1.1.1.3"><mtext mathsize="56%" id="S4.T5.5.5.5.m1.1.1.3.cmml" xref="S4.T5.5.5.5.m1.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.5.m1.1c">\text{AP}_{\text{c}}^{\text{m}}</annotation></semantics></math></th>
<th id="S4.T5.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.4pt 8.0pt;"><math id="S4.T5.6.6.6.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{f}}^{\text{m}}" display="inline"><semantics id="S4.T5.6.6.6.m1.1a"><msubsup id="S4.T5.6.6.6.m1.1.1" xref="S4.T5.6.6.6.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T5.6.6.6.m1.1.1.2.2" xref="S4.T5.6.6.6.m1.1.1.2.2a.cmml">AP</mtext><mtext mathsize="80%" id="S4.T5.6.6.6.m1.1.1.2.3" xref="S4.T5.6.6.6.m1.1.1.2.3a.cmml">f</mtext><mtext mathsize="80%" id="S4.T5.6.6.6.m1.1.1.3" xref="S4.T5.6.6.6.m1.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.6.m1.1b"><apply id="S4.T5.6.6.6.m1.1.1.cmml" xref="S4.T5.6.6.6.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.6.6.6.m1.1.1.1.cmml" xref="S4.T5.6.6.6.m1.1.1">superscript</csymbol><apply id="S4.T5.6.6.6.m1.1.1.2.cmml" xref="S4.T5.6.6.6.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.6.6.6.m1.1.1.2.1.cmml" xref="S4.T5.6.6.6.m1.1.1">subscript</csymbol><ci id="S4.T5.6.6.6.m1.1.1.2.2a.cmml" xref="S4.T5.6.6.6.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T5.6.6.6.m1.1.1.2.2.cmml" xref="S4.T5.6.6.6.m1.1.1.2.2">AP</mtext></ci><ci id="S4.T5.6.6.6.m1.1.1.2.3a.cmml" xref="S4.T5.6.6.6.m1.1.1.2.3"><mtext mathsize="56%" id="S4.T5.6.6.6.m1.1.1.2.3.cmml" xref="S4.T5.6.6.6.m1.1.1.2.3">f</mtext></ci></apply><ci id="S4.T5.6.6.6.m1.1.1.3a.cmml" xref="S4.T5.6.6.6.m1.1.1.3"><mtext mathsize="56%" id="S4.T5.6.6.6.m1.1.1.3.cmml" xref="S4.T5.6.6.6.m1.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.6.m1.1c">\text{AP}_{\text{f}}^{\text{m}}</annotation></semantics></math></th>
<th id="S4.T5.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.4pt 8.0pt;"><math id="S4.T5.7.7.7.m1.1" class="ltx_Math" alttext="\text{AP}_{\text{all}}^{\text{m}}" display="inline"><semantics id="S4.T5.7.7.7.m1.1a"><msubsup id="S4.T5.7.7.7.m1.1.1" xref="S4.T5.7.7.7.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T5.7.7.7.m1.1.1.2.2" xref="S4.T5.7.7.7.m1.1.1.2.2a.cmml">AP</mtext><mtext mathsize="80%" id="S4.T5.7.7.7.m1.1.1.2.3" xref="S4.T5.7.7.7.m1.1.1.2.3a.cmml">all</mtext><mtext mathsize="80%" id="S4.T5.7.7.7.m1.1.1.3" xref="S4.T5.7.7.7.m1.1.1.3a.cmml">m</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.T5.7.7.7.m1.1b"><apply id="S4.T5.7.7.7.m1.1.1.cmml" xref="S4.T5.7.7.7.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.7.7.7.m1.1.1.1.cmml" xref="S4.T5.7.7.7.m1.1.1">superscript</csymbol><apply id="S4.T5.7.7.7.m1.1.1.2.cmml" xref="S4.T5.7.7.7.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.7.7.7.m1.1.1.2.1.cmml" xref="S4.T5.7.7.7.m1.1.1">subscript</csymbol><ci id="S4.T5.7.7.7.m1.1.1.2.2a.cmml" xref="S4.T5.7.7.7.m1.1.1.2.2"><mtext mathsize="80%" id="S4.T5.7.7.7.m1.1.1.2.2.cmml" xref="S4.T5.7.7.7.m1.1.1.2.2">AP</mtext></ci><ci id="S4.T5.7.7.7.m1.1.1.2.3a.cmml" xref="S4.T5.7.7.7.m1.1.1.2.3"><mtext mathsize="56%" id="S4.T5.7.7.7.m1.1.1.2.3.cmml" xref="S4.T5.7.7.7.m1.1.1.2.3">all</mtext></ci></apply><ci id="S4.T5.7.7.7.m1.1.1.3a.cmml" xref="S4.T5.7.7.7.m1.1.1.3"><mtext mathsize="56%" id="S4.T5.7.7.7.m1.1.1.3.cmml" xref="S4.T5.7.7.7.m1.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.7.7.m1.1c">\text{AP}_{\text{all}}^{\text{m}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.7.9.1" class="ltx_tr">
<th id="S4.T5.7.9.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.9.1.1.1" class="ltx_text" style="font-size:80%;">2</span></th>
<td id="S4.T5.7.9.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.9.1.2.1" class="ltx_text" style="font-size:80%;">30.6</span></td>
<td id="S4.T5.7.9.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.9.1.3.1" class="ltx_text" style="font-size:80%;">52.3</span></td>
<td id="S4.T5.7.9.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.9.1.4.1" class="ltx_text" style="font-size:80%;">46.6</span></td>
<td id="S4.T5.7.9.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.9.1.5.1" class="ltx_text" style="font-size:80%;">21.9</span></td>
<td id="S4.T5.7.9.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.9.1.6.1" class="ltx_text" style="font-size:80%;">30.3</span></td>
<td id="S4.T5.7.9.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.9.1.7.1" class="ltx_text" style="font-size:80%;">35.0</span></td>
<td id="S4.T5.7.9.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.9.1.8.1" class="ltx_text" style="font-size:80%;">30.7</span></td>
</tr>
<tr id="S4.T5.7.10.2" class="ltx_tr">
<th id="S4.T5.7.10.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.10.2.1.1" class="ltx_text" style="font-size:80%;">4</span></th>
<td id="S4.T5.7.10.2.2" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.10.2.2.1" class="ltx_text" style="font-size:80%;">29.9</span></td>
<td id="S4.T5.7.10.2.3" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.10.2.3.1" class="ltx_text" style="font-size:80%;">51.2</span></td>
<td id="S4.T5.7.10.2.4" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.10.2.4.1" class="ltx_text" style="font-size:80%;">45.6</span></td>
<td id="S4.T5.7.10.2.5" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.10.2.5.1" class="ltx_text" style="font-size:80%;">21.8</span></td>
<td id="S4.T5.7.10.2.6" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.10.2.6.1" class="ltx_text" style="font-size:80%;">30.2</span></td>
<td id="S4.T5.7.10.2.7" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.10.2.7.1" class="ltx_text" style="font-size:80%;">34.9</span></td>
<td id="S4.T5.7.10.2.8" class="ltx_td ltx_align_center" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.10.2.8.1" class="ltx_text" style="font-size:80%;">30.6</span></td>
</tr>
<tr id="S4.T5.7.11.3" class="ltx_tr">
<th id="S4.T5.7.11.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.11.3.1.1" class="ltx_text" style="font-size:80%;">8</span></th>
<td id="S4.T5.7.11.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.11.3.2.1" class="ltx_text" style="font-size:80%;">29.1</span></td>
<td id="S4.T5.7.11.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.11.3.3.1" class="ltx_text" style="font-size:80%;">50.9</span></td>
<td id="S4.T5.7.11.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.11.3.4.1" class="ltx_text" style="font-size:80%;">45.2</span></td>
<td id="S4.T5.7.11.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.11.3.5.1" class="ltx_text" style="font-size:80%;">22.7</span></td>
<td id="S4.T5.7.11.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.11.3.6.1" class="ltx_text" style="font-size:80%;">30.3</span></td>
<td id="S4.T5.7.11.3.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.11.3.7.1" class="ltx_text" style="font-size:80%;">34.7</span></td>
<td id="S4.T5.7.11.3.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.4pt 8.0pt;"><span id="S4.T5.7.11.3.8.1" class="ltx_text" style="font-size:80%;">30.7</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations and Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we make the first attempt to explore visual clues, <span id="S5.p1.1.1" class="ltx_ERROR undefined">\ie</span>, object co-occurrence, to discover region-word alignments for open-vocabulary object detection. We present CoDet, which effectively leverages cross-image region correspondences and text guidance to discover co-occurring objects for alignment, achieving state-of-the-art results on various OVD benchmarks. On the other hand, our method is orthogonal to previous efforts in aligning regions and words with VLMs. Combining the advantages of both sides is a promising direction of research but is under-explored here. We leave this for further investigation.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Acknowledgements</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">This work has been supported by Hong Kong Research Grant Council - Early Career Scheme (Grant No. 27209621), General Research Fund Scheme (Grant No. 17202422), RGC Theme-based research (T45-701/22-R) and RGC Matching Fund Scheme (RMGS). Part of the described research work is conducted in the JC STEM Lab of Robotics for Soft Materials funded by The Hong Kong Jockey Club Charities Trust.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.4.1" class="ltx_text" style="font-size:90%;">
Yutong Bai, Xinlei Chen, Alexander Kirillov, Alan Yuille, and Alexander C Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.5.1" class="ltx_text" style="font-size:90%;">Point-level region contrast for object detection pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib1.8.3" class="ltx_text" style="font-size:90%;">, pages 16061–16070, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.2.2.1" class="ltx_text" style="font-size:90%;">[2]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="font-size:90%;">
Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, and Ajay Divakaran.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.5.1" class="ltx_text" style="font-size:90%;">Zero-shot object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib2.8.3" class="ltx_text" style="font-size:90%;">, pages 384–400, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.2.2.1" class="ltx_text" style="font-size:90%;">[3]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="font-size:90%;">
Hakan Bilen, Marco Pedersoli, and Tinne Tuytelaars.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.5.1" class="ltx_text" style="font-size:90%;">Weakly supervised object detection with convex clustering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib3.8.3" class="ltx_text" style="font-size:90%;">, pages 1081–1089, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.2.2.1" class="ltx_text" style="font-size:90%;">[4]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="font-size:90%;">
Hakan Bilen and Andrea Vedaldi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.5.1" class="ltx_text" style="font-size:90%;">Weakly supervised deep detection networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib4.8.3" class="ltx_text" style="font-size:90%;">, pages 2846–2854, 2016.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.2.2.1" class="ltx_text" style="font-size:90%;">[5]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.5.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16</span><span id="bib.bib5.8.3" class="ltx_text" style="font-size:90%;">, pages 213–229.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.2.2.1" class="ltx_text" style="font-size:90%;">[6]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">
Peixian Chen, Kekai Sheng, Mengdan Zhang, Yunhang Shen, Ke Li, and Chunhua
Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.5.1" class="ltx_text" style="font-size:90%;">Open vocabulary object detection with proposal mining and prediction
equalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.11134</span><span id="bib.bib6.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.2.2.1" class="ltx_text" style="font-size:90%;">[7]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="font-size:90%;">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.5.1" class="ltx_text" style="font-size:90%;">Microsoft coco captions: Data collection and evaluation server.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1504.00325</span><span id="bib.bib7.7.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.2.2.1" class="ltx_text" style="font-size:90%;">[8]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="font-size:90%;">
Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.5.1" class="ltx_text" style="font-size:90%;">Picie: Unsupervised semantic segmentation using invariance and
equivariance in clustering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib8.8.3" class="ltx_text" style="font-size:90%;">, pages 16794–16804, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.2.2.1" class="ltx_text" style="font-size:90%;">[9]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.4.1" class="ltx_text" style="font-size:90%;">
Ramazan Gokberk Cinbis, Jakob Verbeek, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.5.1" class="ltx_text" style="font-size:90%;">Weakly supervised object localization with multi-fold multiple
instance learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib9.7.2" class="ltx_text" style="font-size:90%;">,
39(1):189–203, 2016.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.2.2.1" class="ltx_text" style="font-size:90%;">[10]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.4.1" class="ltx_text" style="font-size:90%;">
Berkan Demirel, Ramazan Gokberk Cinbis, and Nazli Ikizler-Cinbis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.5.1" class="ltx_text" style="font-size:90%;">Zero-shot object detection by hybrid region embedding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">British Machine Vision Conference (BMVC)</span><span id="bib.bib10.8.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.2.2.1" class="ltx_text" style="font-size:90%;">[11]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.4.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.5.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib11.8.3" class="ltx_text" style="font-size:90%;">, pages 248–255. IEEE, 2009.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.2.2.1" class="ltx_text" style="font-size:90%;">[12]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="font-size:90%;">
Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.5.1" class="ltx_text" style="font-size:90%;">Learning to prompt for open-vocabulary object detection with
vision-language model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib12.8.3" class="ltx_text" style="font-size:90%;">, pages 14084–14093, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.2.2.1" class="ltx_text" style="font-size:90%;">[13]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="font-size:90%;">
Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.5.1" class="ltx_text" style="font-size:90%;">Eva-02: A visual representation for neon genesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.11331</span><span id="bib.bib13.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.2.2.1" class="ltx_text" style="font-size:90%;">[14]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="font-size:90%;">
Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu, Haibing Ren, Xiaolin
Wei, Weidi Xie, and Lin Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.5.1" class="ltx_text" style="font-size:90%;">Promptdet: Towards open-vocabulary detection using uncurated images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision – ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part IX</span><span id="bib.bib14.8.3" class="ltx_text" style="font-size:90%;">, page 701–717,
2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.2.2.1" class="ltx_text" style="font-size:90%;">[15]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.4.1" class="ltx_text" style="font-size:90%;">
Mingfei Gao, Chen Xing, Juan Carlos Niebles, Junnan Li, Ran Xu, Wenhao Liu, and
Caiming Xiong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.5.1" class="ltx_text" style="font-size:90%;">Open vocabulary object detection with pseudo bounding-box labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part X</span><span id="bib.bib15.8.3" class="ltx_text" style="font-size:90%;">, pages 266–282.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.2.2.1" class="ltx_text" style="font-size:90%;">[16]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="font-size:90%;">
Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk,
Quoc V Le, and Barret Zoph.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.5.1" class="ltx_text" style="font-size:90%;">Simple copy-paste is a strong data augmentation method for instance
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib16.8.3" class="ltx_text" style="font-size:90%;">, pages 2918–2928, 2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.2.2.1" class="ltx_text" style="font-size:90%;">[17]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.4.1" class="ltx_text" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.5.1" class="ltx_text" style="font-size:90%;">Generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Communications of the ACM</span><span id="bib.bib17.7.2" class="ltx_text" style="font-size:90%;">, 63(11):139–144, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.2.2.1" class="ltx_text" style="font-size:90%;">[18]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="font-size:90%;">
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.5.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection via vision and language knowledge
distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib18.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.2.2.1" class="ltx_text" style="font-size:90%;">[19]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">
Agrim Gupta, Piotr Dollar, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.5.1" class="ltx_text" style="font-size:90%;">Lvis: A dataset for large vocabulary instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib19.8.3" class="ltx_text" style="font-size:90%;">, pages 5356–5364, 2019.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.2.2.1" class="ltx_text" style="font-size:90%;">[20]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="font-size:90%;">
Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T.
Freeman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.5.1" class="ltx_text" style="font-size:90%;">Unsupervised semantic segmentation by distilling feature
correspondences.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib20.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.2.2.1" class="ltx_text" style="font-size:90%;">[21]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.5.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib21.8.3" class="ltx_text" style="font-size:90%;">, pages 2961–2969, 2017.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.2.2.1" class="ltx_text" style="font-size:90%;">[22]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.5.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib22.8.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.2.2.1" class="ltx_text" style="font-size:90%;">[23]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.4.1" class="ltx_text" style="font-size:90%;">
Olivier J Hénaff, Skanda Koppula, Evan Shelhamer, Daniel Zoran, Andrew
Jaegle, Andrew Zisserman, João Carreira, and Relja Arandjelović.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.5.1" class="ltx_text" style="font-size:90%;">Object discovery and representation networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVII</span><span id="bib.bib23.8.3" class="ltx_text" style="font-size:90%;">, pages 123–143,
2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.2.2.1" class="ltx_text" style="font-size:90%;">[24]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="font-size:90%;">
Hanzhe Hu, Jinshi Cui, and Liwei Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.5.1" class="ltx_text" style="font-size:90%;">Region-aware contrastive learning for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib24.8.3" class="ltx_text" style="font-size:90%;">, pages 16291–16301, 2021.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.2.2.1" class="ltx_text" style="font-size:90%;">[25]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="font-size:90%;">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.5.1" class="ltx_text" style="font-size:90%;">Scaling up visual and vision-language representation learning with
noisy text supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib25.8.3" class="ltx_text" style="font-size:90%;">, pages
4904–4916. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.2.2.1" class="ltx_text" style="font-size:90%;">[26]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="font-size:90%;">
Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.5.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection upon frozen vision and language
models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The Eleventh International Conference on Learning
Representations</span><span id="bib.bib26.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.2.2.1" class="ltx_text" style="font-size:90%;">[27]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.4.1" class="ltx_text" style="font-size:90%;">
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li,
Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.5.1" class="ltx_text" style="font-size:90%;">Grounded language-image pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib27.8.3" class="ltx_text" style="font-size:90%;">, pages 10965–10975, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.2.2.1" class="ltx_text" style="font-size:90%;">[28]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text" style="font-size:90%;">
Chuang Lin, Peize Sun, Yi Jiang, Ping Luo, Lizhen Qu, Gholamreza Haffari,
Zehuan Yuan, and Jianfei Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.5.1" class="ltx_text" style="font-size:90%;">Learning object-language alignments for open-vocabulary object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The Eleventh International Conference on Learning
Representations</span><span id="bib.bib28.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.2.2.1" class="ltx_text" style="font-size:90%;">[29]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.5.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part V 13</span><span id="bib.bib29.8.3" class="ltx_text" style="font-size:90%;">, pages 740–755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.2.2.1" class="ltx_text" style="font-size:90%;">[30]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.4.1" class="ltx_text" style="font-size:90%;">
Weide Liu, Chi Zhang, Guosheng Lin, and Fayao Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.5.1" class="ltx_text" style="font-size:90%;">Crnet: Cross-reference networks for few-shot segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib30.8.3" class="ltx_text" style="font-size:90%;">, pages 4165–4173, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.2.2.1" class="ltx_text" style="font-size:90%;">[31]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.5.1" class="ltx_text" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted
windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib31.8.3" class="ltx_text" style="font-size:90%;">, pages 10012–10022, 2021.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.2.2.1" class="ltx_text" style="font-size:90%;">[32]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="font-size:90%;">
George A Miller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.5.1" class="ltx_text" style="font-size:90%;">Wordnet: a lexical database for english.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Communications of the ACM</span><span id="bib.bib32.7.2" class="ltx_text" style="font-size:90%;">, 38(11):39–41, 1995.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.2.2.1" class="ltx_text" style="font-size:90%;">[33]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.4.1" class="ltx_text" style="font-size:90%;">
Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk
Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa
Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil
Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.5.1" class="ltx_text" style="font-size:90%;">Simple open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision – ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part X</span><span id="bib.bib33.8.3" class="ltx_text" style="font-size:90%;">, page 728–755,
2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.2.2.1" class="ltx_text" style="font-size:90%;">[34]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="font-size:90%;">
Jeffrey Pennington, Richard Socher, and Christopher D Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.5.1" class="ltx_text" style="font-size:90%;">Glove: Global vectors for word representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</span><span id="bib.bib34.8.3" class="ltx_text" style="font-size:90%;">, pages 1532–1543, 2014.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.2.2.1" class="ltx_text" style="font-size:90%;">[35]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.5.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib35.8.3" class="ltx_text" style="font-size:90%;">, pages
8748–8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.2.2.1" class="ltx_text" style="font-size:90%;">[36]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.4.1" class="ltx_text" style="font-size:90%;">
Shafin Rahman, Salman Khan, and Nick Barnes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.5.1" class="ltx_text" style="font-size:90%;">Transductive learning for zero-shot object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib36.8.3" class="ltx_text" style="font-size:90%;">, pages 6082–6091, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.2.2.1" class="ltx_text" style="font-size:90%;">[37]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.4.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.5.1" class="ltx_text" style="font-size:90%;">You only look once: Unified, real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib37.8.3" class="ltx_text" style="font-size:90%;">, pages 779–788, 2016.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.2.2.1" class="ltx_text" style="font-size:90%;">[38]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.4.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.5.1" class="ltx_text" style="font-size:90%;">Yolo9000: better, faster, stronger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib38.8.3" class="ltx_text" style="font-size:90%;">, pages 7263–7271, 2017.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.2.2.1" class="ltx_text" style="font-size:90%;">[39]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.4.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.5.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib39.7.2" class="ltx_text" style="font-size:90%;">, 28, 2015.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.2.2.1" class="ltx_text" style="font-size:90%;">[40]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.4.1" class="ltx_text" style="font-size:90%;">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.5.1" class="ltx_text" style="font-size:90%;">Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span><span id="bib.bib40.8.3" class="ltx_text" style="font-size:90%;">, pages 2556–2565, 2018.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.2.2.1" class="ltx_text" style="font-size:90%;">[41]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.4.1" class="ltx_text" style="font-size:90%;">
Gyungin Shin, Weidi Xie, and Samuel Albanie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.5.1" class="ltx_text" style="font-size:90%;">Reco: Retrieve and co-segment for zero-shot transfer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib41.8.3" class="ltx_text" style="font-size:90%;">,
volume 35, pages 33754–33767, 2022.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.2.2.1" class="ltx_text" style="font-size:90%;">[42]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.4.1" class="ltx_text" style="font-size:90%;">
Zhao Shizhen, Gao Changxin, Shao Yuanjie, Li Lerenhan, Yu Changqian, Ji Zhong,
and Sang Nong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.5.1" class="ltx_text" style="font-size:90%;">Gtnet: Generative transfer network for zero-shot object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI)</span><span id="bib.bib42.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.2.2.1" class="ltx_text" style="font-size:90%;">[43]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.4.1" class="ltx_text" style="font-size:90%;">
Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi
Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.5.1" class="ltx_text" style="font-size:90%;">Sparse r-cnn: End-to-end object detection with learnable proposals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib43.8.3" class="ltx_text" style="font-size:90%;">, pages 14454–14463, 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.2.2.1" class="ltx_text" style="font-size:90%;">[44]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.4.1" class="ltx_text" style="font-size:90%;">
Sara Vicente, Vladimir Kolmogorov, and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.5.1" class="ltx_text" style="font-size:90%;">Cosegmentation revisited: Models and optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2010: 11th European Conference on
Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings,
Part II 11</span><span id="bib.bib44.8.3" class="ltx_text" style="font-size:90%;">, pages 465–479. Springer, 2010.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.2.2.1" class="ltx_text" style="font-size:90%;">[45]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.4.1" class="ltx_text" style="font-size:90%;">
Sara Vicente, Carsten Rother, and Vladimir Kolmogorov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.5.1" class="ltx_text" style="font-size:90%;">Object cosegmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR 2011</span><span id="bib.bib45.8.3" class="ltx_text" style="font-size:90%;">, pages 2217–2224. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.2.2.1" class="ltx_text" style="font-size:90%;">[46]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.4.1" class="ltx_text" style="font-size:90%;">
Fang Wan, Chang Liu, Wei Ke, Xiangyang Ji, Jianbin Jiao, and Qixiang Ye.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.5.1" class="ltx_text" style="font-size:90%;">C-mil: Continuation multiple instance learning for weakly supervised
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib46.8.3" class="ltx_text" style="font-size:90%;">, pages 2199–2208, 2019.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.2.2.1" class="ltx_text" style="font-size:90%;">[47]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.4.1" class="ltx_text" style="font-size:90%;">
Wenguan Wang, Tianfei Zhou, Fisher Yu, Jifeng Dai, Ender Konukoglu, and Luc
Van Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.5.1" class="ltx_text" style="font-size:90%;">Exploring cross-image pixel contrast for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib47.8.3" class="ltx_text" style="font-size:90%;">, pages 7303–7313, 2021.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.2.2.1" class="ltx_text" style="font-size:90%;">[48]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.4.1" class="ltx_text" style="font-size:90%;">
Fangyun Wei, Yue Gao, Zhirong Wu, Han Hu, and Stephen Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.5.1" class="ltx_text" style="font-size:90%;">Aligning pretraining for detection via object-level contrastive
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib48.7.2" class="ltx_text" style="font-size:90%;">,
34:22682–22694, 2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.2.2.1" class="ltx_text" style="font-size:90%;">[49]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.4.1" class="ltx_text" style="font-size:90%;">
Xin Wen, Bingchen Zhao, Anlin Zheng, Xiangyu Zhang, and Xiaojuan Qi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.5.1" class="ltx_text" style="font-size:90%;">Self-supervised visual representation learning with semantic
grouping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib49.8.3" class="ltx_text" style="font-size:90%;">,
volume 35, pages 16423–16438, 2022.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.2.2.1" class="ltx_text" style="font-size:90%;">[50]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.4.1" class="ltx_text" style="font-size:90%;">
Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.5.1" class="ltx_text" style="font-size:90%;">Aligning bag of regions for open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib50.8.3" class="ltx_text" style="font-size:90%;">, pages 15254–15264, 2023.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.2.2.1" class="ltx_text" style="font-size:90%;">[51]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.4.1" class="ltx_text" style="font-size:90%;">
Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan
Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.5.1" class="ltx_text" style="font-size:90%;">Universal instance perception as object discovery and retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib51.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.2.2.1" class="ltx_text" style="font-size:90%;">[52]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.4.1" class="ltx_text" style="font-size:90%;">
Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, and Hang
Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.5.1" class="ltx_text" style="font-size:90%;">Detclipv2: Scalable open-vocabulary object detection pre-training via
word-region alignment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib52.8.3" class="ltx_text" style="font-size:90%;">, pages 23497–23506, 2023.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.2.2.1" class="ltx_text" style="font-size:90%;">[53]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.4.1" class="ltx_text" style="font-size:90%;">
Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo
Li, Chunjing Xu, and Hang Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.5.1" class="ltx_text" style="font-size:90%;">Detclip: Dictionary-enriched visual-concept paralleled pre-training
for open-world detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib53.8.3" class="ltx_text" style="font-size:90%;">,
volume 35, pages 9125–9138, 2022.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.2.2.1" class="ltx_text" style="font-size:90%;">[54]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.4.1" class="ltx_text" style="font-size:90%;">
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
Yonghui Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.5.1" class="ltx_text" style="font-size:90%;">Coca: Contrastive captioners are image-text foundation models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Transactions on Machine Learning Research</span><span id="bib.bib54.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.2.2.1" class="ltx_text" style="font-size:90%;">[55]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.4.1" class="ltx_text" style="font-size:90%;">
Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao,
Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.5.1" class="ltx_text" style="font-size:90%;">Florence: A new foundation model for computer vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.11432</span><span id="bib.bib55.7.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.2.2.1" class="ltx_text" style="font-size:90%;">[56]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.4.1" class="ltx_text" style="font-size:90%;">
Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.5.1" class="ltx_text" style="font-size:90%;">Open-vocabulary detr with conditional matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part IX</span><span id="bib.bib56.8.3" class="ltx_text" style="font-size:90%;">, pages 106–122.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.2.2.1" class="ltx_text" style="font-size:90%;">[57]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.4.1" class="ltx_text" style="font-size:90%;">
Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.5.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection using captions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib57.8.3" class="ltx_text" style="font-size:90%;">, pages 14393–14402, 2021.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.2.2.1" class="ltx_text" style="font-size:90%;">[58]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.4.1" class="ltx_text" style="font-size:90%;">
Feihu Zhang, Philip Torr, René Ranftl, and Stephan Richter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.5.1" class="ltx_text" style="font-size:90%;">Looking beyond single images for contrastive semantic segmentation
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib58.8.3" class="ltx_text" style="font-size:90%;">,
volume 34, pages 3285–3297, 2021.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.2.2.1" class="ltx_text" style="font-size:90%;">[59]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.4.1" class="ltx_text" style="font-size:90%;">
Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella,
Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.5.1" class="ltx_text" style="font-size:90%;">Regionclip: Region-based language-image pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib59.8.3" class="ltx_text" style="font-size:90%;">, pages 16793–16803, 2022.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.2.2.1" class="ltx_text" style="font-size:90%;">[60]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.4.1" class="ltx_text" style="font-size:90%;">
Tianfei Zhou, Wenguan Wang, Ender Konukoglu, and Luc Van Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.5.1" class="ltx_text" style="font-size:90%;">Rethinking semantic segmentation: A prototype view.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib60.8.3" class="ltx_text" style="font-size:90%;">, pages 2582–2593, 2022.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.2.2.1" class="ltx_text" style="font-size:90%;">[61]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.4.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krähenbühl, and
Ishan Misra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.5.1" class="ltx_text" style="font-size:90%;">Detecting twenty-thousand classes using image-level supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part IX</span><span id="bib.bib61.8.3" class="ltx_text" style="font-size:90%;">, pages 350–368.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.2.2.1" class="ltx_text" style="font-size:90%;">[62]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.4.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.5.1" class="ltx_text" style="font-size:90%;">Probabilistic two-stage detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2103.07461</span><span id="bib.bib62.7.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.2.2.1" class="ltx_text" style="font-size:90%;">[63]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.4.1" class="ltx_text" style="font-size:90%;">
Pengkai Zhu, Hanxiao Wang, and Venkatesh Saligrama.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.5.1" class="ltx_text" style="font-size:90%;">Don’t even look once: Synthesizing features for zero-shot detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib63.8.3" class="ltx_text" style="font-size:90%;">, pages 11693–11702, 2020.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.2.2.1" class="ltx_text" style="font-size:90%;">[64]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.4.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.5.1" class="ltx_text" style="font-size:90%;">Deformable detr: Deformable transformers for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib64.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>A Heuristic Baseline for Co-occurrence Discovery</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.11" class="ltx_p">In this section, we introduce the baseline method used for ablation study in Table <a href="#S4.T4.st2" title="In Table 4 ‣ Heuristic vs. Prototype-based co-occurring object discovery. ‣ 4.6 Ablation Study ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a><span id="A1.p1.11.1" class="ltx_text" style="color:#FF0000;">b</span> in more detail. This baseline is adapted from a recently proposed image co-segmentation method ReCo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>. As shown in Figure <a href="#A1.F6" title="Figure 6 ‣ Appendix A A Heuristic Baseline for Co-occurrence Discovery ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, it basically consists of four steps to identify the co-occurring object in the query image: First, it estimates pair-wise region similarity between region proposals of the query image and support images, which is the same as CoDet. This yields a similarity matrix <math id="A1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{S}\in\mathbb{R}^{n\times m\times n}" display="inline"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mi id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">𝐒</mi><mo id="A1.p1.1.m1.1.1.1" xref="A1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml"><mi id="A1.p1.1.m1.1.1.3.2" xref="A1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="A1.p1.1.m1.1.1.3.3" xref="A1.p1.1.m1.1.1.3.3.cmml"><mi id="A1.p1.1.m1.1.1.3.3.2" xref="A1.p1.1.m1.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="A1.p1.1.m1.1.1.3.3.1" xref="A1.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="A1.p1.1.m1.1.1.3.3.3" xref="A1.p1.1.m1.1.1.3.3.3.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="A1.p1.1.m1.1.1.3.3.1a" xref="A1.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="A1.p1.1.m1.1.1.3.3.4" xref="A1.p1.1.m1.1.1.3.3.4.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><in id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1"></in><ci id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2">𝐒</ci><apply id="A1.p1.1.m1.1.1.3.cmml" xref="A1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A1.p1.1.m1.1.1.3.1.cmml" xref="A1.p1.1.m1.1.1.3">superscript</csymbol><ci id="A1.p1.1.m1.1.1.3.2.cmml" xref="A1.p1.1.m1.1.1.3.2">ℝ</ci><apply id="A1.p1.1.m1.1.1.3.3.cmml" xref="A1.p1.1.m1.1.1.3.3"><times id="A1.p1.1.m1.1.1.3.3.1.cmml" xref="A1.p1.1.m1.1.1.3.3.1"></times><ci id="A1.p1.1.m1.1.1.3.3.2.cmml" xref="A1.p1.1.m1.1.1.3.3.2">𝑛</ci><ci id="A1.p1.1.m1.1.1.3.3.3.cmml" xref="A1.p1.1.m1.1.1.3.3.3">𝑚</ci><ci id="A1.p1.1.m1.1.1.3.3.4.cmml" xref="A1.p1.1.m1.1.1.3.3.4">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">\mathbf{S}\in\mathbb{R}^{n\times m\times n}</annotation></semantics></math>, where <math id="A1.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="A1.p1.2.m2.1a"><mi id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><ci id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">n</annotation></semantics></math> stands for the number of proposals per image, and <math id="A1.p1.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A1.p1.3.m3.1a"><mi id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b"><ci id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">m</annotation></semantics></math> stands for the number of support images. Second, it applies a <math id="A1.p1.4.m4.1" class="ltx_Math" alttext="\max" display="inline"><semantics id="A1.p1.4.m4.1a"><mi id="A1.p1.4.m4.1.1" xref="A1.p1.4.m4.1.1.cmml">max</mi><annotation-xml encoding="MathML-Content" id="A1.p1.4.m4.1b"><max id="A1.p1.4.m4.1.1.cmml" xref="A1.p1.4.m4.1.1"></max></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.4.m4.1c">\max</annotation></semantics></math> operator on the last dimension of <math id="A1.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{S}" display="inline"><semantics id="A1.p1.5.m5.1a"><mi id="A1.p1.5.m5.1.1" xref="A1.p1.5.m5.1.1.cmml">𝐒</mi><annotation-xml encoding="MathML-Content" id="A1.p1.5.m5.1b"><ci id="A1.p1.5.m5.1.1.cmml" xref="A1.p1.5.m5.1.1">𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.5.m5.1c">\mathbf{S}</annotation></semantics></math>, which serves to find the nearest neighbor in each support image for each region proposal in the query image. This reduces <math id="A1.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{S}" display="inline"><semantics id="A1.p1.6.m6.1a"><mi id="A1.p1.6.m6.1.1" xref="A1.p1.6.m6.1.1.cmml">𝐒</mi><annotation-xml encoding="MathML-Content" id="A1.p1.6.m6.1b"><ci id="A1.p1.6.m6.1.1.cmml" xref="A1.p1.6.m6.1.1">𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.6.m6.1c">\mathbf{S}</annotation></semantics></math> to an <math id="A1.p1.7.m7.1" class="ltx_Math" alttext="n\times m" display="inline"><semantics id="A1.p1.7.m7.1a"><mrow id="A1.p1.7.m7.1.1" xref="A1.p1.7.m7.1.1.cmml"><mi id="A1.p1.7.m7.1.1.2" xref="A1.p1.7.m7.1.1.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="A1.p1.7.m7.1.1.1" xref="A1.p1.7.m7.1.1.1.cmml">×</mo><mi id="A1.p1.7.m7.1.1.3" xref="A1.p1.7.m7.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.7.m7.1b"><apply id="A1.p1.7.m7.1.1.cmml" xref="A1.p1.7.m7.1.1"><times id="A1.p1.7.m7.1.1.1.cmml" xref="A1.p1.7.m7.1.1.1"></times><ci id="A1.p1.7.m7.1.1.2.cmml" xref="A1.p1.7.m7.1.1.2">𝑛</ci><ci id="A1.p1.7.m7.1.1.3.cmml" xref="A1.p1.7.m7.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.7.m7.1c">n\times m</annotation></semantics></math> matrix. Third, it applies a <math id="A1.p1.8.m8.1" class="ltx_Math" alttext="\operatorname{mean}" display="inline"><semantics id="A1.p1.8.m8.1a"><mi id="A1.p1.8.m8.1.1" xref="A1.p1.8.m8.1.1.cmml">mean</mi><annotation-xml encoding="MathML-Content" id="A1.p1.8.m8.1b"><ci id="A1.p1.8.m8.1.1.cmml" xref="A1.p1.8.m8.1.1">mean</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.8.m8.1c">\operatorname{mean}</annotation></semantics></math> operator on the second dimension of <math id="A1.p1.9.m9.1" class="ltx_Math" alttext="\mathbf{S}" display="inline"><semantics id="A1.p1.9.m9.1a"><mi id="A1.p1.9.m9.1.1" xref="A1.p1.9.m9.1.1.cmml">𝐒</mi><annotation-xml encoding="MathML-Content" id="A1.p1.9.m9.1b"><ci id="A1.p1.9.m9.1.1.cmml" xref="A1.p1.9.m9.1.1">𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.9.m9.1c">\mathbf{S}</annotation></semantics></math> to derive the average support that each proposal has among the support images. Finally, it identifies the co-occurring object as the one with the highest average maximum similarity (support) among support images, by applying an <math id="A1.p1.10.m10.1" class="ltx_Math" alttext="\operatorname{argmax}" display="inline"><semantics id="A1.p1.10.m10.1a"><mi id="A1.p1.10.m10.1.1" xref="A1.p1.10.m10.1.1.cmml">argmax</mi><annotation-xml encoding="MathML-Content" id="A1.p1.10.m10.1b"><ci id="A1.p1.10.m10.1.1.cmml" xref="A1.p1.10.m10.1.1">argmax</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.10.m10.1c">\operatorname{argmax}</annotation></semantics></math> operator on the first dimension of <math id="A1.p1.11.m11.1" class="ltx_Math" alttext="\mathbf{S}" display="inline"><semantics id="A1.p1.11.m11.1a"><mi id="A1.p1.11.m11.1.1" xref="A1.p1.11.m11.1.1.cmml">𝐒</mi><annotation-xml encoding="MathML-Content" id="A1.p1.11.m11.1b"><ci id="A1.p1.11.m11.1.1.cmml" xref="A1.p1.11.m11.1.1">𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.11.m11.1c">\mathbf{S}</annotation></semantics></math>.</p>
</div>
<figure id="A1.F6" class="ltx_figure"><img src="/html/2310.16667/assets/x6.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F6.9.4.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="A1.F6.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustration of the baseline method for co-occurrence discovery<span id="A1.F6.6.3.3" class="ltx_text ltx_font_medium">. <math id="A1.F6.4.1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="A1.F6.4.1.1.m1.1b"><mi id="A1.F6.4.1.1.m1.1.1" xref="A1.F6.4.1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A1.F6.4.1.1.m1.1c"><ci id="A1.F6.4.1.1.m1.1.1.cmml" xref="A1.F6.4.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F6.4.1.1.m1.1d">Q</annotation></semantics></math> and <math id="A1.F6.5.2.2.m2.1" class="ltx_Math" alttext="P" display="inline"><semantics id="A1.F6.5.2.2.m2.1b"><mi id="A1.F6.5.2.2.m2.1.1" xref="A1.F6.5.2.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="A1.F6.5.2.2.m2.1c"><ci id="A1.F6.5.2.2.m2.1.1.cmml" xref="A1.F6.5.2.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F6.5.2.2.m2.1d">P</annotation></semantics></math> are region proposals in the query image and support images, respectively. <math id="A1.F6.6.3.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="A1.F6.6.3.3.m3.1b"><mi id="A1.F6.6.3.3.m3.1.1" xref="A1.F6.6.3.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="A1.F6.6.3.3.m3.1c"><ci id="A1.F6.6.3.3.m3.1.1.cmml" xref="A1.F6.6.3.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F6.6.3.3.m3.1d">S</annotation></semantics></math> is the averaged maximum similarity score across support images.</span></span></figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Further Analysis on Different Alignment Strategies</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">Complementing the discourse in Section <a href="#S4.SS5" title="4.5 Visualization and Analysis ‣ 4 Experiments ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>, we further delineate the performance of different alignment strategies with respect to novel category <math id="A2.p1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{50}" display="inline"><semantics id="A2.p1.1.m1.1a"><msub id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml"><mtext id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2a.cmml">AP</mtext><mn id="A2.p1.1.m1.1.1.3" xref="A2.p1.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1">subscript</csymbol><ci id="A2.p1.1.m1.1.1.2a.cmml" xref="A2.p1.1.m1.1.1.2"><mtext id="A2.p1.1.m1.1.1.2.cmml" xref="A2.p1.1.m1.1.1.2">AP</mtext></ci><cn type="integer" id="A2.p1.1.m1.1.1.3.cmml" xref="A2.p1.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">\text{AP}_{50}</annotation></semantics></math> on OV-COCO in Figure <a href="#A2.F7" title="Figure 7 ‣ Appendix B Further Analysis on Different Alignment Strategies ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. It can be seen that strategies based on region-region similarity or hand-crafted rules (max-size) show steady improvement in novel object recognition across training, whereas the performance of region-word similarity-based method is highly unstable and even decreases in the early stage. A possible explanation is that solely relying on region-word similarity to align regions and words may be more susceptible to errors in pseudo-labels. For instance, if the model incorrectly matches the text label ‘seagull’ with the object ‘dove’ at the initial phase, its supervision signal would pull the two closer in the shared feature space. This negative feedback could directly harm the following pseudo-labeling process, thus, there is a higher probability for the model to make the same mistake.</p>
</div>
<figure id="A2.F7" class="ltx_figure"><img src="/html/2310.16667/assets/x7.png" id="A2.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="276" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="A2.F7.3.2" class="ltx_text" style="font-size:90%;">Performance of different alignment strategies at discrete training stages on OV-COCO.</span></figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Visualization on OV-LVIS and OV-COCO</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We visualize more detection results of CoDet in Figure <a href="#A3.F8" title="Figure 8 ‣ Appendix C Visualization on OV-LVIS and OV-COCO ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and Figure <a href="#A3.F9" title="Figure 9 ‣ Appendix C Visualization on OV-LVIS and OV-COCO ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. On OV-LVIS, we can see that CoDet successfully detects many rare objects, <span id="A3.p1.1.1" class="ltx_ERROR undefined">\eg</span>, gas mask, puffin, horse buggy, heron, satchel, and so on (Figure <a href="#A3.F8" title="Figure 8 ‣ Appendix C Visualization on OV-LVIS and OV-COCO ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). This validates that CoDet can efficiently leverage web-crawled image-text pairs to learn open-word knowledge for novel object recognition. On OV-COCO, our method continues to demonstrate strong open-vocabulary capability and correctly detects some hard samples, <span id="A3.p1.1.2" class="ltx_ERROR undefined">\eg</span>, the occluded ‘tie’ and ‘elephant’ (upper left of Figure <a href="#A3.F9" title="Figure 9 ‣ Appendix C Visualization on OV-LVIS and OV-COCO ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Nevertheless, we also notice that the prediction scores for novel categories are generally lower than base categories, which suggests the model is biased towards base classes in OV-COCO. Such tendency to overfit base categories is also observed in other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>, due to the small training vocabulary of OV-COCO. We believe adopting tricks like focal loss could alleviate this issue and further benefit our method.</p>
</div>
<figure id="A3.F8" class="ltx_figure"><img src="/html/2310.16667/assets/x8.png" id="A3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A3.F8.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of prediction results by CoDet on OV-LVIS<span id="A3.F8.4.2.1" class="ltx_text ltx_font_medium">. For clarity, we only show results for novel categories.</span></span></figcaption>
</figure>
<figure id="A3.F9" class="ltx_figure"><img src="/html/2310.16667/assets/x9.png" id="A3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="A3.F9.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of prediction results by CoDet on OV-COCO<span id="A3.F9.4.2.1" class="ltx_text ltx_font_medium">. Red boxes are for novel categories, while blue boxes are for base categories.</span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Implementation Details</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">Table <a href="#A4.T6" title="Table 6 ‣ Appendix D Implementation Details ‣ CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> lists the detailed hyper-parameter configuration used for our OV-LVIS and OV-COCO experiments. We follow Detic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite> to use low input resolution and large batch size for caption data to achieve better trade-off between efficiency and performance.</p>
</div>
<figure id="A4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A4.T6.6.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="A4.T6.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Hyper-parameter configuration of CoDet.<span id="A4.T6.7.2.1" class="ltx_text ltx_font_medium"> LSJ stands for large scale jittering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>. Resolution refers to the resized short side length of input images.</span></span></figcaption>
<table id="A4.T6.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T6.3.4.1" class="ltx_tr">
<td id="A4.T6.3.4.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A4.T6.3.4.1.1.1" class="ltx_text ltx_font_bold">Configuration</span></td>
<td id="A4.T6.3.4.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A4.T6.3.4.1.2.1" class="ltx_text ltx_font_bold">OV-LVIS</span></td>
<td id="A4.T6.3.4.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A4.T6.3.4.1.3.1" class="ltx_text ltx_font_bold">OV-COCO</span></td>
</tr>
<tr id="A4.T6.3.5.2" class="ltx_tr">
<td id="A4.T6.3.5.2.1" class="ltx_td ltx_align_left ltx_border_t">Optimizer</td>
<td id="A4.T6.3.5.2.2" class="ltx_td ltx_align_center ltx_border_t">AdamW</td>
<td id="A4.T6.3.5.2.3" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
</tr>
<tr id="A4.T6.3.6.3" class="ltx_tr">
<td id="A4.T6.3.6.3.1" class="ltx_td ltx_align_left">Gradient clipping</td>
<td id="A4.T6.3.6.3.2" class="ltx_td ltx_align_center">True</td>
<td id="A4.T6.3.6.3.3" class="ltx_td ltx_align_center">True</td>
</tr>
<tr id="A4.T6.3.7.4" class="ltx_tr">
<td id="A4.T6.3.7.4.1" class="ltx_td ltx_align_left">Learning rate (LR)</td>
<td id="A4.T6.3.7.4.2" class="ltx_td ltx_align_center">2e-4</td>
<td id="A4.T6.3.7.4.3" class="ltx_td ltx_align_center">2e-2</td>
</tr>
<tr id="A4.T6.3.8.5" class="ltx_tr">
<td id="A4.T6.3.8.5.1" class="ltx_td ltx_align_left">Total iterations</td>
<td id="A4.T6.3.8.5.2" class="ltx_td ltx_align_center">90k</td>
<td id="A4.T6.3.8.5.3" class="ltx_td ltx_align_center">90k</td>
</tr>
<tr id="A4.T6.3.9.6" class="ltx_tr">
<td id="A4.T6.3.9.6.1" class="ltx_td ltx_align_left">Warmup iterations</td>
<td id="A4.T6.3.9.6.2" class="ltx_td ltx_align_center">1k</td>
<td id="A4.T6.3.9.6.3" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="A4.T6.1.1" class="ltx_tr">
<td id="A4.T6.1.1.2" class="ltx_td ltx_align_left">Step decay factor</td>
<td id="A4.T6.1.1.3" class="ltx_td ltx_align_center">–</td>
<td id="A4.T6.1.1.1" class="ltx_td ltx_align_center">0.1<math id="A4.T6.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A4.T6.1.1.1.m1.1a"><mo id="A4.T6.1.1.1.m1.1.1" xref="A4.T6.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A4.T6.1.1.1.m1.1b"><times id="A4.T6.1.1.1.m1.1.1.cmml" xref="A4.T6.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.1.1.1.m1.1c">\times</annotation></semantics></math>
</td>
</tr>
<tr id="A4.T6.3.10.7" class="ltx_tr">
<td id="A4.T6.3.10.7.1" class="ltx_td ltx_align_left">Step decay schedule</td>
<td id="A4.T6.3.10.7.2" class="ltx_td ltx_align_center">–</td>
<td id="A4.T6.3.10.7.3" class="ltx_td ltx_align_center">[60k, 80k]</td>
</tr>
<tr id="A4.T6.3.11.8" class="ltx_tr">
<td id="A4.T6.3.11.8.1" class="ltx_td ltx_align_left">Data augmentation</td>
<td id="A4.T6.3.11.8.2" class="ltx_td ltx_align_center">LSJ</td>
<td id="A4.T6.3.11.8.3" class="ltx_td ltx_align_center">none</td>
</tr>
<tr id="A4.T6.3.12.9" class="ltx_tr">
<td id="A4.T6.3.12.9.1" class="ltx_td ltx_align_left">Batch size (detection)</td>
<td id="A4.T6.3.12.9.2" class="ltx_td ltx_align_center">8</td>
<td id="A4.T6.3.12.9.3" class="ltx_td ltx_align_center">2</td>
</tr>
<tr id="A4.T6.3.13.10" class="ltx_tr">
<td id="A4.T6.3.13.10.1" class="ltx_td ltx_align_left">Batch size (caption)</td>
<td id="A4.T6.3.13.10.2" class="ltx_td ltx_align_center">32</td>
<td id="A4.T6.3.13.10.3" class="ltx_td ltx_align_center">8</td>
</tr>
<tr id="A4.T6.3.14.11" class="ltx_tr">
<td id="A4.T6.3.14.11.1" class="ltx_td ltx_align_left">Detection/Caption data ratio</td>
<td id="A4.T6.3.14.11.2" class="ltx_td ltx_align_center">1:4</td>
<td id="A4.T6.3.14.11.3" class="ltx_td ltx_align_center">1:4</td>
</tr>
<tr id="A4.T6.3.15.12" class="ltx_tr">
<td id="A4.T6.3.15.12.1" class="ltx_td ltx_align_left">Federated loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite>
</td>
<td id="A4.T6.3.15.12.2" class="ltx_td ltx_align_center">True</td>
<td id="A4.T6.3.15.12.3" class="ltx_td ltx_align_center">False</td>
</tr>
<tr id="A4.T6.3.16.13" class="ltx_tr">
<td id="A4.T6.3.16.13.1" class="ltx_td ltx_align_left">Repeat factor sampling</td>
<td id="A4.T6.3.16.13.2" class="ltx_td ltx_align_center">True</td>
<td id="A4.T6.3.16.13.3" class="ltx_td ltx_align_center">False</td>
</tr>
<tr id="A4.T6.2.2" class="ltx_tr">
<td id="A4.T6.2.2.1" class="ltx_td ltx_align_left">
<math id="A4.T6.2.2.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{region-word}}" display="inline"><semantics id="A4.T6.2.2.1.m1.1a"><msub id="A4.T6.2.2.1.m1.1.1" xref="A4.T6.2.2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.T6.2.2.1.m1.1.1.2" xref="A4.T6.2.2.1.m1.1.1.2.cmml">ℒ</mi><mtext id="A4.T6.2.2.1.m1.1.1.3" xref="A4.T6.2.2.1.m1.1.1.3a.cmml">region-word</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.2.2.1.m1.1b"><apply id="A4.T6.2.2.1.m1.1.1.cmml" xref="A4.T6.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="A4.T6.2.2.1.m1.1.1.1.cmml" xref="A4.T6.2.2.1.m1.1.1">subscript</csymbol><ci id="A4.T6.2.2.1.m1.1.1.2.cmml" xref="A4.T6.2.2.1.m1.1.1.2">ℒ</ci><ci id="A4.T6.2.2.1.m1.1.1.3a.cmml" xref="A4.T6.2.2.1.m1.1.1.3"><mtext mathsize="70%" id="A4.T6.2.2.1.m1.1.1.3.cmml" xref="A4.T6.2.2.1.m1.1.1.3">region-word</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.2.2.1.m1.1c">\mathcal{L}_{\text{region-word}}</annotation></semantics></math> weight</td>
<td id="A4.T6.2.2.2" class="ltx_td ltx_align_center">0.2</td>
<td id="A4.T6.2.2.3" class="ltx_td ltx_align_center">0.1</td>
</tr>
<tr id="A4.T6.3.3" class="ltx_tr">
<td id="A4.T6.3.3.1" class="ltx_td ltx_align_left ltx_border_bb">
<math id="A4.T6.3.3.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{image-text}}" display="inline"><semantics id="A4.T6.3.3.1.m1.1a"><msub id="A4.T6.3.3.1.m1.1.1" xref="A4.T6.3.3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.T6.3.3.1.m1.1.1.2" xref="A4.T6.3.3.1.m1.1.1.2.cmml">ℒ</mi><mtext id="A4.T6.3.3.1.m1.1.1.3" xref="A4.T6.3.3.1.m1.1.1.3a.cmml">image-text</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.3.3.1.m1.1b"><apply id="A4.T6.3.3.1.m1.1.1.cmml" xref="A4.T6.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="A4.T6.3.3.1.m1.1.1.1.cmml" xref="A4.T6.3.3.1.m1.1.1">subscript</csymbol><ci id="A4.T6.3.3.1.m1.1.1.2.cmml" xref="A4.T6.3.3.1.m1.1.1.2">ℒ</ci><ci id="A4.T6.3.3.1.m1.1.1.3a.cmml" xref="A4.T6.3.3.1.m1.1.1.3"><mtext mathsize="70%" id="A4.T6.3.3.1.m1.1.1.3.cmml" xref="A4.T6.3.3.1.m1.1.1.3">image-text</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.3.3.1.m1.1c">\mathcal{L}_{\text{image-text}}</annotation></semantics></math> weight</td>
<td id="A4.T6.3.3.2" class="ltx_td ltx_align_center ltx_border_bb">0.2</td>
<td id="A4.T6.3.3.3" class="ltx_td ltx_align_center ltx_border_bb">0.1</td>
</tr>
</tbody>
</table>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.16666" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.16667" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.16667">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.16667" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.16668" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 23:57:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
