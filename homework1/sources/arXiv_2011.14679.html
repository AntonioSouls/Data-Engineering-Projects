<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2011.14679] CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild</title><meta property="og:description" content="Human pose estimation from single images is a challenging problem in computer vision that requires large amounts of labeled training data to be solved accurately.
Unfortunately, for many human activities (\egoutdoor spâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2011.14679">

<!--Generated on Sat Mar  9 04:08:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bastian Wandt
<br class="ltx_break">Leibniz University Hannover
<br class="ltx_break">Hannover
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Germany
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">wandt@tnt.uni-hannover.de</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marco Rudolph
<br class="ltx_break">Leibniz University Hannover
<br class="ltx_break">Hannover
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Germany
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">rudolph@tnt.uni-hannover.de</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Petrissa Zell
<br class="ltx_break">Leibniz University Hannover
<br class="ltx_break">Hannover
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Germany
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">zell@tnt.uni-hannover.de</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Helge Rhodin
<br class="ltx_break">UBC Vancouver
<br class="ltx_break">Vancouver
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Canada
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">helge.rhodin@ubc.ca</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bodo Rosenhahn
<br class="ltx_break">Leibniz University Hannover
<br class="ltx_break">Hannover
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Germany
<br class="ltx_break"><span id="id5.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">rosenhahn@tnt.uni-hannover.de</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Human pose estimation from single images is a challenging problem in computer vision that requires large amounts of labeled training data to be solved accurately.
Unfortunately, for many human activities (<span id="id6.id1.1" class="ltx_ERROR undefined">\eg</span>outdoor sports) such training data does not exist and is hard or even impossible to acquire with traditional motion capture systems.
We propose a self-supervised approach that learns a single image 3D pose estimator from unlabeled multi-view data.
To this end, we exploit multi-view consistency constraints to disentangle the observed 2D pose into the underlying 3D pose and camera rotation.
In contrast to most existing methods, we do not require calibrated cameras and can therefore learn from moving cameras.
Nevertheless, in the case of a static camera setup, we present an optional extension to include constant relative camera rotations over multiple views into our framework.
Key to the success are new, unbiased reconstruction objectives that mix information across views and training samples.
The proposed approach is evaluated on two benchmark datasets (Human3.6M and MPII-INF-3DHP) and on the in-the-wild SkiPose dataset.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2011.14679/assets/images/teaser_v3.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>CanonPose learns a monocular 3D human pose estimator from multi-view self-supervision. By estimating 3D poses from different views in a canonical form together with the respective camera rotations we exploit multi-view consistency in the training data. Even for challenging outdoor datasets with moving cameras we achieve convincing 3D pose estimates from single images after training.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Human pose estimation from single images is an ongoing research topic in computer vision.
There exist a large amount of supervised deep learning solutions in the literature. These approaches achieve remarkable results in a supervised setting, <span id="S1.p1.1.1" class="ltx_ERROR undefined">\ie</span>having 2D to 3D annotations, but heavily rely on a vast amount of available training data.
However, there are many activities a person can perform which are not present in common datasets.
For instance, human motions performed during outdoor and/or sports activities, <span id="S1.p1.1.2" class="ltx_ERROR undefined">\eg</span>as shown in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, are hard or even impossible to capture with a commercial motion capture systems.
Therefore, the acquisition of training data is a major practical challenge.
To this end, we propose a novel self-supervised training procedure that does not require any 2D or 3D annotations in the multi-view training dataset and works with uncalibrated cameras.
To acquire 2D joint predictions from images we use a 2D human joint estimator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> that is pretrained on a different dataset with only 2D joint annotations.
The only requirements for our method are at least two temporally synchronised cameras that observe the person of interest from different angles.
No further knowledge about the scene, camera calibration and intrinsics is required.
Several related works consider a sparse set of 3D annotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, unpaired 3D data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, or known camera positions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to solve this problem.
However, such data rarely exists for outdoor settings with moving cameras.
To the best of our knowledge, there are only three competing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> that apply to our setting.
They either require additional knowledge about the scene or observed person, such as scene geometry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and bone lengths constraints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, or sophisticated traditional computer vision algorithms that produce a pseudo ground truth pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">We propose a self-supervised training method which mixes outputs of multiple weight-sharing neural networks.
Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows our training pipeline when using two cameras.
Each individual network takes a single image as input and outputs a 3D pose in a canonical rotation, which gives our method its name <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">CanonPose</em>.
This representation allows for the projection of all estimated 3D poses to any camera of the setup.
Our approach splits into two stages.
The first stage predicts the 2D human pose from an image using a neural network pretrained on the MPII dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, in our case AlphaPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
The second stage lifts these 2D detections to a 3D pose represented in a learned canonical coordinate system.
In a separate path it predicts the camera orientation to rotate the predicted 3D pose back into the camera coordinate system.
Combining the 3D pose from a first view with the rotation predicted from a second view, results in a rotated pose in the second camera coordinate system.
In other words, both 3D poses in the pose coordinate system should be equal and the predicted rotations project it back into the respective camera coordinate systems.
This enables the definition of a reprojection loss for each original and newly combined reprojection.
For static camera setups we propose an optional reprojection loss that is computed by mixing relative camera rotations between samples in a training batch.
Additionally, in contrast to existing self-supervised approaches, we also make use of the confidences that are typically provided by a 2D pose estimator for each predicted 2D joint by including them into the 2D input vector as well as into the reprojection loss formulation.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We evaluate our approach on the two benchmark datasets Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and set the new state of the art in several metrics for self-supervised 3D pose estimation.
Notably, this is without assuming any camera calibration or static cameras.
Our results are competitive to the fully supervised approach of Martinez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> which sets the baseline for single image pose estimation from 2D detections.
Additionally, we show results for the SkiPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> dataset.
This dataset represents all challenges that arise when activities are captured that cannot be performed in the restricted setting of a standard motion capture system.
It consists of outdoor scenes captured on a ski slope and includes fast motions, a large capture volume and pan-tilt-zoom cameras.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Summarizing, our contributions are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We present CanonPose: a self-supervised approach to train a single image 3D pose estimator from unlabeled multi-view images by mixing poses across views.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Our approach requires no prior knowledge about the scene, 3D skeleton or camera calibration.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The proposed method directly employs multi-view images without any laborious pre-processing, such as camera calibration or multi-view geometry estimation.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We integrate the confidences from the 2D joint estimator into the training pipeline.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section we discuss recent 3D human pose estimation approaches by different types of supervision.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Full Supervision</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Recent supervised approaches rely on large datasets that contain millions of images with corresponding 3D pose annotations.
Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> were the first to learn CNNs to directly regress a 3D pose from image input.
By integrating a structured learning framework into CNNs they later improved their work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
Several others followed this end-to-end approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Typically, these end-to-end approaches perform exceptionally well on similar image data.
However, their ability to generalize to other scenes is limited.
Many works tackle this problem by cross dataset training or data augmentation.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">There are other approaches that do not consider the image data directly but use a pretrained 2D joint detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
They benefit from training on large datasets that contain 2D annotations for many human activities in various scenes and are therefore agnostic to the image data.
Martinez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> directly train a neural network on 2D detections and 3D ground truth.
Due to its simplicity it can be trained quickly for many epochs leading to high accuracy and serves as a baseline for many following works.
The approach of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> was extended by Hossain et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> by employing a recurrent neural network for sequences of human poses.
While effective, the major downside of all supervised approaches is that they do not generalize well to unseen poses.
Therefore, their application to in-the-wild scenes is limited.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Weak Supervision</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Weakly supervised approaches only require a small set or even no annotated 2D to 3D correspondences.
An example for a commonly applied evaluation protocol for the Human3.6M dataset assumes that 3D annotations for one of the subjects of the training set are available.
A transfer learning approach is introduced by Mehta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> to allow for in-the-wild pose estimation of datasets where no training data is available.
This framework was later extended by Mehta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> to achieve real-time performance.
Rhodin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> use multi-view images and known camera positions to learn a 3D pose embedding in an unsupervised fashion.
The embedding facilitates the training with only a sparse set of 3D annotations.
This idea was adopted in other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
Another approach is to employ unpaired 2D and 3D poses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
Since these method learn distributions of plausible 3D poses and their properties they generalize better to unseen poses.
Although they are able to reconstruct out-of-distribution poses to a limited degree they struggle with completely unseen poses.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Self-supervised and Unsupervised Learning without 3D Ground Truth</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Recently, the interest in multi-view self-supervised and unsupervised 3D pose estimation is growing.
Our work also falls into this category.
Drover et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> propose an unsupervised approach to monocular human pose estimation.
They randomly project an estimated 3D pose back to 2D.
This 2D projection is then evaluated by a discriminator following adversarial training approaches.
Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> extended <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> with a cycle consistency loss that is computed by lifting the randomly projected 2D pose to 3D and inversing the previously defined random projection.
Although these two approaches are unsupervised, they integrate knowledge about the scene by constraining the camera rotation axis that is used for the random projection.
Rochette <span id="S2.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px3.p1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S2.SS0.SSS0.Px3.p1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> use a large amount of cameras from different viewing angles to achieve on par performance with a comparable fully supervised approach.
However, due to the restriction to the camera setup the practical applicability is limited.
Kocabas et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> propose a multi-view self-supervised approach which does not require any 3D supervision.
They apply traditional computer vision methods, namely epipolar geometry, to 2D pose predictions from multiple views to compute a pseudo ground truth which is then used to train the 3D lifting network.
Although this simple and effective straight-forward approach gives promising results, the laborious preprocessing step is very parameter sensitive and therefore does not generalize well. Moreover, mistakes due to wrongly estimated joints in the 2D prediction step result in a wrong pseudo ground truth.
Iqbal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> tackle this problem by training an end-to-end network that refines the pre-trained 2D pose estimator during the self-supervised training.
Unfortunately, such approaches tend to easily overfit to a specific dataset.
For example, it could learn a background image for the training dataset which leads to exceptional performance on the specific dataset but does not generalize to other backgrounds.
This even happens in self-supervised settings.
Furthermore, Iqbal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> employ a loss on normalized 3D bone lengths which is computed from the ground truth 3D poses of the Human3.6M training set.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p2.1" class="ltx_p">In contrast, our approach does not require knowledge about the scene and camera position or any anthropometric constraints.
Additionally, it is modular such that any 2D pose estimator can be used which makes it agnostic to the image data.
Even though our approach relaxes many constraints of the comparable works it still outperforms them in most experiments.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2011.14679/assets/images/structure_3.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="281" height="179" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Network structure to learn single image 3D pose estimation from multi-view self-supervision. Each lifting network predicts a 3D pose and a camera rotation which is used to project the 3D pose back to 2D. Both networks observe the same 3D pose from different angles. We exploit this fact by applying the camera rotation to the respective other pose. This projects a predicted 3D pose into the other camera and gives an additional reprojection error. At inference time only one view (gray box) is applied.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.3" class="ltx_p">Our approach consists of two steps: first applying an off-the-shelf 2D joint detector to the input images, and second lifting these detections and the respective confidences for each joint to 3D.
The core idea of our approach is that 2D detections from one view can be projected to another view via a canonical pose space.
Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows our pipeline using two cameras.
For simplicity the network structure is shown for only two cameras.
If more cameras are available it is straight-forwardly extended.
A single neural network, the <span id="S3.p1.3.1" class="ltx_text ltx_font_italic">3D lifting network</span>, predicts the 3D pose <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\bm{X}\in\mathbb{R}^{3\times j}" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">ğ‘¿</mi><mo id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml"><mi id="S3.p1.1.m1.1.1.3.2" xref="S3.p1.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.p1.1.m1.1.1.3.3" xref="S3.p1.1.m1.1.1.3.3.cmml"><mn id="S3.p1.1.m1.1.1.3.3.2" xref="S3.p1.1.m1.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p1.1.m1.1.1.3.3.1" xref="S3.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.p1.1.m1.1.1.3.3.3" xref="S3.p1.1.m1.1.1.3.3.3.cmml">j</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><in id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></in><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">ğ‘¿</ci><apply id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.1.cmml" xref="S3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.p1.1.m1.1.1.3.2.cmml" xref="S3.p1.1.m1.1.1.3.2">â„</ci><apply id="S3.p1.1.m1.1.1.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3"><times id="S3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.p1.1.m1.1.1.3.3.1"></times><cn type="integer" id="S3.p1.1.m1.1.1.3.3.2.cmml" xref="S3.p1.1.m1.1.1.3.3.2">3</cn><ci id="S3.p1.1.m1.1.1.3.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3.3">ğ‘—</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\bm{X}\in\mathbb{R}^{3\times j}</annotation></semantics></math> with <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">j</annotation></semantics></math> joints and a rotation <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\bm{R}\in\mathbb{R}^{3\times 3}" display="inline"><semantics id="S3.p1.3.m3.1a"><mrow id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">ğ‘¹</mi><mo id="S3.p1.3.m3.1.1.1" xref="S3.p1.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml"><mi id="S3.p1.3.m3.1.1.3.2" xref="S3.p1.3.m3.1.1.3.2.cmml">â„</mi><mrow id="S3.p1.3.m3.1.1.3.3" xref="S3.p1.3.m3.1.1.3.3.cmml"><mn id="S3.p1.3.m3.1.1.3.3.2" xref="S3.p1.3.m3.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p1.3.m3.1.1.3.3.1" xref="S3.p1.3.m3.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.p1.3.m3.1.1.3.3.3" xref="S3.p1.3.m3.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><in id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1.1"></in><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">ğ‘¹</ci><apply id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.3.1.cmml" xref="S3.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.p1.3.m3.1.1.3.2.cmml" xref="S3.p1.3.m3.1.1.3.2">â„</ci><apply id="S3.p1.3.m3.1.1.3.3.cmml" xref="S3.p1.3.m3.1.1.3.3"><times id="S3.p1.3.m3.1.1.3.3.1.cmml" xref="S3.p1.3.m3.1.1.3.3.1"></times><cn type="integer" id="S3.p1.3.m3.1.1.3.3.2.cmml" xref="S3.p1.3.m3.1.1.3.3.2">3</cn><cn type="integer" id="S3.p1.3.m3.1.1.3.3.3.cmml" xref="S3.p1.3.m3.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\bm{R}\in\mathbb{R}^{3\times 3}</annotation></semantics></math> to rotate the pose to the camera coordinate system.
The pose is represented in a canonical pose coordinate system which is automatically learned during training.
Subsequently, the predicted 3D pose is rotated from the pose coordinate system to the camera coordinate system by the predicted rotation.
This separation into canonical human pose and camera rotation enables us to formulate various reprojection losses for self-supervision across views and samples.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Reprojection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.6" class="ltx_p">Before a 2D pose is lifted to 3D it is normalized by centering it to the root joint and scaled by dividing it by its Frobenius norm.
This sidesteps the scale-depth ambiguity in monocular reconstruction.
In particular, the root centering gives a common rotation point for all 3D predictions.
For each view the predicted 3D pose is rotated into the camera coordinate system by <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\bm{R}\bm{X}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">ğ‘¹</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">ğ‘¿</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ‘¹</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">ğ‘¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\bm{R}\bm{X}</annotation></semantics></math>.
<math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\bm{R}\in\mathbb{R}^{3\times 3}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">ğ‘¹</mi><mo id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.3.2" xref="S3.SS1.p1.2.m2.1.1.3.2.cmml">â„</mi><mrow id="S3.SS1.p1.2.m2.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3.cmml"><mn id="S3.SS1.p1.2.m2.1.1.3.3.2" xref="S3.SS1.p1.2.m2.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.3.3.1" xref="S3.SS1.p1.2.m2.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.SS1.p1.2.m2.1.1.3.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><in id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></in><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ğ‘¹</ci><apply id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2">â„</ci><apply id="S3.SS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3"><times id="S3.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3.1"></times><cn type="integer" id="S3.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3.2">3</cn><cn type="integer" id="S3.SS1.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\bm{R}\in\mathbb{R}^{3\times 3}</annotation></semantics></math> is a rotational matrix such that <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\bm{R}\bm{R}^{T}=\bm{I}_{3}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mrow id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml">ğ‘¹</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.1.2.1" xref="S3.SS1.p1.3.m3.1.1.2.1.cmml">â€‹</mo><msup id="S3.SS1.p1.3.m3.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.2.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2.3.2" xref="S3.SS1.p1.3.m3.1.1.2.3.2.cmml">ğ‘¹</mi><mi id="S3.SS1.p1.3.m3.1.1.2.3.3" xref="S3.SS1.p1.3.m3.1.1.2.3.3.cmml">T</mi></msup></mrow><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">=</mo><msub id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.3.2.cmml">ğ‘°</mi><mn id="S3.SS1.p1.3.m3.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.cmml">3</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><eq id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></eq><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><times id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2.1"></times><ci id="S3.SS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2">ğ‘¹</ci><apply id="S3.SS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.2.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3">superscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3.2">ğ‘¹</ci><ci id="S3.SS1.p1.3.m3.1.1.2.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3.3">ğ‘‡</ci></apply></apply><apply id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.2">ğ‘°</ci><cn type="integer" id="S3.SS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\bm{R}\bm{R}^{T}=\bm{I}_{3}</annotation></semantics></math> with <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\bm{I}_{3}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">ğ‘°</mi><mn id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ‘°</ci><cn type="integer" id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\bm{I}_{3}</annotation></semantics></math> as the <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mn id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><times id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1"></times><cn type="integer" id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">3</cn><cn type="integer" id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">3\times 3</annotation></semantics></math> identity matrix and <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="det(\bm{R})=1" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.2" xref="S3.SS1.p1.6.m6.1.2.cmml"><mrow id="S3.SS1.p1.6.m6.1.2.2" xref="S3.SS1.p1.6.m6.1.2.2.cmml"><mi id="S3.SS1.p1.6.m6.1.2.2.2" xref="S3.SS1.p1.6.m6.1.2.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.6.m6.1.2.2.1" xref="S3.SS1.p1.6.m6.1.2.2.1.cmml">â€‹</mo><mi id="S3.SS1.p1.6.m6.1.2.2.3" xref="S3.SS1.p1.6.m6.1.2.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.6.m6.1.2.2.1a" xref="S3.SS1.p1.6.m6.1.2.2.1.cmml">â€‹</mo><mi id="S3.SS1.p1.6.m6.1.2.2.4" xref="S3.SS1.p1.6.m6.1.2.2.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.6.m6.1.2.2.1b" xref="S3.SS1.p1.6.m6.1.2.2.1.cmml">â€‹</mo><mrow id="S3.SS1.p1.6.m6.1.2.2.5.2" xref="S3.SS1.p1.6.m6.1.2.2.cmml"><mo stretchy="false" id="S3.SS1.p1.6.m6.1.2.2.5.2.1" xref="S3.SS1.p1.6.m6.1.2.2.cmml">(</mo><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">ğ‘¹</mi><mo stretchy="false" id="S3.SS1.p1.6.m6.1.2.2.5.2.2" xref="S3.SS1.p1.6.m6.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.6.m6.1.2.1" xref="S3.SS1.p1.6.m6.1.2.1.cmml">=</mo><mn id="S3.SS1.p1.6.m6.1.2.3" xref="S3.SS1.p1.6.m6.1.2.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.2.cmml" xref="S3.SS1.p1.6.m6.1.2"><eq id="S3.SS1.p1.6.m6.1.2.1.cmml" xref="S3.SS1.p1.6.m6.1.2.1"></eq><apply id="S3.SS1.p1.6.m6.1.2.2.cmml" xref="S3.SS1.p1.6.m6.1.2.2"><times id="S3.SS1.p1.6.m6.1.2.2.1.cmml" xref="S3.SS1.p1.6.m6.1.2.2.1"></times><ci id="S3.SS1.p1.6.m6.1.2.2.2.cmml" xref="S3.SS1.p1.6.m6.1.2.2.2">ğ‘‘</ci><ci id="S3.SS1.p1.6.m6.1.2.2.3.cmml" xref="S3.SS1.p1.6.m6.1.2.2.3">ğ‘’</ci><ci id="S3.SS1.p1.6.m6.1.2.2.4.cmml" xref="S3.SS1.p1.6.m6.1.2.2.4">ğ‘¡</ci><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ‘¹</ci></apply><cn type="integer" id="S3.SS1.p1.6.m6.1.2.3.cmml" xref="S3.SS1.p1.6.m6.1.2.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">det(\bm{R})=1</annotation></semantics></math>.
Since we assume weak perspective cameras, the projection to the camera plane is simply done by removing the depth coordinate, which is expressed as</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\bm{W}_{\mathrm{rep}}=\begin{pmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\end{pmatrix}\bm{R}\bm{X}," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2" xref="S3.E1.m1.2.2.1.1.2.2.cmml">ğ‘¾</mi><mi id="S3.E1.m1.2.2.1.1.2.3" xref="S3.E1.m1.2.2.1.1.2.3.cmml">rep</mi></msub><mo id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.2.1.cmml">(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mtr id="S3.E1.m1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1b" xref="S3.E1.m1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd id="S3.E1.m1.1.1.1.1c" xref="S3.E1.m1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.1.2.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.1.1.1.1d" xref="S3.E1.m1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.3.1.cmml">0</mn></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1e" xref="S3.E1.m1.1.1.1.1.cmml"><mtd id="S3.E1.m1.1.1.1.1f" xref="S3.E1.m1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.2.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.cmml">0</mn></mtd><mtd id="S3.E1.m1.1.1.1.1g" xref="S3.E1.m1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.2.2.1" xref="S3.E1.m1.1.1.1.1.2.2.1.cmml">1</mn></mtd><mtd id="S3.E1.m1.1.1.1.1h" xref="S3.E1.m1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.2.3.1" xref="S3.E1.m1.1.1.1.1.2.3.1.cmml">0</mn></mtd></mtr></mtable><mo id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.2.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.3.1" xref="S3.E1.m1.2.2.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.1.1.3.2" xref="S3.E1.m1.2.2.1.1.3.2.cmml">ğ‘¹</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.3.1a" xref="S3.E1.m1.2.2.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.1.1.3.3" xref="S3.E1.m1.2.2.1.1.3.3.cmml">ğ‘¿</mi></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"></eq><apply id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2">ğ‘¾</ci><ci id="S3.E1.m1.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3">rep</ci></apply><apply id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"><times id="S3.E1.m1.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1"></times><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="latexml" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.3.1">matrix</csymbol><matrix id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><matrixrow id="S3.E1.m1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1.1"><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">1</cn><cn type="integer" id="S3.E1.m1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.2.1">0</cn><cn type="integer" id="S3.E1.m1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.3.1">0</cn></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1.1"><cn type="integer" id="S3.E1.m1.1.1.1.1.2.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1">0</cn><cn type="integer" id="S3.E1.m1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.1">1</cn><cn type="integer" id="S3.E1.m1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.3.1">0</cn></matrixrow></matrix></apply><ci id="S3.E1.m1.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2">ğ‘¹</ci><ci id="S3.E1.m1.2.2.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.3.3">ğ‘¿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\bm{W}_{\mathrm{rep}}=\begin{pmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\end{pmatrix}\bm{R}\bm{X},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.8" class="ltx_p">where <math id="S3.SS1.p1.7.m1.1" class="ltx_Math" alttext="\bm{W}_{rep}\in\mathbb{R}^{2\times j}" display="inline"><semantics id="S3.SS1.p1.7.m1.1a"><mrow id="S3.SS1.p1.7.m1.1.1" xref="S3.SS1.p1.7.m1.1.1.cmml"><msub id="S3.SS1.p1.7.m1.1.1.2" xref="S3.SS1.p1.7.m1.1.1.2.cmml"><mi id="S3.SS1.p1.7.m1.1.1.2.2" xref="S3.SS1.p1.7.m1.1.1.2.2.cmml">ğ‘¾</mi><mrow id="S3.SS1.p1.7.m1.1.1.2.3" xref="S3.SS1.p1.7.m1.1.1.2.3.cmml"><mi id="S3.SS1.p1.7.m1.1.1.2.3.2" xref="S3.SS1.p1.7.m1.1.1.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.7.m1.1.1.2.3.1" xref="S3.SS1.p1.7.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.7.m1.1.1.2.3.3" xref="S3.SS1.p1.7.m1.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.7.m1.1.1.2.3.1a" xref="S3.SS1.p1.7.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.7.m1.1.1.2.3.4" xref="S3.SS1.p1.7.m1.1.1.2.3.4.cmml">p</mi></mrow></msub><mo id="S3.SS1.p1.7.m1.1.1.1" xref="S3.SS1.p1.7.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p1.7.m1.1.1.3" xref="S3.SS1.p1.7.m1.1.1.3.cmml"><mi id="S3.SS1.p1.7.m1.1.1.3.2" xref="S3.SS1.p1.7.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS1.p1.7.m1.1.1.3.3" xref="S3.SS1.p1.7.m1.1.1.3.3.cmml"><mn id="S3.SS1.p1.7.m1.1.1.3.3.2" xref="S3.SS1.p1.7.m1.1.1.3.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.7.m1.1.1.3.3.1" xref="S3.SS1.p1.7.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS1.p1.7.m1.1.1.3.3.3" xref="S3.SS1.p1.7.m1.1.1.3.3.3.cmml">j</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m1.1b"><apply id="S3.SS1.p1.7.m1.1.1.cmml" xref="S3.SS1.p1.7.m1.1.1"><in id="S3.SS1.p1.7.m1.1.1.1.cmml" xref="S3.SS1.p1.7.m1.1.1.1"></in><apply id="S3.SS1.p1.7.m1.1.1.2.cmml" xref="S3.SS1.p1.7.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m1.1.1.2.1.cmml" xref="S3.SS1.p1.7.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.7.m1.1.1.2.2.cmml" xref="S3.SS1.p1.7.m1.1.1.2.2">ğ‘¾</ci><apply id="S3.SS1.p1.7.m1.1.1.2.3.cmml" xref="S3.SS1.p1.7.m1.1.1.2.3"><times id="S3.SS1.p1.7.m1.1.1.2.3.1.cmml" xref="S3.SS1.p1.7.m1.1.1.2.3.1"></times><ci id="S3.SS1.p1.7.m1.1.1.2.3.2.cmml" xref="S3.SS1.p1.7.m1.1.1.2.3.2">ğ‘Ÿ</ci><ci id="S3.SS1.p1.7.m1.1.1.2.3.3.cmml" xref="S3.SS1.p1.7.m1.1.1.2.3.3">ğ‘’</ci><ci id="S3.SS1.p1.7.m1.1.1.2.3.4.cmml" xref="S3.SS1.p1.7.m1.1.1.2.3.4">ğ‘</ci></apply></apply><apply id="S3.SS1.p1.7.m1.1.1.3.cmml" xref="S3.SS1.p1.7.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m1.1.1.3.1.cmml" xref="S3.SS1.p1.7.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.7.m1.1.1.3.2.cmml" xref="S3.SS1.p1.7.m1.1.1.3.2">â„</ci><apply id="S3.SS1.p1.7.m1.1.1.3.3.cmml" xref="S3.SS1.p1.7.m1.1.1.3.3"><times id="S3.SS1.p1.7.m1.1.1.3.3.1.cmml" xref="S3.SS1.p1.7.m1.1.1.3.3.1"></times><cn type="integer" id="S3.SS1.p1.7.m1.1.1.3.3.2.cmml" xref="S3.SS1.p1.7.m1.1.1.3.3.2">2</cn><ci id="S3.SS1.p1.7.m1.1.1.3.3.3.cmml" xref="S3.SS1.p1.7.m1.1.1.3.3.3">ğ‘—</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m1.1c">\bm{W}_{rep}\in\mathbb{R}^{2\times j}</annotation></semantics></math> is called the reprojected 2D pose.
With <math id="S3.SS1.p1.8.m2.1" class="ltx_Math" alttext="\bm{W}" display="inline"><semantics id="S3.SS1.p1.8.m2.1a"><mi id="S3.SS1.p1.8.m2.1.1" xref="S3.SS1.p1.8.m2.1.1.cmml">ğ‘¾</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m2.1b"><ci id="S3.SS1.p1.8.m2.1.1.cmml" xref="S3.SS1.p1.8.m2.1.1">ğ‘¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m2.1c">\bm{W}</annotation></semantics></math> as the input 2D pose we define the <span id="S3.SS1.p1.8.1" class="ltx_text ltx_font_italic">scale-independent reprojection loss</span> as</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\mathcal{L}_{\mathrm{rep}}=\left\|\bm{W}-\frac{\bm{W}_{\mathrm{rep}}}{\|\bm{W}_{\mathrm{rep}}\|_{F}}\right\|_{1}," display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml">â„’</mi><mi id="S3.E2.m1.2.2.1.1.3.3" xref="S3.E2.m1.2.2.1.1.3.3.cmml">rep</mi></msub><mo id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">=</mo><msub id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml">ğ‘¾</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">âˆ’</mo><mfrac id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">ğ‘¾</mi><mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">rep</mi></msub><msub id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.1.cmml">â€–</mo><msub id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">ğ‘¾</mi><mi id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml">rep</mi></msub><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">F</mi></msub></mfrac></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml">1</mn></msub></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></eq><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2">â„’</ci><ci id="S3.E2.m1.2.2.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3">rep</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1">subscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1"></minus><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2">ğ‘¾</ci><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><divide id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1"></divide><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">ğ‘¾</ci><ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">rep</ci></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">ğ‘¾</ci><ci id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">rep</ci></apply></apply><ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">ğ¹</ci></apply></apply></apply></apply><cn type="integer" id="S3.E2.m1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\mathcal{L}_{\mathrm{rep}}=\left\|\bm{W}-\frac{\bm{W}_{\mathrm{rep}}}{\|\bm{W}_{\mathrm{rep}}\|_{F}}\right\|_{1},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.12" class="ltx_p">where <math id="S3.SS1.p1.9.m1.1" class="ltx_math_unparsed" alttext="\|\cdot\|_{1}" display="inline"><semantics id="S3.SS1.p1.9.m1.1a"><mrow id="S3.SS1.p1.9.m1.1b"><mo rspace="0em" id="S3.SS1.p1.9.m1.1.1">âˆ¥</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p1.9.m1.1.2">â‹…</mo><msub id="S3.SS1.p1.9.m1.1.3"><mo lspace="0em" id="S3.SS1.p1.9.m1.1.3.2">âˆ¥</mo><mn id="S3.SS1.p1.9.m1.1.3.3">1</mn></msub></mrow><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m1.1c">\|\cdot\|_{1}</annotation></semantics></math> denotes the <math id="S3.SS1.p1.10.m2.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS1.p1.10.m2.1a"><msub id="S3.SS1.p1.10.m2.1.1" xref="S3.SS1.p1.10.m2.1.1.cmml"><mi id="S3.SS1.p1.10.m2.1.1.2" xref="S3.SS1.p1.10.m2.1.1.2.cmml">L</mi><mn id="S3.SS1.p1.10.m2.1.1.3" xref="S3.SS1.p1.10.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m2.1b"><apply id="S3.SS1.p1.10.m2.1.1.cmml" xref="S3.SS1.p1.10.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m2.1.1.1.cmml" xref="S3.SS1.p1.10.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.10.m2.1.1.2.cmml" xref="S3.SS1.p1.10.m2.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS1.p1.10.m2.1.1.3.cmml" xref="S3.SS1.p1.10.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m2.1c">L_{1}</annotation></semantics></math> norm.
Since the global scale of the 3D pose is unknown and we consider weak perspective projections, scaling the reprojection <math id="S3.SS1.p1.11.m3.1" class="ltx_Math" alttext="\bm{W}_{\mathrm{rep}}" display="inline"><semantics id="S3.SS1.p1.11.m3.1a"><msub id="S3.SS1.p1.11.m3.1.1" xref="S3.SS1.p1.11.m3.1.1.cmml"><mi id="S3.SS1.p1.11.m3.1.1.2" xref="S3.SS1.p1.11.m3.1.1.2.cmml">ğ‘¾</mi><mi id="S3.SS1.p1.11.m3.1.1.3" xref="S3.SS1.p1.11.m3.1.1.3.cmml">rep</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m3.1b"><apply id="S3.SS1.p1.11.m3.1.1.cmml" xref="S3.SS1.p1.11.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m3.1.1.1.cmml" xref="S3.SS1.p1.11.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.11.m3.1.1.2.cmml" xref="S3.SS1.p1.11.m3.1.1.2">ğ‘¾</ci><ci id="S3.SS1.p1.11.m3.1.1.3.cmml" xref="S3.SS1.p1.11.m3.1.1.3">rep</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m3.1c">\bm{W}_{\mathrm{rep}}</annotation></semantics></math> is essential.
Note that the input 2D pose <math id="S3.SS1.p1.12.m4.1" class="ltx_Math" alttext="\bm{W}" display="inline"><semantics id="S3.SS1.p1.12.m4.1a"><mi id="S3.SS1.p1.12.m4.1.1" xref="S3.SS1.p1.12.m4.1.1.cmml">ğ‘¾</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m4.1b"><ci id="S3.SS1.p1.12.m4.1.1.cmml" xref="S3.SS1.p1.12.m4.1.1">ğ‘¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m4.1c">\bm{W}</annotation></semantics></math> is already divided by its Frobenius norm in the preprocessing.
That means both, the input pose and the predicted pose, have the same scale.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.3" class="ltx_p">To ensure that the network predicts a proper rotation, the matrix <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\bm{R}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">ğ‘¹</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ‘¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\bm{R}</annotation></semantics></math> is not predicted directly, but in axis-angle representation.
Let <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="(\theta)" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.2.2"><mo stretchy="false" id="S3.SS1.p2.2.m2.1.2.2.1">(</mo><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">Î¸</mi><mo stretchy="false" id="S3.SS1.p2.2.m2.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">(\theta)</annotation></semantics></math> be a rotational angle and <math id="S3.SS1.p2.3.m3.3" class="ltx_Math" alttext="\bm{\omega}=(\omega_{1},\omega_{2},\omega_{3})" display="inline"><semantics id="S3.SS1.p2.3.m3.3a"><mrow id="S3.SS1.p2.3.m3.3.3" xref="S3.SS1.p2.3.m3.3.3.cmml"><mi id="S3.SS1.p2.3.m3.3.3.5" xref="S3.SS1.p2.3.m3.3.3.5.cmml">ğ</mi><mo id="S3.SS1.p2.3.m3.3.3.4" xref="S3.SS1.p2.3.m3.3.3.4.cmml">=</mo><mrow id="S3.SS1.p2.3.m3.3.3.3.3" xref="S3.SS1.p2.3.m3.3.3.3.4.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.3.3.3.3.4" xref="S3.SS1.p2.3.m3.3.3.3.4.cmml">(</mo><msub id="S3.SS1.p2.3.m3.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.1.1.2.cmml">Ï‰</mi><mn id="S3.SS1.p2.3.m3.1.1.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p2.3.m3.3.3.3.3.5" xref="S3.SS1.p2.3.m3.3.3.3.4.cmml">,</mo><msub id="S3.SS1.p2.3.m3.2.2.2.2.2" xref="S3.SS1.p2.3.m3.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.3.m3.2.2.2.2.2.2" xref="S3.SS1.p2.3.m3.2.2.2.2.2.2.cmml">Ï‰</mi><mn id="S3.SS1.p2.3.m3.2.2.2.2.2.3" xref="S3.SS1.p2.3.m3.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p2.3.m3.3.3.3.3.6" xref="S3.SS1.p2.3.m3.3.3.3.4.cmml">,</mo><msub id="S3.SS1.p2.3.m3.3.3.3.3.3" xref="S3.SS1.p2.3.m3.3.3.3.3.3.cmml"><mi id="S3.SS1.p2.3.m3.3.3.3.3.3.2" xref="S3.SS1.p2.3.m3.3.3.3.3.3.2.cmml">Ï‰</mi><mn id="S3.SS1.p2.3.m3.3.3.3.3.3.3" xref="S3.SS1.p2.3.m3.3.3.3.3.3.3.cmml">3</mn></msub><mo stretchy="false" id="S3.SS1.p2.3.m3.3.3.3.3.7" xref="S3.SS1.p2.3.m3.3.3.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.3b"><apply id="S3.SS1.p2.3.m3.3.3.cmml" xref="S3.SS1.p2.3.m3.3.3"><eq id="S3.SS1.p2.3.m3.3.3.4.cmml" xref="S3.SS1.p2.3.m3.3.3.4"></eq><ci id="S3.SS1.p2.3.m3.3.3.5.cmml" xref="S3.SS1.p2.3.m3.3.3.5">ğ</ci><vector id="S3.SS1.p2.3.m3.3.3.3.4.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3"><apply id="S3.SS1.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.2">ğœ”</ci><cn type="integer" id="S3.SS1.p2.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS1.p2.3.m3.2.2.2.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.3.m3.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2.2.2.2.2">ğœ”</ci><cn type="integer" id="S3.SS1.p2.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.3.m3.2.2.2.2.2.3">2</cn></apply><apply id="S3.SS1.p2.3.m3.3.3.3.3.3.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.3.3.3.3.3.1.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.3">subscript</csymbol><ci id="S3.SS1.p2.3.m3.3.3.3.3.3.2.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.3.2">ğœ”</ci><cn type="integer" id="S3.SS1.p2.3.m3.3.3.3.3.3.3.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.3.3">3</cn></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.3c">\bm{\omega}=(\omega_{1},\omega_{2},\omega_{3})</annotation></semantics></math> denote a rotation axis.
With</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\bm{A}=\begin{pmatrix}0&amp;-\omega_{3}&amp;\omega_{2}\\
\omega_{3}&amp;0&amp;-\omega_{1}\\
-\omega_{2}&amp;\omega_{1}&amp;0\end{pmatrix}" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.2" xref="S3.E3.m1.1.2.cmml"><mi id="S3.E3.m1.1.2.2" xref="S3.E3.m1.1.2.2.cmml">ğ‘¨</mi><mo id="S3.E3.m1.1.2.1" xref="S3.E3.m1.1.2.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.2.cmml"><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.2.1.cmml">(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mtr id="S3.E3.m1.1.1.1.1a" xref="S3.E3.m1.1.1.1.1.cmml"><mtd id="S3.E3.m1.1.1.1.1b" xref="S3.E3.m1.1.1.1.1.cmml"><mn id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">0</mn></mtd><mtd id="S3.E3.m1.1.1.1.1c" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.2.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.2.1a" xref="S3.E3.m1.1.1.1.1.1.2.1.cmml">âˆ’</mo><msub id="S3.E3.m1.1.1.1.1.1.2.1.2" xref="S3.E3.m1.1.1.1.1.1.2.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.2.1.2.2" xref="S3.E3.m1.1.1.1.1.1.2.1.2.2.cmml">Ï‰</mi><mn id="S3.E3.m1.1.1.1.1.1.2.1.2.3" xref="S3.E3.m1.1.1.1.1.1.2.1.2.3.cmml">3</mn></msub></mrow></mtd><mtd id="S3.E3.m1.1.1.1.1d" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.1.3.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.3.1.2" xref="S3.E3.m1.1.1.1.1.1.3.1.2.cmml">Ï‰</mi><mn id="S3.E3.m1.1.1.1.1.1.3.1.3" xref="S3.E3.m1.1.1.1.1.1.3.1.3.cmml">2</mn></msub></mtd></mtr><mtr id="S3.E3.m1.1.1.1.1e" xref="S3.E3.m1.1.1.1.1.cmml"><mtd id="S3.E3.m1.1.1.1.1f" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.2.1.1" xref="S3.E3.m1.1.1.1.1.2.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.2.1.1.2" xref="S3.E3.m1.1.1.1.1.2.1.1.2.cmml">Ï‰</mi><mn id="S3.E3.m1.1.1.1.1.2.1.1.3" xref="S3.E3.m1.1.1.1.1.2.1.1.3.cmml">3</mn></msub></mtd><mtd id="S3.E3.m1.1.1.1.1g" xref="S3.E3.m1.1.1.1.1.cmml"><mn id="S3.E3.m1.1.1.1.1.2.2.1" xref="S3.E3.m1.1.1.1.1.2.2.1.cmml">0</mn></mtd><mtd id="S3.E3.m1.1.1.1.1h" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.2.3.1" xref="S3.E3.m1.1.1.1.1.2.3.1.cmml"><mo id="S3.E3.m1.1.1.1.1.2.3.1a" xref="S3.E3.m1.1.1.1.1.2.3.1.cmml">âˆ’</mo><msub id="S3.E3.m1.1.1.1.1.2.3.1.2" xref="S3.E3.m1.1.1.1.1.2.3.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.2.3.1.2.2" xref="S3.E3.m1.1.1.1.1.2.3.1.2.2.cmml">Ï‰</mi><mn id="S3.E3.m1.1.1.1.1.2.3.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.1.2.3.cmml">1</mn></msub></mrow></mtd></mtr><mtr id="S3.E3.m1.1.1.1.1i" xref="S3.E3.m1.1.1.1.1.cmml"><mtd id="S3.E3.m1.1.1.1.1j" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.3.1.1" xref="S3.E3.m1.1.1.1.1.3.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.3.1.1a" xref="S3.E3.m1.1.1.1.1.3.1.1.cmml">âˆ’</mo><msub id="S3.E3.m1.1.1.1.1.3.1.1.2" xref="S3.E3.m1.1.1.1.1.3.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.3.1.1.2.2" xref="S3.E3.m1.1.1.1.1.3.1.1.2.2.cmml">Ï‰</mi><mn id="S3.E3.m1.1.1.1.1.3.1.1.2.3" xref="S3.E3.m1.1.1.1.1.3.1.1.2.3.cmml">2</mn></msub></mrow></mtd><mtd id="S3.E3.m1.1.1.1.1k" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.3.2.1" xref="S3.E3.m1.1.1.1.1.3.2.1.cmml"><mi id="S3.E3.m1.1.1.1.1.3.2.1.2" xref="S3.E3.m1.1.1.1.1.3.2.1.2.cmml">Ï‰</mi><mn id="S3.E3.m1.1.1.1.1.3.2.1.3" xref="S3.E3.m1.1.1.1.1.3.2.1.3.cmml">1</mn></msub></mtd><mtd id="S3.E3.m1.1.1.1.1l" xref="S3.E3.m1.1.1.1.1.cmml"><mn id="S3.E3.m1.1.1.1.1.3.3.1" xref="S3.E3.m1.1.1.1.1.3.3.1.cmml">0</mn></mtd></mtr></mtable><mo id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.2.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.2.cmml" xref="S3.E3.m1.1.2"><eq id="S3.E3.m1.1.2.1.cmml" xref="S3.E3.m1.1.2.1"></eq><ci id="S3.E3.m1.1.2.2.cmml" xref="S3.E3.m1.1.2.2">ğ‘¨</ci><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.3"><csymbol cd="latexml" id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.3.1">matrix</csymbol><matrix id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><matrixrow id="S3.E3.m1.1.1.1.1a.cmml" xref="S3.E3.m1.1.1.1.1"><cn type="integer" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1">0</cn><apply id="S3.E3.m1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2.1"><minus id="S3.E3.m1.1.1.1.1.1.2.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2.1"></minus><apply id="S3.E3.m1.1.1.1.1.1.2.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.2.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.2.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2.1.2.2">ğœ”</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.1.2.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.2.1.2.3">3</cn></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.3.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.3.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.3.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.3.1.2">ğœ”</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.1.3.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3.1.3">2</cn></apply></matrixrow><matrixrow id="S3.E3.m1.1.1.1.1b.cmml" xref="S3.E3.m1.1.1.1.1"><apply id="S3.E3.m1.1.1.1.1.2.1.1.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.2">ğœ”</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.2.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.3">3</cn></apply><cn type="integer" id="S3.E3.m1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.1">0</cn><apply id="S3.E3.m1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.2.3.1"><minus id="S3.E3.m1.1.1.1.1.2.3.1.1.cmml" xref="S3.E3.m1.1.1.1.1.2.3.1"></minus><apply id="S3.E3.m1.1.1.1.1.2.3.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2.3.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.3.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2.3.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.3.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.3.1.2.2">ğœ”</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.2.3.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3.1.2.3">1</cn></apply></apply></matrixrow><matrixrow id="S3.E3.m1.1.1.1.1c.cmml" xref="S3.E3.m1.1.1.1.1"><apply id="S3.E3.m1.1.1.1.1.3.1.1.cmml" xref="S3.E3.m1.1.1.1.1.3.1.1"><minus id="S3.E3.m1.1.1.1.1.3.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.3.1.1"></minus><apply id="S3.E3.m1.1.1.1.1.3.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.3.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.1.1.2.2">ğœ”</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.3.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.1.1.2.3">2</cn></apply></apply><apply id="S3.E3.m1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.2.1.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.2.1.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2.1.2">ğœ”</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.3.2.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3.2.1.3">1</cn></apply><cn type="integer" id="S3.E3.m1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1">0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\bm{A}=\begin{pmatrix}0&amp;-\omega_{3}&amp;\omega_{2}\\
\omega_{3}&amp;0&amp;-\omega_{1}\\
-\omega_{2}&amp;\omega_{1}&amp;0\end{pmatrix}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.4" class="ltx_p">Rodriguesâ€™ formula is applied to obtain the rotation matrix</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\bm{R}=\bm{I}_{3}+(\sin\theta)\bm{A}+(1-\cos\theta)\bm{A}^{2}." display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.4.cmml">ğ‘¹</mi><mo id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml"><msub id="S3.E4.m1.1.1.1.1.2.4" xref="S3.E4.m1.1.1.1.1.2.4.cmml"><mi id="S3.E4.m1.1.1.1.1.2.4.2" xref="S3.E4.m1.1.1.1.1.2.4.2.cmml">ğ‘°</mi><mn id="S3.E4.m1.1.1.1.1.2.4.3" xref="S3.E4.m1.1.1.1.1.2.4.3.cmml">3</mn></msub><mo id="S3.E4.m1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.2.3.cmml">+</mo><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E4.m1.1.1.1.1.1.1.1.1.1a" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">â¡</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml">Î¸</mi></mrow><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.3.cmml">ğ‘¨</mi></mrow><mo id="S3.E4.m1.1.1.1.1.2.3a" xref="S3.E4.m1.1.1.1.1.2.3.cmml">+</mo><mrow id="S3.E4.m1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.2.2.cmml"><mrow id="S3.E4.m1.1.1.1.1.2.2.1.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.2.2.1.1.2" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.2.2.1.1.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.cmml">1</mn><mo id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.1.cmml">cos</mi><mo lspace="0.167em" id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3a" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.cmml">â¡</mo><mi id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.2.cmml">Î¸</mi></mrow></mrow><mo stretchy="false" id="S3.E4.m1.1.1.1.1.2.2.1.1.3" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.2.2.2.cmml">â€‹</mo><msup id="S3.E4.m1.1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.3.2" xref="S3.E4.m1.1.1.1.1.2.2.3.2.cmml">ğ‘¨</mi><mn id="S3.E4.m1.1.1.1.1.2.2.3.3" xref="S3.E4.m1.1.1.1.1.2.2.3.3.cmml">2</mn></msup></mrow></mrow></mrow><mo lspace="0em" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"></eq><ci id="S3.E4.m1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.4">ğ‘¹</ci><apply id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"><plus id="S3.E4.m1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.3"></plus><apply id="S3.E4.m1.1.1.1.1.2.4.cmml" xref="S3.E4.m1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.4.1.cmml" xref="S3.E4.m1.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.4.2.cmml" xref="S3.E4.m1.1.1.1.1.2.4.2">ğ‘°</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.2.4.3.cmml" xref="S3.E4.m1.1.1.1.1.2.4.3">3</cn></apply><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2"></times><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"><sin id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1"></sin><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2">ğœƒ</ci></apply><ci id="S3.E4.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3">ğ‘¨</ci></apply><apply id="S3.E4.m1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2"><times id="S3.E4.m1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2"></times><apply id="S3.E4.m1.1.1.1.1.2.2.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1"><minus id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1"></minus><cn type="integer" id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2">1</cn><apply id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3"><cos id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.1"></cos><ci id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.2">ğœƒ</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3">superscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3.2">ğ‘¨</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\bm{R}=\bm{I}_{3}+(\sin\theta)\bm{A}+(1-\cos\theta)\bm{A}^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>View-consistency</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">A straight-forward way of ensuring view consistency would be to enforce a loss, such as <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">L</mi><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">L_{2}</annotation></semantics></math>, between the canonical poses predicted by two views.
In theory, that loss should be zero for the correct solution because the same person seen from two different views should have the same canonical pose.
In practice, however, this leads to the lifting network learning 3D poses that are view invariant but no longer in close correspondence to the input pose, preventing the network to converge to plausible solutions in our preliminary experiments.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p">The key insight to the proposed method is that rotations and poses from different views can be mixed to enforce the view consistency as a variant of the previously introduced reprojection objective.
We mix the predicted camera and pose of two views, say view-1 and view-2, by rotating the predicted canonical 3D pose from the source view-1 to the target view-2 by using the rotation from view-2.
For two cameras as in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> there exist four possible combinations of rotations and poses.
The same approach is easily extended to <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">m</annotation></semantics></math> cameras which gives <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="m^{2}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msup id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">m</mi><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ğ‘š</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">m^{2}</annotation></semantics></math> combinations.
During training time all possible combinations are reprojected to the respective cameras.
With this training scheme we enforce multi-view consistency without bias towards trivial solutions.
Note that the lifting network is only applied to a single frame at inference stage and does not need any other inputs.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Confidences</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.4" class="ltx_p">The output of most pretrained 2D joint estimators are 2D heatmaps where each entry indicates the confidence for the presence of the corresponding joint at the associated position in the image.
Commonly, the argmax or soft-argmax is computed and given as input to the following lifting network.
However, this gives an exact joint position independent of the confidence of the 2D detection.
That means uncertain predictions are processed in the same way as certain ones.
We circumvent this problem by two simple modifications.
First, we concatenate the maximum value of each heatmap, which is a surrogate to its confidence, to the 2D pose input vector to our lifting network.
Second, we modify the reprojection error in Eq.Â <a href="#S3.E2" title="In 3.1 Reprojection â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> such that each difference between input and reprojected 2D is linearly weighted with its confidence by</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.4" class="ltx_Math" alttext="\mathcal{L}_{\mathrm{rep,c}}=\left\|\left(\bm{W}-\frac{\bm{W}_{\mathrm{rep}}}{\|\bm{W}_{\mathrm{rep}}\|_{F}}\right)\odot\bm{C}\right\|_{1}," display="block"><semantics id="S3.E5.m1.4a"><mrow id="S3.E5.m1.4.4.1" xref="S3.E5.m1.4.4.1.1.cmml"><mrow id="S3.E5.m1.4.4.1.1" xref="S3.E5.m1.4.4.1.1.cmml"><msub id="S3.E5.m1.4.4.1.1.3" xref="S3.E5.m1.4.4.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.4.4.1.1.3.2" xref="S3.E5.m1.4.4.1.1.3.2.cmml">â„’</mi><mrow id="S3.E5.m1.2.2.2.4" xref="S3.E5.m1.2.2.2.3.cmml"><mi id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml">rep</mi><mo id="S3.E5.m1.2.2.2.4.1" xref="S3.E5.m1.2.2.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.E5.m1.2.2.2.2" xref="S3.E5.m1.2.2.2.2.cmml">c</mi></mrow></msub><mo id="S3.E5.m1.4.4.1.1.2" xref="S3.E5.m1.4.4.1.1.2.cmml">=</mo><msub id="S3.E5.m1.4.4.1.1.1" xref="S3.E5.m1.4.4.1.1.1.cmml"><mrow id="S3.E5.m1.4.4.1.1.1.1.1" xref="S3.E5.m1.4.4.1.1.1.1.2.cmml"><mo id="S3.E5.m1.4.4.1.1.1.1.1.2" xref="S3.E5.m1.4.4.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E5.m1.4.4.1.1.1.1.1.1" xref="S3.E5.m1.4.4.1.1.1.1.1.1.cmml"><mrow id="S3.E5.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">ğ‘¾</mi><mo id="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mfrac id="S3.E5.m1.3.3" xref="S3.E5.m1.3.3.cmml"><msub id="S3.E5.m1.3.3.3" xref="S3.E5.m1.3.3.3.cmml"><mi id="S3.E5.m1.3.3.3.2" xref="S3.E5.m1.3.3.3.2.cmml">ğ‘¾</mi><mi id="S3.E5.m1.3.3.3.3" xref="S3.E5.m1.3.3.3.3.cmml">rep</mi></msub><msub id="S3.E5.m1.3.3.1" xref="S3.E5.m1.3.3.1.cmml"><mrow id="S3.E5.m1.3.3.1.1.1" xref="S3.E5.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.1.1.1.2" xref="S3.E5.m1.3.3.1.1.2.1.cmml">â€–</mo><msub id="S3.E5.m1.3.3.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.cmml"><mi id="S3.E5.m1.3.3.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.2.cmml">ğ‘¾</mi><mi id="S3.E5.m1.3.3.1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.1.1.3.cmml">rep</mi></msub><mo stretchy="false" id="S3.E5.m1.3.3.1.1.1.3" xref="S3.E5.m1.3.3.1.1.2.1.cmml">â€–</mo></mrow><mi id="S3.E5.m1.3.3.1.3" xref="S3.E5.m1.3.3.1.3.cmml">F</mi></msub></mfrac></mrow><mo rspace="0.055em" id="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E5.m1.4.4.1.1.1.1.1.1.2" xref="S3.E5.m1.4.4.1.1.1.1.1.1.2.cmml">âŠ™</mo><mi id="S3.E5.m1.4.4.1.1.1.1.1.1.3" xref="S3.E5.m1.4.4.1.1.1.1.1.1.3.cmml">ğ‘ª</mi></mrow><mo id="S3.E5.m1.4.4.1.1.1.1.1.3" xref="S3.E5.m1.4.4.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E5.m1.4.4.1.1.1.3" xref="S3.E5.m1.4.4.1.1.1.3.cmml">1</mn></msub></mrow><mo id="S3.E5.m1.4.4.1.2" xref="S3.E5.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.4b"><apply id="S3.E5.m1.4.4.1.1.cmml" xref="S3.E5.m1.4.4.1"><eq id="S3.E5.m1.4.4.1.1.2.cmml" xref="S3.E5.m1.4.4.1.1.2"></eq><apply id="S3.E5.m1.4.4.1.1.3.cmml" xref="S3.E5.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.1.1.3.1.cmml" xref="S3.E5.m1.4.4.1.1.3">subscript</csymbol><ci id="S3.E5.m1.4.4.1.1.3.2.cmml" xref="S3.E5.m1.4.4.1.1.3.2">â„’</ci><list id="S3.E5.m1.2.2.2.3.cmml" xref="S3.E5.m1.2.2.2.4"><ci id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1">rep</ci><ci id="S3.E5.m1.2.2.2.2.cmml" xref="S3.E5.m1.2.2.2.2">c</ci></list></apply><apply id="S3.E5.m1.4.4.1.1.1.cmml" xref="S3.E5.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.1.1.1.2.cmml" xref="S3.E5.m1.4.4.1.1.1">subscript</csymbol><apply id="S3.E5.m1.4.4.1.1.1.1.2.cmml" xref="S3.E5.m1.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.E5.m1.4.4.1.1.1.1.1.2">norm</csymbol><apply id="S3.E5.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E5.m1.4.4.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.4.4.1.1.1.1.1.1.2">direct-product</csymbol><apply id="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.4.4.1.1.1.1.1.1.1.1"><minus id="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.4.4.1.1.1.1.1.1.1.1.1.2">ğ‘¾</ci><apply id="S3.E5.m1.3.3.cmml" xref="S3.E5.m1.3.3"><divide id="S3.E5.m1.3.3.2.cmml" xref="S3.E5.m1.3.3"></divide><apply id="S3.E5.m1.3.3.3.cmml" xref="S3.E5.m1.3.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.3.1.cmml" xref="S3.E5.m1.3.3.3">subscript</csymbol><ci id="S3.E5.m1.3.3.3.2.cmml" xref="S3.E5.m1.3.3.3.2">ğ‘¾</ci><ci id="S3.E5.m1.3.3.3.3.cmml" xref="S3.E5.m1.3.3.3.3">rep</ci></apply><apply id="S3.E5.m1.3.3.1.cmml" xref="S3.E5.m1.3.3.1"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.2.cmml" xref="S3.E5.m1.3.3.1">subscript</csymbol><apply id="S3.E5.m1.3.3.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.3.3.1.1.2.1.cmml" xref="S3.E5.m1.3.3.1.1.1.2">norm</csymbol><apply id="S3.E5.m1.3.3.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.2">ğ‘¾</ci><ci id="S3.E5.m1.3.3.1.1.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.3">rep</ci></apply></apply><ci id="S3.E5.m1.3.3.1.3.cmml" xref="S3.E5.m1.3.3.1.3">ğ¹</ci></apply></apply></apply><ci id="S3.E5.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.4.4.1.1.1.1.1.1.3">ğ‘ª</ci></apply></apply><cn type="integer" id="S3.E5.m1.4.4.1.1.1.3.cmml" xref="S3.E5.m1.4.4.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.4c">\mathcal{L}_{\mathrm{rep,c}}=\left\|\left(\bm{W}-\frac{\bm{W}_{\mathrm{rep}}}{\|\bm{W}_{\mathrm{rep}}\|_{F}}\right)\odot\bm{C}\right\|_{1},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.5" class="ltx_p">where</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.1" class="ltx_Math" alttext="\bm{C}=\begin{pmatrix}c_{1}&amp;c_{2}&amp;\dots&amp;c_{j}\\
c_{1}&amp;c_{2}&amp;\dots&amp;c_{j}\\
\end{pmatrix}" display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.2" xref="S3.E6.m1.1.2.cmml"><mi id="S3.E6.m1.1.2.2" xref="S3.E6.m1.1.2.2.cmml">ğ‘ª</mi><mo id="S3.E6.m1.1.2.1" xref="S3.E6.m1.1.2.1.cmml">=</mo><mrow id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.2.cmml"><mo id="S3.E6.m1.1.1.3.1" xref="S3.E6.m1.1.1.2.1.cmml">(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mtr id="S3.E6.m1.1.1.1.1a" xref="S3.E6.m1.1.1.1.1.cmml"><mtd id="S3.E6.m1.1.1.1.1b" xref="S3.E6.m1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.2.cmml">c</mi><mn id="S3.E6.m1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.3.cmml">1</mn></msub></mtd><mtd id="S3.E6.m1.1.1.1.1c" xref="S3.E6.m1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.1.2.1" xref="S3.E6.m1.1.1.1.1.1.2.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.2.1.2" xref="S3.E6.m1.1.1.1.1.1.2.1.2.cmml">c</mi><mn id="S3.E6.m1.1.1.1.1.1.2.1.3" xref="S3.E6.m1.1.1.1.1.1.2.1.3.cmml">2</mn></msub></mtd><mtd id="S3.E6.m1.1.1.1.1d" xref="S3.E6.m1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E6.m1.1.1.1.1.1.3.1" xref="S3.E6.m1.1.1.1.1.1.3.1.cmml">â€¦</mi></mtd><mtd id="S3.E6.m1.1.1.1.1e" xref="S3.E6.m1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.1.4.1" xref="S3.E6.m1.1.1.1.1.1.4.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.4.1.2" xref="S3.E6.m1.1.1.1.1.1.4.1.2.cmml">c</mi><mi id="S3.E6.m1.1.1.1.1.1.4.1.3" xref="S3.E6.m1.1.1.1.1.1.4.1.3.cmml">j</mi></msub></mtd></mtr><mtr id="S3.E6.m1.1.1.1.1f" xref="S3.E6.m1.1.1.1.1.cmml"><mtd id="S3.E6.m1.1.1.1.1g" xref="S3.E6.m1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.2.1.1" xref="S3.E6.m1.1.1.1.1.2.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.2.1.1.2" xref="S3.E6.m1.1.1.1.1.2.1.1.2.cmml">c</mi><mn id="S3.E6.m1.1.1.1.1.2.1.1.3" xref="S3.E6.m1.1.1.1.1.2.1.1.3.cmml">1</mn></msub></mtd><mtd id="S3.E6.m1.1.1.1.1h" xref="S3.E6.m1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.2.2.1" xref="S3.E6.m1.1.1.1.1.2.2.1.cmml"><mi id="S3.E6.m1.1.1.1.1.2.2.1.2" xref="S3.E6.m1.1.1.1.1.2.2.1.2.cmml">c</mi><mn id="S3.E6.m1.1.1.1.1.2.2.1.3" xref="S3.E6.m1.1.1.1.1.2.2.1.3.cmml">2</mn></msub></mtd><mtd id="S3.E6.m1.1.1.1.1i" xref="S3.E6.m1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E6.m1.1.1.1.1.2.3.1" xref="S3.E6.m1.1.1.1.1.2.3.1.cmml">â€¦</mi></mtd><mtd id="S3.E6.m1.1.1.1.1j" xref="S3.E6.m1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.2.4.1" xref="S3.E6.m1.1.1.1.1.2.4.1.cmml"><mi id="S3.E6.m1.1.1.1.1.2.4.1.2" xref="S3.E6.m1.1.1.1.1.2.4.1.2.cmml">c</mi><mi id="S3.E6.m1.1.1.1.1.2.4.1.3" xref="S3.E6.m1.1.1.1.1.2.4.1.3.cmml">j</mi></msub></mtd></mtr></mtable><mo id="S3.E6.m1.1.1.3.2" xref="S3.E6.m1.1.1.2.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.2.cmml" xref="S3.E6.m1.1.2"><eq id="S3.E6.m1.1.2.1.cmml" xref="S3.E6.m1.1.2.1"></eq><ci id="S3.E6.m1.1.2.2.cmml" xref="S3.E6.m1.1.2.2">ğ‘ª</ci><apply id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.3"><csymbol cd="latexml" id="S3.E6.m1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.3.1">matrix</csymbol><matrix id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1"><matrixrow id="S3.E6.m1.1.1.1.1a.cmml" xref="S3.E6.m1.1.1.1.1"><apply id="S3.E6.m1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.2">ğ‘</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.E6.m1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.2.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.2.1">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.2.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.2.1.2">ğ‘</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.1.2.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.2.1.3">2</cn></apply><ci id="S3.E6.m1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.3.1">â€¦</ci><apply id="S3.E6.m1.1.1.1.1.1.4.1.cmml" xref="S3.E6.m1.1.1.1.1.1.4.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.4.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.4.1">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.4.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.4.1.2">ğ‘</ci><ci id="S3.E6.m1.1.1.1.1.1.4.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.4.1.3">ğ‘—</ci></apply></matrixrow><matrixrow id="S3.E6.m1.1.1.1.1b.cmml" xref="S3.E6.m1.1.1.1.1"><apply id="S3.E6.m1.1.1.1.1.2.1.1.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.2.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.2">ğ‘</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.2.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.3">1</cn></apply><apply id="S3.E6.m1.1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.2.1.1.cmml" xref="S3.E6.m1.1.1.1.1.2.2.1">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.2.2.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2.2.1.2">ğ‘</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.2.2.1.3.cmml" xref="S3.E6.m1.1.1.1.1.2.2.1.3">2</cn></apply><ci id="S3.E6.m1.1.1.1.1.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.2.3.1">â€¦</ci><apply id="S3.E6.m1.1.1.1.1.2.4.1.cmml" xref="S3.E6.m1.1.1.1.1.2.4.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.4.1.1.cmml" xref="S3.E6.m1.1.1.1.1.2.4.1">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.2.4.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2.4.1.2">ğ‘</ci><ci id="S3.E6.m1.1.1.1.1.2.4.1.3.cmml" xref="S3.E6.m1.1.1.1.1.2.4.1.3">ğ‘—</ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\bm{C}=\begin{pmatrix}c_{1}&amp;c_{2}&amp;\dots&amp;c_{j}\\
c_{1}&amp;c_{2}&amp;\dots&amp;c_{j}\\
\end{pmatrix}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.3" class="ltx_p">with <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">ğ‘</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">c_{i}</annotation></semantics></math> as the maximum value of the heatmap for joint <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">i</annotation></semantics></math> and <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mo id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">âŠ™</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><csymbol cd="latexml" id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\odot</annotation></semantics></math> as the Hadamard product.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Camera-consistency</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">A reasonable assumption for many practical motion capture setups is that cameras are static during recording a sequence, <span id="S3.SS4.p1.1.1" class="ltx_ERROR undefined">\ie</span>they do not change their position or orientation.
This is the case for the Human3.6M<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In fact, camera angles change between subjects but not during a capture session with one subject.</span></span></span> and 3DHP dataset.
However, this assumption is not mandatory for our proposed method, but an enhancement for scenes with static cameras.
We will show the effect of this optional improvement in the experiments as well as the performance of our approach without it on the SkiPose dataset that contains moving cameras.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.4" class="ltx_p">For a static camera setup all relative rotations between the cameras are equal.
An intuitive approach to enforce static cameras is to calculate an <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><msub id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">L</mi><mn id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">L_{2}</annotation></semantics></math>-loss between the relative rotations over one batch of training samples.
However, a batch-wise loss leads to degraded solutions or had no effect if its weight was set to a low value.
This observation is similar to the findings regarding the canonical pose equality in Sec.Â <a href="#S3.SS2" title="3.2 View-consistency â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
For this reason we propose a similar mixing approach as in Sec.Â <a href="#S3.SS2" title="3.2 View-consistency â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, now over estimates from different samples in one batch.
A relative rotation <math id="S3.SS4.p2.2.m2.2" class="ltx_Math" alttext="\bm{R}_{1,2}" display="inline"><semantics id="S3.SS4.p2.2.m2.2a"><msub id="S3.SS4.p2.2.m2.2.3" xref="S3.SS4.p2.2.m2.2.3.cmml"><mi id="S3.SS4.p2.2.m2.2.3.2" xref="S3.SS4.p2.2.m2.2.3.2.cmml">ğ‘¹</mi><mrow id="S3.SS4.p2.2.m2.2.2.2.4" xref="S3.SS4.p2.2.m2.2.2.2.3.cmml"><mn id="S3.SS4.p2.2.m2.1.1.1.1" xref="S3.SS4.p2.2.m2.1.1.1.1.cmml">1</mn><mo id="S3.SS4.p2.2.m2.2.2.2.4.1" xref="S3.SS4.p2.2.m2.2.2.2.3.cmml">,</mo><mn id="S3.SS4.p2.2.m2.2.2.2.2" xref="S3.SS4.p2.2.m2.2.2.2.2.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.2b"><apply id="S3.SS4.p2.2.m2.2.3.cmml" xref="S3.SS4.p2.2.m2.2.3"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.2.3.1.cmml" xref="S3.SS4.p2.2.m2.2.3">subscript</csymbol><ci id="S3.SS4.p2.2.m2.2.3.2.cmml" xref="S3.SS4.p2.2.m2.2.3.2">ğ‘¹</ci><list id="S3.SS4.p2.2.m2.2.2.2.3.cmml" xref="S3.SS4.p2.2.m2.2.2.2.4"><cn type="integer" id="S3.SS4.p2.2.m2.1.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1.1.1">1</cn><cn type="integer" id="S3.SS4.p2.2.m2.2.2.2.2.cmml" xref="S3.SS4.p2.2.m2.2.2.2.2">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.2c">\bm{R}_{1,2}</annotation></semantics></math> using the rotation matrices <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="\bm{R}_{1}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><msub id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">ğ‘¹</mi><mn id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">ğ‘¹</ci><cn type="integer" id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">\bm{R}_{1}</annotation></semantics></math> and <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="\bm{R}_{2}" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><msub id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">ğ‘¹</mi><mn id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">ğ‘¹</ci><cn type="integer" id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">\bm{R}_{2}</annotation></semantics></math> from view-1 to view-2 respectively, is defined by</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.3" class="ltx_Math" alttext="\bm{R}_{1,2}=\bm{R}_{2}\bm{R}_{1}^{T}." display="block"><semantics id="S3.E7.m1.3a"><mrow id="S3.E7.m1.3.3.1" xref="S3.E7.m1.3.3.1.1.cmml"><mrow id="S3.E7.m1.3.3.1.1" xref="S3.E7.m1.3.3.1.1.cmml"><msub id="S3.E7.m1.3.3.1.1.2" xref="S3.E7.m1.3.3.1.1.2.cmml"><mi id="S3.E7.m1.3.3.1.1.2.2" xref="S3.E7.m1.3.3.1.1.2.2.cmml">ğ‘¹</mi><mrow id="S3.E7.m1.2.2.2.4" xref="S3.E7.m1.2.2.2.3.cmml"><mn id="S3.E7.m1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml">1</mn><mo id="S3.E7.m1.2.2.2.4.1" xref="S3.E7.m1.2.2.2.3.cmml">,</mo><mn id="S3.E7.m1.2.2.2.2" xref="S3.E7.m1.2.2.2.2.cmml">2</mn></mrow></msub><mo id="S3.E7.m1.3.3.1.1.1" xref="S3.E7.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E7.m1.3.3.1.1.3" xref="S3.E7.m1.3.3.1.1.3.cmml"><msub id="S3.E7.m1.3.3.1.1.3.2" xref="S3.E7.m1.3.3.1.1.3.2.cmml"><mi id="S3.E7.m1.3.3.1.1.3.2.2" xref="S3.E7.m1.3.3.1.1.3.2.2.cmml">ğ‘¹</mi><mn id="S3.E7.m1.3.3.1.1.3.2.3" xref="S3.E7.m1.3.3.1.1.3.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.3.3.1.1.3.1" xref="S3.E7.m1.3.3.1.1.3.1.cmml">â€‹</mo><msubsup id="S3.E7.m1.3.3.1.1.3.3" xref="S3.E7.m1.3.3.1.1.3.3.cmml"><mi id="S3.E7.m1.3.3.1.1.3.3.2.2" xref="S3.E7.m1.3.3.1.1.3.3.2.2.cmml">ğ‘¹</mi><mn id="S3.E7.m1.3.3.1.1.3.3.2.3" xref="S3.E7.m1.3.3.1.1.3.3.2.3.cmml">1</mn><mi id="S3.E7.m1.3.3.1.1.3.3.3" xref="S3.E7.m1.3.3.1.1.3.3.3.cmml">T</mi></msubsup></mrow></mrow><mo lspace="0em" id="S3.E7.m1.3.3.1.2" xref="S3.E7.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.3b"><apply id="S3.E7.m1.3.3.1.1.cmml" xref="S3.E7.m1.3.3.1"><eq id="S3.E7.m1.3.3.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.1"></eq><apply id="S3.E7.m1.3.3.1.1.2.cmml" xref="S3.E7.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.2.1.cmml" xref="S3.E7.m1.3.3.1.1.2">subscript</csymbol><ci id="S3.E7.m1.3.3.1.1.2.2.cmml" xref="S3.E7.m1.3.3.1.1.2.2">ğ‘¹</ci><list id="S3.E7.m1.2.2.2.3.cmml" xref="S3.E7.m1.2.2.2.4"><cn type="integer" id="S3.E7.m1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1">1</cn><cn type="integer" id="S3.E7.m1.2.2.2.2.cmml" xref="S3.E7.m1.2.2.2.2">2</cn></list></apply><apply id="S3.E7.m1.3.3.1.1.3.cmml" xref="S3.E7.m1.3.3.1.1.3"><times id="S3.E7.m1.3.3.1.1.3.1.cmml" xref="S3.E7.m1.3.3.1.1.3.1"></times><apply id="S3.E7.m1.3.3.1.1.3.2.cmml" xref="S3.E7.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.3.2.1.cmml" xref="S3.E7.m1.3.3.1.1.3.2">subscript</csymbol><ci id="S3.E7.m1.3.3.1.1.3.2.2.cmml" xref="S3.E7.m1.3.3.1.1.3.2.2">ğ‘¹</ci><cn type="integer" id="S3.E7.m1.3.3.1.1.3.2.3.cmml" xref="S3.E7.m1.3.3.1.1.3.2.3">2</cn></apply><apply id="S3.E7.m1.3.3.1.1.3.3.cmml" xref="S3.E7.m1.3.3.1.1.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.3.3.1.cmml" xref="S3.E7.m1.3.3.1.1.3.3">superscript</csymbol><apply id="S3.E7.m1.3.3.1.1.3.3.2.cmml" xref="S3.E7.m1.3.3.1.1.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.3.3.2.1.cmml" xref="S3.E7.m1.3.3.1.1.3.3">subscript</csymbol><ci id="S3.E7.m1.3.3.1.1.3.3.2.2.cmml" xref="S3.E7.m1.3.3.1.1.3.3.2.2">ğ‘¹</ci><cn type="integer" id="S3.E7.m1.3.3.1.1.3.3.2.3.cmml" xref="S3.E7.m1.3.3.1.1.3.3.2.3">1</cn></apply><ci id="S3.E7.m1.3.3.1.1.3.3.3.cmml" xref="S3.E7.m1.3.3.1.1.3.3.3">ğ‘‡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.3c">\bm{R}_{1,2}=\bm{R}_{2}\bm{R}_{1}^{T}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p2.6" class="ltx_p">Let <math id="S3.SS4.p2.5.m1.3" class="ltx_Math" alttext="\bm{R}_{1,2}^{(s)}" display="inline"><semantics id="S3.SS4.p2.5.m1.3a"><msubsup id="S3.SS4.p2.5.m1.3.4" xref="S3.SS4.p2.5.m1.3.4.cmml"><mi id="S3.SS4.p2.5.m1.3.4.2.2" xref="S3.SS4.p2.5.m1.3.4.2.2.cmml">ğ‘¹</mi><mrow id="S3.SS4.p2.5.m1.2.2.2.4" xref="S3.SS4.p2.5.m1.2.2.2.3.cmml"><mn id="S3.SS4.p2.5.m1.1.1.1.1" xref="S3.SS4.p2.5.m1.1.1.1.1.cmml">1</mn><mo id="S3.SS4.p2.5.m1.2.2.2.4.1" xref="S3.SS4.p2.5.m1.2.2.2.3.cmml">,</mo><mn id="S3.SS4.p2.5.m1.2.2.2.2" xref="S3.SS4.p2.5.m1.2.2.2.2.cmml">2</mn></mrow><mrow id="S3.SS4.p2.5.m1.3.3.1.3" xref="S3.SS4.p2.5.m1.3.4.cmml"><mo stretchy="false" id="S3.SS4.p2.5.m1.3.3.1.3.1" xref="S3.SS4.p2.5.m1.3.4.cmml">(</mo><mi id="S3.SS4.p2.5.m1.3.3.1.1" xref="S3.SS4.p2.5.m1.3.3.1.1.cmml">s</mi><mo stretchy="false" id="S3.SS4.p2.5.m1.3.3.1.3.2" xref="S3.SS4.p2.5.m1.3.4.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m1.3b"><apply id="S3.SS4.p2.5.m1.3.4.cmml" xref="S3.SS4.p2.5.m1.3.4"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m1.3.4.1.cmml" xref="S3.SS4.p2.5.m1.3.4">superscript</csymbol><apply id="S3.SS4.p2.5.m1.3.4.2.cmml" xref="S3.SS4.p2.5.m1.3.4"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m1.3.4.2.1.cmml" xref="S3.SS4.p2.5.m1.3.4">subscript</csymbol><ci id="S3.SS4.p2.5.m1.3.4.2.2.cmml" xref="S3.SS4.p2.5.m1.3.4.2.2">ğ‘¹</ci><list id="S3.SS4.p2.5.m1.2.2.2.3.cmml" xref="S3.SS4.p2.5.m1.2.2.2.4"><cn type="integer" id="S3.SS4.p2.5.m1.1.1.1.1.cmml" xref="S3.SS4.p2.5.m1.1.1.1.1">1</cn><cn type="integer" id="S3.SS4.p2.5.m1.2.2.2.2.cmml" xref="S3.SS4.p2.5.m1.2.2.2.2">2</cn></list></apply><ci id="S3.SS4.p2.5.m1.3.3.1.1.cmml" xref="S3.SS4.p2.5.m1.3.3.1.1">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m1.3c">\bm{R}_{1,2}^{(s)}</annotation></semantics></math> be the predicted relative rotation between view-1 and view-2 of sample <math id="S3.SS4.p2.6.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS4.p2.6.m2.1a"><mi id="S3.SS4.p2.6.m2.1.1" xref="S3.SS4.p2.6.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m2.1b"><ci id="S3.SS4.p2.6.m2.1.1.cmml" xref="S3.SS4.p2.6.m2.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m2.1c">s</annotation></semantics></math>.
We then randomly permute these relative rotations in the batch and use them to reproject the canonical poses similar to Eq.Â <a href="#S3.E1" title="In 3.1 Reprojection â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></p>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.7" class="ltx_Math" alttext="\bm{W}_{\mathrm{rep}}=\begin{pmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\end{pmatrix}\bm{R}_{1,2}^{(s)}\bm{R}_{1}^{(s^{\prime})}\bm{X}^{(s^{\prime})}," display="block"><semantics id="S3.E8.m1.7a"><mrow id="S3.E8.m1.7.7.1" xref="S3.E8.m1.7.7.1.1.cmml"><mrow id="S3.E8.m1.7.7.1.1" xref="S3.E8.m1.7.7.1.1.cmml"><msub id="S3.E8.m1.7.7.1.1.2" xref="S3.E8.m1.7.7.1.1.2.cmml"><mi id="S3.E8.m1.7.7.1.1.2.2" xref="S3.E8.m1.7.7.1.1.2.2.cmml">ğ‘¾</mi><mi id="S3.E8.m1.7.7.1.1.2.3" xref="S3.E8.m1.7.7.1.1.2.3.cmml">rep</mi></msub><mo id="S3.E8.m1.7.7.1.1.1" xref="S3.E8.m1.7.7.1.1.1.cmml">=</mo><mrow id="S3.E8.m1.7.7.1.1.3" xref="S3.E8.m1.7.7.1.1.3.cmml"><mrow id="S3.E8.m1.1.1.3" xref="S3.E8.m1.1.1.2.cmml"><mo id="S3.E8.m1.1.1.3.1" xref="S3.E8.m1.1.1.2.1.cmml">(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E8.m1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.cmml"><mtr id="S3.E8.m1.1.1.1.1a" xref="S3.E8.m1.1.1.1.1.cmml"><mtd id="S3.E8.m1.1.1.1.1b" xref="S3.E8.m1.1.1.1.1.cmml"><mn id="S3.E8.m1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd id="S3.E8.m1.1.1.1.1c" xref="S3.E8.m1.1.1.1.1.cmml"><mn id="S3.E8.m1.1.1.1.1.1.2.1" xref="S3.E8.m1.1.1.1.1.1.2.1.cmml">0</mn></mtd><mtd id="S3.E8.m1.1.1.1.1d" xref="S3.E8.m1.1.1.1.1.cmml"><mn id="S3.E8.m1.1.1.1.1.1.3.1" xref="S3.E8.m1.1.1.1.1.1.3.1.cmml">0</mn></mtd></mtr><mtr id="S3.E8.m1.1.1.1.1e" xref="S3.E8.m1.1.1.1.1.cmml"><mtd id="S3.E8.m1.1.1.1.1f" xref="S3.E8.m1.1.1.1.1.cmml"><mn id="S3.E8.m1.1.1.1.1.2.1.1" xref="S3.E8.m1.1.1.1.1.2.1.1.cmml">0</mn></mtd><mtd id="S3.E8.m1.1.1.1.1g" xref="S3.E8.m1.1.1.1.1.cmml"><mn id="S3.E8.m1.1.1.1.1.2.2.1" xref="S3.E8.m1.1.1.1.1.2.2.1.cmml">1</mn></mtd><mtd id="S3.E8.m1.1.1.1.1h" xref="S3.E8.m1.1.1.1.1.cmml"><mn id="S3.E8.m1.1.1.1.1.2.3.1" xref="S3.E8.m1.1.1.1.1.2.3.1.cmml">0</mn></mtd></mtr></mtable><mo id="S3.E8.m1.1.1.3.2" xref="S3.E8.m1.1.1.2.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E8.m1.7.7.1.1.3.1" xref="S3.E8.m1.7.7.1.1.3.1.cmml">â€‹</mo><msubsup id="S3.E8.m1.7.7.1.1.3.2" xref="S3.E8.m1.7.7.1.1.3.2.cmml"><mi id="S3.E8.m1.7.7.1.1.3.2.2.2" xref="S3.E8.m1.7.7.1.1.3.2.2.2.cmml">ğ‘¹</mi><mrow id="S3.E8.m1.3.3.2.4" xref="S3.E8.m1.3.3.2.3.cmml"><mn id="S3.E8.m1.2.2.1.1" xref="S3.E8.m1.2.2.1.1.cmml">1</mn><mo id="S3.E8.m1.3.3.2.4.1" xref="S3.E8.m1.3.3.2.3.cmml">,</mo><mn id="S3.E8.m1.3.3.2.2" xref="S3.E8.m1.3.3.2.2.cmml">2</mn></mrow><mrow id="S3.E8.m1.4.4.1.3" xref="S3.E8.m1.7.7.1.1.3.2.cmml"><mo stretchy="false" id="S3.E8.m1.4.4.1.3.1" xref="S3.E8.m1.7.7.1.1.3.2.cmml">(</mo><mi id="S3.E8.m1.4.4.1.1" xref="S3.E8.m1.4.4.1.1.cmml">s</mi><mo stretchy="false" id="S3.E8.m1.4.4.1.3.2" xref="S3.E8.m1.7.7.1.1.3.2.cmml">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em" id="S3.E8.m1.7.7.1.1.3.1a" xref="S3.E8.m1.7.7.1.1.3.1.cmml">â€‹</mo><msubsup id="S3.E8.m1.7.7.1.1.3.3" xref="S3.E8.m1.7.7.1.1.3.3.cmml"><mi id="S3.E8.m1.7.7.1.1.3.3.2.2" xref="S3.E8.m1.7.7.1.1.3.3.2.2.cmml">ğ‘¹</mi><mn id="S3.E8.m1.7.7.1.1.3.3.2.3" xref="S3.E8.m1.7.7.1.1.3.3.2.3.cmml">1</mn><mrow id="S3.E8.m1.5.5.1.1" xref="S3.E8.m1.5.5.1.1.1.cmml"><mo stretchy="false" id="S3.E8.m1.5.5.1.1.2" xref="S3.E8.m1.5.5.1.1.1.cmml">(</mo><msup id="S3.E8.m1.5.5.1.1.1" xref="S3.E8.m1.5.5.1.1.1.cmml"><mi id="S3.E8.m1.5.5.1.1.1.2" xref="S3.E8.m1.5.5.1.1.1.2.cmml">s</mi><mo id="S3.E8.m1.5.5.1.1.1.3" xref="S3.E8.m1.5.5.1.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="S3.E8.m1.5.5.1.1.3" xref="S3.E8.m1.5.5.1.1.1.cmml">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em" id="S3.E8.m1.7.7.1.1.3.1b" xref="S3.E8.m1.7.7.1.1.3.1.cmml">â€‹</mo><msup id="S3.E8.m1.7.7.1.1.3.4" xref="S3.E8.m1.7.7.1.1.3.4.cmml"><mi id="S3.E8.m1.7.7.1.1.3.4.2" xref="S3.E8.m1.7.7.1.1.3.4.2.cmml">ğ‘¿</mi><mrow id="S3.E8.m1.6.6.1.1" xref="S3.E8.m1.6.6.1.1.1.cmml"><mo stretchy="false" id="S3.E8.m1.6.6.1.1.2" xref="S3.E8.m1.6.6.1.1.1.cmml">(</mo><msup id="S3.E8.m1.6.6.1.1.1" xref="S3.E8.m1.6.6.1.1.1.cmml"><mi id="S3.E8.m1.6.6.1.1.1.2" xref="S3.E8.m1.6.6.1.1.1.2.cmml">s</mi><mo id="S3.E8.m1.6.6.1.1.1.3" xref="S3.E8.m1.6.6.1.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="S3.E8.m1.6.6.1.1.3" xref="S3.E8.m1.6.6.1.1.1.cmml">)</mo></mrow></msup></mrow></mrow><mo id="S3.E8.m1.7.7.1.2" xref="S3.E8.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.7b"><apply id="S3.E8.m1.7.7.1.1.cmml" xref="S3.E8.m1.7.7.1"><eq id="S3.E8.m1.7.7.1.1.1.cmml" xref="S3.E8.m1.7.7.1.1.1"></eq><apply id="S3.E8.m1.7.7.1.1.2.cmml" xref="S3.E8.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.7.7.1.1.2.1.cmml" xref="S3.E8.m1.7.7.1.1.2">subscript</csymbol><ci id="S3.E8.m1.7.7.1.1.2.2.cmml" xref="S3.E8.m1.7.7.1.1.2.2">ğ‘¾</ci><ci id="S3.E8.m1.7.7.1.1.2.3.cmml" xref="S3.E8.m1.7.7.1.1.2.3">rep</ci></apply><apply id="S3.E8.m1.7.7.1.1.3.cmml" xref="S3.E8.m1.7.7.1.1.3"><times id="S3.E8.m1.7.7.1.1.3.1.cmml" xref="S3.E8.m1.7.7.1.1.3.1"></times><apply id="S3.E8.m1.1.1.2.cmml" xref="S3.E8.m1.1.1.3"><csymbol cd="latexml" id="S3.E8.m1.1.1.2.1.cmml" xref="S3.E8.m1.1.1.3.1">matrix</csymbol><matrix id="S3.E8.m1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1"><matrixrow id="S3.E8.m1.1.1.1.1a.cmml" xref="S3.E8.m1.1.1.1.1"><cn type="integer" id="S3.E8.m1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1">1</cn><cn type="integer" id="S3.E8.m1.1.1.1.1.1.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.2.1">0</cn><cn type="integer" id="S3.E8.m1.1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.1.1.1.3.1">0</cn></matrixrow><matrixrow id="S3.E8.m1.1.1.1.1b.cmml" xref="S3.E8.m1.1.1.1.1"><cn type="integer" id="S3.E8.m1.1.1.1.1.2.1.1.cmml" xref="S3.E8.m1.1.1.1.1.2.1.1">0</cn><cn type="integer" id="S3.E8.m1.1.1.1.1.2.2.1.cmml" xref="S3.E8.m1.1.1.1.1.2.2.1">1</cn><cn type="integer" id="S3.E8.m1.1.1.1.1.2.3.1.cmml" xref="S3.E8.m1.1.1.1.1.2.3.1">0</cn></matrixrow></matrix></apply><apply id="S3.E8.m1.7.7.1.1.3.2.cmml" xref="S3.E8.m1.7.7.1.1.3.2"><csymbol cd="ambiguous" id="S3.E8.m1.7.7.1.1.3.2.1.cmml" xref="S3.E8.m1.7.7.1.1.3.2">superscript</csymbol><apply id="S3.E8.m1.7.7.1.1.3.2.2.cmml" xref="S3.E8.m1.7.7.1.1.3.2"><csymbol cd="ambiguous" id="S3.E8.m1.7.7.1.1.3.2.2.1.cmml" xref="S3.E8.m1.7.7.1.1.3.2">subscript</csymbol><ci id="S3.E8.m1.7.7.1.1.3.2.2.2.cmml" xref="S3.E8.m1.7.7.1.1.3.2.2.2">ğ‘¹</ci><list id="S3.E8.m1.3.3.2.3.cmml" xref="S3.E8.m1.3.3.2.4"><cn type="integer" id="S3.E8.m1.2.2.1.1.cmml" xref="S3.E8.m1.2.2.1.1">1</cn><cn type="integer" id="S3.E8.m1.3.3.2.2.cmml" xref="S3.E8.m1.3.3.2.2">2</cn></list></apply><ci id="S3.E8.m1.4.4.1.1.cmml" xref="S3.E8.m1.4.4.1.1">ğ‘ </ci></apply><apply id="S3.E8.m1.7.7.1.1.3.3.cmml" xref="S3.E8.m1.7.7.1.1.3.3"><csymbol cd="ambiguous" id="S3.E8.m1.7.7.1.1.3.3.1.cmml" xref="S3.E8.m1.7.7.1.1.3.3">superscript</csymbol><apply id="S3.E8.m1.7.7.1.1.3.3.2.cmml" xref="S3.E8.m1.7.7.1.1.3.3"><csymbol cd="ambiguous" id="S3.E8.m1.7.7.1.1.3.3.2.1.cmml" xref="S3.E8.m1.7.7.1.1.3.3">subscript</csymbol><ci id="S3.E8.m1.7.7.1.1.3.3.2.2.cmml" xref="S3.E8.m1.7.7.1.1.3.3.2.2">ğ‘¹</ci><cn type="integer" id="S3.E8.m1.7.7.1.1.3.3.2.3.cmml" xref="S3.E8.m1.7.7.1.1.3.3.2.3">1</cn></apply><apply id="S3.E8.m1.5.5.1.1.1.cmml" xref="S3.E8.m1.5.5.1.1"><csymbol cd="ambiguous" id="S3.E8.m1.5.5.1.1.1.1.cmml" xref="S3.E8.m1.5.5.1.1">superscript</csymbol><ci id="S3.E8.m1.5.5.1.1.1.2.cmml" xref="S3.E8.m1.5.5.1.1.1.2">ğ‘ </ci><ci id="S3.E8.m1.5.5.1.1.1.3.cmml" xref="S3.E8.m1.5.5.1.1.1.3">â€²</ci></apply></apply><apply id="S3.E8.m1.7.7.1.1.3.4.cmml" xref="S3.E8.m1.7.7.1.1.3.4"><csymbol cd="ambiguous" id="S3.E8.m1.7.7.1.1.3.4.1.cmml" xref="S3.E8.m1.7.7.1.1.3.4">superscript</csymbol><ci id="S3.E8.m1.7.7.1.1.3.4.2.cmml" xref="S3.E8.m1.7.7.1.1.3.4.2">ğ‘¿</ci><apply id="S3.E8.m1.6.6.1.1.1.cmml" xref="S3.E8.m1.6.6.1.1"><csymbol cd="ambiguous" id="S3.E8.m1.6.6.1.1.1.1.cmml" xref="S3.E8.m1.6.6.1.1">superscript</csymbol><ci id="S3.E8.m1.6.6.1.1.1.2.cmml" xref="S3.E8.m1.6.6.1.1.1.2">ğ‘ </ci><ci id="S3.E8.m1.6.6.1.1.1.3.cmml" xref="S3.E8.m1.6.6.1.1.1.3">â€²</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.7c">\bm{W}_{\mathrm{rep}}=\begin{pmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\end{pmatrix}\bm{R}_{1,2}^{(s)}\bm{R}_{1}^{(s^{\prime})}\bm{X}^{(s^{\prime})},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p2.9" class="ltx_p">where <math id="S3.SS4.p2.7.m1.1" class="ltx_Math" alttext="\bm{R}_{1}^{(s^{\prime})}" display="inline"><semantics id="S3.SS4.p2.7.m1.1a"><msubsup id="S3.SS4.p2.7.m1.1.2" xref="S3.SS4.p2.7.m1.1.2.cmml"><mi id="S3.SS4.p2.7.m1.1.2.2.2" xref="S3.SS4.p2.7.m1.1.2.2.2.cmml">ğ‘¹</mi><mn id="S3.SS4.p2.7.m1.1.2.2.3" xref="S3.SS4.p2.7.m1.1.2.2.3.cmml">1</mn><mrow id="S3.SS4.p2.7.m1.1.1.1.1" xref="S3.SS4.p2.7.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.p2.7.m1.1.1.1.1.2" xref="S3.SS4.p2.7.m1.1.1.1.1.1.cmml">(</mo><msup id="S3.SS4.p2.7.m1.1.1.1.1.1" xref="S3.SS4.p2.7.m1.1.1.1.1.1.cmml"><mi id="S3.SS4.p2.7.m1.1.1.1.1.1.2" xref="S3.SS4.p2.7.m1.1.1.1.1.1.2.cmml">s</mi><mo id="S3.SS4.p2.7.m1.1.1.1.1.1.3" xref="S3.SS4.p2.7.m1.1.1.1.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="S3.SS4.p2.7.m1.1.1.1.1.3" xref="S3.SS4.p2.7.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.7.m1.1b"><apply id="S3.SS4.p2.7.m1.1.2.cmml" xref="S3.SS4.p2.7.m1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.7.m1.1.2.1.cmml" xref="S3.SS4.p2.7.m1.1.2">superscript</csymbol><apply id="S3.SS4.p2.7.m1.1.2.2.cmml" xref="S3.SS4.p2.7.m1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.7.m1.1.2.2.1.cmml" xref="S3.SS4.p2.7.m1.1.2">subscript</csymbol><ci id="S3.SS4.p2.7.m1.1.2.2.2.cmml" xref="S3.SS4.p2.7.m1.1.2.2.2">ğ‘¹</ci><cn type="integer" id="S3.SS4.p2.7.m1.1.2.2.3.cmml" xref="S3.SS4.p2.7.m1.1.2.2.3">1</cn></apply><apply id="S3.SS4.p2.7.m1.1.1.1.1.1.cmml" xref="S3.SS4.p2.7.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.7.m1.1.1.1.1.1.1.cmml" xref="S3.SS4.p2.7.m1.1.1.1.1">superscript</csymbol><ci id="S3.SS4.p2.7.m1.1.1.1.1.1.2.cmml" xref="S3.SS4.p2.7.m1.1.1.1.1.1.2">ğ‘ </ci><ci id="S3.SS4.p2.7.m1.1.1.1.1.1.3.cmml" xref="S3.SS4.p2.7.m1.1.1.1.1.1.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.7.m1.1c">\bm{R}_{1}^{(s^{\prime})}</annotation></semantics></math> and <math id="S3.SS4.p2.8.m2.1" class="ltx_Math" alttext="\bm{X}^{(s^{\prime})}" display="inline"><semantics id="S3.SS4.p2.8.m2.1a"><msup id="S3.SS4.p2.8.m2.1.2" xref="S3.SS4.p2.8.m2.1.2.cmml"><mi id="S3.SS4.p2.8.m2.1.2.2" xref="S3.SS4.p2.8.m2.1.2.2.cmml">ğ‘¿</mi><mrow id="S3.SS4.p2.8.m2.1.1.1.1" xref="S3.SS4.p2.8.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.p2.8.m2.1.1.1.1.2" xref="S3.SS4.p2.8.m2.1.1.1.1.1.cmml">(</mo><msup id="S3.SS4.p2.8.m2.1.1.1.1.1" xref="S3.SS4.p2.8.m2.1.1.1.1.1.cmml"><mi id="S3.SS4.p2.8.m2.1.1.1.1.1.2" xref="S3.SS4.p2.8.m2.1.1.1.1.1.2.cmml">s</mi><mo id="S3.SS4.p2.8.m2.1.1.1.1.1.3" xref="S3.SS4.p2.8.m2.1.1.1.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="S3.SS4.p2.8.m2.1.1.1.1.3" xref="S3.SS4.p2.8.m2.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.8.m2.1b"><apply id="S3.SS4.p2.8.m2.1.2.cmml" xref="S3.SS4.p2.8.m2.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.8.m2.1.2.1.cmml" xref="S3.SS4.p2.8.m2.1.2">superscript</csymbol><ci id="S3.SS4.p2.8.m2.1.2.2.cmml" xref="S3.SS4.p2.8.m2.1.2.2">ğ‘¿</ci><apply id="S3.SS4.p2.8.m2.1.1.1.1.1.cmml" xref="S3.SS4.p2.8.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.8.m2.1.1.1.1.1.1.cmml" xref="S3.SS4.p2.8.m2.1.1.1.1">superscript</csymbol><ci id="S3.SS4.p2.8.m2.1.1.1.1.1.2.cmml" xref="S3.SS4.p2.8.m2.1.1.1.1.1.2">ğ‘ </ci><ci id="S3.SS4.p2.8.m2.1.1.1.1.1.3.cmml" xref="S3.SS4.p2.8.m2.1.1.1.1.1.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.8.m2.1c">\bm{X}^{(s^{\prime})}</annotation></semantics></math> are the rotation and estimated 3D pose in the current frame and <math id="S3.SS4.p2.9.m3.3" class="ltx_Math" alttext="\bm{R}_{1,2}^{(s)}" display="inline"><semantics id="S3.SS4.p2.9.m3.3a"><msubsup id="S3.SS4.p2.9.m3.3.4" xref="S3.SS4.p2.9.m3.3.4.cmml"><mi id="S3.SS4.p2.9.m3.3.4.2.2" xref="S3.SS4.p2.9.m3.3.4.2.2.cmml">ğ‘¹</mi><mrow id="S3.SS4.p2.9.m3.2.2.2.4" xref="S3.SS4.p2.9.m3.2.2.2.3.cmml"><mn id="S3.SS4.p2.9.m3.1.1.1.1" xref="S3.SS4.p2.9.m3.1.1.1.1.cmml">1</mn><mo id="S3.SS4.p2.9.m3.2.2.2.4.1" xref="S3.SS4.p2.9.m3.2.2.2.3.cmml">,</mo><mn id="S3.SS4.p2.9.m3.2.2.2.2" xref="S3.SS4.p2.9.m3.2.2.2.2.cmml">2</mn></mrow><mrow id="S3.SS4.p2.9.m3.3.3.1.3" xref="S3.SS4.p2.9.m3.3.4.cmml"><mo stretchy="false" id="S3.SS4.p2.9.m3.3.3.1.3.1" xref="S3.SS4.p2.9.m3.3.4.cmml">(</mo><mi id="S3.SS4.p2.9.m3.3.3.1.1" xref="S3.SS4.p2.9.m3.3.3.1.1.cmml">s</mi><mo stretchy="false" id="S3.SS4.p2.9.m3.3.3.1.3.2" xref="S3.SS4.p2.9.m3.3.4.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.9.m3.3b"><apply id="S3.SS4.p2.9.m3.3.4.cmml" xref="S3.SS4.p2.9.m3.3.4"><csymbol cd="ambiguous" id="S3.SS4.p2.9.m3.3.4.1.cmml" xref="S3.SS4.p2.9.m3.3.4">superscript</csymbol><apply id="S3.SS4.p2.9.m3.3.4.2.cmml" xref="S3.SS4.p2.9.m3.3.4"><csymbol cd="ambiguous" id="S3.SS4.p2.9.m3.3.4.2.1.cmml" xref="S3.SS4.p2.9.m3.3.4">subscript</csymbol><ci id="S3.SS4.p2.9.m3.3.4.2.2.cmml" xref="S3.SS4.p2.9.m3.3.4.2.2">ğ‘¹</ci><list id="S3.SS4.p2.9.m3.2.2.2.3.cmml" xref="S3.SS4.p2.9.m3.2.2.2.4"><cn type="integer" id="S3.SS4.p2.9.m3.1.1.1.1.cmml" xref="S3.SS4.p2.9.m3.1.1.1.1">1</cn><cn type="integer" id="S3.SS4.p2.9.m3.2.2.2.2.cmml" xref="S3.SS4.p2.9.m3.2.2.2.2">2</cn></list></apply><ci id="S3.SS4.p2.9.m3.3.3.1.1.cmml" xref="S3.SS4.p2.9.m3.3.3.1.1">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.9.m3.3c">\bm{R}_{1,2}^{(s)}</annotation></semantics></math> is the randomly assigned relative rotation from another sample in the batch<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For the Human3.6M dataset we ensure that relative rotations are only changed in between subjects since camera positions vary between them.</span></span></span>.
The loss is calculated in the same way as the reprojection loss in Eq.Â <a href="#S3.E2" title="In 3.1 Reprojection â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Similar to Sec.Â <a href="#S3.SS2" title="3.2 View-consistency â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> this is easily extended to multiple cameras.
Again, we emphasize that this loss is optional to improve the results for the case of static cameras.
However, our method works without it.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Network Architecture</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2011.14679/assets/images/lifting_network_new_2.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Network structure of the lifting network. The 2D input vector contains the <math id="S3.F3.4.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.F3.4.m1.1b"><mi id="S3.F3.4.m1.1.1" xref="S3.F3.4.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F3.4.m1.1c"><ci id="S3.F3.4.m1.1.1.cmml" xref="S3.F3.4.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.4.m1.1d">x</annotation></semantics></math>- and <math id="S3.F3.5.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.F3.5.m2.1b"><mi id="S3.F3.5.m2.1.1" xref="S3.F3.5.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.F3.5.m2.1c"><ci id="S3.F3.5.m2.1.1.cmml" xref="S3.F3.5.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.5.m2.1d">y</annotation></semantics></math>-coordinates of the 2D pose and the confidence given by the 2D joint detector. It is upscaled using a fully connected layer with <math id="S3.F3.6.m3.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S3.F3.6.m3.1b"><mn id="S3.F3.6.m3.1.1" xref="S3.F3.6.m3.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.F3.6.m3.1c"><cn type="integer" id="S3.F3.6.m3.1.1.cmml" xref="S3.F3.6.m3.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.6.m3.1d">1024</annotation></semantics></math> neurons which then goes to a residual block. After that the network splits into two paths that predict the 3D pose in the canonical space and the camera rotation, respectively. Each of the paths has two consecutive residual blocks followed by a fully connected layer that downscales the features to the required size. The Rodrigues block implements Rodrigues formula (Eq.Â <a href="#S3.E4" title="In 3.1 Reprojection â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) and has no trainable parameters.</figcaption>
</figure>
<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.9" class="ltx_p">Fig.Â <a href="#S3.F3" title="Figure 3 â€£ 3.5 Network Architecture â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the architecture of our lifting network.
The input 2D pose vector is concatenated with a vector containing the confidences for each joint.
It is upscaled to <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><mn id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><cn type="integer" id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">1024</annotation></semantics></math> neurons by a one fully connected layer.
It is followed by a residual block consisting of fully connected layers with dimension <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><mn id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><cn type="integer" id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">1024</annotation></semantics></math>.
Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> the output is fed into two paths, each containing two consecutive residual blocks with identical architecture to the first block.
The 3D pose path directly outputs the 3D coordinates of the predicted pose in the pose coordinate system.
The camera path outputs a three-dimensional vector <math id="S3.SS5.p1.3.m3.1" class="ltx_Math" alttext="\theta\bm{\omega}" display="inline"><semantics id="S3.SS5.p1.3.m3.1a"><mrow id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml"><mi id="S3.SS5.p1.3.m3.1.1.2" xref="S3.SS5.p1.3.m3.1.1.2.cmml">Î¸</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.3.m3.1.1.1" xref="S3.SS5.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S3.SS5.p1.3.m3.1.1.3" xref="S3.SS5.p1.3.m3.1.1.3.cmml">ğ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><apply id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1"><times id="S3.SS5.p1.3.m3.1.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1.1"></times><ci id="S3.SS5.p1.3.m3.1.1.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2">ğœƒ</ci><ci id="S3.SS5.p1.3.m3.1.1.3.cmml" xref="S3.SS5.p1.3.m3.1.1.3">ğ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">\theta\bm{\omega}</annotation></semantics></math> which is the axis angle representation.
The rotation matrix is computed using Rodriguesâ€™ formula as described in Sec.Â <a href="#S3.SS1" title="3.1 Reprojection â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
The activation functions after each layer, except the two output layers, are leaky ReLUâ€™s with a negative slope of <math id="S3.SS5.p1.4.m4.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="S3.SS5.p1.4.m4.1a"><mn id="S3.SS5.p1.4.m4.1.1" xref="S3.SS5.p1.4.m4.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m4.1b"><cn type="float" id="S3.SS5.p1.4.m4.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m4.1c">0.01</annotation></semantics></math>.
We train the network for <math id="S3.SS5.p1.5.m5.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS5.p1.5.m5.1a"><mn id="S3.SS5.p1.5.m5.1.1" xref="S3.SS5.p1.5.m5.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.5.m5.1b"><cn type="integer" id="S3.SS5.p1.5.m5.1.1.cmml" xref="S3.SS5.p1.5.m5.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.5.m5.1c">100</annotation></semantics></math> epochs using the Adam optimizer with an initial learning rate of <math id="S3.SS5.p1.6.m6.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="S3.SS5.p1.6.m6.1a"><mn id="S3.SS5.p1.6.m6.1.1" xref="S3.SS5.p1.6.m6.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.6.m6.1b"><cn type="float" id="S3.SS5.p1.6.m6.1.1.cmml" xref="S3.SS5.p1.6.m6.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.6.m6.1c">0.0001</annotation></semantics></math> and weight decay at epochs <math id="S3.SS5.p1.7.m7.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S3.SS5.p1.7.m7.1a"><mn id="S3.SS5.p1.7.m7.1.1" xref="S3.SS5.p1.7.m7.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.7.m7.1b"><cn type="integer" id="S3.SS5.p1.7.m7.1.1.cmml" xref="S3.SS5.p1.7.m7.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.7.m7.1c">30</annotation></semantics></math>, <math id="S3.SS5.p1.8.m8.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S3.SS5.p1.8.m8.1a"><mn id="S3.SS5.p1.8.m8.1.1" xref="S3.SS5.p1.8.m8.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.8.m8.1b"><cn type="integer" id="S3.SS5.p1.8.m8.1.1.cmml" xref="S3.SS5.p1.8.m8.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.8.m8.1c">60</annotation></semantics></math> and <math id="S3.SS5.p1.9.m9.1" class="ltx_Math" alttext="90" display="inline"><semantics id="S3.SS5.p1.9.m9.1a"><mn id="S3.SS5.p1.9.m9.1.1" xref="S3.SS5.p1.9.m9.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.9.m9.1b"><cn type="integer" id="S3.SS5.p1.9.m9.1.1.cmml" xref="S3.SS5.p1.9.m9.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.9.m9.1c">90</annotation></semantics></math>, respectively.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We perform experiments on the well-known benchmark datasets Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
Additionally, we evaluate on the SkiPose dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> to test the generalizability of our method to real world scenarios.
To conform with our setting of training a single image pose estimator with unlabeled images for a specific set of activities, we train one network for each dataset without using additional datasets.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Metrics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.6" class="ltx_p">For the evaluation on Human3.6M there exist two standard protocols.
Both protocols calculate the <span id="S4.SS1.p1.6.1" class="ltx_text ltx_font_italic">mean per joint position error</span> (MPJPE), i.e. the mean euclidean distance between the reconstructed and the ground truth joint coordinates.
Since a multi-view self-supervised setting does not contain metric data, we adjust the scale of our predictions before calculating the MPJPE.
For a fair comparison with other works we compare to their scale adjusted predictions if they are available.
Protocol-I computes the MPJPE directly whereas Protocol-II first employs a rigid alignment between the poses.
Additional to the MPJPE one protocol for 3DHP calculates the <span id="S4.SS1.p1.6.2" class="ltx_text ltx_font_italic">Percentage of Correct Keypoints</span> (PCK).
As the name suggests it is the percentage of predicted joints that are within a distance of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="150mm" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">150</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.1a" xref="S4.SS1.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.1.m1.1.1.4" xref="S4.SS1.p1.1.m1.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">150</cn><ci id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">ğ‘š</ci><ci id="S4.SS1.p1.1.m1.1.1.4.cmml" xref="S4.SS1.p1.1.m1.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">150mm</annotation></semantics></math> or lower to their corresponding ground truth joint.

<br class="ltx_break">
<br class="ltx_break"><span id="S4.SS1.p1.6.3" class="ltx_text ltx_font_bold">Correct Poses Score (CPS)</span>

<br class="ltx_break">For practical applications, such as motion analysis and prediction, the evaluation of the whole pose is a crucial prerequisite.
Even if a single joint of a pose is incorrect it can change downstream tasks significantly.
The formerly introduced metrics evaluate the quality of the prediction joint by joint.
However, they ignore the assignment of joints to poses and instead average over all joints in the test set.
Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.3 Quantitative Evaluation on Human3.6M and 3DHP â€£ 4 Experiments â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> compares 3D pose estimates with their respective ground truths.
Each column shows two different reconstructions from the same pose.
The reconstructions in the top row have a lower PMPJPE compared to the bottom row.
However, the overall 3D poses appear better reconstructed in the bottom row.
In this section we present a simple yet powerful metric to evaluate such cases, the <span id="S4.SS1.p1.6.4" class="ltx_text ltx_font_italic">Correct Poses Score</span> (CPS).
A pose <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\bm{W}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">ğ‘¾</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">ğ‘¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\bm{W}</annotation></semantics></math> is considered correct if for all joints <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">i</annotation></semantics></math> the Euclidean distance is below a threshold value <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">\theta</annotation></semantics></math>.
Given a pose with joint positions <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="\bm{w}_{i}" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><msub id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml"><mi id="S4.SS1.p1.5.m5.1.1.2" xref="S4.SS1.p1.5.m5.1.1.2.cmml">ğ’˜</mi><mi id="S4.SS1.p1.5.m5.1.1.3" xref="S4.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><apply id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m5.1.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.p1.5.m5.1.1.2">ğ’˜</ci><ci id="S4.SS1.p1.5.m5.1.1.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">\bm{w}_{i}</annotation></semantics></math> and predicted joint positions <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="\hat{\bm{w}}_{i}" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><msub id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml"><mover accent="true" id="S4.SS1.p1.6.m6.1.1.2" xref="S4.SS1.p1.6.m6.1.1.2.cmml"><mi id="S4.SS1.p1.6.m6.1.1.2.2" xref="S4.SS1.p1.6.m6.1.1.2.2.cmml">ğ’˜</mi><mo id="S4.SS1.p1.6.m6.1.1.2.1" xref="S4.SS1.p1.6.m6.1.1.2.1.cmml">^</mo></mover><mi id="S4.SS1.p1.6.m6.1.1.3" xref="S4.SS1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><apply id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">subscript</csymbol><apply id="S4.SS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2"><ci id="S4.SS1.p1.6.m6.1.1.2.1.cmml" xref="S4.SS1.p1.6.m6.1.1.2.1">^</ci><ci id="S4.SS1.p1.6.m6.1.1.2.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2.2">ğ’˜</ci></apply><ci id="S4.SS1.p1.6.m6.1.1.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">\hat{\bm{w}}_{i}</annotation></semantics></math> after rigid alignment, a correct pose is defined by</p>
<table id="S4.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E9.m1.5" class="ltx_Math" alttext="\mathrm{CP}_{\theta}=\begin{cases}1&amp;\|\bm{w}_{i}-\hat{\bm{w}}_{i}\|_{2}&lt;\theta\hskip 11.38109pt\forall i\in\{1,...,j\}\\
0&amp;else\end{cases}." display="block"><semantics id="S4.E9.m1.5a"><mrow id="S4.E9.m1.5.5.1" xref="S4.E9.m1.5.5.1.1.cmml"><mrow id="S4.E9.m1.5.5.1.1" xref="S4.E9.m1.5.5.1.1.cmml"><msub id="S4.E9.m1.5.5.1.1.2" xref="S4.E9.m1.5.5.1.1.2.cmml"><mi id="S4.E9.m1.5.5.1.1.2.2" xref="S4.E9.m1.5.5.1.1.2.2.cmml">CP</mi><mi id="S4.E9.m1.5.5.1.1.2.3" xref="S4.E9.m1.5.5.1.1.2.3.cmml">Î¸</mi></msub><mo id="S4.E9.m1.5.5.1.1.1" xref="S4.E9.m1.5.5.1.1.1.cmml">=</mo><mrow id="S4.E9.m1.4.4" xref="S4.E9.m1.5.5.1.1.3.1.cmml"><mo id="S4.E9.m1.4.4.5" xref="S4.E9.m1.5.5.1.1.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.E9.m1.4.4.4" xref="S4.E9.m1.5.5.1.1.3.1.cmml"><mtr id="S4.E9.m1.4.4.4a" xref="S4.E9.m1.5.5.1.1.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E9.m1.4.4.4b" xref="S4.E9.m1.5.5.1.1.3.1.cmml"><mn id="S4.E9.m1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E9.m1.4.4.4c" xref="S4.E9.m1.5.5.1.1.3.1.cmml"><mrow id="S4.E9.m1.2.2.2.2.2.1.6" xref="S4.E9.m1.2.2.2.2.2.1.7.cmml"><mrow id="S4.E9.m1.2.2.2.2.2.1.5.1" xref="S4.E9.m1.2.2.2.2.2.1.5.1.cmml"><msub id="S4.E9.m1.2.2.2.2.2.1.5.1.1" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.cmml"><mrow id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.2.cmml"><mo stretchy="false" id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.2" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.2.1.cmml">â€–</mo><mrow id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.cmml"><msub id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2.cmml"><mi id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2.2" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2.2.cmml">ğ’˜</mi><mi id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2.3" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.1" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.cmml"><mover accent="true" id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2.cmml"><mi id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2.2" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2.2.cmml">ğ’˜</mi><mo id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2.1" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.3" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.3" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S4.E9.m1.2.2.2.2.2.1.5.1.1.3" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.3.cmml">2</mn></msub><mo id="S4.E9.m1.2.2.2.2.2.1.5.1.2" xref="S4.E9.m1.2.2.2.2.2.1.5.1.2.cmml">&lt;</mo><mi id="S4.E9.m1.2.2.2.2.2.1.4" xref="S4.E9.m1.2.2.2.2.2.1.4.cmml">Î¸</mi></mrow><mspace width="1.307em" id="S4.E9.m1.2.2.2.2.2.1.6.3" xref="S4.E9.m1.2.2.2.2.2.1.7a.cmml"></mspace><mrow id="S4.E9.m1.2.2.2.2.2.1.6.2" xref="S4.E9.m1.2.2.2.2.2.1.6.2.cmml"><mrow id="S4.E9.m1.2.2.2.2.2.1.6.2.2" xref="S4.E9.m1.2.2.2.2.2.1.6.2.2.cmml"><mo rspace="0.167em" id="S4.E9.m1.2.2.2.2.2.1.6.2.2.1" xref="S4.E9.m1.2.2.2.2.2.1.6.2.2.1.cmml">âˆ€</mo><mi id="S4.E9.m1.2.2.2.2.2.1.6.2.2.2" xref="S4.E9.m1.2.2.2.2.2.1.6.2.2.2.cmml">i</mi></mrow><mo id="S4.E9.m1.2.2.2.2.2.1.6.2.1" xref="S4.E9.m1.2.2.2.2.2.1.6.2.1.cmml">âˆˆ</mo><mrow id="S4.E9.m1.2.2.2.2.2.1.6.2.3.2" xref="S4.E9.m1.2.2.2.2.2.1.6.2.3.1.cmml"><mo stretchy="false" id="S4.E9.m1.2.2.2.2.2.1.6.2.3.2.1" xref="S4.E9.m1.2.2.2.2.2.1.6.2.3.1.cmml">{</mo><mn id="S4.E9.m1.2.2.2.2.2.1.1" xref="S4.E9.m1.2.2.2.2.2.1.1.cmml">1</mn><mo id="S4.E9.m1.2.2.2.2.2.1.6.2.3.2.2" xref="S4.E9.m1.2.2.2.2.2.1.6.2.3.1.cmml">,</mo><mi mathvariant="normal" id="S4.E9.m1.2.2.2.2.2.1.2" xref="S4.E9.m1.2.2.2.2.2.1.2.cmml">â€¦</mi><mo id="S4.E9.m1.2.2.2.2.2.1.6.2.3.2.3" xref="S4.E9.m1.2.2.2.2.2.1.6.2.3.1.cmml">,</mo><mi id="S4.E9.m1.2.2.2.2.2.1.3" xref="S4.E9.m1.2.2.2.2.2.1.3.cmml">j</mi><mo stretchy="false" id="S4.E9.m1.2.2.2.2.2.1.6.2.3.2.4" xref="S4.E9.m1.2.2.2.2.2.1.6.2.3.1.cmml">}</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S4.E9.m1.4.4.4d" xref="S4.E9.m1.5.5.1.1.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E9.m1.4.4.4e" xref="S4.E9.m1.5.5.1.1.3.1.cmml"><mn id="S4.E9.m1.3.3.3.3.1.1" xref="S4.E9.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E9.m1.4.4.4f" xref="S4.E9.m1.5.5.1.1.3.1.cmml"><mrow id="S4.E9.m1.4.4.4.4.2.1" xref="S4.E9.m1.4.4.4.4.2.1.cmml"><mi id="S4.E9.m1.4.4.4.4.2.1.2" xref="S4.E9.m1.4.4.4.4.2.1.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E9.m1.4.4.4.4.2.1.1" xref="S4.E9.m1.4.4.4.4.2.1.1.cmml">â€‹</mo><mi id="S4.E9.m1.4.4.4.4.2.1.3" xref="S4.E9.m1.4.4.4.4.2.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E9.m1.4.4.4.4.2.1.1a" xref="S4.E9.m1.4.4.4.4.2.1.1.cmml">â€‹</mo><mi id="S4.E9.m1.4.4.4.4.2.1.4" xref="S4.E9.m1.4.4.4.4.2.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E9.m1.4.4.4.4.2.1.1b" xref="S4.E9.m1.4.4.4.4.2.1.1.cmml">â€‹</mo><mi id="S4.E9.m1.4.4.4.4.2.1.5" xref="S4.E9.m1.4.4.4.4.2.1.5.cmml">e</mi></mrow></mtd></mtr></mtable></mrow></mrow><mo lspace="0em" id="S4.E9.m1.5.5.1.2" xref="S4.E9.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E9.m1.5b"><apply id="S4.E9.m1.5.5.1.1.cmml" xref="S4.E9.m1.5.5.1"><eq id="S4.E9.m1.5.5.1.1.1.cmml" xref="S4.E9.m1.5.5.1.1.1"></eq><apply id="S4.E9.m1.5.5.1.1.2.cmml" xref="S4.E9.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S4.E9.m1.5.5.1.1.2.1.cmml" xref="S4.E9.m1.5.5.1.1.2">subscript</csymbol><ci id="S4.E9.m1.5.5.1.1.2.2.cmml" xref="S4.E9.m1.5.5.1.1.2.2">CP</ci><ci id="S4.E9.m1.5.5.1.1.2.3.cmml" xref="S4.E9.m1.5.5.1.1.2.3">ğœƒ</ci></apply><apply id="S4.E9.m1.5.5.1.1.3.1.cmml" xref="S4.E9.m1.4.4"><csymbol cd="latexml" id="S4.E9.m1.5.5.1.1.3.1.1.cmml" xref="S4.E9.m1.4.4.5">cases</csymbol><cn type="integer" id="S4.E9.m1.1.1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1">1</cn><apply id="S4.E9.m1.2.2.2.2.2.1.7.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6"><csymbol cd="ambiguous" id="S4.E9.m1.2.2.2.2.2.1.7a.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.3">formulae-sequence</csymbol><apply id="S4.E9.m1.2.2.2.2.2.1.5.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1"><lt id="S4.E9.m1.2.2.2.2.2.1.5.1.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.2"></lt><apply id="S4.E9.m1.2.2.2.2.2.1.5.1.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1"><csymbol cd="ambiguous" id="S4.E9.m1.2.2.2.2.2.1.5.1.1.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1">subscript</csymbol><apply id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1"><csymbol cd="latexml" id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.2.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.2">norm</csymbol><apply id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1"><minus id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.1"></minus><apply id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2.2">ğ’˜</ci><ci id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2.3.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3">subscript</csymbol><apply id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2"><ci id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2.1">^</ci><ci id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.2.2">ğ’˜</ci></apply><ci id="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.3.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply></apply><cn type="integer" id="S4.E9.m1.2.2.2.2.2.1.5.1.1.3.cmml" xref="S4.E9.m1.2.2.2.2.2.1.5.1.1.3">2</cn></apply><ci id="S4.E9.m1.2.2.2.2.2.1.4.cmml" xref="S4.E9.m1.2.2.2.2.2.1.4">ğœƒ</ci></apply><apply id="S4.E9.m1.2.2.2.2.2.1.6.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.2"><in id="S4.E9.m1.2.2.2.2.2.1.6.2.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.2.1"></in><apply id="S4.E9.m1.2.2.2.2.2.1.6.2.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.2.2"><csymbol cd="latexml" id="S4.E9.m1.2.2.2.2.2.1.6.2.2.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.2.2.1">for-all</csymbol><ci id="S4.E9.m1.2.2.2.2.2.1.6.2.2.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.2.2.2">ğ‘–</ci></apply><set id="S4.E9.m1.2.2.2.2.2.1.6.2.3.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.6.2.3.2"><cn type="integer" id="S4.E9.m1.2.2.2.2.2.1.1.cmml" xref="S4.E9.m1.2.2.2.2.2.1.1">1</cn><ci id="S4.E9.m1.2.2.2.2.2.1.2.cmml" xref="S4.E9.m1.2.2.2.2.2.1.2">â€¦</ci><ci id="S4.E9.m1.2.2.2.2.2.1.3.cmml" xref="S4.E9.m1.2.2.2.2.2.1.3">ğ‘—</ci></set></apply></apply><cn type="integer" id="S4.E9.m1.3.3.3.3.1.1.cmml" xref="S4.E9.m1.3.3.3.3.1.1">0</cn><apply id="S4.E9.m1.4.4.4.4.2.1.cmml" xref="S4.E9.m1.4.4.4.4.2.1"><times id="S4.E9.m1.4.4.4.4.2.1.1.cmml" xref="S4.E9.m1.4.4.4.4.2.1.1"></times><ci id="S4.E9.m1.4.4.4.4.2.1.2.cmml" xref="S4.E9.m1.4.4.4.4.2.1.2">ğ‘’</ci><ci id="S4.E9.m1.4.4.4.4.2.1.3.cmml" xref="S4.E9.m1.4.4.4.4.2.1.3">ğ‘™</ci><ci id="S4.E9.m1.4.4.4.4.2.1.4.cmml" xref="S4.E9.m1.4.4.4.4.2.1.4">ğ‘ </ci><ci id="S4.E9.m1.4.4.4.4.2.1.5.cmml" xref="S4.E9.m1.4.4.4.4.2.1.5">ğ‘’</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E9.m1.5c">\mathrm{CP}_{\theta}=\begin{cases}1&amp;\|\bm{w}_{i}-\hat{\bm{w}}_{i}\|_{2}&lt;\theta\hskip 11.38109pt\forall i\in\{1,...,j\}\\
0&amp;else\end{cases}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p1.7" class="ltx_p">Additional to the PMPJPE Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.3 Quantitative Evaluation on Human3.6M and 3DHP â€£ 4 Experiments â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the CP@180mm, which classifies the reconstructed poses into correct and incorrect.
The percentage of correct poses is calculated for the test dataset.
To be independent of the threshold, we calculate the area under curve for <math id="S4.SS1.p1.7.m1.2" class="ltx_Math" alttext="\theta\in[0mm,300mm]" display="inline"><semantics id="S4.SS1.p1.7.m1.2a"><mrow id="S4.SS1.p1.7.m1.2.2" xref="S4.SS1.p1.7.m1.2.2.cmml"><mi id="S4.SS1.p1.7.m1.2.2.4" xref="S4.SS1.p1.7.m1.2.2.4.cmml">Î¸</mi><mo id="S4.SS1.p1.7.m1.2.2.3" xref="S4.SS1.p1.7.m1.2.2.3.cmml">âˆˆ</mo><mrow id="S4.SS1.p1.7.m1.2.2.2.2" xref="S4.SS1.p1.7.m1.2.2.2.3.cmml"><mo stretchy="false" id="S4.SS1.p1.7.m1.2.2.2.2.3" xref="S4.SS1.p1.7.m1.2.2.2.3.cmml">[</mo><mrow id="S4.SS1.p1.7.m1.1.1.1.1.1" xref="S4.SS1.p1.7.m1.1.1.1.1.1.cmml"><mn id="S4.SS1.p1.7.m1.1.1.1.1.1.2" xref="S4.SS1.p1.7.m1.1.1.1.1.1.2.cmml">0</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.7.m1.1.1.1.1.1.1" xref="S4.SS1.p1.7.m1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.7.m1.1.1.1.1.1.3" xref="S4.SS1.p1.7.m1.1.1.1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.7.m1.1.1.1.1.1.1a" xref="S4.SS1.p1.7.m1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.7.m1.1.1.1.1.1.4" xref="S4.SS1.p1.7.m1.1.1.1.1.1.4.cmml">m</mi></mrow><mo id="S4.SS1.p1.7.m1.2.2.2.2.4" xref="S4.SS1.p1.7.m1.2.2.2.3.cmml">,</mo><mrow id="S4.SS1.p1.7.m1.2.2.2.2.2" xref="S4.SS1.p1.7.m1.2.2.2.2.2.cmml"><mn id="S4.SS1.p1.7.m1.2.2.2.2.2.2" xref="S4.SS1.p1.7.m1.2.2.2.2.2.2.cmml">300</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.7.m1.2.2.2.2.2.1" xref="S4.SS1.p1.7.m1.2.2.2.2.2.1.cmml">â€‹</mo><mi id="S4.SS1.p1.7.m1.2.2.2.2.2.3" xref="S4.SS1.p1.7.m1.2.2.2.2.2.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.7.m1.2.2.2.2.2.1a" xref="S4.SS1.p1.7.m1.2.2.2.2.2.1.cmml">â€‹</mo><mi id="S4.SS1.p1.7.m1.2.2.2.2.2.4" xref="S4.SS1.p1.7.m1.2.2.2.2.2.4.cmml">m</mi></mrow><mo stretchy="false" id="S4.SS1.p1.7.m1.2.2.2.2.5" xref="S4.SS1.p1.7.m1.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m1.2b"><apply id="S4.SS1.p1.7.m1.2.2.cmml" xref="S4.SS1.p1.7.m1.2.2"><in id="S4.SS1.p1.7.m1.2.2.3.cmml" xref="S4.SS1.p1.7.m1.2.2.3"></in><ci id="S4.SS1.p1.7.m1.2.2.4.cmml" xref="S4.SS1.p1.7.m1.2.2.4">ğœƒ</ci><interval closure="closed" id="S4.SS1.p1.7.m1.2.2.2.3.cmml" xref="S4.SS1.p1.7.m1.2.2.2.2"><apply id="S4.SS1.p1.7.m1.1.1.1.1.1.cmml" xref="S4.SS1.p1.7.m1.1.1.1.1.1"><times id="S4.SS1.p1.7.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.7.m1.1.1.1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.7.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.p1.7.m1.1.1.1.1.1.2">0</cn><ci id="S4.SS1.p1.7.m1.1.1.1.1.1.3.cmml" xref="S4.SS1.p1.7.m1.1.1.1.1.1.3">ğ‘š</ci><ci id="S4.SS1.p1.7.m1.1.1.1.1.1.4.cmml" xref="S4.SS1.p1.7.m1.1.1.1.1.1.4">ğ‘š</ci></apply><apply id="S4.SS1.p1.7.m1.2.2.2.2.2.cmml" xref="S4.SS1.p1.7.m1.2.2.2.2.2"><times id="S4.SS1.p1.7.m1.2.2.2.2.2.1.cmml" xref="S4.SS1.p1.7.m1.2.2.2.2.2.1"></times><cn type="integer" id="S4.SS1.p1.7.m1.2.2.2.2.2.2.cmml" xref="S4.SS1.p1.7.m1.2.2.2.2.2.2">300</cn><ci id="S4.SS1.p1.7.m1.2.2.2.2.2.3.cmml" xref="S4.SS1.p1.7.m1.2.2.2.2.2.3">ğ‘š</ci><ci id="S4.SS1.p1.7.m1.2.2.2.2.2.4.cmml" xref="S4.SS1.p1.7.m1.2.2.2.2.2.4">ğ‘š</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m1.2c">\theta\in[0mm,300mm]</annotation></semantics></math> which defines the CPS.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2011.14679/assets/x1.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="193" height="161" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of PMPJPE and our CP-metric. Each column compares to different predicted 3D reconstructions with the same ground truth. While PMPJPE averages out high individual joint errors which are located in the right arm in the visualized case, CP indicates them. In this way, the correctness of the overall pose is evaluated. Note that for the calculation of the CPS we vary the threshold, which in these examples is <math id="S4.F4.2.m1.1" class="ltx_Math" alttext="180mm" display="inline"><semantics id="S4.F4.2.m1.1b"><mrow id="S4.F4.2.m1.1.1" xref="S4.F4.2.m1.1.1.cmml"><mn id="S4.F4.2.m1.1.1.2" xref="S4.F4.2.m1.1.1.2.cmml">180</mn><mo lspace="0em" rspace="0em" id="S4.F4.2.m1.1.1.1" xref="S4.F4.2.m1.1.1.1.cmml">â€‹</mo><mi id="S4.F4.2.m1.1.1.3" xref="S4.F4.2.m1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.F4.2.m1.1.1.1b" xref="S4.F4.2.m1.1.1.1.cmml">â€‹</mo><mi id="S4.F4.2.m1.1.1.4" xref="S4.F4.2.m1.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.2.m1.1c"><apply id="S4.F4.2.m1.1.1.cmml" xref="S4.F4.2.m1.1.1"><times id="S4.F4.2.m1.1.1.1.cmml" xref="S4.F4.2.m1.1.1.1"></times><cn type="integer" id="S4.F4.2.m1.1.1.2.cmml" xref="S4.F4.2.m1.1.1.2">180</cn><ci id="S4.F4.2.m1.1.1.3.cmml" xref="S4.F4.2.m1.1.1.3">ğ‘š</ci><ci id="S4.F4.2.m1.1.1.4.cmml" xref="S4.F4.2.m1.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.2.m1.1d">180mm</annotation></semantics></math>.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Skeleton Morphing</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">We deploy an off-the-shelf detector AlphaPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> for retrieving the 2D human pose estimation required as input to our method.
The keypoint locations in the datasets used to train AlphaPose and other 2D pose estimation methods differ from the 3D skeleton of the test benchmarks.
For example, the root joint position is not in the middle of the hip joints and the relative position of the neck to the shoulders is different.
We circumvent this problem by training a <span id="S4.SS2.p1.2.1" class="ltx_text ltx_font_italic">2D skeleton morphing network</span>
that predicts the offset between the 2D pose from AlphaPose to the ground truth 2D pose in the dataset.
We train the morphing network on subject <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">1</annotation></semantics></math> of each dataset with the given ground truth poses.
To not include these ground truth poses into our training, subject <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn type="integer" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">1</annotation></semantics></math> is excluded in all experiments. Thereby our data used for the self supervised training does not contain any 2D ground truth data, mimicking real application scenarios.
Note that the morphing network never sees any images and therefore is not able to learn domain specific image features.
In an experimental setting where the skeletal structure does not need to match a different skeleton this step is obsolete.
This is the case for most practical applications.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Quantitative Evaluation on Human3.6M and 3DHP</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.4" class="ltx_p">For the Human3.6M dataset, to keep it consistent with previous approaches, we follow standard protocols and evaluate only on every 64th frame.
However, with a sufficiently fast 2D pose estimator, which is the performance bottle neck of our complete pipeline, we can achieve real-time performance.
TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.3 Quantitative Evaluation on Human3.6M and 3DHP â€£ 4 Experiments â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results of the proposed method compared to other state-of-the-art approaches.
We outperform every other comparable approach in terms of PMPJPE.
Note that we even achieve comparable performance to the fully supervised method of Martinez <span id="S4.SS3.p1.4.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.4.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S4.SS3.p1.4.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> which has a lifting network with similar structure to ours.
Only one other self-supervised approach attains a lower MPJPE, however, by using additional information.
Our analysis revealed that although our pose structure is very accurate (which results in a low PMPJPE) the largest part of the error originates from a slight offset in the rotation.
For example, comparing frame <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="integer" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">1</annotation></semantics></math> from subject <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn type="integer" id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">9</annotation></semantics></math> of the Human3.6M dataset to itself rotated by only <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="15^{\circ}" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><msup id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mn id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">15</mn><mo id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">15</cn><compose id="S4.SS3.p1.3.m3.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">15^{\circ}</annotation></semantics></math> around the longitudinal axis already results in an MPJPE of <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="67.7mm" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mrow id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml"><mn id="S4.SS3.p1.4.m4.1.1.2" xref="S4.SS3.p1.4.m4.1.1.2.cmml">67.7</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.4.m4.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.4.m4.1.1.3" xref="S4.SS3.p1.4.m4.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.4.m4.1.1.1a" xref="S4.SS3.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.4.m4.1.1.4" xref="S4.SS3.p1.4.m4.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><apply id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1"><times id="S4.SS3.p1.4.m4.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1"></times><cn type="float" id="S4.SS3.p1.4.m4.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2">67.7</cn><ci id="S4.SS3.p1.4.m4.1.1.3.cmml" xref="S4.SS3.p1.4.m4.1.1.3">ğ‘š</ci><ci id="S4.SS3.p1.4.m4.1.1.4.cmml" xref="S4.SS3.p1.4.m4.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">67.7mm</annotation></semantics></math>.
Iqbal <span id="S4.SS3.p1.4.2" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.4.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS3.p1.4.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> still set the state of the art in terms of MPJPE.
However, they need bone length constraints which they directly compute from the ground truth 3D data of the training set.
Our approach does not require any predefined priors on the skeletal structure.
Using our static camera constraint (Ours+C) improves the MPJPE significantly.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Evaluation results for the Human3.6M dataset in <math id="S4.T1.2.m1.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S4.T1.2.m1.1b"><mrow id="S4.T1.2.m1.1.1" xref="S4.T1.2.m1.1.1.cmml"><mi id="S4.T1.2.m1.1.1.2" xref="S4.T1.2.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.m1.1.1.1" xref="S4.T1.2.m1.1.1.1.cmml">â€‹</mo><mi id="S4.T1.2.m1.1.1.3" xref="S4.T1.2.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.m1.1c"><apply id="S4.T1.2.m1.1.1.cmml" xref="S4.T1.2.m1.1.1"><times id="S4.T1.2.m1.1.1.1.cmml" xref="S4.T1.2.m1.1.1.1"></times><ci id="S4.T1.2.m1.1.1.2.cmml" xref="S4.T1.2.m1.1.1.2">ğ‘š</ci><ci id="S4.T1.2.m1.1.1.3.cmml" xref="S4.T1.2.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.m1.1d">mm</annotation></semantics></math>. The bottom section, labeled with <span id="S4.T1.10.1" class="ltx_text ltx_font_italic">self</span>, shows methods that can solve our setting. Best results are marked in bold and second best in italic.</figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.4.2" class="ltx_tr">
<th id="S4.T1.4.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.4.2.3.1" class="ltx_text" style="font-size:80%;">Supervision</span></th>
<th id="S4.T1.4.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.4.2.4.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<td id="S4.T1.3.1.1" class="ltx_td ltx_align_center">
<span id="S4.T1.3.1.1.1" class="ltx_text" style="font-size:80%;">MPJPE</span><math id="S4.T1.3.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.3.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T1.3.1.1.m1.1.1" xref="S4.T1.3.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.1.m1.1b"><ci id="S4.T1.3.1.1.m1.1.1.cmml" xref="S4.T1.3.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T1.4.2.2" class="ltx_td ltx_align_center">
<span id="S4.T1.4.2.2.1" class="ltx_text" style="font-size:80%;">PMPJPE</span><math id="S4.T1.4.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.4.2.2.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T1.4.2.2.m1.1.1" xref="S4.T1.4.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.2.2.m1.1b"><ci id="S4.T1.4.2.2.m1.1.1.cmml" xref="S4.T1.4.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.4.3.1" class="ltx_tr">
<th id="S4.T1.4.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.4.3.1.1.1" class="ltx_text" style="font-size:80%;">full</span></th>
<th id="S4.T1.4.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T1.4.3.1.2.1" class="ltx_text" style="font-size:80%;">Martinez </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.3.1.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S4.T1.4.3.1.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.3.1.3.1" class="ltx_text" style="font-size:80%;">67.5</span></td>
<td id="S4.T1.4.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.3.1.4.1" class="ltx_text" style="font-size:80%;">52.5</span></td>
</tr>
<tr id="S4.T1.4.4.2" class="ltx_tr">
<th id="S4.T1.4.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.4.4.2.1.1" class="ltx_text" style="font-size:80%;">weak</span></th>
<th id="S4.T1.4.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T1.4.4.2.2.1" class="ltx_text" style="font-size:80%;">Rhodin </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.4.2.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.T1.4.4.2.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.4.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.4.2.3.1" class="ltx_text" style="font-size:80%;">80.1</span></td>
<td id="S4.T1.4.4.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.4.2.4.1" class="ltx_text" style="font-size:80%;">65.1</span></td>
</tr>
<tr id="S4.T1.4.5.3" class="ltx_tr">
<th id="S4.T1.4.5.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T1.4.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.5.3.2.1" class="ltx_text" style="font-size:80%;">Rhodin </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.5.3.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S4.T1.4.5.3.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.5.3.3.1" class="ltx_text" style="font-size:80%;">122.6</span></td>
<td id="S4.T1.4.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.5.3.4.1" class="ltx_text" style="font-size:80%;">98.2</span></td>
</tr>
<tr id="S4.T1.4.6.4" class="ltx_tr">
<th id="S4.T1.4.6.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T1.4.6.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.6.4.2.1" class="ltx_text" style="font-size:80%;">3D interpreter </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.6.4.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S4.T1.4.6.4.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.6.4.3.1" class="ltx_text" style="font-size:80%;">98.4</span></td>
<td id="S4.T1.4.6.4.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.6.4.4.1" class="ltx_text" style="font-size:80%;">88.6</span></td>
</tr>
<tr id="S4.T1.4.7.5" class="ltx_tr">
<th id="S4.T1.4.7.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T1.4.7.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.7.5.2.1" class="ltx_text" style="font-size:80%;">AIGN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.7.5.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S4.T1.4.7.5.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.7.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.7.5.3.1" class="ltx_text" style="font-size:80%;">97.2</span></td>
<td id="S4.T1.4.7.5.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.7.5.4.1" class="ltx_text" style="font-size:80%;">79.0</span></td>
</tr>
<tr id="S4.T1.4.8.6" class="ltx_tr">
<th id="S4.T1.4.8.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T1.4.8.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.8.6.2.1" class="ltx_text" style="font-size:80%;">RepNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.8.6.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S4.T1.4.8.6.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.8.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.8.6.3.1" class="ltx_text" style="font-size:80%;">89.9</span></td>
<td id="S4.T1.4.8.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.8.6.4.1" class="ltx_text" style="font-size:80%;">65.1</span></td>
</tr>
<tr id="S4.T1.4.9.7" class="ltx_tr">
<th id="S4.T1.4.9.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T1.4.9.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.9.7.2.1" class="ltx_text" style="font-size:80%;">HMR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.9.7.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.T1.4.9.7.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.9.7.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.9.7.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.4.9.7.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.9.7.4.1" class="ltx_text" style="font-size:80%;">66.5</span></td>
</tr>
<tr id="S4.T1.4.10.8" class="ltx_tr">
<th id="S4.T1.4.10.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T1.4.10.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.10.8.2.1" class="ltx_text" style="font-size:80%;">Wang </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.10.8.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S4.T1.4.10.8.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.10.8.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.10.8.3.1" class="ltx_text" style="font-size:80%;">86.4</span></td>
<td id="S4.T1.4.10.8.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.10.8.4.1" class="ltx_text" style="font-size:80%;">62.8</span></td>
</tr>
<tr id="S4.T1.4.11.9" class="ltx_tr">
<th id="S4.T1.4.11.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T1.4.11.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.11.9.2.1" class="ltx_text" style="font-size:80%;">Kolotouros </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.11.9.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S4.T1.4.11.9.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.11.9.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.11.9.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.4.11.9.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.11.9.4.1" class="ltx_text" style="font-size:80%;">62.0</span></td>
</tr>
<tr id="S4.T1.4.12.10" class="ltx_tr">
<th id="S4.T1.4.12.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T1.4.12.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.12.10.2.1" class="ltx_text" style="font-size:80%;">Kundu </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.12.10.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S4.T1.4.12.10.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.12.10.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.12.10.3.1" class="ltx_text" style="font-size:80%;">85.8</span></td>
<td id="S4.T1.4.12.10.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.12.10.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T1.4.13.11" class="ltx_tr">
<th id="S4.T1.4.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T1.4.13.11.1.1" class="ltx_text" style="font-size:80%;">self</span></th>
<th id="S4.T1.4.13.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">
<span id="S4.T1.4.13.11.2.1" class="ltx_text" style="font-size:80%;">Chen </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.13.11.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S4.T1.4.13.11.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.13.11.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.4.13.11.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.4.13.11.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.4.13.11.4.1" class="ltx_text" style="font-size:80%;">68.0</span></td>
</tr>
<tr id="S4.T1.4.14.12" class="ltx_tr">
<th id="S4.T1.4.14.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T1.4.14.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.14.12.2.1" class="ltx_text" style="font-size:80%;">EpipolarPose </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.14.12.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S4.T1.4.14.12.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.14.12.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.14.12.3.1" class="ltx_text" style="font-size:80%;">76.6</span></td>
<td id="S4.T1.4.14.12.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.14.12.4.1" class="ltx_text" style="font-size:80%;">67.5</span></td>
</tr>
<tr id="S4.T1.4.15.13" class="ltx_tr">
<th id="S4.T1.4.15.13.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T1.4.15.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.4.15.13.2.1" class="ltx_text" style="font-size:80%;">Iqbal </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.4.15.13.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T1.4.15.13.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T1.4.15.13.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.15.13.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">69.1</span></td>
<td id="S4.T1.4.15.13.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.15.13.4.1" class="ltx_text" style="font-size:80%;">55.9</span></td>
</tr>
<tr id="S4.T1.4.16.14" class="ltx_tr">
<th id="S4.T1.4.16.14.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S4.T1.4.16.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.4.16.14.2.1" class="ltx_text" style="font-size:80%;">Ours</span></th>
<td id="S4.T1.4.16.14.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.16.14.3.1" class="ltx_text" style="font-size:80%;">81.9</span></td>
<td id="S4.T1.4.16.14.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.16.14.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">53.0</span></td>
</tr>
<tr id="S4.T1.4.17.15" class="ltx_tr">
<th id="S4.T1.4.17.15.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T1.4.17.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.4.17.15.2.1" class="ltx_text" style="font-size:80%;">Ours + C</span></th>
<td id="S4.T1.4.17.15.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.17.15.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">74.3</span></td>
<td id="S4.T1.4.17.15.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.17.15.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">53.0</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.4" class="ltx_p">Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.3 Quantitative Evaluation on Human3.6M and 3DHP â€£ 4 Experiments â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the CPS for our method compared to EpipolarPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which is the only comparable approach with publicly available code, and the 3D pose estimation baseline of Martinez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
On this metric, we outperform EpipolarPose by a large margin.
Note the high threshold of over <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="80mm" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">80</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.1.1.1a" xref="S4.SS3.p2.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p2.1.m1.1.1.4" xref="S4.SS3.p2.1.m1.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">80</cn><ci id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">ğ‘š</ci><ci id="S4.SS3.p2.1.m1.1.1.4.cmml" xref="S4.SS3.p2.1.m1.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">80mm</annotation></semantics></math> that is required by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> to achieve a CP above <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mn id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><cn type="integer" id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">1</annotation></semantics></math>% compared to our threshold slightly below <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="50mm" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mn id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p2.3.m3.1.1.3" xref="S4.SS3.p2.3.m3.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.3.m3.1.1.1a" xref="S4.SS3.p2.3.m3.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p2.3.m3.1.1.4" xref="S4.SS3.p2.3.m3.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><times id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1"></times><cn type="integer" id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">50</cn><ci id="S4.SS3.p2.3.m3.1.1.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3">ğ‘š</ci><ci id="S4.SS3.p2.3.m3.1.1.4.cmml" xref="S4.SS3.p2.3.m3.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">50mm</annotation></semantics></math>.
As for the CPS metric, we are on par with the fully supervised approach of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Since their originally trained model is not publicly available anymore we retrained their model with their provided code to report the new CPS metric. The retrained model achieved a PMPJPE of <math id="S4.SS3.p2.4.m4.1" class="ltx_Math" alttext="53.5" display="inline"><semantics id="S4.SS3.p2.4.m4.1a"><mn id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml">53.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><cn type="float" id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1">53.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">53.5</annotation></semantics></math>mm, which is slightly lower compared to their original number. The new model is used only for reporting CPS.
Fig.Â <a href="#S4.F6" title="Figure 6 â€£ 4.3 Quantitative Evaluation on Human3.6M and 3DHP â€£ 4 Experiments â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows qualitative results for the Human3.6M data set in the first row.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">We also evaluate our approach on the 3DHP dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> following the standard test protocols and metrics.
TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.3 Quantitative Evaluation on Human3.6M and 3DHP â€£ 4 Experiments â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results.
We outperform every other self-supervised approach.
In contrast to other approaches the proposed method does not require calibrated cameras<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The configuration Ours+C only assumes that cameras are static during the sequence, which is a much weaker constraint.</span></span></span> or anthropometric constraints.
For the CPS metric we achieve a score of <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="134.2" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mn id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">134.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><cn type="float" id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">134.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">134.2</annotation></semantics></math>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation results for the 3DHP dataset. The bottom section, labeled with <span id="S4.T2.11.1" class="ltx_text ltx_font_italic">self</span>, shows methods that can solve our setting. Best results are marked in bold and second best in italic. MPJPE and PMPJPE are given in <math id="S4.T2.2.m1.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S4.T2.2.m1.1b"><mrow id="S4.T2.2.m1.1.1" xref="S4.T2.2.m1.1.1.cmml"><mi id="S4.T2.2.m1.1.1.2" xref="S4.T2.2.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.m1.1.1.1" xref="S4.T2.2.m1.1.1.1.cmml">â€‹</mo><mi id="S4.T2.2.m1.1.1.3" xref="S4.T2.2.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><apply id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1"><times id="S4.T2.2.m1.1.1.1.cmml" xref="S4.T2.2.m1.1.1.1"></times><ci id="S4.T2.2.m1.1.1.2.cmml" xref="S4.T2.2.m1.1.1.2">ğ‘š</ci><ci id="S4.T2.2.m1.1.1.3.cmml" xref="S4.T2.2.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">mm</annotation></semantics></math>, PCK is in %.</figcaption>
<table id="S4.T2.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.5.3" class="ltx_tr">
<th id="S4.T2.5.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.3.4.1" class="ltx_text" style="font-size:80%;">Supervision</span></th>
<th id="S4.T2.5.3.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.3.5.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<td id="S4.T2.3.1.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.3.1.1.1" class="ltx_text" style="font-size:80%;">MPJPE</span><math id="S4.T2.3.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.3.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T2.3.1.1.m1.1.1" xref="S4.T2.3.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.m1.1b"><ci id="S4.T2.3.1.1.m1.1.1.cmml" xref="S4.T2.3.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T2.4.2.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.4.2.2.1" class="ltx_text" style="font-size:80%;">PMPJPE</span><math id="S4.T2.4.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.4.2.2.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T2.4.2.2.m1.1.1" xref="S4.T2.4.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.2.m1.1b"><ci id="S4.T2.4.2.2.m1.1.1.cmml" xref="S4.T2.4.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T2.5.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.3.3.1" class="ltx_text" style="font-size:80%;">PCK</span><math id="S4.T2.5.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.5.3.3.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T2.5.3.3.m1.1.1" xref="S4.T2.5.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.3.3.m1.1b"><ci id="S4.T2.5.3.3.m1.1.1.cmml" xref="S4.T2.5.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.5.4.1" class="ltx_tr">
<th id="S4.T2.5.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.4.1.1.1" class="ltx_text" style="font-size:80%;">weak</span></th>
<th id="S4.T2.5.4.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.4.1.2.1" class="ltx_text" style="font-size:80%;">Rhodin </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.4.1.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.T2.5.4.1.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.5.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.4.1.3.1" class="ltx_text" style="font-size:80%;">121.8</span></td>
<td id="S4.T2.5.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.4.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.5.4.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.4.1.5.1" class="ltx_text" style="font-size:80%;">72.7</span></td>
</tr>
<tr id="S4.T2.5.5.2" class="ltx_tr">
<th id="S4.T2.5.5.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S4.T2.5.5.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.2.2.1" class="ltx_text" style="font-size:80%;">HMR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.2.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.T2.5.5.2.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.5.5.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.5.2.3.1" class="ltx_text" style="font-size:80%;">169.5</span></td>
<td id="S4.T2.5.5.2.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.5.2.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.5.5.2.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.5.2.5.1" class="ltx_text" style="font-size:80%;">59.6</span></td>
</tr>
<tr id="S4.T2.5.6.3" class="ltx_tr">
<th id="S4.T2.5.6.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S4.T2.5.6.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.6.3.2.1" class="ltx_text" style="font-size:80%;">Habibie </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.6.3.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S4.T2.5.6.3.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.5.6.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.6.3.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.5.6.3.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.6.3.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.5.6.3.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.6.3.5.1" class="ltx_text" style="font-size:80%;">70.4</span></td>
</tr>
<tr id="S4.T2.5.7.4" class="ltx_tr">
<th id="S4.T2.5.7.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S4.T2.5.7.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.7.4.2.1" class="ltx_text" style="font-size:80%;">Kolotouros </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.7.4.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S4.T2.5.7.4.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.5.7.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.7.4.3.1" class="ltx_text" style="font-size:80%;">124.8</span></td>
<td id="S4.T2.5.7.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.7.4.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.5.7.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.7.4.5.1" class="ltx_text" style="font-size:80%;">66.8</span></td>
</tr>
<tr id="S4.T2.5.8.5" class="ltx_tr">
<th id="S4.T2.5.8.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S4.T2.5.8.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.8.5.2.1" class="ltx_text" style="font-size:80%;">Li </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.8.5.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S4.T2.5.8.5.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.5.8.5.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.8.5.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.5.8.5.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.8.5.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.5.8.5.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.8.5.5.1" class="ltx_text" style="font-size:80%;">74.1</span></td>
</tr>
<tr id="S4.T2.5.9.6" class="ltx_tr">
<th id="S4.T2.5.9.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S4.T2.5.9.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.9.6.2.1" class="ltx_text" style="font-size:80%;">Kundu </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.9.6.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S4.T2.5.9.6.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.5.9.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.9.6.3.1" class="ltx_text" style="font-size:80%;">103.8</span></td>
<td id="S4.T2.5.9.6.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.9.6.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.5.9.6.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.9.6.5.1" class="ltx_text" style="font-size:80%;">82.1</span></td>
</tr>
<tr id="S4.T2.5.10.7" class="ltx_tr">
<th id="S4.T2.5.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.10.7.1.1" class="ltx_text" style="font-size:80%;">self</span></th>
<th id="S4.T2.5.10.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.10.7.2.1" class="ltx_text" style="font-size:80%;">Chen </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.10.7.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S4.T2.5.10.7.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.5.10.7.3" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.5.10.7.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.10.7.4.1" class="ltx_text" style="font-size:80%;">71.1</span></td>
<td id="S4.T2.5.10.7.5" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr id="S4.T2.5.11.8" class="ltx_tr">
<th id="S4.T2.5.11.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S4.T2.5.11.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.11.8.2.1" class="ltx_text" style="font-size:80%;">EpipolarPose </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.11.8.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S4.T2.5.11.8.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.5.11.8.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.11.8.3.1" class="ltx_text" style="font-size:80%;">125.7</span></td>
<td id="S4.T2.5.11.8.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.11.8.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.5.11.8.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.11.8.5.1" class="ltx_text" style="font-size:80%;">64.7</span></td>
</tr>
<tr id="S4.T2.5.12.9" class="ltx_tr">
<th id="S4.T2.5.12.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S4.T2.5.12.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.12.9.2.1" class="ltx_text" style="font-size:80%;">Iqbal </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.12.9.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T2.5.12.9.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.5.12.9.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.12.9.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">110.1</span></td>
<td id="S4.T2.5.12.9.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.12.9.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.5.12.9.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.12.9.5.1" class="ltx_text ltx_font_italic" style="font-size:80%;">76.5</span></td>
</tr>
<tr id="S4.T2.5.13.10" class="ltx_tr">
<th id="S4.T2.5.13.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S4.T2.5.13.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.13.10.2.1" class="ltx_text" style="font-size:80%;">Ours</span></th>
<td id="S4.T2.5.13.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.13.10.3.1" class="ltx_text" style="font-size:80%;">119.2</span></td>
<td id="S4.T2.5.13.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.13.10.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">68.7</span></td>
<td id="S4.T2.5.13.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.13.10.5.1" class="ltx_text" style="font-size:80%;">69.0</span></td>
</tr>
<tr id="S4.T2.5.14.11" class="ltx_tr">
<th id="S4.T2.5.14.11.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S4.T2.5.14.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.14.11.2.1" class="ltx_text" style="font-size:80%;">Ours + C</span></th>
<td id="S4.T2.5.14.11.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.14.11.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">104.0</span></td>
<td id="S4.T2.5.14.11.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.14.11.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">70.3</span></td>
<td id="S4.T2.5.14.11.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.14.11.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">77.0</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2011.14679/assets/images/cp_comparison_crop.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="209" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of CPS curves for distances from <math id="S4.F5.3.m1.1" class="ltx_Math" alttext="1mm" display="inline"><semantics id="S4.F5.3.m1.1b"><mrow id="S4.F5.3.m1.1.1" xref="S4.F5.3.m1.1.1.cmml"><mn id="S4.F5.3.m1.1.1.2" xref="S4.F5.3.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.F5.3.m1.1.1.1" xref="S4.F5.3.m1.1.1.1.cmml">â€‹</mo><mi id="S4.F5.3.m1.1.1.3" xref="S4.F5.3.m1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.F5.3.m1.1.1.1b" xref="S4.F5.3.m1.1.1.1.cmml">â€‹</mo><mi id="S4.F5.3.m1.1.1.4" xref="S4.F5.3.m1.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.3.m1.1c"><apply id="S4.F5.3.m1.1.1.cmml" xref="S4.F5.3.m1.1.1"><times id="S4.F5.3.m1.1.1.1.cmml" xref="S4.F5.3.m1.1.1.1"></times><cn type="integer" id="S4.F5.3.m1.1.1.2.cmml" xref="S4.F5.3.m1.1.1.2">1</cn><ci id="S4.F5.3.m1.1.1.3.cmml" xref="S4.F5.3.m1.1.1.3">ğ‘š</ci><ci id="S4.F5.3.m1.1.1.4.cmml" xref="S4.F5.3.m1.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.3.m1.1d">1mm</annotation></semantics></math> to <math id="S4.F5.4.m2.1" class="ltx_Math" alttext="300mm" display="inline"><semantics id="S4.F5.4.m2.1b"><mrow id="S4.F5.4.m2.1.1" xref="S4.F5.4.m2.1.1.cmml"><mn id="S4.F5.4.m2.1.1.2" xref="S4.F5.4.m2.1.1.2.cmml">300</mn><mo lspace="0em" rspace="0em" id="S4.F5.4.m2.1.1.1" xref="S4.F5.4.m2.1.1.1.cmml">â€‹</mo><mi id="S4.F5.4.m2.1.1.3" xref="S4.F5.4.m2.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.F5.4.m2.1.1.1b" xref="S4.F5.4.m2.1.1.1.cmml">â€‹</mo><mi id="S4.F5.4.m2.1.1.4" xref="S4.F5.4.m2.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.4.m2.1c"><apply id="S4.F5.4.m2.1.1.cmml" xref="S4.F5.4.m2.1.1"><times id="S4.F5.4.m2.1.1.1.cmml" xref="S4.F5.4.m2.1.1.1"></times><cn type="integer" id="S4.F5.4.m2.1.1.2.cmml" xref="S4.F5.4.m2.1.1.2">300</cn><ci id="S4.F5.4.m2.1.1.3.cmml" xref="S4.F5.4.m2.1.1.3">ğ‘š</ci><ci id="S4.F5.4.m2.1.1.4.cmml" xref="S4.F5.4.m2.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.4.m2.1d">300mm</annotation></semantics></math> with corresponding AUC for the Human3.6M dataset. A higher value means a better result, <span id="S4.F5.6.1" class="ltx_ERROR undefined">\ie</span>the leftmost curve achieves the best result in terms of CP.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2011.14679/assets/images/qualitative_results.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="198" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Qualitative results for the Human3.6M dataset (top) and for the challenging SkiPose dataset (bottom).</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Moving cameras</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Our main motivation is to enable 3D human pose estimation in the wild by using a multi-view camera system with temporally synchronised cameras.
Moreover, the performed activity should be very challenging to capture and hard to simulate in a traditional motion capture studio.
That means a straight-forward activity domain transfer, <span id="S4.SS4.p1.1.1" class="ltx_ERROR undefined">\eg</span>pretraining or combined training with a different dataset, is not reasonable.
The SkiPose dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> comprises all challenges of this motivation.
It features competitive alpine skiers performing giant slalom runs.
To record this dataset huge effort was taken to setup and calibrate the cameras and keep them in place after calibration.
Additionally, the cameras are rotating and zooming to keep the alpine skier in the field of view.
The proposed method can deal with all these difficulties since it does not require a calibrated or static setup and works with multiple synchronised cameras.
Since the camera setup is not static we cannot apply the relative rotation constraint here.
TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.4 Moving cameras â€£ 4 Experiments â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows our results in comparison to Rhodin <span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS4.p1.1.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.SS4.p1.1.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>.
Since they consider a (sparse-)supervised setting and known camera positions a direct comparison is not possible and only serves as a baseline.
Fig.Â <a href="#S4.F6" title="Figure 6 â€£ 4.3 Quantitative Evaluation on Human3.6M and 3DHP â€£ 4 Experiments â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows qualitative results for the SkiPose dataset in the second row.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation results for the SkiPose dataset. The result for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> was estimated from a bar plot in the paper. Since <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> considers a (sparse-)supervised setting and known camera position it is only shown as a baseline. MPJPE, PMPJPE and CPS are given in <math id="S4.T3.2.m1.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S4.T3.2.m1.1b"><mrow id="S4.T3.2.m1.1.1" xref="S4.T3.2.m1.1.1.cmml"><mi id="S4.T3.2.m1.1.1.2" xref="S4.T3.2.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T3.2.m1.1.1.1" xref="S4.T3.2.m1.1.1.1.cmml">â€‹</mo><mi id="S4.T3.2.m1.1.1.3" xref="S4.T3.2.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.m1.1c"><apply id="S4.T3.2.m1.1.1.cmml" xref="S4.T3.2.m1.1.1"><times id="S4.T3.2.m1.1.1.1.cmml" xref="S4.T3.2.m1.1.1.1"></times><ci id="S4.T3.2.m1.1.1.2.cmml" xref="S4.T3.2.m1.1.1.2">ğ‘š</ci><ci id="S4.T3.2.m1.1.1.3.cmml" xref="S4.T3.2.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.m1.1d">mm</annotation></semantics></math>, PCK is in %.</figcaption>
<table id="S4.T3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.6.4" class="ltx_tr">
<th id="S4.T3.6.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.4.5.1" class="ltx_text" style="font-size:80%;">Supervision</span></th>
<th id="S4.T3.6.4.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.4.6.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<th id="S4.T3.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.3.1.1.1" class="ltx_text" style="font-size:80%;">MPJPE</span><math id="S4.T3.3.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.3.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.3.1.1.m1.1.1" xref="S4.T3.3.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.1.1.m1.1b"><ci id="S4.T3.3.1.1.m1.1.1.cmml" xref="S4.T3.3.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T3.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.4.2.2.1" class="ltx_text" style="font-size:80%;">PMPJPE</span><math id="S4.T3.4.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.4.2.2.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.4.2.2.m1.1.1" xref="S4.T3.4.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.2.2.m1.1b"><ci id="S4.T3.4.2.2.m1.1.1.cmml" xref="S4.T3.4.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T3.5.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.5.3.3.1" class="ltx_text" style="font-size:80%;">PCK</span><math id="S4.T3.5.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.5.3.3.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.5.3.3.m1.1.1" xref="S4.T3.5.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.3.3.m1.1b"><ci id="S4.T3.5.3.3.m1.1.1.cmml" xref="S4.T3.5.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T3.6.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.6.4.4.1" class="ltx_text" style="font-size:80%;">CPS</span><math id="S4.T3.6.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.6.4.4.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.6.4.4.m1.1.1" xref="S4.T3.6.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.4.4.m1.1b"><ci id="S4.T3.6.4.4.m1.1.1.cmml" xref="S4.T3.6.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.4.4.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.6.5.1" class="ltx_tr">
<th id="S4.T3.6.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.5.1.1.1" class="ltx_text" style="font-size:80%;">weak</span></th>
<th id="S4.T3.6.5.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.6.5.1.2.1" class="ltx_text" style="font-size:80%;">Rhodin </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.6.5.1.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.T3.6.5.1.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.6.5.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.5.1.3.1" class="ltx_text" style="font-size:80%;">85</span></td>
<td id="S4.T3.6.5.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.5.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.6.5.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.5.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.6.5.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.5.1.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T3.6.6.2" class="ltx_tr">
<th id="S4.T3.6.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.6.2.1.1" class="ltx_text" style="font-size:80%;">self</span></th>
<th id="S4.T3.6.6.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.6.2.2.1" class="ltx_text" style="font-size:80%;">Ours</span></th>
<td id="S4.T3.6.6.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.6.2.3.1" class="ltx_text" style="font-size:80%;">128.1</span></td>
<td id="S4.T3.6.6.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.6.2.4.1" class="ltx_text" style="font-size:80%;">89.6</span></td>
<td id="S4.T3.6.6.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.6.2.5.1" class="ltx_text" style="font-size:80%;">67.1</span></td>
<td id="S4.T3.6.6.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.6.6.2.6.1" class="ltx_text" style="font-size:80%;">108.7</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Studies</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.2" class="ltx_p">To analyze our approach we perform a number of ablation studies.
First, to simulate a practical setting with limited resources, we reduced the number of cameras to train the model.
Table.Â <a href="#S4.T4" title="Table 4 â€£ 4.5 Ablation Studies â€£ 4 Experiments â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the results for the training with only the first two or three cameras.
While the performance expectedly slightly drops due to the lower number of training samples and views our approach still produces good results which underlines its applicability in real world scenarios.
In a second experiment we show the impact of using the confidences from the 2D joint estimator as inputs to the network and for the calculation of the reprojection error.
They significantly impact the performance of our model and produce a gain of <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="19.4" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mn id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">19.4</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><cn type="float" id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">19.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">19.4</annotation></semantics></math>mm in MPJPE and <math id="S4.SS5.p1.2.m2.1" class="ltx_Math" alttext="11.2" display="inline"><semantics id="S4.SS5.p1.2.m2.1a"><mn id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml">11.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><cn type="float" id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">11.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">11.2</annotation></semantics></math>mm in PMPJPE.
To prove that the proposed mixing of rotations and poses to achieve view- and camera-consistency is superior to simple equality constraints, we performed experiments with such equality constraints.
The results show that indeed our mixing approach is an essential part to make it work.
We also trained with ground truth 2D annotations to compute a lower bound for the proposed method.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation studies on the Human3.6M dataset. All values are given in <math id="S4.T4.2.m1.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S4.T4.2.m1.1b"><mrow id="S4.T4.2.m1.1.1" xref="S4.T4.2.m1.1.1.cmml"><mi id="S4.T4.2.m1.1.1.2" xref="S4.T4.2.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T4.2.m1.1.1.1" xref="S4.T4.2.m1.1.1.1.cmml">â€‹</mo><mi id="S4.T4.2.m1.1.1.3" xref="S4.T4.2.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.2.m1.1c"><apply id="S4.T4.2.m1.1.1.cmml" xref="S4.T4.2.m1.1.1"><times id="S4.T4.2.m1.1.1.1.cmml" xref="S4.T4.2.m1.1.1.1"></times><ci id="S4.T4.2.m1.1.1.2.cmml" xref="S4.T4.2.m1.1.1.2">ğ‘š</ci><ci id="S4.T4.2.m1.1.1.3.cmml" xref="S4.T4.2.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.m1.1d">mm</annotation></semantics></math>.</figcaption>
<table id="S4.T4.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.5.3" class="ltx_tr">
<th id="S4.T4.5.3.4" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T4.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span id="S4.T4.3.1.1.1" class="ltx_text" style="font-size:80%;">MPJPE</span><math id="S4.T4.3.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.3.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T4.3.1.1.m1.1.1" xref="S4.T4.3.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.1.1.m1.1b"><ci id="S4.T4.3.1.1.m1.1.1.cmml" xref="S4.T4.3.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T4.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span id="S4.T4.4.2.2.1" class="ltx_text" style="font-size:80%;">PMPJPE</span><math id="S4.T4.4.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.4.2.2.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T4.4.2.2.m1.1.1" xref="S4.T4.4.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.2.2.m1.1b"><ci id="S4.T4.4.2.2.m1.1.1.cmml" xref="S4.T4.4.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T4.5.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span id="S4.T4.5.3.3.1" class="ltx_text" style="font-size:80%;">CPS</span><math id="S4.T4.5.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.5.3.3.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T4.5.3.3.m1.1.1" xref="S4.T4.5.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.3.3.m1.1b"><ci id="S4.T4.5.3.3.m1.1.1.cmml" xref="S4.T4.5.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.5.4.1" class="ltx_tr">
<th id="S4.T4.5.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T4.5.4.1.1.1" class="ltx_text" style="font-size:80%;">2 cams</span></th>
<td id="S4.T4.5.4.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.5.4.1.2.1" class="ltx_text" style="font-size:80%;">82.7</span></td>
<td id="S4.T4.5.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.5.4.1.3.1" class="ltx_text" style="font-size:80%;">61.2</span></td>
<td id="S4.T4.5.4.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.5.4.1.4.1" class="ltx_text" style="font-size:80%;">148.5</span></td>
</tr>
<tr id="S4.T4.5.5.2" class="ltx_tr">
<th id="S4.T4.5.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.5.5.2.1.1" class="ltx_text" style="font-size:80%;">3 cams</span></th>
<td id="S4.T4.5.5.2.2" class="ltx_td ltx_align_center"><span id="S4.T4.5.5.2.2.1" class="ltx_text" style="font-size:80%;">82.0</span></td>
<td id="S4.T4.5.5.2.3" class="ltx_td ltx_align_center"><span id="S4.T4.5.5.2.3.1" class="ltx_text" style="font-size:80%;">62.2</span></td>
<td id="S4.T4.5.5.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.5.5.2.4.1" class="ltx_text" style="font-size:80%;">145.6</span></td>
</tr>
<tr id="S4.T4.5.6.3" class="ltx_tr">
<th id="S4.T4.5.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.5.6.3.1.1" class="ltx_text" style="font-size:80%;">w/o confidences</span></th>
<td id="S4.T4.5.6.3.2" class="ltx_td ltx_align_center"><span id="S4.T4.5.6.3.2.1" class="ltx_text" style="font-size:80%;">95.6</span></td>
<td id="S4.T4.5.6.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.5.6.3.3.1" class="ltx_text" style="font-size:80%;">65.0</span></td>
<td id="S4.T4.5.6.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.5.6.3.4.1" class="ltx_text" style="font-size:80%;">142.5</span></td>
</tr>
<tr id="S4.T4.5.7.4" class="ltx_tr">
<th id="S4.T4.5.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.5.7.4.1.1" class="ltx_text" style="font-size:80%;">ground truth 2D</span></th>
<td id="S4.T4.5.7.4.2" class="ltx_td ltx_align_center"><span id="S4.T4.5.7.4.2.1" class="ltx_text" style="font-size:80%;">65.9</span></td>
<td id="S4.T4.5.7.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.5.7.4.3.1" class="ltx_text" style="font-size:80%;">51.4</span></td>
<td id="S4.T4.5.7.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.5.7.4.4.1" class="ltx_text" style="font-size:80%;">187.1</span></td>
</tr>
<tr id="S4.T4.5.8.5" class="ltx_tr">
<th id="S4.T4.5.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.5.8.5.1.1" class="ltx_text" style="font-size:80%;">direct pose equality</span></th>
<td id="S4.T4.5.8.5.2" class="ltx_td ltx_align_center"><span id="S4.T4.5.8.5.2.1" class="ltx_text" style="font-size:80%;">554.3</span></td>
<td id="S4.T4.5.8.5.3" class="ltx_td ltx_align_center"><span id="S4.T4.5.8.5.3.1" class="ltx_text" style="font-size:80%;">360.8</span></td>
<td id="S4.T4.5.8.5.4" class="ltx_td ltx_align_center"><span id="S4.T4.5.8.5.4.1" class="ltx_text" style="font-size:80%;">0.0</span></td>
</tr>
<tr id="S4.T4.5.9.6" class="ltx_tr">
<th id="S4.T4.5.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.5.9.6.1.1" class="ltx_text" style="font-size:80%;">direct camera equality</span></th>
<td id="S4.T4.5.9.6.2" class="ltx_td ltx_align_center"><span id="S4.T4.5.9.6.2.1" class="ltx_text" style="font-size:80%;">617.9</span></td>
<td id="S4.T4.5.9.6.3" class="ltx_td ltx_align_center"><span id="S4.T4.5.9.6.3.1" class="ltx_text" style="font-size:80%;">374.5</span></td>
<td id="S4.T4.5.9.6.4" class="ltx_td ltx_align_center"><span id="S4.T4.5.9.6.4.1" class="ltx_text" style="font-size:80%;">0.0</span></td>
</tr>
<tr id="S4.T4.5.10.7" class="ltx_tr">
<th id="S4.T4.5.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.5.10.7.1.1" class="ltx_text" style="font-size:80%;">full (4 cams)</span></th>
<td id="S4.T4.5.10.7.2" class="ltx_td ltx_align_center"><span id="S4.T4.5.10.7.2.1" class="ltx_text" style="font-size:80%;">81.9</span></td>
<td id="S4.T4.5.10.7.3" class="ltx_td ltx_align_center"><span id="S4.T4.5.10.7.3.1" class="ltx_text" style="font-size:80%;">53.0</span></td>
<td id="S4.T4.5.10.7.4" class="ltx_td ltx_align_center"><span id="S4.T4.5.10.7.4.1" class="ltx_text" style="font-size:80%;">167.6</span></td>
</tr>
<tr id="S4.T4.5.11.8" class="ltx_tr">
<th id="S4.T4.5.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.5.11.8.1.1" class="ltx_text" style="font-size:80%;">full+C (4 cams)</span></th>
<td id="S4.T4.5.11.8.2" class="ltx_td ltx_align_center"><span id="S4.T4.5.11.8.2.1" class="ltx_text" style="font-size:80%;">74.3</span></td>
<td id="S4.T4.5.11.8.3" class="ltx_td ltx_align_center"><span id="S4.T4.5.11.8.3.1" class="ltx_text" style="font-size:80%;">53.0</span></td>
<td id="S4.T4.5.11.8.4" class="ltx_td ltx_align_center"><span id="S4.T4.5.11.8.4.1" class="ltx_text" style="font-size:80%;">167.3</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Are We Learning a Canonical Pose Basis?</h3>

<figure id="S4.F7" class="ltx_figure"><img src="/html/2011.14679/assets/images/canonical_all_2.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="252" height="119" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Visualization of the canonical pose space from the Human3.6M dataset. Left and middle: Canonical poses for the same 3D pose predicted from 4 different views. Right: 10 randomly sampled canonical poses. Our network automatically learns a disentanglement of a 2D pose into 3D and a camera rotation.</figcaption>
</figure>
<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.3" class="ltx_p">Finally, we evaluate the claim that we learn a canonical pose basis.
To visualize the disentanglement for different 3D poses Fig.Â <a href="#S4.F7" title="Figure 7 â€£ 4.6 Are We Learning a Canonical Pose Basis? â€£ 4 Experiments â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows a visualization of reconstructed 3D poses in the canonical basis obtained from 4 views on the left and in the middle.
The right image shows <math id="S4.SS6.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS6.p1.1.m1.1a"><mn id="S4.SS6.p1.1.m1.1.1" xref="S4.SS6.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.1.m1.1b"><cn type="integer" id="S4.SS6.p1.1.m1.1.1.cmml" xref="S4.SS6.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.1.m1.1c">10</annotation></semantics></math> randomly picked reconstructions in the canonical space.
Although the similarity of the poses is not enforced directly as described in Sec.Â <a href="#S3.SS2" title="3.2 View-consistency â€£ 3 Method â€£ CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> the poses are similarly oriented in the canonical space.
In particular, the hip joints are aligned which leads to a similar alignment of the upper body.
The standard deviation for the hip joints of the canonical poses from the test set of Human3.6M are <math id="S4.SS6.p1.2.m2.1" class="ltx_Math" alttext="7.9mm" display="inline"><semantics id="S4.SS6.p1.2.m2.1a"><mrow id="S4.SS6.p1.2.m2.1.1" xref="S4.SS6.p1.2.m2.1.1.cmml"><mn id="S4.SS6.p1.2.m2.1.1.2" xref="S4.SS6.p1.2.m2.1.1.2.cmml">7.9</mn><mo lspace="0em" rspace="0em" id="S4.SS6.p1.2.m2.1.1.1" xref="S4.SS6.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S4.SS6.p1.2.m2.1.1.3" xref="S4.SS6.p1.2.m2.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS6.p1.2.m2.1.1.1a" xref="S4.SS6.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S4.SS6.p1.2.m2.1.1.4" xref="S4.SS6.p1.2.m2.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.2.m2.1b"><apply id="S4.SS6.p1.2.m2.1.1.cmml" xref="S4.SS6.p1.2.m2.1.1"><times id="S4.SS6.p1.2.m2.1.1.1.cmml" xref="S4.SS6.p1.2.m2.1.1.1"></times><cn type="float" id="S4.SS6.p1.2.m2.1.1.2.cmml" xref="S4.SS6.p1.2.m2.1.1.2">7.9</cn><ci id="S4.SS6.p1.2.m2.1.1.3.cmml" xref="S4.SS6.p1.2.m2.1.1.3">ğ‘š</ci><ci id="S4.SS6.p1.2.m2.1.1.4.cmml" xref="S4.SS6.p1.2.m2.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.2.m2.1c">7.9mm</annotation></semantics></math> and <math id="S4.SS6.p1.3.m3.1" class="ltx_Math" alttext="7.7mm" display="inline"><semantics id="S4.SS6.p1.3.m3.1a"><mrow id="S4.SS6.p1.3.m3.1.1" xref="S4.SS6.p1.3.m3.1.1.cmml"><mn id="S4.SS6.p1.3.m3.1.1.2" xref="S4.SS6.p1.3.m3.1.1.2.cmml">7.7</mn><mo lspace="0em" rspace="0em" id="S4.SS6.p1.3.m3.1.1.1" xref="S4.SS6.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S4.SS6.p1.3.m3.1.1.3" xref="S4.SS6.p1.3.m3.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS6.p1.3.m3.1.1.1a" xref="S4.SS6.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S4.SS6.p1.3.m3.1.1.4" xref="S4.SS6.p1.3.m3.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.3.m3.1b"><apply id="S4.SS6.p1.3.m3.1.1.cmml" xref="S4.SS6.p1.3.m3.1.1"><times id="S4.SS6.p1.3.m3.1.1.1.cmml" xref="S4.SS6.p1.3.m3.1.1.1"></times><cn type="float" id="S4.SS6.p1.3.m3.1.1.2.cmml" xref="S4.SS6.p1.3.m3.1.1.2">7.7</cn><ci id="S4.SS6.p1.3.m3.1.1.3.cmml" xref="S4.SS6.p1.3.m3.1.1.3">ğ‘š</ci><ci id="S4.SS6.p1.3.m3.1.1.4.cmml" xref="S4.SS6.p1.3.m3.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.3.m3.1c">7.7mm</annotation></semantics></math> for the right and left hip, respectively.
This underlines that pose and rotation are disentangled plausibly by our network.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We present CanonPose, a neural network trained for single image 3D human pose estimation from multi-view data without 2D or 3D annotations.
Given a pretrained 2D human pose estimator we exploit multi-view consistency to automatically decompose a 2D observation into a canonical 3D pose and a camera rotation that is used to reproject it back to the observation after mixing.
Since our approach does not require either 2D nor 3D annotations for the multi-view data it is practically applicable to many in-the-wild scenarios, including outdoor scenes with moving cameras.
We not only achieve state-of-the-art results on benchmark datasets with less prerequisites compared to other approaches, but also show promising results on challenging outdoor scenes.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Ching-Hang Chen and Deva Ramanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">3d human pose estimation= 2d pose estimation+ matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 7035â€“7043, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dylan Drover, Stefan Stojanov,
and JamesÂ M Rehg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Unsupervised 3d pose estimation with geometric self-supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 5714â€“5724, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Zhihua Chen, Xiaoli Liu, Bing Sheng, and Ping Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Garnet: Graph attention residual networks based on adversarial
learning for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Computer Graphics</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 276â€“287, Cham, 2020.
Springer International Publishing.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Dylan Drover, Ching-Hang Chen, Amit Agrawal, Ambrish Tyagi, and Cong
PhuocÂ Huynh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Can 3d pose be learned from 2d projections alone?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
Workshops (ECCV)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 0â€“0, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Yu Du, Yongkang Wong, Yonghao Liu, Feilin Han, Yilin Gui, Zhen Wang, Mohan
Kankanhalli, and Weidong Geng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Marker-less 3D human motion capture with monocular image sequence
and height-maps.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 20â€“36.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, and Song-Chun Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Learning pose grammar to encode human body configuration for 3d pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In SheilaÂ A. McIlraith and KilianÂ Q. Weinberger, editors, </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages
6821â€“6828. AAAI Press, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">RMPE: Regional multi-person pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Julian Habekost, Takaaki Shiratori, Yuting Ye, and Taku Komura.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Learning 3d global human motion estimation from unpaired, disjoint
datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, Gerard Pons-Moll, and Christian
Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">In the wild human pose estimation using explicit 2d features and
intermediate 3d representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Human3.6m: Large scale datasets and predictive methods for 3d human
sensing in natural environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI)</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 36(7):1325â€“1339, 2014.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Umar Iqbal, Pavlo Molchanov, and Jan Kautz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Weakly-supervised 3d human pose learning via multi-view images in the
wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Angjoo Kanazawa, MichaelÂ J. Black, DavidÂ W. Jacobs, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">End-to-end recovery of human shape and pose.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 7122â€“7131. IEEE Computer Society, 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Muhammed Kocabas, Nikos Athanasiou, and MichaelÂ J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Vibe: Video inference for human body pose and shape estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Muhammed Kocabas, Salih Karagoz, and Emre Akbas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Self-supervised learning of 3d human pose using multi-view geometry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Nikos Kolotouros, Georgios Pavlakos, MichaelÂ J. Black, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Learning to reconstruct 3D human pose and shape via model-fitting
in the loop.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings International Conference on Computer Vision
(ICCV)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 2252â€“2261. IEEE, Oct. 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">ISSN: 2380-7504.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
JogendraÂ Nath Kundu, Siddharth Seth, Varun Jampani, Mugalodi Rakesh,
R.Â Venkatesh Babu, and Anirban Chakraborty.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Self-supervised 3d human pose estimation via part guided novel image
synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Crowdpose: Efficient crowded scenes pose estimation and a new
benchmark.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.00324</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Sijin Li and AntoniÂ B. Chan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">3d human pose estimation from monocular images with deep
convolutional neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Asian Conference on Computer Vision (ACCV)</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, volume 9004,
pages 332â€“347, Germany, 11 2014. Springer Verlag.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Shichao Li, Lei Ke, Kevin Pratama, Yu-Wing Tai, Chi-Keung Tang, and Kwang-Ting
Cheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Cascaded deep monocular 3d human pose estimation with evolutionary
training data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Sijin Li, Weichen Zhang, and AntoniÂ B. Chan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Maximum-margin structured learning with deep networks for 3d human
pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision (ICCV)</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, ICCV
â€™15, pages 2848â€“2856, Washington, DC, USA, 2015. IEEE Computer Society.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Yang Li, Kan Li, Shuai Jiang, Ziyue Zhang, Congzhentao Huang, and Richard YiÂ Da
Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Geometry-driven self-supervised method for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">,
34(07):11442â€“11449, Apr. 2020.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Chenxu Luo, Xiao Chu, and AlanÂ L. Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Orinet: A fully convolutional network for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">British Machine Vision Conference (BMVC)</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pageÂ 92, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Julieta Martinez, Rayat Hossain, Javier Romero, and JamesÂ J. Little.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">A simple yet effective baseline for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision (ICCV)</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko,
Weipeng Xu, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Monocular 3d human pose estimation in the wild using improved cnn
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on 3D Vision (3DV)</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Mohamed
Elgharib, Pascal Fua, Hans-Peter Seidel, Helge Rhodin, Gerard Pons-Moll, and
Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">XNect: Real-time multi-person 3D motion capture with a single
RGB camera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">volumeÂ 39, 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin, Mohammad
Shafiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Vnect: Real-time 3d human pose estimation with a single rgb camera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, volumeÂ 36, 7 2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Francesc Moreno-Noguer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">3d human pose estimation from a single image via distance matrix
regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Qiang Nie, Ziwei Liu, and Yunhui Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Unsupervised 3d human pose representation with viewpoint and pose
disentanglement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Sungheon Park, Jihye Hwang, and Nojun Kwak.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">3d human pose estimation using convolutional neural networks with 2d
pose information.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages
156â€“169, 2016.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Ordinal depth supervision for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 7307â€“7316, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Georgios Pavlakos, Xiaowei Zhou, KonstantinosÂ G. Derpanis, and Kostas
Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Coarse-to-fine volumetric prediction for single-image 3d human pose.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">,
pages 1263â€“1272, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Mir Rayat ImtiazÂ Hossain and JamesÂ J. Little.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Exploiting temporal information for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Helge Rhodin, Victor Constantin, Isinsu Katircioglu, Mathieu Salzmann, and
Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Neural scene decomposition for multi-person motion capture.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 7703â€“7713, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Helge Rhodin, Mathieu Salzmann, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Unsupervised geometry-aware representation learning for 3d human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Helge Rhodin, JÃ¶rg SpÃ¶rri, Isinsu Katircioglu, Victor Constantin,
FrÃ©dÃ©ric Meyer, Erich MÃ¼ller, Mathieu Salzmann, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Learning monocular 3d human pose estimation from multi-view images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 8437â€“8446, 2018.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Guillaume Rochette, Chris Russell, and Richard Bowden.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Weakly-supervised 3d pose estimation from a single image using
multi-view consistency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">BMVC</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Gregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">LCR-Net: Localization-Classification-Regression for Human Pose.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 1216â€“1224, Honolulu, United States, July 2017. IEEE.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
JÃ¶rg SpÃ¶rri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Reasearch dedicated to sports injury prevention-theâ€™sequence of
preventionâ€™on the example of alpine ski racing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Habilitation with Venia Docendi in Biomechanics</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 1(2):7, 2016.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Xiao Sun, Jiaxiang Shang, Shuang Liang, and Yichen Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Compositional human pose regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 2602â€“2611, 2017.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, and Yichen Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Integral human pose regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 529â€“545, 2018.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Bugra Tekin, Isinsu Katircioglu, Mathieu Salzmann, Vincent Lepetit, and Pascal
Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Structured prediction of 3d human pose with deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">British Machine Vision Conference (BMVC)</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Denis Tome, Chris Russell, and Lourdes Agapito.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Lifting from the deep: Convolutional 3d pose estimation from a single
image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, July 2017.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Hsiao-YuÂ F. Tung, AdamÂ W. Harley, William Seto, and Katerina Fragkiadaki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and
image-to-image translation from unpaired supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision (ICCV)</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages
4364â€“4372, 2017.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Bastian Wandt and Bodo Rosenhahn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Repnet: Weakly supervised training of an adversarial reprojection
network for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Chaoyang Wang, Chen Kong, and Simon Lucey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Distill knowledge from nrsfm for weakly supervised 3d pose learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, October 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Jiajun Wu, Tianfan Xue, JosephÂ J Lim, Yuandong Tian, JoshuaÂ B Tenenbaum,
Antonio Torralba, and WilliamÂ T Freeman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Single image 3d interpreter network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Jingwei Xu, Zhenbo Yu, Bingbing Ni, Jiancheng Yang, Xiaokang Yang, and Wenjun
Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Deep kinematics analysis for monocular 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Andrei Zanfir, EduardÂ Gabriel Bazavan, Hongyi Xu, WilliamÂ T. Freeman, Rahul
Sukthankar, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Weakly supervised 3d human pose and shape reconstruction with
normalizing flows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
editors, </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision â€“ ECCV 2020</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 465â€“481, Cham, 2020.
Springer International Publishing.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Kun Zhou, Xiaoguang Han, Nianjuan Jiang, Kui Jia, and Jiangbo Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Hemlets pose: Learning part-centric heatmap triplets for accurate 3d
human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 2344â€“2353, 2019.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2011.14678" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2011.14679" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2011.14679">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2011.14679" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2011.14680" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 04:08:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
