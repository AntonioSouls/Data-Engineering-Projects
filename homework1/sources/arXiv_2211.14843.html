<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.14843] Learning Object-Language Alignments for Open-Vocabulary Object Detection</title><meta property="og:description" content="Existing object detection methods are bounded in a fixed-set vocabulary by costly labeled data.
When dealing with novel categories, the model has to be retrained with more bounding box annotations.
Natural language supâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning Object-Language Alignments for Open-Vocabulary Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Learning Object-Language Alignments for Open-Vocabulary Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.14843">

<!--Generated on Thu Mar 14 05:42:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Learning Object-Language Alignments for Open-Vocabulary Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_text ltx_align_center">Chuang Lin<sup id="id1.1.id1.1" class="ltx_sup">1</sup>â€ƒâ€ƒPeize Sun<sup id="id1.1.id1.2" class="ltx_sup">3</sup>â€ƒâ€ƒYi Jiang<sup id="id1.1.id1.3" class="ltx_sup">2</sup>â€ƒâ€ƒPing Luo<sup id="id1.1.id1.4" class="ltx_sup">3</sup>â€ƒâ€ƒLizhen Qu<sup id="id1.1.id1.5" class="ltx_sup">1</sup></span>
<br class="ltx_break">
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_align_center ltx_font_bold">
Gholamreza Haffari<sup id="id2.2.id2.1" class="ltx_sup"><span id="id2.2.id2.1.1" class="ltx_text ltx_font_medium">1</span></sup>â€ƒâ€ƒZehuan Yuan<sup id="id2.2.id2.2" class="ltx_sup"><span id="id2.2.id2.2.1" class="ltx_text ltx_font_medium">2</span></sup>â€ƒâ€ƒJianfei Cai<sup id="id2.2.id2.3" class="ltx_sup"><span id="id2.2.id2.3.1" class="ltx_text ltx_font_medium">1</span></sup></span>
<br class="ltx_break">
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_align_center"><sup id="id3.3.id3.1" class="ltx_sup">1</sup> Monash University â€ƒâ€ƒ<sup id="id3.3.id3.2" class="ltx_sup">2</sup> ByteDance â€ƒâ€ƒ<sup id="id3.3.id3.3" class="ltx_sup">3</sup> The University of Hong Kong</span>
<br class="ltx_break">
<br class="ltx_break">
</span><span class="ltx_author_notes">This work was performed while Chuang Lin (chuang.lin@monash.edu) worked as an intern at
ByteDance.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Existing object detection methods are bounded in a fixed-set vocabulary by costly labeled data.
When dealing with novel categories, the model has to be retrained with more bounding box annotations.
Natural language supervision is an attractive alternative for its annotation-free attributes and broader object concepts.
However, learning open-vocabulary object detection from language is challenging since image-text pairs do not contain fine-grained object-language alignments.
Previous solutions rely on either expensive grounding annotations or distilling classification-oriented vision models.
In this paper,
we propose a novel open-vocabulary object detection framework directly learning from image-text pair data.
We formulate object-language alignment as <span id="id4.id1.1" class="ltx_text ltx_font_italic">a set matching problem</span> between a set of image region features and a set of word embeddings. It enables us to train an open-vocabulary object detector on image-text pairs in a much simple and effective way.
Extensive experiments on two benchmark datasets, COCO and LVIS, demonstrate our superior performance over the competing approaches on novel categories, e.g. achieving 32.0% mAP on COCO and 21.7% mask mAP on LVIS.
Code is available at: <a target="_blank" href="https://github.com/clin1223/VLDet" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/clin1223/VLDet</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">In the past few years, significant breakthroughs of visual models have been witnessed on close-set recognition, where the object categories are pre-defined by the datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Johnson-Roberson etÂ al., <a href="#bib.bib27" title="" class="ltx_ref">2016</a>; Sakaridis etÂ al., <a href="#bib.bib43" title="" class="ltx_ref">2018</a>; Geiger etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2013</a>; Yu etÂ al., <a href="#bib.bib51" title="" class="ltx_ref">2018</a>)</cite>. Such achievements are not the final answer to artificial intelligence, since human intelligence can perceive the world in an open environment. This motivates the community moving towards the open-world setting, where visual recognition models are expected to recognize arbitrary novel categories in a zero-shot manner.
Towards this goal, learning visual models from language supervisions becomes more and more popular due to their open attributes, rich semantics, various data sources and nearly-free annotation cost. Particularly, many recent image classification works
<cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a href="#bib.bib52" title="" class="ltx_ref">2022</a>; Yuan etÂ al., <a href="#bib.bib54" title="" class="ltx_ref">2021</a>; Zhai etÂ al., <a href="#bib.bib56" title="" class="ltx_ref">2021</a>; Jia etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Radford etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>; Zhou etÂ al., <a href="#bib.bib60" title="" class="ltx_ref">2021a</a>; Rao etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2021</a>; Huynh etÂ al., <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> successfully expand their vocabulary sizes by learning from a large set of image-text pairs and demonstrate very impressive zero-shot ability. Following the success in open-vocabulary image classification, a natural extension is to tackle the object detection task,
i.e., open-vocabulary object detection (OVOD).</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Object detection is a fundamental problem in computer vision aiming to localize object instances in an image and classify their categoriesÂ <cite class="ltx_cite ltx_citemacro_citep">(Girshick, <a href="#bib.bib20" title="" class="ltx_ref">2015</a>; Ren etÂ al., <a href="#bib.bib42" title="" class="ltx_ref">2015</a>; Lin etÂ al., <a href="#bib.bib33" title="" class="ltx_ref">2017</a>; He etÂ al., <a href="#bib.bib24" title="" class="ltx_ref">2017</a>; Cai &amp; Vasconcelos, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>.
Training an ordinary object detector relies on manually-annotated bounding boxes and categories for objects (Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> Left). Similarly, learning object detection from language supervision requires object-language annotations. Prior studies investigated using such annotations for visual grounding tasksÂ <cite class="ltx_cite ltx_citemacro_citep">(Krishna etÂ al., <a href="#bib.bib29" title="" class="ltx_ref">2017</a>; Yu etÂ al., <a href="#bib.bib53" title="" class="ltx_ref">2016</a>; Plummer etÂ al., <a href="#bib.bib38" title="" class="ltx_ref">2015</a>)</cite>.
Most of the existing open-vocabulary object detection worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Kamath etÂ al., <a href="#bib.bib28" title="" class="ltx_ref">2021</a>; Cai etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite> depend entirely or partially on the grounding annotations, which however is not scalable because such annotations are even more costly than annotating object detection data.
To reduce the annotation cost for open-vocabulary object detection, a handful of recent studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Du etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2022</a>; Gu etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite> distill visual region features from classification-oriented models by cropping images. However, their performance is limited by the pre-trained models, trained based on global image-text matching instead of region-word matching.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In this paper, we propose a simple yet effective end-to-end vision-and-language framework for open-vocabulary object detection, termed <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">VLDet</span>, which directly trains an object detector from image-text pairs without
relying on expensive grounding annotations or distilling classification-oriented vision models.
Our key insight is that
extracting region-word pairs from image-text pairs can be formulated as <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">a set matching problem</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Carion etÂ al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>; Sun etÂ al., <a href="#bib.bib46" title="" class="ltx_ref">2021b</a>; <a href="#bib.bib45" title="" class="ltx_ref">a</a>; Fang etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>, which can be effectively solved by finding a bipartite matchingÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuhn, <a href="#bib.bib30" title="" class="ltx_ref">1955</a>)</cite> between regions and words with minimal global matching cost.
Specifically, we treat image region features as a set and word embedding as another set with the the dot-product similarity as region-word alignment score.
To find the lowest cost, the optimal bipartite matching will force each image region to be aligned with its corresponding word under the global supervision of the image-text pair.
By replacing the classification loss in object detection with the optimal region-word alignment loss, our approach can help match each image region to the corresponding word and accomplish the object detection task.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">We conduct extensive experiments on the open-vocabulary setting, in which an object detection dataset with localized object annotations is reserved for base categories, while the dataset of image-text pairs is used for novel categories.
The results show that our method significantly improves the performance of detecting novel categories.
On the open-vocabulary dataset COCO, our method outperforms the SOTA model PB-OVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite> by 1.2% mAP in terms of detecting novel classes using COCO Caption dataÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite>.
On the open-vocabulary dataset LVIS, our method surpasses the SOTA method DetProÂ <cite class="ltx_cite ltx_citemacro_citep">(Du etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> by 1.9% mAP<sup id="S1.p4.1.1" class="ltx_sup"><span id="S1.p4.1.1.1" class="ltx_text ltx_font_italic">mask</span></sup> in terms of detecting novel classes using CC3MÂ <cite class="ltx_cite ltx_citemacro_citep">(Sharma etÂ al., <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite> dataset.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">The contributions of this paper are summarized as follows:
1) We introduce an open-vocabulary object detector method to learn object-language alignments directly from image-text pair data.
2) We propose to formulate region-word alignments as a set-matching problem and solve it efficiently with the Hungarian algorithmÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuhn, <a href="#bib.bib30" title="" class="ltx_ref">1955</a>)</cite>. 3) Extensive experiments on two benchmark datasets, COCO and LVIS, demonstrate our superior performance, particularly on detecting novel categories.
</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2211.14843/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Left: conventional object detection. Right: our proposed open-vocabulary object detection, where we focus on using the corpus of image-text pairs to learn region-word alignments for object detection.
By converting the image into a set of regions and the caption into a set of words, the region-word alignments can be solved as a <span id="S1.F1.2.1" class="ltx_text ltx_font_italic">set-matching problem</span>.
In this way, our method is able to directly train the object detector with image-text pairs covering a large variety of object categories.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Weakly-Supervised Object Detection (WSOD).</span>
WSOD aims to train an object detector using image-level labels as supervision without any bounding box annotation.
Multi-instance learningÂ <cite class="ltx_cite ltx_citemacro_citep">(Dietterich etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">1997</a>)</cite> is a well studied strategy for solving this problemÂ <cite class="ltx_cite ltx_citemacro_citep">(Bilen etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2015</a>; Bilen &amp; Vedaldi, <a href="#bib.bib2" title="" class="ltx_ref">2016</a>; Cinbis etÂ al., <a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite>.
It learns each category label assignment based on the top-scoring strategy, Ä±.e. assigning the top-scoring proposal to the corresponding image label.
As no bounding box supervision in training, OICRÂ <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al., <a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite> proposes to refine the instance classifier online using spatial relation, thereby improving the localization quality.
Cap2DetÂ <cite class="ltx_cite ltx_citemacro_citep">(Ye etÂ al., <a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite> further utilizes a text classifier to convert captions into image labels.
Based on the student-teacher framework, Omni-DETRÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite> uses different types of weak labels to generate accurate pseudo labels through a bipartite matching as filtering mechanism.
Different from WSOD, OVOD benefits from the fully-annotated base class data, resulting in accurate proposals to match with corresponding words for target classes not known in advance.
Directly using WSOD methods for OVOD tends to assign a region proposal to the mostly matching class, which is likely to be a base class due to the full supervision for the bass classes.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Multi-Modal Object Detection.</span>
A recent line of work considers instance-wise vision-language tasks, where the free-form language from captions is required to align with the objects.
GLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> unifies object detection and visual grounding into a grounded language-image pre-training model.
MDETRÂ <cite class="ltx_cite ltx_citemacro_citep">(Kamath etÂ al., <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> develops an end-to-end text-modulated detection system derived from DETRÂ <cite class="ltx_cite ltx_citemacro_citep">(Carion etÂ al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>.
XDETRÂ <cite class="ltx_cite ltx_citemacro_citep">(Cai etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite> independently trains the two streams, visual detector and language encoder, and align the region features and word embeddings via dot product operations.
All these multi-modal detectors extend their vocabulary fully or partially relying on annotated grounding data, i.e. object-language pairs with ground truth bounding boxes. In contrast, our model directly learns the region-word alignments for the novel classes by bipartite matching from the image-text pairs without bounding box annotations.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Open-Vocabulary Object Detection (OVOD).</span>
Recently, there is a trend toward improving the generalization ability of object detectors to new categories via multi-modal knowledge.
Open-vocabulary object detection aims to increase the vocabulary of object detection with image-text pairs.
As the first work of OVOD,
OVR-CNNÂ <cite class="ltx_cite ltx_citemacro_citep">(Zareian etÂ al., <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite> uses a corpus of image-caption pairs to pretrain a vision-language model and transfers the visual backbone to the supervised object detector.
With the huge progress in image-level open vocabulary recognitionÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a href="#bib.bib52" title="" class="ltx_ref">2022</a>; Yuan etÂ al., <a href="#bib.bib54" title="" class="ltx_ref">2021</a>; Zhai etÂ al., <a href="#bib.bib56" title="" class="ltx_ref">2021</a>; Jia etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Radford etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite>, distilling the knowledge from the off-the-shelf vision-language model like CLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite> has become a popular solution for OVOD tasksÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhong etÂ al., <a href="#bib.bib59" title="" class="ltx_ref">2022</a>; Minderer etÂ al., <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>.
For instance, <cite class="ltx_cite ltx_citemacro_cite">Zhong etÂ al. (<a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Gao etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> generate the pseudo region annotations from the pre-trained vision-language model and use them as training data for detectors.
To get a better localization ability for generating pseudo region annotations, <cite class="ltx_cite ltx_citemacro_cite">Zhao etÂ al. (<a href="#bib.bib58" title="" class="ltx_ref">2022</a>)</cite> use another two-stage detector as a strong region proposal network (RPN).
In order to use the CLIP class embedding effectively, DeProÂ <cite class="ltx_cite ltx_citemacro_citep">(Du etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> designs a fine-grained automatic prompt learning.
Different from these works, we scale up the vocabulary of object detection by using the annotation-free data of image-text pairs.
<span id="S2.p3.1.2" class="ltx_text ltx_font_italic">As it is easy to extract noun words from captions, our object vocabulary is easily to be extended and scaled.</span>
Besides, some work focuses on improving image segmentation in open vocabulary settings.
<cite class="ltx_cite ltx_citemacro_cite">Ghiasi etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite> organizes pixels into groups first and then learns the alignments between captions and predicted masks.
Another close work is DeticÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite> that focuses on increasing the performance on novel classes with image classification data by
supervising the max-size proposal with all image labels.
Unlike Detic, we try to align image regions and caption words by bipartite matching.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.15" class="ltx_p">A conventional object detector can be considered as trained with <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="(o_{i},g_{i})" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.2.2" xref="S3.SS1.p1.1.m1.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.3.cmml">(</mo><msub id="S3.SS1.p1.1.m1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.2.cmml">o</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p1.1.m1.2.2.2.4" xref="S3.SS1.p1.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.1.m1.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.cmml"><mi id="S3.SS1.p1.1.m1.2.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.2.cmml">g</mi><mi id="S3.SS1.p1.1.m1.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p1.1.m1.2.2.2.5" xref="S3.SS1.p1.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><interval closure="open" id="S3.SS1.p1.1.m1.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2"><apply id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.2">ğ‘œ</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2">ğ‘”</ci><ci id="S3.SS1.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.3">ğ‘–</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">(o_{i},g_{i})</annotation></semantics></math>, where <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="o_{i}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">o</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ğ‘œ</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">o_{i}</annotation></semantics></math> denotes the <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">i</annotation></semantics></math>-<math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="th" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><times id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></times><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ‘¡</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">th</annotation></semantics></math> object/region/box feature of an input image <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">I</annotation></semantics></math>, and <math id="S3.SS1.p1.6.m6.2" class="ltx_Math" alttext="g_{i}=(b_{i},c_{i})" display="inline"><semantics id="S3.SS1.p1.6.m6.2a"><mrow id="S3.SS1.p1.6.m6.2.2" xref="S3.SS1.p1.6.m6.2.2.cmml"><msub id="S3.SS1.p1.6.m6.2.2.4" xref="S3.SS1.p1.6.m6.2.2.4.cmml"><mi id="S3.SS1.p1.6.m6.2.2.4.2" xref="S3.SS1.p1.6.m6.2.2.4.2.cmml">g</mi><mi id="S3.SS1.p1.6.m6.2.2.4.3" xref="S3.SS1.p1.6.m6.2.2.4.3.cmml">i</mi></msub><mo id="S3.SS1.p1.6.m6.2.2.3" xref="S3.SS1.p1.6.m6.2.2.3.cmml">=</mo><mrow id="S3.SS1.p1.6.m6.2.2.2.2" xref="S3.SS1.p1.6.m6.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.6.m6.2.2.2.2.3" xref="S3.SS1.p1.6.m6.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p1.6.m6.1.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.1.1.2" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml">b</mi><mi id="S3.SS1.p1.6.m6.1.1.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p1.6.m6.2.2.2.2.4" xref="S3.SS1.p1.6.m6.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.6.m6.2.2.2.2.2" xref="S3.SS1.p1.6.m6.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.6.m6.2.2.2.2.2.2" xref="S3.SS1.p1.6.m6.2.2.2.2.2.2.cmml">c</mi><mi id="S3.SS1.p1.6.m6.2.2.2.2.2.3" xref="S3.SS1.p1.6.m6.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p1.6.m6.2.2.2.2.5" xref="S3.SS1.p1.6.m6.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.2b"><apply id="S3.SS1.p1.6.m6.2.2.cmml" xref="S3.SS1.p1.6.m6.2.2"><eq id="S3.SS1.p1.6.m6.2.2.3.cmml" xref="S3.SS1.p1.6.m6.2.2.3"></eq><apply id="S3.SS1.p1.6.m6.2.2.4.cmml" xref="S3.SS1.p1.6.m6.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.2.2.4.1.cmml" xref="S3.SS1.p1.6.m6.2.2.4">subscript</csymbol><ci id="S3.SS1.p1.6.m6.2.2.4.2.cmml" xref="S3.SS1.p1.6.m6.2.2.4.2">ğ‘”</ci><ci id="S3.SS1.p1.6.m6.2.2.4.3.cmml" xref="S3.SS1.p1.6.m6.2.2.4.3">ğ‘–</ci></apply><interval closure="open" id="S3.SS1.p1.6.m6.2.2.2.3.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2"><apply id="S3.SS1.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS1.p1.6.m6.2.2.2.2.2.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.6.m6.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2.2.2">ğ‘</ci><ci id="S3.SS1.p1.6.m6.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.2c">g_{i}=(b_{i},c_{i})</annotation></semantics></math> denotes the corresponding labels with the bounding-box coordinates <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="b_{i}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">b</mi><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">b_{i}</annotation></semantics></math> and their associated categories <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="c_{i}\in\mathbb{C}^{base}" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mrow id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><msub id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2.2" xref="S3.SS1.p1.8.m8.1.1.2.2.cmml">c</mi><mi id="S3.SS1.p1.8.m8.1.1.2.3" xref="S3.SS1.p1.8.m8.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p1.8.m8.1.1.1" xref="S3.SS1.p1.8.m8.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml"><mi id="S3.SS1.p1.8.m8.1.1.3.2" xref="S3.SS1.p1.8.m8.1.1.3.2.cmml">â„‚</mi><mrow id="S3.SS1.p1.8.m8.1.1.3.3" xref="S3.SS1.p1.8.m8.1.1.3.3.cmml"><mi id="S3.SS1.p1.8.m8.1.1.3.3.2" xref="S3.SS1.p1.8.m8.1.1.3.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.8.m8.1.1.3.3.1" xref="S3.SS1.p1.8.m8.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.8.m8.1.1.3.3.3" xref="S3.SS1.p1.8.m8.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.8.m8.1.1.3.3.1a" xref="S3.SS1.p1.8.m8.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.8.m8.1.1.3.3.4" xref="S3.SS1.p1.8.m8.1.1.3.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.8.m8.1.1.3.3.1b" xref="S3.SS1.p1.8.m8.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.8.m8.1.1.3.3.5" xref="S3.SS1.p1.8.m8.1.1.3.3.5.cmml">e</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><in id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1.1"></in><apply id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.2.1.cmml" xref="S3.SS1.p1.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2.2">ğ‘</ci><ci id="S3.SS1.p1.8.m8.1.1.2.3.cmml" xref="S3.SS1.p1.8.m8.1.1.2.3">ğ‘–</ci></apply><apply id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.3.1.cmml" xref="S3.SS1.p1.8.m8.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.3.2.cmml" xref="S3.SS1.p1.8.m8.1.1.3.2">â„‚</ci><apply id="S3.SS1.p1.8.m8.1.1.3.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3"><times id="S3.SS1.p1.8.m8.1.1.3.3.1.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.1"></times><ci id="S3.SS1.p1.8.m8.1.1.3.3.2.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.2">ğ‘</ci><ci id="S3.SS1.p1.8.m8.1.1.3.3.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.3">ğ‘</ci><ci id="S3.SS1.p1.8.m8.1.1.3.3.4.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.4">ğ‘ </ci><ci id="S3.SS1.p1.8.m8.1.1.3.3.5.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.5">ğ‘’</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">c_{i}\in\mathbb{C}^{base}</annotation></semantics></math>, where <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="\mathbb{C}^{base}" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><msup id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">â„‚</mi><mrow id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml"><mi id="S3.SS1.p1.9.m9.1.1.3.2" xref="S3.SS1.p1.9.m9.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.9.m9.1.1.3.1" xref="S3.SS1.p1.9.m9.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.9.m9.1.1.3.3" xref="S3.SS1.p1.9.m9.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.9.m9.1.1.3.1a" xref="S3.SS1.p1.9.m9.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.9.m9.1.1.3.4" xref="S3.SS1.p1.9.m9.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.9.m9.1.1.3.1b" xref="S3.SS1.p1.9.m9.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.9.m9.1.1.3.5" xref="S3.SS1.p1.9.m9.1.1.3.5.cmml">e</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">superscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">â„‚</ci><apply id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3"><times id="S3.SS1.p1.9.m9.1.1.3.1.cmml" xref="S3.SS1.p1.9.m9.1.1.3.1"></times><ci id="S3.SS1.p1.9.m9.1.1.3.2.cmml" xref="S3.SS1.p1.9.m9.1.1.3.2">ğ‘</ci><ci id="S3.SS1.p1.9.m9.1.1.3.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3.3">ğ‘</ci><ci id="S3.SS1.p1.9.m9.1.1.3.4.cmml" xref="S3.SS1.p1.9.m9.1.1.3.4">ğ‘ </ci><ci id="S3.SS1.p1.9.m9.1.1.3.5.cmml" xref="S3.SS1.p1.9.m9.1.1.3.5">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">\mathbb{C}^{base}</annotation></semantics></math> is the set of base classes, or pre-defined classes.
Open-vocabulary Object Detection (OVOD)Â <cite class="ltx_cite ltx_citemacro_citep">(Zareian etÂ al., <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite> aims to acquire a large vocabulary of knowledge from a corpus of image-caption pairs to
extend detection of pre-defined classes to novel classes. In other words, our goal is to build an object detector, trained on a dataset with base-class bounding box annotations and a dataset of image-caption pairs <math id="S3.SS1.p1.10.m10.2" class="ltx_Math" alttext="\left\langle I,C\right\rangle" display="inline"><semantics id="S3.SS1.p1.10.m10.2a"><mrow id="S3.SS1.p1.10.m10.2.3.2" xref="S3.SS1.p1.10.m10.2.3.1.cmml"><mo id="S3.SS1.p1.10.m10.2.3.2.1" xref="S3.SS1.p1.10.m10.2.3.1.cmml">âŸ¨</mo><mi id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml">I</mi><mo id="S3.SS1.p1.10.m10.2.3.2.2" xref="S3.SS1.p1.10.m10.2.3.1.cmml">,</mo><mi id="S3.SS1.p1.10.m10.2.2" xref="S3.SS1.p1.10.m10.2.2.cmml">C</mi><mo id="S3.SS1.p1.10.m10.2.3.2.3" xref="S3.SS1.p1.10.m10.2.3.1.cmml">âŸ©</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.2b"><list id="S3.SS1.p1.10.m10.2.3.1.cmml" xref="S3.SS1.p1.10.m10.2.3.2"><ci id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">ğ¼</ci><ci id="S3.SS1.p1.10.m10.2.2.cmml" xref="S3.SS1.p1.10.m10.2.2">ğ¶</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.2c">\left\langle I,C\right\rangle</annotation></semantics></math> associated with a large vocabulary <math id="S3.SS1.p1.11.m11.1" class="ltx_Math" alttext="\mathbb{C}^{open}" display="inline"><semantics id="S3.SS1.p1.11.m11.1a"><msup id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml"><mi id="S3.SS1.p1.11.m11.1.1.2" xref="S3.SS1.p1.11.m11.1.1.2.cmml">â„‚</mi><mrow id="S3.SS1.p1.11.m11.1.1.3" xref="S3.SS1.p1.11.m11.1.1.3.cmml"><mi id="S3.SS1.p1.11.m11.1.1.3.2" xref="S3.SS1.p1.11.m11.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.11.m11.1.1.3.1" xref="S3.SS1.p1.11.m11.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.11.m11.1.1.3.3" xref="S3.SS1.p1.11.m11.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.11.m11.1.1.3.1a" xref="S3.SS1.p1.11.m11.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.11.m11.1.1.3.4" xref="S3.SS1.p1.11.m11.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.11.m11.1.1.3.1b" xref="S3.SS1.p1.11.m11.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.11.m11.1.1.3.5" xref="S3.SS1.p1.11.m11.1.1.3.5.cmml">n</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><apply id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m11.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">superscript</csymbol><ci id="S3.SS1.p1.11.m11.1.1.2.cmml" xref="S3.SS1.p1.11.m11.1.1.2">â„‚</ci><apply id="S3.SS1.p1.11.m11.1.1.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3"><times id="S3.SS1.p1.11.m11.1.1.3.1.cmml" xref="S3.SS1.p1.11.m11.1.1.3.1"></times><ci id="S3.SS1.p1.11.m11.1.1.3.2.cmml" xref="S3.SS1.p1.11.m11.1.1.3.2">ğ‘œ</ci><ci id="S3.SS1.p1.11.m11.1.1.3.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3.3">ğ‘</ci><ci id="S3.SS1.p1.11.m11.1.1.3.4.cmml" xref="S3.SS1.p1.11.m11.1.1.3.4">ğ‘’</ci><ci id="S3.SS1.p1.11.m11.1.1.3.5.cmml" xref="S3.SS1.p1.11.m11.1.1.3.5">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">\mathbb{C}^{open}</annotation></semantics></math>, to detect objects of novel classes <math id="S3.SS1.p1.12.m12.1" class="ltx_Math" alttext="\mathbb{C}^{novel}" display="inline"><semantics id="S3.SS1.p1.12.m12.1a"><msup id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml"><mi id="S3.SS1.p1.12.m12.1.1.2" xref="S3.SS1.p1.12.m12.1.1.2.cmml">â„‚</mi><mrow id="S3.SS1.p1.12.m12.1.1.3" xref="S3.SS1.p1.12.m12.1.1.3.cmml"><mi id="S3.SS1.p1.12.m12.1.1.3.2" xref="S3.SS1.p1.12.m12.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.12.m12.1.1.3.1" xref="S3.SS1.p1.12.m12.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.12.m12.1.1.3.3" xref="S3.SS1.p1.12.m12.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.12.m12.1.1.3.1a" xref="S3.SS1.p1.12.m12.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.12.m12.1.1.3.4" xref="S3.SS1.p1.12.m12.1.1.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.12.m12.1.1.3.1b" xref="S3.SS1.p1.12.m12.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.12.m12.1.1.3.5" xref="S3.SS1.p1.12.m12.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.12.m12.1.1.3.1c" xref="S3.SS1.p1.12.m12.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.12.m12.1.1.3.6" xref="S3.SS1.p1.12.m12.1.1.3.6.cmml">l</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><apply id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.12.m12.1.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">superscript</csymbol><ci id="S3.SS1.p1.12.m12.1.1.2.cmml" xref="S3.SS1.p1.12.m12.1.1.2">â„‚</ci><apply id="S3.SS1.p1.12.m12.1.1.3.cmml" xref="S3.SS1.p1.12.m12.1.1.3"><times id="S3.SS1.p1.12.m12.1.1.3.1.cmml" xref="S3.SS1.p1.12.m12.1.1.3.1"></times><ci id="S3.SS1.p1.12.m12.1.1.3.2.cmml" xref="S3.SS1.p1.12.m12.1.1.3.2">ğ‘›</ci><ci id="S3.SS1.p1.12.m12.1.1.3.3.cmml" xref="S3.SS1.p1.12.m12.1.1.3.3">ğ‘œ</ci><ci id="S3.SS1.p1.12.m12.1.1.3.4.cmml" xref="S3.SS1.p1.12.m12.1.1.3.4">ğ‘£</ci><ci id="S3.SS1.p1.12.m12.1.1.3.5.cmml" xref="S3.SS1.p1.12.m12.1.1.3.5">ğ‘’</ci><ci id="S3.SS1.p1.12.m12.1.1.3.6.cmml" xref="S3.SS1.p1.12.m12.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">\mathbb{C}^{novel}</annotation></semantics></math> during testing.
Note that the vocabulary <math id="S3.SS1.p1.13.m13.1" class="ltx_Math" alttext="\mathbb{C}^{base}" display="inline"><semantics id="S3.SS1.p1.13.m13.1a"><msup id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml"><mi id="S3.SS1.p1.13.m13.1.1.2" xref="S3.SS1.p1.13.m13.1.1.2.cmml">â„‚</mi><mrow id="S3.SS1.p1.13.m13.1.1.3" xref="S3.SS1.p1.13.m13.1.1.3.cmml"><mi id="S3.SS1.p1.13.m13.1.1.3.2" xref="S3.SS1.p1.13.m13.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.13.m13.1.1.3.1" xref="S3.SS1.p1.13.m13.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.13.m13.1.1.3.3" xref="S3.SS1.p1.13.m13.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.13.m13.1.1.3.1a" xref="S3.SS1.p1.13.m13.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.13.m13.1.1.3.4" xref="S3.SS1.p1.13.m13.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.13.m13.1.1.3.1b" xref="S3.SS1.p1.13.m13.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.13.m13.1.1.3.5" xref="S3.SS1.p1.13.m13.1.1.3.5.cmml">e</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><apply id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m13.1.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1">superscript</csymbol><ci id="S3.SS1.p1.13.m13.1.1.2.cmml" xref="S3.SS1.p1.13.m13.1.1.2">â„‚</ci><apply id="S3.SS1.p1.13.m13.1.1.3.cmml" xref="S3.SS1.p1.13.m13.1.1.3"><times id="S3.SS1.p1.13.m13.1.1.3.1.cmml" xref="S3.SS1.p1.13.m13.1.1.3.1"></times><ci id="S3.SS1.p1.13.m13.1.1.3.2.cmml" xref="S3.SS1.p1.13.m13.1.1.3.2">ğ‘</ci><ci id="S3.SS1.p1.13.m13.1.1.3.3.cmml" xref="S3.SS1.p1.13.m13.1.1.3.3">ğ‘</ci><ci id="S3.SS1.p1.13.m13.1.1.3.4.cmml" xref="S3.SS1.p1.13.m13.1.1.3.4">ğ‘ </ci><ci id="S3.SS1.p1.13.m13.1.1.3.5.cmml" xref="S3.SS1.p1.13.m13.1.1.3.5">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">\mathbb{C}^{base}</annotation></semantics></math>and <math id="S3.SS1.p1.14.m14.1" class="ltx_Math" alttext="\mathbb{C}^{novel}" display="inline"><semantics id="S3.SS1.p1.14.m14.1a"><msup id="S3.SS1.p1.14.m14.1.1" xref="S3.SS1.p1.14.m14.1.1.cmml"><mi id="S3.SS1.p1.14.m14.1.1.2" xref="S3.SS1.p1.14.m14.1.1.2.cmml">â„‚</mi><mrow id="S3.SS1.p1.14.m14.1.1.3" xref="S3.SS1.p1.14.m14.1.1.3.cmml"><mi id="S3.SS1.p1.14.m14.1.1.3.2" xref="S3.SS1.p1.14.m14.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.14.m14.1.1.3.1" xref="S3.SS1.p1.14.m14.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.14.m14.1.1.3.3" xref="S3.SS1.p1.14.m14.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.14.m14.1.1.3.1a" xref="S3.SS1.p1.14.m14.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.14.m14.1.1.3.4" xref="S3.SS1.p1.14.m14.1.1.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.14.m14.1.1.3.1b" xref="S3.SS1.p1.14.m14.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.14.m14.1.1.3.5" xref="S3.SS1.p1.14.m14.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.14.m14.1.1.3.1c" xref="S3.SS1.p1.14.m14.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.14.m14.1.1.3.6" xref="S3.SS1.p1.14.m14.1.1.3.6.cmml">l</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m14.1b"><apply id="S3.SS1.p1.14.m14.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.14.m14.1.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1">superscript</csymbol><ci id="S3.SS1.p1.14.m14.1.1.2.cmml" xref="S3.SS1.p1.14.m14.1.1.2">â„‚</ci><apply id="S3.SS1.p1.14.m14.1.1.3.cmml" xref="S3.SS1.p1.14.m14.1.1.3"><times id="S3.SS1.p1.14.m14.1.1.3.1.cmml" xref="S3.SS1.p1.14.m14.1.1.3.1"></times><ci id="S3.SS1.p1.14.m14.1.1.3.2.cmml" xref="S3.SS1.p1.14.m14.1.1.3.2">ğ‘›</ci><ci id="S3.SS1.p1.14.m14.1.1.3.3.cmml" xref="S3.SS1.p1.14.m14.1.1.3.3">ğ‘œ</ci><ci id="S3.SS1.p1.14.m14.1.1.3.4.cmml" xref="S3.SS1.p1.14.m14.1.1.3.4">ğ‘£</ci><ci id="S3.SS1.p1.14.m14.1.1.3.5.cmml" xref="S3.SS1.p1.14.m14.1.1.3.5">ğ‘’</ci><ci id="S3.SS1.p1.14.m14.1.1.3.6.cmml" xref="S3.SS1.p1.14.m14.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m14.1c">\mathbb{C}^{novel}</annotation></semantics></math> might or might not overlap with <math id="S3.SS1.p1.15.m15.1" class="ltx_Math" alttext="\mathbb{C}^{open}" display="inline"><semantics id="S3.SS1.p1.15.m15.1a"><msup id="S3.SS1.p1.15.m15.1.1" xref="S3.SS1.p1.15.m15.1.1.cmml"><mi id="S3.SS1.p1.15.m15.1.1.2" xref="S3.SS1.p1.15.m15.1.1.2.cmml">â„‚</mi><mrow id="S3.SS1.p1.15.m15.1.1.3" xref="S3.SS1.p1.15.m15.1.1.3.cmml"><mi id="S3.SS1.p1.15.m15.1.1.3.2" xref="S3.SS1.p1.15.m15.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.15.m15.1.1.3.1" xref="S3.SS1.p1.15.m15.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.15.m15.1.1.3.3" xref="S3.SS1.p1.15.m15.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.15.m15.1.1.3.1a" xref="S3.SS1.p1.15.m15.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.15.m15.1.1.3.4" xref="S3.SS1.p1.15.m15.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.15.m15.1.1.3.1b" xref="S3.SS1.p1.15.m15.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.15.m15.1.1.3.5" xref="S3.SS1.p1.15.m15.1.1.3.5.cmml">n</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m15.1b"><apply id="S3.SS1.p1.15.m15.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.15.m15.1.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1">superscript</csymbol><ci id="S3.SS1.p1.15.m15.1.1.2.cmml" xref="S3.SS1.p1.15.m15.1.1.2">â„‚</ci><apply id="S3.SS1.p1.15.m15.1.1.3.cmml" xref="S3.SS1.p1.15.m15.1.1.3"><times id="S3.SS1.p1.15.m15.1.1.3.1.cmml" xref="S3.SS1.p1.15.m15.1.1.3.1"></times><ci id="S3.SS1.p1.15.m15.1.1.3.2.cmml" xref="S3.SS1.p1.15.m15.1.1.3.2">ğ‘œ</ci><ci id="S3.SS1.p1.15.m15.1.1.3.3.cmml" xref="S3.SS1.p1.15.m15.1.1.3.3">ğ‘</ci><ci id="S3.SS1.p1.15.m15.1.1.3.4.cmml" xref="S3.SS1.p1.15.m15.1.1.3.4">ğ‘’</ci><ci id="S3.SS1.p1.15.m15.1.1.3.5.cmml" xref="S3.SS1.p1.15.m15.1.1.3.5">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m15.1c">\mathbb{C}^{open}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3.1 Overview â€£ 3 Method â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives an overview of our proposed VLDet framework.
We propose to learn fine-grained region-word alignments with the corpus of image-text pairs, which is formulated as a set-matching problem, i.e., matching a set of object candidates with a set of word candidates. In the following, we first describe how to formulate the learning of region-word alignment from image-text pairs as a bipartite match problem in SectionÂ <a href="#S3.SS2" title="3.2 Learning Object-Language Alignments by Bipartite Matching â€£ 3 Method â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and then depict the network architecture followed by some method details in SectionÂ <a href="#S3.SS3" title="3.3 Network Architecture â€£ 3 Method â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2211.14843/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="141" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of our proposed <span id="S3.F2.2.1" class="ltx_text ltx_font_bold">VLDet</span> framework.
We propose to learn fine-grained region-word alignments with the corpus of image-text pairs, which is formulated as a bipartite matching problem between image regions and word candidates.
</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Learning Object-Language Alignments by Bipartite Matching</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.8" class="ltx_p"><span id="S3.SS2.p1.8.1" class="ltx_text ltx_font_bold">Recap on Bipartite Matching.</span>
The Bipartite Matching describes the following problem: supposing there are <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">X</annotation></semantics></math> workers and <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ğ‘Œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">Y</annotation></semantics></math> jobs.
Each worker has a subset of jobs that he/she is capable to finish.
Each job can only accept one worker and each worker can be appointed to only one job.
Because each worker has different skills, the cost <math id="S3.SS2.p1.3.m3.2" class="ltx_Math" alttext="d_{x,y}" display="inline"><semantics id="S3.SS2.p1.3.m3.2a"><msub id="S3.SS2.p1.3.m3.2.3" xref="S3.SS2.p1.3.m3.2.3.cmml"><mi id="S3.SS2.p1.3.m3.2.3.2" xref="S3.SS2.p1.3.m3.2.3.2.cmml">d</mi><mrow id="S3.SS2.p1.3.m3.2.2.2.4" xref="S3.SS2.p1.3.m3.2.2.2.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.1.cmml">x</mi><mo id="S3.SS2.p1.3.m3.2.2.2.4.1" xref="S3.SS2.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p1.3.m3.2.2.2.2" xref="S3.SS2.p1.3.m3.2.2.2.2.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.2b"><apply id="S3.SS2.p1.3.m3.2.3.cmml" xref="S3.SS2.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.2.3.1.cmml" xref="S3.SS2.p1.3.m3.2.3">subscript</csymbol><ci id="S3.SS2.p1.3.m3.2.3.2.cmml" xref="S3.SS2.p1.3.m3.2.3.2">ğ‘‘</ci><list id="S3.SS2.p1.3.m3.2.2.2.3.cmml" xref="S3.SS2.p1.3.m3.2.2.2.4"><ci id="S3.SS2.p1.3.m3.1.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1.1">ğ‘¥</ci><ci id="S3.SS2.p1.3.m3.2.2.2.2.cmml" xref="S3.SS2.p1.3.m3.2.2.2.2">ğ‘¦</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.2c">d_{x,y}</annotation></semantics></math> required to perform a job <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">y</annotation></semantics></math> depends on the worker <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">x</annotation></semantics></math> who is assigned to it.
The goal is to determine the optimum assignment <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="M^{*}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><msup id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">M</mi><mo id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">ğ‘€</ci><times id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">M^{*}</annotation></semantics></math> such that the total cost is minimized or the team effectiveness is maximized.
For this purpose, given a matching matrix <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">ğŒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">ğŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">\mathbf{M}</annotation></semantics></math>, if element <math id="S3.SS2.p1.8.m8.2" class="ltx_Math" alttext="m_{i,j}=1" display="inline"><semantics id="S3.SS2.p1.8.m8.2a"><mrow id="S3.SS2.p1.8.m8.2.3" xref="S3.SS2.p1.8.m8.2.3.cmml"><msub id="S3.SS2.p1.8.m8.2.3.2" xref="S3.SS2.p1.8.m8.2.3.2.cmml"><mi id="S3.SS2.p1.8.m8.2.3.2.2" xref="S3.SS2.p1.8.m8.2.3.2.2.cmml">m</mi><mrow id="S3.SS2.p1.8.m8.2.2.2.4" xref="S3.SS2.p1.8.m8.2.2.2.3.cmml"><mi id="S3.SS2.p1.8.m8.1.1.1.1" xref="S3.SS2.p1.8.m8.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p1.8.m8.2.2.2.4.1" xref="S3.SS2.p1.8.m8.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p1.8.m8.2.2.2.2" xref="S3.SS2.p1.8.m8.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.SS2.p1.8.m8.2.3.1" xref="S3.SS2.p1.8.m8.2.3.1.cmml">=</mo><mn id="S3.SS2.p1.8.m8.2.3.3" xref="S3.SS2.p1.8.m8.2.3.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.2b"><apply id="S3.SS2.p1.8.m8.2.3.cmml" xref="S3.SS2.p1.8.m8.2.3"><eq id="S3.SS2.p1.8.m8.2.3.1.cmml" xref="S3.SS2.p1.8.m8.2.3.1"></eq><apply id="S3.SS2.p1.8.m8.2.3.2.cmml" xref="S3.SS2.p1.8.m8.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.2.3.2.1.cmml" xref="S3.SS2.p1.8.m8.2.3.2">subscript</csymbol><ci id="S3.SS2.p1.8.m8.2.3.2.2.cmml" xref="S3.SS2.p1.8.m8.2.3.2.2">ğ‘š</ci><list id="S3.SS2.p1.8.m8.2.2.2.3.cmml" xref="S3.SS2.p1.8.m8.2.2.2.4"><ci id="S3.SS2.p1.8.m8.1.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1">ğ‘–</ci><ci id="S3.SS2.p1.8.m8.2.2.2.2.cmml" xref="S3.SS2.p1.8.m8.2.2.2.2">ğ‘—</ci></list></apply><cn type="integer" id="S3.SS2.p1.8.m8.2.3.3.cmml" xref="S3.SS2.p1.8.m8.2.3.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.2c">m_{i,j}=1</annotation></semantics></math>, it indicates a matching; otherwise, not matching.
We can formulate this matching problem as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.12" class="ltx_Math" alttext="\begin{split}\min\limits_{\mathbf{M}}\sum_{i=1}^{X}\sum_{j=1}^{Y}d_{i,j}m_{i,j}\\
\end{split}" display="block"><semantics id="S3.E1.m1.12a"><mtable displaystyle="true" id="S3.E1.m1.12.12" xref="S3.E1.m1.12.13.1.cmml"><mtr id="S3.E1.m1.12.12a" xref="S3.E1.m1.12.13.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.12.12b" xref="S3.E1.m1.12.13.1.cmml"><mrow id="S3.E1.m1.12.12.12.12.12" xref="S3.E1.m1.12.13.1.cmml"><munder id="S3.E1.m1.12.12.12.12.12.14" xref="S3.E1.m1.12.13.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">min</mi><mi id="S3.E1.m1.2.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.2.1.cmml">ğŒ</mi></munder><mo lspace="0em" rspace="0em" id="S3.E1.m1.12.12.12.12.12.13" xref="S3.E1.m1.12.13.1.1.cmml">â€‹</mo><mrow id="S3.E1.m1.12.12.12.12.12.15" xref="S3.E1.m1.12.13.1.cmml"><munderover id="S3.E1.m1.12.12.12.12.12.15.1" xref="S3.E1.m1.12.13.1.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.cmml">âˆ‘</mo><mrow id="S3.E1.m1.4.4.4.4.4.4.1" xref="S3.E1.m1.4.4.4.4.4.4.1.cmml"><mi id="S3.E1.m1.4.4.4.4.4.4.1.2" xref="S3.E1.m1.4.4.4.4.4.4.1.2.cmml">i</mi><mo id="S3.E1.m1.4.4.4.4.4.4.1.1" xref="S3.E1.m1.4.4.4.4.4.4.1.1.cmml">=</mo><mn id="S3.E1.m1.4.4.4.4.4.4.1.3" xref="S3.E1.m1.4.4.4.4.4.4.1.3.cmml">1</mn></mrow><mi id="S3.E1.m1.5.5.5.5.5.5.1" xref="S3.E1.m1.5.5.5.5.5.5.1.cmml">X</mi></munderover><mrow id="S3.E1.m1.12.12.12.12.12.15.2" xref="S3.E1.m1.12.13.1.cmml"><munderover id="S3.E1.m1.12.12.12.12.12.15.2.1" xref="S3.E1.m1.12.13.1.cmml"><mo movablelimits="false" id="S3.E1.m1.6.6.6.6.6.6" xref="S3.E1.m1.6.6.6.6.6.6.cmml">âˆ‘</mo><mrow id="S3.E1.m1.7.7.7.7.7.7.1" xref="S3.E1.m1.7.7.7.7.7.7.1.cmml"><mi id="S3.E1.m1.7.7.7.7.7.7.1.2" xref="S3.E1.m1.7.7.7.7.7.7.1.2.cmml">j</mi><mo id="S3.E1.m1.7.7.7.7.7.7.1.1" xref="S3.E1.m1.7.7.7.7.7.7.1.1.cmml">=</mo><mn id="S3.E1.m1.7.7.7.7.7.7.1.3" xref="S3.E1.m1.7.7.7.7.7.7.1.3.cmml">1</mn></mrow><mi id="S3.E1.m1.8.8.8.8.8.8.1" xref="S3.E1.m1.8.8.8.8.8.8.1.cmml">Y</mi></munderover><mrow id="S3.E1.m1.12.12.12.12.12.15.2.2" xref="S3.E1.m1.12.13.1.cmml"><msub id="S3.E1.m1.12.12.12.12.12.15.2.2.2" xref="S3.E1.m1.12.13.1.cmml"><mi id="S3.E1.m1.9.9.9.9.9.9" xref="S3.E1.m1.9.9.9.9.9.9.cmml">d</mi><mrow id="S3.E1.m1.10.10.10.10.10.10.1.4" xref="S3.E1.m1.10.10.10.10.10.10.1.3.cmml"><mi id="S3.E1.m1.10.10.10.10.10.10.1.1" xref="S3.E1.m1.10.10.10.10.10.10.1.1.cmml">i</mi><mo id="S3.E1.m1.10.10.10.10.10.10.1.4.1" xref="S3.E1.m1.10.10.10.10.10.10.1.3.cmml">,</mo><mi id="S3.E1.m1.10.10.10.10.10.10.1.2" xref="S3.E1.m1.10.10.10.10.10.10.1.2.cmml">j</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.12.12.12.12.12.15.2.2.1" xref="S3.E1.m1.12.13.1.1.cmml">â€‹</mo><msub id="S3.E1.m1.12.12.12.12.12.15.2.2.3" xref="S3.E1.m1.12.13.1.cmml"><mi id="S3.E1.m1.11.11.11.11.11.11" xref="S3.E1.m1.11.11.11.11.11.11.cmml">m</mi><mrow id="S3.E1.m1.12.12.12.12.12.12.1.4" xref="S3.E1.m1.12.12.12.12.12.12.1.3.cmml"><mi id="S3.E1.m1.12.12.12.12.12.12.1.1" xref="S3.E1.m1.12.12.12.12.12.12.1.1.cmml">i</mi><mo id="S3.E1.m1.12.12.12.12.12.12.1.4.1" xref="S3.E1.m1.12.12.12.12.12.12.1.3.cmml">,</mo><mi id="S3.E1.m1.12.12.12.12.12.12.1.2" xref="S3.E1.m1.12.12.12.12.12.12.1.2.cmml">j</mi></mrow></msub></mrow></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.12b"><apply id="S3.E1.m1.12.13.1.cmml" xref="S3.E1.m1.12.12"><times id="S3.E1.m1.12.13.1.1.cmml" xref="S3.E1.m1.12.12.12.12.12.13"></times><apply id="S3.E1.m1.12.13.1.2.cmml" xref="S3.E1.m1.12.12"><csymbol cd="ambiguous" id="S3.E1.m1.12.13.1.2.1.cmml" xref="S3.E1.m1.12.12.12.12.12.13">subscript</csymbol><min id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"></min><ci id="S3.E1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1">ğŒ</ci></apply><apply id="S3.E1.m1.12.13.1.3.cmml" xref="S3.E1.m1.12.12"><apply id="S3.E1.m1.12.13.1.3.1.cmml" xref="S3.E1.m1.12.12"><csymbol cd="ambiguous" id="S3.E1.m1.12.13.1.3.1.1.cmml" xref="S3.E1.m1.12.12.12.12.12.13">superscript</csymbol><apply id="S3.E1.m1.12.13.1.3.1.2.cmml" xref="S3.E1.m1.12.12"><csymbol cd="ambiguous" id="S3.E1.m1.12.13.1.3.1.2.1.cmml" xref="S3.E1.m1.12.12.12.12.12.13">subscript</csymbol><sum id="S3.E1.m1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"></sum><apply id="S3.E1.m1.4.4.4.4.4.4.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4.1"><eq id="S3.E1.m1.4.4.4.4.4.4.1.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4.1.1"></eq><ci id="S3.E1.m1.4.4.4.4.4.4.1.2.cmml" xref="S3.E1.m1.4.4.4.4.4.4.1.2">ğ‘–</ci><cn type="integer" id="S3.E1.m1.4.4.4.4.4.4.1.3.cmml" xref="S3.E1.m1.4.4.4.4.4.4.1.3">1</cn></apply></apply><ci id="S3.E1.m1.5.5.5.5.5.5.1.cmml" xref="S3.E1.m1.5.5.5.5.5.5.1">ğ‘‹</ci></apply><apply id="S3.E1.m1.12.13.1.3.2.cmml" xref="S3.E1.m1.12.12"><apply id="S3.E1.m1.12.13.1.3.2.1.cmml" xref="S3.E1.m1.12.12"><csymbol cd="ambiguous" id="S3.E1.m1.12.13.1.3.2.1.1.cmml" xref="S3.E1.m1.12.12.12.12.12.13">superscript</csymbol><apply id="S3.E1.m1.12.13.1.3.2.1.2.cmml" xref="S3.E1.m1.12.12"><csymbol cd="ambiguous" id="S3.E1.m1.12.13.1.3.2.1.2.1.cmml" xref="S3.E1.m1.12.12.12.12.12.13">subscript</csymbol><sum id="S3.E1.m1.6.6.6.6.6.6.cmml" xref="S3.E1.m1.6.6.6.6.6.6"></sum><apply id="S3.E1.m1.7.7.7.7.7.7.1.cmml" xref="S3.E1.m1.7.7.7.7.7.7.1"><eq id="S3.E1.m1.7.7.7.7.7.7.1.1.cmml" xref="S3.E1.m1.7.7.7.7.7.7.1.1"></eq><ci id="S3.E1.m1.7.7.7.7.7.7.1.2.cmml" xref="S3.E1.m1.7.7.7.7.7.7.1.2">ğ‘—</ci><cn type="integer" id="S3.E1.m1.7.7.7.7.7.7.1.3.cmml" xref="S3.E1.m1.7.7.7.7.7.7.1.3">1</cn></apply></apply><ci id="S3.E1.m1.8.8.8.8.8.8.1.cmml" xref="S3.E1.m1.8.8.8.8.8.8.1">ğ‘Œ</ci></apply><apply id="S3.E1.m1.12.13.1.3.2.2.cmml" xref="S3.E1.m1.12.12"><times id="S3.E1.m1.12.13.1.3.2.2.1.cmml" xref="S3.E1.m1.12.12.12.12.12.13"></times><apply id="S3.E1.m1.12.13.1.3.2.2.2.cmml" xref="S3.E1.m1.12.12"><csymbol cd="ambiguous" id="S3.E1.m1.12.13.1.3.2.2.2.1.cmml" xref="S3.E1.m1.12.12.12.12.12.13">subscript</csymbol><ci id="S3.E1.m1.9.9.9.9.9.9.cmml" xref="S3.E1.m1.9.9.9.9.9.9">ğ‘‘</ci><list id="S3.E1.m1.10.10.10.10.10.10.1.3.cmml" xref="S3.E1.m1.10.10.10.10.10.10.1.4"><ci id="S3.E1.m1.10.10.10.10.10.10.1.1.cmml" xref="S3.E1.m1.10.10.10.10.10.10.1.1">ğ‘–</ci><ci id="S3.E1.m1.10.10.10.10.10.10.1.2.cmml" xref="S3.E1.m1.10.10.10.10.10.10.1.2">ğ‘—</ci></list></apply><apply id="S3.E1.m1.12.13.1.3.2.2.3.cmml" xref="S3.E1.m1.12.12"><csymbol cd="ambiguous" id="S3.E1.m1.12.13.1.3.2.2.3.1.cmml" xref="S3.E1.m1.12.12.12.12.12.13">subscript</csymbol><ci id="S3.E1.m1.11.11.11.11.11.11.cmml" xref="S3.E1.m1.11.11.11.11.11.11">ğ‘š</ci><list id="S3.E1.m1.12.12.12.12.12.12.1.3.cmml" xref="S3.E1.m1.12.12.12.12.12.12.1.4"><ci id="S3.E1.m1.12.12.12.12.12.12.1.1.cmml" xref="S3.E1.m1.12.12.12.12.12.12.1.1">ğ‘–</ci><ci id="S3.E1.m1.12.12.12.12.12.12.1.2.cmml" xref="S3.E1.m1.12.12.12.12.12.12.1.2">ğ‘—</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.12c">\begin{split}\min\limits_{\mathbf{M}}\sum_{i=1}^{X}\sum_{j=1}^{Y}d_{i,j}m_{i,j}\\
\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.12" class="ltx_p">The constraints is to ensure each job being assigned to one worker if there are more workers; otherwise, ensure each worker being assigned to one job.
In our case, the jobs are object regions <math id="S3.SS2.p1.9.m1.1" class="ltx_Math" alttext="\mathbf{r}_{i}" display="inline"><semantics id="S3.SS2.p1.9.m1.1a"><msub id="S3.SS2.p1.9.m1.1.1" xref="S3.SS2.p1.9.m1.1.1.cmml"><mi id="S3.SS2.p1.9.m1.1.1.2" xref="S3.SS2.p1.9.m1.1.1.2.cmml">ğ«</mi><mi id="S3.SS2.p1.9.m1.1.1.3" xref="S3.SS2.p1.9.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m1.1b"><apply id="S3.SS2.p1.9.m1.1.1.cmml" xref="S3.SS2.p1.9.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.9.m1.1.1.1.cmml" xref="S3.SS2.p1.9.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.9.m1.1.1.2.cmml" xref="S3.SS2.p1.9.m1.1.1.2">ğ«</ci><ci id="S3.SS2.p1.9.m1.1.1.3.cmml" xref="S3.SS2.p1.9.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m1.1c">\mathbf{r}_{i}</annotation></semantics></math> from image <math id="S3.SS2.p1.10.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS2.p1.10.m2.1a"><mi id="S3.SS2.p1.10.m2.1.1" xref="S3.SS2.p1.10.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m2.1b"><ci id="S3.SS2.p1.10.m2.1.1.cmml" xref="S3.SS2.p1.10.m2.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m2.1c">I</annotation></semantics></math> and the workers are words <math id="S3.SS2.p1.11.m3.1" class="ltx_Math" alttext="\mathbf{w}_{j}" display="inline"><semantics id="S3.SS2.p1.11.m3.1a"><msub id="S3.SS2.p1.11.m3.1.1" xref="S3.SS2.p1.11.m3.1.1.cmml"><mi id="S3.SS2.p1.11.m3.1.1.2" xref="S3.SS2.p1.11.m3.1.1.2.cmml">ğ°</mi><mi id="S3.SS2.p1.11.m3.1.1.3" xref="S3.SS2.p1.11.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m3.1b"><apply id="S3.SS2.p1.11.m3.1.1.cmml" xref="S3.SS2.p1.11.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.11.m3.1.1.1.cmml" xref="S3.SS2.p1.11.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.11.m3.1.1.2.cmml" xref="S3.SS2.p1.11.m3.1.1.2">ğ°</ci><ci id="S3.SS2.p1.11.m3.1.1.3.cmml" xref="S3.SS2.p1.11.m3.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m3.1c">\mathbf{w}_{j}</annotation></semantics></math> from caption <math id="S3.SS2.p1.12.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p1.12.m4.1a"><mi id="S3.SS2.p1.12.m4.1.1" xref="S3.SS2.p1.12.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m4.1b"><ci id="S3.SS2.p1.12.m4.1.1.cmml" xref="S3.SS2.p1.12.m4.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m4.1c">C</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Learning Object-Language Alignments from Image-Text Pairs.</span> Because the image-text pairs data is easy to acquire, we exploit it to sharply increase the coverage of object classes by leveraging the large vocabulary from an image-text dataset.
However, critical information is missing for the existing object detection models, which require labeling of image regions with words for training.
Hence, the key challenge is to find the correspondences between the set of regions and the set of words.
Instead of generating pseudo bounding boxes and labels for each image, we propose to formulate the problem of region-word alignments as an optimal bipartite matching problem.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.8" class="ltx_p">Given an image <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">I</annotation></semantics></math>,
the candidate region features from the image encoder is defined as <math id="S3.SS2.p3.2.m2.4" class="ltx_Math" alttext="\mathbf{R}=[\mathbf{r}_{1},\mathbf{r}_{2},\ldots,\mathbf{r}_{m}]" display="inline"><semantics id="S3.SS2.p3.2.m2.4a"><mrow id="S3.SS2.p3.2.m2.4.4" xref="S3.SS2.p3.2.m2.4.4.cmml"><mi id="S3.SS2.p3.2.m2.4.4.5" xref="S3.SS2.p3.2.m2.4.4.5.cmml">ğ‘</mi><mo id="S3.SS2.p3.2.m2.4.4.4" xref="S3.SS2.p3.2.m2.4.4.4.cmml">=</mo><mrow id="S3.SS2.p3.2.m2.4.4.3.3" xref="S3.SS2.p3.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS2.p3.2.m2.4.4.3.3.4" xref="S3.SS2.p3.2.m2.4.4.3.4.cmml">[</mo><msub id="S3.SS2.p3.2.m2.2.2.1.1.1" xref="S3.SS2.p3.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS2.p3.2.m2.2.2.1.1.1.2" xref="S3.SS2.p3.2.m2.2.2.1.1.1.2.cmml">ğ«</mi><mn id="S3.SS2.p3.2.m2.2.2.1.1.1.3" xref="S3.SS2.p3.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p3.2.m2.4.4.3.3.5" xref="S3.SS2.p3.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS2.p3.2.m2.3.3.2.2.2" xref="S3.SS2.p3.2.m2.3.3.2.2.2.cmml"><mi id="S3.SS2.p3.2.m2.3.3.2.2.2.2" xref="S3.SS2.p3.2.m2.3.3.2.2.2.2.cmml">ğ«</mi><mn id="S3.SS2.p3.2.m2.3.3.2.2.2.3" xref="S3.SS2.p3.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.p3.2.m2.4.4.3.3.6" xref="S3.SS2.p3.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">â€¦</mi><mo id="S3.SS2.p3.2.m2.4.4.3.3.7" xref="S3.SS2.p3.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS2.p3.2.m2.4.4.3.3.3" xref="S3.SS2.p3.2.m2.4.4.3.3.3.cmml"><mi id="S3.SS2.p3.2.m2.4.4.3.3.3.2" xref="S3.SS2.p3.2.m2.4.4.3.3.3.2.cmml">ğ«</mi><mi id="S3.SS2.p3.2.m2.4.4.3.3.3.3" xref="S3.SS2.p3.2.m2.4.4.3.3.3.3.cmml">m</mi></msub><mo stretchy="false" id="S3.SS2.p3.2.m2.4.4.3.3.8" xref="S3.SS2.p3.2.m2.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.4b"><apply id="S3.SS2.p3.2.m2.4.4.cmml" xref="S3.SS2.p3.2.m2.4.4"><eq id="S3.SS2.p3.2.m2.4.4.4.cmml" xref="S3.SS2.p3.2.m2.4.4.4"></eq><ci id="S3.SS2.p3.2.m2.4.4.5.cmml" xref="S3.SS2.p3.2.m2.4.4.5">ğ‘</ci><list id="S3.SS2.p3.2.m2.4.4.3.4.cmml" xref="S3.SS2.p3.2.m2.4.4.3.3"><apply id="S3.SS2.p3.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.1.2">ğ«</ci><cn type="integer" id="S3.SS2.p3.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS2.p3.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.3.3.2.2.2.1.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.2.2">ğ«</ci><cn type="integer" id="S3.SS2.p3.2.m2.3.3.2.2.2.3.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">â€¦</ci><apply id="S3.SS2.p3.2.m2.4.4.3.3.3.cmml" xref="S3.SS2.p3.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.4.4.3.3.3.1.cmml" xref="S3.SS2.p3.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S3.SS2.p3.2.m2.4.4.3.3.3.2.cmml" xref="S3.SS2.p3.2.m2.4.4.3.3.3.2">ğ«</ci><ci id="S3.SS2.p3.2.m2.4.4.3.3.3.3.cmml" xref="S3.SS2.p3.2.m2.4.4.3.3.3.3">ğ‘š</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.4c">\mathbf{R}=[\mathbf{r}_{1},\mathbf{r}_{2},\ldots,\mathbf{r}_{m}]</annotation></semantics></math>, where <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">m</annotation></semantics></math> is the number of the regions.
Given a caption <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">C</annotation></semantics></math>, we extract all the nouns from the caption and embed each noun with the language encoder. The word embedding is defined as <math id="S3.SS2.p3.5.m5.5" class="ltx_Math" alttext="\mathbf{W}=[\mathbf{w}_{1},\mathbf{w}_{2},\ldots,\mathbf{w}_{|\mathbf{W}|}]" display="inline"><semantics id="S3.SS2.p3.5.m5.5a"><mrow id="S3.SS2.p3.5.m5.5.5" xref="S3.SS2.p3.5.m5.5.5.cmml"><mi id="S3.SS2.p3.5.m5.5.5.5" xref="S3.SS2.p3.5.m5.5.5.5.cmml">ğ–</mi><mo id="S3.SS2.p3.5.m5.5.5.4" xref="S3.SS2.p3.5.m5.5.5.4.cmml">=</mo><mrow id="S3.SS2.p3.5.m5.5.5.3.3" xref="S3.SS2.p3.5.m5.5.5.3.4.cmml"><mo stretchy="false" id="S3.SS2.p3.5.m5.5.5.3.3.4" xref="S3.SS2.p3.5.m5.5.5.3.4.cmml">[</mo><msub id="S3.SS2.p3.5.m5.3.3.1.1.1" xref="S3.SS2.p3.5.m5.3.3.1.1.1.cmml"><mi id="S3.SS2.p3.5.m5.3.3.1.1.1.2" xref="S3.SS2.p3.5.m5.3.3.1.1.1.2.cmml">ğ°</mi><mn id="S3.SS2.p3.5.m5.3.3.1.1.1.3" xref="S3.SS2.p3.5.m5.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p3.5.m5.5.5.3.3.5" xref="S3.SS2.p3.5.m5.5.5.3.4.cmml">,</mo><msub id="S3.SS2.p3.5.m5.4.4.2.2.2" xref="S3.SS2.p3.5.m5.4.4.2.2.2.cmml"><mi id="S3.SS2.p3.5.m5.4.4.2.2.2.2" xref="S3.SS2.p3.5.m5.4.4.2.2.2.2.cmml">ğ°</mi><mn id="S3.SS2.p3.5.m5.4.4.2.2.2.3" xref="S3.SS2.p3.5.m5.4.4.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.p3.5.m5.5.5.3.3.6" xref="S3.SS2.p3.5.m5.5.5.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p3.5.m5.2.2" xref="S3.SS2.p3.5.m5.2.2.cmml">â€¦</mi><mo id="S3.SS2.p3.5.m5.5.5.3.3.7" xref="S3.SS2.p3.5.m5.5.5.3.4.cmml">,</mo><msub id="S3.SS2.p3.5.m5.5.5.3.3.3" xref="S3.SS2.p3.5.m5.5.5.3.3.3.cmml"><mi id="S3.SS2.p3.5.m5.5.5.3.3.3.2" xref="S3.SS2.p3.5.m5.5.5.3.3.3.2.cmml">ğ°</mi><mrow id="S3.SS2.p3.5.m5.1.1.1.3" xref="S3.SS2.p3.5.m5.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.5.m5.1.1.1.3.1" xref="S3.SS2.p3.5.m5.1.1.1.2.1.cmml">|</mo><mi id="S3.SS2.p3.5.m5.1.1.1.1" xref="S3.SS2.p3.5.m5.1.1.1.1.cmml">ğ–</mi><mo stretchy="false" id="S3.SS2.p3.5.m5.1.1.1.3.2" xref="S3.SS2.p3.5.m5.1.1.1.2.1.cmml">|</mo></mrow></msub><mo stretchy="false" id="S3.SS2.p3.5.m5.5.5.3.3.8" xref="S3.SS2.p3.5.m5.5.5.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.5b"><apply id="S3.SS2.p3.5.m5.5.5.cmml" xref="S3.SS2.p3.5.m5.5.5"><eq id="S3.SS2.p3.5.m5.5.5.4.cmml" xref="S3.SS2.p3.5.m5.5.5.4"></eq><ci id="S3.SS2.p3.5.m5.5.5.5.cmml" xref="S3.SS2.p3.5.m5.5.5.5">ğ–</ci><list id="S3.SS2.p3.5.m5.5.5.3.4.cmml" xref="S3.SS2.p3.5.m5.5.5.3.3"><apply id="S3.SS2.p3.5.m5.3.3.1.1.1.cmml" xref="S3.SS2.p3.5.m5.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.3.3.1.1.1.1.cmml" xref="S3.SS2.p3.5.m5.3.3.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m5.3.3.1.1.1.2.cmml" xref="S3.SS2.p3.5.m5.3.3.1.1.1.2">ğ°</ci><cn type="integer" id="S3.SS2.p3.5.m5.3.3.1.1.1.3.cmml" xref="S3.SS2.p3.5.m5.3.3.1.1.1.3">1</cn></apply><apply id="S3.SS2.p3.5.m5.4.4.2.2.2.cmml" xref="S3.SS2.p3.5.m5.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.4.4.2.2.2.1.cmml" xref="S3.SS2.p3.5.m5.4.4.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.5.m5.4.4.2.2.2.2.cmml" xref="S3.SS2.p3.5.m5.4.4.2.2.2.2">ğ°</ci><cn type="integer" id="S3.SS2.p3.5.m5.4.4.2.2.2.3.cmml" xref="S3.SS2.p3.5.m5.4.4.2.2.2.3">2</cn></apply><ci id="S3.SS2.p3.5.m5.2.2.cmml" xref="S3.SS2.p3.5.m5.2.2">â€¦</ci><apply id="S3.SS2.p3.5.m5.5.5.3.3.3.cmml" xref="S3.SS2.p3.5.m5.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.5.5.3.3.3.1.cmml" xref="S3.SS2.p3.5.m5.5.5.3.3.3">subscript</csymbol><ci id="S3.SS2.p3.5.m5.5.5.3.3.3.2.cmml" xref="S3.SS2.p3.5.m5.5.5.3.3.3.2">ğ°</ci><apply id="S3.SS2.p3.5.m5.1.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.1.3"><abs id="S3.SS2.p3.5.m5.1.1.1.2.1.cmml" xref="S3.SS2.p3.5.m5.1.1.1.3.1"></abs><ci id="S3.SS2.p3.5.m5.1.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1">ğ–</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.5c">\mathbf{W}=[\mathbf{w}_{1},\mathbf{w}_{2},\ldots,\mathbf{w}_{|\mathbf{W}|}]</annotation></semantics></math>, where the <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="|\mathbf{W}|" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mrow id="S3.SS2.p3.6.m6.1.2.2" xref="S3.SS2.p3.6.m6.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p3.6.m6.1.2.2.1" xref="S3.SS2.p3.6.m6.1.2.1.1.cmml">|</mo><mi id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">ğ–</mi><mo stretchy="false" id="S3.SS2.p3.6.m6.1.2.2.2" xref="S3.SS2.p3.6.m6.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.2.1.cmml" xref="S3.SS2.p3.6.m6.1.2.2"><abs id="S3.SS2.p3.6.m6.1.2.1.1.cmml" xref="S3.SS2.p3.6.m6.1.2.2.1"></abs><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">ğ–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">|\mathbf{W}|</annotation></semantics></math> is the number of nouns in the caption.
Usually, we have <math id="S3.SS2.p3.7.m7.1" class="ltx_Math" alttext="m&gt;|\mathbf{W}|" display="inline"><semantics id="S3.SS2.p3.7.m7.1a"><mrow id="S3.SS2.p3.7.m7.1.2" xref="S3.SS2.p3.7.m7.1.2.cmml"><mi id="S3.SS2.p3.7.m7.1.2.2" xref="S3.SS2.p3.7.m7.1.2.2.cmml">m</mi><mo id="S3.SS2.p3.7.m7.1.2.1" xref="S3.SS2.p3.7.m7.1.2.1.cmml">&gt;</mo><mrow id="S3.SS2.p3.7.m7.1.2.3.2" xref="S3.SS2.p3.7.m7.1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p3.7.m7.1.2.3.2.1" xref="S3.SS2.p3.7.m7.1.2.3.1.1.cmml">|</mo><mi id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml">ğ–</mi><mo stretchy="false" id="S3.SS2.p3.7.m7.1.2.3.2.2" xref="S3.SS2.p3.7.m7.1.2.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><apply id="S3.SS2.p3.7.m7.1.2.cmml" xref="S3.SS2.p3.7.m7.1.2"><gt id="S3.SS2.p3.7.m7.1.2.1.cmml" xref="S3.SS2.p3.7.m7.1.2.1"></gt><ci id="S3.SS2.p3.7.m7.1.2.2.cmml" xref="S3.SS2.p3.7.m7.1.2.2">ğ‘š</ci><apply id="S3.SS2.p3.7.m7.1.2.3.1.cmml" xref="S3.SS2.p3.7.m7.1.2.3.2"><abs id="S3.SS2.p3.7.m7.1.2.3.1.1.cmml" xref="S3.SS2.p3.7.m7.1.2.3.2.1"></abs><ci id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">ğ–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">m&gt;|\mathbf{W}|</annotation></semantics></math>, as the region proposal network provides sufficient candidate regions.
We define each region as a â€œjobâ€ which tries to find the most suitable â€œworkerâ€ and each word as a â€œworkerâ€ who finds the most confident â€œjobâ€.
In this context, our approach converts the region and word assignment task into a set-to-set bipartite matching problem from a global perspective.
The cost between image regions and words is defined as
the alignment scores <math id="S3.SS2.p3.8.m8.1" class="ltx_Math" alttext="\mathbf{S}=\mathbf{W}\mathbf{R}^{\top}" display="inline"><semantics id="S3.SS2.p3.8.m8.1a"><mrow id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.2" xref="S3.SS2.p3.8.m8.1.1.2.cmml">ğ’</mi><mo id="S3.SS2.p3.8.m8.1.1.1" xref="S3.SS2.p3.8.m8.1.1.1.cmml">=</mo><msup id="S3.SS2.p3.8.m8.1.1.3" xref="S3.SS2.p3.8.m8.1.1.3.cmml"><mi id="S3.SS2.p3.8.m8.1.1.3.2" xref="S3.SS2.p3.8.m8.1.1.3.2.cmml">ğ–ğ‘</mi><mo id="S3.SS2.p3.8.m8.1.1.3.3" xref="S3.SS2.p3.8.m8.1.1.3.3.cmml">âŠ¤</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><apply id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1"><eq id="S3.SS2.p3.8.m8.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1.1"></eq><ci id="S3.SS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.2">ğ’</ci><apply id="S3.SS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.1.1.3.1.cmml" xref="S3.SS2.p3.8.m8.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.8.m8.1.1.3.2.cmml" xref="S3.SS2.p3.8.m8.1.1.3.2">ğ–ğ‘</ci><csymbol cd="latexml" id="S3.SS2.p3.8.m8.1.1.3.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3.3">top</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">\mathbf{S}=\mathbf{W}\mathbf{R}^{\top}</annotation></semantics></math>.
The bipartite matching problem can then be efficiently solved by the off-the-shelf Hungarian AlgorithmÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuhn, <a href="#bib.bib30" title="" class="ltx_ref">1955</a>)</cite>.
After matching, the classifier head of the detection model is optimized by the following cross-entropy loss:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="L_{region-word}=\sum_{i=1}^{|W|}-\left[\log\sigma(s_{ik})+\sum_{j\in W^{\prime}}log(1-\sigma(s_{jk}))\right]," display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml">L</mi><mrow id="S3.E2.m1.2.2.1.1.3.3" xref="S3.E2.m1.2.2.1.1.3.3.cmml"><mrow id="S3.E2.m1.2.2.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.3.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.2.2" xref="S3.E2.m1.2.2.1.1.3.3.2.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.3.2.1" xref="S3.E2.m1.2.2.1.1.3.3.2.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.3.2.3" xref="S3.E2.m1.2.2.1.1.3.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.3.2.1a" xref="S3.E2.m1.2.2.1.1.3.3.2.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.3.2.4" xref="S3.E2.m1.2.2.1.1.3.3.2.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.3.2.1b" xref="S3.E2.m1.2.2.1.1.3.3.2.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.3.2.5" xref="S3.E2.m1.2.2.1.1.3.3.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.3.2.1c" xref="S3.E2.m1.2.2.1.1.3.3.2.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.3.2.6" xref="S3.E2.m1.2.2.1.1.3.3.2.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.3.2.1d" xref="S3.E2.m1.2.2.1.1.3.3.2.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.3.2.7" xref="S3.E2.m1.2.2.1.1.3.3.2.7.cmml">n</mi></mrow><mo id="S3.E2.m1.2.2.1.1.3.3.1" xref="S3.E2.m1.2.2.1.1.3.3.1.cmml">âˆ’</mo><mrow id="S3.E2.m1.2.2.1.1.3.3.3" xref="S3.E2.m1.2.2.1.1.3.3.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.3.2" xref="S3.E2.m1.2.2.1.1.3.3.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.3.3.1" xref="S3.E2.m1.2.2.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.3.3.3" xref="S3.E2.m1.2.2.1.1.3.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.3.3.1a" xref="S3.E2.m1.2.2.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.3.3.4" xref="S3.E2.m1.2.2.1.1.3.3.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.3.3.1b" xref="S3.E2.m1.2.2.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.3.3.5" xref="S3.E2.m1.2.2.1.1.3.3.3.5.cmml">d</mi></mrow></mrow></msub><mo rspace="0.111em" id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><munderover id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.2.2.1.1.1.3.2.2" xref="S3.E2.m1.2.2.1.1.1.3.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.2.2.1.1.1.3.2.3" xref="S3.E2.m1.2.2.1.1.1.3.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.3.2.3.2" xref="S3.E2.m1.2.2.1.1.1.3.2.3.2.cmml">i</mi><mo id="S3.E2.m1.2.2.1.1.1.3.2.3.1" xref="S3.E2.m1.2.2.1.1.1.3.2.3.1.cmml">=</mo><mn id="S3.E2.m1.2.2.1.1.1.3.2.3.3" xref="S3.E2.m1.2.2.1.1.1.3.2.3.3.cmml">1</mn></mrow><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">W</mi><mo stretchy="false" id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo></mrow></munderover><mo lspace="0em" id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.2.cmml">âˆ’</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3a" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">â¡</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.cmml">Ïƒ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">s</mi><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></mrow></msub><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.055em" id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml">+</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml"><munder id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.cmml"><mo movablelimits="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.1.cmml">âˆˆ</mo><msup id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3.2.cmml">W</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3.3.cmml">â€²</mo></msup></mrow></munder><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.4" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.2a" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.5" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.2b" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.cmml"><mn id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.3.cmml">1</mn><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.2.cmml">âˆ’</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.3.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2.cmml">s</mi><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></mrow></msub><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></eq><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2">ğ¿</ci><apply id="S3.E2.m1.2.2.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3"><minus id="S3.E2.m1.2.2.1.1.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1"></minus><apply id="S3.E2.m1.2.2.1.1.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2"><times id="S3.E2.m1.2.2.1.1.3.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.1"></times><ci id="S3.E2.m1.2.2.1.1.3.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.2">ğ‘Ÿ</ci><ci id="S3.E2.m1.2.2.1.1.3.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3">ğ‘’</ci><ci id="S3.E2.m1.2.2.1.1.3.3.2.4.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.4">ğ‘”</ci><ci id="S3.E2.m1.2.2.1.1.3.3.2.5.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.5">ğ‘–</ci><ci id="S3.E2.m1.2.2.1.1.3.3.2.6.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.6">ğ‘œ</ci><ci id="S3.E2.m1.2.2.1.1.3.3.2.7.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.7">ğ‘›</ci></apply><apply id="S3.E2.m1.2.2.1.1.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3"><times id="S3.E2.m1.2.2.1.1.3.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.1"></times><ci id="S3.E2.m1.2.2.1.1.3.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.2">ğ‘¤</ci><ci id="S3.E2.m1.2.2.1.1.3.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3">ğ‘œ</ci><ci id="S3.E2.m1.2.2.1.1.3.3.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.4">ğ‘Ÿ</ci><ci id="S3.E2.m1.2.2.1.1.3.3.3.5.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.5">ğ‘‘</ci></apply></apply></apply><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2"></minus><apply id="S3.E2.m1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.3">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.3">subscript</csymbol><sum id="S3.E2.m1.2.2.1.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.3.2.2"></sum><apply id="S3.E2.m1.2.2.1.1.1.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3.2.3"><eq id="S3.E2.m1.2.2.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.3.2.3.1"></eq><ci id="S3.E2.m1.2.2.1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.3.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.3"><abs id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.3.1"></abs><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">ğ‘Š</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><plus id="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3"></plus><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3"><log id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.1"></log><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2">ğœ</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">ğ‘ </ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2">ğ‘–</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘˜</ci></apply></apply></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2"><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2">subscript</csymbol><sum id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3"><in id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.1"></in><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.2">ğ‘—</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3.2">ğ‘Š</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.3.3.3">â€²</ci></apply></apply></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.2"></times><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.3">ğ‘™</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.4.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.4">ğ‘œ</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.5.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.5">ğ‘”</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.2"></minus><cn type="integer" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.3">1</cn><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.2"></times><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.3">ğœ</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2">ğ‘ </ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.2">ğ‘—</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.3">ğ‘˜</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">L_{region-word}=\sum_{i=1}^{|W|}-\left[\log\sigma(s_{ik})+\sum_{j\in W^{\prime}}log(1-\sigma(s_{jk}))\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p3.13" class="ltx_p">where <math id="S3.SS2.p3.9.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS2.p3.9.m1.1a"><mi id="S3.SS2.p3.9.m1.1.1" xref="S3.SS2.p3.9.m1.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m1.1b"><ci id="S3.SS2.p3.9.m1.1.1.cmml" xref="S3.SS2.p3.9.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m1.1c">\sigma</annotation></semantics></math> is the sigmoid activation, <math id="S3.SS2.p3.10.m2.1" class="ltx_Math" alttext="s_{ik}" display="inline"><semantics id="S3.SS2.p3.10.m2.1a"><msub id="S3.SS2.p3.10.m2.1.1" xref="S3.SS2.p3.10.m2.1.1.cmml"><mi id="S3.SS2.p3.10.m2.1.1.2" xref="S3.SS2.p3.10.m2.1.1.2.cmml">s</mi><mrow id="S3.SS2.p3.10.m2.1.1.3" xref="S3.SS2.p3.10.m2.1.1.3.cmml"><mi id="S3.SS2.p3.10.m2.1.1.3.2" xref="S3.SS2.p3.10.m2.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.10.m2.1.1.3.1" xref="S3.SS2.p3.10.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p3.10.m2.1.1.3.3" xref="S3.SS2.p3.10.m2.1.1.3.3.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.m2.1b"><apply id="S3.SS2.p3.10.m2.1.1.cmml" xref="S3.SS2.p3.10.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.10.m2.1.1.1.cmml" xref="S3.SS2.p3.10.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.10.m2.1.1.2.cmml" xref="S3.SS2.p3.10.m2.1.1.2">ğ‘ </ci><apply id="S3.SS2.p3.10.m2.1.1.3.cmml" xref="S3.SS2.p3.10.m2.1.1.3"><times id="S3.SS2.p3.10.m2.1.1.3.1.cmml" xref="S3.SS2.p3.10.m2.1.1.3.1"></times><ci id="S3.SS2.p3.10.m2.1.1.3.2.cmml" xref="S3.SS2.p3.10.m2.1.1.3.2">ğ‘–</ci><ci id="S3.SS2.p3.10.m2.1.1.3.3.cmml" xref="S3.SS2.p3.10.m2.1.1.3.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.10.m2.1c">s_{ik}</annotation></semantics></math> is the alignment score of the <math id="S3.SS2.p3.11.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p3.11.m3.1a"><mi id="S3.SS2.p3.11.m3.1.1" xref="S3.SS2.p3.11.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.11.m3.1b"><ci id="S3.SS2.p3.11.m3.1.1.cmml" xref="S3.SS2.p3.11.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.11.m3.1c">i</annotation></semantics></math>-th word embedding and its corresponding <math id="S3.SS2.p3.12.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p3.12.m4.1a"><mi id="S3.SS2.p3.12.m4.1.1" xref="S3.SS2.p3.12.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.12.m4.1b"><ci id="S3.SS2.p3.12.m4.1.1.cmml" xref="S3.SS2.p3.12.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.12.m4.1c">k</annotation></semantics></math>-th region feature matched by the bipartite matching, and <math id="S3.SS2.p3.13.m5.1" class="ltx_Math" alttext="W^{\prime}" display="inline"><semantics id="S3.SS2.p3.13.m5.1a"><msup id="S3.SS2.p3.13.m5.1.1" xref="S3.SS2.p3.13.m5.1.1.cmml"><mi id="S3.SS2.p3.13.m5.1.1.2" xref="S3.SS2.p3.13.m5.1.1.2.cmml">W</mi><mo id="S3.SS2.p3.13.m5.1.1.3" xref="S3.SS2.p3.13.m5.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.13.m5.1b"><apply id="S3.SS2.p3.13.m5.1.1.cmml" xref="S3.SS2.p3.13.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.13.m5.1.1.1.cmml" xref="S3.SS2.p3.13.m5.1.1">superscript</csymbol><ci id="S3.SS2.p3.13.m5.1.1.2.cmml" xref="S3.SS2.p3.13.m5.1.1.2">ğ‘Š</ci><ci id="S3.SS2.p3.13.m5.1.1.3.cmml" xref="S3.SS2.p3.13.m5.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.13.m5.1c">W^{\prime}</annotation></semantics></math> is the set of nouns from other captions in the same batch.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p">Besides, we further consider image-text pairs as special region-word pairs.
Particularly, we extract the RoI feature of an image by considering the entire image as a special region and the entire caption feature from the text encoder as a special word.
For an image, we consider its caption as a positive sample and other captions in the same minibatch as negative samples.
We use a similar binary cross-entropy loss <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="L_{image-text}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">L</mi><mrow id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml"><mrow id="S3.SS2.p4.1.m1.1.1.3.2" xref="S3.SS2.p4.1.m1.1.1.3.2.cmml"><mi id="S3.SS2.p4.1.m1.1.1.3.2.2" xref="S3.SS2.p4.1.m1.1.1.3.2.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.2.1" xref="S3.SS2.p4.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.2.3" xref="S3.SS2.p4.1.m1.1.1.3.2.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.2.1a" xref="S3.SS2.p4.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.2.4" xref="S3.SS2.p4.1.m1.1.1.3.2.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.2.1b" xref="S3.SS2.p4.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.2.5" xref="S3.SS2.p4.1.m1.1.1.3.2.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.2.1c" xref="S3.SS2.p4.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.2.6" xref="S3.SS2.p4.1.m1.1.1.3.2.6.cmml">e</mi></mrow><mo id="S3.SS2.p4.1.m1.1.1.3.1" xref="S3.SS2.p4.1.m1.1.1.3.1.cmml">âˆ’</mo><mrow id="S3.SS2.p4.1.m1.1.1.3.3" xref="S3.SS2.p4.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p4.1.m1.1.1.3.3.2" xref="S3.SS2.p4.1.m1.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.3.1" xref="S3.SS2.p4.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.3.3" xref="S3.SS2.p4.1.m1.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.3.1a" xref="S3.SS2.p4.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.3.4" xref="S3.SS2.p4.1.m1.1.1.3.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.3.1b" xref="S3.SS2.p4.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.3.5" xref="S3.SS2.p4.1.m1.1.1.3.3.5.cmml">t</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">ğ¿</ci><apply id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3"><minus id="S3.SS2.p4.1.m1.1.1.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.3.1"></minus><apply id="S3.SS2.p4.1.m1.1.1.3.2.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2"><times id="S3.SS2.p4.1.m1.1.1.3.2.1.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2.1"></times><ci id="S3.SS2.p4.1.m1.1.1.3.2.2.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2.2">ğ‘–</ci><ci id="S3.SS2.p4.1.m1.1.1.3.2.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2.3">ğ‘š</ci><ci id="S3.SS2.p4.1.m1.1.1.3.2.4.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2.4">ğ‘</ci><ci id="S3.SS2.p4.1.m1.1.1.3.2.5.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2.5">ğ‘”</ci><ci id="S3.SS2.p4.1.m1.1.1.3.2.6.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2.6">ğ‘’</ci></apply><apply id="S3.SS2.p4.1.m1.1.1.3.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3.3"><times id="S3.SS2.p4.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p4.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p4.1.m1.1.1.3.3.2">ğ‘¡</ci><ci id="S3.SS2.p4.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3.3.3">ğ‘’</ci><ci id="S3.SS2.p4.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.p4.1.m1.1.1.3.3.4">ğ‘¥</ci><ci id="S3.SS2.p4.1.m1.1.1.3.3.5.cmml" xref="S3.SS2.p4.1.m1.1.1.3.3.5">ğ‘¡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">L_{image-text}</annotation></semantics></math> for image-text pairs as Eq.Â <a href="#S3.E2" title="In 3.2 Learning Object-Language Alignments by Bipartite Matching â€£ 3 Method â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.2" class="ltx_p"><span id="S3.SS2.p5.2.1" class="ltx_text ltx_font_bold">Object Vocabulary.</span> The object vocabulary during the training can be the object labels <math id="S3.SS2.p5.1.m1.2" class="ltx_Math" alttext="\{\mathbb{C}^{base},\mathbb{C}^{novel}\}" display="inline"><semantics id="S3.SS2.p5.1.m1.2a"><mrow id="S3.SS2.p5.1.m1.2.2.2" xref="S3.SS2.p5.1.m1.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p5.1.m1.2.2.2.3" xref="S3.SS2.p5.1.m1.2.2.3.cmml">{</mo><msup id="S3.SS2.p5.1.m1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.cmml">â„‚</mi><mrow id="S3.SS2.p5.1.m1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.1.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.3.1" xref="S3.SS2.p5.1.m1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.3.1a" xref="S3.SS2.p5.1.m1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.3.4" xref="S3.SS2.p5.1.m1.1.1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.3.1b" xref="S3.SS2.p5.1.m1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.3.5" xref="S3.SS2.p5.1.m1.1.1.1.1.3.5.cmml">e</mi></mrow></msup><mo id="S3.SS2.p5.1.m1.2.2.2.4" xref="S3.SS2.p5.1.m1.2.2.3.cmml">,</mo><msup id="S3.SS2.p5.1.m1.2.2.2.2" xref="S3.SS2.p5.1.m1.2.2.2.2.cmml"><mi id="S3.SS2.p5.1.m1.2.2.2.2.2" xref="S3.SS2.p5.1.m1.2.2.2.2.2.cmml">â„‚</mi><mrow id="S3.SS2.p5.1.m1.2.2.2.2.3" xref="S3.SS2.p5.1.m1.2.2.2.2.3.cmml"><mi id="S3.SS2.p5.1.m1.2.2.2.2.3.2" xref="S3.SS2.p5.1.m1.2.2.2.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.2.2.2.2.3.1" xref="S3.SS2.p5.1.m1.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.1.m1.2.2.2.2.3.3" xref="S3.SS2.p5.1.m1.2.2.2.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.2.2.2.2.3.1a" xref="S3.SS2.p5.1.m1.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.1.m1.2.2.2.2.3.4" xref="S3.SS2.p5.1.m1.2.2.2.2.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.2.2.2.2.3.1b" xref="S3.SS2.p5.1.m1.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.1.m1.2.2.2.2.3.5" xref="S3.SS2.p5.1.m1.2.2.2.2.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.2.2.2.2.3.1c" xref="S3.SS2.p5.1.m1.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.1.m1.2.2.2.2.3.6" xref="S3.SS2.p5.1.m1.2.2.2.2.3.6.cmml">l</mi></mrow></msup><mo stretchy="false" id="S3.SS2.p5.1.m1.2.2.2.5" xref="S3.SS2.p5.1.m1.2.2.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.2b"><set id="S3.SS2.p5.1.m1.2.2.3.cmml" xref="S3.SS2.p5.1.m1.2.2.2"><apply id="S3.SS2.p5.1.m1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2">â„‚</ci><apply id="S3.SS2.p5.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3"><times id="S3.SS2.p5.1.m1.1.1.1.1.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3.1"></times><ci id="S3.SS2.p5.1.m1.1.1.1.1.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3.2">ğ‘</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3.3">ğ‘</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.3.4.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3.4">ğ‘ </ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.3.5.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3.5">ğ‘’</ci></apply></apply><apply id="S3.SS2.p5.1.m1.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2">superscript</csymbol><ci id="S3.SS2.p5.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2.2">â„‚</ci><apply id="S3.SS2.p5.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2.3"><times id="S3.SS2.p5.1.m1.2.2.2.2.3.1.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2.3.1"></times><ci id="S3.SS2.p5.1.m1.2.2.2.2.3.2.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2.3.2">ğ‘›</ci><ci id="S3.SS2.p5.1.m1.2.2.2.2.3.3.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2.3.3">ğ‘œ</ci><ci id="S3.SS2.p5.1.m1.2.2.2.2.3.4.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2.3.4">ğ‘£</ci><ci id="S3.SS2.p5.1.m1.2.2.2.2.3.5.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2.3.5">ğ‘’</ci><ci id="S3.SS2.p5.1.m1.2.2.2.2.3.6.cmml" xref="S3.SS2.p5.1.m1.2.2.2.2.3.6">ğ‘™</ci></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.2c">\{\mathbb{C}^{base},\mathbb{C}^{novel}\}</annotation></semantics></math> defined in the dataset, as many recent worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>; Gao etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> do for LVIS and COCO. However, we notice that it is not strictly following the open-vocabulary setting. In our design, we set the object vocabulary as all nouns in the image captions in each training batch. From the perspective of the whole training process, our object vocabulary <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="\mathbb{C}^{open}" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><msup id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">â„‚</mi><mrow id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml"><mi id="S3.SS2.p5.2.m2.1.1.3.2" xref="S3.SS2.p5.2.m2.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.2.m2.1.1.3.1" xref="S3.SS2.p5.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.2.m2.1.1.3.3" xref="S3.SS2.p5.2.m2.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.2.m2.1.1.3.1a" xref="S3.SS2.p5.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.2.m2.1.1.3.4" xref="S3.SS2.p5.2.m2.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.2.m2.1.1.3.1b" xref="S3.SS2.p5.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.2.m2.1.1.3.5" xref="S3.SS2.p5.2.m2.1.1.3.5.cmml">n</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">â„‚</ci><apply id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3"><times id="S3.SS2.p5.2.m2.1.1.3.1.cmml" xref="S3.SS2.p5.2.m2.1.1.3.1"></times><ci id="S3.SS2.p5.2.m2.1.1.3.2.cmml" xref="S3.SS2.p5.2.m2.1.1.3.2">ğ‘œ</ci><ci id="S3.SS2.p5.2.m2.1.1.3.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3.3">ğ‘</ci><ci id="S3.SS2.p5.2.m2.1.1.3.4.cmml" xref="S3.SS2.p5.2.m2.1.1.3.4">ğ‘’</ci><ci id="S3.SS2.p5.2.m2.1.1.3.5.cmml" xref="S3.SS2.p5.2.m2.1.1.3.5">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\mathbb{C}^{open}</annotation></semantics></math> size is much larger than the dataset label space. Our experiments show that this setting not only realizes the desirable open-vocabulary detection, but also achieves better performance than previous works. More details are discussed in the experiment section.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Network Architecture</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Our VLDet network includes three components:
a visual object detector, a text encoder, and an alignment between regions and words.
We choose the two-stage framework Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_citep">(Ren etÂ al., <a href="#bib.bib42" title="" class="ltx_ref">2015</a>; Cai &amp; Vasconcelos, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>; Zhou etÂ al., <a href="#bib.bib61" title="" class="ltx_ref">2021b</a>)</cite> as our object detector component.
The first stage in the object detector remains the same as Faster R-CNN, predicting object proposals by the region proposal network. To adapt the two-stage object detector into the open-vocabulary setting, we modify the second-stage in two aspects: (1) we use the class-agnostic localization head instead of the class-specific one. In this way, The localization head predicts bounding boxes regardless of their categories.
(2) We replace the trainable classifier weights with the language embeddings to convert the detector to the open-vocabulary setting.
We use a pretrained language model CLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite> as our text encoder.
Noting that there is no specific design in the object detector architecture, it is easy to replace Faster R-CNN by any other detector like Transformer-based detectorsÂ <cite class="ltx_cite ltx_citemacro_citep">(Carion etÂ al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>; Zhu etÂ al., <a href="#bib.bib63" title="" class="ltx_ref">2020</a>; Meng etÂ al., <a href="#bib.bib35" title="" class="ltx_ref">2021</a>; Zhang etÂ al., <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Training and Inference.</span>
The training data consists of a dataset annotated with base-classes a dataset of image-text pairs.
On the dataset with base-classes, the model is trained in a classical two-stage detection pipeline including classification and localization.
For each image-caption pair, the image encoder generates candidate regions by taking the image as input and the text encoder encodes nouns in the caption as word embeddings.
After that, the region features and word embeddings are aligned by the bipartite matching, followed by optimizing the model parameters. During inference, VLDet adopts the inference process of the two-stage object detector by incorporating the language embeddings into the classifier.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">COCO and COCO Caption.</span> Following open-vocabulary COCO setting (OV-COCO)Â <cite class="ltx_cite ltx_citemacro_citep">(Zareian etÂ al., <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite>, the COCO-2017 dataset is manually divided into 48 base classes and 17 novel classes, which are proposed by the zero-shot object detectionÂ <cite class="ltx_cite ltx_citemacro_citep">(Bansal etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>.
We keep 107,761 images with base class annotations as the training set and 4,836 images with base and novel class annotations as the validation set.
For images-text pairs data, we use COCO CaptionÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite> training set, which contains 5 human-generated captions for each image.
FollowingÂ <cite class="ltx_cite ltx_citemacro_citep">(Gu etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>; Zareian etÂ al., <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite>, we report mean Average Precision (mAP) at an IoU of 0.5.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">LVIS and Conceptual Captions.</span>
To study a large-scale generalized setting for open-vocabulary object detection, we conduct experiments on the LVISÂ <cite class="ltx_cite ltx_citemacro_citep">(Gupta etÂ al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite> benchmark with Conceptual Captions (CC3M)Â <cite class="ltx_cite ltx_citemacro_citep">(Sharma etÂ al., <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite> as the paired image-text training data.
LVIS dataset with object detection and instance segmentation annotations has diverse categories, which is more suitable for the open-vocabulary object detection task.
In this setting, we follow ViLDÂ <cite class="ltx_cite ltx_citemacro_citep">(Gu etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite> with the common classes and frequency classes as base classes (866 categories) and rare classes as novel classes (337 categories).
We remove novel class annotations in training and predict all categories in testing.
Different from COCO CaptionÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite> using the same images as COCO-2017, CC3MÂ <cite class="ltx_cite ltx_citemacro_citep">(Sharma etÂ al., <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite> collects 3 million image-text pairs from the web.
Following the official LVIS setting, we train a mask head on base-class detection data and report the mask AP for all categories.
</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">In each mini-batch, the ratio of base-class detection data and image-text pair data is 1:4.
For better generalized ability, we use fixed CLIP text encoder to embed the caption and object words.
To reduce training time, we always initialize the parameters from the detector trained by the fully supervised base-class detection data followingÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">In the open-vocabulary COCO experiments, we basically follow the OVR-CNNÂ <cite class="ltx_cite ltx_citemacro_citep">(Zareian etÂ al., <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite> setting without any data augmentation.
Our model adopts Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_citep">(Ren etÂ al., <a href="#bib.bib42" title="" class="ltx_ref">2015</a>)</cite> with ResNet50-C4Â <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite> as the backbone.
For the warmup, we increase the learning rate from 0 to 0.002 for the first 1000 iterations.
The model is trained for 90,000 iterations using SGD optimizer with batch size 8 and the learning rate is scaled down by a factor of 10 at 60,000 and 80,000 iterations.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">In open-vocabulary LVIS experiments, we follow DeticÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite> to adopt CenterNet2Â <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib61" title="" class="ltx_ref">2021b</a>)</cite> with ResNet50Â <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite> as backbone.
We use large scale jitteringÂ <cite class="ltx_cite ltx_citemacro_citep">(Ghiasi etÂ al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite> and repeat factor sampling as data augmentation.
For the warmup, we increase the learning rate from 0 to 2e-4 for the first 1000 iterations.
The model is trained for 90,000 iterations using Adam optimizer with batch size 16.
All the expriments are conducted on 8 NVIDIA V100 GPUs.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Open-vocabulary object detection results on COCO dataset. We follow the OVR-CNNÂ <cite class="ltx_cite ltx_citemacro_citep">(Zareian etÂ al., <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite> using the same training data of COCO CaptionÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite> and outperform the state-of-art method PB-OVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> on novel classes.</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">Method</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">Novel AP</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.1.1.3.1" class="ltx_text" style="color:#808080;">Base AP</span></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.1.1.4.1" class="ltx_text" style="color:#808080;">Overall AP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">Base-only</th>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">1.3</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.2.1.3.1" class="ltx_text" style="color:#808080;">52.8</span></td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.2.1.4.1" class="ltx_text" style="color:#808080;">39.3</span></td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:11.4pt;padding-right:11.4pt;">OVR-CNNÂ <cite class="ltx_cite ltx_citemacro_citep">(Zareian etÂ al., <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">22.8</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.3.2.3.1" class="ltx_text" style="color:#808080;">46.0</span></td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.3.2.4.1" class="ltx_text" style="color:#808080;">39.9</span></td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:11.4pt;padding-right:11.4pt;">DeticÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">27.8</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.4.3.3.1" class="ltx_text" style="color:#808080;">47.1</span></td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.4.3.4.1" class="ltx_text" style="color:#808080;">42.0</span></td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:11.4pt;padding-right:11.4pt;">RegionCLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhong etÂ al., <a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">26.8</td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.5.4.3.1" class="ltx_text" style="color:#808080;">54.8</span></td>
<td id="S4.T1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.5.4.4.1" class="ltx_text" style="color:#808080;">47.5</span></td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:11.4pt;padding-right:11.4pt;">ViLDÂ <cite class="ltx_cite ltx_citemacro_citep">(Gu etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">27.6</td>
<td id="S4.T1.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.6.5.3.1" class="ltx_text" style="color:#808080;">59.5</span></td>
<td id="S4.T1.1.6.5.4" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.6.5.4.1" class="ltx_text" style="color:#808080;">51.3</span></td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<th id="S4.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:11.4pt;padding-right:11.4pt;">PB-OVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">30.8</td>
<td id="S4.T1.1.7.6.3" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.7.6.3.1" class="ltx_text" style="color:#808080;">46.1</span></td>
<td id="S4.T1.1.7.6.4" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.7.6.4.1" class="ltx_text" style="color:#808080;">42.1</span></td>
</tr>
<tr id="S4.T1.1.8.7" class="ltx_tr">
<th id="S4.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-left:11.4pt;padding-right:11.4pt;">Our</th>
<td id="S4.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.8.7.2.1" class="ltx_text ltx_font_bold">32.0</span></td>
<td id="S4.T1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.8.7.3.1" class="ltx_text" style="color:#808080;">50.6</span></td>
<td id="S4.T1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S4.T1.1.8.7.4.1" class="ltx_text" style="color:#808080;">45.8</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Open-vocabulary object detection results on LVIS dataset for two different backbones ResNet50 (RN50)Â <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite> and Swin-BÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>. Base-only method only uses base class bounding box, which is considered as our baseline.</figcaption>
<table id="S4.T2.4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.4.4.4" class="ltx_tr">
<th id="S4.T2.4.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Method</th>
<td id="S4.T2.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Backbone</td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">mAP<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="{}^{mask}_{Novel}" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mmultiscripts id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.2.2" xref="S4.T2.1.1.1.1.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T2.1.1.1.1.m1.1.1a" xref="S4.T2.1.1.1.1.m1.1.1.cmml"></mprescripts><mrow id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.3.2" xref="S4.T2.1.1.1.1.m1.1.1.3.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.3.1" xref="S4.T2.1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T2.1.1.1.1.m1.1.1.3.3" xref="S4.T2.1.1.1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.3.1a" xref="S4.T2.1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T2.1.1.1.1.m1.1.1.3.4" xref="S4.T2.1.1.1.1.m1.1.1.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.3.1b" xref="S4.T2.1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T2.1.1.1.1.m1.1.1.3.5" xref="S4.T2.1.1.1.1.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.3.1c" xref="S4.T2.1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T2.1.1.1.1.m1.1.1.3.6" xref="S4.T2.1.1.1.1.m1.1.1.3.6.cmml">l</mi></mrow><mrow id="S4.T2.1.1.1.1.m1.1.1b" xref="S4.T2.1.1.1.1.m1.1.1.cmml"></mrow><mrow id="S4.T2.1.1.1.1.m1.1.1c" xref="S4.T2.1.1.1.1.m1.1.1.cmml"></mrow><mrow id="S4.T2.1.1.1.1.m1.1.1.2.3" xref="S4.T2.1.1.1.1.m1.1.1.2.3.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.2.3.2" xref="S4.T2.1.1.1.1.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.2.3.1" xref="S4.T2.1.1.1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.1.1.1.1.m1.1.1.2.3.3" xref="S4.T2.1.1.1.1.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.2.3.1a" xref="S4.T2.1.1.1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.1.1.1.1.m1.1.1.2.3.4" xref="S4.T2.1.1.1.1.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.2.3.1b" xref="S4.T2.1.1.1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.1.1.1.1.m1.1.1.2.3.5" xref="S4.T2.1.1.1.1.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">subscript</csymbol><apply id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T2.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2.2">absent</csymbol><apply id="S4.T2.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2.3"><times id="S4.T2.1.1.1.1.m1.1.1.2.3.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2.3.1"></times><ci id="S4.T2.1.1.1.1.m1.1.1.2.3.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2.3.2">ğ‘š</ci><ci id="S4.T2.1.1.1.1.m1.1.1.2.3.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2.3.3">ğ‘</ci><ci id="S4.T2.1.1.1.1.m1.1.1.2.3.4.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2.3.4">ğ‘ </ci><ci id="S4.T2.1.1.1.1.m1.1.1.2.3.5.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><apply id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3"><times id="S4.T2.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.1"></times><ci id="S4.T2.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.2">ğ‘</ci><ci id="S4.T2.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.3">ğ‘œ</ci><ci id="S4.T2.1.1.1.1.m1.1.1.3.4.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.4">ğ‘£</ci><ci id="S4.T2.1.1.1.1.m1.1.1.3.5.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.5">ğ‘’</ci><ci id="S4.T2.1.1.1.1.m1.1.1.3.6.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">{}^{mask}_{Novel}</annotation></semantics></math>
</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">mAP<math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="{}^{mask}_{c}" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mmultiscripts id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml"><mi id="S4.T2.2.2.2.2.m1.1.1.2.2" xref="S4.T2.2.2.2.2.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T2.2.2.2.2.m1.1.1a" xref="S4.T2.2.2.2.2.m1.1.1.cmml"></mprescripts><mi id="S4.T2.2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.2.m1.1.1.3.cmml">c</mi><mrow id="S4.T2.2.2.2.2.m1.1.1b" xref="S4.T2.2.2.2.2.m1.1.1.cmml"></mrow><mrow id="S4.T2.2.2.2.2.m1.1.1c" xref="S4.T2.2.2.2.2.m1.1.1.cmml"></mrow><mrow id="S4.T2.2.2.2.2.m1.1.1.2.3" xref="S4.T2.2.2.2.2.m1.1.1.2.3.cmml"><mi id="S4.T2.2.2.2.2.m1.1.1.2.3.2" xref="S4.T2.2.2.2.2.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.2.m1.1.1.2.3.1" xref="S4.T2.2.2.2.2.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.2.2.2.2.m1.1.1.2.3.3" xref="S4.T2.2.2.2.2.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.2.m1.1.1.2.3.1a" xref="S4.T2.2.2.2.2.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.2.2.2.2.m1.1.1.2.3.4" xref="S4.T2.2.2.2.2.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.2.m1.1.1.2.3.1b" xref="S4.T2.2.2.2.2.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.2.2.2.2.m1.1.1.2.3.5" xref="S4.T2.2.2.2.2.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">subscript</csymbol><apply id="S4.T2.2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.2.m1.1.1.2.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T2.2.2.2.2.m1.1.1.2.2.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2.2">absent</csymbol><apply id="S4.T2.2.2.2.2.m1.1.1.2.3.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2.3"><times id="S4.T2.2.2.2.2.m1.1.1.2.3.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2.3.1"></times><ci id="S4.T2.2.2.2.2.m1.1.1.2.3.2.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2.3.2">ğ‘š</ci><ci id="S4.T2.2.2.2.2.m1.1.1.2.3.3.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2.3.3">ğ‘</ci><ci id="S4.T2.2.2.2.2.m1.1.1.2.3.4.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2.3.4">ğ‘ </ci><ci id="S4.T2.2.2.2.2.m1.1.1.2.3.5.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><ci id="S4.T2.2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.2.2.2.2.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">{}^{mask}_{c}</annotation></semantics></math>
</td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">mAP<math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="{}^{mask}_{f}" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><mmultiscripts id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml"><mi id="S4.T2.3.3.3.3.m1.1.1.2.2" xref="S4.T2.3.3.3.3.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T2.3.3.3.3.m1.1.1a" xref="S4.T2.3.3.3.3.m1.1.1.cmml"></mprescripts><mi id="S4.T2.3.3.3.3.m1.1.1.3" xref="S4.T2.3.3.3.3.m1.1.1.3.cmml">f</mi><mrow id="S4.T2.3.3.3.3.m1.1.1b" xref="S4.T2.3.3.3.3.m1.1.1.cmml"></mrow><mrow id="S4.T2.3.3.3.3.m1.1.1c" xref="S4.T2.3.3.3.3.m1.1.1.cmml"></mrow><mrow id="S4.T2.3.3.3.3.m1.1.1.2.3" xref="S4.T2.3.3.3.3.m1.1.1.2.3.cmml"><mi id="S4.T2.3.3.3.3.m1.1.1.2.3.2" xref="S4.T2.3.3.3.3.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T2.3.3.3.3.m1.1.1.2.3.1" xref="S4.T2.3.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.3.3.3.3.m1.1.1.2.3.3" xref="S4.T2.3.3.3.3.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.3.3.3.3.m1.1.1.2.3.1a" xref="S4.T2.3.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.3.3.3.3.m1.1.1.2.3.4" xref="S4.T2.3.3.3.3.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T2.3.3.3.3.m1.1.1.2.3.1b" xref="S4.T2.3.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.3.3.3.3.m1.1.1.2.3.5" xref="S4.T2.3.3.3.3.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><apply id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.3.3.3.m1.1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">subscript</csymbol><apply id="S4.T2.3.3.3.3.m1.1.1.2.cmml" xref="S4.T2.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.3.3.3.m1.1.1.2.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T2.3.3.3.3.m1.1.1.2.2.cmml" xref="S4.T2.3.3.3.3.m1.1.1.2.2">absent</csymbol><apply id="S4.T2.3.3.3.3.m1.1.1.2.3.cmml" xref="S4.T2.3.3.3.3.m1.1.1.2.3"><times id="S4.T2.3.3.3.3.m1.1.1.2.3.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1.2.3.1"></times><ci id="S4.T2.3.3.3.3.m1.1.1.2.3.2.cmml" xref="S4.T2.3.3.3.3.m1.1.1.2.3.2">ğ‘š</ci><ci id="S4.T2.3.3.3.3.m1.1.1.2.3.3.cmml" xref="S4.T2.3.3.3.3.m1.1.1.2.3.3">ğ‘</ci><ci id="S4.T2.3.3.3.3.m1.1.1.2.3.4.cmml" xref="S4.T2.3.3.3.3.m1.1.1.2.3.4">ğ‘ </ci><ci id="S4.T2.3.3.3.3.m1.1.1.2.3.5.cmml" xref="S4.T2.3.3.3.3.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><ci id="S4.T2.3.3.3.3.m1.1.1.3.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">{}^{mask}_{f}</annotation></semantics></math>
</td>
<td id="S4.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">mAP<math id="S4.T2.4.4.4.4.m1.1" class="ltx_Math" alttext="{}^{mask}_{all}" display="inline"><semantics id="S4.T2.4.4.4.4.m1.1a"><mmultiscripts id="S4.T2.4.4.4.4.m1.1.1" xref="S4.T2.4.4.4.4.m1.1.1.cmml"><mi id="S4.T2.4.4.4.4.m1.1.1.2.2" xref="S4.T2.4.4.4.4.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T2.4.4.4.4.m1.1.1a" xref="S4.T2.4.4.4.4.m1.1.1.cmml"></mprescripts><mrow id="S4.T2.4.4.4.4.m1.1.1.3" xref="S4.T2.4.4.4.4.m1.1.1.3.cmml"><mi id="S4.T2.4.4.4.4.m1.1.1.3.2" xref="S4.T2.4.4.4.4.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.4.4.4.4.m1.1.1.3.1" xref="S4.T2.4.4.4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T2.4.4.4.4.m1.1.1.3.3" xref="S4.T2.4.4.4.4.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T2.4.4.4.4.m1.1.1.3.1a" xref="S4.T2.4.4.4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T2.4.4.4.4.m1.1.1.3.4" xref="S4.T2.4.4.4.4.m1.1.1.3.4.cmml">l</mi></mrow><mrow id="S4.T2.4.4.4.4.m1.1.1b" xref="S4.T2.4.4.4.4.m1.1.1.cmml"></mrow><mrow id="S4.T2.4.4.4.4.m1.1.1c" xref="S4.T2.4.4.4.4.m1.1.1.cmml"></mrow><mrow id="S4.T2.4.4.4.4.m1.1.1.2.3" xref="S4.T2.4.4.4.4.m1.1.1.2.3.cmml"><mi id="S4.T2.4.4.4.4.m1.1.1.2.3.2" xref="S4.T2.4.4.4.4.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T2.4.4.4.4.m1.1.1.2.3.1" xref="S4.T2.4.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.4.4.4.4.m1.1.1.2.3.3" xref="S4.T2.4.4.4.4.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.4.4.4.4.m1.1.1.2.3.1a" xref="S4.T2.4.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.4.4.4.4.m1.1.1.2.3.4" xref="S4.T2.4.4.4.4.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T2.4.4.4.4.m1.1.1.2.3.1b" xref="S4.T2.4.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T2.4.4.4.4.m1.1.1.2.3.5" xref="S4.T2.4.4.4.4.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.m1.1b"><apply id="S4.T2.4.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.4.4.4.4.m1.1.1.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1">subscript</csymbol><apply id="S4.T2.4.4.4.4.m1.1.1.2.cmml" xref="S4.T2.4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.4.4.4.4.m1.1.1.2.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T2.4.4.4.4.m1.1.1.2.2.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2.2">absent</csymbol><apply id="S4.T2.4.4.4.4.m1.1.1.2.3.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2.3"><times id="S4.T2.4.4.4.4.m1.1.1.2.3.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2.3.1"></times><ci id="S4.T2.4.4.4.4.m1.1.1.2.3.2.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2.3.2">ğ‘š</ci><ci id="S4.T2.4.4.4.4.m1.1.1.2.3.3.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2.3.3">ğ‘</ci><ci id="S4.T2.4.4.4.4.m1.1.1.2.3.4.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2.3.4">ğ‘ </ci><ci id="S4.T2.4.4.4.4.m1.1.1.2.3.5.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><apply id="S4.T2.4.4.4.4.m1.1.1.3.cmml" xref="S4.T2.4.4.4.4.m1.1.1.3"><times id="S4.T2.4.4.4.4.m1.1.1.3.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1.3.1"></times><ci id="S4.T2.4.4.4.4.m1.1.1.3.2.cmml" xref="S4.T2.4.4.4.4.m1.1.1.3.2">ğ‘</ci><ci id="S4.T2.4.4.4.4.m1.1.1.3.3.cmml" xref="S4.T2.4.4.4.4.m1.1.1.3.3">ğ‘™</ci><ci id="S4.T2.4.4.4.4.m1.1.1.3.4.cmml" xref="S4.T2.4.4.4.4.m1.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.m1.1c">{}^{mask}_{all}</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.4.4.5.1" class="ltx_tr">
<th id="S4.T2.4.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Base-only</th>
<td id="S4.T2.4.4.5.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">RN50</td>
<td id="S4.T2.4.4.5.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">16.3</td>
<td id="S4.T2.4.4.5.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">31.0</td>
<td id="S4.T2.4.4.5.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">35.4</td>
<td id="S4.T2.4.4.5.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">30.0</td>
</tr>
<tr id="S4.T2.4.4.6.2" class="ltx_tr">
<th id="S4.T2.4.4.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">ViLDÂ <cite class="ltx_cite ltx_citemacro_citep">(Gu etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S4.T2.4.4.6.2.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">RN50</td>
<td id="S4.T2.4.4.6.2.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">16.6</td>
<td id="S4.T2.4.4.6.2.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">24.6</td>
<td id="S4.T2.4.4.6.2.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">30.3</td>
<td id="S4.T2.4.4.6.2.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">25.5</td>
</tr>
<tr id="S4.T2.4.4.7.3" class="ltx_tr">
<th id="S4.T2.4.4.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">RegionCLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhong etÂ al., <a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.4.4.7.3.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">RN50</td>
<td id="S4.T2.4.4.7.3.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">17.1</td>
<td id="S4.T2.4.4.7.3.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">27.4</td>
<td id="S4.T2.4.4.7.3.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">34.0</td>
<td id="S4.T2.4.4.7.3.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">28.2</td>
</tr>
<tr id="S4.T2.4.4.8.4" class="ltx_tr">
<th id="S4.T2.4.4.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">DetProÂ <cite class="ltx_cite ltx_citemacro_citep">(Du etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.4.4.8.4.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">RN50</td>
<td id="S4.T2.4.4.8.4.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">19.8</td>
<td id="S4.T2.4.4.8.4.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">25.6</td>
<td id="S4.T2.4.4.8.4.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">28.9</td>
<td id="S4.T2.4.4.8.4.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">25.9</td>
</tr>
<tr id="S4.T2.4.4.9.5" class="ltx_tr">
<th id="S4.T2.4.4.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">DeticÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.4.4.9.5.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">RN50</td>
<td id="S4.T2.4.4.9.5.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">19.5</td>
<td id="S4.T2.4.4.9.5.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T2.4.4.9.5.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T2.4.4.9.5.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">30.9</td>
</tr>
<tr id="S4.T2.4.4.10.6" class="ltx_tr">
<th id="S4.T2.4.4.10.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">Our</th>
<td id="S4.T2.4.4.10.6.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">RN50</td>
<td id="S4.T2.4.4.10.6.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S4.T2.4.4.10.6.3.1" class="ltx_text ltx_font_bold">21.7</span></td>
<td id="S4.T2.4.4.10.6.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">29.8</td>
<td id="S4.T2.4.4.10.6.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">34.3</td>
<td id="S4.T2.4.4.10.6.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">30.1</td>
</tr>
<tr id="S4.T2.4.4.11.7" class="ltx_tr">
<th id="S4.T2.4.4.11.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Base-only</th>
<td id="S4.T2.4.4.11.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Swin-B</td>
<td id="S4.T2.4.4.11.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">21.9</td>
<td id="S4.T2.4.4.11.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">40.5</td>
<td id="S4.T2.4.4.11.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">43.3</td>
<td id="S4.T2.4.4.11.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">38.4</td>
</tr>
<tr id="S4.T2.4.4.12.8" class="ltx_tr">
<th id="S4.T2.4.4.12.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">DeticÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.4.4.12.8.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">Swin-B</td>
<td id="S4.T2.4.4.12.8.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">23.9</td>
<td id="S4.T2.4.4.12.8.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">40.2</td>
<td id="S4.T2.4.4.12.8.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">42.8</td>
<td id="S4.T2.4.4.12.8.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">38.4</td>
</tr>
<tr id="S4.T2.4.4.13.9" class="ltx_tr">
<th id="S4.T2.4.4.13.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;">Our</th>
<td id="S4.T2.4.4.13.9.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;">Swin-B</td>
<td id="S4.T2.4.4.13.9.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S4.T2.4.4.13.9.3.1" class="ltx_text ltx_font_bold">26.3</span></td>
<td id="S4.T2.4.4.13.9.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;">39.4</td>
<td id="S4.T2.4.4.13.9.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;">41.9</td>
<td id="S4.T2.4.4.13.9.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;">38.1</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2211.14843/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="257" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visualization of the bipartite matching results. Here we show how the proposed region matched with their corresponding words. Best viewed on screen.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Open-Vocabulary COCO</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 â€£ 4.2 Implementation Details â€£ 4 Experiments â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the performance comparisons of different methods for open-vocabulary COCO datasets.
It can be seen that our model performs the best on novel classes, suggesting the superiority of using the bipartite matching loss with the image-text pairs.
Base-only method indicates the Faster R-CNN trained with the fully supervised COCO base-category detection data with CLIP embeddings as the classifier head.
Although CLIP has the generalized ability to the novel class, it only achieves <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="1.3" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">1.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="float" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">1.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">1.3</annotation></semantics></math> mAP.
Although ViLD and RegionCLIP distills the region proposal features using the CLIP, their performances are inferior to ours on novel categories, which is the main metric in the open-vocabulary object detection setting.
These distillation methods require both the image encoder and the text encoder from the pretrained CLIP model to learn the matching between the image regions and the vocabulary. Thus, their performances on novel classes are limited by the pre-trained model, which is trained for global image-text matching instead of region-word matching.
Compared with the state-of-the-art method PB-OVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> which leverages the COCO CaptionÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite>, VGÂ <cite class="ltx_cite ltx_citemacro_citep">(Krishna etÂ al., <a href="#bib.bib29" title="" class="ltx_ref">2017</a>)</cite> and SBU Caption datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Ordonez etÂ al., <a href="#bib.bib37" title="" class="ltx_ref">2011</a>)</cite> with about 1M training image-text pairs,
our method still outperforms it on all metrics while we use only 10K images of COCO Caption as training data.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Open-Vocabulary LVIS</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 â€£ 4.2 Implementation Details â€£ 4 Experiments â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the performance comparisons on the open-vocabulary LVIS object detection.
The â€œBased-onlyâ€ method is only fully supervised on the base categories, which is the baseline of our method.
As mentioned above, since we use fix CLIP embeddings as the classifier head, the base-only method has a certain generalization ability.
The proposed model outperforms the state-of-the-art method DetProÂ <cite class="ltx_cite ltx_citemacro_citep">(Du etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> by 1.9 % AP on novel classes.
RegionCLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhong etÂ al., <a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite> uses a CLIP model to predict pseudo-labels for image regions and pretrains a visual model with the region-text pairs.
We can see that it is effective to improve the generalization ability to novel classes.
Similar strategy is used in recent worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>.
However, such two-step pseudo-label method is easy to amplify false predictions and it relies on good initialization of the teacher model to generate good pseudo-labels.
In contrast, our proposed method is assigning class labels from a global perspective and optimizing the object detection model without finetuning.
To further explore the scalability of our model, we use Swin-B as our backbone, and our model outperforms the ResNet backbone with 4.5% mAP on novel classes.
Compared with Detic, our model achieves 2.4% gain on OV-LVIS novel classes, suggesting the effectiveness of the label assignment from the global perspective.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Visualization.</span>
As shown in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.2 Implementation Details â€£ 4 Experiments â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we visualize some cases of matching results of CC3M in open vocabulary LVIS setting.
As we can see, the model can extract promising region-words pairs from image-text data, avoiding expensive annotations.
The results of the matching include a variety of objects, such as â€œjarâ€, â€œtypewriterâ€, and â€œcameraâ€, which indicates that our method significantly expands the vocabulary for object detection.
It demonstrates the generalization ability of our method to novel classes.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Studies</h3>

<div id="S4.SS5.p1" class="ltx_para ltx_noindent">
<p id="S4.SS5.p1.1" class="ltx_p"><span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_bold">Object Vocabulary Size.</span>
In our VLDet, we follow the previous workÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhong etÂ al., <a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite> using all nouns in COCO Caption and CC3M and filtering out low-frequency words, resulting in 4764/6250 concepts left.
We notice that many works recently use the LVIS and COCO label spaces as the object vocabulary during training, which do not strictly follow the open-vocabulary setting.
We further analyze the effect of training our model with different vocabulary sizes.
In TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.5 Ablation Studies â€£ 4 Experiments â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we replace our object vocabulary with the category names in COCO and LVIS datasets, i.e. only using the category names in the captions.
We can see from Table 3 that a larger vocabulary significantly boosts the unseen class performance.
It achieves gains of 1.8% and 1.5% on the novel categories of OV-COCO and OV-LVIS, respectively, indicating that training with a large vocabulary size leads to better generalization.
In other words, with a bigger vocabulary, the model can learn more object language alignments which benefits the novel-class performance during inference.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para ltx_noindent">
<p id="S4.SS5.p2.1" class="ltx_p"><span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_bold">One-to-One vs. One-to-Many.</span>
The key to extract a set of image region-word pairs from an image-text pair is to optimize the assignment problem from a global perspective.
To further investigate the impact of assignment algorithms, we implement two global algorithms, HungarianÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuhn, <a href="#bib.bib30" title="" class="ltx_ref">1955</a>)</cite> and SinkhornÂ <cite class="ltx_cite ltx_citemacro_citep">(Cuturi, <a href="#bib.bib11" title="" class="ltx_ref">2013</a>)</cite> algorithms, where the former does one-to-one region-word assignment, and the latter provides soft alignments between one word and many regions.
Considering that there may be multiple instances of the same category in an image, Sinkhorn algorithm is able to produce multiple regions to the same word, while it might also introduce more noisy region-word pairs.
From TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.5 Ablation Studies â€£ 4 Experiments â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we observe that the one-to-one assignment achieves 32.0 AP and 21.7 AP for novel classes, both outperforming the one-to-many assignments of 29.1 AP and 18.5 AP. The one-to-one assignment assumption sharply reduces mis-alignments by providing each word with a high-quality image region.
<br class="ltx_break"><span id="S4.SS5.p2.1.2" class="ltx_text ltx_font_italic">Discussion on noisy image-text pairs.</span>
In VLDet, we employ the Hungarian algorithm to assign each word a unique region. When dealing with multi-word expressions in a text, such as â€œa basket of orangesâ€, we select the most confident region-word pair and ignore the remaining ones, such as the one associated with â€œorangesâ€.
Similarly, if the caption is incomplete (which is often the case, since the caption may not describe every instance in the image), we will ignore those unmatched regions as well.
Note that those regions will not be optimized as background, and the loss is only computed with the regions in the matching results.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para ltx_noindent">
<p id="S4.SS5.p3.1" class="ltx_p"><span id="S4.SS5.p3.1.1" class="ltx_text ltx_font_bold">Different Strategies for Region-Word Alignment.</span>
FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.5 Ablation Studies â€£ 4 Experiments â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows a comparison to different strategies for the region-word alignment in weakly supervised methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>; Redmon &amp; Farhadi, <a href="#bib.bib41" title="" class="ltx_ref">2017</a>; Yao etÂ al., <a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite>.
For fair comparison, we use the region-word alignment loss alone with different strategies for training.
â€œMax-scoreâ€Â <cite class="ltx_cite ltx_citemacro_citep">(Redmon &amp; Farhadi, <a href="#bib.bib41" title="" class="ltx_ref">2017</a>; Yao etÂ al., <a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite> indicates each word selects the proposal with the largest similarity score, which is widely used in weakly-supervised object detection.
However, it tends to assigns each region proposal to a base class due to the full supervision for the bass classes. In contrast, our formulated set-to-set matching aims to minimize the global matching cost to find each region proposal a distinguished corresponding word, where strongly matched base-class regions can push other region proposals to explore the matching with novel classes.
â€œMax-sizeâ€Â <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>, assigns all nouns in a caption to the proposal with the max size.
The results in FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.5 Ablation Studies â€£ 4 Experiments â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> show that our method achieves the best performance for the novel classes on both OV-COCO and OV-LVIS.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation study on vocabulary size. We replace our object vocabulary with the category names of OV-COCO and OV-LVIS. The results show that training with a large vocabulary size enables better generalization.</figcaption>
<table id="S4.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.4.5.1" class="ltx_tr">
<th id="S4.T3.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;" rowspan="2"><span id="S4.T3.4.5.1.1.1" class="ltx_text">Vocabulary</span></th>
<th id="S4.T3.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="3">OV-COCO</th>
<th id="S4.T3.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="3">OV-LVIS</th>
</tr>
<tr id="S4.T3.4.4" class="ltx_tr">
<th id="S4.T3.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">Size</th>
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">mAP<sub id="S4.T3.1.1.1.1" class="ltx_sub"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_italic">novel</span></sub>
</th>
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">mAP<sub id="S4.T3.2.2.2.1" class="ltx_sub"><span id="S4.T3.2.2.2.1.1" class="ltx_text ltx_font_italic">all</span></sub>
</th>
<th id="S4.T3.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">Size</th>
<th id="S4.T3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">mAP<math id="S4.T3.3.3.3.m1.1" class="ltx_Math" alttext="{}^{mask}_{novel}" display="inline"><semantics id="S4.T3.3.3.3.m1.1a"><mmultiscripts id="S4.T3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.m1.1.1.cmml"><mi id="S4.T3.3.3.3.m1.1.1.2.2" xref="S4.T3.3.3.3.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T3.3.3.3.m1.1.1a" xref="S4.T3.3.3.3.m1.1.1.cmml"></mprescripts><mrow id="S4.T3.3.3.3.m1.1.1.3" xref="S4.T3.3.3.3.m1.1.1.3.cmml"><mi id="S4.T3.3.3.3.m1.1.1.3.2" xref="S4.T3.3.3.3.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.T3.3.3.3.m1.1.1.3.1" xref="S4.T3.3.3.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T3.3.3.3.m1.1.1.3.3" xref="S4.T3.3.3.3.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.T3.3.3.3.m1.1.1.3.1a" xref="S4.T3.3.3.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T3.3.3.3.m1.1.1.3.4" xref="S4.T3.3.3.3.m1.1.1.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.T3.3.3.3.m1.1.1.3.1b" xref="S4.T3.3.3.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T3.3.3.3.m1.1.1.3.5" xref="S4.T3.3.3.3.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T3.3.3.3.m1.1.1.3.1c" xref="S4.T3.3.3.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T3.3.3.3.m1.1.1.3.6" xref="S4.T3.3.3.3.m1.1.1.3.6.cmml">l</mi></mrow><mrow id="S4.T3.3.3.3.m1.1.1b" xref="S4.T3.3.3.3.m1.1.1.cmml"></mrow><mrow id="S4.T3.3.3.3.m1.1.1c" xref="S4.T3.3.3.3.m1.1.1.cmml"></mrow><mrow id="S4.T3.3.3.3.m1.1.1.2.3" xref="S4.T3.3.3.3.m1.1.1.2.3.cmml"><mi id="S4.T3.3.3.3.m1.1.1.2.3.2" xref="S4.T3.3.3.3.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T3.3.3.3.m1.1.1.2.3.1" xref="S4.T3.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T3.3.3.3.m1.1.1.2.3.3" xref="S4.T3.3.3.3.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T3.3.3.3.m1.1.1.2.3.1a" xref="S4.T3.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T3.3.3.3.m1.1.1.2.3.4" xref="S4.T3.3.3.3.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T3.3.3.3.m1.1.1.2.3.1b" xref="S4.T3.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T3.3.3.3.m1.1.1.2.3.5" xref="S4.T3.3.3.3.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><apply id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.3.3.3.m1.1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">subscript</csymbol><apply id="S4.T3.3.3.3.m1.1.1.2.cmml" xref="S4.T3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.3.3.3.m1.1.1.2.1.cmml" xref="S4.T3.3.3.3.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T3.3.3.3.m1.1.1.2.2.cmml" xref="S4.T3.3.3.3.m1.1.1.2.2">absent</csymbol><apply id="S4.T3.3.3.3.m1.1.1.2.3.cmml" xref="S4.T3.3.3.3.m1.1.1.2.3"><times id="S4.T3.3.3.3.m1.1.1.2.3.1.cmml" xref="S4.T3.3.3.3.m1.1.1.2.3.1"></times><ci id="S4.T3.3.3.3.m1.1.1.2.3.2.cmml" xref="S4.T3.3.3.3.m1.1.1.2.3.2">ğ‘š</ci><ci id="S4.T3.3.3.3.m1.1.1.2.3.3.cmml" xref="S4.T3.3.3.3.m1.1.1.2.3.3">ğ‘</ci><ci id="S4.T3.3.3.3.m1.1.1.2.3.4.cmml" xref="S4.T3.3.3.3.m1.1.1.2.3.4">ğ‘ </ci><ci id="S4.T3.3.3.3.m1.1.1.2.3.5.cmml" xref="S4.T3.3.3.3.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><apply id="S4.T3.3.3.3.m1.1.1.3.cmml" xref="S4.T3.3.3.3.m1.1.1.3"><times id="S4.T3.3.3.3.m1.1.1.3.1.cmml" xref="S4.T3.3.3.3.m1.1.1.3.1"></times><ci id="S4.T3.3.3.3.m1.1.1.3.2.cmml" xref="S4.T3.3.3.3.m1.1.1.3.2">ğ‘›</ci><ci id="S4.T3.3.3.3.m1.1.1.3.3.cmml" xref="S4.T3.3.3.3.m1.1.1.3.3">ğ‘œ</ci><ci id="S4.T3.3.3.3.m1.1.1.3.4.cmml" xref="S4.T3.3.3.3.m1.1.1.3.4">ğ‘£</ci><ci id="S4.T3.3.3.3.m1.1.1.3.5.cmml" xref="S4.T3.3.3.3.m1.1.1.3.5">ğ‘’</ci><ci id="S4.T3.3.3.3.m1.1.1.3.6.cmml" xref="S4.T3.3.3.3.m1.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">{}^{mask}_{novel}</annotation></semantics></math>
</th>
<th id="S4.T3.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">mAP<math id="S4.T3.4.4.4.m1.1" class="ltx_Math" alttext="{}^{mask}_{all}" display="inline"><semantics id="S4.T3.4.4.4.m1.1a"><mmultiscripts id="S4.T3.4.4.4.m1.1.1" xref="S4.T3.4.4.4.m1.1.1.cmml"><mi id="S4.T3.4.4.4.m1.1.1.2.2" xref="S4.T3.4.4.4.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T3.4.4.4.m1.1.1a" xref="S4.T3.4.4.4.m1.1.1.cmml"></mprescripts><mrow id="S4.T3.4.4.4.m1.1.1.3" xref="S4.T3.4.4.4.m1.1.1.3.cmml"><mi id="S4.T3.4.4.4.m1.1.1.3.2" xref="S4.T3.4.4.4.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T3.4.4.4.m1.1.1.3.1" xref="S4.T3.4.4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T3.4.4.4.m1.1.1.3.3" xref="S4.T3.4.4.4.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T3.4.4.4.m1.1.1.3.1a" xref="S4.T3.4.4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T3.4.4.4.m1.1.1.3.4" xref="S4.T3.4.4.4.m1.1.1.3.4.cmml">l</mi></mrow><mrow id="S4.T3.4.4.4.m1.1.1b" xref="S4.T3.4.4.4.m1.1.1.cmml"></mrow><mrow id="S4.T3.4.4.4.m1.1.1c" xref="S4.T3.4.4.4.m1.1.1.cmml"></mrow><mrow id="S4.T3.4.4.4.m1.1.1.2.3" xref="S4.T3.4.4.4.m1.1.1.2.3.cmml"><mi id="S4.T3.4.4.4.m1.1.1.2.3.2" xref="S4.T3.4.4.4.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T3.4.4.4.m1.1.1.2.3.1" xref="S4.T3.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T3.4.4.4.m1.1.1.2.3.3" xref="S4.T3.4.4.4.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T3.4.4.4.m1.1.1.2.3.1a" xref="S4.T3.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T3.4.4.4.m1.1.1.2.3.4" xref="S4.T3.4.4.4.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T3.4.4.4.m1.1.1.2.3.1b" xref="S4.T3.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T3.4.4.4.m1.1.1.2.3.5" xref="S4.T3.4.4.4.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.m1.1b"><apply id="S4.T3.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.4.4.4.m1.1.1.1.cmml" xref="S4.T3.4.4.4.m1.1.1">subscript</csymbol><apply id="S4.T3.4.4.4.m1.1.1.2.cmml" xref="S4.T3.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.4.4.4.m1.1.1.2.1.cmml" xref="S4.T3.4.4.4.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T3.4.4.4.m1.1.1.2.2.cmml" xref="S4.T3.4.4.4.m1.1.1.2.2">absent</csymbol><apply id="S4.T3.4.4.4.m1.1.1.2.3.cmml" xref="S4.T3.4.4.4.m1.1.1.2.3"><times id="S4.T3.4.4.4.m1.1.1.2.3.1.cmml" xref="S4.T3.4.4.4.m1.1.1.2.3.1"></times><ci id="S4.T3.4.4.4.m1.1.1.2.3.2.cmml" xref="S4.T3.4.4.4.m1.1.1.2.3.2">ğ‘š</ci><ci id="S4.T3.4.4.4.m1.1.1.2.3.3.cmml" xref="S4.T3.4.4.4.m1.1.1.2.3.3">ğ‘</ci><ci id="S4.T3.4.4.4.m1.1.1.2.3.4.cmml" xref="S4.T3.4.4.4.m1.1.1.2.3.4">ğ‘ </ci><ci id="S4.T3.4.4.4.m1.1.1.2.3.5.cmml" xref="S4.T3.4.4.4.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><apply id="S4.T3.4.4.4.m1.1.1.3.cmml" xref="S4.T3.4.4.4.m1.1.1.3"><times id="S4.T3.4.4.4.m1.1.1.3.1.cmml" xref="S4.T3.4.4.4.m1.1.1.3.1"></times><ci id="S4.T3.4.4.4.m1.1.1.3.2.cmml" xref="S4.T3.4.4.4.m1.1.1.3.2">ğ‘</ci><ci id="S4.T3.4.4.4.m1.1.1.3.3.cmml" xref="S4.T3.4.4.4.m1.1.1.3.3">ğ‘™</ci><ci id="S4.T3.4.4.4.m1.1.1.3.4.cmml" xref="S4.T3.4.4.4.m1.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.m1.1c">{}^{mask}_{all}</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.6.1" class="ltx_tr">
<td id="S4.T3.4.6.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">Category names</td>
<td id="S4.T3.4.6.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">65</td>
<td id="S4.T3.4.6.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">28.2</td>
<td id="S4.T3.4.6.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">42.8</td>
<td id="S4.T3.4.6.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">1203</td>
<td id="S4.T3.4.6.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">18.9</td>
<td id="S4.T3.4.6.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">31.2</td>
</tr>
<tr id="S4.T3.4.7.2" class="ltx_tr">
<td id="S4.T3.4.7.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">All nouns from caption</td>
<td id="S4.T3.4.7.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">4764</td>
<td id="S4.T3.4.7.2.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:8.0pt;padding-right:8.0pt;">30.0</td>
<td id="S4.T3.4.7.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">44.6</td>
<td id="S4.T3.4.7.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">6750</td>
<td id="S4.T3.4.7.2.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:8.0pt;padding-right:8.0pt;">20.4</td>
<td id="S4.T3.4.7.2.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:8.0pt;padding-right:8.0pt;">30.4</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation study on matching strategy. We implement Hungarian algorithmÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuhn, <a href="#bib.bib30" title="" class="ltx_ref">1955</a>)</cite> for one-to-one region-word matching and Sinkhorn algorithmÂ <cite class="ltx_cite ltx_citemacro_citep">(Cuturi, <a href="#bib.bib11" title="" class="ltx_ref">2013</a>)</cite> for one-to-many matching.</figcaption>
<table id="S4.T4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.4.5.1" class="ltx_tr">
<th id="S4.T4.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;" rowspan="2"><span id="S4.T4.4.5.1.1.1" class="ltx_text">Matching Strategy</span></th>
<th id="S4.T4.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;" colspan="2">OV-COCO</th>
<th id="S4.T4.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;" colspan="2">OV-LVIS</th>
</tr>
<tr id="S4.T4.4.4" class="ltx_tr">
<th id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">mAP<sub id="S4.T4.1.1.1.1" class="ltx_sub"><span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_font_italic">novel</span></sub>
</th>
<th id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">mAP<sub id="S4.T4.2.2.2.1" class="ltx_sub"><span id="S4.T4.2.2.2.1.1" class="ltx_text ltx_font_italic">all</span></sub>
</th>
<th id="S4.T4.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">mAP<math id="S4.T4.3.3.3.m1.1" class="ltx_Math" alttext="{}^{mask}_{novel}" display="inline"><semantics id="S4.T4.3.3.3.m1.1a"><mmultiscripts id="S4.T4.3.3.3.m1.1.1" xref="S4.T4.3.3.3.m1.1.1.cmml"><mi id="S4.T4.3.3.3.m1.1.1.2.2" xref="S4.T4.3.3.3.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T4.3.3.3.m1.1.1a" xref="S4.T4.3.3.3.m1.1.1.cmml"></mprescripts><mrow id="S4.T4.3.3.3.m1.1.1.3" xref="S4.T4.3.3.3.m1.1.1.3.cmml"><mi id="S4.T4.3.3.3.m1.1.1.3.2" xref="S4.T4.3.3.3.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.T4.3.3.3.m1.1.1.3.1" xref="S4.T4.3.3.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T4.3.3.3.m1.1.1.3.3" xref="S4.T4.3.3.3.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.T4.3.3.3.m1.1.1.3.1a" xref="S4.T4.3.3.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T4.3.3.3.m1.1.1.3.4" xref="S4.T4.3.3.3.m1.1.1.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.T4.3.3.3.m1.1.1.3.1b" xref="S4.T4.3.3.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T4.3.3.3.m1.1.1.3.5" xref="S4.T4.3.3.3.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T4.3.3.3.m1.1.1.3.1c" xref="S4.T4.3.3.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T4.3.3.3.m1.1.1.3.6" xref="S4.T4.3.3.3.m1.1.1.3.6.cmml">l</mi></mrow><mrow id="S4.T4.3.3.3.m1.1.1b" xref="S4.T4.3.3.3.m1.1.1.cmml"></mrow><mrow id="S4.T4.3.3.3.m1.1.1c" xref="S4.T4.3.3.3.m1.1.1.cmml"></mrow><mrow id="S4.T4.3.3.3.m1.1.1.2.3" xref="S4.T4.3.3.3.m1.1.1.2.3.cmml"><mi id="S4.T4.3.3.3.m1.1.1.2.3.2" xref="S4.T4.3.3.3.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T4.3.3.3.m1.1.1.2.3.1" xref="S4.T4.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T4.3.3.3.m1.1.1.2.3.3" xref="S4.T4.3.3.3.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T4.3.3.3.m1.1.1.2.3.1a" xref="S4.T4.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T4.3.3.3.m1.1.1.2.3.4" xref="S4.T4.3.3.3.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T4.3.3.3.m1.1.1.2.3.1b" xref="S4.T4.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T4.3.3.3.m1.1.1.2.3.5" xref="S4.T4.3.3.3.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.m1.1b"><apply id="S4.T4.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.3.3.3.m1.1.1.1.cmml" xref="S4.T4.3.3.3.m1.1.1">subscript</csymbol><apply id="S4.T4.3.3.3.m1.1.1.2.cmml" xref="S4.T4.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.3.3.3.m1.1.1.2.1.cmml" xref="S4.T4.3.3.3.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T4.3.3.3.m1.1.1.2.2.cmml" xref="S4.T4.3.3.3.m1.1.1.2.2">absent</csymbol><apply id="S4.T4.3.3.3.m1.1.1.2.3.cmml" xref="S4.T4.3.3.3.m1.1.1.2.3"><times id="S4.T4.3.3.3.m1.1.1.2.3.1.cmml" xref="S4.T4.3.3.3.m1.1.1.2.3.1"></times><ci id="S4.T4.3.3.3.m1.1.1.2.3.2.cmml" xref="S4.T4.3.3.3.m1.1.1.2.3.2">ğ‘š</ci><ci id="S4.T4.3.3.3.m1.1.1.2.3.3.cmml" xref="S4.T4.3.3.3.m1.1.1.2.3.3">ğ‘</ci><ci id="S4.T4.3.3.3.m1.1.1.2.3.4.cmml" xref="S4.T4.3.3.3.m1.1.1.2.3.4">ğ‘ </ci><ci id="S4.T4.3.3.3.m1.1.1.2.3.5.cmml" xref="S4.T4.3.3.3.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><apply id="S4.T4.3.3.3.m1.1.1.3.cmml" xref="S4.T4.3.3.3.m1.1.1.3"><times id="S4.T4.3.3.3.m1.1.1.3.1.cmml" xref="S4.T4.3.3.3.m1.1.1.3.1"></times><ci id="S4.T4.3.3.3.m1.1.1.3.2.cmml" xref="S4.T4.3.3.3.m1.1.1.3.2">ğ‘›</ci><ci id="S4.T4.3.3.3.m1.1.1.3.3.cmml" xref="S4.T4.3.3.3.m1.1.1.3.3">ğ‘œ</ci><ci id="S4.T4.3.3.3.m1.1.1.3.4.cmml" xref="S4.T4.3.3.3.m1.1.1.3.4">ğ‘£</ci><ci id="S4.T4.3.3.3.m1.1.1.3.5.cmml" xref="S4.T4.3.3.3.m1.1.1.3.5">ğ‘’</ci><ci id="S4.T4.3.3.3.m1.1.1.3.6.cmml" xref="S4.T4.3.3.3.m1.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.m1.1c">{}^{mask}_{novel}</annotation></semantics></math>
</th>
<th id="S4.T4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">mAP<math id="S4.T4.4.4.4.m1.1" class="ltx_Math" alttext="{}^{mask}_{all}" display="inline"><semantics id="S4.T4.4.4.4.m1.1a"><mmultiscripts id="S4.T4.4.4.4.m1.1.1" xref="S4.T4.4.4.4.m1.1.1.cmml"><mi id="S4.T4.4.4.4.m1.1.1.2.2" xref="S4.T4.4.4.4.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T4.4.4.4.m1.1.1a" xref="S4.T4.4.4.4.m1.1.1.cmml"></mprescripts><mrow id="S4.T4.4.4.4.m1.1.1.3" xref="S4.T4.4.4.4.m1.1.1.3.cmml"><mi id="S4.T4.4.4.4.m1.1.1.3.2" xref="S4.T4.4.4.4.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T4.4.4.4.m1.1.1.3.1" xref="S4.T4.4.4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T4.4.4.4.m1.1.1.3.3" xref="S4.T4.4.4.4.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T4.4.4.4.m1.1.1.3.1a" xref="S4.T4.4.4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T4.4.4.4.m1.1.1.3.4" xref="S4.T4.4.4.4.m1.1.1.3.4.cmml">l</mi></mrow><mrow id="S4.T4.4.4.4.m1.1.1b" xref="S4.T4.4.4.4.m1.1.1.cmml"></mrow><mrow id="S4.T4.4.4.4.m1.1.1c" xref="S4.T4.4.4.4.m1.1.1.cmml"></mrow><mrow id="S4.T4.4.4.4.m1.1.1.2.3" xref="S4.T4.4.4.4.m1.1.1.2.3.cmml"><mi id="S4.T4.4.4.4.m1.1.1.2.3.2" xref="S4.T4.4.4.4.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T4.4.4.4.m1.1.1.2.3.1" xref="S4.T4.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T4.4.4.4.m1.1.1.2.3.3" xref="S4.T4.4.4.4.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T4.4.4.4.m1.1.1.2.3.1a" xref="S4.T4.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T4.4.4.4.m1.1.1.2.3.4" xref="S4.T4.4.4.4.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T4.4.4.4.m1.1.1.2.3.1b" xref="S4.T4.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T4.4.4.4.m1.1.1.2.3.5" xref="S4.T4.4.4.4.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.m1.1b"><apply id="S4.T4.4.4.4.m1.1.1.cmml" xref="S4.T4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.4.4.4.m1.1.1.1.cmml" xref="S4.T4.4.4.4.m1.1.1">subscript</csymbol><apply id="S4.T4.4.4.4.m1.1.1.2.cmml" xref="S4.T4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.4.4.4.m1.1.1.2.1.cmml" xref="S4.T4.4.4.4.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T4.4.4.4.m1.1.1.2.2.cmml" xref="S4.T4.4.4.4.m1.1.1.2.2">absent</csymbol><apply id="S4.T4.4.4.4.m1.1.1.2.3.cmml" xref="S4.T4.4.4.4.m1.1.1.2.3"><times id="S4.T4.4.4.4.m1.1.1.2.3.1.cmml" xref="S4.T4.4.4.4.m1.1.1.2.3.1"></times><ci id="S4.T4.4.4.4.m1.1.1.2.3.2.cmml" xref="S4.T4.4.4.4.m1.1.1.2.3.2">ğ‘š</ci><ci id="S4.T4.4.4.4.m1.1.1.2.3.3.cmml" xref="S4.T4.4.4.4.m1.1.1.2.3.3">ğ‘</ci><ci id="S4.T4.4.4.4.m1.1.1.2.3.4.cmml" xref="S4.T4.4.4.4.m1.1.1.2.3.4">ğ‘ </ci><ci id="S4.T4.4.4.4.m1.1.1.2.3.5.cmml" xref="S4.T4.4.4.4.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><apply id="S4.T4.4.4.4.m1.1.1.3.cmml" xref="S4.T4.4.4.4.m1.1.1.3"><times id="S4.T4.4.4.4.m1.1.1.3.1.cmml" xref="S4.T4.4.4.4.m1.1.1.3.1"></times><ci id="S4.T4.4.4.4.m1.1.1.3.2.cmml" xref="S4.T4.4.4.4.m1.1.1.3.2">ğ‘</ci><ci id="S4.T4.4.4.4.m1.1.1.3.3.cmml" xref="S4.T4.4.4.4.m1.1.1.3.3">ğ‘™</ci><ci id="S4.T4.4.4.4.m1.1.1.3.4.cmml" xref="S4.T4.4.4.4.m1.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.m1.1c">{}^{mask}_{all}</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.4.6.1" class="ltx_tr">
<th id="S4.T4.4.6.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">One-to-Many</th>
<td id="S4.T4.4.6.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">29.1</td>
<td id="S4.T4.4.6.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">44.6</td>
<td id="S4.T4.4.6.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">18.5</td>
<td id="S4.T4.4.6.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">29.2</td>
</tr>
<tr id="S4.T4.4.7.2" class="ltx_tr">
<th id="S4.T4.4.7.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">One-to-One</th>
<td id="S4.T4.4.7.2.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:12.0pt;padding-right:12.0pt;">32.0</td>
<td id="S4.T4.4.7.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">45.8</td>
<td id="S4.T4.4.7.2.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:12.0pt;padding-right:12.0pt;">21.7</td>
<td id="S4.T4.4.7.2.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:12.0pt;padding-right:12.0pt;">30.1</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2211.14843/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="128" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>We show a comparison of different alignment strategies in terms of novel-class mAP on both open-vocabulary COCO and LVIS. Different from the bipartite matching that learns the alignment from the global perspective, â€œmax-scoreâ€Â <cite class="ltx_cite ltx_citemacro_citep">(Redmon &amp; Farhadi, <a href="#bib.bib41" title="" class="ltx_ref">2017</a>; Yao etÂ al., <a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite> does the alignment based on the highest similarity score and â€œmax-sizeâ€Â <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite> assigns the max size proposal to the image labels.</figcaption>
</figure>
<div id="S4.SS5.p4" class="ltx_para ltx_noindent">
<p id="S4.SS5.p4.1" class="ltx_p"><span id="S4.SS5.p4.1.1" class="ltx_text ltx_font_bold">Image-Text Alignment.</span>
We conduct ablation experiments to evaluate the effectiveness of different training losses of our method in TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.5 Ablation Studies â€£ 4 Experiments â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
We observe that using the region-word alignment loss alone achieves promising results.
It demonstrates the effectiveness of the bipartite matching strategy for learning region-word alignment.
Furthermore, we can find that using image-text pair as additional supervision can improve the performance.
Intuitively, the image-text pairs data can provide contextual information which complements semantics beyond nouns.
A similar conclusion can be found inÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>; Zhong etÂ al., <a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation study on training losses. It performs best when both the region-word alignment loss and the image-text alignment loss are used, suggesting that they benefit from each other.</figcaption>
<table id="S4.T5.6.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.2.2.2" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;" rowspan="2"><span id="S4.T5.1.1.1.1.1" class="ltx_text"><math id="S4.T5.1.1.1.1.1.m1.1" class="ltx_Math" alttext="L_{region-word}" display="inline"><semantics id="S4.T5.1.1.1.1.1.m1.1a"><msub id="S4.T5.1.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T5.1.1.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.1.1.m1.1.1.2.cmml">L</mi><mrow id="S4.T5.1.1.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.1.1.m1.1.1.3.cmml"><mrow id="S4.T5.1.1.1.1.1.m1.1.1.3.2" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.cmml"><mi id="S4.T5.1.1.1.1.1.m1.1.1.3.2.2" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.1.1.m1.1.1.3.2.1" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.T5.1.1.1.1.1.m1.1.1.3.2.3" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.1.1.m1.1.1.3.2.1a" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.T5.1.1.1.1.1.m1.1.1.3.2.4" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.1.1.m1.1.1.3.2.1b" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.T5.1.1.1.1.1.m1.1.1.3.2.5" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.1.1.m1.1.1.3.2.1c" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.T5.1.1.1.1.1.m1.1.1.3.2.6" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.1.1.m1.1.1.3.2.1d" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.T5.1.1.1.1.1.m1.1.1.3.2.7" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.7.cmml">n</mi></mrow><mo id="S4.T5.1.1.1.1.1.m1.1.1.3.1" xref="S4.T5.1.1.1.1.1.m1.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.T5.1.1.1.1.1.m1.1.1.3.3" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.cmml"><mi id="S4.T5.1.1.1.1.1.m1.1.1.3.3.2" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.1.1.m1.1.1.3.3.1" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.T5.1.1.1.1.1.m1.1.1.3.3.3" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.1.1.m1.1.1.3.3.1a" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.T5.1.1.1.1.1.m1.1.1.3.3.4" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.1.1.m1.1.1.3.3.1b" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.T5.1.1.1.1.1.m1.1.1.3.3.5" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.5.cmml">d</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T5.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.2">ğ¿</ci><apply id="S4.T5.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3"><minus id="S4.T5.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.1"></minus><apply id="S4.T5.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2"><times id="S4.T5.1.1.1.1.1.m1.1.1.3.2.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.1"></times><ci id="S4.T5.1.1.1.1.1.m1.1.1.3.2.2.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.2">ğ‘Ÿ</ci><ci id="S4.T5.1.1.1.1.1.m1.1.1.3.2.3.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.3">ğ‘’</ci><ci id="S4.T5.1.1.1.1.1.m1.1.1.3.2.4.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.4">ğ‘”</ci><ci id="S4.T5.1.1.1.1.1.m1.1.1.3.2.5.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.5">ğ‘–</ci><ci id="S4.T5.1.1.1.1.1.m1.1.1.3.2.6.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.6">ğ‘œ</ci><ci id="S4.T5.1.1.1.1.1.m1.1.1.3.2.7.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.2.7">ğ‘›</ci></apply><apply id="S4.T5.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3"><times id="S4.T5.1.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.1"></times><ci id="S4.T5.1.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.2">ğ‘¤</ci><ci id="S4.T5.1.1.1.1.1.m1.1.1.3.3.3.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.3">ğ‘œ</ci><ci id="S4.T5.1.1.1.1.1.m1.1.1.3.3.4.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.4">ğ‘Ÿ</ci><ci id="S4.T5.1.1.1.1.1.m1.1.1.3.3.5.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3.3.5">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.1.m1.1c">L_{region-word}</annotation></semantics></math></span></th>
<th id="S4.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;" rowspan="2"><span id="S4.T5.2.2.2.2.1" class="ltx_text"><math id="S4.T5.2.2.2.2.1.m1.1" class="ltx_Math" alttext="L_{image-text}" display="inline"><semantics id="S4.T5.2.2.2.2.1.m1.1a"><msub id="S4.T5.2.2.2.2.1.m1.1.1" xref="S4.T5.2.2.2.2.1.m1.1.1.cmml"><mi id="S4.T5.2.2.2.2.1.m1.1.1.2" xref="S4.T5.2.2.2.2.1.m1.1.1.2.cmml">L</mi><mrow id="S4.T5.2.2.2.2.1.m1.1.1.3" xref="S4.T5.2.2.2.2.1.m1.1.1.3.cmml"><mrow id="S4.T5.2.2.2.2.1.m1.1.1.3.2" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.cmml"><mi id="S4.T5.2.2.2.2.1.m1.1.1.3.2.2" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.2.2.2.1.m1.1.1.3.2.1" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.T5.2.2.2.2.1.m1.1.1.3.2.3" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.2.2.2.1.m1.1.1.3.2.1a" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.T5.2.2.2.2.1.m1.1.1.3.2.4" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.2.2.2.1.m1.1.1.3.2.1b" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.T5.2.2.2.2.1.m1.1.1.3.2.5" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.2.2.2.1.m1.1.1.3.2.1c" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.T5.2.2.2.2.1.m1.1.1.3.2.6" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.6.cmml">e</mi></mrow><mo id="S4.T5.2.2.2.2.1.m1.1.1.3.1" xref="S4.T5.2.2.2.2.1.m1.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.T5.2.2.2.2.1.m1.1.1.3.3" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.cmml"><mi id="S4.T5.2.2.2.2.1.m1.1.1.3.3.2" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.2.2.2.1.m1.1.1.3.3.1" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.T5.2.2.2.2.1.m1.1.1.3.3.3" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.2.2.2.1.m1.1.1.3.3.1a" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.T5.2.2.2.2.1.m1.1.1.3.3.4" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.2.2.2.1.m1.1.1.3.3.1b" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.T5.2.2.2.2.1.m1.1.1.3.3.5" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.5.cmml">t</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.1.m1.1b"><apply id="S4.T5.2.2.2.2.1.m1.1.1.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T5.2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.2">ğ¿</ci><apply id="S4.T5.2.2.2.2.1.m1.1.1.3.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3"><minus id="S4.T5.2.2.2.2.1.m1.1.1.3.1.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.1"></minus><apply id="S4.T5.2.2.2.2.1.m1.1.1.3.2.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2"><times id="S4.T5.2.2.2.2.1.m1.1.1.3.2.1.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.1"></times><ci id="S4.T5.2.2.2.2.1.m1.1.1.3.2.2.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.2">ğ‘–</ci><ci id="S4.T5.2.2.2.2.1.m1.1.1.3.2.3.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.3">ğ‘š</ci><ci id="S4.T5.2.2.2.2.1.m1.1.1.3.2.4.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.4">ğ‘</ci><ci id="S4.T5.2.2.2.2.1.m1.1.1.3.2.5.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.5">ğ‘”</ci><ci id="S4.T5.2.2.2.2.1.m1.1.1.3.2.6.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.2.6">ğ‘’</ci></apply><apply id="S4.T5.2.2.2.2.1.m1.1.1.3.3.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3"><times id="S4.T5.2.2.2.2.1.m1.1.1.3.3.1.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.1"></times><ci id="S4.T5.2.2.2.2.1.m1.1.1.3.3.2.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.2">ğ‘¡</ci><ci id="S4.T5.2.2.2.2.1.m1.1.1.3.3.3.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.3">ğ‘’</ci><ci id="S4.T5.2.2.2.2.1.m1.1.1.3.3.4.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.4">ğ‘¥</ci><ci id="S4.T5.2.2.2.2.1.m1.1.1.3.3.5.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3.3.5">ğ‘¡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.1.m1.1c">L_{image-text}</annotation></semantics></math></span></th>
<th id="S4.T5.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;" colspan="2">OV-COCO</th>
<th id="S4.T5.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;" colspan="2">OV-LVIS</th>
</tr>
<tr id="S4.T5.6.6.6" class="ltx_tr">
<th id="S4.T5.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">mAP<sub id="S4.T5.3.3.3.1.1" class="ltx_sub"><span id="S4.T5.3.3.3.1.1.1" class="ltx_text ltx_font_italic">novel</span></sub>
</th>
<th id="S4.T5.4.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">mAP<sub id="S4.T5.4.4.4.2.1" class="ltx_sub"><span id="S4.T5.4.4.4.2.1.1" class="ltx_text ltx_font_italic">all</span></sub>
</th>
<th id="S4.T5.5.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">mAP<math id="S4.T5.5.5.5.3.m1.1" class="ltx_Math" alttext="{}^{mask}_{novel}" display="inline"><semantics id="S4.T5.5.5.5.3.m1.1a"><mmultiscripts id="S4.T5.5.5.5.3.m1.1.1" xref="S4.T5.5.5.5.3.m1.1.1.cmml"><mi id="S4.T5.5.5.5.3.m1.1.1.2.2" xref="S4.T5.5.5.5.3.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T5.5.5.5.3.m1.1.1a" xref="S4.T5.5.5.5.3.m1.1.1.cmml"></mprescripts><mrow id="S4.T5.5.5.5.3.m1.1.1.3" xref="S4.T5.5.5.5.3.m1.1.1.3.cmml"><mi id="S4.T5.5.5.5.3.m1.1.1.3.2" xref="S4.T5.5.5.5.3.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.T5.5.5.5.3.m1.1.1.3.1" xref="S4.T5.5.5.5.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T5.5.5.5.3.m1.1.1.3.3" xref="S4.T5.5.5.5.3.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.T5.5.5.5.3.m1.1.1.3.1a" xref="S4.T5.5.5.5.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T5.5.5.5.3.m1.1.1.3.4" xref="S4.T5.5.5.5.3.m1.1.1.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.T5.5.5.5.3.m1.1.1.3.1b" xref="S4.T5.5.5.5.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T5.5.5.5.3.m1.1.1.3.5" xref="S4.T5.5.5.5.3.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T5.5.5.5.3.m1.1.1.3.1c" xref="S4.T5.5.5.5.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T5.5.5.5.3.m1.1.1.3.6" xref="S4.T5.5.5.5.3.m1.1.1.3.6.cmml">l</mi></mrow><mrow id="S4.T5.5.5.5.3.m1.1.1b" xref="S4.T5.5.5.5.3.m1.1.1.cmml"></mrow><mrow id="S4.T5.5.5.5.3.m1.1.1c" xref="S4.T5.5.5.5.3.m1.1.1.cmml"></mrow><mrow id="S4.T5.5.5.5.3.m1.1.1.2.3" xref="S4.T5.5.5.5.3.m1.1.1.2.3.cmml"><mi id="S4.T5.5.5.5.3.m1.1.1.2.3.2" xref="S4.T5.5.5.5.3.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T5.5.5.5.3.m1.1.1.2.3.1" xref="S4.T5.5.5.5.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T5.5.5.5.3.m1.1.1.2.3.3" xref="S4.T5.5.5.5.3.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T5.5.5.5.3.m1.1.1.2.3.1a" xref="S4.T5.5.5.5.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T5.5.5.5.3.m1.1.1.2.3.4" xref="S4.T5.5.5.5.3.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T5.5.5.5.3.m1.1.1.2.3.1b" xref="S4.T5.5.5.5.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T5.5.5.5.3.m1.1.1.2.3.5" xref="S4.T5.5.5.5.3.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.5.3.m1.1b"><apply id="S4.T5.5.5.5.3.m1.1.1.cmml" xref="S4.T5.5.5.5.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.5.5.5.3.m1.1.1.1.cmml" xref="S4.T5.5.5.5.3.m1.1.1">subscript</csymbol><apply id="S4.T5.5.5.5.3.m1.1.1.2.cmml" xref="S4.T5.5.5.5.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.5.5.5.3.m1.1.1.2.1.cmml" xref="S4.T5.5.5.5.3.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T5.5.5.5.3.m1.1.1.2.2.cmml" xref="S4.T5.5.5.5.3.m1.1.1.2.2">absent</csymbol><apply id="S4.T5.5.5.5.3.m1.1.1.2.3.cmml" xref="S4.T5.5.5.5.3.m1.1.1.2.3"><times id="S4.T5.5.5.5.3.m1.1.1.2.3.1.cmml" xref="S4.T5.5.5.5.3.m1.1.1.2.3.1"></times><ci id="S4.T5.5.5.5.3.m1.1.1.2.3.2.cmml" xref="S4.T5.5.5.5.3.m1.1.1.2.3.2">ğ‘š</ci><ci id="S4.T5.5.5.5.3.m1.1.1.2.3.3.cmml" xref="S4.T5.5.5.5.3.m1.1.1.2.3.3">ğ‘</ci><ci id="S4.T5.5.5.5.3.m1.1.1.2.3.4.cmml" xref="S4.T5.5.5.5.3.m1.1.1.2.3.4">ğ‘ </ci><ci id="S4.T5.5.5.5.3.m1.1.1.2.3.5.cmml" xref="S4.T5.5.5.5.3.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><apply id="S4.T5.5.5.5.3.m1.1.1.3.cmml" xref="S4.T5.5.5.5.3.m1.1.1.3"><times id="S4.T5.5.5.5.3.m1.1.1.3.1.cmml" xref="S4.T5.5.5.5.3.m1.1.1.3.1"></times><ci id="S4.T5.5.5.5.3.m1.1.1.3.2.cmml" xref="S4.T5.5.5.5.3.m1.1.1.3.2">ğ‘›</ci><ci id="S4.T5.5.5.5.3.m1.1.1.3.3.cmml" xref="S4.T5.5.5.5.3.m1.1.1.3.3">ğ‘œ</ci><ci id="S4.T5.5.5.5.3.m1.1.1.3.4.cmml" xref="S4.T5.5.5.5.3.m1.1.1.3.4">ğ‘£</ci><ci id="S4.T5.5.5.5.3.m1.1.1.3.5.cmml" xref="S4.T5.5.5.5.3.m1.1.1.3.5">ğ‘’</ci><ci id="S4.T5.5.5.5.3.m1.1.1.3.6.cmml" xref="S4.T5.5.5.5.3.m1.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.5.3.m1.1c">{}^{mask}_{novel}</annotation></semantics></math>
</th>
<th id="S4.T5.6.6.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">mAP<math id="S4.T5.6.6.6.4.m1.1" class="ltx_Math" alttext="{}^{mask}_{all}" display="inline"><semantics id="S4.T5.6.6.6.4.m1.1a"><mmultiscripts id="S4.T5.6.6.6.4.m1.1.1" xref="S4.T5.6.6.6.4.m1.1.1.cmml"><mi id="S4.T5.6.6.6.4.m1.1.1.2.2" xref="S4.T5.6.6.6.4.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T5.6.6.6.4.m1.1.1a" xref="S4.T5.6.6.6.4.m1.1.1.cmml"></mprescripts><mrow id="S4.T5.6.6.6.4.m1.1.1.3" xref="S4.T5.6.6.6.4.m1.1.1.3.cmml"><mi id="S4.T5.6.6.6.4.m1.1.1.3.2" xref="S4.T5.6.6.6.4.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T5.6.6.6.4.m1.1.1.3.1" xref="S4.T5.6.6.6.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T5.6.6.6.4.m1.1.1.3.3" xref="S4.T5.6.6.6.4.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T5.6.6.6.4.m1.1.1.3.1a" xref="S4.T5.6.6.6.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T5.6.6.6.4.m1.1.1.3.4" xref="S4.T5.6.6.6.4.m1.1.1.3.4.cmml">l</mi></mrow><mrow id="S4.T5.6.6.6.4.m1.1.1b" xref="S4.T5.6.6.6.4.m1.1.1.cmml"></mrow><mrow id="S4.T5.6.6.6.4.m1.1.1c" xref="S4.T5.6.6.6.4.m1.1.1.cmml"></mrow><mrow id="S4.T5.6.6.6.4.m1.1.1.2.3" xref="S4.T5.6.6.6.4.m1.1.1.2.3.cmml"><mi id="S4.T5.6.6.6.4.m1.1.1.2.3.2" xref="S4.T5.6.6.6.4.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T5.6.6.6.4.m1.1.1.2.3.1" xref="S4.T5.6.6.6.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T5.6.6.6.4.m1.1.1.2.3.3" xref="S4.T5.6.6.6.4.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T5.6.6.6.4.m1.1.1.2.3.1a" xref="S4.T5.6.6.6.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T5.6.6.6.4.m1.1.1.2.3.4" xref="S4.T5.6.6.6.4.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T5.6.6.6.4.m1.1.1.2.3.1b" xref="S4.T5.6.6.6.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.T5.6.6.6.4.m1.1.1.2.3.5" xref="S4.T5.6.6.6.4.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.6.4.m1.1b"><apply id="S4.T5.6.6.6.4.m1.1.1.cmml" xref="S4.T5.6.6.6.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.6.6.6.4.m1.1.1.1.cmml" xref="S4.T5.6.6.6.4.m1.1.1">subscript</csymbol><apply id="S4.T5.6.6.6.4.m1.1.1.2.cmml" xref="S4.T5.6.6.6.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.6.6.6.4.m1.1.1.2.1.cmml" xref="S4.T5.6.6.6.4.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T5.6.6.6.4.m1.1.1.2.2.cmml" xref="S4.T5.6.6.6.4.m1.1.1.2.2">absent</csymbol><apply id="S4.T5.6.6.6.4.m1.1.1.2.3.cmml" xref="S4.T5.6.6.6.4.m1.1.1.2.3"><times id="S4.T5.6.6.6.4.m1.1.1.2.3.1.cmml" xref="S4.T5.6.6.6.4.m1.1.1.2.3.1"></times><ci id="S4.T5.6.6.6.4.m1.1.1.2.3.2.cmml" xref="S4.T5.6.6.6.4.m1.1.1.2.3.2">ğ‘š</ci><ci id="S4.T5.6.6.6.4.m1.1.1.2.3.3.cmml" xref="S4.T5.6.6.6.4.m1.1.1.2.3.3">ğ‘</ci><ci id="S4.T5.6.6.6.4.m1.1.1.2.3.4.cmml" xref="S4.T5.6.6.6.4.m1.1.1.2.3.4">ğ‘ </ci><ci id="S4.T5.6.6.6.4.m1.1.1.2.3.5.cmml" xref="S4.T5.6.6.6.4.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><apply id="S4.T5.6.6.6.4.m1.1.1.3.cmml" xref="S4.T5.6.6.6.4.m1.1.1.3"><times id="S4.T5.6.6.6.4.m1.1.1.3.1.cmml" xref="S4.T5.6.6.6.4.m1.1.1.3.1"></times><ci id="S4.T5.6.6.6.4.m1.1.1.3.2.cmml" xref="S4.T5.6.6.6.4.m1.1.1.3.2">ğ‘</ci><ci id="S4.T5.6.6.6.4.m1.1.1.3.3.cmml" xref="S4.T5.6.6.6.4.m1.1.1.3.3">ğ‘™</ci><ci id="S4.T5.6.6.6.4.m1.1.1.3.4.cmml" xref="S4.T5.6.6.6.4.m1.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.6.4.m1.1c">{}^{mask}_{all}</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.6.6.7.1" class="ltx_tr">
<th id="S4.T5.6.6.7.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">âœ“</th>
<td id="S4.T5.6.6.7.1.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;"></td>
<th id="S4.T5.6.6.7.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">30.0</th>
<td id="S4.T5.6.6.7.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">44.6</td>
<th id="S4.T5.6.6.7.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">20.4</th>
<td id="S4.T5.6.6.7.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">30.4</td>
</tr>
<tr id="S4.T5.6.6.8.2" class="ltx_tr">
<th id="S4.T5.6.6.8.2.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:11.4pt;padding-right:11.4pt;"></th>
<td id="S4.T5.6.6.8.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">âœ“</td>
<th id="S4.T5.6.6.8.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:11.4pt;padding-right:11.4pt;">21.0</th>
<td id="S4.T5.6.6.8.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">43.8</td>
<th id="S4.T5.6.6.8.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:11.4pt;padding-right:11.4pt;">17.4</th>
<td id="S4.T5.6.6.8.2.6" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">30.4</td>
</tr>
<tr id="S4.T5.6.6.9.3" class="ltx_tr">
<th id="S4.T5.6.6.9.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding-left:11.4pt;padding-right:11.4pt;">âœ“</th>
<td id="S4.T5.6.6.9.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">âœ“</td>
<th id="S4.T5.6.6.9.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding-left:11.4pt;padding-right:11.4pt;">32.0</th>
<td id="S4.T5.6.6.9.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">45.8</td>
<th id="S4.T5.6.6.9.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding-left:11.4pt;padding-right:11.4pt;">21.7</th>
<td id="S4.T5.6.6.9.3.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:11.4pt;padding-right:11.4pt;">30.1</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T6" class="ltx_table ltx_align_floatright">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Transfer to other datasets. We evaluated COCO-trained model on PASCAL VOCÂ <cite class="ltx_cite ltx_citemacro_citep">(Everingham etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2010</a>)</cite> test set and LVIS validation set without re-training. All results are box AP<sub id="S4.T6.4.1" class="ltx_sub"><span id="S4.T6.4.1.1" class="ltx_text ltx_font_italic">50</span></sub>.</figcaption>
<table id="S4.T6.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.5.1.1" class="ltx_tr">
<th id="S4.T6.5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">Method</th>
<th id="S4.T6.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">PASCAL VOC</th>
<th id="S4.T6.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">LVIS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.5.2.1" class="ltx_tr">
<td id="S4.T6.5.2.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">OVRCNN</td>
<td id="S4.T6.5.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">52.9</td>
<td id="S4.T6.5.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">5.2</td>
</tr>
<tr id="S4.T6.5.3.2" class="ltx_tr">
<td id="S4.T6.5.3.2.1" class="ltx_td ltx_align_center" style="padding-left:9.0pt;padding-right:9.0pt;">PB-OVD</td>
<td id="S4.T6.5.3.2.2" class="ltx_td ltx_align_center" style="padding-left:9.0pt;padding-right:9.0pt;">59.2</td>
<td id="S4.T6.5.3.2.3" class="ltx_td ltx_align_center" style="padding-left:9.0pt;padding-right:9.0pt;">8.0</td>
</tr>
<tr id="S4.T6.5.4.3" class="ltx_tr">
<td id="S4.T6.5.4.3.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:9.0pt;padding-right:9.0pt;">Our</td>
<td id="S4.T6.5.4.3.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:9.0pt;padding-right:9.0pt;">61.7</td>
<td id="S4.T6.5.4.3.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:9.0pt;padding-right:9.0pt;">10.0</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Transfer to Other Datasets.</h3>

<div id="S4.SS6.p1" class="ltx_para ltx_noindent">
<p id="S4.SS6.p1.1" class="ltx_p">To evaluate the generalization ability of our model, we conduct the experiments on transferring COCO-trained model to PASCAL VOCÂ <cite class="ltx_cite ltx_citemacro_citep">(Everingham etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2010</a>)</cite> test set and LVIS validation set without re-training.
We directly use the model trained with COCO Caption data for OV-COCO and replace the class embeddings of the classifier head.
PASCAL VOC is a widely used object detection dataset that contains 20 object categories, where 9 of them are unseen classes in COCO.
Directly transferring models without using any training images is challenging due to the domain gap.
Whatâ€™s more, LVIS includes 1203 object categories which is much larger than COCO label space.
As shown in Table <a href="#S4.T6" title="Table 6 â€£ 4.5 Ablation Studies â€£ 4 Experiments â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, our model achieves 2.5 % and 2.0% improvement on VOC test set and LVIS validation set, demonstrating the effectiveness of our method on diverse image domains and language vocabularies.
While LVIS includes category names that do not appear in COCO Caption concepts, our model can learn close concepts during training, which facilitates the transfer to LVIS.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and Limitations</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">We have presented VLDet which aims to learn the annotation-free object-language alignment from the image-caption pairs for open-vocabulary object detection.
Our key idea is to extract the region-word pairs from a global perspective using bipartite matching by converting images into regions and text into words.
We only use a single object detection model supervised by the free image-text pairs without any object-language annotations for novel classes.
The extensive experiments show that our detector VLDet achieves a new state-of-the-art performance on both open-vocabulary COCO and LVIS.
We hope this work can push the direction of OVOD and inspire more work on large-scale free image-text pair data.
The potential limitations of VLDet are: 1) it does not consider the bias of vision-and-language data; 2) we have not investigated even larger data volumes, e.g.Â Conceptual Captions 12MÂ <cite class="ltx_cite ltx_citemacro_citep">(Changpinyo etÂ al., <a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>.
We leave them for future works.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal etÂ al. (2018)</span>
<span class="ltx_bibblock">
Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, and Ajay Divakaran.

</span>
<span class="ltx_bibblock">Zero-shot object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</em>, pp.Â  384â€“400, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bilen &amp; Vedaldi (2016)</span>
<span class="ltx_bibblock">
Hakan Bilen and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">Weakly supervised deep detection networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pp.Â  2846â€“2854, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bilen etÂ al. (2015)</span>
<span class="ltx_bibblock">
Hakan Bilen, Marco Pedersoli, and Tinne Tuytelaars.

</span>
<span class="ltx_bibblock">Weakly supervised object detection with convex clustering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pp.Â  1081â€“1089, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai &amp; Vasconcelos (2018)</span>
<span class="ltx_bibblock">
Zhaowei Cai and Nuno Vasconcelos.

</span>
<span class="ltx_bibblock">Cascade r-cnn: Delving into high quality object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pp.Â  6154â€“6162, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zhaowei Cai, Gukyeong Kwon, Avinash Ravichandran, Erhan Bas, Zhuowen Tu, Rahul
Bhotika, and Stefano Soatto.

</span>
<span class="ltx_bibblock">X-detr: A versatile architecture for instance-wise vision-language
tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.05626</em>, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion etÂ al. (2020)</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pp.Â  213â€“229.
Springer, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo etÂ al. (2021)</span>
<span class="ltx_bibblock">
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.

</span>
<span class="ltx_bibblock">Conceptual 12M: Pushing web-scale image-text pre-training to
recognize long-tail visual concepts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2015)</span>
<span class="ltx_bibblock">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
DollÃ¡r, and CÂ Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1504.00325</em>, 2015.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed ElÂ Kholy, Faisal Ahmed, Zhe Gan,
YuÂ Cheng, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pp.Â  104â€“120.
Springer, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cinbis etÂ al. (2016)</span>
<span class="ltx_bibblock">
RamazanÂ Gokberk Cinbis, Jakob Verbeek, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Weakly supervised object localization with multi-fold multiple
instance learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, 39(1):189â€“203, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuturi (2013)</span>
<span class="ltx_bibblock">
Marco Cuturi.

</span>
<span class="ltx_bibblock">Sinkhorn distances: Lightspeed computation of optimal transport.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 26, 2013.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dietterich etÂ al. (1997)</span>
<span class="ltx_bibblock">
ThomasÂ G Dietterich, RichardÂ H Lathrop, and TomÃ¡s Lozano-PÃ©rez.

</span>
<span class="ltx_bibblock">Solving the multiple instance problem with axis-parallel rectangles.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence</em>, 89(1-2):31â€“71,
1997.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du etÂ al. (2022)</span>
<span class="ltx_bibblock">
YuÂ Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li.

</span>
<span class="ltx_bibblock">Learning to prompt for open-vocabulary object detection with
vision-language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.14940</em>, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everingham etÂ al. (2010)</span>
<span class="ltx_bibblock">
Mark Everingham, Luc VanÂ Gool, ChristopherÂ KI Williams, John Winn, and Andrew
Zisserman.

</span>
<span class="ltx_bibblock">The pascal visual object classes (voc) challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 88(2):303â€“338, 2010.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Yuxin Fang, Shusheng Yang, Xinggang Wang, YuÂ Li, Chen Fang, Ying Shan, Bin
Feng, and Wenyu Liu.

</span>
<span class="ltx_bibblock">Instances as queries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pp.Â  6910â€“6919, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Mingfei Gao, Chen Xing, JuanÂ Carlos Niebles, Junnan Li, Ran Xu, Wenhao Liu, and
Caiming Xiong.

</span>
<span class="ltx_bibblock">Towards open vocabulary object detection without human-provided
bounding boxes.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.09452</em>, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiger etÂ al. (2013)</span>
<span class="ltx_bibblock">
Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun.

</span>
<span class="ltx_bibblock">Vision meets robotics: The kitti dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">The International Journal of Robotics Research</em>, 32(11):1231â€“1237, 2013.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghiasi etÂ al. (2021)</span>
<span class="ltx_bibblock">
Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, EkinÂ D Cubuk,
QuocÂ V Le, and Barret Zoph.

</span>
<span class="ltx_bibblock">Simple copy-paste is a strong data augmentation method for instance
segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pp.Â  2918â€“2928, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghiasi etÂ al. (2022)</span>
<span class="ltx_bibblock">
Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin.

</span>
<span class="ltx_bibblock">Scaling open-vocabulary image segmentation with image-level labels.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pp.Â  540â€“557.
Springer, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girshick (2015)</span>
<span class="ltx_bibblock">
Ross Girshick.

</span>
<span class="ltx_bibblock">Fast r-cnn.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, pp.Â  1440â€“1448, 2015.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.

</span>
<span class="ltx_bibblock">Open-vocabulary object detection via vision and language knowledge
distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.13921</em>, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta etÂ al. (2019)</span>
<span class="ltx_bibblock">
Agrim Gupta, Piotr Dollar, and Ross Girshick.

</span>
<span class="ltx_bibblock">Lvis: A dataset for large vocabulary instance segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pp.Â  5356â€“5364, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pp.Â  770â€“778, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2017)</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, pp.Â  2961â€“2969, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huynh etÂ al. (2021)</span>
<span class="ltx_bibblock">
Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan Elhamifar.

</span>
<span class="ltx_bibblock">Open-vocabulary instance segmentation via robust cross-modal
pseudo-labeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.12698</em>, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia etÂ al. (2021)</span>
<span class="ltx_bibblock">
Chao Jia, Yinfei Yang, YeÂ Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig.

</span>
<span class="ltx_bibblock">Scaling up visual and vision-language representation learning with
noisy text supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp.Â 4904â€“4916. PMLR, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson-Roberson etÂ al. (2016)</span>
<span class="ltx_bibblock">
Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, SharathÂ Nittur Sridhar,
Karl Rosaen, and Ram Vasudevan.

</span>
<span class="ltx_bibblock">Driving in the matrix: Can virtual worlds replace human-generated
annotations for real world tasks?

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.01983</em>, 2016.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamath etÂ al. (2021)</span>
<span class="ltx_bibblock">
Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and
Nicolas Carion.

</span>
<span class="ltx_bibblock">Mdetr-modulated detection for end-to-end multi-modal understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pp.Â  1780â€“1790, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, DavidÂ A Shamma, etÂ al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123(1):32â€“73, 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuhn (1955)</span>
<span class="ltx_bibblock">
HaroldÂ W Kuhn.

</span>
<span class="ltx_bibblock">The hungarian method for the assignment problem.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Naval research logistics quarterly</em>, 2(1-2):83â€“97, 1955.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2021)</span>
<span class="ltx_bibblock">
LiunianÂ Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li,
Yiwu Zhong, Lijuan Wang, LuÂ Yuan, Lei Zhang, Jenq-Neng Hwang, etÂ al.

</span>
<span class="ltx_bibblock">Grounded language-image pre-training.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.03857</em>, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2020)</span>
<span class="ltx_bibblock">
Xiujun Li, XiÂ Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
Wang, Houdong Hu, LiÂ Dong, Furu Wei, etÂ al.

</span>
<span class="ltx_bibblock">Oscar: Object-semantics aligned pre-training for vision-language
tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pp.Â  121â€“137.
Springer, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2017)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan,
and Serge Belongie.

</span>
<span class="ltx_bibblock">Feature pyramid networks for object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pp.Â  2117â€“2125, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021)</span>
<span class="ltx_bibblock">
ZeÂ Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pp.Â  10012â€“10022, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng etÂ al. (2021)</span>
<span class="ltx_bibblock">
Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei
Sun, and Jingdong Wang.

</span>
<span class="ltx_bibblock">Conditional detr for fast training convergence.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pp.Â  3651â€“3660, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Minderer etÂ al. (2022)</span>
<span class="ltx_bibblock">
Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk
Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa
Dehghani, Zhuoran Shen, etÂ al.

</span>
<span class="ltx_bibblock">Simple open-vocabulary object detection with vision transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.06230</em>, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ordonez etÂ al. (2011)</span>
<span class="ltx_bibblock">
Vicente Ordonez, Girish Kulkarni, and Tamara Berg.

</span>
<span class="ltx_bibblock">Im2text: Describing images using 1 million captioned photographs.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 24, 2011.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plummer etÂ al. (2015)</span>
<span class="ltx_bibblock">
BryanÂ A Plummer, Liwei Wang, ChrisÂ M Cervantes, JuanÂ C Caicedo, Julia
Hockenmaier, and Svetlana Lazebnik.

</span>
<span class="ltx_bibblock">Flickr30k entities: Collecting region-to-phrase correspondences for
richer image-to-sentence models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pp.Â  2641â€“2649, 2015.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
etÂ al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp.Â 8748â€“8763. PMLR, 2021.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang,
Jie Zhou, and Jiwen Lu.

</span>
<span class="ltx_bibblock">Denseclip: Language-guided dense prediction with context-aware
prompting.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.01518</em>, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Redmon &amp; Farhadi (2017)</span>
<span class="ltx_bibblock">
Joseph Redmon and Ali Farhadi.

</span>
<span class="ltx_bibblock">Yolo9000: better, faster, stronger.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pp.Â  7263â€“7271, 2017.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. (2015)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pp.Â 91â€“99, 2015.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaridis etÂ al. (2018)</span>
<span class="ltx_bibblock">
Christos Sakaridis, Dengxin Dai, and Luc VanÂ Gool.

</span>
<span class="ltx_bibblock">Semantic foggy scene understanding with synthetic data.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 126(9):973â€“992, 2018.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma etÂ al. (2018)</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.

</span>
<span class="ltx_bibblock">Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pp.Â  2556â€“2565,
2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2021a)</span>
<span class="ltx_bibblock">
Peize Sun, YiÂ Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan, Changhu Wang, and Ping
Luo.

</span>
<span class="ltx_bibblock">What makes for end-to-end object detection?

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp.Â 9934â€“9944. PMLR, 2021a.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2021b)</span>
<span class="ltx_bibblock">
Peize Sun, Rufeng Zhang, YiÂ Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi
Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, etÂ al.

</span>
<span class="ltx_bibblock">Sparse r-cnn: End-to-end object detection with learnable proposals.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pp.Â  14454â€“14463, 2021b.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2017)</span>
<span class="ltx_bibblock">
Peng Tang, Xinggang Wang, Xiang Bai, and Wenyu Liu.

</span>
<span class="ltx_bibblock">Multiple instance detection network with online instance classifier
refinement.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pp.Â  2843â€“2851, 2017.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Pei Wang, Zhaowei Cai, Hao Yang, Gurumurthy Swaminathan, Nuno Vasconcelos,
Bernt Schiele, and Stefano Soatto.

</span>
<span class="ltx_bibblock">Omni-detr: Omni-supervised object detection with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pp.Â  9367â€“9376, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Lewei Yao, Runhui Huang, LuÂ Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan
Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu.

</span>
<span class="ltx_bibblock">Filip: Fine-grained interactive language-image pre-training.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.07783</em>, 2021.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye etÂ al. (2019)</span>
<span class="ltx_bibblock">
Keren Ye, Mingda Zhang, Adriana Kovashka, Wei Li, Danfeng Qin, and Jesse
Berent.

</span>
<span class="ltx_bibblock">Cap2det: Learning to amplify weak caption supervision for object
detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pp.Â  9686â€“9695, 2019.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2018)</span>
<span class="ltx_bibblock">
Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht
Madhavan, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Bdd100k: A diverse driving video database with scalable annotation
tooling.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.04687</em>, 2(5):6,
2018.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
Yonghui Wu.

</span>
<span class="ltx_bibblock">Coca: Contrastive captioners are image-text foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01917</em>, 2022.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2016)</span>
<span class="ltx_bibblock">
Licheng Yu, Patrick Poirson, Shan Yang, AlexanderÂ C Berg, and TamaraÂ L Berg.

</span>
<span class="ltx_bibblock">Modeling context in referring expressions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pp.Â  69â€“85.
Springer, 2016.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan etÂ al. (2021)</span>
<span class="ltx_bibblock">
LuÂ Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao,
Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, etÂ al.

</span>
<span class="ltx_bibblock">Florence: A new foundation model for computer vision.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.11432</em>, 2021.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zareian etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alireza Zareian, KevinÂ Dela Rosa, DerekÂ Hao Hu, and Shih-Fu Chang.

</span>
<span class="ltx_bibblock">Open-vocabulary object detection using captions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pp.Â  14393â€“14402, 2021.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai etÂ al. (2021)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers,
Alexander Kolesnikov, and Lucas Beyer.

</span>
<span class="ltx_bibblock">Lit: Zero-shot transfer with locked-image text tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.07991</em>, 2021.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, LionelÂ M Ni, and
Heung-Yeung Shum.

</span>
<span class="ltx_bibblock">Dino: Detr with improved denoising anchor boxes for end-to-end object
detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.03605</em>, 2022.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Shiyu Zhao, Zhixing Zhang, Samuel Schulter, Long Zhao, BGÂ VijayÂ Kumar,
Anastasis Stathopoulos, Manmohan Chandraker, and DimitrisÂ N Metaxas.

</span>
<span class="ltx_bibblock">Exploiting unlabeled data with vision and language models for object
detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pp.Â  159â€“175.
Springer, 2022.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella,
LiunianÂ Harold Li, Luowei Zhou, Xiyang Dai, LuÂ Yuan, Yin Li, etÂ al.

</span>
<span class="ltx_bibblock">Regionclip: Region-based language-image pretraining.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pp.Â  16793â€“16803, 2022.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2021a)</span>
<span class="ltx_bibblock">
Chong Zhou, ChenÂ Change Loy, and BoÂ Dai.

</span>
<span class="ltx_bibblock">Denseclip: Extract free dense labels from clip.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.01071</em>, 2021a.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2021b)</span>
<span class="ltx_bibblock">
Xingyi Zhou, Vladlen Koltun, and Philipp KrÃ¤henbÃ¼hl.

</span>
<span class="ltx_bibblock">Probabilistic two-stage detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.07461</em>, 2021b.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2022)</span>
<span class="ltx_bibblock">
Xingyi Zhou, Rohit Girdhar, Armand Joulin, Phillip KrÃ¤henbÃ¼hl, and
Ishan Misra.

</span>
<span class="ltx_bibblock">Detecting twenty-thousand classes using image-level supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.02605</em>, 2022.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Deformable detr: Deformable transformers for end-to-end object
detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.04159</em>, 2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p"><span id="A1.p1.1.1" class="ltx_text ltx_font_bold">1. Detailed Framework</span></p>
</div>
<figure id="A1.F5" class="ltx_figure"><img src="/html/2211.14843/assets/x5.png" id="A1.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Overview of our proposed <span id="A1.F5.2.1" class="ltx_text ltx_font_bold">VLDet</span> framework, which consists of two training parts. The bottom part (red arrows), also the inference path, is a classical two-stage Faster R-CNN pipeline trained with well annotated base-class data. The top part (black arrows) is the one to learn fine-grained object-word alignments with the corpus of image-text pairs, which is formulated as a bipartite matching problem, i.e., matching two sets of region and word candidates.
For easy illustration, we did not draw the regression heads of Faster R-CNN here.
</figcaption>
</figure>
<div id="A1.p2" class="ltx_para ltx_noindent">
<p id="A1.p2.1" class="ltx_p"><span id="A1.p2.1.1" class="ltx_text ltx_font_bold">2. The Standard LVIS Benchmark</span></p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">To investigate how helpful the image-text pairs data is to the fully supervised setting, we conducted experiments on the standard LVIS benchmark.
In this setting, we use all the annotations including rare classes and base classes in the training set for Box-Supervised method.
DeticÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite> and our method use additional image-text pairs in CC3MÂ <cite class="ltx_cite ltx_citemacro_citep">(Sharma etÂ al., <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite>.
As the TableÂ <span id="A1.p3.1.1" class="ltx_text" style="color:#FF0000;">1</span> shown, compared with the sate-of-the-art method Detic, our model further improves 0.8% mAP on rare classes and archives comparable results on the common and frequency classes.</p>
</div>
<figure id="A1.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Results on standard LVIS benchmark. We evaluate Detic and our method using all classes annotations in the LVIS training set and additional images-text pairs in CC3M. </figcaption>
<table id="A1.T7.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T7.4.4" class="ltx_tr">
<th id="A1.T7.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">Method</th>
<th id="A1.T7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">mAP<math id="A1.T7.1.1.1.m1.1" class="ltx_Math" alttext="{}^{mask}_{r}" display="inline"><semantics id="A1.T7.1.1.1.m1.1a"><mmultiscripts id="A1.T7.1.1.1.m1.1.1" xref="A1.T7.1.1.1.m1.1.1.cmml"><mi id="A1.T7.1.1.1.m1.1.1.2.2" xref="A1.T7.1.1.1.m1.1.1.2.2.cmml"></mi><mprescripts id="A1.T7.1.1.1.m1.1.1a" xref="A1.T7.1.1.1.m1.1.1.cmml"></mprescripts><mi id="A1.T7.1.1.1.m1.1.1.3" xref="A1.T7.1.1.1.m1.1.1.3.cmml">r</mi><mrow id="A1.T7.1.1.1.m1.1.1b" xref="A1.T7.1.1.1.m1.1.1.cmml"></mrow><mrow id="A1.T7.1.1.1.m1.1.1c" xref="A1.T7.1.1.1.m1.1.1.cmml"></mrow><mrow id="A1.T7.1.1.1.m1.1.1.2.3" xref="A1.T7.1.1.1.m1.1.1.2.3.cmml"><mi id="A1.T7.1.1.1.m1.1.1.2.3.2" xref="A1.T7.1.1.1.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="A1.T7.1.1.1.m1.1.1.2.3.1" xref="A1.T7.1.1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.1.1.1.m1.1.1.2.3.3" xref="A1.T7.1.1.1.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="A1.T7.1.1.1.m1.1.1.2.3.1a" xref="A1.T7.1.1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.1.1.1.m1.1.1.2.3.4" xref="A1.T7.1.1.1.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="A1.T7.1.1.1.m1.1.1.2.3.1b" xref="A1.T7.1.1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.1.1.1.m1.1.1.2.3.5" xref="A1.T7.1.1.1.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="A1.T7.1.1.1.m1.1b"><apply id="A1.T7.1.1.1.m1.1.1.cmml" xref="A1.T7.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.T7.1.1.1.m1.1.1.1.cmml" xref="A1.T7.1.1.1.m1.1.1">subscript</csymbol><apply id="A1.T7.1.1.1.m1.1.1.2.cmml" xref="A1.T7.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.T7.1.1.1.m1.1.1.2.1.cmml" xref="A1.T7.1.1.1.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="A1.T7.1.1.1.m1.1.1.2.2.cmml" xref="A1.T7.1.1.1.m1.1.1.2.2">absent</csymbol><apply id="A1.T7.1.1.1.m1.1.1.2.3.cmml" xref="A1.T7.1.1.1.m1.1.1.2.3"><times id="A1.T7.1.1.1.m1.1.1.2.3.1.cmml" xref="A1.T7.1.1.1.m1.1.1.2.3.1"></times><ci id="A1.T7.1.1.1.m1.1.1.2.3.2.cmml" xref="A1.T7.1.1.1.m1.1.1.2.3.2">ğ‘š</ci><ci id="A1.T7.1.1.1.m1.1.1.2.3.3.cmml" xref="A1.T7.1.1.1.m1.1.1.2.3.3">ğ‘</ci><ci id="A1.T7.1.1.1.m1.1.1.2.3.4.cmml" xref="A1.T7.1.1.1.m1.1.1.2.3.4">ğ‘ </ci><ci id="A1.T7.1.1.1.m1.1.1.2.3.5.cmml" xref="A1.T7.1.1.1.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><ci id="A1.T7.1.1.1.m1.1.1.3.cmml" xref="A1.T7.1.1.1.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.1.1.1.m1.1c">{}^{mask}_{r}</annotation></semantics></math>
</th>
<th id="A1.T7.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">mAP<math id="A1.T7.2.2.2.m1.1" class="ltx_Math" alttext="{}^{mask}_{c}" display="inline"><semantics id="A1.T7.2.2.2.m1.1a"><mmultiscripts id="A1.T7.2.2.2.m1.1.1" xref="A1.T7.2.2.2.m1.1.1.cmml"><mi id="A1.T7.2.2.2.m1.1.1.2.2" xref="A1.T7.2.2.2.m1.1.1.2.2.cmml"></mi><mprescripts id="A1.T7.2.2.2.m1.1.1a" xref="A1.T7.2.2.2.m1.1.1.cmml"></mprescripts><mi id="A1.T7.2.2.2.m1.1.1.3" xref="A1.T7.2.2.2.m1.1.1.3.cmml">c</mi><mrow id="A1.T7.2.2.2.m1.1.1b" xref="A1.T7.2.2.2.m1.1.1.cmml"></mrow><mrow id="A1.T7.2.2.2.m1.1.1c" xref="A1.T7.2.2.2.m1.1.1.cmml"></mrow><mrow id="A1.T7.2.2.2.m1.1.1.2.3" xref="A1.T7.2.2.2.m1.1.1.2.3.cmml"><mi id="A1.T7.2.2.2.m1.1.1.2.3.2" xref="A1.T7.2.2.2.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="A1.T7.2.2.2.m1.1.1.2.3.1" xref="A1.T7.2.2.2.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.2.2.2.m1.1.1.2.3.3" xref="A1.T7.2.2.2.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="A1.T7.2.2.2.m1.1.1.2.3.1a" xref="A1.T7.2.2.2.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.2.2.2.m1.1.1.2.3.4" xref="A1.T7.2.2.2.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="A1.T7.2.2.2.m1.1.1.2.3.1b" xref="A1.T7.2.2.2.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.2.2.2.m1.1.1.2.3.5" xref="A1.T7.2.2.2.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="A1.T7.2.2.2.m1.1b"><apply id="A1.T7.2.2.2.m1.1.1.cmml" xref="A1.T7.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="A1.T7.2.2.2.m1.1.1.1.cmml" xref="A1.T7.2.2.2.m1.1.1">subscript</csymbol><apply id="A1.T7.2.2.2.m1.1.1.2.cmml" xref="A1.T7.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="A1.T7.2.2.2.m1.1.1.2.1.cmml" xref="A1.T7.2.2.2.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="A1.T7.2.2.2.m1.1.1.2.2.cmml" xref="A1.T7.2.2.2.m1.1.1.2.2">absent</csymbol><apply id="A1.T7.2.2.2.m1.1.1.2.3.cmml" xref="A1.T7.2.2.2.m1.1.1.2.3"><times id="A1.T7.2.2.2.m1.1.1.2.3.1.cmml" xref="A1.T7.2.2.2.m1.1.1.2.3.1"></times><ci id="A1.T7.2.2.2.m1.1.1.2.3.2.cmml" xref="A1.T7.2.2.2.m1.1.1.2.3.2">ğ‘š</ci><ci id="A1.T7.2.2.2.m1.1.1.2.3.3.cmml" xref="A1.T7.2.2.2.m1.1.1.2.3.3">ğ‘</ci><ci id="A1.T7.2.2.2.m1.1.1.2.3.4.cmml" xref="A1.T7.2.2.2.m1.1.1.2.3.4">ğ‘ </ci><ci id="A1.T7.2.2.2.m1.1.1.2.3.5.cmml" xref="A1.T7.2.2.2.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><ci id="A1.T7.2.2.2.m1.1.1.3.cmml" xref="A1.T7.2.2.2.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.2.2.2.m1.1c">{}^{mask}_{c}</annotation></semantics></math>
</th>
<th id="A1.T7.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">mAP<math id="A1.T7.3.3.3.m1.1" class="ltx_Math" alttext="{}^{mask}_{f}" display="inline"><semantics id="A1.T7.3.3.3.m1.1a"><mmultiscripts id="A1.T7.3.3.3.m1.1.1" xref="A1.T7.3.3.3.m1.1.1.cmml"><mi id="A1.T7.3.3.3.m1.1.1.2.2" xref="A1.T7.3.3.3.m1.1.1.2.2.cmml"></mi><mprescripts id="A1.T7.3.3.3.m1.1.1a" xref="A1.T7.3.3.3.m1.1.1.cmml"></mprescripts><mi id="A1.T7.3.3.3.m1.1.1.3" xref="A1.T7.3.3.3.m1.1.1.3.cmml">f</mi><mrow id="A1.T7.3.3.3.m1.1.1b" xref="A1.T7.3.3.3.m1.1.1.cmml"></mrow><mrow id="A1.T7.3.3.3.m1.1.1c" xref="A1.T7.3.3.3.m1.1.1.cmml"></mrow><mrow id="A1.T7.3.3.3.m1.1.1.2.3" xref="A1.T7.3.3.3.m1.1.1.2.3.cmml"><mi id="A1.T7.3.3.3.m1.1.1.2.3.2" xref="A1.T7.3.3.3.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="A1.T7.3.3.3.m1.1.1.2.3.1" xref="A1.T7.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.3.3.3.m1.1.1.2.3.3" xref="A1.T7.3.3.3.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="A1.T7.3.3.3.m1.1.1.2.3.1a" xref="A1.T7.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.3.3.3.m1.1.1.2.3.4" xref="A1.T7.3.3.3.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="A1.T7.3.3.3.m1.1.1.2.3.1b" xref="A1.T7.3.3.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.3.3.3.m1.1.1.2.3.5" xref="A1.T7.3.3.3.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="A1.T7.3.3.3.m1.1b"><apply id="A1.T7.3.3.3.m1.1.1.cmml" xref="A1.T7.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="A1.T7.3.3.3.m1.1.1.1.cmml" xref="A1.T7.3.3.3.m1.1.1">subscript</csymbol><apply id="A1.T7.3.3.3.m1.1.1.2.cmml" xref="A1.T7.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="A1.T7.3.3.3.m1.1.1.2.1.cmml" xref="A1.T7.3.3.3.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="A1.T7.3.3.3.m1.1.1.2.2.cmml" xref="A1.T7.3.3.3.m1.1.1.2.2">absent</csymbol><apply id="A1.T7.3.3.3.m1.1.1.2.3.cmml" xref="A1.T7.3.3.3.m1.1.1.2.3"><times id="A1.T7.3.3.3.m1.1.1.2.3.1.cmml" xref="A1.T7.3.3.3.m1.1.1.2.3.1"></times><ci id="A1.T7.3.3.3.m1.1.1.2.3.2.cmml" xref="A1.T7.3.3.3.m1.1.1.2.3.2">ğ‘š</ci><ci id="A1.T7.3.3.3.m1.1.1.2.3.3.cmml" xref="A1.T7.3.3.3.m1.1.1.2.3.3">ğ‘</ci><ci id="A1.T7.3.3.3.m1.1.1.2.3.4.cmml" xref="A1.T7.3.3.3.m1.1.1.2.3.4">ğ‘ </ci><ci id="A1.T7.3.3.3.m1.1.1.2.3.5.cmml" xref="A1.T7.3.3.3.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><ci id="A1.T7.3.3.3.m1.1.1.3.cmml" xref="A1.T7.3.3.3.m1.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.3.3.3.m1.1c">{}^{mask}_{f}</annotation></semantics></math>
</th>
<th id="A1.T7.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">mAP<math id="A1.T7.4.4.4.m1.1" class="ltx_Math" alttext="{}^{mask}_{all}" display="inline"><semantics id="A1.T7.4.4.4.m1.1a"><mmultiscripts id="A1.T7.4.4.4.m1.1.1" xref="A1.T7.4.4.4.m1.1.1.cmml"><mi id="A1.T7.4.4.4.m1.1.1.2.2" xref="A1.T7.4.4.4.m1.1.1.2.2.cmml"></mi><mprescripts id="A1.T7.4.4.4.m1.1.1a" xref="A1.T7.4.4.4.m1.1.1.cmml"></mprescripts><mrow id="A1.T7.4.4.4.m1.1.1.3" xref="A1.T7.4.4.4.m1.1.1.3.cmml"><mi id="A1.T7.4.4.4.m1.1.1.3.2" xref="A1.T7.4.4.4.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A1.T7.4.4.4.m1.1.1.3.1" xref="A1.T7.4.4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="A1.T7.4.4.4.m1.1.1.3.3" xref="A1.T7.4.4.4.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="A1.T7.4.4.4.m1.1.1.3.1a" xref="A1.T7.4.4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="A1.T7.4.4.4.m1.1.1.3.4" xref="A1.T7.4.4.4.m1.1.1.3.4.cmml">l</mi></mrow><mrow id="A1.T7.4.4.4.m1.1.1b" xref="A1.T7.4.4.4.m1.1.1.cmml"></mrow><mrow id="A1.T7.4.4.4.m1.1.1c" xref="A1.T7.4.4.4.m1.1.1.cmml"></mrow><mrow id="A1.T7.4.4.4.m1.1.1.2.3" xref="A1.T7.4.4.4.m1.1.1.2.3.cmml"><mi id="A1.T7.4.4.4.m1.1.1.2.3.2" xref="A1.T7.4.4.4.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="A1.T7.4.4.4.m1.1.1.2.3.1" xref="A1.T7.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.4.4.4.m1.1.1.2.3.3" xref="A1.T7.4.4.4.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="A1.T7.4.4.4.m1.1.1.2.3.1a" xref="A1.T7.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.4.4.4.m1.1.1.2.3.4" xref="A1.T7.4.4.4.m1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="A1.T7.4.4.4.m1.1.1.2.3.1b" xref="A1.T7.4.4.4.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="A1.T7.4.4.4.m1.1.1.2.3.5" xref="A1.T7.4.4.4.m1.1.1.2.3.5.cmml">k</mi></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="A1.T7.4.4.4.m1.1b"><apply id="A1.T7.4.4.4.m1.1.1.cmml" xref="A1.T7.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="A1.T7.4.4.4.m1.1.1.1.cmml" xref="A1.T7.4.4.4.m1.1.1">subscript</csymbol><apply id="A1.T7.4.4.4.m1.1.1.2.cmml" xref="A1.T7.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="A1.T7.4.4.4.m1.1.1.2.1.cmml" xref="A1.T7.4.4.4.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="A1.T7.4.4.4.m1.1.1.2.2.cmml" xref="A1.T7.4.4.4.m1.1.1.2.2">absent</csymbol><apply id="A1.T7.4.4.4.m1.1.1.2.3.cmml" xref="A1.T7.4.4.4.m1.1.1.2.3"><times id="A1.T7.4.4.4.m1.1.1.2.3.1.cmml" xref="A1.T7.4.4.4.m1.1.1.2.3.1"></times><ci id="A1.T7.4.4.4.m1.1.1.2.3.2.cmml" xref="A1.T7.4.4.4.m1.1.1.2.3.2">ğ‘š</ci><ci id="A1.T7.4.4.4.m1.1.1.2.3.3.cmml" xref="A1.T7.4.4.4.m1.1.1.2.3.3">ğ‘</ci><ci id="A1.T7.4.4.4.m1.1.1.2.3.4.cmml" xref="A1.T7.4.4.4.m1.1.1.2.3.4">ğ‘ </ci><ci id="A1.T7.4.4.4.m1.1.1.2.3.5.cmml" xref="A1.T7.4.4.4.m1.1.1.2.3.5">ğ‘˜</ci></apply></apply><apply id="A1.T7.4.4.4.m1.1.1.3.cmml" xref="A1.T7.4.4.4.m1.1.1.3"><times id="A1.T7.4.4.4.m1.1.1.3.1.cmml" xref="A1.T7.4.4.4.m1.1.1.3.1"></times><ci id="A1.T7.4.4.4.m1.1.1.3.2.cmml" xref="A1.T7.4.4.4.m1.1.1.3.2">ğ‘</ci><ci id="A1.T7.4.4.4.m1.1.1.3.3.cmml" xref="A1.T7.4.4.4.m1.1.1.3.3">ğ‘™</ci><ci id="A1.T7.4.4.4.m1.1.1.3.4.cmml" xref="A1.T7.4.4.4.m1.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.4.4.4.m1.1c">{}^{mask}_{all}</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T7.4.5.1" class="ltx_tr">
<th id="A1.T7.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">Box-Supervised</th>
<td id="A1.T7.4.5.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">25.6</td>
<td id="A1.T7.4.5.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">30.4</td>
<td id="A1.T7.4.5.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">35.2</td>
<td id="A1.T7.4.5.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.0pt;padding-right:9.0pt;">31.5</td>
</tr>
<tr id="A1.T7.4.6.2" class="ltx_tr">
<th id="A1.T7.4.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:9.0pt;padding-right:9.0pt;">DeticÂ <cite class="ltx_cite ltx_citemacro_cite">Zhou etÂ al. (<a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="A1.T7.4.6.2.2" class="ltx_td ltx_align_center" style="padding-left:9.0pt;padding-right:9.0pt;">27.6</td>
<td id="A1.T7.4.6.2.3" class="ltx_td ltx_align_center" style="padding-left:9.0pt;padding-right:9.0pt;"><span id="A1.T7.4.6.2.3.1" class="ltx_text ltx_font_bold">31.0</span></td>
<td id="A1.T7.4.6.2.4" class="ltx_td ltx_align_center" style="padding-left:9.0pt;padding-right:9.0pt;"><span id="A1.T7.4.6.2.4.1" class="ltx_text ltx_font_bold">35.4</span></td>
<td id="A1.T7.4.6.2.5" class="ltx_td ltx_align_center" style="padding-left:9.0pt;padding-right:9.0pt;"><span id="A1.T7.4.6.2.5.1" class="ltx_text ltx_font_bold">32.2</span></td>
</tr>
<tr id="A1.T7.4.7.3" class="ltx_tr">
<th id="A1.T7.4.7.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding-left:9.0pt;padding-right:9.0pt;">Our</th>
<td id="A1.T7.4.7.3.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:9.0pt;padding-right:9.0pt;"><span id="A1.T7.4.7.3.2.1" class="ltx_text ltx_font_bold">28.4</span></td>
<td id="A1.T7.4.7.3.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:9.0pt;padding-right:9.0pt;"><span id="A1.T7.4.7.3.3.1" class="ltx_text ltx_font_bold">31.0</span></td>
<td id="A1.T7.4.7.3.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:9.0pt;padding-right:9.0pt;">35.1</td>
<td id="A1.T7.4.7.3.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:9.0pt;padding-right:9.0pt;">32.1</td>
</tr>
</tbody>
</table>
</figure>
<div id="A1.p4" class="ltx_para ltx_noindent">
<p id="A1.p4.1" class="ltx_p"><span id="A1.p4.1.1" class="ltx_text ltx_font_bold">3. Visualization of OV-COCO</span></p>
</div>
<div id="A1.p5" class="ltx_para ltx_noindent">
<p id="A1.p5.1" class="ltx_p">As shown in FigureÂ <a href="#A1.F6" title="Figure 6 â€£ Appendix A Appendix â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we visualize some cases of matching results of open vocabulary COCO setting. As we can see, the model can extract promising region-words pairs from image-text data. The results of the matching include a variety of objects, such as â€œostrichâ€, â€œduffle bagâ€, and â€œadult bearâ€ which significantly expands the vocabulary for object detection. The results demonstrate generalization ability of our method to novel class.</p>
</div>
<figure id="A1.F6" class="ltx_figure"><img src="/html/2211.14843/assets/x6.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="387" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
Visualization of the bipartite matching results in open-vocabulary COCO setting.</figcaption>
</figure>
<div id="A1.p6" class="ltx_para ltx_noindent">
<p id="A1.p6.1" class="ltx_p"><span id="A1.p6.1.1" class="ltx_text ltx_font_bold">4. Failure Cases</span></p>
</div>
<div id="A1.p7" class="ltx_para ltx_noindent">
<p id="A1.p7.1" class="ltx_p">We provide some failure cases of VLDet in FigureÂ <a href="#A1.F7" title="Figure 7 â€£ Appendix A Appendix â€£ Learning Object-Language Alignments for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
These images is training images from CC3M in the open-vocabulary LVIS setting.
We noticed that the region proposal network fails to provide the candidate region for â€œTV characterâ€, resulting in erroneous region-word pairs in sub-figure (1).
This observation indicates that there is a scope of improve if a strong region proposal network could extensively providing the candidate regions.
Besides, as sub-figure (2) shown, our method cannot handle the case where the target object â€œportâ€ mentioned in caption but not appeared in the image.
We leave these problems for the future study.</p>
</div>
<figure id="A1.F7" class="ltx_figure"><img src="/html/2211.14843/assets/x7.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Failure cases of our method.</figcaption>
</figure>
<div id="A1.p8" class="ltx_para ltx_noindent">
<p id="A1.p8.1" class="ltx_p"><span id="A1.p8.1.1" class="ltx_text ltx_font_bold">5. Vision and Language Representation Learning</span></p>
</div>
<div id="A1.p9" class="ltx_para ltx_noindent">
<p id="A1.p9.1" class="ltx_p">Vision and language tasks, such as visual question answering and natural language for visual reasoning, require a unified understanding of both images and language.
Pioneering works of multi-modal representation learning leverage the region-based visual features from object detection for better visual semantics.
For example,
OscarÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> introduces object tags and region features to learn the cross-domain semantics with a universal Transformer, significantly improving visual language understanding and generation tasks.
ImageBERTÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite> utilizes the masked object classification and region feature regression as pre-training tasks.
UNITERÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> learns generalizable contextualized embeddings by word-region alignments to encourage similar output distributions across multiple modalities.
Different from these works that aim to learn a universal semantic space for cross-modal understanding, we focus on increasing the generalization ability of object detection by the weak language supervision.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.14842" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.14843" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.14843">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.14843" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.14844" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 05:42:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
