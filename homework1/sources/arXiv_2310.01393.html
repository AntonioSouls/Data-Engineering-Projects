<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.01393] Open-Vocabulary Object Detection via Dynamic Self-Training</title><meta property="og:description" content="Open-vocabulary object detection (OVOD) aims to detect the objects beyond the set of classes observed during training. This work presents a simple yet effective strategy that leverages the zero-shot classification abilâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Open-Vocabulary Object Detection via Dynamic Self-Training">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Open-Vocabulary Object Detection via Dynamic Self-Training">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.01393">

<!--Generated on Wed Feb 28 03:08:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Open-Vocabulary Object Detection via Dynamic Self-Training</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shilin Xu<sup id="id15.15.id1" class="ltx_sup"><span id="id15.15.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Xiangtai Li<math id="id3.3.m2.3" class="ltx_Math" alttext="{}^{2*}\textsuperscript{$\dagger$}" display="inline"><semantics id="id3.3.m2.3a"><mmultiscripts id="id3.3.m2.3.4" xref="id3.3.m2.3.4.cmml"><mtext id="id3.3.m2.1.1" xref="id3.3.m2.1.1e.cmml"><sup id="id3.3.m2.1.1.1nest" class="ltx_sup"><XMath xmlns="http://dlmf.nist.gov/LaTeXML" id="id3.3.m2.1.1c" fragid="id3.3.m2.1.1.1.m1.1nest" xref="id3.3.m2.1.1e.cmml"><XMTok name="dagger" role="MULOP" id="id3.3.m2.1.1d" fragid="id3.3.m2.1.1.1.m1.1.1nest" xref="id3.3.m2.1.1e.cmml">â€ </XMTok></XMath></sup></mtext><mprescripts id="id3.3.m2.3.4a" xref="id3.3.m2.3.4.cmml"></mprescripts><mrow id="id3.3.m2.3.4b" xref="id3.3.m2.3.4.cmml"></mrow><mrow id="id3.3.m2.3.3.2.4" xref="id3.3.m2.3.3.2.3.cmml"><mn id="id3.3.m2.2.2.1.1" xref="id3.3.m2.2.2.1.1.cmml">2</mn><mo lspace="0.222em" id="id3.3.m2.3.3.2.4.1" xref="id3.3.m2.3.3.2.3.cmml">â£</mo><mo id="id3.3.m2.3.3.2.2" xref="id3.3.m2.3.3.2.2.cmml">âˆ—</mo></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="id3.3.m2.3b"><apply id="id3.3.m2.3.4.cmml" xref="id3.3.m2.3.4"><csymbol cd="ambiguous" id="id3.3.m2.3.4.1.cmml" xref="id3.3.m2.3.4">superscript</csymbol><ci id="id3.3.m2.1.1e.cmml" xref="id3.3.m2.1.1"><mtext id="id3.3.m2.1.1.cmml" xref="id3.3.m2.1.1"><sup id="id3.3.m2.1.1.1anest" class="ltx_sup"><XMath xmlns="http://dlmf.nist.gov/LaTeXML" id="id3.3.m2.1.1c.cmml" fragid="id3.3.m2.1.1.1.m1.1anest" xref="id3.3.m2.1.1"><XMTok name="dagger" role="MULOP" id="id3.3.m2.1.1d.cmml" fragid="id3.3.m2.1.1.1.m1.1.1anest" xref="id3.3.m2.1.1">â€ </XMTok></XMath></sup></mtext></ci><list id="id3.3.m2.3.3.2.3.cmml" xref="id3.3.m2.3.3.2.4"><cn type="integer" id="id3.3.m2.2.2.1.1.cmml" xref="id3.3.m2.2.2.1.1">2</cn><times id="id3.3.m2.3.3.2.2.cmml" xref="id3.3.m2.3.3.2.2"></times></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m2.3c">{}^{2*}\textsuperscript{$\dagger$}</annotation></semantics></math>, Size Wu<sup id="id16.16.id2" class="ltx_sup">2</sup>, Wenwei Zhang<sup id="id17.17.id3" class="ltx_sup"><span id="id17.17.id3.1" class="ltx_text ltx_font_italic">2,3</span></sup>, Yining Li<sup id="id18.18.id4" class="ltx_sup">3</sup>,
<br class="ltx_break">Guangliang Cheng<sup id="id19.19.id5" class="ltx_sup"><span id="id19.19.id5.1" class="ltx_text ltx_font_italic">4,5</span></sup>, Yunhai Tong<sup id="id20.20.id6" class="ltx_sup">1</sup>, Chen Change Loy<sup id="id21.21.id7" class="ltx_sup">2</sup> 
<br class="ltx_break"><sup id="id22.22.id8" class="ltx_sup"><span id="id22.22.id8.1" class="ltx_text ltx_font_italic" style="font-size:90%;">1</span></sup><span id="id14.14.4" class="ltx_text" style="font-size:90%;">Peking University
<sup id="id14.14.4.1" class="ltx_sup"><span id="id14.14.4.1.1" class="ltx_text ltx_font_italic">2</span></sup>S-Lab, Nanyang Technological University 
<br class="ltx_break"><sup id="id14.14.4.2" class="ltx_sup"><span id="id14.14.4.2.1" class="ltx_text ltx_font_italic">3</span></sup>Shanghai AI Laboratory
<sup id="id14.14.4.3" class="ltx_sup"><span id="id14.14.4.3.1" class="ltx_text ltx_font_italic">4</span></sup>SenseTime Research
<sup id="id14.14.4.4" class="ltx_sup"><span id="id14.14.4.4.1" class="ltx_text ltx_font_italic">5</span></sup>University of Liverpool

<br class="ltx_break">Project Page: <a target="_blank" href="https://github.com/xushilin1/dst-det" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/xushilin1/dst-det</a> 
</span>
</span><span class="ltx_author_notes">The first two authors contributed equally to this work. <sup id="id2.2.1" class="ltx_sup"><math id="id2.2.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id2.2.1.m1.1a"><mo id="id2.2.1.m1.1.1" xref="id2.2.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="id2.2.1.m1.1b"><ci id="id2.2.1.m1.1.1.cmml" xref="id2.2.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.1.m1.1c">\dagger</annotation></semantics></math></sup> Project Leader.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id23.id1" class="ltx_p">Open-vocabulary object detection (OVOD) aims to detect the objects <span id="id23.id1.1" class="ltx_text ltx_font_italic">beyond</span> the set of classes observed during training. This work presents a simple yet effective strategy that leverages the zero-shot classification ability of pre-trained vision-language models (VLM), such as CLIP, to directly discover proposals of possible novel classes. Unlike previous works that ignore novel classes during training and rely solely on the region proposal network (RPN) for novel object detection, our method selectively filters proposals based on specific design criteria. The resulting sets of identified proposals serve as pseudo-labels of potential novel classes during the training phase.
This self-training strategy improves the recall and accuracy of novel classes without requiring additional annotations or datasets.
We further propose a simple offline pseudo-label generation strategy to refine the object detector. Empirical evaluations on three datasets, including LVIS, V3Det, and COCO, demonstrate significant improvements over the baseline performance without incurring additional parameters or computational costs during inference. In particular, compared with previous F-VLM, our method achieves a 1.7% improvement on the LVIS dataset.
We also achieve over 6.5% improvement on the recent challenging V3Det dataset. When combined with the recent method CLIPSelf, our method also achieves 46.7 novel class AP on COCO without introducing extra data for pertaining.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.01393/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="237" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.5.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.6.2" class="ltx_text" style="font-size:90%;">Compared with other methods using previous labels. <span id="S1.F1.6.2.1" class="ltx_text ltx_font_bold">(a).</span> Previous methods usually obtain pseudo labels from extra unlabeled data. They first train a class-agnostic detector to generate object proposals and classify these proposals using VLMs. <span id="S1.F1.6.2.2" class="ltx_text ltx_font_bold">(b).</span> Our DST-Det constructs an end-to-end pipeline and generates pseudo labels during training. We <span id="S1.F1.6.2.3" class="ltx_text ltx_font_bold">do not</span> use extra data, extra learnable components, and two-stage pipelines.</span></figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2310.01393/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="476" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.5.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.6.2" class="ltx_text" style="font-size:90%;">Illustration of our motivation and framework. <span id="S1.F2.6.2.1" class="ltx_text ltx_font_bold">(a).</span> Our DST-Det incorporates novel class labels to supervise the detection head during training. <span id="S1.F2.6.2.2" class="ltx_text ltx_font_bold">(b).</span> Experiments on OV-COCO and OV-LVIS using CLIP with ground truth box for zero-shot classification. We observe high top-1 and top-5 accuracy in classifying novel classes. <span id="S1.F2.6.2.3" class="ltx_text ltx_font_bold">(c).</span> Illustration of our dynamic self-training pipeline with the pseudo labels.</span></figcaption>
</figure>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.3.2" class="ltx_text" style="font-size:90%;">Compared with existing methods using pseudo labels for OVD. Our method does not need extra data or a two-stage pipeline for training. We also do not need extra components. Moreover, our method can be a plug-in method when VLMs are available.</span></figcaption>
<div id="S1.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:497.1pt;height:91pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S1.T1.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.4.1.1.1" class="ltx_tr">
<th id="S1.T1.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Method</th>
<th id="S1.T1.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Extra Data</th>
<th id="S1.T1.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Two Stage Pipeline</th>
<th id="S1.T1.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Extra Components</th>
<th id="S1.T1.4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Plug-In Method</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.4.1.2.1" class="ltx_tr">
<th id="S1.T1.4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">VL-PLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
</th>
<td id="S1.T1.4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
<td id="S1.T1.4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
<td id="S1.T1.4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
<td id="S1.T1.4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">âœ—</td>
</tr>
<tr id="S1.T1.4.1.3.2" class="ltx_tr">
<th id="S1.T1.4.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">PB-OVDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<td id="S1.T1.4.1.3.2.2" class="ltx_td ltx_align_center">âœ“</td>
<td id="S1.T1.4.1.3.2.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S1.T1.4.1.3.2.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S1.T1.4.1.3.2.5" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S1.T1.4.1.4.3" class="ltx_tr">
<th id="S1.T1.4.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">SAS-DetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>
</th>
<td id="S1.T1.4.1.4.3.2" class="ltx_td ltx_align_center">âœ—</td>
<td id="S1.T1.4.1.4.3.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S1.T1.4.1.4.3.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S1.T1.4.1.4.3.5" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S1.T1.4.1.5.4" class="ltx_tr">
<th id="S1.T1.4.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">DST-Det (ours)</th>
<td id="S1.T1.4.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">âœ—</td>
<td id="S1.T1.4.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">âœ—</td>
<td id="S1.T1.4.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">âœ—</td>
<td id="S1.T1.4.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">âœ“</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Object detection is a fundamental task in computer vision, involving localization and recognition of objects within images.
Previous detection methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> are limited to detecting only <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">predefined categories</span> learned during the training phase. This limitation results in a considerably smaller vocabulary compared to human cognition. Although directly extending the categories of large datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> would be an ideal solution, it requires an overwhelming amount of human manual annotation. Recently, Open-Vocabulary Object Detection (OVOD)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> has emerged as a promising research direction to overcome the constraints of a fixed vocabulary and enable the detection of objects beyond predefined categories.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Typical solutions of OVOD rely on pre-trained VLMsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. These VLMs have been trained on large-scale image-text pairs and possess <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">strong</span> zero-shot classification capability. Prior worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> have attempted to leverage VLMs for OVOD by replacing learned classifiers in traditional detectors with text embeddings derived from VLMs. Leveraging VLMsâ€™ exceptional zero-shot classification ability enables the detector to assign objects to novel classes based on their semantic similarity to the embedded text representations.
Moreover, several recent approachesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> aim to distill knowledge from VLMs by aligning individual region embeddings with visual features extracted from VLMs via diverse distillation loss designs. This alignment process facilitates the transfer of semantic understanding from VLMs to the object detectors. Additionally, some studiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> attempt to build open-vocabulary detectors directly upon frozen visual foundation models. Although these methods have demonstrated impressive performance in detecting novel objects, a substantial gap exists between training and testing for novel classes when the vocabulary sizeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> is larger, since all novel classes are seen as background classes in training. For example, a more recent dataset V3DetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> contains over 10,000 classes. There are over 5,000 classes that are novel classes. However, during training, all these classes are treated as background.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we rethink the training pipeline of OVOD and propose a new dynamic self-training approach by exploiting the novel class knowledge of VLMs. As shown in Fig.Â <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a), most previous OVOD methods adopt the same training pipeline by using the base class annotations and treating novel objects as background. During testing, the novel objects are discovered by the region proposal network (RPN). Thus, a conceptual gap exists between the training and testing phases when dealing with novel classes. Our approach aims to bridge this gap by utilizing the outputs of CLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> models as pseudo labels during the training phase.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Meanwhile, several previous methods also use pseudo labels. As shown in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a), previous methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> need a two-stage framework; they first obtained pseudo labels from unlabeled data or image-caption datasets using VLMs, and then the detector is trained using generated pseudo labels and base class annotations. Our method provides an end-to-end training pipeline and generates pseudo labels dynamically during training. We list more features in Tab.Â <a href="#S1.T1" title="Table 1 â€£ 1 Introduction â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To substantiate our motivation, we first conduct a toy experiment as illustrated in Fig.Â <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b), where we calculate the top-1 and top-5 accuracy for both base classes and novel classes on LVIS and COCO datasets by using CLIPâ€™s visual features and text embeddings. Specifically, we use the ground-truth boxes to obtain visual features from the feature maps of CLIP and calculate cosine similarity with text embeddings for zero-shot classification. Our results show that the top-5 accuracy for novel classes on LVIS and COCO datasets is already <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">remarkably</span> high. This observation inspires us to consider directly using CLIP outputs as pseudo labels during the training phase. As shown in Fig.Â <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a, c), we present the DST-Det (dynamic self-training detection) framework, which directly adopts the large vocabulary information provided by CLIP for training.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To reduce the extra computation cost of the CLIP visual model, we let the object detection and the pseudo-labeling generation share the frozen CLIP visual backbone. This decision was supported by recent developments in exploiting frozen foundation modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. Our DST-Det is based on the two-stage detector Mask R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and incorporates a dynamic pseudo-labeling module that mines novel classes from negative proposals during training. Those proposals with high novel class scores can be considered foreground objects. By adopting simple threshold and scoring operations, we effectively suppress noises in the pseudo labels, such as substantial background contents and useless image crops. These designs <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">do not</span> incur any additional learnable parameters for novel class discovery and the process in dynamic since region proposals vary in each iteration. We apply this operation to both the RPN and the Region-of-Interest Head (RoIhead) in the training stage: during RPN training, we force the discovered novel objects to be foreground objects; during RoIHead training, we add the novel class labels directly to the classification target. Moreover, we propose an offline refinement process using the trained detector to generate pseudo labels, boosting performance since the final detector has converged and can directly output more stable proposals.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We show the effectiveness of DST-Det on the OV-LVISÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, OV-V3DetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and OV-COCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> benchmarks.
Our proposed method consistently outperforms existing state-of-the-art methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> on LVIS and V3Det without introducing any extra parameters and inference costs.
Specifically, DST-Det achieves 34.5% rare mask AP for novel categories on OV-LVIS. With the Faster-RCNN frameworkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, our method achieves 13.5% novel classes mAP on V3Det, which boosts previous methods by 6.8 % mAP.
Moreover, our method improves considerably on the smaller vocabulary dataset COCO compared to the baseline method.
In particular, with recent workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> as a strong baseline, our method achieves the new state-of-the-art result with 46.7% AP on novel classes.
We also provide detailed ablation studies and visual analyses, both of which validate the effectiveness of the DST-Det framework.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Close-Set Object Detection.</span> Modern object detection methods can be broadly categorized into one-stage and two-stage. One-stage detectors Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> directly classify and regress bounding boxes using a set of predefined anchors, where anchors can be defined as corners or center points. Recently, several works have adopted query-based approachesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> to replace the anchor design in previous works. Meanwhile, long-tail object detection aims to address class imbalance issues in object detection. To tackle this challenge, various methods have been proposed, such as data re-samplingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, loss re-weightingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, and decoupled trainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. However, all these methods cannot be generalized to novel categories. Our method focuses on the open-vocabulary detection setting.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Open-Vocabulary Object Detection (OVOD).</span> This task extends the detectorâ€™s vocabulary at inference time, where the detector can recognize objects not encountered during the training. Recently, OVODÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> has drawn increasing attention due to the emergence of vision-language modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. On the one hand, several worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> attempt to improve OVOD performance by letting the object detector learn knowledge from advanced VLMs.
For instance, ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> effectively distills the knowledge of CLIP into a Mask R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
DetProÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> improves the distillation-based method using learnable category prompts, while BARONÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> proposes to lift the distillation from single regions to a bag of regions.
On the other hand, F-VLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> directly builds a two-stage detector upon frozen VLMs (i.e. CLIP) and trains the detector heads only. It fuses the predicted score and the CLIP score for open-vocabulary recognition.
Moreover, several worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> aim for better VLMs pre-training for OVOD.
Both types of open-vocabulary object detectors treat novel objects as background during training but target them as foreground objects in testing. This gap hinders the detector from discovering objects of novel categories.
We aim to bridge this gap by leveraging stronger vision-language models, enabling improved detection performance for novel classes.
Several worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> also propose pseudo labels to improve OVD tasks. However, these works require <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">extra VLM tuning and large caption datasets</span> for pre-labeling, which makes the pipeline complex. In contrast, our method is simpler and elegant, <span id="S2.p2.1.3" class="ltx_text ltx_font_italic">without</span> extra pipelines or parameter tuning.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Vision-Language Pre-training and Alignment.</span> Several worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> study the pre-training of VLMs in cases of improving various downstream tasks, including recognition, captioning, and text generation. In particular, contrastive learning has achieved impressively aligned image and text representations by training VLMs on large-scale datasets, as demonstrated by several worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. For example, the representative work, CLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, can perform zero-shot classification on ImageNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> without fine-tuning or re-training. Inspired by CLIPâ€™s generalizability in visual recognition, various attempts have been made to adapt CLIPâ€™s knowledge to dense prediction models such as image segmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> and object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> in the context of open-vocabulary recognition. Meanwhile, several worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> use CLIP to extract pseudo labels for dense prediction tasks. Our method effectively explores CLIPâ€™s ability in the case of OVOD, where CLIP models help discover the object of novel categories to bridge the conceptual gap between training and testing.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminaries</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.8" class="ltx_p"><span id="S3.SS1.p1.8.1" class="ltx_text ltx_font_bold">Problem Setting.</span>
Given an image <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{I}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi mathvariant="normal" id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">I</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathrm{I}</annotation></semantics></math>, the object detector should output a set of boxes <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="b_{i}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">b</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">b_{i}</annotation></semantics></math> and their corresponding class labels <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">c</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">c_{i}</annotation></semantics></math>. Here, <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="b_{i}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">b</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">b_{i}</annotation></semantics></math> is a vector of length four representing the coordinates of the bounding box around the object, <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">c</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">c_{i}</annotation></semantics></math> is a scalar indicating the class label assigned to that object. The OVOD detector has a significantly larger vocabulary size for class labels. During the training phase, the OVOD detector can only access the detection labels of base classes <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="C_{B}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">C</mi><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">ğ¶</ci><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">ğµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">C_{B}</annotation></semantics></math>. But it is required to detect objects belonging to both the base classes <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="C_{B}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">C</mi><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">ğ¶</ci><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">ğµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">C_{B}</annotation></semantics></math> and the novel classes <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="C_{N}" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">C</mi><mi id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">ğ¶</ci><ci id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">C_{N}</annotation></semantics></math> at test time. The novel objects are unavailable during the training and are always treated as the background.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.2" class="ltx_p"><span id="S3.SS1.p2.2.1" class="ltx_text ltx_font_bold">Architecture Overview.</span>
Most previous OVOD methods adopt a two-stage detector. Therefore, we take the Mask R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as an example to demonstrate how it can be adapted to the OVOD task by leveraging the textual information from pre-trained vision-language models. Mask R-CNN consists of two stages: a Region Proposal Network (RPN) and a Region-of-Interest Head (RoIHead). The RPN denoted as <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{\Phi}_{\mathrm{RPN}}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">RPN</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">Î¦</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">RPN</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathrm{\Phi}_{\mathrm{RPN}}</annotation></semantics></math> generates object proposals, and the RoIHead denoted as <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathrm{\Phi}_{\mathrm{RoI}}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">RoI</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">Î¦</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">RoI</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\mathrm{\Phi}_{\mathrm{RoI}}</annotation></semantics></math> refines the proposalsâ€™ locations and predicts their corresponding class labels. This process can be formulated as follows:</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m2.2" class="ltx_Math" alttext="\displaystyle\{r_{i}\}_{i=1}^{M}=\mathrm{\Phi}_{\mathrm{RoI}}\circ\mathrm{\Phi}_{\mathrm{RPN}}\circ\mathrm{\Phi}_{\mathrm{Enc}}(\text{I})," display="inline"><semantics id="S3.E1.m2.2a"><mrow id="S3.E1.m2.2.2.1" xref="S3.E1.m2.2.2.1.1.cmml"><mrow id="S3.E1.m2.2.2.1.1" xref="S3.E1.m2.2.2.1.1.cmml"><msubsup id="S3.E1.m2.2.2.1.1.1" xref="S3.E1.m2.2.2.1.1.1.cmml"><mrow id="S3.E1.m2.2.2.1.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m2.2.2.1.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.1.2.cmml">{</mo><msub id="S3.E1.m2.2.2.1.1.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m2.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.2.cmml">r</mi><mi id="S3.E1.m2.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m2.2.2.1.1.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.E1.m2.2.2.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.3.cmml"><mi id="S3.E1.m2.2.2.1.1.1.1.3.2" xref="S3.E1.m2.2.2.1.1.1.1.3.2.cmml">i</mi><mo id="S3.E1.m2.2.2.1.1.1.1.3.1" xref="S3.E1.m2.2.2.1.1.1.1.3.1.cmml">=</mo><mn id="S3.E1.m2.2.2.1.1.1.1.3.3" xref="S3.E1.m2.2.2.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.E1.m2.2.2.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.3.cmml">M</mi></msubsup><mo id="S3.E1.m2.2.2.1.1.2" xref="S3.E1.m2.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1.m2.2.2.1.1.3" xref="S3.E1.m2.2.2.1.1.3.cmml"><mrow id="S3.E1.m2.2.2.1.1.3.2" xref="S3.E1.m2.2.2.1.1.3.2.cmml"><msub id="S3.E1.m2.2.2.1.1.3.2.2" xref="S3.E1.m2.2.2.1.1.3.2.2.cmml"><mi mathvariant="normal" id="S3.E1.m2.2.2.1.1.3.2.2.2" xref="S3.E1.m2.2.2.1.1.3.2.2.2.cmml">Î¦</mi><mi id="S3.E1.m2.2.2.1.1.3.2.2.3" xref="S3.E1.m2.2.2.1.1.3.2.2.3.cmml">RoI</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m2.2.2.1.1.3.2.1" xref="S3.E1.m2.2.2.1.1.3.2.1.cmml">âˆ˜</mo><msub id="S3.E1.m2.2.2.1.1.3.2.3" xref="S3.E1.m2.2.2.1.1.3.2.3.cmml"><mi mathvariant="normal" id="S3.E1.m2.2.2.1.1.3.2.3.2" xref="S3.E1.m2.2.2.1.1.3.2.3.2.cmml">Î¦</mi><mi id="S3.E1.m2.2.2.1.1.3.2.3.3" xref="S3.E1.m2.2.2.1.1.3.2.3.3.cmml">RPN</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m2.2.2.1.1.3.2.1a" xref="S3.E1.m2.2.2.1.1.3.2.1.cmml">âˆ˜</mo><msub id="S3.E1.m2.2.2.1.1.3.2.4" xref="S3.E1.m2.2.2.1.1.3.2.4.cmml"><mi mathvariant="normal" id="S3.E1.m2.2.2.1.1.3.2.4.2" xref="S3.E1.m2.2.2.1.1.3.2.4.2.cmml">Î¦</mi><mi id="S3.E1.m2.2.2.1.1.3.2.4.3" xref="S3.E1.m2.2.2.1.1.3.2.4.3.cmml">Enc</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m2.2.2.1.1.3.1" xref="S3.E1.m2.2.2.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E1.m2.2.2.1.1.3.3.2" xref="S3.E1.m2.1.1a.cmml"><mo stretchy="false" id="S3.E1.m2.2.2.1.1.3.3.2.1" xref="S3.E1.m2.1.1a.cmml">(</mo><mtext id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml">I</mtext><mo stretchy="false" id="S3.E1.m2.2.2.1.1.3.3.2.2" xref="S3.E1.m2.1.1a.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m2.2.2.1.2" xref="S3.E1.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.2b"><apply id="S3.E1.m2.2.2.1.1.cmml" xref="S3.E1.m2.2.2.1"><eq id="S3.E1.m2.2.2.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.2"></eq><apply id="S3.E1.m2.2.2.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1">superscript</csymbol><apply id="S3.E1.m2.2.2.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.1.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1">subscript</csymbol><set id="S3.E1.m2.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1"><apply id="S3.E1.m2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m2.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.2">ğ‘Ÿ</ci><ci id="S3.E1.m2.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.E1.m2.2.2.1.1.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.1.1.3"><eq id="S3.E1.m2.2.2.1.1.1.1.3.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.3.1"></eq><ci id="S3.E1.m2.2.2.1.1.1.1.3.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.E1.m2.2.2.1.1.1.1.3.3.cmml" xref="S3.E1.m2.2.2.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.E1.m2.2.2.1.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.1.3">ğ‘€</ci></apply><apply id="S3.E1.m2.2.2.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.3"><times id="S3.E1.m2.2.2.1.1.3.1.cmml" xref="S3.E1.m2.2.2.1.1.3.1"></times><apply id="S3.E1.m2.2.2.1.1.3.2.cmml" xref="S3.E1.m2.2.2.1.1.3.2"><compose id="S3.E1.m2.2.2.1.1.3.2.1.cmml" xref="S3.E1.m2.2.2.1.1.3.2.1"></compose><apply id="S3.E1.m2.2.2.1.1.3.2.2.cmml" xref="S3.E1.m2.2.2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.1.1.3.2.2.1.cmml" xref="S3.E1.m2.2.2.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m2.2.2.1.1.3.2.2.2.cmml" xref="S3.E1.m2.2.2.1.1.3.2.2.2">Î¦</ci><ci id="S3.E1.m2.2.2.1.1.3.2.2.3.cmml" xref="S3.E1.m2.2.2.1.1.3.2.2.3">RoI</ci></apply><apply id="S3.E1.m2.2.2.1.1.3.2.3.cmml" xref="S3.E1.m2.2.2.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.1.1.3.2.3.1.cmml" xref="S3.E1.m2.2.2.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m2.2.2.1.1.3.2.3.2.cmml" xref="S3.E1.m2.2.2.1.1.3.2.3.2">Î¦</ci><ci id="S3.E1.m2.2.2.1.1.3.2.3.3.cmml" xref="S3.E1.m2.2.2.1.1.3.2.3.3">RPN</ci></apply><apply id="S3.E1.m2.2.2.1.1.3.2.4.cmml" xref="S3.E1.m2.2.2.1.1.3.2.4"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.1.1.3.2.4.1.cmml" xref="S3.E1.m2.2.2.1.1.3.2.4">subscript</csymbol><ci id="S3.E1.m2.2.2.1.1.3.2.4.2.cmml" xref="S3.E1.m2.2.2.1.1.3.2.4.2">Î¦</ci><ci id="S3.E1.m2.2.2.1.1.3.2.4.3.cmml" xref="S3.E1.m2.2.2.1.1.3.2.4.3">Enc</ci></apply></apply><ci id="S3.E1.m2.1.1a.cmml" xref="S3.E1.m2.2.2.1.1.3.3.2"><mtext id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1">I</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.2c">\displaystyle\{r_{i}\}_{i=1}^{M}=\mathrm{\Phi}_{\mathrm{RoI}}\circ\mathrm{\Phi}_{\mathrm{RPN}}\circ\mathrm{\Phi}_{\mathrm{Enc}}(\text{I}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle\{b_{i},c_{i}\}_{i=1}^{M}=\{\mathrm{\Phi}_{\mathrm{box}}(r_{i}),\mathrm{\Phi}_{\mathrm{cls}}(r_{i})\}_{i=1}^{M}," display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml">{</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">b</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.4" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml">,</mo><msub id="S3.E2.m1.1.1.1.1.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.cmml">c</mi><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E2.m1.1.1.1.1.2.2.2.2.5" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml">}</mo></mrow><mrow id="S3.E2.m1.1.1.1.1.2.2.4" xref="S3.E2.m1.1.1.1.1.2.2.4.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.4.2" xref="S3.E2.m1.1.1.1.1.2.2.4.2.cmml">i</mi><mo id="S3.E2.m1.1.1.1.1.2.2.4.1" xref="S3.E2.m1.1.1.1.1.2.2.4.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.2.2.4.3" xref="S3.E2.m1.1.1.1.1.2.2.4.3.cmml">1</mn></mrow><mi id="S3.E2.m1.1.1.1.1.2.4" xref="S3.E2.m1.1.1.1.1.2.4.cmml">M</mi></msubsup><mo id="S3.E2.m1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.5.cmml">=</mo><msubsup id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml"><mrow id="S3.E2.m1.1.1.1.1.4.2.2.2" xref="S3.E2.m1.1.1.1.1.4.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.4.2.2.2.3" xref="S3.E2.m1.1.1.1.1.4.2.2.3.cmml">{</mo><mrow id="S3.E2.m1.1.1.1.1.3.1.1.1.1" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.3.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.3.2.cmml">Î¦</mi><mi id="S3.E2.m1.1.1.1.1.3.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.3.3.cmml">box</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.2.cmml">r</mi><mi id="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.4.2.2.2.4" xref="S3.E2.m1.1.1.1.1.4.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.1.1.1.1.4.2.2.2.2" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.cmml"><msub id="S3.E2.m1.1.1.1.1.4.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.3.cmml"><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.4.2.2.2.2.3.2" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.3.2.cmml">Î¦</mi><mi id="S3.E2.m1.1.1.1.1.4.2.2.2.2.3.3" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.3.3.cmml">cls</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.4.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.2" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.2" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.2.cmml">r</mi><mi id="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.3" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.3" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.4.2.2.2.5" xref="S3.E2.m1.1.1.1.1.4.2.2.3.cmml">}</mo></mrow><mrow id="S3.E2.m1.1.1.1.1.4.2.4" xref="S3.E2.m1.1.1.1.1.4.2.4.cmml"><mi id="S3.E2.m1.1.1.1.1.4.2.4.2" xref="S3.E2.m1.1.1.1.1.4.2.4.2.cmml">i</mi><mo id="S3.E2.m1.1.1.1.1.4.2.4.1" xref="S3.E2.m1.1.1.1.1.4.2.4.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.4.2.4.3" xref="S3.E2.m1.1.1.1.1.4.2.4.3.cmml">1</mn></mrow><mi id="S3.E2.m1.1.1.1.1.4.4" xref="S3.E2.m1.1.1.1.1.4.4.cmml">M</mi></msubsup></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.5"></eq><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2">subscript</csymbol><set id="S3.E2.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3">ğ‘–</ci></apply></set><apply id="S3.E2.m1.1.1.1.1.2.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4"><eq id="S3.E2.m1.1.1.1.1.2.2.4.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4.1"></eq><ci id="S3.E2.m1.1.1.1.1.2.2.4.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4.2">ğ‘–</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.2.2.4.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.1.1.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.4">ğ‘€</ci></apply><apply id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.1.1.1.1.4">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.2.3.cmml" xref="S3.E2.m1.1.1.1.1.4">subscript</csymbol><set id="S3.E2.m1.1.1.1.1.4.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2"><apply id="S3.E2.m1.1.1.1.1.3.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.3.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.3.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.3.2">Î¦</ci><ci id="S3.E2.m1.1.1.1.1.3.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.3.3">box</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.2">ğ‘Ÿ</ci><ci id="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.4.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2"><times id="S3.E2.m1.1.1.1.1.4.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.2"></times><apply id="S3.E2.m1.1.1.1.1.4.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.2.2.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.4.2.2.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.3.2">Î¦</ci><ci id="S3.E2.m1.1.1.1.1.4.2.2.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.3.3">cls</ci></apply><apply id="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.2">ğ‘Ÿ</ci><ci id="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2.2.1.1.1.3">ğ‘–</ci></apply></apply></set><apply id="S3.E2.m1.1.1.1.1.4.2.4.cmml" xref="S3.E2.m1.1.1.1.1.4.2.4"><eq id="S3.E2.m1.1.1.1.1.4.2.4.1.cmml" xref="S3.E2.m1.1.1.1.1.4.2.4.1"></eq><ci id="S3.E2.m1.1.1.1.1.4.2.4.2.cmml" xref="S3.E2.m1.1.1.1.1.4.2.4.2">ğ‘–</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.4.2.4.3.cmml" xref="S3.E2.m1.1.1.1.1.4.2.4.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.1.1.4.4.cmml" xref="S3.E2.m1.1.1.1.1.4.4">ğ‘€</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle\{b_{i},c_{i}\}_{i=1}^{M}=\{\mathrm{\Phi}_{\mathrm{box}}(r_{i}),\mathrm{\Phi}_{\mathrm{cls}}(r_{i})\}_{i=1}^{M},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.11" class="ltx_p">where <math id="S3.SS1.p2.3.m1.1" class="ltx_Math" alttext="\mathrm{\Phi}_{\mathrm{Enc}}" display="inline"><semantics id="S3.SS1.p2.3.m1.1a"><msub id="S3.SS1.p2.3.m1.1.1" xref="S3.SS1.p2.3.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.3.m1.1.1.2" xref="S3.SS1.p2.3.m1.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p2.3.m1.1.1.3" xref="S3.SS1.p2.3.m1.1.1.3.cmml">Enc</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m1.1b"><apply id="S3.SS1.p2.3.m1.1.1.cmml" xref="S3.SS1.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m1.1.1.1.cmml" xref="S3.SS1.p2.3.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m1.1.1.2.cmml" xref="S3.SS1.p2.3.m1.1.1.2">Î¦</ci><ci id="S3.SS1.p2.3.m1.1.1.3.cmml" xref="S3.SS1.p2.3.m1.1.1.3">Enc</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m1.1c">\mathrm{\Phi}_{\mathrm{Enc}}</annotation></semantics></math> is an image encoderÂ <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In this paper, we ignore the Feature Pyramid Network (FPN)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for brevity.</span></span></span> that maps the input image <span id="S3.SS1.p2.11.1" class="ltx_text ltx_markedasmath">I</span> to a series of multi-scale feature maps. <math id="S3.SS1.p2.5.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS1.p2.5.m3.1a"><mi id="S3.SS1.p2.5.m3.1.1" xref="S3.SS1.p2.5.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m3.1b"><ci id="S3.SS1.p2.5.m3.1.1.cmml" xref="S3.SS1.p2.5.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m3.1c">M</annotation></semantics></math> is the number of proposals generated by <math id="S3.SS1.p2.6.m4.1" class="ltx_Math" alttext="\mathrm{\Phi}_{\mathrm{RPN}}" display="inline"><semantics id="S3.SS1.p2.6.m4.1a"><msub id="S3.SS1.p2.6.m4.1.1" xref="S3.SS1.p2.6.m4.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.6.m4.1.1.2" xref="S3.SS1.p2.6.m4.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p2.6.m4.1.1.3" xref="S3.SS1.p2.6.m4.1.1.3.cmml">RPN</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m4.1b"><apply id="S3.SS1.p2.6.m4.1.1.cmml" xref="S3.SS1.p2.6.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m4.1.1.1.cmml" xref="S3.SS1.p2.6.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m4.1.1.2.cmml" xref="S3.SS1.p2.6.m4.1.1.2">Î¦</ci><ci id="S3.SS1.p2.6.m4.1.1.3.cmml" xref="S3.SS1.p2.6.m4.1.1.3">RPN</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m4.1c">\mathrm{\Phi}_{\mathrm{RPN}}</annotation></semantics></math>. <math id="S3.SS1.p2.7.m5.1" class="ltx_Math" alttext="\mathrm{\Phi}_{\mathrm{RoI}}" display="inline"><semantics id="S3.SS1.p2.7.m5.1a"><msub id="S3.SS1.p2.7.m5.1.1" xref="S3.SS1.p2.7.m5.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.7.m5.1.1.2" xref="S3.SS1.p2.7.m5.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p2.7.m5.1.1.3" xref="S3.SS1.p2.7.m5.1.1.3.cmml">RoI</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m5.1b"><apply id="S3.SS1.p2.7.m5.1.1.cmml" xref="S3.SS1.p2.7.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m5.1.1.1.cmml" xref="S3.SS1.p2.7.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m5.1.1.2.cmml" xref="S3.SS1.p2.7.m5.1.1.2">Î¦</ci><ci id="S3.SS1.p2.7.m5.1.1.3.cmml" xref="S3.SS1.p2.7.m5.1.1.3">RoI</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m5.1c">\mathrm{\Phi}_{\mathrm{RoI}}</annotation></semantics></math> extracts the region embedding <math id="S3.SS1.p2.8.m6.1" class="ltx_Math" alttext="r_{i}" display="inline"><semantics id="S3.SS1.p2.8.m6.1a"><msub id="S3.SS1.p2.8.m6.1.1" xref="S3.SS1.p2.8.m6.1.1.cmml"><mi id="S3.SS1.p2.8.m6.1.1.2" xref="S3.SS1.p2.8.m6.1.1.2.cmml">r</mi><mi id="S3.SS1.p2.8.m6.1.1.3" xref="S3.SS1.p2.8.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m6.1b"><apply id="S3.SS1.p2.8.m6.1.1.cmml" xref="S3.SS1.p2.8.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m6.1.1.1.cmml" xref="S3.SS1.p2.8.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m6.1.1.2.cmml" xref="S3.SS1.p2.8.m6.1.1.2">ğ‘Ÿ</ci><ci id="S3.SS1.p2.8.m6.1.1.3.cmml" xref="S3.SS1.p2.8.m6.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m6.1c">r_{i}</annotation></semantics></math> from the feature maps given a box proposal. Then the box regression network <math id="S3.SS1.p2.9.m7.1" class="ltx_Math" alttext="\mathrm{\Phi}_{\mathrm{box}}" display="inline"><semantics id="S3.SS1.p2.9.m7.1a"><msub id="S3.SS1.p2.9.m7.1.1" xref="S3.SS1.p2.9.m7.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.9.m7.1.1.2" xref="S3.SS1.p2.9.m7.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p2.9.m7.1.1.3" xref="S3.SS1.p2.9.m7.1.1.3.cmml">box</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m7.1b"><apply id="S3.SS1.p2.9.m7.1.1.cmml" xref="S3.SS1.p2.9.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m7.1.1.1.cmml" xref="S3.SS1.p2.9.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.9.m7.1.1.2.cmml" xref="S3.SS1.p2.9.m7.1.1.2">Î¦</ci><ci id="S3.SS1.p2.9.m7.1.1.3.cmml" xref="S3.SS1.p2.9.m7.1.1.3">box</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m7.1c">\mathrm{\Phi}_{\mathrm{box}}</annotation></semantics></math> refines the coordinates of the bounding boxes, and the classification network <math id="S3.SS1.p2.10.m8.1" class="ltx_Math" alttext="\mathrm{\Phi}_{\mathrm{cls}}" display="inline"><semantics id="S3.SS1.p2.10.m8.1a"><msub id="S3.SS1.p2.10.m8.1.1" xref="S3.SS1.p2.10.m8.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.10.m8.1.1.2" xref="S3.SS1.p2.10.m8.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p2.10.m8.1.1.3" xref="S3.SS1.p2.10.m8.1.1.3.cmml">cls</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m8.1b"><apply id="S3.SS1.p2.10.m8.1.1.cmml" xref="S3.SS1.p2.10.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m8.1.1.1.cmml" xref="S3.SS1.p2.10.m8.1.1">subscript</csymbol><ci id="S3.SS1.p2.10.m8.1.1.2.cmml" xref="S3.SS1.p2.10.m8.1.1.2">Î¦</ci><ci id="S3.SS1.p2.10.m8.1.1.3.cmml" xref="S3.SS1.p2.10.m8.1.1.3">cls</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m8.1c">\mathrm{\Phi}_{\mathrm{cls}}</annotation></semantics></math> predicts the class label of the object within the bounding box. We use <math id="S3.SS1.p2.11.m9.1" class="ltx_Math" alttext="\circ" display="inline"><semantics id="S3.SS1.p2.11.m9.1a"><mo id="S3.SS1.p2.11.m9.1.1" xref="S3.SS1.p2.11.m9.1.1.cmml">âˆ˜</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m9.1b"><compose id="S3.SS1.p2.11.m9.1.1.cmml" xref="S3.SS1.p2.11.m9.1.1"></compose></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m9.1c">\circ</annotation></semantics></math> to represent the cascade of different components.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.4" class="ltx_p">The classification head is learnable in closed-vocabulary object detection and maps the region embedding into predefined classes.
However, in the open-vocabulary scenarioÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, the classifier is substituted with text embeddings generated by pre-trained VLMs (i.e., CLIP) and is frozen during the training. The text embedding <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="t_{c}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">t</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">ğ‘¡</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">t_{c}</annotation></semantics></math> for the <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">c</annotation></semantics></math>-th object category is generated by sending the category name into a CLIP text encoder using either a single template prompt, â€a photo of category,â€ or multiple template prompts. And for a region embedding <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">r</annotation></semantics></math>, its classification score of <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">c</annotation></semantics></math>-th category is calculated as follows:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_math_unparsed" alttext="p_{c}=\frac{\text{exp}(\tau\cdot&lt;r,t_{c}&gt;)}{\sum_{i=0}^{C}\text{exp}(\tau\cdot&lt;r,t_{i}&gt;)}" display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.3"><msub id="S3.E3.m1.2.3.2"><mi id="S3.E3.m1.2.3.2.2">p</mi><mi id="S3.E3.m1.2.3.2.3">c</mi></msub><mo id="S3.E3.m1.2.3.1">=</mo><mfrac id="S3.E3.m1.2.2"><mrow id="S3.E3.m1.1.1.1"><mtext id="S3.E3.m1.1.1.1.2">exp</mtext><mrow id="S3.E3.m1.1.1.1.3"><mo stretchy="false" id="S3.E3.m1.1.1.1.3.1">(</mo><mi id="S3.E3.m1.1.1.1.3.2">Ï„</mi><mo lspace="0.222em" rspace="0em" id="S3.E3.m1.1.1.1.3.3">â‹…</mo><mo lspace="0em" id="S3.E3.m1.1.1.1.3.4">&lt;</mo><mi id="S3.E3.m1.1.1.1.1">r</mi><mo id="S3.E3.m1.1.1.1.3.5">,</mo><msub id="S3.E3.m1.1.1.1.3.6"><mi id="S3.E3.m1.1.1.1.3.6.2">t</mi><mi id="S3.E3.m1.1.1.1.3.6.3">c</mi></msub><mo rspace="0em" id="S3.E3.m1.1.1.1.3.7">&gt;</mo><mo stretchy="false" id="S3.E3.m1.1.1.1.3.8">)</mo></mrow></mrow><mrow id="S3.E3.m1.2.2.2"><msubsup id="S3.E3.m1.2.2.2.2"><mo id="S3.E3.m1.2.2.2.2.2.2">âˆ‘</mo><mrow id="S3.E3.m1.2.2.2.2.2.3"><mi id="S3.E3.m1.2.2.2.2.2.3.2">i</mi><mo id="S3.E3.m1.2.2.2.2.2.3.1">=</mo><mn id="S3.E3.m1.2.2.2.2.2.3.3">0</mn></mrow><mi id="S3.E3.m1.2.2.2.2.3">C</mi></msubsup><mtext id="S3.E3.m1.2.2.2.3">exp</mtext><mrow id="S3.E3.m1.2.2.2.4"><mo stretchy="false" id="S3.E3.m1.2.2.2.4.1">(</mo><mi id="S3.E3.m1.2.2.2.4.2">Ï„</mi><mo lspace="0.222em" rspace="0em" id="S3.E3.m1.2.2.2.4.3">â‹…</mo><mo lspace="0em" id="S3.E3.m1.2.2.2.4.4">&lt;</mo><mi id="S3.E3.m1.2.2.2.1">r</mi><mo id="S3.E3.m1.2.2.2.4.5">,</mo><msub id="S3.E3.m1.2.2.2.4.6"><mi id="S3.E3.m1.2.2.2.4.6.2">t</mi><mi id="S3.E3.m1.2.2.2.4.6.3">i</mi></msub><mo rspace="0em" id="S3.E3.m1.2.2.2.4.7">&gt;</mo><mo stretchy="false" id="S3.E3.m1.2.2.2.4.8">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex" id="S3.E3.m1.2b">p_{c}=\frac{\text{exp}(\tau\cdot&lt;r,t_{c}&gt;)}{\sum_{i=0}^{C}\text{exp}(\tau\cdot&lt;r,t_{i}&gt;)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.6" class="ltx_p">where <math id="S3.SS1.p3.5.m1.1" class="ltx_math_unparsed" alttext="&lt;\cdot,\cdot&gt;" display="inline"><semantics id="S3.SS1.p3.5.m1.1a"><mrow id="S3.SS1.p3.5.m1.1b"><mo rspace="0em" id="S3.SS1.p3.5.m1.1.1">&lt;</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p3.5.m1.1.2">â‹…</mo><mo rspace="0em" id="S3.SS1.p3.5.m1.1.3">,</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p3.5.m1.1.4">â‹…</mo><mo lspace="0em" id="S3.SS1.p3.5.m1.1.5">&gt;</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m1.1c">&lt;\cdot,\cdot&gt;</annotation></semantics></math> is the cosine similarity, and <math id="S3.SS1.p3.6.m2.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS1.p3.6.m2.1a"><mi id="S3.SS1.p3.6.m2.1.1" xref="S3.SS1.p3.6.m2.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m2.1b"><ci id="S3.SS1.p3.6.m2.1.1.cmml" xref="S3.SS1.p3.6.m2.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m2.1c">\tau</annotation></semantics></math> is a learnable or fixed temperature to re-scale the value.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.4" class="ltx_p"><span id="S3.SS1.p4.4.1" class="ltx_text ltx_font_bold">Frozen CLIP as Backbone.</span>

To reduce computation cost and strengthen the open-vocabulary ability of the object detector, we use the CLIP image encoder as the detector backbone in <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mathrm{\Phi}_{\mathrm{Enc}}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><msub id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">Enc</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">Î¦</ci><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">Enc</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\mathrm{\Phi}_{\mathrm{Enc}}</annotation></semantics></math>. We keep the parameters of our backbone fixed to preserve the vision-language alignment during training following F-VLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Then, the inference stage of the detector can benefit from both the detection score described in Eq.Â <a href="#S3.E3" title="Equation 3 â€£ 3.1 Preliminaries â€£ 3 Method â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and the CLIP score obtained by replacing the region embedding in Eq.Â <a href="#S3.E3" title="Equation 3 â€£ 3.1 Preliminaries â€£ 3 Method â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> with the CLIP representation of the corresponding region proposal as depicted in Fig.Â <a href="#S3.F3" title="Figure 3 â€£ 3.1 Preliminaries â€£ 3 Method â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(c). Specifically, we apply the RoIAlign function <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="\mathrm{\Phi}_{\mathrm{RoI}}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml">RoI</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">Î¦</ci><ci id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">RoI</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\mathrm{\Phi}_{\mathrm{RoI}}</annotation></semantics></math> to the top-level feature map of <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="\mathrm{\Phi}_{\mathrm{Enc}}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><msub id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml">Enc</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">Î¦</ci><ci id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3">Enc</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\mathrm{\Phi}_{\mathrm{Enc}}</annotation></semantics></math>. Then, the CLIP representations of the region proposals are obtained by pooling the outputs of the RoIAlign via the attention pooling module of the CLIP image encoder. Given a region proposal during the inference, the score of the <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mi id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><ci id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">c</annotation></semantics></math>-th candidate category is obtained by merging the two types of scores via geometric mean:</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="s_{c}=\begin{cases}p_{c}^{1-\alpha}\cdot q_{c}^{\alpha}\quad\text{if }i\in C_{B}\\
p_{c}^{1-\beta}\cdot q_{c}^{\beta}\quad\text{if }i\in C_{N}\end{cases}" display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.3" xref="S3.E4.m1.2.3.cmml"><msub id="S3.E4.m1.2.3.2" xref="S3.E4.m1.2.3.2.cmml"><mi id="S3.E4.m1.2.3.2.2" xref="S3.E4.m1.2.3.2.2.cmml">s</mi><mi id="S3.E4.m1.2.3.2.3" xref="S3.E4.m1.2.3.2.3.cmml">c</mi></msub><mo id="S3.E4.m1.2.3.1" xref="S3.E4.m1.2.3.1.cmml">=</mo><mrow id="S3.E4.m1.2.2" xref="S3.E4.m1.2.3.3.1.cmml"><mo id="S3.E4.m1.2.2.3" xref="S3.E4.m1.2.3.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.3.3.1.cmml"><mtr id="S3.E4.m1.2.2.2a" xref="S3.E4.m1.2.3.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.2.2.2b" xref="S3.E4.m1.2.3.3.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.2.3.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">p</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">c</mi><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mn id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">1</mn><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">âˆ’</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">Î±</mi></mrow></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml">â‹…</mo><msubsup id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">q</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">c</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">Î±</mi></msubsup></mrow><mspace width="1em" id="S3.E4.m1.1.1.1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.1.1.1.2.3.cmml"></mspace><mrow id="S3.E4.m1.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2.cmml"><mtext id="S3.E4.m1.1.1.1.1.1.1.2.2.2.2" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2.2a.cmml">ifÂ </mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2.2.2.1" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2.1.cmml">â€‹</mo><mi id="S3.E4.m1.1.1.1.1.1.1.2.2.2.3" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi></mrow></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.3.cmml">âˆˆ</mo><msub id="S3.E4.m1.1.1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.1.1.4.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.4.2" xref="S3.E4.m1.1.1.1.1.1.1.4.2.cmml">C</mi><mi id="S3.E4.m1.1.1.1.1.1.1.4.3" xref="S3.E4.m1.1.1.1.1.1.1.4.3.cmml">B</mi></msub></mrow></mtd><mtd id="S3.E4.m1.2.2.2c" xref="S3.E4.m1.2.3.3.1.1.cmml"></mtd></mtr><mtr id="S3.E4.m1.2.2.2d" xref="S3.E4.m1.2.3.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.2.2.2e" xref="S3.E4.m1.2.3.3.1.cmml"><mrow id="S3.E4.m1.2.2.2.2.1.1" xref="S3.E4.m1.2.2.2.2.1.1.cmml"><mrow id="S3.E4.m1.2.2.2.2.1.1.2.2" xref="S3.E4.m1.2.2.2.2.1.1.2.3.cmml"><mrow id="S3.E4.m1.2.2.2.2.1.1.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.cmml"><msubsup id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.2.2" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.2.2.cmml">p</mi><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.2.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.2.3.cmml">c</mi><mrow id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.cmml"><mn id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.2" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.2.cmml">1</mn><mo id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.1" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.1.cmml">âˆ’</mo><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.3.cmml">Î²</mi></mrow></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.2.2.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.1.cmml">â‹…</mo><msubsup id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.2" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.2.cmml">q</mi><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.3.cmml">c</mi><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3.cmml">Î²</mi></msubsup></mrow><mspace width="1em" id="S3.E4.m1.2.2.2.2.1.1.2.2.3" xref="S3.E4.m1.2.2.2.2.1.1.2.3.cmml"></mspace><mrow id="S3.E4.m1.2.2.2.2.1.1.2.2.2" xref="S3.E4.m1.2.2.2.2.1.1.2.2.2.cmml"><mtext id="S3.E4.m1.2.2.2.2.1.1.2.2.2.2" xref="S3.E4.m1.2.2.2.2.1.1.2.2.2.2a.cmml">ifÂ </mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.2.2.1.1.2.2.2.1" xref="S3.E4.m1.2.2.2.2.1.1.2.2.2.1.cmml">â€‹</mo><mi id="S3.E4.m1.2.2.2.2.1.1.2.2.2.3" xref="S3.E4.m1.2.2.2.2.1.1.2.2.2.3.cmml">i</mi></mrow></mrow><mo id="S3.E4.m1.2.2.2.2.1.1.3" xref="S3.E4.m1.2.2.2.2.1.1.3.cmml">âˆˆ</mo><msub id="S3.E4.m1.2.2.2.2.1.1.4" xref="S3.E4.m1.2.2.2.2.1.1.4.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.4.2" xref="S3.E4.m1.2.2.2.2.1.1.4.2.cmml">C</mi><mi id="S3.E4.m1.2.2.2.2.1.1.4.3" xref="S3.E4.m1.2.2.2.2.1.1.4.3.cmml">N</mi></msub></mrow></mtd><mtd id="S3.E4.m1.2.2.2f" xref="S3.E4.m1.2.3.3.1.1.cmml"></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.3.cmml" xref="S3.E4.m1.2.3"><eq id="S3.E4.m1.2.3.1.cmml" xref="S3.E4.m1.2.3.1"></eq><apply id="S3.E4.m1.2.3.2.cmml" xref="S3.E4.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.3.2.1.cmml" xref="S3.E4.m1.2.3.2">subscript</csymbol><ci id="S3.E4.m1.2.3.2.2.cmml" xref="S3.E4.m1.2.3.2.2">ğ‘ </ci><ci id="S3.E4.m1.2.3.2.3.cmml" xref="S3.E4.m1.2.3.2.3">ğ‘</ci></apply><apply id="S3.E4.m1.2.3.3.1.cmml" xref="S3.E4.m1.2.2"><csymbol cd="latexml" id="S3.E4.m1.2.3.3.1.1.cmml" xref="S3.E4.m1.2.2.3">cases</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><in id="S3.E4.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3"></in><list id="S3.E4.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2"><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1"><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1">â‹…</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.2">ğ‘</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.3">ğ‘</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3"><minus id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.1"></minus><cn type="integer" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2">1</cn><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3">ğ›¼</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2">ğ‘</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3">ğ‘</ci></apply><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3">ğ›¼</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2"><times id="S3.E4.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2.1"></times><ci id="S3.E4.m1.1.1.1.1.1.1.2.2.2.2a.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2.2"><mtext id="S3.E4.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2.2">ifÂ </mtext></ci><ci id="S3.E4.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2.3">ğ‘–</ci></apply></list><apply id="S3.E4.m1.1.1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.4.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.4.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.4.2">ğ¶</ci><ci id="S3.E4.m1.1.1.1.1.1.1.4.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.4.3">ğµ</ci></apply></apply><ci id="S3.E4.m1.2.3.3.1.3a.cmml" xref="S3.E4.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E4.m1.2.3.3.1.3.cmml" xref="S3.E4.m1.2.2.3">otherwise</mtext></ci><apply id="S3.E4.m1.2.2.2.2.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1"><in id="S3.E4.m1.2.2.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3"></in><list id="S3.E4.m1.2.2.2.2.1.1.2.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2.2"><apply id="S3.E4.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1"><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.1">â‹…</ci><apply id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.2.2">ğ‘</ci><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.2.3">ğ‘</ci></apply><apply id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3"><minus id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.1"></minus><cn type="integer" id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.2">1</cn><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.3.3">ğ›½</ci></apply></apply><apply id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.2">ğ‘</ci><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.3">ğ‘</ci></apply><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3">ğ›½</ci></apply></apply><apply id="S3.E4.m1.2.2.2.2.1.1.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2.2.2"><times id="S3.E4.m1.2.2.2.2.1.1.2.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2.2.2.1"></times><ci id="S3.E4.m1.2.2.2.2.1.1.2.2.2.2a.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2.2.2.2"><mtext id="S3.E4.m1.2.2.2.2.1.1.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2.2.2.2">ifÂ </mtext></ci><ci id="S3.E4.m1.2.2.2.2.1.1.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2.2.2.3">ğ‘–</ci></apply></list><apply id="S3.E4.m1.2.2.2.2.1.1.4.cmml" xref="S3.E4.m1.2.2.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.4.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.4">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.1.4.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.4.2">ğ¶</ci><ci id="S3.E4.m1.2.2.2.2.1.1.4.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.4.3">ğ‘</ci></apply></apply><ci id="S3.E4.m1.2.3.3.1.5a.cmml" xref="S3.E4.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E4.m1.2.3.3.1.5.cmml" xref="S3.E4.m1.2.2.3">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">s_{c}=\begin{cases}p_{c}^{1-\alpha}\cdot q_{c}^{\alpha}\quad\text{if }i\in C_{B}\\
p_{c}^{1-\beta}\cdot q_{c}^{\beta}\quad\text{if }i\in C_{N}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p5.4" class="ltx_p">where <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="q_{c}" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><msub id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml"><mi id="S3.SS1.p5.1.m1.1.1.2" xref="S3.SS1.p5.1.m1.1.1.2.cmml">q</mi><mi id="S3.SS1.p5.1.m1.1.1.3" xref="S3.SS1.p5.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><apply id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p5.1.m1.1.1.2.cmml" xref="S3.SS1.p5.1.m1.1.1.2">ğ‘</ci><ci id="S3.SS1.p5.1.m1.1.1.3.cmml" xref="S3.SS1.p5.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">q_{c}</annotation></semantics></math> is the CLIP score, <math id="S3.SS1.p5.2.m2.4" class="ltx_Math" alttext="\alpha,\beta\in[0,1]" display="inline"><semantics id="S3.SS1.p5.2.m2.4a"><mrow id="S3.SS1.p5.2.m2.4.5" xref="S3.SS1.p5.2.m2.4.5.cmml"><mrow id="S3.SS1.p5.2.m2.4.5.2.2" xref="S3.SS1.p5.2.m2.4.5.2.1.cmml"><mi id="S3.SS1.p5.2.m2.3.3" xref="S3.SS1.p5.2.m2.3.3.cmml">Î±</mi><mo id="S3.SS1.p5.2.m2.4.5.2.2.1" xref="S3.SS1.p5.2.m2.4.5.2.1.cmml">,</mo><mi id="S3.SS1.p5.2.m2.4.4" xref="S3.SS1.p5.2.m2.4.4.cmml">Î²</mi></mrow><mo id="S3.SS1.p5.2.m2.4.5.1" xref="S3.SS1.p5.2.m2.4.5.1.cmml">âˆˆ</mo><mrow id="S3.SS1.p5.2.m2.4.5.3.2" xref="S3.SS1.p5.2.m2.4.5.3.1.cmml"><mo stretchy="false" id="S3.SS1.p5.2.m2.4.5.3.2.1" xref="S3.SS1.p5.2.m2.4.5.3.1.cmml">[</mo><mn id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">0</mn><mo id="S3.SS1.p5.2.m2.4.5.3.2.2" xref="S3.SS1.p5.2.m2.4.5.3.1.cmml">,</mo><mn id="S3.SS1.p5.2.m2.2.2" xref="S3.SS1.p5.2.m2.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS1.p5.2.m2.4.5.3.2.3" xref="S3.SS1.p5.2.m2.4.5.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.4b"><apply id="S3.SS1.p5.2.m2.4.5.cmml" xref="S3.SS1.p5.2.m2.4.5"><in id="S3.SS1.p5.2.m2.4.5.1.cmml" xref="S3.SS1.p5.2.m2.4.5.1"></in><list id="S3.SS1.p5.2.m2.4.5.2.1.cmml" xref="S3.SS1.p5.2.m2.4.5.2.2"><ci id="S3.SS1.p5.2.m2.3.3.cmml" xref="S3.SS1.p5.2.m2.3.3">ğ›¼</ci><ci id="S3.SS1.p5.2.m2.4.4.cmml" xref="S3.SS1.p5.2.m2.4.4">ğ›½</ci></list><interval closure="closed" id="S3.SS1.p5.2.m2.4.5.3.1.cmml" xref="S3.SS1.p5.2.m2.4.5.3.2"><cn type="integer" id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">0</cn><cn type="integer" id="S3.SS1.p5.2.m2.2.2.cmml" xref="S3.SS1.p5.2.m2.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.4c">\alpha,\beta\in[0,1]</annotation></semantics></math> control the weights of the CLIP scores for base and novel classes, and <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="C_{B}" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><msub id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml"><mi id="S3.SS1.p5.3.m3.1.1.2" xref="S3.SS1.p5.3.m3.1.1.2.cmml">C</mi><mi id="S3.SS1.p5.3.m3.1.1.3" xref="S3.SS1.p5.3.m3.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><apply id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.3.m3.1.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p5.3.m3.1.1.2.cmml" xref="S3.SS1.p5.3.m3.1.1.2">ğ¶</ci><ci id="S3.SS1.p5.3.m3.1.1.3.cmml" xref="S3.SS1.p5.3.m3.1.1.3">ğµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">C_{B}</annotation></semantics></math> and <math id="S3.SS1.p5.4.m4.1" class="ltx_Math" alttext="C_{N}" display="inline"><semantics id="S3.SS1.p5.4.m4.1a"><msub id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml"><mi id="S3.SS1.p5.4.m4.1.1.2" xref="S3.SS1.p5.4.m4.1.1.2.cmml">C</mi><mi id="S3.SS1.p5.4.m4.1.1.3" xref="S3.SS1.p5.4.m4.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><apply id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.4.m4.1.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p5.4.m4.1.1.2.cmml" xref="S3.SS1.p5.4.m4.1.1.2">ğ¶</ci><ci id="S3.SS1.p5.4.m4.1.1.3.cmml" xref="S3.SS1.p5.4.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">C_{N}</annotation></semantics></math> represent the base classes and novel classes, respectively.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2310.01393/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="475" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.5.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.6.2" class="ltx_text" style="font-size:90%;">Illustration of DST framework. <span id="S3.F3.6.2.1" class="ltx_text ltx_font_bold">(a)</span> The meta-architecture of DST-Det. <span id="S3.F3.6.2.2" class="ltx_text ltx_font_bold">(b)</span> The proposed pseudo-labeling module (PLM). The PLM is inserted into the two stages of the detector, including the RPN and RoIHead. During training, PLM takes the top-level feature map from the CLIP image encoder and text embedding of object classes as input and generates the pseudo labels for the RPN and the RoIHead. <span id="S3.F3.6.2.3" class="ltx_text ltx_font_bold">(c)</span> The process of extracting CLIP representation for region proposals. The RoIAlign operation is applied to the top-level feature map, the output of which is then pooled by the Attention Pooling layer (AttnPooling) of the CLIP image encoder. </span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>DST-Det: Dynamic Self-Training For OVOD</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Motivation of DST-Det.</span>
Previous works have addressed the problem of open-ended classification in OVOD by utilizing the text embeddings from pre-trained VLMs (e.g., CLIP). However, a conceptual gap still exists between training and testing for novel classes. As shown in Fig.Â <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, annotations for novel objects are considered background during training. However, during the test phase, they are expected to be detected as foreground and classified into a specific novel class based on their detection score.
A new training strategy is needed to bridge the gap between the training and test phases by utilizing CLIPâ€™s outputs as pseudo labels for novel classes.
In the toy experiments depicted in Fig.Â <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b), we treat the ground-truth bounding boxes as region proposals and get the CLIP score for each region proposal, and use the CLIP scores to verify the zero-shot classification ability of CLIP on OV-COCO and OV-LVIS datasets. How to get the CLIP score has been illustrated in Sec.Â <a href="#S3.SS1" title="3.1 Preliminaries â€£ 3 Method â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> and Fig.Â <a href="#S3.F3" title="Figure 3 â€£ 3.1 Preliminaries â€£ 3 Method â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(c). The results indicate that the top-5 accuracy suffices for discovering novel classes, which motivates us to consider using CLIP outputs as pseudo labels during training, given the RPN proposals and a large pre-defined vocabulary.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.2" class="ltx_p"><span id="S3.SS2.p2.2.1" class="ltx_text ltx_font_bold">Pseudo-Labeling Module (PLM).</span>
We believe that negative proposals, which have a low overlap with the ground truth boxes, may contain objects of potential novel classes. Therefore, we introduce the Pseudo-Labeling Module (PLM) to avoid treating these proposals as background. As shown in Fig.Â <a href="#S3.F3" title="Figure 3 â€£ 3.1 Preliminaries â€£ 3 Method â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b), we first extract the CLIP representations (named as region embeddings in the figure) of the negative proposals using the approach depicted in Fig.Â <a href="#S3.F3" title="Figure 3 â€£ 3.1 Preliminaries â€£ 3 Method â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(c). Then, we calculate the cosine similarity scores between the region embeddings and the text embeddings of the object class names. After obtaining the similarity scores, we filter out those proposals classified as base classes or background classes and those with CLIP scores lower than a threshold. The remaining few proposals can be identified as novel objects, and we randomly select <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{K}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi mathvariant="normal" id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">K</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathrm{K}</annotation></semantics></math> proposals as pseudo labels and change the classification target of these proposals during training. We typically set <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathrm{K}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi mathvariant="normal" id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">K</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\mathrm{K}</annotation></semantics></math> to 4 by the trade-off of computation and final results. The targets of the positive proposals remain unchanged in this process. More details can be found in the appendix.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Deployment on RPN and RoIHead.</span>
During the RPN stage, it typically produces numerous negative proposals due to the dense anchor head. To reduce the computation cost of the PLM, we leverage Non-Maximum Suppression (NMS) to eliminate redundant proposals and limit the number of negative proposals to a fixed amount, such as 1000.
In contrast, there are fewer negative proposals during the RoIHead stage than in the RPN stage. Therefore, we send all negative proposals to the PLM. The PLM will change the target of some negative proposals. We convert the classification target from background to foreground for the RPN head, a class-agnostic region proposal network. For the classification branch of RoIHead, we change the classification target of negative proposals from background to pseudo labels generated by the PLM. Note that we only apply the pseudo labels produced by PLM to classification losses. The box regression and mask losses are unchanged.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Offline Refinement For Self-Training.</span> Moreover, we propose an offline self-training strategy for OVOD in addition to dynamic self-training. This strategy involves using the trained model to predict the training dataset. The prediction for novel classes with high classification scores will be saved as pseudo labels. The pseudo labels with origin base annotations in the training set will be served as new training sets. The offline refinement works like the teacher-student model. It is only applied after the detector is trained. The generated high-quality pseudo labels are dumped as new supervision. In Sec.Â <a href="#S4.SS2" title="4.2 Ablation Study and Analysis â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we present evidence demonstrating the effectiveness of both approaches. Both PLMs and offline refinement serve as two components of our DST framework.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Discussion.</span> The proposed PLM is only used during the training without introducing extra parameters and computation costs during the inference. The pseudo-labeling process requires a vocabulary that contains potential novel object classes. We can either obtain the novel classes from the detection datasets following DeticÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> or collect a wider range of object classes from external sources, e.g., the classes defined in image classification datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. We validate the effectiveness of both choices in TableÂ <a href="#S4.F3.sf6" title="Figure 3(f) â€£ Table 3 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(f)</span></a>. Moreover, the PLM training is single-stage and end-to-end. We list the detailed progress in Algo.Â <a href="#alg1" title="Algorithm 1 â€£ 3.2 DST-Det: Dynamic Self-Training For OVOD â€£ 3 Method â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> table.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.9.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Dynamic Self-Pseudo-Labeling Process</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="alg1.7" class="ltx_p ltx_figure_panel"><span id="alg1.7.1" class="ltx_text ltx_font_bold">Input</span>: negative proposal set <math id="alg1.1.m1.1" class="ltx_Math" alttext="P_{N}" display="inline"><semantics id="alg1.1.m1.1a"><msub id="alg1.1.m1.1.1" xref="alg1.1.m1.1.1.cmml"><mi id="alg1.1.m1.1.1.2" xref="alg1.1.m1.1.1.2.cmml">P</mi><mi id="alg1.1.m1.1.1.3" xref="alg1.1.m1.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.1.m1.1b"><apply id="alg1.1.m1.1.1.cmml" xref="alg1.1.m1.1.1"><csymbol cd="ambiguous" id="alg1.1.m1.1.1.1.cmml" xref="alg1.1.m1.1.1">subscript</csymbol><ci id="alg1.1.m1.1.1.2.cmml" xref="alg1.1.m1.1.1.2">ğ‘ƒ</ci><ci id="alg1.1.m1.1.1.3.cmml" xref="alg1.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.1.m1.1c">P_{N}</annotation></semantics></math>, score <math id="alg1.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="alg1.2.m2.1a"><mi id="alg1.2.m2.1.1" xref="alg1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="alg1.2.m2.1b"><ci id="alg1.2.m2.1.1.cmml" xref="alg1.2.m2.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.2.m2.1c">S</annotation></semantics></math> of <math id="alg1.3.m3.1" class="ltx_Math" alttext="P_{N}" display="inline"><semantics id="alg1.3.m3.1a"><msub id="alg1.3.m3.1.1" xref="alg1.3.m3.1.1.cmml"><mi id="alg1.3.m3.1.1.2" xref="alg1.3.m3.1.1.2.cmml">P</mi><mi id="alg1.3.m3.1.1.3" xref="alg1.3.m3.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.3.m3.1b"><apply id="alg1.3.m3.1.1.cmml" xref="alg1.3.m3.1.1"><csymbol cd="ambiguous" id="alg1.3.m3.1.1.1.cmml" xref="alg1.3.m3.1.1">subscript</csymbol><ci id="alg1.3.m3.1.1.2.cmml" xref="alg1.3.m3.1.1.2">ğ‘ƒ</ci><ci id="alg1.3.m3.1.1.3.cmml" xref="alg1.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.3.m3.1c">P_{N}</annotation></semantics></math>, top-level features <math id="alg1.4.m4.1" class="ltx_Math" alttext="F" display="inline"><semantics id="alg1.4.m4.1a"><mi id="alg1.4.m4.1.1" xref="alg1.4.m4.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="alg1.4.m4.1b"><ci id="alg1.4.m4.1.1.cmml" xref="alg1.4.m4.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.4.m4.1c">F</annotation></semantics></math> from frozen backbone, text embeddings <math id="alg1.5.m5.1" class="ltx_Math" alttext="t" display="inline"><semantics id="alg1.5.m5.1a"><mi id="alg1.5.m5.1.1" xref="alg1.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="alg1.5.m5.1b"><ci id="alg1.5.m5.1.1.cmml" xref="alg1.5.m5.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.5.m5.1c">t</annotation></semantics></math> for all categories, classification label <math id="alg1.6.m6.1" class="ltx_Math" alttext="L_{cls}" display="inline"><semantics id="alg1.6.m6.1a"><msub id="alg1.6.m6.1.1" xref="alg1.6.m6.1.1.cmml"><mi id="alg1.6.m6.1.1.2" xref="alg1.6.m6.1.1.2.cmml">L</mi><mrow id="alg1.6.m6.1.1.3" xref="alg1.6.m6.1.1.3.cmml"><mi id="alg1.6.m6.1.1.3.2" xref="alg1.6.m6.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg1.6.m6.1.1.3.1" xref="alg1.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="alg1.6.m6.1.1.3.3" xref="alg1.6.m6.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.6.m6.1.1.3.1a" xref="alg1.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="alg1.6.m6.1.1.3.4" xref="alg1.6.m6.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="alg1.6.m6.1b"><apply id="alg1.6.m6.1.1.cmml" xref="alg1.6.m6.1.1"><csymbol cd="ambiguous" id="alg1.6.m6.1.1.1.cmml" xref="alg1.6.m6.1.1">subscript</csymbol><ci id="alg1.6.m6.1.1.2.cmml" xref="alg1.6.m6.1.1.2">ğ¿</ci><apply id="alg1.6.m6.1.1.3.cmml" xref="alg1.6.m6.1.1.3"><times id="alg1.6.m6.1.1.3.1.cmml" xref="alg1.6.m6.1.1.3.1"></times><ci id="alg1.6.m6.1.1.3.2.cmml" xref="alg1.6.m6.1.1.3.2">ğ‘</ci><ci id="alg1.6.m6.1.1.3.3.cmml" xref="alg1.6.m6.1.1.3.3">ğ‘™</ci><ci id="alg1.6.m6.1.1.3.4.cmml" xref="alg1.6.m6.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.6.m6.1c">L_{cls}</annotation></semantics></math> 
<br class="ltx_break">Â Â Â Â Â  <span id="alg1.7.2" class="ltx_text ltx_font_bold">Output</span>: pseudo classification label <math id="alg1.7.m7.1" class="ltx_Math" alttext="L^{\prime}_{cls}" display="inline"><semantics id="alg1.7.m7.1a"><msubsup id="alg1.7.m7.1.1" xref="alg1.7.m7.1.1.cmml"><mi id="alg1.7.m7.1.1.2.2" xref="alg1.7.m7.1.1.2.2.cmml">L</mi><mrow id="alg1.7.m7.1.1.3" xref="alg1.7.m7.1.1.3.cmml"><mi id="alg1.7.m7.1.1.3.2" xref="alg1.7.m7.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg1.7.m7.1.1.3.1" xref="alg1.7.m7.1.1.3.1.cmml">â€‹</mo><mi id="alg1.7.m7.1.1.3.3" xref="alg1.7.m7.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.7.m7.1.1.3.1a" xref="alg1.7.m7.1.1.3.1.cmml">â€‹</mo><mi id="alg1.7.m7.1.1.3.4" xref="alg1.7.m7.1.1.3.4.cmml">s</mi></mrow><mo id="alg1.7.m7.1.1.2.3" xref="alg1.7.m7.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="alg1.7.m7.1b"><apply id="alg1.7.m7.1.1.cmml" xref="alg1.7.m7.1.1"><csymbol cd="ambiguous" id="alg1.7.m7.1.1.1.cmml" xref="alg1.7.m7.1.1">subscript</csymbol><apply id="alg1.7.m7.1.1.2.cmml" xref="alg1.7.m7.1.1"><csymbol cd="ambiguous" id="alg1.7.m7.1.1.2.1.cmml" xref="alg1.7.m7.1.1">superscript</csymbol><ci id="alg1.7.m7.1.1.2.2.cmml" xref="alg1.7.m7.1.1.2.2">ğ¿</ci><ci id="alg1.7.m7.1.1.2.3.cmml" xref="alg1.7.m7.1.1.2.3">â€²</ci></apply><apply id="alg1.7.m7.1.1.3.cmml" xref="alg1.7.m7.1.1.3"><times id="alg1.7.m7.1.1.3.1.cmml" xref="alg1.7.m7.1.1.3.1"></times><ci id="alg1.7.m7.1.1.3.2.cmml" xref="alg1.7.m7.1.1.3.2">ğ‘</ci><ci id="alg1.7.m7.1.1.3.3.cmml" xref="alg1.7.m7.1.1.3.3">ğ‘™</ci><ci id="alg1.7.m7.1.1.3.4.cmml" xref="alg1.7.m7.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.7.m7.1c">L^{\prime}_{cls}</annotation></semantics></math></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="alg1.10" class="ltx_listing ltx_figure_panel ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span>candidates = []

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><span id="alg1.l2.2" class="ltx_text ltx_font_bold">for</span>Â <math id="alg1.l2.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="alg1.l2.m1.1a"><mi id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">i</annotation></semantics></math>, <math id="alg1.l2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="alg1.l2.m2.1a"><mi id="alg1.l2.m2.1.1" xref="alg1.l2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="alg1.l2.m2.1b"><ci id="alg1.l2.m2.1.1.cmml" xref="alg1.l2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m2.1c">p</annotation></semantics></math> <span id="alg1.l2.3" class="ltx_text ltx_font_bold">in</span> enumerate(<math id="alg1.l2.m3.1" class="ltx_Math" alttext="P_{N}" display="inline"><semantics id="alg1.l2.m3.1a"><msub id="alg1.l2.m3.1.1" xref="alg1.l2.m3.1.1.cmml"><mi id="alg1.l2.m3.1.1.2" xref="alg1.l2.m3.1.1.2.cmml">P</mi><mi id="alg1.l2.m3.1.1.3" xref="alg1.l2.m3.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l2.m3.1b"><apply id="alg1.l2.m3.1.1.cmml" xref="alg1.l2.m3.1.1"><csymbol cd="ambiguous" id="alg1.l2.m3.1.1.1.cmml" xref="alg1.l2.m3.1.1">subscript</csymbol><ci id="alg1.l2.m3.1.1.2.cmml" xref="alg1.l2.m3.1.1.2">ğ‘ƒ</ci><ci id="alg1.l2.m3.1.1.3.cmml" xref="alg1.l2.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m3.1c">P_{N}</annotation></semantics></math>)Â <span id="alg1.l2.4" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span>Â Â Â Â Â <span id="alg1.l3.2" class="ltx_text ltx_font_bold">if</span>Â <math id="alg1.l3.m1.1" class="ltx_Math" alttext="S_{i}&gt;0" display="inline"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><msub id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml"><mi id="alg1.l3.m1.1.1.2.2" xref="alg1.l3.m1.1.1.2.2.cmml">S</mi><mi id="alg1.l3.m1.1.1.2.3" xref="alg1.l3.m1.1.1.2.3.cmml">i</mi></msub><mo id="alg1.l3.m1.1.1.1" xref="alg1.l3.m1.1.1.1.cmml">&gt;</mo><mn id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><gt id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1"></gt><apply id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.2.1.cmml" xref="alg1.l3.m1.1.1.2">subscript</csymbol><ci id="alg1.l3.m1.1.1.2.2.cmml" xref="alg1.l3.m1.1.1.2.2">ğ‘†</ci><ci id="alg1.l3.m1.1.1.2.3.cmml" xref="alg1.l3.m1.1.1.2.3">ğ‘–</ci></apply><cn type="integer" id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">S_{i}&gt;0</annotation></semantics></math>Â <span id="alg1.l3.3" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span>Â Â Â Â Â Â Â Â Â <math id="alg1.l4.m1.2" class="ltx_Math" alttext="r_{i}\leftarrow\text{RoIAlign}(F,p)" display="inline"><semantics id="alg1.l4.m1.2a"><mrow id="alg1.l4.m1.2.3" xref="alg1.l4.m1.2.3.cmml"><msub id="alg1.l4.m1.2.3.2" xref="alg1.l4.m1.2.3.2.cmml"><mi id="alg1.l4.m1.2.3.2.2" xref="alg1.l4.m1.2.3.2.2.cmml">r</mi><mi id="alg1.l4.m1.2.3.2.3" xref="alg1.l4.m1.2.3.2.3.cmml">i</mi></msub><mo stretchy="false" id="alg1.l4.m1.2.3.1" xref="alg1.l4.m1.2.3.1.cmml">â†</mo><mrow id="alg1.l4.m1.2.3.3" xref="alg1.l4.m1.2.3.3.cmml"><mtext id="alg1.l4.m1.2.3.3.2" xref="alg1.l4.m1.2.3.3.2a.cmml">RoIAlign</mtext><mo lspace="0em" rspace="0em" id="alg1.l4.m1.2.3.3.1" xref="alg1.l4.m1.2.3.3.1.cmml">â€‹</mo><mrow id="alg1.l4.m1.2.3.3.3.2" xref="alg1.l4.m1.2.3.3.3.1.cmml"><mo stretchy="false" id="alg1.l4.m1.2.3.3.3.2.1" xref="alg1.l4.m1.2.3.3.3.1.cmml">(</mo><mi id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">F</mi><mo id="alg1.l4.m1.2.3.3.3.2.2" xref="alg1.l4.m1.2.3.3.3.1.cmml">,</mo><mi id="alg1.l4.m1.2.2" xref="alg1.l4.m1.2.2.cmml">p</mi><mo stretchy="false" id="alg1.l4.m1.2.3.3.3.2.3" xref="alg1.l4.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.2b"><apply id="alg1.l4.m1.2.3.cmml" xref="alg1.l4.m1.2.3"><ci id="alg1.l4.m1.2.3.1.cmml" xref="alg1.l4.m1.2.3.1">â†</ci><apply id="alg1.l4.m1.2.3.2.cmml" xref="alg1.l4.m1.2.3.2"><csymbol cd="ambiguous" id="alg1.l4.m1.2.3.2.1.cmml" xref="alg1.l4.m1.2.3.2">subscript</csymbol><ci id="alg1.l4.m1.2.3.2.2.cmml" xref="alg1.l4.m1.2.3.2.2">ğ‘Ÿ</ci><ci id="alg1.l4.m1.2.3.2.3.cmml" xref="alg1.l4.m1.2.3.2.3">ğ‘–</ci></apply><apply id="alg1.l4.m1.2.3.3.cmml" xref="alg1.l4.m1.2.3.3"><times id="alg1.l4.m1.2.3.3.1.cmml" xref="alg1.l4.m1.2.3.3.1"></times><ci id="alg1.l4.m1.2.3.3.2a.cmml" xref="alg1.l4.m1.2.3.3.2"><mtext id="alg1.l4.m1.2.3.3.2.cmml" xref="alg1.l4.m1.2.3.3.2">RoIAlign</mtext></ci><interval closure="open" id="alg1.l4.m1.2.3.3.3.1.cmml" xref="alg1.l4.m1.2.3.3.3.2"><ci id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">ğ¹</ci><ci id="alg1.l4.m1.2.2.cmml" xref="alg1.l4.m1.2.2">ğ‘</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.2c">r_{i}\leftarrow\text{RoIAlign}(F,p)</annotation></semantics></math> <span id="alg1.l4.2" class="ltx_text" style="color:#000000;">/* region embeddings */</span>

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span>Â Â Â Â Â Â Â Â Â <math id="alg1.l5.m1.2" class="ltx_Math" alttext="s_{i}\leftarrow\text{MatMul}(r_{i},t)" display="inline"><semantics id="alg1.l5.m1.2a"><mrow id="alg1.l5.m1.2.2" xref="alg1.l5.m1.2.2.cmml"><msub id="alg1.l5.m1.2.2.3" xref="alg1.l5.m1.2.2.3.cmml"><mi id="alg1.l5.m1.2.2.3.2" xref="alg1.l5.m1.2.2.3.2.cmml">s</mi><mi id="alg1.l5.m1.2.2.3.3" xref="alg1.l5.m1.2.2.3.3.cmml">i</mi></msub><mo stretchy="false" id="alg1.l5.m1.2.2.2" xref="alg1.l5.m1.2.2.2.cmml">â†</mo><mrow id="alg1.l5.m1.2.2.1" xref="alg1.l5.m1.2.2.1.cmml"><mtext id="alg1.l5.m1.2.2.1.3" xref="alg1.l5.m1.2.2.1.3a.cmml">MatMul</mtext><mo lspace="0em" rspace="0em" id="alg1.l5.m1.2.2.1.2" xref="alg1.l5.m1.2.2.1.2.cmml">â€‹</mo><mrow id="alg1.l5.m1.2.2.1.1.1" xref="alg1.l5.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="alg1.l5.m1.2.2.1.1.1.2" xref="alg1.l5.m1.2.2.1.1.2.cmml">(</mo><msub id="alg1.l5.m1.2.2.1.1.1.1" xref="alg1.l5.m1.2.2.1.1.1.1.cmml"><mi id="alg1.l5.m1.2.2.1.1.1.1.2" xref="alg1.l5.m1.2.2.1.1.1.1.2.cmml">r</mi><mi id="alg1.l5.m1.2.2.1.1.1.1.3" xref="alg1.l5.m1.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="alg1.l5.m1.2.2.1.1.1.3" xref="alg1.l5.m1.2.2.1.1.2.cmml">,</mo><mi id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">t</mi><mo stretchy="false" id="alg1.l5.m1.2.2.1.1.1.4" xref="alg1.l5.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.2b"><apply id="alg1.l5.m1.2.2.cmml" xref="alg1.l5.m1.2.2"><ci id="alg1.l5.m1.2.2.2.cmml" xref="alg1.l5.m1.2.2.2">â†</ci><apply id="alg1.l5.m1.2.2.3.cmml" xref="alg1.l5.m1.2.2.3"><csymbol cd="ambiguous" id="alg1.l5.m1.2.2.3.1.cmml" xref="alg1.l5.m1.2.2.3">subscript</csymbol><ci id="alg1.l5.m1.2.2.3.2.cmml" xref="alg1.l5.m1.2.2.3.2">ğ‘ </ci><ci id="alg1.l5.m1.2.2.3.3.cmml" xref="alg1.l5.m1.2.2.3.3">ğ‘–</ci></apply><apply id="alg1.l5.m1.2.2.1.cmml" xref="alg1.l5.m1.2.2.1"><times id="alg1.l5.m1.2.2.1.2.cmml" xref="alg1.l5.m1.2.2.1.2"></times><ci id="alg1.l5.m1.2.2.1.3a.cmml" xref="alg1.l5.m1.2.2.1.3"><mtext id="alg1.l5.m1.2.2.1.3.cmml" xref="alg1.l5.m1.2.2.1.3">MatMul</mtext></ci><interval closure="open" id="alg1.l5.m1.2.2.1.1.2.cmml" xref="alg1.l5.m1.2.2.1.1.1"><apply id="alg1.l5.m1.2.2.1.1.1.1.cmml" xref="alg1.l5.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l5.m1.2.2.1.1.1.1.1.cmml" xref="alg1.l5.m1.2.2.1.1.1.1">subscript</csymbol><ci id="alg1.l5.m1.2.2.1.1.1.1.2.cmml" xref="alg1.l5.m1.2.2.1.1.1.1.2">ğ‘Ÿ</ci><ci id="alg1.l5.m1.2.2.1.1.1.1.3.cmml" xref="alg1.l5.m1.2.2.1.1.1.1.3">ğ‘–</ci></apply><ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">ğ‘¡</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.2c">s_{i}\leftarrow\text{MatMul}(r_{i},t)</annotation></semantics></math>

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span>Â Â Â Â Â Â Â Â Â <span id="alg1.l6.2" class="ltx_text ltx_font_bold">if</span>Â <math id="alg1.l6.m1.2" class="ltx_Math" alttext="\text{max}(s_{i})&gt;\text{0.8 and argmax}(s_{i})\in\text{novel classes}" display="inline"><semantics id="alg1.l6.m1.2a"><mrow id="alg1.l6.m1.2.2" xref="alg1.l6.m1.2.2.cmml"><mrow id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml"><mtext id="alg1.l6.m1.1.1.1.3" xref="alg1.l6.m1.1.1.1.3a.cmml">max</mtext><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.1.2" xref="alg1.l6.m1.1.1.1.2.cmml">â€‹</mo><mrow id="alg1.l6.m1.1.1.1.1.1" xref="alg1.l6.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.l6.m1.1.1.1.1.1.2" xref="alg1.l6.m1.1.1.1.1.1.1.cmml">(</mo><msub id="alg1.l6.m1.1.1.1.1.1.1" xref="alg1.l6.m1.1.1.1.1.1.1.cmml"><mi id="alg1.l6.m1.1.1.1.1.1.1.2" xref="alg1.l6.m1.1.1.1.1.1.1.2.cmml">s</mi><mi id="alg1.l6.m1.1.1.1.1.1.1.3" xref="alg1.l6.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="alg1.l6.m1.1.1.1.1.1.3" xref="alg1.l6.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="alg1.l6.m1.2.2.4" xref="alg1.l6.m1.2.2.4.cmml">&gt;</mo><mrow id="alg1.l6.m1.2.2.2" xref="alg1.l6.m1.2.2.2.cmml"><mtext id="alg1.l6.m1.2.2.2.3" xref="alg1.l6.m1.2.2.2.3a.cmml">0.8 and argmax</mtext><mo lspace="0em" rspace="0em" id="alg1.l6.m1.2.2.2.2" xref="alg1.l6.m1.2.2.2.2.cmml">â€‹</mo><mrow id="alg1.l6.m1.2.2.2.1.1" xref="alg1.l6.m1.2.2.2.1.1.1.cmml"><mo stretchy="false" id="alg1.l6.m1.2.2.2.1.1.2" xref="alg1.l6.m1.2.2.2.1.1.1.cmml">(</mo><msub id="alg1.l6.m1.2.2.2.1.1.1" xref="alg1.l6.m1.2.2.2.1.1.1.cmml"><mi id="alg1.l6.m1.2.2.2.1.1.1.2" xref="alg1.l6.m1.2.2.2.1.1.1.2.cmml">s</mi><mi id="alg1.l6.m1.2.2.2.1.1.1.3" xref="alg1.l6.m1.2.2.2.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="alg1.l6.m1.2.2.2.1.1.3" xref="alg1.l6.m1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="alg1.l6.m1.2.2.5" xref="alg1.l6.m1.2.2.5.cmml">âˆˆ</mo><mtext id="alg1.l6.m1.2.2.6" xref="alg1.l6.m1.2.2.6a.cmml">novel classes</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.2b"><apply id="alg1.l6.m1.2.2.cmml" xref="alg1.l6.m1.2.2"><and id="alg1.l6.m1.2.2a.cmml" xref="alg1.l6.m1.2.2"></and><apply id="alg1.l6.m1.2.2b.cmml" xref="alg1.l6.m1.2.2"><gt id="alg1.l6.m1.2.2.4.cmml" xref="alg1.l6.m1.2.2.4"></gt><apply id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1"><times id="alg1.l6.m1.1.1.1.2.cmml" xref="alg1.l6.m1.1.1.1.2"></times><ci id="alg1.l6.m1.1.1.1.3a.cmml" xref="alg1.l6.m1.1.1.1.3"><mtext id="alg1.l6.m1.1.1.1.3.cmml" xref="alg1.l6.m1.1.1.1.3">max</mtext></ci><apply id="alg1.l6.m1.1.1.1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1.1.1">subscript</csymbol><ci id="alg1.l6.m1.1.1.1.1.1.1.2.cmml" xref="alg1.l6.m1.1.1.1.1.1.1.2">ğ‘ </ci><ci id="alg1.l6.m1.1.1.1.1.1.1.3.cmml" xref="alg1.l6.m1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="alg1.l6.m1.2.2.2.cmml" xref="alg1.l6.m1.2.2.2"><times id="alg1.l6.m1.2.2.2.2.cmml" xref="alg1.l6.m1.2.2.2.2"></times><ci id="alg1.l6.m1.2.2.2.3a.cmml" xref="alg1.l6.m1.2.2.2.3"><mtext id="alg1.l6.m1.2.2.2.3.cmml" xref="alg1.l6.m1.2.2.2.3">0.8 and argmax</mtext></ci><apply id="alg1.l6.m1.2.2.2.1.1.1.cmml" xref="alg1.l6.m1.2.2.2.1.1"><csymbol cd="ambiguous" id="alg1.l6.m1.2.2.2.1.1.1.1.cmml" xref="alg1.l6.m1.2.2.2.1.1">subscript</csymbol><ci id="alg1.l6.m1.2.2.2.1.1.1.2.cmml" xref="alg1.l6.m1.2.2.2.1.1.1.2">ğ‘ </ci><ci id="alg1.l6.m1.2.2.2.1.1.1.3.cmml" xref="alg1.l6.m1.2.2.2.1.1.1.3">ğ‘–</ci></apply></apply></apply><apply id="alg1.l6.m1.2.2c.cmml" xref="alg1.l6.m1.2.2"><in id="alg1.l6.m1.2.2.5.cmml" xref="alg1.l6.m1.2.2.5"></in><share href="#alg1.l6.m1.2.2.2.cmml" id="alg1.l6.m1.2.2d.cmml" xref="alg1.l6.m1.2.2"></share><ci id="alg1.l6.m1.2.2.6a.cmml" xref="alg1.l6.m1.2.2.6"><mtext id="alg1.l6.m1.2.2.6.cmml" xref="alg1.l6.m1.2.2.6">novel classes</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.2c">\text{max}(s_{i})&gt;\text{0.8 and argmax}(s_{i})\in\text{novel classes}</annotation></semantics></math>Â <span id="alg1.l6.3" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span>Â Â Â Â Â Â Â Â Â Â Â Â Â Â <math id="alg1.l7.m1.1" class="ltx_Math" alttext="\text{candidates}\leftarrow i" display="inline"><semantics id="alg1.l7.m1.1a"><mrow id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mtext id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2a.cmml">candidates</mtext><mo stretchy="false" id="alg1.l7.m1.1.1.1" xref="alg1.l7.m1.1.1.1.cmml">â†</mo><mi id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><ci id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1">â†</ci><ci id="alg1.l7.m1.1.1.2a.cmml" xref="alg1.l7.m1.1.1.2"><mtext id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2">candidates</mtext></ci><ci id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">\text{candidates}\leftarrow i</annotation></semantics></math>
Â Â Â Â Â Â Â Â Â Â Â Â Â Â 
</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span><math id="alg1.l8.m1.1" class="ltx_Math" alttext="\text{selected\_idx}\leftarrow\text{randomly select K RoIs from candidates}" display="inline"><semantics id="alg1.l8.m1.1a"><mrow id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml"><mtext id="alg1.l8.m1.1.1.2" xref="alg1.l8.m1.1.1.2a.cmml">selected_idx</mtext><mo stretchy="false" id="alg1.l8.m1.1.1.1" xref="alg1.l8.m1.1.1.1.cmml">â†</mo><mtext id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3a.cmml">randomly select K RoIs from candidates</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1"><ci id="alg1.l8.m1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1">â†</ci><ci id="alg1.l8.m1.1.1.2a.cmml" xref="alg1.l8.m1.1.1.2"><mtext id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2">selected_idx</mtext></ci><ci id="alg1.l8.m1.1.1.3a.cmml" xref="alg1.l8.m1.1.1.3"><mtext id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3">randomly select K RoIs from candidates</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">\text{selected\_idx}\leftarrow\text{randomly select K RoIs from candidates}</annotation></semantics></math>

</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span><math id="alg1.l9.m1.1" class="ltx_Math" alttext="L^{\prime}_{cls}\leftarrow L_{cls}" display="inline"><semantics id="alg1.l9.m1.1a"><mrow id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml"><msubsup id="alg1.l9.m1.1.1.2" xref="alg1.l9.m1.1.1.2.cmml"><mi id="alg1.l9.m1.1.1.2.2.2" xref="alg1.l9.m1.1.1.2.2.2.cmml">L</mi><mrow id="alg1.l9.m1.1.1.2.3" xref="alg1.l9.m1.1.1.2.3.cmml"><mi id="alg1.l9.m1.1.1.2.3.2" xref="alg1.l9.m1.1.1.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m1.1.1.2.3.1" xref="alg1.l9.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="alg1.l9.m1.1.1.2.3.3" xref="alg1.l9.m1.1.1.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m1.1.1.2.3.1a" xref="alg1.l9.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="alg1.l9.m1.1.1.2.3.4" xref="alg1.l9.m1.1.1.2.3.4.cmml">s</mi></mrow><mo id="alg1.l9.m1.1.1.2.2.3" xref="alg1.l9.m1.1.1.2.2.3.cmml">â€²</mo></msubsup><mo stretchy="false" id="alg1.l9.m1.1.1.1" xref="alg1.l9.m1.1.1.1.cmml">â†</mo><msub id="alg1.l9.m1.1.1.3" xref="alg1.l9.m1.1.1.3.cmml"><mi id="alg1.l9.m1.1.1.3.2" xref="alg1.l9.m1.1.1.3.2.cmml">L</mi><mrow id="alg1.l9.m1.1.1.3.3" xref="alg1.l9.m1.1.1.3.3.cmml"><mi id="alg1.l9.m1.1.1.3.3.2" xref="alg1.l9.m1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m1.1.1.3.3.1" xref="alg1.l9.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="alg1.l9.m1.1.1.3.3.3" xref="alg1.l9.m1.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m1.1.1.3.3.1a" xref="alg1.l9.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="alg1.l9.m1.1.1.3.3.4" xref="alg1.l9.m1.1.1.3.3.4.cmml">s</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><apply id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1"><ci id="alg1.l9.m1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1">â†</ci><apply id="alg1.l9.m1.1.1.2.cmml" xref="alg1.l9.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l9.m1.1.1.2.1.cmml" xref="alg1.l9.m1.1.1.2">subscript</csymbol><apply id="alg1.l9.m1.1.1.2.2.cmml" xref="alg1.l9.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l9.m1.1.1.2.2.1.cmml" xref="alg1.l9.m1.1.1.2">superscript</csymbol><ci id="alg1.l9.m1.1.1.2.2.2.cmml" xref="alg1.l9.m1.1.1.2.2.2">ğ¿</ci><ci id="alg1.l9.m1.1.1.2.2.3.cmml" xref="alg1.l9.m1.1.1.2.2.3">â€²</ci></apply><apply id="alg1.l9.m1.1.1.2.3.cmml" xref="alg1.l9.m1.1.1.2.3"><times id="alg1.l9.m1.1.1.2.3.1.cmml" xref="alg1.l9.m1.1.1.2.3.1"></times><ci id="alg1.l9.m1.1.1.2.3.2.cmml" xref="alg1.l9.m1.1.1.2.3.2">ğ‘</ci><ci id="alg1.l9.m1.1.1.2.3.3.cmml" xref="alg1.l9.m1.1.1.2.3.3">ğ‘™</ci><ci id="alg1.l9.m1.1.1.2.3.4.cmml" xref="alg1.l9.m1.1.1.2.3.4">ğ‘ </ci></apply></apply><apply id="alg1.l9.m1.1.1.3.cmml" xref="alg1.l9.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l9.m1.1.1.3.1.cmml" xref="alg1.l9.m1.1.1.3">subscript</csymbol><ci id="alg1.l9.m1.1.1.3.2.cmml" xref="alg1.l9.m1.1.1.3.2">ğ¿</ci><apply id="alg1.l9.m1.1.1.3.3.cmml" xref="alg1.l9.m1.1.1.3.3"><times id="alg1.l9.m1.1.1.3.3.1.cmml" xref="alg1.l9.m1.1.1.3.3.1"></times><ci id="alg1.l9.m1.1.1.3.3.2.cmml" xref="alg1.l9.m1.1.1.3.3.2">ğ‘</ci><ci id="alg1.l9.m1.1.1.3.3.3.cmml" xref="alg1.l9.m1.1.1.3.3.3">ğ‘™</ci><ci id="alg1.l9.m1.1.1.3.3.4.cmml" xref="alg1.l9.m1.1.1.3.3.4">ğ‘ </ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">L^{\prime}_{cls}\leftarrow L_{cls}</annotation></semantics></math>

</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span><math id="alg1.l10.m1.1" class="ltx_Math" alttext="L^{\prime}_{cls}[\text{selected\_idx}]\leftarrow\text{foreground or pseudo novel classes}" display="inline"><semantics id="alg1.l10.m1.1a"><mrow id="alg1.l10.m1.1.2" xref="alg1.l10.m1.1.2.cmml"><mrow id="alg1.l10.m1.1.2.2" xref="alg1.l10.m1.1.2.2.cmml"><msubsup id="alg1.l10.m1.1.2.2.2" xref="alg1.l10.m1.1.2.2.2.cmml"><mi id="alg1.l10.m1.1.2.2.2.2.2" xref="alg1.l10.m1.1.2.2.2.2.2.cmml">L</mi><mrow id="alg1.l10.m1.1.2.2.2.3" xref="alg1.l10.m1.1.2.2.2.3.cmml"><mi id="alg1.l10.m1.1.2.2.2.3.2" xref="alg1.l10.m1.1.2.2.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.2.2.2.3.1" xref="alg1.l10.m1.1.2.2.2.3.1.cmml">â€‹</mo><mi id="alg1.l10.m1.1.2.2.2.3.3" xref="alg1.l10.m1.1.2.2.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.2.2.2.3.1a" xref="alg1.l10.m1.1.2.2.2.3.1.cmml">â€‹</mo><mi id="alg1.l10.m1.1.2.2.2.3.4" xref="alg1.l10.m1.1.2.2.2.3.4.cmml">s</mi></mrow><mo id="alg1.l10.m1.1.2.2.2.2.3" xref="alg1.l10.m1.1.2.2.2.2.3.cmml">â€²</mo></msubsup><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.2.2.1" xref="alg1.l10.m1.1.2.2.1.cmml">â€‹</mo><mrow id="alg1.l10.m1.1.2.2.3.2" xref="alg1.l10.m1.1.2.2.3.1.cmml"><mo stretchy="false" id="alg1.l10.m1.1.2.2.3.2.1" xref="alg1.l10.m1.1.2.2.3.1.1.cmml">[</mo><mtext id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1a.cmml">selected_idx</mtext><mo stretchy="false" id="alg1.l10.m1.1.2.2.3.2.2" xref="alg1.l10.m1.1.2.2.3.1.1.cmml">]</mo></mrow></mrow><mo stretchy="false" id="alg1.l10.m1.1.2.1" xref="alg1.l10.m1.1.2.1.cmml">â†</mo><mtext id="alg1.l10.m1.1.2.3" xref="alg1.l10.m1.1.2.3a.cmml">foreground or pseudo novel classes</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><apply id="alg1.l10.m1.1.2.cmml" xref="alg1.l10.m1.1.2"><ci id="alg1.l10.m1.1.2.1.cmml" xref="alg1.l10.m1.1.2.1">â†</ci><apply id="alg1.l10.m1.1.2.2.cmml" xref="alg1.l10.m1.1.2.2"><times id="alg1.l10.m1.1.2.2.1.cmml" xref="alg1.l10.m1.1.2.2.1"></times><apply id="alg1.l10.m1.1.2.2.2.cmml" xref="alg1.l10.m1.1.2.2.2"><csymbol cd="ambiguous" id="alg1.l10.m1.1.2.2.2.1.cmml" xref="alg1.l10.m1.1.2.2.2">subscript</csymbol><apply id="alg1.l10.m1.1.2.2.2.2.cmml" xref="alg1.l10.m1.1.2.2.2"><csymbol cd="ambiguous" id="alg1.l10.m1.1.2.2.2.2.1.cmml" xref="alg1.l10.m1.1.2.2.2">superscript</csymbol><ci id="alg1.l10.m1.1.2.2.2.2.2.cmml" xref="alg1.l10.m1.1.2.2.2.2.2">ğ¿</ci><ci id="alg1.l10.m1.1.2.2.2.2.3.cmml" xref="alg1.l10.m1.1.2.2.2.2.3">â€²</ci></apply><apply id="alg1.l10.m1.1.2.2.2.3.cmml" xref="alg1.l10.m1.1.2.2.2.3"><times id="alg1.l10.m1.1.2.2.2.3.1.cmml" xref="alg1.l10.m1.1.2.2.2.3.1"></times><ci id="alg1.l10.m1.1.2.2.2.3.2.cmml" xref="alg1.l10.m1.1.2.2.2.3.2">ğ‘</ci><ci id="alg1.l10.m1.1.2.2.2.3.3.cmml" xref="alg1.l10.m1.1.2.2.2.3.3">ğ‘™</ci><ci id="alg1.l10.m1.1.2.2.2.3.4.cmml" xref="alg1.l10.m1.1.2.2.2.3.4">ğ‘ </ci></apply></apply><apply id="alg1.l10.m1.1.2.2.3.1.cmml" xref="alg1.l10.m1.1.2.2.3.2"><csymbol cd="latexml" id="alg1.l10.m1.1.2.2.3.1.1.cmml" xref="alg1.l10.m1.1.2.2.3.2.1">delimited-[]</csymbol><ci id="alg1.l10.m1.1.1a.cmml" xref="alg1.l10.m1.1.1"><mtext id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1">selected_idx</mtext></ci></apply></apply><ci id="alg1.l10.m1.1.2.3a.cmml" xref="alg1.l10.m1.1.2.3"><mtext id="alg1.l10.m1.1.2.3.cmml" xref="alg1.l10.m1.1.2.3">foreground or pseudo novel classes</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">L^{\prime}_{cls}[\text{selected\_idx}]\leftarrow\text{foreground or pseudo novel classes}</annotation></semantics></math>

</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span>Use <math id="alg1.l11.m1.1" class="ltx_Math" alttext="L^{\prime}_{cls}[\text{selected\_idx}]" display="inline"><semantics id="alg1.l11.m1.1a"><mrow id="alg1.l11.m1.1.2" xref="alg1.l11.m1.1.2.cmml"><msubsup id="alg1.l11.m1.1.2.2" xref="alg1.l11.m1.1.2.2.cmml"><mi id="alg1.l11.m1.1.2.2.2.2" xref="alg1.l11.m1.1.2.2.2.2.cmml">L</mi><mrow id="alg1.l11.m1.1.2.2.3" xref="alg1.l11.m1.1.2.2.3.cmml"><mi id="alg1.l11.m1.1.2.2.3.2" xref="alg1.l11.m1.1.2.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.2.2.3.1" xref="alg1.l11.m1.1.2.2.3.1.cmml">â€‹</mo><mi id="alg1.l11.m1.1.2.2.3.3" xref="alg1.l11.m1.1.2.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.2.2.3.1a" xref="alg1.l11.m1.1.2.2.3.1.cmml">â€‹</mo><mi id="alg1.l11.m1.1.2.2.3.4" xref="alg1.l11.m1.1.2.2.3.4.cmml">s</mi></mrow><mo id="alg1.l11.m1.1.2.2.2.3" xref="alg1.l11.m1.1.2.2.2.3.cmml">â€²</mo></msubsup><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.2.1" xref="alg1.l11.m1.1.2.1.cmml">â€‹</mo><mrow id="alg1.l11.m1.1.2.3.2" xref="alg1.l11.m1.1.2.3.1.cmml"><mo stretchy="false" id="alg1.l11.m1.1.2.3.2.1" xref="alg1.l11.m1.1.2.3.1.1.cmml">[</mo><mtext id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1a.cmml">selected_idx</mtext><mo stretchy="false" id="alg1.l11.m1.1.2.3.2.2" xref="alg1.l11.m1.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><apply id="alg1.l11.m1.1.2.cmml" xref="alg1.l11.m1.1.2"><times id="alg1.l11.m1.1.2.1.cmml" xref="alg1.l11.m1.1.2.1"></times><apply id="alg1.l11.m1.1.2.2.cmml" xref="alg1.l11.m1.1.2.2"><csymbol cd="ambiguous" id="alg1.l11.m1.1.2.2.1.cmml" xref="alg1.l11.m1.1.2.2">subscript</csymbol><apply id="alg1.l11.m1.1.2.2.2.cmml" xref="alg1.l11.m1.1.2.2"><csymbol cd="ambiguous" id="alg1.l11.m1.1.2.2.2.1.cmml" xref="alg1.l11.m1.1.2.2">superscript</csymbol><ci id="alg1.l11.m1.1.2.2.2.2.cmml" xref="alg1.l11.m1.1.2.2.2.2">ğ¿</ci><ci id="alg1.l11.m1.1.2.2.2.3.cmml" xref="alg1.l11.m1.1.2.2.2.3">â€²</ci></apply><apply id="alg1.l11.m1.1.2.2.3.cmml" xref="alg1.l11.m1.1.2.2.3"><times id="alg1.l11.m1.1.2.2.3.1.cmml" xref="alg1.l11.m1.1.2.2.3.1"></times><ci id="alg1.l11.m1.1.2.2.3.2.cmml" xref="alg1.l11.m1.1.2.2.3.2">ğ‘</ci><ci id="alg1.l11.m1.1.2.2.3.3.cmml" xref="alg1.l11.m1.1.2.2.3.3">ğ‘™</ci><ci id="alg1.l11.m1.1.2.2.3.4.cmml" xref="alg1.l11.m1.1.2.2.3.4">ğ‘ </ci></apply></apply><apply id="alg1.l11.m1.1.2.3.1.cmml" xref="alg1.l11.m1.1.2.3.2"><csymbol cd="latexml" id="alg1.l11.m1.1.2.3.1.1.cmml" xref="alg1.l11.m1.1.2.3.2.1">delimited-[]</csymbol><ci id="alg1.l11.m1.1.1a.cmml" xref="alg1.l11.m1.1.1"><mtext id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">selected_idx</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">L^{\prime}_{cls}[\text{selected\_idx}]</annotation></semantics></math> to train the RPN and RoI head for the classification branch.

</div>
</div>
</div>
</div>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training and Inference</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">The training losses adhere to the default settings of Mask R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, which comprises three primary losses: classification loss, box regression loss, and mask segmentation loss. Specifically, only the classification loss will be changed based on the pseudo labels, while other losses will remain unmodified. It is important to note that the generated labels are utilized for recognition purposes rather than localization. Therefore, the final loss is expressed as:<math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="L=\lambda_{\text{ps\_cls}}L_{\text{pl\_cls}}+\lambda_{\text{box}}L_{\text{box}}+\lambda_{\text{mask}}L_{\text{mask}}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">L</mi><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml"><mrow id="S3.SS3.p1.1.m1.1.1.3.2" xref="S3.SS3.p1.1.m1.1.1.3.2.cmml"><msub id="S3.SS3.p1.1.m1.1.1.3.2.2" xref="S3.SS3.p1.1.m1.1.1.3.2.2.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.2.2.2" xref="S3.SS3.p1.1.m1.1.1.3.2.2.2.cmml">Î»</mi><mtext id="S3.SS3.p1.1.m1.1.1.3.2.2.3" xref="S3.SS3.p1.1.m1.1.1.3.2.2.3a.cmml">ps_cls</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.3.2.1" xref="S3.SS3.p1.1.m1.1.1.3.2.1.cmml">â€‹</mo><msub id="S3.SS3.p1.1.m1.1.1.3.2.3" xref="S3.SS3.p1.1.m1.1.1.3.2.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.2.3.2" xref="S3.SS3.p1.1.m1.1.1.3.2.3.2.cmml">L</mi><mtext id="S3.SS3.p1.1.m1.1.1.3.2.3.3" xref="S3.SS3.p1.1.m1.1.1.3.2.3.3a.cmml">pl_cls</mtext></msub></mrow><mo id="S3.SS3.p1.1.m1.1.1.3.1" xref="S3.SS3.p1.1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.SS3.p1.1.m1.1.1.3.3" xref="S3.SS3.p1.1.m1.1.1.3.3.cmml"><msub id="S3.SS3.p1.1.m1.1.1.3.3.2" xref="S3.SS3.p1.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.3.2.2" xref="S3.SS3.p1.1.m1.1.1.3.3.2.2.cmml">Î»</mi><mtext id="S3.SS3.p1.1.m1.1.1.3.3.2.3" xref="S3.SS3.p1.1.m1.1.1.3.3.2.3a.cmml">box</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.3.3.1" xref="S3.SS3.p1.1.m1.1.1.3.3.1.cmml">â€‹</mo><msub id="S3.SS3.p1.1.m1.1.1.3.3.3" xref="S3.SS3.p1.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.3.3.2" xref="S3.SS3.p1.1.m1.1.1.3.3.3.2.cmml">L</mi><mtext id="S3.SS3.p1.1.m1.1.1.3.3.3.3" xref="S3.SS3.p1.1.m1.1.1.3.3.3.3a.cmml">box</mtext></msub></mrow><mo id="S3.SS3.p1.1.m1.1.1.3.1a" xref="S3.SS3.p1.1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.SS3.p1.1.m1.1.1.3.4" xref="S3.SS3.p1.1.m1.1.1.3.4.cmml"><msub id="S3.SS3.p1.1.m1.1.1.3.4.2" xref="S3.SS3.p1.1.m1.1.1.3.4.2.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.4.2.2" xref="S3.SS3.p1.1.m1.1.1.3.4.2.2.cmml">Î»</mi><mtext id="S3.SS3.p1.1.m1.1.1.3.4.2.3" xref="S3.SS3.p1.1.m1.1.1.3.4.2.3a.cmml">mask</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.3.4.1" xref="S3.SS3.p1.1.m1.1.1.3.4.1.cmml">â€‹</mo><msub id="S3.SS3.p1.1.m1.1.1.3.4.3" xref="S3.SS3.p1.1.m1.1.1.3.4.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.4.3.2" xref="S3.SS3.p1.1.m1.1.1.3.4.3.2.cmml">L</mi><mtext id="S3.SS3.p1.1.m1.1.1.3.4.3.3" xref="S3.SS3.p1.1.m1.1.1.3.4.3.3a.cmml">mask</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><eq id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></eq><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">ğ¿</ci><apply id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3"><plus id="S3.SS3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.1"></plus><apply id="S3.SS3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2"><times id="S3.SS3.p1.1.m1.1.1.3.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2.1"></times><apply id="S3.SS3.p1.1.m1.1.1.3.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.3.2.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.3.2.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2.2.2">ğœ†</ci><ci id="S3.SS3.p1.1.m1.1.1.3.2.2.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2.2.3"><mtext mathsize="70%" id="S3.SS3.p1.1.m1.1.1.3.2.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2.2.3">ps_cls</mtext></ci></apply><apply id="S3.SS3.p1.1.m1.1.1.3.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.3.2.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.3.2.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2.3.2">ğ¿</ci><ci id="S3.SS3.p1.1.m1.1.1.3.2.3.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2.3.3"><mtext mathsize="70%" id="S3.SS3.p1.1.m1.1.1.3.2.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2.3.3">pl_cls</mtext></ci></apply></apply><apply id="S3.SS3.p1.1.m1.1.1.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3"><times id="S3.SS3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.1"></times><apply id="S3.SS3.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.2.2">ğœ†</ci><ci id="S3.SS3.p1.1.m1.1.1.3.3.2.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.2.3"><mtext mathsize="70%" id="S3.SS3.p1.1.m1.1.1.3.3.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.2.3">box</mtext></ci></apply><apply id="S3.SS3.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.3.2">ğ¿</ci><ci id="S3.SS3.p1.1.m1.1.1.3.3.3.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.3.3"><mtext mathsize="70%" id="S3.SS3.p1.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.3.3">box</mtext></ci></apply></apply><apply id="S3.SS3.p1.1.m1.1.1.3.4.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4"><times id="S3.SS3.p1.1.m1.1.1.3.4.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4.1"></times><apply id="S3.SS3.p1.1.m1.1.1.3.4.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.3.4.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.3.4.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4.2.2">ğœ†</ci><ci id="S3.SS3.p1.1.m1.1.1.3.4.2.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4.2.3"><mtext mathsize="70%" id="S3.SS3.p1.1.m1.1.1.3.4.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4.2.3">mask</mtext></ci></apply><apply id="S3.SS3.p1.1.m1.1.1.3.4.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4.3"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.3.4.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4.3">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.3.4.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4.3.2">ğ¿</ci><ci id="S3.SS3.p1.1.m1.1.1.3.4.3.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4.3.3"><mtext mathsize="70%" id="S3.SS3.p1.1.m1.1.1.3.4.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.4.3.3">mask</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">L=\lambda_{\text{ps\_cls}}L_{\text{pl\_cls}}+\lambda_{\text{box}}L_{\text{box}}+\lambda_{\text{mask}}L_{\text{mask}}</annotation></semantics></math>. <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\lambda</annotation></semantics></math> denotes the weight assigned to each loss, and <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="L_{\text{pl\_cls}}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><msub id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">L</mi><mtext id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3a.cmml">pl_cls</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">ğ¿</ci><ci id="S3.SS3.p1.3.m3.1.1.3a.cmml" xref="S3.SS3.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">pl_cls</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">L_{\text{pl\_cls}}</annotation></semantics></math> is our modified pseudo label classification loss. Specifically, we set a weight of 0.9 for the background class and 1.0 for all other classes. For inference, we adopt the same score fusion pipeline (Equ.Â <a href="#S3.E4" title="Equation 4 â€£ 3.1 Preliminaries â€£ 3 Method â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) as stated in Sec.Â <a href="#S3.SS1" title="3.1 Preliminaries â€£ 3 Method â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T2" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.13.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.14.2" class="ltx_text" style="font-size:90%;">Results on open-vocabulary object detection benchmarks. We achieve state-of-the-art results on both OV-LVIS and V3Det benchmarks.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T2.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" style="width:433.6pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.T2.4.5.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.T2.4.6.2" class="ltx_text" style="font-size:90%;">OV-LVIS benchmark</span></figcaption>
<div id="S4.T2.4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:313.6pt;height:342pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T2.4.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Method</th>
<th id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Backbone</th>
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP<sub id="S4.T2.1.1.1.1.1.1" class="ltx_sub"><span id="S4.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">r</span></sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.4.4.5.1" class="ltx_tr">
<td id="S4.T2.4.4.4.5.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S4.T2.4.4.4.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RN50</td>
<td id="S4.T2.4.4.4.5.1.3" class="ltx_td ltx_align_center ltx_border_t">16.6</td>
</tr>
<tr id="S4.T2.4.4.4.6.2" class="ltx_tr">
<td id="S4.T2.4.4.4.6.2.1" class="ltx_td ltx_align_center ltx_border_r">OV-DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="S4.T2.4.4.4.6.2.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.4.4.4.6.2.3" class="ltx_td ltx_align_center">17.4</td>
</tr>
<tr id="S4.T2.4.4.4.7.3" class="ltx_tr">
<td id="S4.T2.4.4.4.7.3.1" class="ltx_td ltx_align_center ltx_border_r">DetProÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S4.T2.4.4.4.7.3.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.4.4.4.7.3.3" class="ltx_td ltx_align_center">19.8</td>
</tr>
<tr id="S4.T2.4.4.4.8.4" class="ltx_tr">
<td id="S4.T2.4.4.4.8.4.1" class="ltx_td ltx_align_center ltx_border_r">OC-OVDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S4.T2.4.4.4.8.4.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.4.4.4.8.4.3" class="ltx_td ltx_align_center">21.1</td>
</tr>
<tr id="S4.T2.4.4.4.9.5" class="ltx_tr">
<td id="S4.T2.4.4.4.9.5.1" class="ltx_td ltx_align_center ltx_border_r">RegionCLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
</td>
<td id="S4.T2.4.4.4.9.5.2" class="ltx_td ltx_align_center ltx_border_r">RN50x4</td>
<td id="S4.T2.4.4.4.9.5.3" class="ltx_td ltx_align_center">22.0</td>
</tr>
<tr id="S4.T2.4.4.4.10.6" class="ltx_tr">
<td id="S4.T2.4.4.4.10.6.1" class="ltx_td ltx_align_center ltx_border_r">CORAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S4.T2.4.4.4.10.6.2" class="ltx_td ltx_align_center ltx_border_r">RN50x4</td>
<td id="S4.T2.4.4.4.10.6.3" class="ltx_td ltx_align_center">22.2</td>
</tr>
<tr id="S4.T2.4.4.4.11.7" class="ltx_tr">
<td id="S4.T2.4.4.4.11.7.1" class="ltx_td ltx_align_center ltx_border_r">BARONÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S4.T2.4.4.4.11.7.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.4.4.4.11.7.3" class="ltx_td ltx_align_center">22.6</td>
</tr>
<tr id="S4.T2.4.4.4.12.8" class="ltx_tr">
<td id="S4.T2.4.4.4.12.8.1" class="ltx_td ltx_align_center ltx_border_r">EdaDetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S4.T2.4.4.4.12.8.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.4.4.4.12.8.3" class="ltx_td ltx_align_center">23.7</td>
</tr>
<tr id="S4.T2.4.4.4.13.9" class="ltx_tr">
<td id="S4.T2.4.4.4.13.9.1" class="ltx_td ltx_align_center ltx_border_r">MultiModalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T2.4.4.4.13.9.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.4.4.4.13.9.3" class="ltx_td ltx_align_center">27.0</td>
</tr>
<tr id="S4.T2.4.4.4.14.10" class="ltx_tr">
<td id="S4.T2.4.4.4.14.10.1" class="ltx_td ltx_align_center ltx_border_r">F-VLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</td>
<td id="S4.T2.4.4.4.14.10.2" class="ltx_td ltx_align_center ltx_border_r">RN50x64</td>
<td id="S4.T2.4.4.4.14.10.3" class="ltx_td ltx_align_center">32.8</td>
</tr>
<tr id="S4.T2.4.4.4.15.11" class="ltx_tr">
<td id="S4.T2.4.4.4.15.11.1" class="ltx_td ltx_align_center ltx_border_r">DeticÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>
</td>
<td id="S4.T2.4.4.4.15.11.2" class="ltx_td ltx_align_center ltx_border_r">SwinB</td>
<td id="S4.T2.4.4.4.15.11.3" class="ltx_td ltx_align_center">33.8</td>
</tr>
<tr id="S4.T2.4.4.4.16.12" class="ltx_tr">
<td id="S4.T2.4.4.4.16.12.1" class="ltx_td ltx_align_center ltx_border_r">RO-ViTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S4.T2.4.4.4.16.12.2" class="ltx_td ltx_align_center ltx_border_r">ViT-H/16</td>
<td id="S4.T2.4.4.4.16.12.3" class="ltx_td ltx_align_center">34.1</td>
</tr>
<tr id="S4.T2.4.4.4.17.13" class="ltx_tr">
<td id="S4.T2.4.4.4.17.13.1" class="ltx_td ltx_align_center ltx_border_r">CLIPSelfÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S4.T2.4.4.4.17.13.2" class="ltx_td ltx_align_center ltx_border_r">ViT-B/16</td>
<td id="S4.T2.4.4.4.17.13.3" class="ltx_td ltx_align_center">25.3</td>
</tr>
<tr id="S4.T2.4.4.4.18.14" class="ltx_tr">
<td id="S4.T2.4.4.4.18.14.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">default baseline</td>
<td id="S4.T2.4.4.4.18.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RN50x16</td>
<td id="S4.T2.4.4.4.18.14.3" class="ltx_td ltx_align_center ltx_border_t">26.3</td>
</tr>
<tr id="S4.T2.2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">DST-Det</td>
<td id="S4.T2.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r">RN50x16</td>
<td id="S4.T2.2.2.2.2.1" class="ltx_td ltx_align_center">28.4 (2.1 <math id="S4.T2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.2.1.m1.1.1" xref="S4.T2.2.2.2.2.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.1.m1.1b"><ci id="S4.T2.2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.2.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
<tr id="S4.T2.4.4.4.19.15" class="ltx_tr">
<td id="S4.T2.4.4.4.19.15.1" class="ltx_td ltx_align_center ltx_border_r">default baseline</td>
<td id="S4.T2.4.4.4.19.15.2" class="ltx_td ltx_align_center ltx_border_r">RN50x64</td>
<td id="S4.T2.4.4.4.19.15.3" class="ltx_td ltx_align_center">32.0</td>
</tr>
<tr id="S4.T2.3.3.3.3" class="ltx_tr">
<td id="S4.T2.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r">DST-Det</td>
<td id="S4.T2.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">RN50x64</td>
<td id="S4.T2.3.3.3.3.1" class="ltx_td ltx_align_center">34.5 (2.5 <math id="S4.T2.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T2.3.3.3.3.1.m1.1.1" xref="S4.T2.3.3.3.3.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.1.m1.1b"><ci id="S4.T2.3.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math> )</td>
</tr>
<tr id="S4.T2.4.4.4.4" class="ltx_tr">
<td id="S4.T2.4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">DST-Det+CLIPSelfÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S4.T2.4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">ViT-B/16</td>
<td id="S4.T2.4.4.4.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">26.2 (0.9 <math id="S4.T2.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.4.4.4.4.1.m1.1a"><mo stretchy="false" id="S4.T2.4.4.4.4.1.m1.1.1" xref="S4.T2.4.4.4.4.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.1.m1.1b"><ci id="S4.T2.4.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T2.11" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" style="width:433.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T2.8.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" style="width:433.6pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.T2.8.4.5.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.T2.8.4.6.2" class="ltx_text" style="font-size:90%;">OV-COCO benchmark</span></figcaption>
<div id="S4.T2.8.4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:319.8pt;height:288pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T2.8.4.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.5.1.1.1.1" class="ltx_tr">
<th id="S4.T2.5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Method</th>
<th id="S4.T2.5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Backbone</th>
<th id="S4.T2.5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AP<math id="S4.T2.5.1.1.1.1.1.m1.1" class="ltx_Math" alttext="{}_{50}^{\mathrm{novel}}" display="inline"><semantics id="S4.T2.5.1.1.1.1.1.m1.1a"><mmultiscripts id="S4.T2.5.1.1.1.1.1.m1.1.1" xref="S4.T2.5.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.5.1.1.1.1.1.m1.1.1.2.2" xref="S4.T2.5.1.1.1.1.1.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T2.5.1.1.1.1.1.m1.1.1a" xref="S4.T2.5.1.1.1.1.1.m1.1.1.cmml"></mprescripts><mrow id="S4.T2.5.1.1.1.1.1.m1.1.1b" xref="S4.T2.5.1.1.1.1.1.m1.1.1.cmml"></mrow><mi id="S4.T2.5.1.1.1.1.1.m1.1.1.3" xref="S4.T2.5.1.1.1.1.1.m1.1.1.3.cmml">novel</mi><mn id="S4.T2.5.1.1.1.1.1.m1.1.1.2.3" xref="S4.T2.5.1.1.1.1.1.m1.1.1.2.3.cmml">50</mn><mrow id="S4.T2.5.1.1.1.1.1.m1.1.1c" xref="S4.T2.5.1.1.1.1.1.m1.1.1.cmml"></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T2.5.1.1.1.1.1.m1.1b"><apply id="S4.T2.5.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.5.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.5.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.5.1.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S4.T2.5.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.5.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.5.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T2.5.1.1.1.1.1.m1.1.1">subscript</csymbol><csymbol cd="latexml" id="S4.T2.5.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T2.5.1.1.1.1.1.m1.1.1.2.2">absent</csymbol><cn type="integer" id="S4.T2.5.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T2.5.1.1.1.1.1.m1.1.1.2.3">50</cn></apply><ci id="S4.T2.5.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.5.1.1.1.1.1.m1.1.1.3">novel</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.1.1.1.1.1.m1.1c">{}_{50}^{\mathrm{novel}}</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.8.4.4.4.5.1" class="ltx_tr">
<td id="S4.T2.8.4.4.4.5.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OV-RCNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RN50</td>
<td id="S4.T2.8.4.4.4.5.1.3" class="ltx_td ltx_align_center ltx_border_t">17.5</td>
</tr>
<tr id="S4.T2.8.4.4.4.6.2" class="ltx_tr">
<td id="S4.T2.8.4.4.4.6.2.1" class="ltx_td ltx_align_center ltx_border_r">RegionCLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.6.2.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.8.4.4.4.6.2.3" class="ltx_td ltx_align_center">26.8</td>
</tr>
<tr id="S4.T2.8.4.4.4.7.3" class="ltx_tr">
<td id="S4.T2.8.4.4.4.7.3.1" class="ltx_td ltx_align_center ltx_border_r">ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.7.3.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.8.4.4.4.7.3.3" class="ltx_td ltx_align_center">27.6</td>
</tr>
<tr id="S4.T2.8.4.4.4.8.4" class="ltx_tr">
<td id="S4.T2.8.4.4.4.8.4.1" class="ltx_td ltx_align_center ltx_border_r">DeticÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.8.4.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.8.4.4.4.8.4.3" class="ltx_td ltx_align_center">27.8</td>
</tr>
<tr id="S4.T2.8.4.4.4.9.5" class="ltx_tr">
<td id="S4.T2.8.4.4.4.9.5.1" class="ltx_td ltx_align_center ltx_border_r">F-VLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.9.5.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.8.4.4.4.9.5.3" class="ltx_td ltx_align_center">28.0</td>
</tr>
<tr id="S4.T2.8.4.4.4.10.6" class="ltx_tr">
<td id="S4.T2.8.4.4.4.10.6.1" class="ltx_td ltx_align_center ltx_border_r">OV-DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.10.6.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.8.4.4.4.10.6.3" class="ltx_td ltx_align_center">29.4</td>
</tr>
<tr id="S4.T2.8.4.4.4.11.7" class="ltx_tr">
<td id="S4.T2.8.4.4.4.11.7.1" class="ltx_td ltx_align_center ltx_border_r">VLDetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.11.7.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.8.4.4.4.11.7.3" class="ltx_td ltx_align_center">32.0</td>
</tr>
<tr id="S4.T2.8.4.4.4.12.8" class="ltx_tr">
<td id="S4.T2.8.4.4.4.12.8.1" class="ltx_td ltx_align_center ltx_border_r">RO-ViTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.12.8.2" class="ltx_td ltx_align_center ltx_border_r">ViT-L/16</td>
<td id="S4.T2.8.4.4.4.12.8.3" class="ltx_td ltx_align_center">33.0</td>
</tr>
<tr id="S4.T2.8.4.4.4.13.9" class="ltx_tr">
<td id="S4.T2.8.4.4.4.13.9.1" class="ltx_td ltx_align_center ltx_border_r">RegionCLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.13.9.2" class="ltx_td ltx_align_center ltx_border_r">RN50x4</td>
<td id="S4.T2.8.4.4.4.13.9.3" class="ltx_td ltx_align_center">39.3</td>
</tr>
<tr id="S4.T2.8.4.4.4.14.10" class="ltx_tr">
<td id="S4.T2.8.4.4.4.14.10.1" class="ltx_td ltx_align_center ltx_border_r">CLIPSelfÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.14.10.2" class="ltx_td ltx_align_center ltx_border_r">ViT-B/16</td>
<td id="S4.T2.8.4.4.4.14.10.3" class="ltx_td ltx_align_center">37.6</td>
</tr>
<tr id="S4.T2.8.4.4.4.15.11" class="ltx_tr">
<td id="S4.T2.8.4.4.4.15.11.1" class="ltx_td ltx_align_center ltx_border_r">CLIPSelfÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.15.11.2" class="ltx_td ltx_align_center ltx_border_r">ViT-L/16</td>
<td id="S4.T2.8.4.4.4.15.11.3" class="ltx_td ltx_align_center">44.3</td>
</tr>
<tr id="S4.T2.8.4.4.4.16.12" class="ltx_tr">
<td id="S4.T2.8.4.4.4.16.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">default baseline</td>
<td id="S4.T2.8.4.4.4.16.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RN50x64</td>
<td id="S4.T2.8.4.4.4.16.12.3" class="ltx_td ltx_align_center ltx_border_t">27.4</td>
</tr>
<tr id="S4.T2.6.2.2.2.2" class="ltx_tr">
<td id="S4.T2.6.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">DST-Det</td>
<td id="S4.T2.6.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r">RN50x64</td>
<td id="S4.T2.6.2.2.2.2.1" class="ltx_td ltx_align_center">33.8 (6.4 <math id="S4.T2.6.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.6.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T2.6.2.2.2.2.1.m1.1.1" xref="S4.T2.6.2.2.2.2.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.2.2.2.2.1.m1.1b"><ci id="S4.T2.6.2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.6.2.2.2.2.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math> )</td>
</tr>
<tr id="S4.T2.7.3.3.3.3" class="ltx_tr">
<td id="S4.T2.7.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r">DST-Det + CLIPSelfÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S4.T2.7.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">ViT-B/16</td>
<td id="S4.T2.7.3.3.3.3.1" class="ltx_td ltx_align_center">41.3 (4.7 <math id="S4.T2.7.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.7.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T2.7.3.3.3.3.1.m1.1.1" xref="S4.T2.7.3.3.3.3.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.3.3.3.3.1.m1.1b"><ci id="S4.T2.7.3.3.3.3.1.m1.1.1.cmml" xref="S4.T2.7.3.3.3.3.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
<tr id="S4.T2.8.4.4.4.4" class="ltx_tr">
<td id="S4.T2.8.4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">DST-Det + CLIPSelfÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S4.T2.8.4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">ViT-L/14</td>
<td id="S4.T2.8.4.4.4.4.1" class="ltx_td ltx_align_center ltx_border_bb">46.7 (2.4 <math id="S4.T2.8.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.8.4.4.4.4.1.m1.1a"><mo stretchy="false" id="S4.T2.8.4.4.4.4.1.m1.1.1" xref="S4.T2.8.4.4.4.4.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.4.4.4.4.1.m1.1b"><ci id="S4.T2.8.4.4.4.4.1.m1.1.1.cmml" xref="S4.T2.8.4.4.4.4.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.4.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T2.11.7" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" style="width:433.6pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.T2.11.7.4.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.T2.11.7.5.2" class="ltx_text" style="font-size:90%;">OV-V3Det benchmark</span></figcaption>
<div id="S4.T2.11.7.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:295.7pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T2.11.7.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.9.5.1.1.1" class="ltx_tr">
<th id="S4.T2.9.5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Method</th>
<th id="S4.T2.9.5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Backbone</th>
<th id="S4.T2.9.5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AP<sup id="S4.T2.9.5.1.1.1.1.1" class="ltx_sup"><span id="S4.T2.9.5.1.1.1.1.1.1" class="ltx_text ltx_font_italic">novel</span></sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.11.7.3.3.4.1" class="ltx_tr">
<td id="S4.T2.11.7.3.3.4.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DeticÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>
</td>
<td id="S4.T2.11.7.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RN50</td>
<td id="S4.T2.11.7.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t">6.7</td>
</tr>
<tr id="S4.T2.11.7.3.3.5.2" class="ltx_tr">
<td id="S4.T2.11.7.3.3.5.2.1" class="ltx_td ltx_align_center ltx_border_r">RegionClipÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
</td>
<td id="S4.T2.11.7.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.11.7.3.3.5.2.3" class="ltx_td ltx_align_center">3.1</td>
</tr>
<tr id="S4.T2.11.7.3.3.6.3" class="ltx_tr">
<td id="S4.T2.11.7.3.3.6.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">default baseline</td>
<td id="S4.T2.11.7.3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RN50</td>
<td id="S4.T2.11.7.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_t">3.9</td>
</tr>
<tr id="S4.T2.10.6.2.2.2" class="ltx_tr">
<td id="S4.T2.10.6.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">DST-Det</td>
<td id="S4.T2.10.6.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r">RN50</td>
<td id="S4.T2.10.6.2.2.2.1" class="ltx_td ltx_align_center">7.2 (3.3 <math id="S4.T2.10.6.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.10.6.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T2.10.6.2.2.2.1.m1.1.1" xref="S4.T2.10.6.2.2.2.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.6.2.2.2.1.m1.1b"><ci id="S4.T2.10.6.2.2.2.1.m1.1.1.cmml" xref="S4.T2.10.6.2.2.2.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.6.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
<tr id="S4.T2.11.7.3.3.7.4" class="ltx_tr">
<td id="S4.T2.11.7.3.3.7.4.1" class="ltx_td ltx_align_center ltx_border_r">default baseline</td>
<td id="S4.T2.11.7.3.3.7.4.2" class="ltx_td ltx_align_center ltx_border_r">RN50x64</td>
<td id="S4.T2.11.7.3.3.7.4.3" class="ltx_td ltx_align_center">7.0</td>
</tr>
<tr id="S4.T2.11.7.3.3.3" class="ltx_tr">
<td id="S4.T2.11.7.3.3.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">DST-Det</td>
<td id="S4.T2.11.7.3.3.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">RN50x64</td>
<td id="S4.T2.11.7.3.3.3.1" class="ltx_td ltx_align_center ltx_border_bb">13.5 (6.5 <math id="S4.T2.11.7.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.11.7.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T2.11.7.3.3.3.1.m1.1.1" xref="S4.T2.11.7.3.3.3.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.7.3.3.3.1.m1.1b"><ci id="S4.T2.11.7.3.3.3.1.m1.1.1.cmml" xref="S4.T2.11.7.3.3.3.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.7.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
</div>
</div>
</figure>
<figure id="S4.T3" class="ltx_table">

<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.16.5.1" class="ltx_text" style="font-size:113%;">Table 3</span>: </span><span id="S4.T3.8.4" class="ltx_text" style="font-size:113%;">Ablation studies and comparative analysis on LVIS 1.0 dataset. <math id="S4.T3.5.1.m1.1" class="ltx_Math" alttext="\mathrm{PLM}^{1}" display="inline"><semantics id="S4.T3.5.1.m1.1b"><msup id="S4.T3.5.1.m1.1.1" xref="S4.T3.5.1.m1.1.1.cmml"><mi id="S4.T3.5.1.m1.1.1.2" xref="S4.T3.5.1.m1.1.1.2.cmml">PLM</mi><mn id="S4.T3.5.1.m1.1.1.3" xref="S4.T3.5.1.m1.1.1.3.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T3.5.1.m1.1c"><apply id="S4.T3.5.1.m1.1.1.cmml" xref="S4.T3.5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.5.1.m1.1.1.1.cmml" xref="S4.T3.5.1.m1.1.1">superscript</csymbol><ci id="S4.T3.5.1.m1.1.1.2.cmml" xref="S4.T3.5.1.m1.1.1.2">PLM</ci><cn type="integer" id="S4.T3.5.1.m1.1.1.3.cmml" xref="S4.T3.5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.1.m1.1d">\mathrm{PLM}^{1}</annotation></semantics></math> means PLM in RPN and <math id="S4.T3.6.2.m2.1" class="ltx_Math" alttext="\mathrm{PLM}^{2}" display="inline"><semantics id="S4.T3.6.2.m2.1b"><msup id="S4.T3.6.2.m2.1.1" xref="S4.T3.6.2.m2.1.1.cmml"><mi id="S4.T3.6.2.m2.1.1.2" xref="S4.T3.6.2.m2.1.1.2.cmml">PLM</mi><mn id="S4.T3.6.2.m2.1.1.3" xref="S4.T3.6.2.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T3.6.2.m2.1c"><apply id="S4.T3.6.2.m2.1.1.cmml" xref="S4.T3.6.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T3.6.2.m2.1.1.1.cmml" xref="S4.T3.6.2.m2.1.1">superscript</csymbol><ci id="S4.T3.6.2.m2.1.1.2.cmml" xref="S4.T3.6.2.m2.1.1.2">PLM</ci><cn type="integer" id="S4.T3.6.2.m2.1.1.3.cmml" xref="S4.T3.6.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.2.m2.1d">\mathrm{PLM}^{2}</annotation></semantics></math> means PLM in RoI head. By default, we add two PLMs. We mainly report <math id="S4.T3.7.3.m3.1" class="ltx_Math" alttext="\text{AP}_{r}" display="inline"><semantics id="S4.T3.7.3.m3.1b"><msub id="S4.T3.7.3.m3.1.1" xref="S4.T3.7.3.m3.1.1.cmml"><mtext id="S4.T3.7.3.m3.1.1.2" xref="S4.T3.7.3.m3.1.1.2a.cmml">AP</mtext><mi id="S4.T3.7.3.m3.1.1.3" xref="S4.T3.7.3.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.7.3.m3.1c"><apply id="S4.T3.7.3.m3.1.1.cmml" xref="S4.T3.7.3.m3.1.1"><csymbol cd="ambiguous" id="S4.T3.7.3.m3.1.1.1.cmml" xref="S4.T3.7.3.m3.1.1">subscript</csymbol><ci id="S4.T3.7.3.m3.1.1.2a.cmml" xref="S4.T3.7.3.m3.1.1.2"><mtext id="S4.T3.7.3.m3.1.1.2.cmml" xref="S4.T3.7.3.m3.1.1.2">AP</mtext></ci><ci id="S4.T3.7.3.m3.1.1.3.cmml" xref="S4.T3.7.3.m3.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.3.m3.1d">\text{AP}_{r}</annotation></semantics></math> for reference. <math id="S4.T3.8.4.m4.1" class="ltx_Math" alttext="\text{AP}_{all}" display="inline"><semantics id="S4.T3.8.4.m4.1b"><msub id="S4.T3.8.4.m4.1.1" xref="S4.T3.8.4.m4.1.1.cmml"><mtext id="S4.T3.8.4.m4.1.1.2" xref="S4.T3.8.4.m4.1.1.2a.cmml">AP</mtext><mrow id="S4.T3.8.4.m4.1.1.3" xref="S4.T3.8.4.m4.1.1.3.cmml"><mi id="S4.T3.8.4.m4.1.1.3.2" xref="S4.T3.8.4.m4.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T3.8.4.m4.1.1.3.1" xref="S4.T3.8.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.T3.8.4.m4.1.1.3.3" xref="S4.T3.8.4.m4.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T3.8.4.m4.1.1.3.1b" xref="S4.T3.8.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.T3.8.4.m4.1.1.3.4" xref="S4.T3.8.4.m4.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T3.8.4.m4.1c"><apply id="S4.T3.8.4.m4.1.1.cmml" xref="S4.T3.8.4.m4.1.1"><csymbol cd="ambiguous" id="S4.T3.8.4.m4.1.1.1.cmml" xref="S4.T3.8.4.m4.1.1">subscript</csymbol><ci id="S4.T3.8.4.m4.1.1.2a.cmml" xref="S4.T3.8.4.m4.1.1.2"><mtext id="S4.T3.8.4.m4.1.1.2.cmml" xref="S4.T3.8.4.m4.1.1.2">AP</mtext></ci><apply id="S4.T3.8.4.m4.1.1.3.cmml" xref="S4.T3.8.4.m4.1.1.3"><times id="S4.T3.8.4.m4.1.1.3.1.cmml" xref="S4.T3.8.4.m4.1.1.3.1"></times><ci id="S4.T3.8.4.m4.1.1.3.2.cmml" xref="S4.T3.8.4.m4.1.1.3.2">ğ‘</ci><ci id="S4.T3.8.4.m4.1.1.3.3.cmml" xref="S4.T3.8.4.m4.1.1.3.3">ğ‘™</ci><ci id="S4.T3.8.4.m4.1.1.3.4.cmml" xref="S4.T3.8.4.m4.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.4.m4.1d">\text{AP}_{all}</annotation></semantics></math> is also used for all classes. All methods use the strong RN50x64 backbone on LVIS. For results on COCO, we report results using RN50.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<table id="S4.F3.sf1.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F3.sf1.3.3" class="ltx_tr">
<th id="S4.F3.sf1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.F3.sf1.3.3.4.1" class="ltx_text" style="font-size:80%;">baseline</span></th>
<th id="S4.F3.sf1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf1.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{PLM^{1}}" display="inline"><semantics id="S4.F3.sf1.1.1.1.m1.1a"><msup id="S4.F3.sf1.1.1.1.m1.1.1" xref="S4.F3.sf1.1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="S4.F3.sf1.1.1.1.m1.1.1.2" xref="S4.F3.sf1.1.1.1.m1.1.1.2.cmml">PLM</mi><mn mathsize="80%" id="S4.F3.sf1.1.1.1.m1.1.1.3" xref="S4.F3.sf1.1.1.1.m1.1.1.3.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="S4.F3.sf1.1.1.1.m1.1b"><apply id="S4.F3.sf1.1.1.1.m1.1.1.cmml" xref="S4.F3.sf1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf1.1.1.1.m1.1.1.1.cmml" xref="S4.F3.sf1.1.1.1.m1.1.1">superscript</csymbol><ci id="S4.F3.sf1.1.1.1.m1.1.1.2.cmml" xref="S4.F3.sf1.1.1.1.m1.1.1.2">PLM</ci><cn type="integer" id="S4.F3.sf1.1.1.1.m1.1.1.3.cmml" xref="S4.F3.sf1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf1.1.1.1.m1.1c">\mathrm{PLM^{1}}</annotation></semantics></math></th>
<th id="S4.F3.sf1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf1.2.2.2.m1.1" class="ltx_Math" alttext="\mathrm{PLM^{2}}" display="inline"><semantics id="S4.F3.sf1.2.2.2.m1.1a"><msup id="S4.F3.sf1.2.2.2.m1.1.1" xref="S4.F3.sf1.2.2.2.m1.1.1.cmml"><mi mathsize="80%" id="S4.F3.sf1.2.2.2.m1.1.1.2" xref="S4.F3.sf1.2.2.2.m1.1.1.2.cmml">PLM</mi><mn mathsize="80%" id="S4.F3.sf1.2.2.2.m1.1.1.3" xref="S4.F3.sf1.2.2.2.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.F3.sf1.2.2.2.m1.1b"><apply id="S4.F3.sf1.2.2.2.m1.1.1.cmml" xref="S4.F3.sf1.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf1.2.2.2.m1.1.1.1.cmml" xref="S4.F3.sf1.2.2.2.m1.1.1">superscript</csymbol><ci id="S4.F3.sf1.2.2.2.m1.1.1.2.cmml" xref="S4.F3.sf1.2.2.2.m1.1.1.2">PLM</ci><cn type="integer" id="S4.F3.sf1.2.2.2.m1.1.1.3.cmml" xref="S4.F3.sf1.2.2.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf1.2.2.2.m1.1c">\mathrm{PLM^{2}}</annotation></semantics></math></th>
<th id="S4.F3.sf1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf1.3.3.3.m1.1" class="ltx_Math" alttext="\text{AP}_{r}" display="inline"><semantics id="S4.F3.sf1.3.3.3.m1.1a"><msub id="S4.F3.sf1.3.3.3.m1.1.1" xref="S4.F3.sf1.3.3.3.m1.1.1.cmml"><mtext mathsize="80%" id="S4.F3.sf1.3.3.3.m1.1.1.2" xref="S4.F3.sf1.3.3.3.m1.1.1.2a.cmml">AP</mtext><mi mathsize="80%" id="S4.F3.sf1.3.3.3.m1.1.1.3" xref="S4.F3.sf1.3.3.3.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F3.sf1.3.3.3.m1.1b"><apply id="S4.F3.sf1.3.3.3.m1.1.1.cmml" xref="S4.F3.sf1.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf1.3.3.3.m1.1.1.1.cmml" xref="S4.F3.sf1.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.F3.sf1.3.3.3.m1.1.1.2a.cmml" xref="S4.F3.sf1.3.3.3.m1.1.1.2"><mtext mathsize="80%" id="S4.F3.sf1.3.3.3.m1.1.1.2.cmml" xref="S4.F3.sf1.3.3.3.m1.1.1.2">AP</mtext></ci><ci id="S4.F3.sf1.3.3.3.m1.1.1.3.cmml" xref="S4.F3.sf1.3.3.3.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf1.3.3.3.m1.1c">\text{AP}_{r}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F3.sf1.3.4.1" class="ltx_tr">
<th id="S4.F3.sf1.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.F3.sf1.3.4.1.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></th>
<td id="S4.F3.sf1.3.4.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.F3.sf1.3.4.1.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.F3.sf1.3.4.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.F3.sf1.3.4.1.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.F3.sf1.3.4.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.F3.sf1.3.4.1.4.1" class="ltx_text" style="font-size:80%;">28.3</span></td>
</tr>
<tr id="S4.F3.sf1.3.5.2" class="ltx_tr">
<th id="S4.F3.sf1.3.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.F3.sf1.3.5.2.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></th>
<td id="S4.F3.sf1.3.5.2.2" class="ltx_td ltx_align_center"><span id="S4.F3.sf1.3.5.2.2.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S4.F3.sf1.3.5.2.3" class="ltx_td ltx_align_center"><span id="S4.F3.sf1.3.5.2.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.F3.sf1.3.5.2.4" class="ltx_td ltx_align_center"><span id="S4.F3.sf1.3.5.2.4.1" class="ltx_text" style="font-size:80%;">30.4</span></td>
</tr>
<tr id="S4.F3.sf1.3.6.3" class="ltx_tr">
<th id="S4.F3.sf1.3.6.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.F3.sf1.3.6.3.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></th>
<td id="S4.F3.sf1.3.6.3.2" class="ltx_td ltx_align_center"><span id="S4.F3.sf1.3.6.3.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.F3.sf1.3.6.3.3" class="ltx_td ltx_align_center"><span id="S4.F3.sf1.3.6.3.3.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S4.F3.sf1.3.6.3.4" class="ltx_td ltx_align_center"><span id="S4.F3.sf1.3.6.3.4.1" class="ltx_text" style="font-size:80%;">29.5</span></td>
</tr>
<tr id="S4.F3.sf1.3.7.4" class="ltx_tr" style="background-color:#ECECEC;">
<th id="S4.F3.sf1.3.7.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.F3.sf1.3.7.4.1.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">âœ“</span></th>
<td id="S4.F3.sf1.3.7.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf1.3.7.4.2.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">âœ“</span></td>
<td id="S4.F3.sf1.3.7.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf1.3.7.4.3.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">âœ“</span></td>
<td id="S4.F3.sf1.3.7.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf1.3.7.4.4.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">31.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.6.1.1" class="ltx_text" style="font-size:113%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<table id="S4.F3.sf2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F3.sf2.2.2" class="ltx_tr">
<th id="S4.F3.sf2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.F3.sf2.2.2.3.1" class="ltx_text" style="font-size:80%;">Setting</span></th>
<th id="S4.F3.sf2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf2.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{r}" display="inline"><semantics id="S4.F3.sf2.1.1.1.m1.1a"><msub id="S4.F3.sf2.1.1.1.m1.1.1" xref="S4.F3.sf2.1.1.1.m1.1.1.cmml"><mtext mathsize="80%" id="S4.F3.sf2.1.1.1.m1.1.1.2" xref="S4.F3.sf2.1.1.1.m1.1.1.2a.cmml">AP</mtext><mi mathsize="80%" id="S4.F3.sf2.1.1.1.m1.1.1.3" xref="S4.F3.sf2.1.1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F3.sf2.1.1.1.m1.1b"><apply id="S4.F3.sf2.1.1.1.m1.1.1.cmml" xref="S4.F3.sf2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf2.1.1.1.m1.1.1.1.cmml" xref="S4.F3.sf2.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.F3.sf2.1.1.1.m1.1.1.2a.cmml" xref="S4.F3.sf2.1.1.1.m1.1.1.2"><mtext mathsize="80%" id="S4.F3.sf2.1.1.1.m1.1.1.2.cmml" xref="S4.F3.sf2.1.1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.F3.sf2.1.1.1.m1.1.1.3.cmml" xref="S4.F3.sf2.1.1.1.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf2.1.1.1.m1.1c">\text{AP}_{r}</annotation></semantics></math></th>
<th id="S4.F3.sf2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf2.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}_{all}" display="inline"><semantics id="S4.F3.sf2.2.2.2.m1.1a"><msub id="S4.F3.sf2.2.2.2.m1.1.1" xref="S4.F3.sf2.2.2.2.m1.1.1.cmml"><mtext mathsize="80%" id="S4.F3.sf2.2.2.2.m1.1.1.2" xref="S4.F3.sf2.2.2.2.m1.1.1.2a.cmml">AP</mtext><mrow id="S4.F3.sf2.2.2.2.m1.1.1.3" xref="S4.F3.sf2.2.2.2.m1.1.1.3.cmml"><mi mathsize="80%" id="S4.F3.sf2.2.2.2.m1.1.1.3.2" xref="S4.F3.sf2.2.2.2.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf2.2.2.2.m1.1.1.3.1" xref="S4.F3.sf2.2.2.2.m1.1.1.3.1.cmml">â€‹</mo><mi mathsize="80%" id="S4.F3.sf2.2.2.2.m1.1.1.3.3" xref="S4.F3.sf2.2.2.2.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf2.2.2.2.m1.1.1.3.1a" xref="S4.F3.sf2.2.2.2.m1.1.1.3.1.cmml">â€‹</mo><mi mathsize="80%" id="S4.F3.sf2.2.2.2.m1.1.1.3.4" xref="S4.F3.sf2.2.2.2.m1.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.F3.sf2.2.2.2.m1.1b"><apply id="S4.F3.sf2.2.2.2.m1.1.1.cmml" xref="S4.F3.sf2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf2.2.2.2.m1.1.1.1.cmml" xref="S4.F3.sf2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.F3.sf2.2.2.2.m1.1.1.2a.cmml" xref="S4.F3.sf2.2.2.2.m1.1.1.2"><mtext mathsize="80%" id="S4.F3.sf2.2.2.2.m1.1.1.2.cmml" xref="S4.F3.sf2.2.2.2.m1.1.1.2">AP</mtext></ci><apply id="S4.F3.sf2.2.2.2.m1.1.1.3.cmml" xref="S4.F3.sf2.2.2.2.m1.1.1.3"><times id="S4.F3.sf2.2.2.2.m1.1.1.3.1.cmml" xref="S4.F3.sf2.2.2.2.m1.1.1.3.1"></times><ci id="S4.F3.sf2.2.2.2.m1.1.1.3.2.cmml" xref="S4.F3.sf2.2.2.2.m1.1.1.3.2">ğ‘</ci><ci id="S4.F3.sf2.2.2.2.m1.1.1.3.3.cmml" xref="S4.F3.sf2.2.2.2.m1.1.1.3.3">ğ‘™</ci><ci id="S4.F3.sf2.2.2.2.m1.1.1.3.4.cmml" xref="S4.F3.sf2.2.2.2.m1.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf2.2.2.2.m1.1c">\text{AP}_{all}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F3.sf2.2.3.1" class="ltx_tr">
<th id="S4.F3.sf2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.F3.sf2.2.3.1.1.1" class="ltx_text" style="font-size:80%;">baseline</span></th>
<td id="S4.F3.sf2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf2.2.3.1.2.1" class="ltx_text" style="font-size:80%;">28.3</span></td>
<td id="S4.F3.sf2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf2.2.3.1.3.1" class="ltx_text" style="font-size:80%;">30.2</span></td>
</tr>
<tr id="S4.F3.sf2.2.4.2" class="ltx_tr">
<th id="S4.F3.sf2.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.F3.sf2.2.4.2.1.1" class="ltx_text" style="font-size:80%;">box loss</span></th>
<td id="S4.F3.sf2.2.4.2.2" class="ltx_td ltx_align_center"><span id="S4.F3.sf2.2.4.2.2.1" class="ltx_text" style="font-size:80%;">10.3</span></td>
<td id="S4.F3.sf2.2.4.2.3" class="ltx_td ltx_align_center"><span id="S4.F3.sf2.2.4.2.3.1" class="ltx_text" style="font-size:80%;">13.6</span></td>
</tr>
<tr id="S4.F3.sf2.2.5.3" class="ltx_tr">
<th id="S4.F3.sf2.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.F3.sf2.2.5.3.1.1" class="ltx_text" style="font-size:80%;">class + box loss</span></th>
<td id="S4.F3.sf2.2.5.3.2" class="ltx_td ltx_align_center"><span id="S4.F3.sf2.2.5.3.2.1" class="ltx_text" style="font-size:80%;">20.3</span></td>
<td id="S4.F3.sf2.2.5.3.3" class="ltx_td ltx_align_center"><span id="S4.F3.sf2.2.5.3.3.1" class="ltx_text" style="font-size:80%;">25.2</span></td>
</tr>
<tr id="S4.F3.sf2.2.6.4" class="ltx_tr" style="background-color:#ECECEC;">
<th id="S4.F3.sf2.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.F3.sf2.2.6.4.1.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">class loss</span></th>
<td id="S4.F3.sf2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf2.2.6.4.2.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">31.8</span></td>
<td id="S4.F3.sf2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf2.2.6.4.3.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">31.5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.5.1.1" class="ltx_text" style="font-size:113%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<table id="S4.F3.sf3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F3.sf3.1.1" class="ltx_tr">
<th id="S4.F3.sf3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.F3.sf3.1.1.2.1" class="ltx_text" style="font-size:80%;">Setting</span></th>
<th id="S4.F3.sf3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.F3.sf3.1.1.3.1" class="ltx_text" style="font-size:80%;">Schedule</span></th>
<th id="S4.F3.sf3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf3.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{r}" display="inline"><semantics id="S4.F3.sf3.1.1.1.m1.1a"><msub id="S4.F3.sf3.1.1.1.m1.1.1" xref="S4.F3.sf3.1.1.1.m1.1.1.cmml"><mtext mathsize="80%" id="S4.F3.sf3.1.1.1.m1.1.1.2" xref="S4.F3.sf3.1.1.1.m1.1.1.2a.cmml">AP</mtext><mi mathsize="80%" id="S4.F3.sf3.1.1.1.m1.1.1.3" xref="S4.F3.sf3.1.1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F3.sf3.1.1.1.m1.1b"><apply id="S4.F3.sf3.1.1.1.m1.1.1.cmml" xref="S4.F3.sf3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf3.1.1.1.m1.1.1.1.cmml" xref="S4.F3.sf3.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.F3.sf3.1.1.1.m1.1.1.2a.cmml" xref="S4.F3.sf3.1.1.1.m1.1.1.2"><mtext mathsize="80%" id="S4.F3.sf3.1.1.1.m1.1.1.2.cmml" xref="S4.F3.sf3.1.1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.F3.sf3.1.1.1.m1.1.1.3.cmml" xref="S4.F3.sf3.1.1.1.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf3.1.1.1.m1.1c">\text{AP}_{r}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F3.sf3.1.2.1" class="ltx_tr">
<td id="S4.F3.sf3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf3.1.2.1.1.1" class="ltx_text" style="font-size:80%;">baseline</span></td>
<td id="S4.F3.sf3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf3.1.2.1.2.1" class="ltx_text" style="font-size:80%;">7.4 e</span></td>
<td id="S4.F3.sf3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf3.1.2.1.3.1" class="ltx_text" style="font-size:80%;">28.3</span></td>
</tr>
<tr id="S4.F3.sf3.1.3.2" class="ltx_tr" style="background-color:#ECECEC;">
<td id="S4.F3.sf3.1.3.2.1" class="ltx_td ltx_align_center"><span id="S4.F3.sf3.1.3.2.1.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">w DST</span></td>
<td id="S4.F3.sf3.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.F3.sf3.1.3.2.2.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">7.4 e</span></td>
<td id="S4.F3.sf3.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.F3.sf3.1.3.2.3.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">32.2</span></td>
</tr>
<tr id="S4.F3.sf3.1.4.3" class="ltx_tr">
<td id="S4.F3.sf3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf3.1.4.3.1.1" class="ltx_text" style="font-size:80%;">baseline</span></td>
<td id="S4.F3.sf3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf3.1.4.3.2.1" class="ltx_text" style="font-size:80%;">14.7 e</span></td>
<td id="S4.F3.sf3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf3.1.4.3.3.1" class="ltx_text" style="font-size:80%;">31.2</span></td>
</tr>
<tr id="S4.F3.sf3.1.5.4" class="ltx_tr" style="background-color:#ECECEC;">
<td id="S4.F3.sf3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf3.1.5.4.1.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">w DST</span></td>
<td id="S4.F3.sf3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf3.1.5.4.2.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">14.7 e</span></td>
<td id="S4.F3.sf3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf3.1.5.4.3.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">33.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf3.4.1.1" class="ltx_text" style="font-size:113%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<table id="S4.F3.sf4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F3.sf4.1.1" class="ltx_tr">
<th id="S4.F3.sf4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.F3.sf4.1.1.2.1" class="ltx_text" style="font-size:80%;">dataset</span></th>
<th id="S4.F3.sf4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.F3.sf4.1.1.3.1" class="ltx_text" style="font-size:80%;">method</span></th>
<th id="S4.F3.sf4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf4.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{r}" display="inline"><semantics id="S4.F3.sf4.1.1.1.m1.1a"><msub id="S4.F3.sf4.1.1.1.m1.1.1" xref="S4.F3.sf4.1.1.1.m1.1.1.cmml"><mtext mathsize="80%" id="S4.F3.sf4.1.1.1.m1.1.1.2" xref="S4.F3.sf4.1.1.1.m1.1.1.2a.cmml">AP</mtext><mi mathsize="80%" id="S4.F3.sf4.1.1.1.m1.1.1.3" xref="S4.F3.sf4.1.1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F3.sf4.1.1.1.m1.1b"><apply id="S4.F3.sf4.1.1.1.m1.1.1.cmml" xref="S4.F3.sf4.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf4.1.1.1.m1.1.1.1.cmml" xref="S4.F3.sf4.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.F3.sf4.1.1.1.m1.1.1.2a.cmml" xref="S4.F3.sf4.1.1.1.m1.1.1.2"><mtext mathsize="80%" id="S4.F3.sf4.1.1.1.m1.1.1.2.cmml" xref="S4.F3.sf4.1.1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.F3.sf4.1.1.1.m1.1.1.3.cmml" xref="S4.F3.sf4.1.1.1.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf4.1.1.1.m1.1c">\text{AP}_{r}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F3.sf4.1.2.1" class="ltx_tr">
<td id="S4.F3.sf4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.F3.sf4.1.2.1.1.1" class="ltx_text" style="font-size:80%;">LVIS</span></td>
<td id="S4.F3.sf4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.F3.sf4.1.2.1.2.1" class="ltx_text" style="font-size:80%;">baseline + PLM</span></td>
<td id="S4.F3.sf4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.F3.sf4.1.2.1.3.1" class="ltx_text" style="font-size:80%;">31.8</span></td>
</tr>
<tr id="S4.F3.sf4.1.3.2" class="ltx_tr" style="background-color:#ECECEC;">
<td id="S4.F3.sf4.1.3.2.1" class="ltx_td ltx_align_center"><span id="S4.F3.sf4.1.3.2.1.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">LVIS</span></td>
<td id="S4.F3.sf4.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.F3.sf4.1.3.2.2.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">w OR</span></td>
<td id="S4.F3.sf4.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.F3.sf4.1.3.2.3.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">32.2</span></td>
</tr>
<tr id="S4.F3.sf4.1.4.3" class="ltx_tr">
<td id="S4.F3.sf4.1.4.3.1" class="ltx_td ltx_align_center"><span id="S4.F3.sf4.1.4.3.1.1" class="ltx_text" style="font-size:80%;">COCO</span></td>
<td id="S4.F3.sf4.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.F3.sf4.1.4.3.2.1" class="ltx_text" style="font-size:80%;">baseline + PLM</span></td>
<td id="S4.F3.sf4.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.F3.sf4.1.4.3.3.1" class="ltx_text" style="font-size:80%;">24.5</span></td>
</tr>
<tr id="S4.F3.sf4.1.5.4" class="ltx_tr" style="background-color:#ECECEC;">
<td id="S4.F3.sf4.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf4.1.5.4.1.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">COCO</span></td>
<td id="S4.F3.sf4.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf4.1.5.4.2.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">w OR</span></td>
<td id="S4.F3.sf4.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf4.1.5.4.3.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">28.5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf4.4.1.1" class="ltx_text" style="font-size:113%;">(d)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf5" class="ltx_figure ltx_figure_panel ltx_align_center">
<table id="S4.F3.sf5.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F3.sf5.2.2" class="ltx_tr">
<th id="S4.F3.sf5.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.F3.sf5.2.2.3.1" class="ltx_text" style="font-size:80%;">CLIP score</span></th>
<th id="S4.F3.sf5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf5.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{K}" display="inline"><semantics id="S4.F3.sf5.1.1.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S4.F3.sf5.1.1.1.m1.1.1" xref="S4.F3.sf5.1.1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.F3.sf5.1.1.1.m1.1b"><ci id="S4.F3.sf5.1.1.1.m1.1.1.cmml" xref="S4.F3.sf5.1.1.1.m1.1.1">K</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf5.1.1.1.m1.1c">\mathrm{K}</annotation></semantics></math></th>
<th id="S4.F3.sf5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf5.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}_{r}" display="inline"><semantics id="S4.F3.sf5.2.2.2.m1.1a"><msub id="S4.F3.sf5.2.2.2.m1.1.1" xref="S4.F3.sf5.2.2.2.m1.1.1.cmml"><mtext mathsize="80%" id="S4.F3.sf5.2.2.2.m1.1.1.2" xref="S4.F3.sf5.2.2.2.m1.1.1.2a.cmml">AP</mtext><mi mathsize="80%" id="S4.F3.sf5.2.2.2.m1.1.1.3" xref="S4.F3.sf5.2.2.2.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F3.sf5.2.2.2.m1.1b"><apply id="S4.F3.sf5.2.2.2.m1.1.1.cmml" xref="S4.F3.sf5.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf5.2.2.2.m1.1.1.1.cmml" xref="S4.F3.sf5.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.F3.sf5.2.2.2.m1.1.1.2a.cmml" xref="S4.F3.sf5.2.2.2.m1.1.1.2"><mtext mathsize="80%" id="S4.F3.sf5.2.2.2.m1.1.1.2.cmml" xref="S4.F3.sf5.2.2.2.m1.1.1.2">AP</mtext></ci><ci id="S4.F3.sf5.2.2.2.m1.1.1.3.cmml" xref="S4.F3.sf5.2.2.2.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf5.2.2.2.m1.1c">\text{AP}_{r}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F3.sf5.2.3.1" class="ltx_tr">
<td id="S4.F3.sf5.2.3.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf5.2.3.1.1.1" class="ltx_text" style="font-size:80%;">0.5</span></td>
<td id="S4.F3.sf5.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf5.2.3.1.2.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="S4.F3.sf5.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf5.2.3.1.3.1" class="ltx_text" style="font-size:80%;">23.3</span></td>
</tr>
<tr id="S4.F3.sf5.2.4.2" class="ltx_tr" style="background-color:#ECECEC;">
<td id="S4.F3.sf5.2.4.2.1" class="ltx_td ltx_align_center"><span id="S4.F3.sf5.2.4.2.1.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">0.8</span></td>
<td id="S4.F3.sf5.2.4.2.2" class="ltx_td ltx_align_center"><span id="S4.F3.sf5.2.4.2.2.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">4</span></td>
<td id="S4.F3.sf5.2.4.2.3" class="ltx_td ltx_align_center"><span id="S4.F3.sf5.2.4.2.3.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">31.8</span></td>
</tr>
<tr id="S4.F3.sf5.2.5.3" class="ltx_tr">
<td id="S4.F3.sf5.2.5.3.1" class="ltx_td ltx_align_center"><span id="S4.F3.sf5.2.5.3.1.1" class="ltx_text" style="font-size:80%;">0.4</span></td>
<td id="S4.F3.sf5.2.5.3.2" class="ltx_td ltx_align_center"><span id="S4.F3.sf5.2.5.3.2.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="S4.F3.sf5.2.5.3.3" class="ltx_td ltx_align_center"><span id="S4.F3.sf5.2.5.3.3.1" class="ltx_text" style="font-size:80%;">13.2</span></td>
</tr>
<tr id="S4.F3.sf5.2.6.4" class="ltx_tr">
<td id="S4.F3.sf5.2.6.4.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf5.2.6.4.1.1" class="ltx_text" style="font-size:80%;">0.8</span></td>
<td id="S4.F3.sf5.2.6.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf5.2.6.4.2.1" class="ltx_text" style="font-size:80%;">15</span></td>
<td id="S4.F3.sf5.2.6.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf5.2.6.4.3.1" class="ltx_text" style="font-size:80%;">25.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf5.5.1.1" class="ltx_text" style="font-size:113%;">(e)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf6" class="ltx_figure ltx_figure_panel ltx_align_center">
<table id="S4.F3.sf6.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F3.sf6.2.2" class="ltx_tr">
<th id="S4.F3.sf6.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.F3.sf6.2.2.3.1" class="ltx_text" style="font-size:80%;">Source</span></th>
<th id="S4.F3.sf6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf6.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{r}" display="inline"><semantics id="S4.F3.sf6.1.1.1.m1.1a"><msub id="S4.F3.sf6.1.1.1.m1.1.1" xref="S4.F3.sf6.1.1.1.m1.1.1.cmml"><mtext mathsize="80%" id="S4.F3.sf6.1.1.1.m1.1.1.2" xref="S4.F3.sf6.1.1.1.m1.1.1.2a.cmml">AP</mtext><mi mathsize="80%" id="S4.F3.sf6.1.1.1.m1.1.1.3" xref="S4.F3.sf6.1.1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F3.sf6.1.1.1.m1.1b"><apply id="S4.F3.sf6.1.1.1.m1.1.1.cmml" xref="S4.F3.sf6.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf6.1.1.1.m1.1.1.1.cmml" xref="S4.F3.sf6.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.F3.sf6.1.1.1.m1.1.1.2a.cmml" xref="S4.F3.sf6.1.1.1.m1.1.1.2"><mtext mathsize="80%" id="S4.F3.sf6.1.1.1.m1.1.1.2.cmml" xref="S4.F3.sf6.1.1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.F3.sf6.1.1.1.m1.1.1.3.cmml" xref="S4.F3.sf6.1.1.1.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf6.1.1.1.m1.1c">\text{AP}_{r}</annotation></semantics></math></th>
<th id="S4.F3.sf6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf6.2.2.2.m1.1" class="ltx_Math" alttext="\text{AR}_{all}" display="inline"><semantics id="S4.F3.sf6.2.2.2.m1.1a"><msub id="S4.F3.sf6.2.2.2.m1.1.1" xref="S4.F3.sf6.2.2.2.m1.1.1.cmml"><mtext mathsize="80%" id="S4.F3.sf6.2.2.2.m1.1.1.2" xref="S4.F3.sf6.2.2.2.m1.1.1.2a.cmml">AR</mtext><mrow id="S4.F3.sf6.2.2.2.m1.1.1.3" xref="S4.F3.sf6.2.2.2.m1.1.1.3.cmml"><mi mathsize="80%" id="S4.F3.sf6.2.2.2.m1.1.1.3.2" xref="S4.F3.sf6.2.2.2.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf6.2.2.2.m1.1.1.3.1" xref="S4.F3.sf6.2.2.2.m1.1.1.3.1.cmml">â€‹</mo><mi mathsize="80%" id="S4.F3.sf6.2.2.2.m1.1.1.3.3" xref="S4.F3.sf6.2.2.2.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf6.2.2.2.m1.1.1.3.1a" xref="S4.F3.sf6.2.2.2.m1.1.1.3.1.cmml">â€‹</mo><mi mathsize="80%" id="S4.F3.sf6.2.2.2.m1.1.1.3.4" xref="S4.F3.sf6.2.2.2.m1.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.F3.sf6.2.2.2.m1.1b"><apply id="S4.F3.sf6.2.2.2.m1.1.1.cmml" xref="S4.F3.sf6.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf6.2.2.2.m1.1.1.1.cmml" xref="S4.F3.sf6.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.F3.sf6.2.2.2.m1.1.1.2a.cmml" xref="S4.F3.sf6.2.2.2.m1.1.1.2"><mtext mathsize="80%" id="S4.F3.sf6.2.2.2.m1.1.1.2.cmml" xref="S4.F3.sf6.2.2.2.m1.1.1.2">AR</mtext></ci><apply id="S4.F3.sf6.2.2.2.m1.1.1.3.cmml" xref="S4.F3.sf6.2.2.2.m1.1.1.3"><times id="S4.F3.sf6.2.2.2.m1.1.1.3.1.cmml" xref="S4.F3.sf6.2.2.2.m1.1.1.3.1"></times><ci id="S4.F3.sf6.2.2.2.m1.1.1.3.2.cmml" xref="S4.F3.sf6.2.2.2.m1.1.1.3.2">ğ‘</ci><ci id="S4.F3.sf6.2.2.2.m1.1.1.3.3.cmml" xref="S4.F3.sf6.2.2.2.m1.1.1.3.3">ğ‘™</ci><ci id="S4.F3.sf6.2.2.2.m1.1.1.3.4.cmml" xref="S4.F3.sf6.2.2.2.m1.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf6.2.2.2.m1.1c">\text{AR}_{all}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F3.sf6.2.3.1" class="ltx_tr">
<th id="S4.F3.sf6.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.F3.sf6.2.3.1.1.1" class="ltx_text" style="font-size:80%;">base names only</span></th>
<td id="S4.F3.sf6.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf6.2.3.1.2.1" class="ltx_text" style="font-size:80%;">28.3</span></td>
<td id="S4.F3.sf6.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf6.2.3.1.3.1" class="ltx_text" style="font-size:80%;">30.2</span></td>
</tr>
<tr id="S4.F3.sf6.2.4.2" class="ltx_tr" style="background-color:#ECECEC;">
<th id="S4.F3.sf6.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.F3.sf6.2.4.2.1.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">using LVIS rare</span></th>
<td id="S4.F3.sf6.2.4.2.2" class="ltx_td ltx_align_center"><span id="S4.F3.sf6.2.4.2.2.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">31.8</span></td>
<td id="S4.F3.sf6.2.4.2.3" class="ltx_td ltx_align_center"><span id="S4.F3.sf6.2.4.2.3.1" class="ltx_text" style="font-size:80%;background-color:#ECECEC;">31.5</span></td>
</tr>
<tr id="S4.F3.sf6.2.5.3" class="ltx_tr">
<th id="S4.F3.sf6.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.F3.sf6.2.5.3.1.1" class="ltx_text" style="font-size:80%;">using ImageNet</span></th>
<td id="S4.F3.sf6.2.5.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf6.2.5.3.2.1" class="ltx_text" style="font-size:80%;">31.5</span></td>
<td id="S4.F3.sf6.2.5.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F3.sf6.2.5.3.3.1" class="ltx_text" style="font-size:80%;">31.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf6.5.1.1" class="ltx_text" style="font-size:113%;">(f)</span> </span></figcaption>
</figure>
</div>
</div>
</figure>
<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.2" class="ltx_p"><span id="S4.p1.2.1" class="ltx_text ltx_font_bold">Datasets and Metrics.</span> We carry out experiments on three detection datasets, including OV-LVISÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, OV-COCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, and recent challenging OV-V3DetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. For LVIS, we adopt the settings proposed by ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and mainly report the <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{r}" display="inline"><semantics id="S4.p1.1.m1.1a"><msub id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mtext id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2a.cmml">AP</mtext><mi id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.p1.1.m1.1.1.2a.cmml" xref="S4.p1.1.m1.1.1.2"><mtext id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\text{AP}_{r}</annotation></semantics></math>, representing the AP specifically for rare categories. We report box AP at a threshold of 0.5 for COCO and mean AP at threshold <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="0.5\sim 0.95" display="inline"><semantics id="S4.p1.2.m2.1a"><mrow id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mn id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml">0.5</mn><mo id="S4.p1.2.m2.1.1.1" xref="S4.p1.2.m2.1.1.1.cmml">âˆ¼</mo><mn id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1">similar-to</csymbol><cn type="float" id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2">0.5</cn><cn type="float" id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">0.5\sim 0.95</annotation></semantics></math> for V3Det.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Implementation Details.</span> We follow the settings of F-VLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> for a fair comparison. We use the Mask R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> framework with feature pyramid networkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> as our detector. All the class names are transferred into CLIP text embedding, followingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. For the â€backgroundâ€ category, we input the word â€backgroundâ€ into the multiple templates prompt and get a fixed text embedding from the CLIP text encoder. We only use the base boxes for training and all class names as of the known vocabulary as the pre-knowledge of PLM. For the final results on LVIS, we train the model for 59 epochs. For ablation studies, we train the model for 14.7 or 7.4 epochs. For the COCO dataset, we follow the previous worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. For the V3Det dataset, we adopt the default setting of the origin baselinesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.2" class="ltx_p"><span id="S4.p3.2.1" class="ltx_text ltx_font_bold">Dataset and Training Details on OV-V3Det.</span>
OV-V3DetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> is a vast vocabulary visual detection dataset. It contains extremely large categories, which consist of 13,029 categories on real-world images. The train split of V3Det 1,361,181 objects in 184,523 images, the val split has 178,475 objects in 30,000 images, and the test split has
190,144 objects in 30,000 images. For the open vocabulary object detection setting, V3Det randomly samples 6,501 categories as base classes <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="C_{base}" display="inline"><semantics id="S4.p3.1.m1.1a"><msub id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">C</mi><mrow id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml"><mi id="S4.p3.1.m1.1.1.3.2" xref="S4.p3.1.m1.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.3.1" xref="S4.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.p3.1.m1.1.1.3.3" xref="S4.p3.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.3.1a" xref="S4.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.p3.1.m1.1.1.3.4" xref="S4.p3.1.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.3.1b" xref="S4.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.p3.1.m1.1.1.3.5" xref="S4.p3.1.m1.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">ğ¶</ci><apply id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3"><times id="S4.p3.1.m1.1.1.3.1.cmml" xref="S4.p3.1.m1.1.1.3.1"></times><ci id="S4.p3.1.m1.1.1.3.2.cmml" xref="S4.p3.1.m1.1.1.3.2">ğ‘</ci><ci id="S4.p3.1.m1.1.1.3.3.cmml" xref="S4.p3.1.m1.1.1.3.3">ğ‘</ci><ci id="S4.p3.1.m1.1.1.3.4.cmml" xref="S4.p3.1.m1.1.1.3.4">ğ‘ </ci><ci id="S4.p3.1.m1.1.1.3.5.cmml" xref="S4.p3.1.m1.1.1.3.5">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">C_{base}</annotation></semantics></math> and the remaining 6,528 categories as the novel classes <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="C_{novel}" display="inline"><semantics id="S4.p3.2.m2.1a"><msub id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mi id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">C</mi><mrow id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml"><mi id="S4.p3.2.m2.1.1.3.2" xref="S4.p3.2.m2.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.3.1" xref="S4.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.p3.2.m2.1.1.3.3" xref="S4.p3.2.m2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.3.1a" xref="S4.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.p3.2.m2.1.1.3.4" xref="S4.p3.2.m2.1.1.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.3.1b" xref="S4.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.p3.2.m2.1.1.3.5" xref="S4.p3.2.m2.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.3.1c" xref="S4.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.p3.2.m2.1.1.3.6" xref="S4.p3.2.m2.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1">subscript</csymbol><ci id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">ğ¶</ci><apply id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3"><times id="S4.p3.2.m2.1.1.3.1.cmml" xref="S4.p3.2.m2.1.1.3.1"></times><ci id="S4.p3.2.m2.1.1.3.2.cmml" xref="S4.p3.2.m2.1.1.3.2">ğ‘›</ci><ci id="S4.p3.2.m2.1.1.3.3.cmml" xref="S4.p3.2.m2.1.1.3.3">ğ‘œ</ci><ci id="S4.p3.2.m2.1.1.3.4.cmml" xref="S4.p3.2.m2.1.1.3.4">ğ‘£</ci><ci id="S4.p3.2.m2.1.1.3.5.cmml" xref="S4.p3.2.m2.1.1.3.5">ğ‘’</ci><ci id="S4.p3.2.m2.1.1.3.6.cmml" xref="S4.p3.2.m2.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">C_{novel}</annotation></semantics></math>. For training a detector for V3Det, we use the SGD optimizer with a weight decay of 1e-4, a momentum of 0.9, and a learning rate of 0.02. We train our model for 2x with a batch size of 32.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Dataset and Training Details on OV-LVIS.</span>
LVISÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> has a large vocabulary and a long-tailed distribution of object instances. The LVIS dataset contains bounding box and instance labels for 1203 classes across 100k images from the COCO dataset. The classes are categorized into three sets based on their occurrence in training images â€“ rare, common, and frequent. And for open vocabulary object detection, we adopt the settings proposed by ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In this approach, annotations that belong to common and frequent categories are categorized as base categories. On the other hand, annotations belonging to rare categories are treated as novel categories. We use the SGD optimizer with a learning rate of 0.36 and a weight decay of 1e-4. We train our model for 46.1k iterations with a batch size of 256.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Dataset and Training Details on OV-COCO.</span>
We follow the setting of ovr-cnnÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> and split COCO2017 into 48 base classes and 17 novel classes. We train our model in this setting for 11.25k interactions with a batch size of 64. Other settings are the same as OV-LVIS.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Evaluation Protocol on Each Dataset.</span>
We evaluate the model on the standard COCO, LVIS, and V3Det datasets to assess its performance. We report the box average precision at threshold 0.5 of novel classes and mask average precision for rare classes box mean average precision at different threshold <math id="S4.p6.1.m1.1" class="ltx_math_unparsed" alttext="\text{ (i.e., }0.5\sim 0.95)" display="inline"><semantics id="S4.p6.1.m1.1a"><mrow id="S4.p6.1.m1.1b"><mtext id="S4.p6.1.m1.1.1">Â (i.e.,Â </mtext><mn id="S4.p6.1.m1.1.2">0.5</mn><mo id="S4.p6.1.m1.1.3">âˆ¼</mo><mn id="S4.p6.1.m1.1.4">0.95</mn><mo stretchy="false" id="S4.p6.1.m1.1.5">)</mo></mrow><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">\text{ (i.e., }0.5\sim 0.95)</annotation></semantics></math> of novel classes for COCO, LVIS, and V3Det respectively.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.4.2" class="ltx_text" style="font-size:90%;">GFLops and Parameter Analysis. Our method <span id="S4.T4.4.2.1" class="ltx_text ltx_font_bold">does not</span> bring extra flops or parameters during inference.</span></figcaption>
<table id="S4.T4.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.5.1.1" class="ltx_tr">
<th id="S4.T4.5.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="S4.T4.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Method</th>
<th id="S4.T4.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Backbone</th>
<th id="S4.T4.5.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">GFLops</th>
<th id="S4.T4.5.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Learnable Param.</th>
<th id="S4.T4.5.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Frozen Param.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.5.2.1" class="ltx_tr">
<td id="S4.T4.5.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T4.5.2.1.1.1" class="ltx_text">Train</span></td>
<td id="S4.T4.5.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.5.2.1.2.1" class="ltx_text">Baseline</span></td>
<td id="S4.T4.5.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CLIP RN50</td>
<td id="S4.T4.5.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">343G</td>
<td id="S4.T4.5.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.9M</td>
<td id="S4.T4.5.2.1.6" class="ltx_td ltx_align_center ltx_border_t">38.3M</td>
</tr>
<tr id="S4.T4.5.3.2" class="ltx_tr">
<td id="S4.T4.5.3.2.1" class="ltx_td ltx_align_center ltx_border_r">CLIP RN50x64</td>
<td id="S4.T4.5.3.2.2" class="ltx_td ltx_align_center ltx_border_r">1565G</td>
<td id="S4.T4.5.3.2.3" class="ltx_td ltx_align_center ltx_border_r">23.9M</td>
<td id="S4.T4.5.3.2.4" class="ltx_td ltx_align_center">420M</td>
</tr>
<tr id="S4.T4.5.4.3" class="ltx_tr">
<td id="S4.T4.5.4.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.5.4.3.1.1" class="ltx_text">Ours</span></td>
<td id="S4.T4.5.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CLIP RN50</td>
<td id="S4.T4.5.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">561G</td>
<td id="S4.T4.5.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.9M</td>
<td id="S4.T4.5.4.3.5" class="ltx_td ltx_align_center ltx_border_t">38.3M</td>
</tr>
<tr id="S4.T4.5.5.4" class="ltx_tr">
<td id="S4.T4.5.5.4.1" class="ltx_td ltx_align_center ltx_border_r">CLIP RN50x64</td>
<td id="S4.T4.5.5.4.2" class="ltx_td ltx_align_center ltx_border_r">4961G</td>
<td id="S4.T4.5.5.4.3" class="ltx_td ltx_align_center ltx_border_r">23.9M</td>
<td id="S4.T4.5.5.4.4" class="ltx_td ltx_align_center">420M</td>
</tr>
<tr id="S4.T4.5.6.5" class="ltx_tr">
<td id="S4.T4.5.6.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T4.5.6.5.1.1" class="ltx_text">Inference</span></td>
<td id="S4.T4.5.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.5.6.5.2.1" class="ltx_text">Baseline</span></td>
<td id="S4.T4.5.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CLIP RN50</td>
<td id="S4.T4.5.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">813G</td>
<td id="S4.T4.5.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.9M</td>
<td id="S4.T4.5.6.5.6" class="ltx_td ltx_align_center ltx_border_t">38.3M</td>
</tr>
<tr id="S4.T4.5.7.6" class="ltx_tr">
<td id="S4.T4.5.7.6.1" class="ltx_td ltx_align_center ltx_border_r">CLIP RN50x64</td>
<td id="S4.T4.5.7.6.2" class="ltx_td ltx_align_center ltx_border_r">8157G</td>
<td id="S4.T4.5.7.6.3" class="ltx_td ltx_align_center ltx_border_r">23.9M</td>
<td id="S4.T4.5.7.6.4" class="ltx_td ltx_align_center">420M</td>
</tr>
<tr id="S4.T4.5.8.7" class="ltx_tr">
<td id="S4.T4.5.8.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.5.8.7.1.1" class="ltx_text">Ours</span></td>
<td id="S4.T4.5.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CLIP RN50</td>
<td id="S4.T4.5.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">813G</td>
<td id="S4.T4.5.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.9M</td>
<td id="S4.T4.5.8.7.5" class="ltx_td ltx_align_center ltx_border_t">38.3M</td>
</tr>
<tr id="S4.T4.5.9.8" class="ltx_tr">
<td id="S4.T4.5.9.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">CLIP RN50x64</td>
<td id="S4.T4.5.9.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">8157G</td>
<td id="S4.T4.5.9.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">23.9M</td>
<td id="S4.T4.5.9.8.4" class="ltx_td ltx_align_center ltx_border_bb">420M</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Main Results</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Results on LVIS OVOD.</span> Tab.Â <a href="#S4.T2" title="Table 2 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a) shows the results of our approaches with other state-of-the-art approaches on the LVIS dataset. Due to the limited computing power, we use a smaller batch size (64 vs. 256) and shorter training schedule (59 epochs vs. 118 epochs) to build our baseline, which results in 32.0 % mAP, which is lower than F-VLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. After applying our methods, we obtain 34.5% mAP on rare classes, about 1.7% mAP higher than F-VLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, without introducing any new parameters and cost during inference. Compared with other stronger baselines, including VLDetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and DeticÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, our method does not use any extra data and achieves about 0.7-8.2% mAP gains. We also find consistent gains over different VLM baselines. We further show the generalization ability of our methods on recent work CLIPSelfÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> in the last row of Tab.Â <a href="#S4.T2" title="Table 2 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Results on COCO OVOD.</span> Tab.Â <a href="#S4.T2" title="Table 2 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (b) shows the results of our approaches with other state-of-the-art approaches on the COCO dataset. We achieve 27.4% novel box AP when adopting the frozen Mask R-CNN baseline. After using our offline refinement for the self-training module, our method can achieve a 6.4% improvement on novel box AP. Both experiment results indicate the effectiveness and generalization ability of our approach. Moreover, we also verify our method on a stronger detector, CLIPSelfÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. With a stronger VLM as the backbone and a stronger detector as the baseline, our method still achieves 2.4-4.7% improvement on novel classes. This indicates our proposed method is a general and plug-in component to improve VLM-based OVOD methods.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Results on V3Det OVOD.</span> V3Det is a more challenging dataset with a larger vocabulary size than both LVIS and COCO. As shown in Tab.Â <a href="#S4.T2" title="Table 2 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (c), our methods achieve new state-of-the-art results on different backbones. In particular, with RN50x64, our method achieves 13.5 % mAP on novel classes, significantly outperforming previous STOA by 6.8 %. Moreover, with different backbones, our methods can still improve the strong baseline via 3.3-6.5%.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Study and Analysis</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In this section, we conduct detailed ablation studies and analyses on our DST-Det.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Effectiveness of PLM.</span> In Tab.Â <a href="#S4.F3.sf1" title="Figure 3(a) â€£ Table 3 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>, we first verify the effectiveness of PLM. Adding PLM in RPN obtains 1.9 % mAP improvements while inserting PLM in RoI heads leads to 1.2% gains. This indicates the novel classes are more sensitive to RPN. As shown in the last row, combining both yields better results, thereby confirming the orthogonal effect of the two classification losses. As a result, the PLM results in a notable improvement of over 3.0%.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.2" class="ltx_p"><span id="S4.SS2.p3.2.1" class="ltx_text ltx_font_bold">Ablation on Loss Design in PLMs.</span> In PLM, we only adopt the classification loss of generated labels and ignore the box loss. This is motivated by the fact that most generated proposals are inaccurate, adversely affecting the localization ability developed using normal base annotations. To validate this assertion, we add the box loss into the PLM. As shown in Tab.Â <a href="#S4.F3.sf2" title="Figure 3(b) â€£ Table 3 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>, we observe a significant drop in both <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="\text{AP}_{r}" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><msub id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mtext id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2a.cmml">AP</mtext><mi id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.2a.cmml" xref="S4.SS2.p3.1.m1.1.1.2"><mtext id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">AP</mtext></ci><ci id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\text{AP}_{r}</annotation></semantics></math> and <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="\text{AP}_{all}" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><msub id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mtext id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2a.cmml">AP</mtext><mrow id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3.cmml"><mi id="S4.SS2.p3.2.m2.1.1.3.2" xref="S4.SS2.p3.2.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.2.m2.1.1.3.1" xref="S4.SS2.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS2.p3.2.m2.1.1.3.3" xref="S4.SS2.p3.2.m2.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.2.m2.1.1.3.1a" xref="S4.SS2.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS2.p3.2.m2.1.1.3.4" xref="S4.SS2.p3.2.m2.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p3.2.m2.1.1.2a.cmml" xref="S4.SS2.p3.2.m2.1.1.2"><mtext id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">AP</mtext></ci><apply id="S4.SS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3"><times id="S4.SS2.p3.2.m2.1.1.3.1.cmml" xref="S4.SS2.p3.2.m2.1.1.3.1"></times><ci id="S4.SS2.p3.2.m2.1.1.3.2.cmml" xref="S4.SS2.p3.2.m2.1.1.3.2">ğ‘</ci><ci id="S4.SS2.p3.2.m2.1.1.3.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3.3">ğ‘™</ci><ci id="S4.SS2.p3.2.m2.1.1.3.4.cmml" xref="S4.SS2.p3.2.m2.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">\text{AP}_{all}</annotation></semantics></math>. Furthermore, our objective is to enable the detector to recognize the novel objects or part of the novel objects. As shown in Fig.Â <a href="#S4.F4" title="Figure 4 â€£ 4.2 Ablation Study and Analysis â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, repetitive proposals still persist even with filtering operations in PLMs.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.3" class="ltx_p"><span id="S4.SS2.p4.3.1" class="ltx_text ltx_font_bold">Ablation on Effect of Pseudo Score and Pseudo Label Number.</span> We adopt CLIP score and training samples <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="\mathrm{K}" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mi mathvariant="normal" id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><ci id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">K</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\mathrm{K}</annotation></semantics></math> to select the possible proposals, since most proposals are background noises. As shown in Tab.Â <a href="#S4.F3.sf5" title="Figure 3(e) â€£ Table 3 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(e)</span></a>, decreasing the CLIP score and increasing training samples lead to inferior performance. Decreasing the score may involve more irrelevant objects or context. Increasing training examples may result in more repeated and occluded objects. Moreover, we also visualize the embeddings of novel proposals during the training with different <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="\mathrm{K}" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mi mathvariant="normal" id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><ci id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">K</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">\mathrm{K}</annotation></semantics></math> in Fig.Â <a href="#S4.F4" title="Figure 4 â€£ 4.2 Ablation Study and Analysis â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(a). With more training examples, the novel class embeddings become less discriminative, contributing to increased noise. Thus, we set <math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="\mathrm{K}" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><mi mathvariant="normal" id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><ci id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1">K</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">\mathrm{K}</annotation></semantics></math> to 4 and the CLIP score to 0.8 by default.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Ablation on Offline Refinement.</span> In Tab.Â <a href="#S4.F3.sf4" title="Figure 3(d) â€£ Table 3 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(d)</span></a>, we further show the effectiveness of offline refinement (OR). We find improvements on both COCO and LVIS datasets. However, the improvement of COCO is more significant than LVIS. This is because the total number of novel objects in COCO is larger than in LVIS. In summary, both OR and PLM work in complementary, which indicates that simple offline refinement is not a trivial extension.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Ablation on Various Schedules.</span> In Tab.Â <a href="#S4.F3.sf3" title="Figure 3(c) â€£ Table 3 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(c)</span></a>, we verify the effectiveness of longer schedule training. After extending the schedule into 14.7 epochs, we still observe over 2.0% improvements. Finally, we extend the training schedule to 59 epochs and still obtain 2.5% improvements, as shown in Tab.Â <a href="#S4.T2" title="Table 2 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a). This indicates our method can be scaled up with longer training and stronger baselines.</p>
</div>
<figure id="S4.T5" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.3.2" class="ltx_text" style="font-size:90%;">More Ablation Studies on Offline Refinement (OR).</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf7" class="ltx_figure ltx_figure_panel ltx_align_center">
<table id="S4.F3.sf7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F3.sf7.1.1" class="ltx_tr">
<th id="S4.F3.sf7.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Method</th>
<th id="S4.F3.sf7.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Backbone</th>
<th id="S4.F3.sf7.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Dataset</th>
<th id="S4.F3.sf7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf7.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{novel}" display="inline"><semantics id="S4.F3.sf7.1.1.1.m1.1a"><msub id="S4.F3.sf7.1.1.1.m1.1.1" xref="S4.F3.sf7.1.1.1.m1.1.1.cmml"><mtext id="S4.F3.sf7.1.1.1.m1.1.1.2" xref="S4.F3.sf7.1.1.1.m1.1.1.2a.cmml">AP</mtext><mrow id="S4.F3.sf7.1.1.1.m1.1.1.3" xref="S4.F3.sf7.1.1.1.m1.1.1.3.cmml"><mi id="S4.F3.sf7.1.1.1.m1.1.1.3.2" xref="S4.F3.sf7.1.1.1.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf7.1.1.1.m1.1.1.3.1" xref="S4.F3.sf7.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.F3.sf7.1.1.1.m1.1.1.3.3" xref="S4.F3.sf7.1.1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf7.1.1.1.m1.1.1.3.1a" xref="S4.F3.sf7.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.F3.sf7.1.1.1.m1.1.1.3.4" xref="S4.F3.sf7.1.1.1.m1.1.1.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf7.1.1.1.m1.1.1.3.1b" xref="S4.F3.sf7.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.F3.sf7.1.1.1.m1.1.1.3.5" xref="S4.F3.sf7.1.1.1.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf7.1.1.1.m1.1.1.3.1c" xref="S4.F3.sf7.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.F3.sf7.1.1.1.m1.1.1.3.6" xref="S4.F3.sf7.1.1.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.F3.sf7.1.1.1.m1.1b"><apply id="S4.F3.sf7.1.1.1.m1.1.1.cmml" xref="S4.F3.sf7.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf7.1.1.1.m1.1.1.1.cmml" xref="S4.F3.sf7.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.F3.sf7.1.1.1.m1.1.1.2a.cmml" xref="S4.F3.sf7.1.1.1.m1.1.1.2"><mtext id="S4.F3.sf7.1.1.1.m1.1.1.2.cmml" xref="S4.F3.sf7.1.1.1.m1.1.1.2">AP</mtext></ci><apply id="S4.F3.sf7.1.1.1.m1.1.1.3.cmml" xref="S4.F3.sf7.1.1.1.m1.1.1.3"><times id="S4.F3.sf7.1.1.1.m1.1.1.3.1.cmml" xref="S4.F3.sf7.1.1.1.m1.1.1.3.1"></times><ci id="S4.F3.sf7.1.1.1.m1.1.1.3.2.cmml" xref="S4.F3.sf7.1.1.1.m1.1.1.3.2">ğ‘›</ci><ci id="S4.F3.sf7.1.1.1.m1.1.1.3.3.cmml" xref="S4.F3.sf7.1.1.1.m1.1.1.3.3">ğ‘œ</ci><ci id="S4.F3.sf7.1.1.1.m1.1.1.3.4.cmml" xref="S4.F3.sf7.1.1.1.m1.1.1.3.4">ğ‘£</ci><ci id="S4.F3.sf7.1.1.1.m1.1.1.3.5.cmml" xref="S4.F3.sf7.1.1.1.m1.1.1.3.5">ğ‘’</ci><ci id="S4.F3.sf7.1.1.1.m1.1.1.3.6.cmml" xref="S4.F3.sf7.1.1.1.m1.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf7.1.1.1.m1.1c">\text{AP}_{novel}</annotation></semantics></math></th>
<th id="S4.F3.sf7.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">gains</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F3.sf7.1.2.1" class="ltx_tr">
<td id="S4.F3.sf7.1.2.1.1" class="ltx_td ltx_align_center ltx_border_tt">baseline + PLM</td>
<td id="S4.F3.sf7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">RN50</td>
<td id="S4.F3.sf7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">COCO</td>
<td id="S4.F3.sf7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">24.5</td>
<td id="S4.F3.sf7.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">-</td>
</tr>
<tr id="S4.F3.sf7.1.3.2" class="ltx_tr">
<td id="S4.F3.sf7.1.3.2.1" class="ltx_td ltx_align_center">+OR</td>
<td id="S4.F3.sf7.1.3.2.2" class="ltx_td ltx_align_center">RN50</td>
<td id="S4.F3.sf7.1.3.2.3" class="ltx_td ltx_align_center">COCO</td>
<td id="S4.F3.sf7.1.3.2.4" class="ltx_td ltx_align_center">28.5</td>
<td id="S4.F3.sf7.1.3.2.5" class="ltx_td ltx_align_center">4.0</td>
</tr>
<tr id="S4.F3.sf7.1.4.3" class="ltx_tr">
<td id="S4.F3.sf7.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t">baseline +PLM</td>
<td id="S4.F3.sf7.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">RN50x64</td>
<td id="S4.F3.sf7.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">LVIS</td>
<td id="S4.F3.sf7.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">31.8</td>
<td id="S4.F3.sf7.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.F3.sf7.1.5.4" class="ltx_tr">
<td id="S4.F3.sf7.1.5.4.1" class="ltx_td ltx_align_center">+OR</td>
<td id="S4.F3.sf7.1.5.4.2" class="ltx_td ltx_align_center">LVIS</td>
<td id="S4.F3.sf7.1.5.4.3" class="ltx_td ltx_align_center">RN50x64</td>
<td id="S4.F3.sf7.1.5.4.4" class="ltx_td ltx_align_center">32.2</td>
<td id="S4.F3.sf7.1.5.4.5" class="ltx_td ltx_align_center">0.4</td>
</tr>
<tr id="S4.F3.sf7.1.6.5" class="ltx_tr">
<td id="S4.F3.sf7.1.6.5.1" class="ltx_td ltx_align_center ltx_border_t">baseline + PLM</td>
<td id="S4.F3.sf7.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">RN50</td>
<td id="S4.F3.sf7.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">V3Det</td>
<td id="S4.F3.sf7.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">6.9</td>
<td id="S4.F3.sf7.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.F3.sf7.1.7.6" class="ltx_tr">
<td id="S4.F3.sf7.1.7.6.1" class="ltx_td ltx_align_center ltx_border_bb">+OR</td>
<td id="S4.F3.sf7.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb">V3Det</td>
<td id="S4.F3.sf7.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb">RN50</td>
<td id="S4.F3.sf7.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb">7.2</td>
<td id="S4.F3.sf7.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb">0.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf7.3.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf8" class="ltx_figure ltx_figure_panel ltx_align_center">
<table id="S4.F3.sf8.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F3.sf8.2.2" class="ltx_tr">
<th id="S4.F3.sf8.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Setting</th>
<th id="S4.F3.sf8.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf8.1.1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{novel}" display="inline"><semantics id="S4.F3.sf8.1.1.1.m1.1a"><msub id="S4.F3.sf8.1.1.1.m1.1.1" xref="S4.F3.sf8.1.1.1.m1.1.1.cmml"><mtext id="S4.F3.sf8.1.1.1.m1.1.1.2" xref="S4.F3.sf8.1.1.1.m1.1.1.2a.cmml">AP</mtext><mrow id="S4.F3.sf8.1.1.1.m1.1.1.3" xref="S4.F3.sf8.1.1.1.m1.1.1.3.cmml"><mi id="S4.F3.sf8.1.1.1.m1.1.1.3.2" xref="S4.F3.sf8.1.1.1.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf8.1.1.1.m1.1.1.3.1" xref="S4.F3.sf8.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.F3.sf8.1.1.1.m1.1.1.3.3" xref="S4.F3.sf8.1.1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf8.1.1.1.m1.1.1.3.1a" xref="S4.F3.sf8.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.F3.sf8.1.1.1.m1.1.1.3.4" xref="S4.F3.sf8.1.1.1.m1.1.1.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf8.1.1.1.m1.1.1.3.1b" xref="S4.F3.sf8.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.F3.sf8.1.1.1.m1.1.1.3.5" xref="S4.F3.sf8.1.1.1.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf8.1.1.1.m1.1.1.3.1c" xref="S4.F3.sf8.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.F3.sf8.1.1.1.m1.1.1.3.6" xref="S4.F3.sf8.1.1.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.F3.sf8.1.1.1.m1.1b"><apply id="S4.F3.sf8.1.1.1.m1.1.1.cmml" xref="S4.F3.sf8.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf8.1.1.1.m1.1.1.1.cmml" xref="S4.F3.sf8.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.F3.sf8.1.1.1.m1.1.1.2a.cmml" xref="S4.F3.sf8.1.1.1.m1.1.1.2"><mtext id="S4.F3.sf8.1.1.1.m1.1.1.2.cmml" xref="S4.F3.sf8.1.1.1.m1.1.1.2">AP</mtext></ci><apply id="S4.F3.sf8.1.1.1.m1.1.1.3.cmml" xref="S4.F3.sf8.1.1.1.m1.1.1.3"><times id="S4.F3.sf8.1.1.1.m1.1.1.3.1.cmml" xref="S4.F3.sf8.1.1.1.m1.1.1.3.1"></times><ci id="S4.F3.sf8.1.1.1.m1.1.1.3.2.cmml" xref="S4.F3.sf8.1.1.1.m1.1.1.3.2">ğ‘›</ci><ci id="S4.F3.sf8.1.1.1.m1.1.1.3.3.cmml" xref="S4.F3.sf8.1.1.1.m1.1.1.3.3">ğ‘œ</ci><ci id="S4.F3.sf8.1.1.1.m1.1.1.3.4.cmml" xref="S4.F3.sf8.1.1.1.m1.1.1.3.4">ğ‘£</ci><ci id="S4.F3.sf8.1.1.1.m1.1.1.3.5.cmml" xref="S4.F3.sf8.1.1.1.m1.1.1.3.5">ğ‘’</ci><ci id="S4.F3.sf8.1.1.1.m1.1.1.3.6.cmml" xref="S4.F3.sf8.1.1.1.m1.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf8.1.1.1.m1.1c">\text{AP}_{novel}</annotation></semantics></math></th>
<th id="S4.F3.sf8.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.F3.sf8.2.2.2.m1.1" class="ltx_Math" alttext="\text{AP}_{all}" display="inline"><semantics id="S4.F3.sf8.2.2.2.m1.1a"><msub id="S4.F3.sf8.2.2.2.m1.1.1" xref="S4.F3.sf8.2.2.2.m1.1.1.cmml"><mtext id="S4.F3.sf8.2.2.2.m1.1.1.2" xref="S4.F3.sf8.2.2.2.m1.1.1.2a.cmml">AP</mtext><mrow id="S4.F3.sf8.2.2.2.m1.1.1.3" xref="S4.F3.sf8.2.2.2.m1.1.1.3.cmml"><mi id="S4.F3.sf8.2.2.2.m1.1.1.3.2" xref="S4.F3.sf8.2.2.2.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf8.2.2.2.m1.1.1.3.1" xref="S4.F3.sf8.2.2.2.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.F3.sf8.2.2.2.m1.1.1.3.3" xref="S4.F3.sf8.2.2.2.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.F3.sf8.2.2.2.m1.1.1.3.1a" xref="S4.F3.sf8.2.2.2.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.F3.sf8.2.2.2.m1.1.1.3.4" xref="S4.F3.sf8.2.2.2.m1.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.F3.sf8.2.2.2.m1.1b"><apply id="S4.F3.sf8.2.2.2.m1.1.1.cmml" xref="S4.F3.sf8.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.sf8.2.2.2.m1.1.1.1.cmml" xref="S4.F3.sf8.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.F3.sf8.2.2.2.m1.1.1.2a.cmml" xref="S4.F3.sf8.2.2.2.m1.1.1.2"><mtext id="S4.F3.sf8.2.2.2.m1.1.1.2.cmml" xref="S4.F3.sf8.2.2.2.m1.1.1.2">AP</mtext></ci><apply id="S4.F3.sf8.2.2.2.m1.1.1.3.cmml" xref="S4.F3.sf8.2.2.2.m1.1.1.3"><times id="S4.F3.sf8.2.2.2.m1.1.1.3.1.cmml" xref="S4.F3.sf8.2.2.2.m1.1.1.3.1"></times><ci id="S4.F3.sf8.2.2.2.m1.1.1.3.2.cmml" xref="S4.F3.sf8.2.2.2.m1.1.1.3.2">ğ‘</ci><ci id="S4.F3.sf8.2.2.2.m1.1.1.3.3.cmml" xref="S4.F3.sf8.2.2.2.m1.1.1.3.3">ğ‘™</ci><ci id="S4.F3.sf8.2.2.2.m1.1.1.3.4.cmml" xref="S4.F3.sf8.2.2.2.m1.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf8.2.2.2.m1.1c">\text{AP}_{all}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F3.sf8.2.3.1" class="ltx_tr">
<th id="S4.F3.sf8.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">baseline + PLM</th>
<td id="S4.F3.sf8.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">24.5</td>
<td id="S4.F3.sf8.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">38.3</td>
</tr>
<tr id="S4.F3.sf8.2.4.2" class="ltx_tr">
<th id="S4.F3.sf8.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">using GT box</th>
<td id="S4.F3.sf8.2.4.2.2" class="ltx_td ltx_align_center">34.5</td>
<td id="S4.F3.sf8.2.4.2.3" class="ltx_td ltx_align_center">40.5</td>
</tr>
<tr id="S4.F3.sf8.2.5.3" class="ltx_tr" style="background-color:#ECECEC;">
<th id="S4.F3.sf8.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.F3.sf8.2.5.3.1.1" class="ltx_text" style="background-color:#ECECEC;">score = 0.9</span></th>
<td id="S4.F3.sf8.2.5.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf8.2.5.3.2.1" class="ltx_text" style="background-color:#ECECEC;">28.5</span></td>
<td id="S4.F3.sf8.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F3.sf8.2.5.3.3.1" class="ltx_text" style="background-color:#ECECEC;">38.5</span></td>
</tr>
<tr id="S4.F3.sf8.2.6.4" class="ltx_tr">
<th id="S4.F3.sf8.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">score = 0.7</th>
<td id="S4.F3.sf8.2.6.4.2" class="ltx_td ltx_align_center">27.2</td>
<td id="S4.F3.sf8.2.6.4.3" class="ltx_td ltx_align_center">37.8</td>
</tr>
<tr id="S4.F3.sf8.2.7.5" class="ltx_tr">
<th id="S4.F3.sf8.2.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">score = 0.5</th>
<td id="S4.F3.sf8.2.7.5.2" class="ltx_td ltx_align_center ltx_border_bb">23.3</td>
<td id="S4.F3.sf8.2.7.5.3" class="ltx_td ltx_align_center ltx_border_bb">37.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf8.4.1.1" class="ltx_text" style="font-size:90%;">(h)</span> </span></figcaption>
</figure>
</div>
</div>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2310.01393/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="237" height="298" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Visual Analysis of DST framework. (a), We present a t-SNE analysis on the novel region embeddings during training. Different color represents different classes. We find that using fewer training samples works well. (b), We show visual improvements over the strong baseline. Our method can detect and segment novel classes, as shown on the right side of the data pair.</span></figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2310.01393/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="297" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Visual examples. Left: We visualize the class-agnostic ground truth bounding boxes. The green boxes represent the ground truth of base classes and will be used as foreground supervision, while the red boxes represent the ground truth of possible novel classes that are not allowed during training. Right: The red boxes represent the pseudo labels we selected from negative proposals.</span></figcaption>
</figure>
<div id="S4.SS2.p7" class="ltx_para ltx_noindent">
<p id="S4.SS2.p7.1" class="ltx_p"><span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_bold">Ablation on Source of Vocabulary.</span> We also verify the vocabulary source for novel classes in Tab.Â <a href="#S4.F3.sf6" title="Figure 3(f) â€£ Table 3 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(f)</span></a>. We experiment with two types of sources: the classes (including rare classes) defined in LVISÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and the classes defined in ImageNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. As shown in Tab.Â <a href="#S4.F3.sf6" title="Figure 3(f) â€£ Table 3 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(f)</span></a>, our approach achieves considerable performance even when we <span id="S4.SS2.p7.1.2" class="ltx_text ltx_font_italic">do not</span> have access to the novel classes in the test dataset and use an external source of potential novel classes, i.e., ImageNet.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para ltx_noindent">
<p id="S4.SS2.p8.1" class="ltx_p"><span id="S4.SS2.p8.1.1" class="ltx_text ltx_font_bold">GFLops and Parameter Analysis.</span>
In Tab.Â <a href="#S4.T4" title="Table 4 â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we list the GFLops and number of parameters during training and inference. Our method uses a frozen backbone and a learnable detection head, and the parameters mainly come from the backbone. When using a large backbone, CLIP RN50x64Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, the number of learnable parameters accounts for only one-twentieth of the total number. During training, our method will get 512 proposals per image and obtain its embeddings through RoIAlignÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and <span id="S4.SS2.p8.1.2" class="ltx_text ltx_font_italic">AttentionPool</span> operation of CLIP. Compared to the baseline model, our method contributes a significant amount of GFLops from <span id="S4.SS2.p8.1.3" class="ltx_text ltx_font_italic">AttentionPool</span> operation. During inference, we use the same inference pipeline in the baseline method and use 1000 proposals per image. A larger number of proposals leads to higher GFLops.</p>
</div>
<div id="S4.SS2.p9" class="ltx_para ltx_noindent">
<p id="S4.SS2.p9.1" class="ltx_p"><span id="S4.SS2.p9.1.1" class="ltx_text ltx_font_bold">More Ablation on Offline Refinement.</span> In Tab.Â <a href="#S4.F3.sf7" title="Figure 3(g) â€£ Table 5 â€£ 4.2 Ablation Study and Analysis â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(g)</span></a> and Tab.Â <a href="#S4.F3.sf8" title="Figure 3(h) â€£ Table 5 â€£ 4.2 Ablation Study and Analysis â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(h)</span></a>, we conduct a more detailed analysis of our proposed offline refinements. In Tab.Â <a href="#S4.F3.sf7" title="Figure 3(g) â€£ Table 5 â€£ 4.2 Ablation Study and Analysis â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(g)</span></a>, we find that the OR greatly improves COCO dataset performance, while the limited improvement is on large vocabulary datasets. This is because the detectors trained on the COCO dataset better recall novel objects, which is also verified by previous worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Since our PLM has already improved both AR and AP in LVIS and V3Det, the gains of OR are limited. We argue a stronger detector will have more gains. We also explore the score in OR in Tab.Â <a href="#S4.F3.sf8" title="Figure 3(h) â€£ Table 5 â€£ 4.2 Ablation Study and Analysis â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(h)</span></a>, where we find keeping a higher score is important in offline pseudo labels generation.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Visualization Analysis</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Visual Improvements Analysis.</span> In Fig.Â <a href="#S4.F4" title="Figure 4 â€£ 4.2 Ablation Study and Analysis â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (b), we present several visual improvements over a strong baseline (32.0 <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\text{AP}_{r}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><msub id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mtext id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2a.cmml">AP</mtext><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2a.cmml" xref="S4.SS3.p1.1.m1.1.1.2"><mtext id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">AP</mtext></ci><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\text{AP}_{r}</annotation></semantics></math>) on LVIS. Compared to the baseline, our method demonstrates effective detection and segmentation of novel classes, such as soup and beer bottles.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2310.01393/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="160" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Failure case visualization. The red boxes on the left images represent the ground truth annotations of novel classes; the right images are our predictions.</span></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Qualitative Novel Class Example in DST.</span> We further visualize the generated novel examples in Fig.Â <a href="#S4.F4" title="Figure 4 â€£ 4.2 Ablation Study and Analysis â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Most visual examples exhibit satisfactory quality after the filtering operation in PLMs. Compared with previous methods, which treat the novel classes as background, our method directly forces the detector to train the classification head with the assistance of VLM. Despite overlapping or repetitive proposals, we can successfully train the detector by modifying only the classification loss within the RPN and RoI heads. Additional visual examples are presented in the appendix.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Failure Cases Visualization.</span>
In Fig.Â <a href="#S4.F6" title="Figure 6 â€£ 4.3 Visualization Analysis â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we present several failure examples on LVIS. Although our method has introduced the pseudo-labeling module for novel classes during training, many objects of novel classes cannot be detected or classified correctly.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Pseudo Labels Visualization.</span> This section presents additional visualization results. Specifically, we present the pseudo labels generated by our pseudo-labeling module in Â <a href="#S4.F5" title="Figure 5 â€£ 4.2 Ablation Study and Analysis â€£ 4 Experiments â€£ Open-Vocabulary Object Detection via Dynamic Self-Training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The visualization results are alongside the ground truth annotations for comparison. Our pseudo labels can successfully recall the novel class annotations not seen during training.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper presents DST-Det, a novel open vocabulary object detection method that incorporates a dynamic self-training strategy. By rethinking the pipeline of previous two-stage open vocabulary object detection methods, we identified a conceptual gap between training and testing for novel classes. To address this issue, we introduce a dynamic self-training strategy that generates pseudo labels for novel classes and iteratively optimizes the model with the newly discovered pseudo labels. Our experiments demonstrate that this approach can significantly improve the performance of mask average precision (AP) for novel classes, making it a promising approach for real-world applications. We hope our dynamic self-training strategy can help the community mitigate the gap between the training and testing for the OVOD task.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Limitation.</span> Our proposed approach leverages a better VLMs model to generate high-quality pseudo labels, where the zero-shot ability of VLMs affects the quality of novel class labels. However, with more strong VLMs in the futureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, our method is more feasible for large vocabulary applications.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Board Impact.</span> Our method designs the first dynamic self-training on OVOD via mining the self-knowledge from the VLM model and RPN heads. Without extra training data or learnable components, we obtain significant boosts among strong baseline on COCO and LVIS datasets. Our method can be easily extended into other related domains, including open vocabulary instance/semantic segmentation. This will be our further work. Rather than achieving STOA results, our goal is to fully explore the potential of VLM in the detector, which makes our method a generalized approach for various VLM and detectors.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Acknowledgement.</span> This work is supported by the National Key Research and Development Program of China (No.2023YFC3807603). This study is also supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contributions from the industry partner(s). It is also supported by Singapore MOE AcRF Tier 2 (MOE-T2EP20120-0001). We also gratefully acknowledge the support of SenseTime Research for providing the computing resources for this work.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Data Availability Statement/</span>
All the datasets used in this paper are available online. COCOÂ <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://cocodataset.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cocodataset.org/</a></span></span></span>, LVISÂ <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://www.lvisdataset.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.lvisdataset.org/</a></span></span></span>, and V3DetÂ <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://v3det.openxlab.org.cn/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://v3det.openxlab.org.cn/</a></span></span></span> can be downloaded from their official website accordingly.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Flamingo: a visual language model for few-shot learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Relja Arandjeloviâ€™c, Alex Andonian, Arthur Mensch, OlivierÂ J. Hâ€™enaff, Jean-Baptiste Alayrac, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Three ways to improve feature alignment for open vocabulary detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv: 2303.13518</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
MariaÂ Alejandra Bravo, Sudhanshu Mittal, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Localized vision-language matching for open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">GCPR</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Kyle Buettner and Adriana Kovashka.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Enhancing the role of context in region-word alignment for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv: 2303.10093</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Peixian Chen, Kekai Sheng, Mengdan Zhang, Yunhang Shen, Ke Li, and Chunhua Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Open vocabulary object detection with proposal mining and prediction equalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv: 2206.11134</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Han-Cheol Cho, WonÂ Young Jhoo, Woohyun Kang, and Byungseok Roh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection using pseudo caption labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv: 2303.13040</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Learning to prompt for open-vocabulary object detection with vision-language model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu, Haibing Ren, Xiaolin Wei, Weidi Xie, and Lin Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Promptdet: Towards open-vocabulary detection using uncurated images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Mingfei Gao, Chen Xing, JuanÂ Carlos Niebles, Junnan Li, Ran Xu, Wenhao Liu, and Caiming Xiong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Open vocabulary object detection with pseudo bounding-box labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Scaling open-vocabulary image segmentation with image-level labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection via vision and language knowledge distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Agrim Gupta, Piotr Dollar, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Lvis: A dataset for large vocabulary instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan Elhamifar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Scaling up visual and vision-language representation learning with noisy text supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Prannay Kaul, Weidi Xie, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Multi-modal classifiers for open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Prannay Kaul, Weidi Xie, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Multi-modal classifiers for open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Dahun Kim, Anelia Angelova, and Weicheng Kuo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Contrastive feature masking open-vocabulary vision transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Dahun Kim, Anelia Angelova, and Weicheng Kuo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Region-aware pretraining for open-vocabulary object detection with vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Wonjae Kim, Bokyung Son, and Ildoo Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Vilt: Vision-and-language transformer without convolution or region supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Weicheng Kuo, Yin Cui, Xiuye Gu, A.Â J. Piergiovanni, and Anelia Angelova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">F-vlm: Open-vocabulary object detection upon frozen vision and language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven ChuÂ Hong Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Align before fuse: Vision and language representation learning with momentum distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
LiunianÂ Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Grounded language-image pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Scaling language-image pre-training via masking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Yu Li, Tao Wang, Bingyi Kang, Sheng Tang, Chunfeng Wang, Jintao Li, and Jiashi Feng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Overcoming classifier imbalance for long-tail object detection with balanced group softmax.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Chuang Lin, Peize Sun, Yi Jiang, Ping Luo, Lizhen Qu, Gholamreza Haffari, Zehuan Yuan, and Jianfei Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Learning object-language alignments for open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Jiaying Lin and Shaogang Gong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Gridclip: One-stage object detection by grid-level clip representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:2303.09252</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr DollÃ¡r, RossÂ B. Girshick, Kaiming He, Bharath Hariharan, and SergeÂ J. Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Focal loss for dense object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Jialun Liu, Yifan Sun, Chuchu Han, Zhaopeng Dou, and Wenhui Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Deep representation learning on long-tailed data: A learnable embedding augmentation perspective.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Shilong Liu, Feng Li, Hao Zhang, XiaoÂ Bin Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Dab-detr: Dynamic anchor boxes are better queries for detr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
W. Liu, Dragomir Anguelov, D. Erhan, Christian Szegedy, ScottÂ E. Reed, Cheng-Yang Fu, and AlexanderÂ C. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Ssd: Single shot multibox detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Zongyang Ma, Guan Luo, Jin Gao, Liang Li, Yuxin Chen, Shaoru Wang, Congxuan Zhang, and Weiming Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary one-stage detection with hierarchical visual-language knowledge distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Matthias Minderer, AlexeyÂ A. Gritsenko, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Scaling open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv: 2306.09683</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Matthias Minderer, AlexeyÂ A. Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Simple open-vocabulary object detection with vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Denseclip: Language-guided dense prediction with context-aware prompting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Hanoona Rasheed, Muhammad Maaz, MuhammadÂ Uzair Khattak, Salman Khan, and FahadÂ Shahbaz Khan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Bridging the gap between object and image-level representations for open-vocabulary detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon, SantoshÂ Kumar Divvala, RossÂ B. Girshick, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">You only look once: Unified, real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Jiawei Ren, Cunjun Yu, Xiao Ma, Haiyu Zhao, Shuai Yi, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Balanced meta-softmax for long-tailed visual recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Cheng Shi and Sibei Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Edadet: Open-vocabulary object detection using early dense alignment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Hengcan Shi, Munawar Hayat, and Jianfei Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection via scene graph discovery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv: 2307.03339</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Hengcan Shi, Munawar Hayat, Yicheng Wu, and Jianfei Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Proposalclip: unsupervised open-category object proposal generation via exploiting clip cues.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Hwanjun Song and Jihwan Bang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Prompt-guided transformers for end-to-end open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv: 2303.14386</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">SparseR-CNN: End-to-end object detection with learnable proposals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Jingru Tan, Xin Lu, Gang Zhang, Changqing Yin, and Quanquan Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Equalization loss v2: A new gradient balance approach for long-tailed object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, Wanli Ouyang, Changqing Yin, and Junjie Yan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Equalization loss for long-tailed object recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Mingxing Tan, Ruoming Pang, and QuocÂ V Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Efficientdet: Scalable and efficient object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Zhi Tian, Chunhua Shen, Hao Chen, and Tong He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">FCOS: A simple and strong anchor-free object detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">V3det: Vast vocabulary visual detection dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Tao Wang and Nan Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Learning to detect and segment for open vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Tao Wang, Yu Li, Bingyi Kang, Junnan Li, Junhao Liew, Sheng Tang, Steven Hoi, and Jiashi Feng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">The devil is in classification: A simple framework for long-tail instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Adaptive class suppression loss for long-tail object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Jialian Wu, Liangchen Song, Tiancai Wang, Qian Zhang, and Junsong Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Forest r-cnn: Large-vocabulary long-tailed object detection and instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM-MM</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and ChenÂ Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Aligning bag of regions for open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li, Wentao Liu, and ChenÂ Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Clipself: Vision transformer distills itself for open-vocabulary dense prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint</span><span id="bib.bib61.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Wentao Liu, and ChenÂ Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Clim: Contrastive language-image mosaic for region representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.11376</span><span id="bib.bib62.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Xiaoshi Wu, Feng Zhu, Rui Zhao, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Cora: Adapting clip for open-vocabulary detection with region prompting and anchor pre-matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib63.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Jiahao Xie, Wei Li, Xiangtai Li, Ziwei Liu, YewÂ Soon Ong, and ChenÂ Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Mosaicfusion: Diffusion models as data augmenters for large vocabulary instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2309.13042</span><span id="bib.bib64.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini DeÂ Mello.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">ODISE: Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib65.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv: 2308.02487</span><span id="bib.bib66.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and ChenÂ Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary detr with conditional matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Alireza Zareian, KevinÂ Dela Rosa, DerekÂ Hao Hu, and Shih-Fu Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection using captions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib68.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib68.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Hao Zhang, Feng Li, Xueyan Zou, Siyi Liu, Chun yue Li, Jianfeng Gao, Jianwei Yang, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">A simple framework for open-vocabulary segmentation and detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">rXiv: 2303.08131</span><span id="bib.bib69.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and StanÂ Z. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib70.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib70.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">Distribution alignment: A unified framework for long-tail visual recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib71.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib71.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Shiyu Zhao, Samuel Schulter, Long Zhao, Zhixing Zhang, Yumin Suh, Manmohan Chandraker, DimitrisÂ N Metaxas, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Improving pseudo labels for open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.06412</span><span id="bib.bib72.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
Shiyu Zhao, Zhixing Zhang, Samuel Schulter, Long Zhao, VijayÂ Kumar B.G, Anastasis Stathopoulos, Manmohan Chandraker, and DimitrisÂ N. Metaxas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">Exploiting unlabeled data with vision and language models for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib73.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib73.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text" style="font-size:90%;">
Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, LiunianÂ Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.2.1" class="ltx_text" style="font-size:90%;">Regionclip: Region-based language-image pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib74.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib74.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text" style="font-size:90%;">
Chong Zhou, ChenÂ Change Loy, and Bo Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.2.1" class="ltx_text" style="font-size:90%;">Extract free dense labels from clip.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib75.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib75.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text" style="font-size:90%;">
Hao Zhou, Tiancheng Shen, Xu Yang, Hai Huang, Xiangtai Li, Lu Qi, and Ming-Hsuan Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.2.1" class="ltx_text" style="font-size:90%;">Rethinking evaluation metrics of open-vocabulary segmentaion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2311.03352</span><span id="bib.bib76.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Rohit Girdhar, Armand Joulin, Phillip KrÃ¤henbÃ¼hl, and Ishan Misra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.2.1" class="ltx_text" style="font-size:90%;">Detecting twenty-thousand classes using image-level supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib77.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib77.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Vladlen Koltun, and Philipp KrÃ¤henbÃ¼hl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.2.1" class="ltx_text" style="font-size:90%;">Probabilistic two-stage detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib78.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:2103.07461</span><span id="bib.bib78.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Dequan Wang, and Philipp KrÃ¤henbÃ¼hl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.2.1" class="ltx_text" style="font-size:90%;">Objects as points.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib79.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1904.07850</span><span id="bib.bib79.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.2.1" class="ltx_text" style="font-size:90%;">Deformable detr: Deformable transformers for end-to-end object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib80.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib80.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.01392" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.01393" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.01393">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.01393" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.01394" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 03:08:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
