<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.08447] RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model</title><meta property="og:description" content="The performance of perception systems developed for autonomous driving vehicles has seen significant improvements over the last few years. This improvement was associated with the increasing use of LiDAR sensors and po…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.08447">

<!--Generated on Thu Feb 29 14:12:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Radar Object detection Autonomous driving.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Mohamed Bin Zayed University of Artificial Intelligence, UAE
<span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{yahia.dalbah,jean.lahoud,hisham.cholakkal}@mbzuai.ac.ae</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yahia Dalbah
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-1488-4794" title="ORCID identifier" class="ltx_ref">0000-0003-1488-4794</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jean Lahoud
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-0315-6484" title="ORCID identifier" class="ltx_ref">0000-0003-0315-6484</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hisham Cholakkal
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-8230-9065" title="ORCID identifier" class="ltx_ref">0000-0002-8230-9065</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The performance of perception systems developed for autonomous driving vehicles has seen significant improvements over the last few years. This improvement was associated with the increasing use of LiDAR sensors and point cloud data to facilitate the task of object detection and recognition in autonomous driving. However, LiDAR and camera systems show deteriorating performances when used in unfavorable conditions like dusty and rainy weather. Radars on the other hand operate on relatively longer wavelengths which allows for much more robust measurements in these conditions. Despite that, radar-centric data sets do not get a lot of attention in the development of deep learning techniques for radar perception. In this work, we consider the radar object detection problem, in which the radar frequency data is the only input into the detection framework. We further investigate the challenges of using radar-only data in deep learning models. We propose a transformers-based model, named RadarFormer, that utilizes state-of-the-art developments in vision deep learning. Our model also introduces a channel-chirp-time merging module that reduces the size and complexity of our models by more than 10 times without compromising accuracy.
Comprehensive experiments on the CRUW radar dataset demonstrate the advantages of the proposed method. Our RadarFormer performs favorably against the state-of-the-art methods while being 2x faster during inference and requiring only one-tenth of their model parameters. The code associated with this paper is available at <a target="_blank" href="https://github.com/YahiDar/RadarFormer" title="" class="ltx_ref ltx_href">https://github.com/YahiDar/RadarFormer</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Radar Object detection Autonomous driving.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Autonomous driving technology heavily relies on a combination of cameras and LiDAR sensors, mostly due to the complementary benefits that LiDAR sensors bring to most detection pipelines. LiDAR sensors provide dense and detailed point cloud maps using rotating sensors with spherical/semi-spherical coverage of the surrounding area. These sensors’ pre-processed data features can be easily integrated with images from camera sensors in autonomous driving.
However, LiDAR sensors have shorter wavelengths, causing the following limitations in LiDAR object detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. (i) Signals are highly prone to errors under poor weather conditions and occlusion (ii) they have a relatively shorter sensing range.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In contrast to LiDAR signals, radar’s Frequency Modulated Continuous Wave (FMCW) signals operate at the millimeter-wave (mmW) band, or in the frequency band between 30 to 300 GHz. The mmW band is much lower than visible light, which allows radar signals to go through occlusion particles such as smoke and dust, enabling radars to function more robustly in extreme weather conditions. Furthermore, radar signals’ longer wavelength (mmW) provides a larger range for detection with acquisition capabilities reaching up to 3000 meters. Radars are also more accessible and cheaper to introduce to dynamic systems compared to LiDARs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">It is possible to extract point cloud data from raw radar signals, however, it is more common to extract them as radar frequency (RF) image-like data. Radar signals are sent and received through a multi-input multi-output (MIMO) antenna array, which is then passed through a series of fast Fourier Transform (FFT) to extract range-angle-doppler maps. These maps compactly describe the 3D space in the range plane (distance to detection), azimuth plane (angle of arrival), and doppler information (relative velocity) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. An example of RF data and LiDAR data in comparison can be seen in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08447/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08447/assets/x2.png" id="S1.F1.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="78" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The first row shows an example of different data samples from a radar-frequency image-like data point (from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>),
a point-cloud radar-frequency data sample (from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>), and a point-cloud LiDAR sample (from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>).
The RF image heatmap signifies the magnitude of the echoed radar signal from the radar, with blue being the minimum. The second row shows a sample RGB image and its corresponding RF image. Our model, RadarFormer, takes in the RF image only and produces a heatmap prediction with the object class (shown in different colors), illustrated in the rightmost image.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In the RF image, we see an RF range-azimuth (RA) map shown in an image-like processed format. The second image shows RF point cloud data, which can be compared to the LiDAR point cloud data, where we see that LiDAR sensors provide more detailed object descriptions compared to radar data. However, radar data can provide readings from a longer range and contain velocity information, as discussed earlier.
Multiple works demonstrated the use of radar data as a feasible alternative to cameras and LiDARs in object classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> explored radars as an opportunity to be fused with other sensors such as camera-radar fusion to produce more accurate predictions.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The increasing availability of radar frequency data now has opened the path to explore more complex approaches for radar perception. Common radar datasets provide a variety of RF data as input, for instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> provides only the RA map, while <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> provides RA, Range-Doppler (RD), and Range-Azimuth-Doppler (RAD) maps. Some datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> provide the original radar tensors in addition to the maps, while others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> provide the digitized output after the Analog to Digital Converter (ADC) stage. Radar data can also be provided as point cloud data, as was shown earlier and provided by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> with range-azimuth information in a 3D space point cloud data. The previous works for radar object detection used computationally expensive models that use large 3D convolutions and might be impractical in generating rapid real-time predictions.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this work, we propose a transformer-based deep-learning model that operates on radar frequency data exclusively and produces state-of-the-art results in object detection and classification. The proposed model is lightweight in size and generates inferences in real-time, making it suitable for the task of autonomous detection. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (bottom row) shows a camera RGB image, a radar RF image, and the corresponding ground truth annotation from CRUW dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. The proposed RadarFormer takes only the RF image as input (without the RGB image) and produces a heatmap prediction of the localized object class shown in the rightmost image. The key contributions of the proposed approach are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We explore the effectiveness of vision transformers in radar perception and introduce a novel architecture, RadarFormer, for real-time radar object detection. To the best of our knowledge, we are the first to introduce a transformer-based architecture for RF maps data for the task of object detection.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a channel-chirp-time merging module that contributes to reducing the size of radar perception models and using less computationally expensive modules.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Our proposed method, RadarFormer, achieves state-of-the-art performance with one-tenth the model size of the previous state-of-the-art model
and a two-times faster inference speed.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Object Detection on Radar Data</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Radar data is usually visualized as two-dimensional or three-dimensional maps with two sets of channel dimensions. The first is the real and complex part of the RF signal, and the other is the chirps of the echoed signal. This data form makes them suitable for multiple deep learning models utilizing Convolutional Neural Networks (CNN). While conventional CNNs do not work with point-cloud data, multiple models were still explored through the voxelization of input data, such as LidarMultiNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, PanopticPolarNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, or works that encourage interaction between the model CNNs and voxelized input like JS3C-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. For 2D image-like data, multiple works detail the process of generating RA, RD, RAD, and RAMaps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. The output of said maps is then passed through a Constant False Alarm Rate (CFAR) algorithm, which checks the amplitude of pixels to determine their magnitude relative to the average noise level in surrounding pixels, and classifies pixels as ‘object’ and ‘non-object’. After CFAR, object classes are then determined following different techniques. For example, CRUW data set uses a 3D object localization and class recognition to generate ground truth for the data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. It is common to also use temporally and spatially aligned cameras and LiDars to generate annotations for object detection purposes without classifications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. In a similar fashion, the work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> generated the annotations using radar LiDAR fusion with the latter being the ground truth. Other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> use clustering techniques that rely on the rich doppler information properties of radars to create clustered annotations, with some manual annotations as a quality check <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Following the generation of data sets, most of the works in the literature heavily rely on CNN-based deep learning models. RADDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> uses a radar-tailored ResNet backbone followed by a YOLO-inspired <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> dual detection head to produce object detections and classifications. Encoder-decoder style models are very popular and were adapted differently to different datasets. RODNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> uses a stacked-hourglass model to generate predictions, while TMVA-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> uses a temporal-multi-map encoder decoder in their CARRADA data set. Other works like RadSegNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and LidarMultiNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> introduces the encoder-decoder block after the input voxelization step to point cloud data. Following this discussion, we notice a pattern in the over-reliance on CNN-based deep learning models.
While they perform adequately in most detection-based models, our proposed model extends beyond CNNs to include more developed deep learning techniques for radar perception systems.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2304.08447/assets/x3.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="55" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Original RODNet hourglass with inception model as per <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Given the benefits of radar frequency data for object detection and classification, multiple radar datasets have been collected for this purpose. The Camera-Radar of University of Washington (CRUW) data set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> contains RF images collected through radar sensors with synchronized cameras connected. The FMCW mmW radars along with the camera collect synchronized radar maps and images at 30 frames per second (FPS) with 255 chirps per frame and a range and azimuth resolutions of 0.23 m and 15°, respectively. The data set contains three classes: pedestrians, cyclists, and cars, with around 5 objects per frame on average. RODNet was proposed as a ‘student’ module that learns alongside a camera-radar fusion (CRF) cross-modal approach. RODNet alone takes only the RF images and produces confidence maps (ConfMaps) which are passed later into a location-based non-maximum suppression (L-NMS).
Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Object Detection on Radar Data ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the RODNet model architecture, which consists of a chirp merging module (M-Net) that downsamples chirps into one layer, followed by the stacked-hourglass architecture featuring the temporal inception convolutional layers. The model reported an average precision (AP) and average recall (AR) of 77.40% and 80.80%, respectively, without the CRF and using only a camera-only supervision method, which will function as our baseline in terms of the accuracy of predictions.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08447/assets/x4.png" id="S2.F3.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="111" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08447/assets/x5.png" id="S2.F3.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="438" height="75" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Our transformer-based proposed models, 2D transformers (top) and RadarFormer (bottom). The input to both models is the output from the output of the channel-chirp-time merging module, which will be discussed in Section <a href="#S3.SS3" title="3.3 Use of 2D Information ‣ 3 The Proposed RadarFormer ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. The output of the ViT transformers downsamples the input, requiring an upsample block to retrieve the original resolution.
The same flow is followed in the MaXViT-based model, without any resolution changes to the inputs/outputs.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The Proposed RadarFormer</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">While CNNs have been a dominant architectural design block for a lot of tasks in both image recognition and radar-based recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, we propose a transformer-based architecture that utilizes recent developments in deep learning transformer techniques, shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Object Detection on Radar Data ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Our model introduces a hybrid model between CNNs, transformers, and multi-axis attention following the work done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. We also introduce an updated channel-chirp merging module that includes the temporal domain. Our module extends the merging of extra channels to include residuals connecting the temporal domain in the downsampling/upsampling stream and is shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.3 Use of 2D Information ‣ 3 The Proposed RadarFormer ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Transformers</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.17" class="ltx_p">In recent years, vision transformers (ViTs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> were introduced as a new paradigm that is fully free of convolutional networks and produced state-of-the-art results for image recognition tasks. Various approaches and versions of self-attention modules were proposed with some works reporting state-of-the-art results in object detection without the use of CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Transformers were originally developed for Natural Language Processing (NLP) tasks using 1D sequence inputs. Transformers were then repurposed for images in ViTs, taking in a 2D input image of size <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="x\in\mathbb{R}^{H\times W\times C}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">x</mi><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.3.2" xref="S3.SS1.p1.1.m1.1.1.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.3.3.1" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.3.3.1a" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.4" xref="S3.SS1.p1.1.m1.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><in id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></in><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑥</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3"><times id="S3.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.2">𝐻</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.3">𝑊</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.4">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">x\in\mathbb{R}^{H\times W\times C}</annotation></semantics></math>, where <math id="S3.SS1.p1.2.m2.2" class="ltx_Math" alttext="(H,W)" display="inline"><semantics id="S3.SS1.p1.2.m2.2a"><mrow id="S3.SS1.p1.2.m2.2.3.2" xref="S3.SS1.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.2.3.2.1" xref="S3.SS1.p1.2.m2.2.3.1.cmml">(</mo><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">H</mi><mo id="S3.SS1.p1.2.m2.2.3.2.2" xref="S3.SS1.p1.2.m2.2.3.1.cmml">,</mo><mi id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">W</mi><mo stretchy="false" id="S3.SS1.p1.2.m2.2.3.2.3" xref="S3.SS1.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.2b"><interval closure="open" id="S3.SS1.p1.2.m2.2.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.2"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝐻</ci><ci id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2">𝑊</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.2c">(H,W)</annotation></semantics></math> is the image resolution and <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">C</annotation></semantics></math> is the number of channels. Said images are then flattened into a sequence of patches, <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="x_{p}\in\mathbb{R}^{N\times(P^{2}\cdot C)}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.2" xref="S3.SS1.p1.4.m4.1.2.cmml"><msub id="S3.SS1.p1.4.m4.1.2.2" xref="S3.SS1.p1.4.m4.1.2.2.cmml"><mi id="S3.SS1.p1.4.m4.1.2.2.2" xref="S3.SS1.p1.4.m4.1.2.2.2.cmml">x</mi><mi id="S3.SS1.p1.4.m4.1.2.2.3" xref="S3.SS1.p1.4.m4.1.2.2.3.cmml">p</mi></msub><mo id="S3.SS1.p1.4.m4.1.2.1" xref="S3.SS1.p1.4.m4.1.2.1.cmml">∈</mo><msup id="S3.SS1.p1.4.m4.1.2.3" xref="S3.SS1.p1.4.m4.1.2.3.cmml"><mi id="S3.SS1.p1.4.m4.1.2.3.2" xref="S3.SS1.p1.4.m4.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.1.3" xref="S3.SS1.p1.4.m4.1.1.1.3.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.4.m4.1.1.1.2" xref="S3.SS1.p1.4.m4.1.1.1.2.cmml">×</mo><mrow id="S3.SS1.p1.4.m4.1.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m4.1.1.1.1.1.2" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.4.m4.1.1.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml"><msup id="S3.SS1.p1.4.m4.1.1.1.1.1.1.2" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.2" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.2.cmml">P</mi><mn id="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.3" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.3.cmml">2</mn></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.4.m4.1.1.1.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.1.cmml">⋅</mo><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.1.3" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.cmml">C</mi></mrow><mo stretchy="false" id="S3.SS1.p1.4.m4.1.1.1.1.1.3" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.2.cmml" xref="S3.SS1.p1.4.m4.1.2"><in id="S3.SS1.p1.4.m4.1.2.1.cmml" xref="S3.SS1.p1.4.m4.1.2.1"></in><apply id="S3.SS1.p1.4.m4.1.2.2.cmml" xref="S3.SS1.p1.4.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.2.2.1.cmml" xref="S3.SS1.p1.4.m4.1.2.2">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.2.2.2.cmml" xref="S3.SS1.p1.4.m4.1.2.2.2">𝑥</ci><ci id="S3.SS1.p1.4.m4.1.2.2.3.cmml" xref="S3.SS1.p1.4.m4.1.2.2.3">𝑝</ci></apply><apply id="S3.SS1.p1.4.m4.1.2.3.cmml" xref="S3.SS1.p1.4.m4.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.2.3.1.cmml" xref="S3.SS1.p1.4.m4.1.2.3">superscript</csymbol><ci id="S3.SS1.p1.4.m4.1.2.3.2.cmml" xref="S3.SS1.p1.4.m4.1.2.3.2">ℝ</ci><apply id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"><times id="S3.SS1.p1.4.m4.1.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.1.2"></times><ci id="S3.SS1.p1.4.m4.1.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.1.3">𝑁</ci><apply id="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1"><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.1">⋅</ci><apply id="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.2">𝑃</ci><cn type="integer" id="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.3">2</cn></apply><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.3">𝐶</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">x_{p}\in\mathbb{R}^{N\times(P^{2}\cdot C)}</annotation></semantics></math>, where <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="x_{p}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑥</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">x_{p}</annotation></semantics></math> denotes a single patch, <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="N=\frac{H\times W}{P^{2}}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">N</mi><mo id="S3.SS1.p1.6.m6.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.cmml">=</mo><mfrac id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml"><mrow id="S3.SS1.p1.6.m6.1.1.3.2" xref="S3.SS1.p1.6.m6.1.1.3.2.cmml"><mi id="S3.SS1.p1.6.m6.1.1.3.2.2" xref="S3.SS1.p1.6.m6.1.1.3.2.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.6.m6.1.1.3.2.1" xref="S3.SS1.p1.6.m6.1.1.3.2.1.cmml">×</mo><mi id="S3.SS1.p1.6.m6.1.1.3.2.3" xref="S3.SS1.p1.6.m6.1.1.3.2.3.cmml">W</mi></mrow><msup id="S3.SS1.p1.6.m6.1.1.3.3" xref="S3.SS1.p1.6.m6.1.1.3.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.3.3.2" xref="S3.SS1.p1.6.m6.1.1.3.3.2.cmml">P</mi><mn id="S3.SS1.p1.6.m6.1.1.3.3.3" xref="S3.SS1.p1.6.m6.1.1.3.3.3.cmml">2</mn></msup></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><eq id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1"></eq><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">𝑁</ci><apply id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3"><divide id="S3.SS1.p1.6.m6.1.1.3.1.cmml" xref="S3.SS1.p1.6.m6.1.1.3"></divide><apply id="S3.SS1.p1.6.m6.1.1.3.2.cmml" xref="S3.SS1.p1.6.m6.1.1.3.2"><times id="S3.SS1.p1.6.m6.1.1.3.2.1.cmml" xref="S3.SS1.p1.6.m6.1.1.3.2.1"></times><ci id="S3.SS1.p1.6.m6.1.1.3.2.2.cmml" xref="S3.SS1.p1.6.m6.1.1.3.2.2">𝐻</ci><ci id="S3.SS1.p1.6.m6.1.1.3.2.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3.2.3">𝑊</ci></apply><apply id="S3.SS1.p1.6.m6.1.1.3.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.3.3.1.cmml" xref="S3.SS1.p1.6.m6.1.1.3.3">superscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.3.3.2.cmml" xref="S3.SS1.p1.6.m6.1.1.3.3.2">𝑃</ci><cn type="integer" id="S3.SS1.p1.6.m6.1.1.3.3.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">N=\frac{H\times W}{P^{2}}</annotation></semantics></math> denotes the length of said sequence, and <math id="S3.SS1.p1.7.m7.2" class="ltx_Math" alttext="(P,P)" display="inline"><semantics id="S3.SS1.p1.7.m7.2a"><mrow id="S3.SS1.p1.7.m7.2.3.2" xref="S3.SS1.p1.7.m7.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.7.m7.2.3.2.1" xref="S3.SS1.p1.7.m7.2.3.1.cmml">(</mo><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">P</mi><mo id="S3.SS1.p1.7.m7.2.3.2.2" xref="S3.SS1.p1.7.m7.2.3.1.cmml">,</mo><mi id="S3.SS1.p1.7.m7.2.2" xref="S3.SS1.p1.7.m7.2.2.cmml">P</mi><mo stretchy="false" id="S3.SS1.p1.7.m7.2.3.2.3" xref="S3.SS1.p1.7.m7.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.2b"><interval closure="open" id="S3.SS1.p1.7.m7.2.3.1.cmml" xref="S3.SS1.p1.7.m7.2.3.2"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝑃</ci><ci id="S3.SS1.p1.7.m7.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2">𝑃</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.2c">(P,P)</annotation></semantics></math> denotes the size of every patch. Transformers were expanded to 3D data by including temporal information, volumetric medical images, or spatial 3D images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. In 3D sequences, we have an input of size <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="x\in\mathbb{R}^{D\times H\times W\times C}" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mrow id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">x</mi><mo id="S3.SS1.p1.8.m8.1.1.1" xref="S3.SS1.p1.8.m8.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml"><mi id="S3.SS1.p1.8.m8.1.1.3.2" xref="S3.SS1.p1.8.m8.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.8.m8.1.1.3.3" xref="S3.SS1.p1.8.m8.1.1.3.3.cmml"><mi id="S3.SS1.p1.8.m8.1.1.3.3.2" xref="S3.SS1.p1.8.m8.1.1.3.3.2.cmml">D</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.8.m8.1.1.3.3.1" xref="S3.SS1.p1.8.m8.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.8.m8.1.1.3.3.3" xref="S3.SS1.p1.8.m8.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.8.m8.1.1.3.3.1a" xref="S3.SS1.p1.8.m8.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.8.m8.1.1.3.3.4" xref="S3.SS1.p1.8.m8.1.1.3.3.4.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.8.m8.1.1.3.3.1b" xref="S3.SS1.p1.8.m8.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.8.m8.1.1.3.3.5" xref="S3.SS1.p1.8.m8.1.1.3.3.5.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><in id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1.1"></in><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">𝑥</ci><apply id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.3.1.cmml" xref="S3.SS1.p1.8.m8.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.3.2.cmml" xref="S3.SS1.p1.8.m8.1.1.3.2">ℝ</ci><apply id="S3.SS1.p1.8.m8.1.1.3.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3"><times id="S3.SS1.p1.8.m8.1.1.3.3.1.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.1"></times><ci id="S3.SS1.p1.8.m8.1.1.3.3.2.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.2">𝐷</ci><ci id="S3.SS1.p1.8.m8.1.1.3.3.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.3">𝐻</ci><ci id="S3.SS1.p1.8.m8.1.1.3.3.4.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.4">𝑊</ci><ci id="S3.SS1.p1.8.m8.1.1.3.3.5.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.5">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">x\in\mathbb{R}^{D\times H\times W\times C}</annotation></semantics></math>, where <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><mi id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><ci id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">D</annotation></semantics></math> is the depth of the data (or the third domain of the respective application). Here, we have a sequence of patches shaped as <math id="S3.SS1.p1.10.m10.1" class="ltx_Math" alttext="x_{p}\in\mathbb{R}^{N\times(P^{3}\cdot C)}" display="inline"><semantics id="S3.SS1.p1.10.m10.1a"><mrow id="S3.SS1.p1.10.m10.1.2" xref="S3.SS1.p1.10.m10.1.2.cmml"><msub id="S3.SS1.p1.10.m10.1.2.2" xref="S3.SS1.p1.10.m10.1.2.2.cmml"><mi id="S3.SS1.p1.10.m10.1.2.2.2" xref="S3.SS1.p1.10.m10.1.2.2.2.cmml">x</mi><mi id="S3.SS1.p1.10.m10.1.2.2.3" xref="S3.SS1.p1.10.m10.1.2.2.3.cmml">p</mi></msub><mo id="S3.SS1.p1.10.m10.1.2.1" xref="S3.SS1.p1.10.m10.1.2.1.cmml">∈</mo><msup id="S3.SS1.p1.10.m10.1.2.3" xref="S3.SS1.p1.10.m10.1.2.3.cmml"><mi id="S3.SS1.p1.10.m10.1.2.3.2" xref="S3.SS1.p1.10.m10.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.10.m10.1.1.1" xref="S3.SS1.p1.10.m10.1.1.1.cmml"><mi id="S3.SS1.p1.10.m10.1.1.1.3" xref="S3.SS1.p1.10.m10.1.1.1.3.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.10.m10.1.1.1.2" xref="S3.SS1.p1.10.m10.1.1.1.2.cmml">×</mo><mrow id="S3.SS1.p1.10.m10.1.1.1.1.1" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.10.m10.1.1.1.1.1.2" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.10.m10.1.1.1.1.1.1" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.cmml"><msup id="S3.SS1.p1.10.m10.1.1.1.1.1.1.2" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.10.m10.1.1.1.1.1.1.2.2" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.2.2.cmml">P</mi><mn id="S3.SS1.p1.10.m10.1.1.1.1.1.1.2.3" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.2.3.cmml">3</mn></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.10.m10.1.1.1.1.1.1.1" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.1.cmml">⋅</mo><mi id="S3.SS1.p1.10.m10.1.1.1.1.1.1.3" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.3.cmml">C</mi></mrow><mo stretchy="false" id="S3.SS1.p1.10.m10.1.1.1.1.1.3" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.2.cmml" xref="S3.SS1.p1.10.m10.1.2"><in id="S3.SS1.p1.10.m10.1.2.1.cmml" xref="S3.SS1.p1.10.m10.1.2.1"></in><apply id="S3.SS1.p1.10.m10.1.2.2.cmml" xref="S3.SS1.p1.10.m10.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.2.2.1.cmml" xref="S3.SS1.p1.10.m10.1.2.2">subscript</csymbol><ci id="S3.SS1.p1.10.m10.1.2.2.2.cmml" xref="S3.SS1.p1.10.m10.1.2.2.2">𝑥</ci><ci id="S3.SS1.p1.10.m10.1.2.2.3.cmml" xref="S3.SS1.p1.10.m10.1.2.2.3">𝑝</ci></apply><apply id="S3.SS1.p1.10.m10.1.2.3.cmml" xref="S3.SS1.p1.10.m10.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.2.3.1.cmml" xref="S3.SS1.p1.10.m10.1.2.3">superscript</csymbol><ci id="S3.SS1.p1.10.m10.1.2.3.2.cmml" xref="S3.SS1.p1.10.m10.1.2.3.2">ℝ</ci><apply id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1.1"><times id="S3.SS1.p1.10.m10.1.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.1.2"></times><ci id="S3.SS1.p1.10.m10.1.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.1.3">𝑁</ci><apply id="S3.SS1.p1.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1"><ci id="S3.SS1.p1.10.m10.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.1">⋅</ci><apply id="S3.SS1.p1.10.m10.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.2.2">𝑃</ci><cn type="integer" id="S3.SS1.p1.10.m10.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.2.3">3</cn></apply><ci id="S3.SS1.p1.10.m10.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.3">𝐶</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">x_{p}\in\mathbb{R}^{N\times(P^{3}\cdot C)}</annotation></semantics></math> where <math id="S3.SS1.p1.11.m11.1" class="ltx_Math" alttext="N=\frac{H\times W\times D}{P^{3}}" display="inline"><semantics id="S3.SS1.p1.11.m11.1a"><mrow id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml"><mi id="S3.SS1.p1.11.m11.1.1.2" xref="S3.SS1.p1.11.m11.1.1.2.cmml">N</mi><mo id="S3.SS1.p1.11.m11.1.1.1" xref="S3.SS1.p1.11.m11.1.1.1.cmml">=</mo><mfrac id="S3.SS1.p1.11.m11.1.1.3" xref="S3.SS1.p1.11.m11.1.1.3.cmml"><mrow id="S3.SS1.p1.11.m11.1.1.3.2" xref="S3.SS1.p1.11.m11.1.1.3.2.cmml"><mi id="S3.SS1.p1.11.m11.1.1.3.2.2" xref="S3.SS1.p1.11.m11.1.1.3.2.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.11.m11.1.1.3.2.1" xref="S3.SS1.p1.11.m11.1.1.3.2.1.cmml">×</mo><mi id="S3.SS1.p1.11.m11.1.1.3.2.3" xref="S3.SS1.p1.11.m11.1.1.3.2.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.11.m11.1.1.3.2.1a" xref="S3.SS1.p1.11.m11.1.1.3.2.1.cmml">×</mo><mi id="S3.SS1.p1.11.m11.1.1.3.2.4" xref="S3.SS1.p1.11.m11.1.1.3.2.4.cmml">D</mi></mrow><msup id="S3.SS1.p1.11.m11.1.1.3.3" xref="S3.SS1.p1.11.m11.1.1.3.3.cmml"><mi id="S3.SS1.p1.11.m11.1.1.3.3.2" xref="S3.SS1.p1.11.m11.1.1.3.3.2.cmml">P</mi><mn id="S3.SS1.p1.11.m11.1.1.3.3.3" xref="S3.SS1.p1.11.m11.1.1.3.3.3.cmml">3</mn></msup></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><apply id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1"><eq id="S3.SS1.p1.11.m11.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1.1"></eq><ci id="S3.SS1.p1.11.m11.1.1.2.cmml" xref="S3.SS1.p1.11.m11.1.1.2">𝑁</ci><apply id="S3.SS1.p1.11.m11.1.1.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3"><divide id="S3.SS1.p1.11.m11.1.1.3.1.cmml" xref="S3.SS1.p1.11.m11.1.1.3"></divide><apply id="S3.SS1.p1.11.m11.1.1.3.2.cmml" xref="S3.SS1.p1.11.m11.1.1.3.2"><times id="S3.SS1.p1.11.m11.1.1.3.2.1.cmml" xref="S3.SS1.p1.11.m11.1.1.3.2.1"></times><ci id="S3.SS1.p1.11.m11.1.1.3.2.2.cmml" xref="S3.SS1.p1.11.m11.1.1.3.2.2">𝐻</ci><ci id="S3.SS1.p1.11.m11.1.1.3.2.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3.2.3">𝑊</ci><ci id="S3.SS1.p1.11.m11.1.1.3.2.4.cmml" xref="S3.SS1.p1.11.m11.1.1.3.2.4">𝐷</ci></apply><apply id="S3.SS1.p1.11.m11.1.1.3.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m11.1.1.3.3.1.cmml" xref="S3.SS1.p1.11.m11.1.1.3.3">superscript</csymbol><ci id="S3.SS1.p1.11.m11.1.1.3.3.2.cmml" xref="S3.SS1.p1.11.m11.1.1.3.3.2">𝑃</ci><cn type="integer" id="S3.SS1.p1.11.m11.1.1.3.3.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">N=\frac{H\times W\times D}{P^{3}}</annotation></semantics></math> and <math id="S3.SS1.p1.12.m12.3" class="ltx_Math" alttext="(P,P,P)" display="inline"><semantics id="S3.SS1.p1.12.m12.3a"><mrow id="S3.SS1.p1.12.m12.3.4.2" xref="S3.SS1.p1.12.m12.3.4.1.cmml"><mo stretchy="false" id="S3.SS1.p1.12.m12.3.4.2.1" xref="S3.SS1.p1.12.m12.3.4.1.cmml">(</mo><mi id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml">P</mi><mo id="S3.SS1.p1.12.m12.3.4.2.2" xref="S3.SS1.p1.12.m12.3.4.1.cmml">,</mo><mi id="S3.SS1.p1.12.m12.2.2" xref="S3.SS1.p1.12.m12.2.2.cmml">P</mi><mo id="S3.SS1.p1.12.m12.3.4.2.3" xref="S3.SS1.p1.12.m12.3.4.1.cmml">,</mo><mi id="S3.SS1.p1.12.m12.3.3" xref="S3.SS1.p1.12.m12.3.3.cmml">P</mi><mo stretchy="false" id="S3.SS1.p1.12.m12.3.4.2.4" xref="S3.SS1.p1.12.m12.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.3b"><vector id="S3.SS1.p1.12.m12.3.4.1.cmml" xref="S3.SS1.p1.12.m12.3.4.2"><ci id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">𝑃</ci><ci id="S3.SS1.p1.12.m12.2.2.cmml" xref="S3.SS1.p1.12.m12.2.2">𝑃</ci><ci id="S3.SS1.p1.12.m12.3.3.cmml" xref="S3.SS1.p1.12.m12.3.3">𝑃</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.3c">(P,P,P)</annotation></semantics></math> is the size of the patch. These patches are associated with positional embeddings to preserve the positional information tied to the original data and then passed into the transformer encoder. The basic transformer encoder in ViT consists of a multi-head self-attention (MSA) and multi-layer perceptron (MLP) blocks, which produce an output <math id="S3.SS1.p1.13.m13.1" class="ltx_Math" alttext="x_{z}\in\mathbb{R}^{N\times S}" display="inline"><semantics id="S3.SS1.p1.13.m13.1a"><mrow id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml"><msub id="S3.SS1.p1.13.m13.1.1.2" xref="S3.SS1.p1.13.m13.1.1.2.cmml"><mi id="S3.SS1.p1.13.m13.1.1.2.2" xref="S3.SS1.p1.13.m13.1.1.2.2.cmml">x</mi><mi id="S3.SS1.p1.13.m13.1.1.2.3" xref="S3.SS1.p1.13.m13.1.1.2.3.cmml">z</mi></msub><mo id="S3.SS1.p1.13.m13.1.1.1" xref="S3.SS1.p1.13.m13.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.13.m13.1.1.3" xref="S3.SS1.p1.13.m13.1.1.3.cmml"><mi id="S3.SS1.p1.13.m13.1.1.3.2" xref="S3.SS1.p1.13.m13.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.13.m13.1.1.3.3" xref="S3.SS1.p1.13.m13.1.1.3.3.cmml"><mi id="S3.SS1.p1.13.m13.1.1.3.3.2" xref="S3.SS1.p1.13.m13.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.13.m13.1.1.3.3.1" xref="S3.SS1.p1.13.m13.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.13.m13.1.1.3.3.3" xref="S3.SS1.p1.13.m13.1.1.3.3.3.cmml">S</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><apply id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1"><in id="S3.SS1.p1.13.m13.1.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1.1"></in><apply id="S3.SS1.p1.13.m13.1.1.2.cmml" xref="S3.SS1.p1.13.m13.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m13.1.1.2.1.cmml" xref="S3.SS1.p1.13.m13.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.13.m13.1.1.2.2.cmml" xref="S3.SS1.p1.13.m13.1.1.2.2">𝑥</ci><ci id="S3.SS1.p1.13.m13.1.1.2.3.cmml" xref="S3.SS1.p1.13.m13.1.1.2.3">𝑧</ci></apply><apply id="S3.SS1.p1.13.m13.1.1.3.cmml" xref="S3.SS1.p1.13.m13.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m13.1.1.3.1.cmml" xref="S3.SS1.p1.13.m13.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.13.m13.1.1.3.2.cmml" xref="S3.SS1.p1.13.m13.1.1.3.2">ℝ</ci><apply id="S3.SS1.p1.13.m13.1.1.3.3.cmml" xref="S3.SS1.p1.13.m13.1.1.3.3"><times id="S3.SS1.p1.13.m13.1.1.3.3.1.cmml" xref="S3.SS1.p1.13.m13.1.1.3.3.1"></times><ci id="S3.SS1.p1.13.m13.1.1.3.3.2.cmml" xref="S3.SS1.p1.13.m13.1.1.3.3.2">𝑁</ci><ci id="S3.SS1.p1.13.m13.1.1.3.3.3.cmml" xref="S3.SS1.p1.13.m13.1.1.3.3.3">𝑆</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">x_{z}\in\mathbb{R}^{N\times S}</annotation></semantics></math>, where <math id="S3.SS1.p1.14.m14.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS1.p1.14.m14.1a"><mi id="S3.SS1.p1.14.m14.1.1" xref="S3.SS1.p1.14.m14.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m14.1b"><ci id="S3.SS1.p1.14.m14.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m14.1c">S</annotation></semantics></math> is the projection following the MLP blocks. The MSA block learns a mapping between a query (q), a corresponding key (k), and a value (v) representation of the encoder output, measured by <math id="S3.SS1.p1.15.m15.1" class="ltx_Math" alttext="\text{[{q,k,v}]}=x_{z}\textbf{U\textsubscript{qkv}}" display="inline"><semantics id="S3.SS1.p1.15.m15.1a"><mrow id="S3.SS1.p1.15.m15.1.1" xref="S3.SS1.p1.15.m15.1.1.cmml"><mrow id="S3.SS1.p1.15.m15.1.1.2" xref="S3.SS1.p1.15.m15.1.1.2d.cmml"><mtext id="S3.SS1.p1.15.m15.1.1.2a" xref="S3.SS1.p1.15.m15.1.1.2d.cmml">[</mtext><mtext class="ltx_mathvariant_bold" id="S3.SS1.p1.15.m15.1.1.2b" xref="S3.SS1.p1.15.m15.1.1.2d.cmml">q,k,v</mtext><mtext id="S3.SS1.p1.15.m15.1.1.2c" xref="S3.SS1.p1.15.m15.1.1.2d.cmml">]</mtext></mrow><mo id="S3.SS1.p1.15.m15.1.1.1" xref="S3.SS1.p1.15.m15.1.1.1.cmml">=</mo><mrow id="S3.SS1.p1.15.m15.1.1.3" xref="S3.SS1.p1.15.m15.1.1.3.cmml"><msub id="S3.SS1.p1.15.m15.1.1.3.2" xref="S3.SS1.p1.15.m15.1.1.3.2.cmml"><mi id="S3.SS1.p1.15.m15.1.1.3.2.2" xref="S3.SS1.p1.15.m15.1.1.3.2.2.cmml">x</mi><mi id="S3.SS1.p1.15.m15.1.1.3.2.3" xref="S3.SS1.p1.15.m15.1.1.3.2.3.cmml">z</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.15.m15.1.1.3.1" xref="S3.SS1.p1.15.m15.1.1.3.1.cmml">​</mo><mrow id="S3.SS1.p1.15.m15.1.1.3.3" xref="S3.SS1.p1.15.m15.1.1.3.3e.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p1.15.m15.1.1.3.3a" xref="S3.SS1.p1.15.m15.1.1.3.3e.cmml">U</mtext><mtext id="S3.SS1.p1.15.m15.1.1.3.3b" xref="S3.SS1.p1.15.m15.1.1.3.3e.cmml"><sub id="S3.SS1.p1.15.m15.1.1.3.3.2nest" class="ltx_sub"><span id="S3.SS1.p1.15.m15.1.1.3.3.2.1nest" class="ltx_text ltx_font_bold">qkv</span></sub></mtext></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m15.1b"><apply id="S3.SS1.p1.15.m15.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1"><eq id="S3.SS1.p1.15.m15.1.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1.1"></eq><ci id="S3.SS1.p1.15.m15.1.1.2d.cmml" xref="S3.SS1.p1.15.m15.1.1.2"><mrow id="S3.SS1.p1.15.m15.1.1.2.cmml" xref="S3.SS1.p1.15.m15.1.1.2"><mtext id="S3.SS1.p1.15.m15.1.1.2a.cmml" xref="S3.SS1.p1.15.m15.1.1.2">[</mtext><mtext class="ltx_mathvariant_bold" id="S3.SS1.p1.15.m15.1.1.2b.cmml" xref="S3.SS1.p1.15.m15.1.1.2">q,k,v</mtext><mtext id="S3.SS1.p1.15.m15.1.1.2c.cmml" xref="S3.SS1.p1.15.m15.1.1.2">]</mtext></mrow></ci><apply id="S3.SS1.p1.15.m15.1.1.3.cmml" xref="S3.SS1.p1.15.m15.1.1.3"><times id="S3.SS1.p1.15.m15.1.1.3.1.cmml" xref="S3.SS1.p1.15.m15.1.1.3.1"></times><apply id="S3.SS1.p1.15.m15.1.1.3.2.cmml" xref="S3.SS1.p1.15.m15.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p1.15.m15.1.1.3.2.1.cmml" xref="S3.SS1.p1.15.m15.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p1.15.m15.1.1.3.2.2.cmml" xref="S3.SS1.p1.15.m15.1.1.3.2.2">𝑥</ci><ci id="S3.SS1.p1.15.m15.1.1.3.2.3.cmml" xref="S3.SS1.p1.15.m15.1.1.3.2.3">𝑧</ci></apply><ci id="S3.SS1.p1.15.m15.1.1.3.3e.cmml" xref="S3.SS1.p1.15.m15.1.1.3.3"><mrow id="S3.SS1.p1.15.m15.1.1.3.3.cmml" xref="S3.SS1.p1.15.m15.1.1.3.3"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p1.15.m15.1.1.3.3a.cmml" xref="S3.SS1.p1.15.m15.1.1.3.3">U</mtext><mtext id="S3.SS1.p1.15.m15.1.1.3.3b.cmml" xref="S3.SS1.p1.15.m15.1.1.3.3"><sub id="S3.SS1.p1.15.m15.1.1.3.3.2anest" class="ltx_sub"><span id="S3.SS1.p1.15.m15.1.1.3.3.2.1anest" class="ltx_text ltx_font_bold">qkv</span></sub></mtext></mrow></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m15.1c">\text{[{q,k,v}]}=x_{z}\textbf{U\textsubscript{qkv}}</annotation></semantics></math>
where <math id="S3.SS1.p1.16.m16.1" class="ltx_Math" alttext="U\textsubscript{qkv}" display="inline"><semantics id="S3.SS1.p1.16.m16.1a"><mrow id="S3.SS1.p1.16.m16.1.1" xref="S3.SS1.p1.16.m16.1.1.cmml"><mi id="S3.SS1.p1.16.m16.1.1.2" xref="S3.SS1.p1.16.m16.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.16.m16.1.1.1" xref="S3.SS1.p1.16.m16.1.1.1.cmml">​</mo><mtext id="S3.SS1.p1.16.m16.1.1.3" xref="S3.SS1.p1.16.m16.1.1.3b.cmml"><sub id="S3.SS1.p1.16.m16.1.1.3.1nest" class="ltx_sub">qkv</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.16.m16.1b"><apply id="S3.SS1.p1.16.m16.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1"><times id="S3.SS1.p1.16.m16.1.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1.1"></times><ci id="S3.SS1.p1.16.m16.1.1.2.cmml" xref="S3.SS1.p1.16.m16.1.1.2">𝑈</ci><ci id="S3.SS1.p1.16.m16.1.1.3b.cmml" xref="S3.SS1.p1.16.m16.1.1.3"><mtext id="S3.SS1.p1.16.m16.1.1.3.cmml" xref="S3.SS1.p1.16.m16.1.1.3"><sub id="S3.SS1.p1.16.m16.1.1.3.1anest" class="ltx_sub">qkv</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.16.m16.1c">U\textsubscript{qkv}</annotation></semantics></math> is the projection MLP weights. The <span id="S3.SS1.p1.17.1" class="ltx_text ltx_font_bold">q</span> and <span id="S3.SS1.p1.17.2" class="ltx_text ltx_font_bold">k</span> representations are then used to find the attention weights <math id="S3.SS1.p1.17.m17.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p1.17.m17.1a"><mi id="S3.SS1.p1.17.m17.1.1" xref="S3.SS1.p1.17.m17.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.17.m17.1b"><ci id="S3.SS1.p1.17.m17.1.1.cmml" xref="S3.SS1.p1.17.m17.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.17.m17.1c">A</annotation></semantics></math> through</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\textbf{A}=\text{Softmax}(\frac{\textbf{qk}^{\top}}{\sqrt{S_{l}}})" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.2" xref="S3.E1.m1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.2.2" xref="S3.E1.m1.1.2.2a.cmml">A</mtext><mo id="S3.E1.m1.1.2.1" xref="S3.E1.m1.1.2.1.cmml">=</mo><mrow id="S3.E1.m1.1.2.3" xref="S3.E1.m1.1.2.3.cmml"><mtext id="S3.E1.m1.1.2.3.2" xref="S3.E1.m1.1.2.3.2a.cmml">Softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.3.1" xref="S3.E1.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E1.m1.1.2.3.3.2" xref="S3.E1.m1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.2.3.3.2.1" xref="S3.E1.m1.1.1.cmml">(</mo><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msup id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2a.cmml">qk</mtext><mo id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml">⊤</mo></msup><msqrt id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">S</mi><mi id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml">l</mi></msub></msqrt></mfrac><mo stretchy="false" id="S3.E1.m1.1.2.3.3.2.2" xref="S3.E1.m1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.2.cmml" xref="S3.E1.m1.1.2"><eq id="S3.E1.m1.1.2.1.cmml" xref="S3.E1.m1.1.2.1"></eq><ci id="S3.E1.m1.1.2.2a.cmml" xref="S3.E1.m1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.2.2.cmml" xref="S3.E1.m1.1.2.2">A</mtext></ci><apply id="S3.E1.m1.1.2.3.cmml" xref="S3.E1.m1.1.2.3"><times id="S3.E1.m1.1.2.3.1.cmml" xref="S3.E1.m1.1.2.3.1"></times><ci id="S3.E1.m1.1.2.3.2a.cmml" xref="S3.E1.m1.1.2.3.2"><mtext id="S3.E1.m1.1.2.3.2.cmml" xref="S3.E1.m1.1.2.3.2">Softmax</mtext></ci><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.2.3.3.2"><divide id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.2.3.3.2"></divide><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2">superscript</csymbol><ci id="S3.E1.m1.1.1.2.2a.cmml" xref="S3.E1.m1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">qk</mtext></ci><csymbol cd="latexml" id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3">top</csymbol></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><root id="S3.E1.m1.1.1.3a.cmml" xref="S3.E1.m1.1.1.3"></root><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">𝑆</ci><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">𝑙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\textbf{A}=\text{Softmax}(\frac{\textbf{qk}^{\top}}{\sqrt{S_{l}}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.25" class="ltx_p"><math id="S3.SS1.p1.18.m1.1" class="ltx_Math" alttext="S_{l}" display="inline"><semantics id="S3.SS1.p1.18.m1.1a"><msub id="S3.SS1.p1.18.m1.1.1" xref="S3.SS1.p1.18.m1.1.1.cmml"><mi id="S3.SS1.p1.18.m1.1.1.2" xref="S3.SS1.p1.18.m1.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.18.m1.1.1.3" xref="S3.SS1.p1.18.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.18.m1.1b"><apply id="S3.SS1.p1.18.m1.1.1.cmml" xref="S3.SS1.p1.18.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.18.m1.1.1.1.cmml" xref="S3.SS1.p1.18.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.18.m1.1.1.2.cmml" xref="S3.SS1.p1.18.m1.1.1.2">𝑆</ci><ci id="S3.SS1.p1.18.m1.1.1.3.cmml" xref="S3.SS1.p1.18.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.18.m1.1c">S_{l}</annotation></semantics></math> a scaled version of <math id="S3.SS1.p1.19.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS1.p1.19.m2.1a"><mi id="S3.SS1.p1.19.m2.1.1" xref="S3.SS1.p1.19.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.19.m2.1b"><ci id="S3.SS1.p1.19.m2.1.1.cmml" xref="S3.SS1.p1.19.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.19.m2.1c">S</annotation></semantics></math> by a factor <math id="S3.SS1.p1.20.m3.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS1.p1.20.m3.1a"><mi id="S3.SS1.p1.20.m3.1.1" xref="S3.SS1.p1.20.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.20.m3.1b"><ci id="S3.SS1.p1.20.m3.1.1.cmml" xref="S3.SS1.p1.20.m3.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.20.m3.1c">l</annotation></semantics></math> and is set to <math id="S3.SS1.p1.21.m4.1" class="ltx_Math" alttext="\frac{S}{l}" display="inline"><semantics id="S3.SS1.p1.21.m4.1a"><mfrac id="S3.SS1.p1.21.m4.1.1" xref="S3.SS1.p1.21.m4.1.1.cmml"><mi id="S3.SS1.p1.21.m4.1.1.2" xref="S3.SS1.p1.21.m4.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.21.m4.1.1.3" xref="S3.SS1.p1.21.m4.1.1.3.cmml">l</mi></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.21.m4.1b"><apply id="S3.SS1.p1.21.m4.1.1.cmml" xref="S3.SS1.p1.21.m4.1.1"><divide id="S3.SS1.p1.21.m4.1.1.1.cmml" xref="S3.SS1.p1.21.m4.1.1"></divide><ci id="S3.SS1.p1.21.m4.1.1.2.cmml" xref="S3.SS1.p1.21.m4.1.1.2">𝑆</ci><ci id="S3.SS1.p1.21.m4.1.1.3.cmml" xref="S3.SS1.p1.21.m4.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.21.m4.1c">\frac{S}{l}</annotation></semantics></math>, keeping the total number of parameters constant given a variation in the number of key values <span id="S3.SS1.p1.25.1" class="ltx_text ltx_markedasmath ltx_font_bold">k</span>. Using the attention weights, we then measure the self-attention <math id="S3.SS1.p1.23.m6.1" class="ltx_Math" alttext="SA" display="inline"><semantics id="S3.SS1.p1.23.m6.1a"><mrow id="S3.SS1.p1.23.m6.1.1" xref="S3.SS1.p1.23.m6.1.1.cmml"><mi id="S3.SS1.p1.23.m6.1.1.2" xref="S3.SS1.p1.23.m6.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.23.m6.1.1.1" xref="S3.SS1.p1.23.m6.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.23.m6.1.1.3" xref="S3.SS1.p1.23.m6.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.23.m6.1b"><apply id="S3.SS1.p1.23.m6.1.1.cmml" xref="S3.SS1.p1.23.m6.1.1"><times id="S3.SS1.p1.23.m6.1.1.1.cmml" xref="S3.SS1.p1.23.m6.1.1.1"></times><ci id="S3.SS1.p1.23.m6.1.1.2.cmml" xref="S3.SS1.p1.23.m6.1.1.2">𝑆</ci><ci id="S3.SS1.p1.23.m6.1.1.3.cmml" xref="S3.SS1.p1.23.m6.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.23.m6.1c">SA</annotation></semantics></math> with
<math id="S3.SS1.p1.24.m7.1" class="ltx_Math" alttext="\text{SA(}x_{z}\text{)}=A\text{v}" display="inline"><semantics id="S3.SS1.p1.24.m7.1a"><mrow id="S3.SS1.p1.24.m7.1.1" xref="S3.SS1.p1.24.m7.1.1.cmml"><mrow id="S3.SS1.p1.24.m7.1.1.2" xref="S3.SS1.p1.24.m7.1.1.2.cmml"><mtext id="S3.SS1.p1.24.m7.1.1.2.2" xref="S3.SS1.p1.24.m7.1.1.2.2a.cmml">SA(</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.p1.24.m7.1.1.2.1" xref="S3.SS1.p1.24.m7.1.1.2.1.cmml">​</mo><msub id="S3.SS1.p1.24.m7.1.1.2.3" xref="S3.SS1.p1.24.m7.1.1.2.3.cmml"><mi id="S3.SS1.p1.24.m7.1.1.2.3.2" xref="S3.SS1.p1.24.m7.1.1.2.3.2.cmml">x</mi><mi id="S3.SS1.p1.24.m7.1.1.2.3.3" xref="S3.SS1.p1.24.m7.1.1.2.3.3.cmml">z</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.24.m7.1.1.2.1a" xref="S3.SS1.p1.24.m7.1.1.2.1.cmml">​</mo><mtext id="S3.SS1.p1.24.m7.1.1.2.4" xref="S3.SS1.p1.24.m7.1.1.2.4a.cmml">)</mtext></mrow><mo id="S3.SS1.p1.24.m7.1.1.1" xref="S3.SS1.p1.24.m7.1.1.1.cmml">=</mo><mrow id="S3.SS1.p1.24.m7.1.1.3" xref="S3.SS1.p1.24.m7.1.1.3.cmml"><mi id="S3.SS1.p1.24.m7.1.1.3.2" xref="S3.SS1.p1.24.m7.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.24.m7.1.1.3.1" xref="S3.SS1.p1.24.m7.1.1.3.1.cmml">​</mo><mtext id="S3.SS1.p1.24.m7.1.1.3.3" xref="S3.SS1.p1.24.m7.1.1.3.3a.cmml">v</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.24.m7.1b"><apply id="S3.SS1.p1.24.m7.1.1.cmml" xref="S3.SS1.p1.24.m7.1.1"><eq id="S3.SS1.p1.24.m7.1.1.1.cmml" xref="S3.SS1.p1.24.m7.1.1.1"></eq><apply id="S3.SS1.p1.24.m7.1.1.2.cmml" xref="S3.SS1.p1.24.m7.1.1.2"><times id="S3.SS1.p1.24.m7.1.1.2.1.cmml" xref="S3.SS1.p1.24.m7.1.1.2.1"></times><ci id="S3.SS1.p1.24.m7.1.1.2.2a.cmml" xref="S3.SS1.p1.24.m7.1.1.2.2"><mtext id="S3.SS1.p1.24.m7.1.1.2.2.cmml" xref="S3.SS1.p1.24.m7.1.1.2.2">SA(</mtext></ci><apply id="S3.SS1.p1.24.m7.1.1.2.3.cmml" xref="S3.SS1.p1.24.m7.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.24.m7.1.1.2.3.1.cmml" xref="S3.SS1.p1.24.m7.1.1.2.3">subscript</csymbol><ci id="S3.SS1.p1.24.m7.1.1.2.3.2.cmml" xref="S3.SS1.p1.24.m7.1.1.2.3.2">𝑥</ci><ci id="S3.SS1.p1.24.m7.1.1.2.3.3.cmml" xref="S3.SS1.p1.24.m7.1.1.2.3.3">𝑧</ci></apply><ci id="S3.SS1.p1.24.m7.1.1.2.4a.cmml" xref="S3.SS1.p1.24.m7.1.1.2.4"><mtext id="S3.SS1.p1.24.m7.1.1.2.4.cmml" xref="S3.SS1.p1.24.m7.1.1.2.4">)</mtext></ci></apply><apply id="S3.SS1.p1.24.m7.1.1.3.cmml" xref="S3.SS1.p1.24.m7.1.1.3"><times id="S3.SS1.p1.24.m7.1.1.3.1.cmml" xref="S3.SS1.p1.24.m7.1.1.3.1"></times><ci id="S3.SS1.p1.24.m7.1.1.3.2.cmml" xref="S3.SS1.p1.24.m7.1.1.3.2">𝐴</ci><ci id="S3.SS1.p1.24.m7.1.1.3.3a.cmml" xref="S3.SS1.p1.24.m7.1.1.3.3"><mtext id="S3.SS1.p1.24.m7.1.1.3.3.cmml" xref="S3.SS1.p1.24.m7.1.1.3.3">v</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.24.m7.1c">\text{SA(}x_{z}\text{)}=A\text{v}</annotation></semantics></math>.
MSA scales the previous expression up to a sequence of self-attention heads in the form of a vector with each head having its own unique set of weights in <math id="S3.SS1.p1.25.m8.1" class="ltx_Math" alttext="U\textsubscript{msa}" display="inline"><semantics id="S3.SS1.p1.25.m8.1a"><mrow id="S3.SS1.p1.25.m8.1.1" xref="S3.SS1.p1.25.m8.1.1.cmml"><mi id="S3.SS1.p1.25.m8.1.1.2" xref="S3.SS1.p1.25.m8.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.25.m8.1.1.1" xref="S3.SS1.p1.25.m8.1.1.1.cmml">​</mo><mtext id="S3.SS1.p1.25.m8.1.1.3" xref="S3.SS1.p1.25.m8.1.1.3b.cmml"><sub id="S3.SS1.p1.25.m8.1.1.3.1nest" class="ltx_sub">msa</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.25.m8.1b"><apply id="S3.SS1.p1.25.m8.1.1.cmml" xref="S3.SS1.p1.25.m8.1.1"><times id="S3.SS1.p1.25.m8.1.1.1.cmml" xref="S3.SS1.p1.25.m8.1.1.1"></times><ci id="S3.SS1.p1.25.m8.1.1.2.cmml" xref="S3.SS1.p1.25.m8.1.1.2">𝑈</ci><ci id="S3.SS1.p1.25.m8.1.1.3b.cmml" xref="S3.SS1.p1.25.m8.1.1.3"><mtext id="S3.SS1.p1.25.m8.1.1.3.cmml" xref="S3.SS1.p1.25.m8.1.1.3"><sub id="S3.SS1.p1.25.m8.1.1.3.1anest" class="ltx_sub">msa</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.25.m8.1c">U\textsubscript{msa}</annotation></semantics></math>, described by</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\text{MSA(}x_{z}\text{)}=\text{[SA\textsubscript{1}({x\textsubscript{z}});SA\textsubscript{2}({x\textsubscript{z}}); ... ;SA\textsubscript{m}({x\textsubscript{z}})]}\textbf{U\textsubscript{msa}}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mrow id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mtext id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2a.cmml">MSA(</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.2.1" xref="S3.E2.m1.1.1.2.1.cmml">​</mo><msub id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml"><mi id="S3.E2.m1.1.1.2.3.2" xref="S3.E2.m1.1.1.2.3.2.cmml">x</mi><mi id="S3.E2.m1.1.1.2.3.3" xref="S3.E2.m1.1.1.2.3.3.cmml">z</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.2.1a" xref="S3.E2.m1.1.1.2.1.cmml">​</mo><mtext id="S3.E2.m1.1.1.2.4" xref="S3.E2.m1.1.1.2.4a.cmml">)</mtext></mrow><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3ae.cmml"><mrow id="S3.E2.m1.1.1.3a" xref="S3.E2.m1.1.1.3ae.cmml"><mtext id="S3.E2.m1.1.1.3b" xref="S3.E2.m1.1.1.3ae.cmml">[SA</mtext><mtext id="S3.E2.m1.1.1.3c" xref="S3.E2.m1.1.1.3ae.cmml"><sub id="S3.E2.m1.1.1.3.1.1nest" class="ltx_sub">1</sub></mtext><mtext id="S3.E2.m1.1.1.3e" xref="S3.E2.m1.1.1.3ae.cmml">(</mtext><mrow id="S3.E2.m1.1.1.3f" xref="S3.E2.m1.1.1.3ae.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3g" xref="S3.E2.m1.1.1.3ae.cmml">x</mtext><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3h" xref="S3.E2.m1.1.1.3ae.cmml"><sub id="S3.E2.m1.1.1.3.1.2.1nest" class="ltx_sub">z</sub></mtext></mrow><mtext id="S3.E2.m1.1.1.3j" xref="S3.E2.m1.1.1.3ae.cmml">);SA</mtext><mtext id="S3.E2.m1.1.1.3k" xref="S3.E2.m1.1.1.3ae.cmml"><sub id="S3.E2.m1.1.1.3.1.3nest" class="ltx_sub">2</sub></mtext><mtext id="S3.E2.m1.1.1.3m" xref="S3.E2.m1.1.1.3ae.cmml">(</mtext><mrow id="S3.E2.m1.1.1.3n" xref="S3.E2.m1.1.1.3ae.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3o" xref="S3.E2.m1.1.1.3ae.cmml">x</mtext><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3p" xref="S3.E2.m1.1.1.3ae.cmml"><sub id="S3.E2.m1.1.1.3.1.4.1nest" class="ltx_sub">z</sub></mtext></mrow><mtext id="S3.E2.m1.1.1.3r" xref="S3.E2.m1.1.1.3ae.cmml">); … ;SA</mtext><mtext id="S3.E2.m1.1.1.3s" xref="S3.E2.m1.1.1.3ae.cmml"><sub id="S3.E2.m1.1.1.3.1.5nest" class="ltx_sub">m</sub></mtext><mtext id="S3.E2.m1.1.1.3u" xref="S3.E2.m1.1.1.3ae.cmml">(</mtext><mrow id="S3.E2.m1.1.1.3v" xref="S3.E2.m1.1.1.3ae.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3w" xref="S3.E2.m1.1.1.3ae.cmml">x</mtext><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3x" xref="S3.E2.m1.1.1.3ae.cmml"><sub id="S3.E2.m1.1.1.3.1.6.1nest" class="ltx_sub">z</sub></mtext></mrow><mtext id="S3.E2.m1.1.1.3z" xref="S3.E2.m1.1.1.3ae.cmml">)]</mtext></mrow><mrow id="S3.E2.m1.1.1.3aa" xref="S3.E2.m1.1.1.3ae.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3ab" xref="S3.E2.m1.1.1.3ae.cmml">U</mtext><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3ac" xref="S3.E2.m1.1.1.3ae.cmml"><sub id="S3.E2.m1.1.1.3.2.1nest" class="ltx_sub">msa</sub></mtext></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><times id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2.1"></times><ci id="S3.E2.m1.1.1.2.2a.cmml" xref="S3.E2.m1.1.1.2.2"><mtext id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">MSA(</mtext></ci><apply id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.3.1.cmml" xref="S3.E2.m1.1.1.2.3">subscript</csymbol><ci id="S3.E2.m1.1.1.2.3.2.cmml" xref="S3.E2.m1.1.1.2.3.2">𝑥</ci><ci id="S3.E2.m1.1.1.2.3.3.cmml" xref="S3.E2.m1.1.1.2.3.3">𝑧</ci></apply><ci id="S3.E2.m1.1.1.2.4a.cmml" xref="S3.E2.m1.1.1.2.4"><mtext id="S3.E2.m1.1.1.2.4.cmml" xref="S3.E2.m1.1.1.2.4">)</mtext></ci></apply><ci id="S3.E2.m1.1.1.3ae.cmml" xref="S3.E2.m1.1.1.3"><mrow id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><mrow id="S3.E2.m1.1.1.3a.cmml" xref="S3.E2.m1.1.1.3"><mtext id="S3.E2.m1.1.1.3b.cmml" xref="S3.E2.m1.1.1.3">[SA</mtext><mtext id="S3.E2.m1.1.1.3c.cmml" xref="S3.E2.m1.1.1.3"><sub id="S3.E2.m1.1.1.3.1.1anest" class="ltx_sub">1</sub></mtext><mtext id="S3.E2.m1.1.1.3e.cmml" xref="S3.E2.m1.1.1.3">(</mtext><mrow id="S3.E2.m1.1.1.3f.cmml" xref="S3.E2.m1.1.1.3"><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3g.cmml" xref="S3.E2.m1.1.1.3">x</mtext><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3h.cmml" xref="S3.E2.m1.1.1.3"><sub id="S3.E2.m1.1.1.3.1.2.1anest" class="ltx_sub">z</sub></mtext></mrow><mtext id="S3.E2.m1.1.1.3j.cmml" xref="S3.E2.m1.1.1.3">);SA</mtext><mtext id="S3.E2.m1.1.1.3k.cmml" xref="S3.E2.m1.1.1.3"><sub id="S3.E2.m1.1.1.3.1.3anest" class="ltx_sub">2</sub></mtext><mtext id="S3.E2.m1.1.1.3m.cmml" xref="S3.E2.m1.1.1.3">(</mtext><mrow id="S3.E2.m1.1.1.3n.cmml" xref="S3.E2.m1.1.1.3"><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3o.cmml" xref="S3.E2.m1.1.1.3">x</mtext><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3p.cmml" xref="S3.E2.m1.1.1.3"><sub id="S3.E2.m1.1.1.3.1.4.1anest" class="ltx_sub">z</sub></mtext></mrow><mtext id="S3.E2.m1.1.1.3r.cmml" xref="S3.E2.m1.1.1.3">); … ;SA</mtext><mtext id="S3.E2.m1.1.1.3s.cmml" xref="S3.E2.m1.1.1.3"><sub id="S3.E2.m1.1.1.3.1.5anest" class="ltx_sub">m</sub></mtext><mtext id="S3.E2.m1.1.1.3u.cmml" xref="S3.E2.m1.1.1.3">(</mtext><mrow id="S3.E2.m1.1.1.3v.cmml" xref="S3.E2.m1.1.1.3"><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3w.cmml" xref="S3.E2.m1.1.1.3">x</mtext><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3x.cmml" xref="S3.E2.m1.1.1.3"><sub id="S3.E2.m1.1.1.3.1.6.1anest" class="ltx_sub">z</sub></mtext></mrow><mtext id="S3.E2.m1.1.1.3z.cmml" xref="S3.E2.m1.1.1.3">)]</mtext></mrow><mrow id="S3.E2.m1.1.1.3aa.cmml" xref="S3.E2.m1.1.1.3"><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3ab.cmml" xref="S3.E2.m1.1.1.3">U</mtext><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.1.1.3ac.cmml" xref="S3.E2.m1.1.1.3"><sub id="S3.E2.m1.1.1.3.2.1anest" class="ltx_sub">msa</sub></mtext></mrow></mrow></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\text{MSA(}x_{z}\text{)}=\text{[SA\textsubscript{1}({x\textsubscript{z}});SA\textsubscript{2}({x\textsubscript{z}}); ... ;SA\textsubscript{m}({x\textsubscript{z}})]}\textbf{U\textsubscript{msa}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The data set we operate on can be adjusted to work in both 3D and 2D (including the temporal domain or with a downsampled temporal domain), as will be shown in Section <a href="#S3.SS3" title="3.3 Use of 2D Information ‣ 3 The Proposed RadarFormer ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. RadarFormer’s early stages used expensive 3D transformers that are based on ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and improved by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. While this provided good results, the computational complexity was still high. We can use 2D transformers by passing our input data through our proposed channel-chirp-time merging stream (Section <a href="#S3.SS3" title="3.3 Use of 2D Information ‣ 3 The Proposed RadarFormer ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>), providing a much lighter model and higher accuracy as well. Using 2D data allows us to explore a computationally inexpensive 2D variation of attention as we will see in Section <a href="#S3.SS2" title="3.2 Attention Variation ‣ 3 The Proposed RadarFormer ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Attention Variation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.4" class="ltx_p">Following ViT, multiple works have been developed that aim at more robust vision transformer models.
Some works introduced many features to image transformers such as hybrid models between transformers and CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, with many surveys evaluating and contrasting these models and their variations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. In addition to introducing hybrid models, variations in attention modules were also explored by many works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Our module uses a transformer block, called MaXViT (Multi-Axis Vision Transformer) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, which consists of an inverted residual block, MBConv <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, followed by an attention block and grid attention. The MBConv consists of 3 convolutional layers with a wide-narrow-wide style of channels, and a small-large-small style of kernel sizes, with a residual connecting the first convolution to the last one. The multi-axis attention block creates windowed partitions of the input and performs self-attention on these partitions, whose shape is <math id="S3.SS2.p1.1.m1.4" class="ltx_Math" alttext="(\frac{H}{P},\frac{W}{P},P\times P,C)" display="inline"><semantics id="S3.SS2.p1.1.m1.4a"><mrow id="S3.SS2.p1.1.m1.4.4.1" xref="S3.SS2.p1.1.m1.4.4.2.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.4.4.1.2" xref="S3.SS2.p1.1.m1.4.4.2.cmml">(</mo><mfrac id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">H</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">P</mi></mfrac><mo id="S3.SS2.p1.1.m1.4.4.1.3" xref="S3.SS2.p1.1.m1.4.4.2.cmml">,</mo><mfrac id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml"><mi id="S3.SS2.p1.1.m1.2.2.2" xref="S3.SS2.p1.1.m1.2.2.2.cmml">W</mi><mi id="S3.SS2.p1.1.m1.2.2.3" xref="S3.SS2.p1.1.m1.2.2.3.cmml">P</mi></mfrac><mo id="S3.SS2.p1.1.m1.4.4.1.4" xref="S3.SS2.p1.1.m1.4.4.2.cmml">,</mo><mrow id="S3.SS2.p1.1.m1.4.4.1.1" xref="S3.SS2.p1.1.m1.4.4.1.1.cmml"><mi id="S3.SS2.p1.1.m1.4.4.1.1.2" xref="S3.SS2.p1.1.m1.4.4.1.1.2.cmml">P</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.4.4.1.1.1" xref="S3.SS2.p1.1.m1.4.4.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.1.m1.4.4.1.1.3" xref="S3.SS2.p1.1.m1.4.4.1.1.3.cmml">P</mi></mrow><mo id="S3.SS2.p1.1.m1.4.4.1.5" xref="S3.SS2.p1.1.m1.4.4.2.cmml">,</mo><mi id="S3.SS2.p1.1.m1.3.3" xref="S3.SS2.p1.1.m1.3.3.cmml">C</mi><mo stretchy="false" id="S3.SS2.p1.1.m1.4.4.1.6" xref="S3.SS2.p1.1.m1.4.4.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.4b"><vector id="S3.SS2.p1.1.m1.4.4.2.cmml" xref="S3.SS2.p1.1.m1.4.4.1"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><divide id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"></divide><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝐻</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝑃</ci></apply><apply id="S3.SS2.p1.1.m1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2"><divide id="S3.SS2.p1.1.m1.2.2.1.cmml" xref="S3.SS2.p1.1.m1.2.2"></divide><ci id="S3.SS2.p1.1.m1.2.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2.2">𝑊</ci><ci id="S3.SS2.p1.1.m1.2.2.3.cmml" xref="S3.SS2.p1.1.m1.2.2.3">𝑃</ci></apply><apply id="S3.SS2.p1.1.m1.4.4.1.1.cmml" xref="S3.SS2.p1.1.m1.4.4.1.1"><times id="S3.SS2.p1.1.m1.4.4.1.1.1.cmml" xref="S3.SS2.p1.1.m1.4.4.1.1.1"></times><ci id="S3.SS2.p1.1.m1.4.4.1.1.2.cmml" xref="S3.SS2.p1.1.m1.4.4.1.1.2">𝑃</ci><ci id="S3.SS2.p1.1.m1.4.4.1.1.3.cmml" xref="S3.SS2.p1.1.m1.4.4.1.1.3">𝑃</ci></apply><ci id="S3.SS2.p1.1.m1.3.3.cmml" xref="S3.SS2.p1.1.m1.3.3">𝐶</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.4c">(\frac{H}{P},\frac{W}{P},P\times P,C)</annotation></semantics></math>, creating non-overlapping windows of size <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="P\times P" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">P</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></times><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑃</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">P\times P</annotation></semantics></math> following the notation used earlier. Similarly, a grid partitioning module uses a <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="G\times G" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">G</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><times id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></times><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝐺</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">G\times G</annotation></semantics></math> uniform grid to partition the input with adaptive size <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="\frac{H}{G}\times\frac{W}{G}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mfrac id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2.2" xref="S3.SS2.p1.4.m4.1.1.2.2.cmml">H</mi><mi id="S3.SS2.p1.4.m4.1.1.2.3" xref="S3.SS2.p1.4.m4.1.1.2.3.cmml">G</mi></mfrac><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml">×</mo><mfrac id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">W</mi><mi id="S3.SS2.p1.4.m4.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.3.3.cmml">G</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><times id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1"></times><apply id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2"><divide id="S3.SS2.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.p1.4.m4.1.1.2"></divide><ci id="S3.SS2.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2.2">𝐻</ci><ci id="S3.SS2.p1.4.m4.1.1.2.3.cmml" xref="S3.SS2.p1.4.m4.1.1.2.3">𝐺</ci></apply><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><divide id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3"></divide><ci id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">𝑊</ci><ci id="S3.SS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3">𝐺</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\frac{H}{G}\times\frac{W}{G}</annotation></semantics></math>, resulting in a dilated mix of tokens that provide global information. In the module proposition, stacking both window and grid attention provides local and global contexts in transformer operations, hence the name multi-axis attention.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">As an attempt to improve the transformers models, we explored multiple variations of transformer models and attentions along with convolutional combinations. The first is a high-low attention (HiLo) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> approach that splits attention into a high-resolution branch and a low-resolution (downsampled) branch. This approach did provide more consistency in training but did not perform adequately, where it capped at 74.2% AP. This led us to explore the attention and resolution variation of the models more thoroughly. To this end, we explored a high-resolution transformer (HRFormer)-like architecture, following the work proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. HRFormer was very computationally expensive both in its 3D and 2D variants and did not perform well. The number of trainable parameters, 872.9 million, was too large to justify its use, and too large for the data set to train and performed poorly as expected. In our proposition, MaxVIT blocks performed consistently better with a lot of variations when compared to other baseline models. Other architectures like UNETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and UNETR++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> were computationally expensive in their 3D format and did not provide an accuracy high enough to justify their use. However, downsampling to 2D and using a UNETR/ViT-inspired transformer design provided a good baseline for transformer architectures which we referred to as ’2D Transformer’ in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Object Detection on Radar Data ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We discuss the quantitative results of relevant models in Section <a href="#S4.SS2" title="4.2 Quantitative Results ‣ 4 Experiments ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Use of 2D Information</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.8" class="ltx_p">In the original model of RODNet, the input has a shape of <math id="S3.SS3.p1.1.m1.6" class="ltx_Math" alttext="(B,2,T,C,H,W)" display="inline"><semantics id="S3.SS3.p1.1.m1.6a"><mrow id="S3.SS3.p1.1.m1.6.7.2" xref="S3.SS3.p1.1.m1.6.7.1.cmml"><mo stretchy="false" id="S3.SS3.p1.1.m1.6.7.2.1" xref="S3.SS3.p1.1.m1.6.7.1.cmml">(</mo><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">B</mi><mo id="S3.SS3.p1.1.m1.6.7.2.2" xref="S3.SS3.p1.1.m1.6.7.1.cmml">,</mo><mn id="S3.SS3.p1.1.m1.2.2" xref="S3.SS3.p1.1.m1.2.2.cmml">2</mn><mo id="S3.SS3.p1.1.m1.6.7.2.3" xref="S3.SS3.p1.1.m1.6.7.1.cmml">,</mo><mi id="S3.SS3.p1.1.m1.3.3" xref="S3.SS3.p1.1.m1.3.3.cmml">T</mi><mo id="S3.SS3.p1.1.m1.6.7.2.4" xref="S3.SS3.p1.1.m1.6.7.1.cmml">,</mo><mi id="S3.SS3.p1.1.m1.4.4" xref="S3.SS3.p1.1.m1.4.4.cmml">C</mi><mo id="S3.SS3.p1.1.m1.6.7.2.5" xref="S3.SS3.p1.1.m1.6.7.1.cmml">,</mo><mi id="S3.SS3.p1.1.m1.5.5" xref="S3.SS3.p1.1.m1.5.5.cmml">H</mi><mo id="S3.SS3.p1.1.m1.6.7.2.6" xref="S3.SS3.p1.1.m1.6.7.1.cmml">,</mo><mi id="S3.SS3.p1.1.m1.6.6" xref="S3.SS3.p1.1.m1.6.6.cmml">W</mi><mo stretchy="false" id="S3.SS3.p1.1.m1.6.7.2.7" xref="S3.SS3.p1.1.m1.6.7.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.6b"><vector id="S3.SS3.p1.1.m1.6.7.1.cmml" xref="S3.SS3.p1.1.m1.6.7.2"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝐵</ci><cn type="integer" id="S3.SS3.p1.1.m1.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2">2</cn><ci id="S3.SS3.p1.1.m1.3.3.cmml" xref="S3.SS3.p1.1.m1.3.3">𝑇</ci><ci id="S3.SS3.p1.1.m1.4.4.cmml" xref="S3.SS3.p1.1.m1.4.4">𝐶</ci><ci id="S3.SS3.p1.1.m1.5.5.cmml" xref="S3.SS3.p1.1.m1.5.5">𝐻</ci><ci id="S3.SS3.p1.1.m1.6.6.cmml" xref="S3.SS3.p1.1.m1.6.6">𝑊</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.6c">(B,2,T,C,H,W)</annotation></semantics></math>, where <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">B</annotation></semantics></math> is the batch size, <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">C</annotation></semantics></math> is the number of chirps, <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">T</annotation></semantics></math> is the window size (temporal), <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mn id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><cn type="integer" id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">2</annotation></semantics></math> is the number of RF channels (being a constant referring to the real and imaginary magnitudes of RF signals), and <math id="S3.SS3.p1.6.m6.2" class="ltx_Math" alttext="H,W" display="inline"><semantics id="S3.SS3.p1.6.m6.2a"><mrow id="S3.SS3.p1.6.m6.2.3.2" xref="S3.SS3.p1.6.m6.2.3.1.cmml"><mi id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">H</mi><mo id="S3.SS3.p1.6.m6.2.3.2.1" xref="S3.SS3.p1.6.m6.2.3.1.cmml">,</mo><mi id="S3.SS3.p1.6.m6.2.2" xref="S3.SS3.p1.6.m6.2.2.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.2b"><list id="S3.SS3.p1.6.m6.2.3.1.cmml" xref="S3.SS3.p1.6.m6.2.3.2"><ci id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">𝐻</ci><ci id="S3.SS3.p1.6.m6.2.2.cmml" xref="S3.SS3.p1.6.m6.2.2">𝑊</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.2c">H,W</annotation></semantics></math> are the height and width of the RF image, respectively. The RF channels and chirps are merged into one dimension using M-Net, a channel-chirp merging module suggested in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. We extend M-Net to go beyond channel-chirp merging and propose a module that includes temporal merging as well whose inputs and outputs are shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.3 Use of 2D Information ‣ 3 The Proposed RadarFormer ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The chirps and RF channels dimensions are compressed first into one channel with dimension <math id="S3.SS3.p1.7.m7.1" class="ltx_Math" alttext="C_{h}" display="inline"><semantics id="S3.SS3.p1.7.m7.1a"><msub id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml"><mi id="S3.SS3.p1.7.m7.1.1.2" xref="S3.SS3.p1.7.m7.1.1.2.cmml">C</mi><mi id="S3.SS3.p1.7.m7.1.1.3" xref="S3.SS3.p1.7.m7.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><apply id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.p1.7.m7.1.1.2.cmml" xref="S3.SS3.p1.7.m7.1.1.2">𝐶</ci><ci id="S3.SS3.p1.7.m7.1.1.3.cmml" xref="S3.SS3.p1.7.m7.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">C_{h}</annotation></semantics></math>, shown as <math id="S3.SS3.p1.8.m8.5" class="ltx_Math" alttext="(B,C_{h},T,H,W)" display="inline"><semantics id="S3.SS3.p1.8.m8.5a"><mrow id="S3.SS3.p1.8.m8.5.5.1" xref="S3.SS3.p1.8.m8.5.5.2.cmml"><mo stretchy="false" id="S3.SS3.p1.8.m8.5.5.1.2" xref="S3.SS3.p1.8.m8.5.5.2.cmml">(</mo><mi id="S3.SS3.p1.8.m8.1.1" xref="S3.SS3.p1.8.m8.1.1.cmml">B</mi><mo id="S3.SS3.p1.8.m8.5.5.1.3" xref="S3.SS3.p1.8.m8.5.5.2.cmml">,</mo><msub id="S3.SS3.p1.8.m8.5.5.1.1" xref="S3.SS3.p1.8.m8.5.5.1.1.cmml"><mi id="S3.SS3.p1.8.m8.5.5.1.1.2" xref="S3.SS3.p1.8.m8.5.5.1.1.2.cmml">C</mi><mi id="S3.SS3.p1.8.m8.5.5.1.1.3" xref="S3.SS3.p1.8.m8.5.5.1.1.3.cmml">h</mi></msub><mo id="S3.SS3.p1.8.m8.5.5.1.4" xref="S3.SS3.p1.8.m8.5.5.2.cmml">,</mo><mi id="S3.SS3.p1.8.m8.2.2" xref="S3.SS3.p1.8.m8.2.2.cmml">T</mi><mo id="S3.SS3.p1.8.m8.5.5.1.5" xref="S3.SS3.p1.8.m8.5.5.2.cmml">,</mo><mi id="S3.SS3.p1.8.m8.3.3" xref="S3.SS3.p1.8.m8.3.3.cmml">H</mi><mo id="S3.SS3.p1.8.m8.5.5.1.6" xref="S3.SS3.p1.8.m8.5.5.2.cmml">,</mo><mi id="S3.SS3.p1.8.m8.4.4" xref="S3.SS3.p1.8.m8.4.4.cmml">W</mi><mo stretchy="false" id="S3.SS3.p1.8.m8.5.5.1.7" xref="S3.SS3.p1.8.m8.5.5.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.5b"><vector id="S3.SS3.p1.8.m8.5.5.2.cmml" xref="S3.SS3.p1.8.m8.5.5.1"><ci id="S3.SS3.p1.8.m8.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1">𝐵</ci><apply id="S3.SS3.p1.8.m8.5.5.1.1.cmml" xref="S3.SS3.p1.8.m8.5.5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.8.m8.5.5.1.1.1.cmml" xref="S3.SS3.p1.8.m8.5.5.1.1">subscript</csymbol><ci id="S3.SS3.p1.8.m8.5.5.1.1.2.cmml" xref="S3.SS3.p1.8.m8.5.5.1.1.2">𝐶</ci><ci id="S3.SS3.p1.8.m8.5.5.1.1.3.cmml" xref="S3.SS3.p1.8.m8.5.5.1.1.3">ℎ</ci></apply><ci id="S3.SS3.p1.8.m8.2.2.cmml" xref="S3.SS3.p1.8.m8.2.2">𝑇</ci><ci id="S3.SS3.p1.8.m8.3.3.cmml" xref="S3.SS3.p1.8.m8.3.3">𝐻</ci><ci id="S3.SS3.p1.8.m8.4.4.cmml" xref="S3.SS3.p1.8.m8.4.4">𝑊</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.5c">(B,C_{h},T,H,W)</annotation></semantics></math>, and then downsampled in the temporal domain to be passed into the models. However, we preserve temporal context through a temporal residual connection towards the upsampling stream at the end before generating the ConfMaps.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2304.08447/assets/x6.png" id="S3.F4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="187" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The M-Net module (top) reported in the original RODNet architecture and used by us and proposed temporal downsampling/upsampling flow (bottom). The M-Net module takes in the full RF image with shape <math id="S3.F4.6.m1.5" class="ltx_Math" alttext="(2,T,C,H,W)" display="inline"><semantics id="S3.F4.6.m1.5b"><mrow id="S3.F4.6.m1.5.6.2" xref="S3.F4.6.m1.5.6.1.cmml"><mo stretchy="false" id="S3.F4.6.m1.5.6.2.1" xref="S3.F4.6.m1.5.6.1.cmml">(</mo><mn id="S3.F4.6.m1.1.1" xref="S3.F4.6.m1.1.1.cmml">2</mn><mo id="S3.F4.6.m1.5.6.2.2" xref="S3.F4.6.m1.5.6.1.cmml">,</mo><mi id="S3.F4.6.m1.2.2" xref="S3.F4.6.m1.2.2.cmml">T</mi><mo id="S3.F4.6.m1.5.6.2.3" xref="S3.F4.6.m1.5.6.1.cmml">,</mo><mi id="S3.F4.6.m1.3.3" xref="S3.F4.6.m1.3.3.cmml">C</mi><mo id="S3.F4.6.m1.5.6.2.4" xref="S3.F4.6.m1.5.6.1.cmml">,</mo><mi id="S3.F4.6.m1.4.4" xref="S3.F4.6.m1.4.4.cmml">H</mi><mo id="S3.F4.6.m1.5.6.2.5" xref="S3.F4.6.m1.5.6.1.cmml">,</mo><mi id="S3.F4.6.m1.5.5" xref="S3.F4.6.m1.5.5.cmml">W</mi><mo stretchy="false" id="S3.F4.6.m1.5.6.2.6" xref="S3.F4.6.m1.5.6.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.6.m1.5c"><vector id="S3.F4.6.m1.5.6.1.cmml" xref="S3.F4.6.m1.5.6.2"><cn type="integer" id="S3.F4.6.m1.1.1.cmml" xref="S3.F4.6.m1.1.1">2</cn><ci id="S3.F4.6.m1.2.2.cmml" xref="S3.F4.6.m1.2.2">𝑇</ci><ci id="S3.F4.6.m1.3.3.cmml" xref="S3.F4.6.m1.3.3">𝐶</ci><ci id="S3.F4.6.m1.4.4.cmml" xref="S3.F4.6.m1.4.4">𝐻</ci><ci id="S3.F4.6.m1.5.5.cmml" xref="S3.F4.6.m1.5.5">𝑊</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.6.m1.5d">(2,T,C,H,W)</annotation></semantics></math> and merges the channels (2) and the chirps <math id="S3.F4.7.m2.1" class="ltx_Math" alttext="(C)" display="inline"><semantics id="S3.F4.7.m2.1b"><mrow id="S3.F4.7.m2.1.2.2"><mo stretchy="false" id="S3.F4.7.m2.1.2.2.1">(</mo><mi id="S3.F4.7.m2.1.1" xref="S3.F4.7.m2.1.1.cmml">C</mi><mo stretchy="false" id="S3.F4.7.m2.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.7.m2.1c"><ci id="S3.F4.7.m2.1.1.cmml" xref="S3.F4.7.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.7.m2.1d">(C)</annotation></semantics></math> into one dimension, <math id="S3.F4.8.m3.1" class="ltx_Math" alttext="C_{h}" display="inline"><semantics id="S3.F4.8.m3.1b"><msub id="S3.F4.8.m3.1.1" xref="S3.F4.8.m3.1.1.cmml"><mi id="S3.F4.8.m3.1.1.2" xref="S3.F4.8.m3.1.1.2.cmml">C</mi><mi id="S3.F4.8.m3.1.1.3" xref="S3.F4.8.m3.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F4.8.m3.1c"><apply id="S3.F4.8.m3.1.1.cmml" xref="S3.F4.8.m3.1.1"><csymbol cd="ambiguous" id="S3.F4.8.m3.1.1.1.cmml" xref="S3.F4.8.m3.1.1">subscript</csymbol><ci id="S3.F4.8.m3.1.1.2.cmml" xref="S3.F4.8.m3.1.1.2">𝐶</ci><ci id="S3.F4.8.m3.1.1.3.cmml" xref="S3.F4.8.m3.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.8.m3.1d">C_{h}</annotation></semantics></math>. The second module downsamples the temporal domain to receive a 2D tensor of shape <math id="S3.F4.9.m4.3" class="ltx_Math" alttext="(C_{h},H,W)" display="inline"><semantics id="S3.F4.9.m4.3b"><mrow id="S3.F4.9.m4.3.3.1" xref="S3.F4.9.m4.3.3.2.cmml"><mo stretchy="false" id="S3.F4.9.m4.3.3.1.2" xref="S3.F4.9.m4.3.3.2.cmml">(</mo><msub id="S3.F4.9.m4.3.3.1.1" xref="S3.F4.9.m4.3.3.1.1.cmml"><mi id="S3.F4.9.m4.3.3.1.1.2" xref="S3.F4.9.m4.3.3.1.1.2.cmml">C</mi><mi id="S3.F4.9.m4.3.3.1.1.3" xref="S3.F4.9.m4.3.3.1.1.3.cmml">h</mi></msub><mo id="S3.F4.9.m4.3.3.1.3" xref="S3.F4.9.m4.3.3.2.cmml">,</mo><mi id="S3.F4.9.m4.1.1" xref="S3.F4.9.m4.1.1.cmml">H</mi><mo id="S3.F4.9.m4.3.3.1.4" xref="S3.F4.9.m4.3.3.2.cmml">,</mo><mi id="S3.F4.9.m4.2.2" xref="S3.F4.9.m4.2.2.cmml">W</mi><mo stretchy="false" id="S3.F4.9.m4.3.3.1.5" xref="S3.F4.9.m4.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.9.m4.3c"><vector id="S3.F4.9.m4.3.3.2.cmml" xref="S3.F4.9.m4.3.3.1"><apply id="S3.F4.9.m4.3.3.1.1.cmml" xref="S3.F4.9.m4.3.3.1.1"><csymbol cd="ambiguous" id="S3.F4.9.m4.3.3.1.1.1.cmml" xref="S3.F4.9.m4.3.3.1.1">subscript</csymbol><ci id="S3.F4.9.m4.3.3.1.1.2.cmml" xref="S3.F4.9.m4.3.3.1.1.2">𝐶</ci><ci id="S3.F4.9.m4.3.3.1.1.3.cmml" xref="S3.F4.9.m4.3.3.1.1.3">ℎ</ci></apply><ci id="S3.F4.9.m4.1.1.cmml" xref="S3.F4.9.m4.1.1">𝐻</ci><ci id="S3.F4.9.m4.2.2.cmml" xref="S3.F4.9.m4.2.2">𝑊</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.9.m4.3d">(C_{h},H,W)</annotation></semantics></math>. The tensors are connected via element-wise addition to their upsampling counterpart after the models forward the input. Both modules are cascaded and output a 3D output of shape <math id="S3.F4.10.m5.4" class="ltx_Math" alttext="(C_{h},T,H,W)" display="inline"><semantics id="S3.F4.10.m5.4b"><mrow id="S3.F4.10.m5.4.4.1" xref="S3.F4.10.m5.4.4.2.cmml"><mo stretchy="false" id="S3.F4.10.m5.4.4.1.2" xref="S3.F4.10.m5.4.4.2.cmml">(</mo><msub id="S3.F4.10.m5.4.4.1.1" xref="S3.F4.10.m5.4.4.1.1.cmml"><mi id="S3.F4.10.m5.4.4.1.1.2" xref="S3.F4.10.m5.4.4.1.1.2.cmml">C</mi><mi id="S3.F4.10.m5.4.4.1.1.3" xref="S3.F4.10.m5.4.4.1.1.3.cmml">h</mi></msub><mo id="S3.F4.10.m5.4.4.1.3" xref="S3.F4.10.m5.4.4.2.cmml">,</mo><mi id="S3.F4.10.m5.1.1" xref="S3.F4.10.m5.1.1.cmml">T</mi><mo id="S3.F4.10.m5.4.4.1.4" xref="S3.F4.10.m5.4.4.2.cmml">,</mo><mi id="S3.F4.10.m5.2.2" xref="S3.F4.10.m5.2.2.cmml">H</mi><mo id="S3.F4.10.m5.4.4.1.5" xref="S3.F4.10.m5.4.4.2.cmml">,</mo><mi id="S3.F4.10.m5.3.3" xref="S3.F4.10.m5.3.3.cmml">W</mi><mo stretchy="false" id="S3.F4.10.m5.4.4.1.6" xref="S3.F4.10.m5.4.4.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.10.m5.4c"><vector id="S3.F4.10.m5.4.4.2.cmml" xref="S3.F4.10.m5.4.4.1"><apply id="S3.F4.10.m5.4.4.1.1.cmml" xref="S3.F4.10.m5.4.4.1.1"><csymbol cd="ambiguous" id="S3.F4.10.m5.4.4.1.1.1.cmml" xref="S3.F4.10.m5.4.4.1.1">subscript</csymbol><ci id="S3.F4.10.m5.4.4.1.1.2.cmml" xref="S3.F4.10.m5.4.4.1.1.2">𝐶</ci><ci id="S3.F4.10.m5.4.4.1.1.3.cmml" xref="S3.F4.10.m5.4.4.1.1.3">ℎ</ci></apply><ci id="S3.F4.10.m5.1.1.cmml" xref="S3.F4.10.m5.1.1">𝑇</ci><ci id="S3.F4.10.m5.2.2.cmml" xref="S3.F4.10.m5.2.2">𝐻</ci><ci id="S3.F4.10.m5.3.3.cmml" xref="S3.F4.10.m5.3.3">𝑊</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.10.m5.4d">(C_{h},T,H,W)</annotation></semantics></math>.</figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The motive for downsampling comes from two observations.
First, 3D convolutions and temporal 3D convolutions are extremely computationally expensive and their weights have a large size, especially when using many filters. This is undesirable in the context of autonomous vehicles that have controllers with limited computing and storage capabilities. Second, we noticed that downsampling 3D inputs into 2D, performing all deep learning operations after downsampling the frames into one frame, and upsampling before inference with residuals did not compromise the accuracy to a noticeable level. The downsampling however reduced the inference time to one-fifth of the original value and the size of the weights to one-tenth of the original value. We can conclude that the effect of the time domain is less pronounced when treated as separate channels and employing expensive 3D convolutions or 3D transformer modules like UNETR is not necessary. Other models in radar perception used temporal downsampling techniques like TMVA-NET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The reduced base RODNet which has a temporal downsampling/upsampling stream added to is referred to as ’2D CNN’ from this point forward. It is worth mentioning that downsampling is more useful than distributing channels. We noticed that when we skip the 3D downsampling module and instead stack our model input to the shape <math id="S3.SS3.p2.1.m1.4" class="ltx_Math" alttext="(T\times B,C_{h},H,W)" display="inline"><semantics id="S3.SS3.p2.1.m1.4a"><mrow id="S3.SS3.p2.1.m1.4.4.2" xref="S3.SS3.p2.1.m1.4.4.3.cmml"><mo stretchy="false" id="S3.SS3.p2.1.m1.4.4.2.3" xref="S3.SS3.p2.1.m1.4.4.3.cmml">(</mo><mrow id="S3.SS3.p2.1.m1.3.3.1.1" xref="S3.SS3.p2.1.m1.3.3.1.1.cmml"><mi id="S3.SS3.p2.1.m1.3.3.1.1.2" xref="S3.SS3.p2.1.m1.3.3.1.1.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.1.m1.3.3.1.1.1" xref="S3.SS3.p2.1.m1.3.3.1.1.1.cmml">×</mo><mi id="S3.SS3.p2.1.m1.3.3.1.1.3" xref="S3.SS3.p2.1.m1.3.3.1.1.3.cmml">B</mi></mrow><mo id="S3.SS3.p2.1.m1.4.4.2.4" xref="S3.SS3.p2.1.m1.4.4.3.cmml">,</mo><msub id="S3.SS3.p2.1.m1.4.4.2.2" xref="S3.SS3.p2.1.m1.4.4.2.2.cmml"><mi id="S3.SS3.p2.1.m1.4.4.2.2.2" xref="S3.SS3.p2.1.m1.4.4.2.2.2.cmml">C</mi><mi id="S3.SS3.p2.1.m1.4.4.2.2.3" xref="S3.SS3.p2.1.m1.4.4.2.2.3.cmml">h</mi></msub><mo id="S3.SS3.p2.1.m1.4.4.2.5" xref="S3.SS3.p2.1.m1.4.4.3.cmml">,</mo><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">H</mi><mo id="S3.SS3.p2.1.m1.4.4.2.6" xref="S3.SS3.p2.1.m1.4.4.3.cmml">,</mo><mi id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2.cmml">W</mi><mo stretchy="false" id="S3.SS3.p2.1.m1.4.4.2.7" xref="S3.SS3.p2.1.m1.4.4.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.4b"><vector id="S3.SS3.p2.1.m1.4.4.3.cmml" xref="S3.SS3.p2.1.m1.4.4.2"><apply id="S3.SS3.p2.1.m1.3.3.1.1.cmml" xref="S3.SS3.p2.1.m1.3.3.1.1"><times id="S3.SS3.p2.1.m1.3.3.1.1.1.cmml" xref="S3.SS3.p2.1.m1.3.3.1.1.1"></times><ci id="S3.SS3.p2.1.m1.3.3.1.1.2.cmml" xref="S3.SS3.p2.1.m1.3.3.1.1.2">𝑇</ci><ci id="S3.SS3.p2.1.m1.3.3.1.1.3.cmml" xref="S3.SS3.p2.1.m1.3.3.1.1.3">𝐵</ci></apply><apply id="S3.SS3.p2.1.m1.4.4.2.2.cmml" xref="S3.SS3.p2.1.m1.4.4.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.4.4.2.2.1.cmml" xref="S3.SS3.p2.1.m1.4.4.2.2">subscript</csymbol><ci id="S3.SS3.p2.1.m1.4.4.2.2.2.cmml" xref="S3.SS3.p2.1.m1.4.4.2.2.2">𝐶</ci><ci id="S3.SS3.p2.1.m1.4.4.2.2.3.cmml" xref="S3.SS3.p2.1.m1.4.4.2.2.3">ℎ</ci></apply><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝐻</ci><ci id="S3.SS3.p2.1.m1.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2">𝑊</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.4c">(T\times B,C_{h},H,W)</annotation></semantics></math> we lose a lot of contextual temporal information. This information was retained through the convolutional filters and the residual connections between the input and output.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">We noticed that RODNet’s performance degraded by a negligible margin by downsampling the temporal domain to a single channel and performing most of the operation with reduced numbers of channels with 2D convolutions. In addition, the model did produce heavily fluctuating results as the model trains, which initially was attributed to the configuration of the learning rate, however, altering the learning rate value, the scheduling, and the optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> did not make the model converge to a consistent value. Furthermore, the reproducibility of networks trained on this data set was inconsistent and sometimes inaccurate. The same model could be trained using the same configurations and yield completely different prediction accuracies. Replicating the results of reported values of RODNet was also not feasible despite attempting to train the same model multiple times. Introducing residual connections between the input downsampling convolutions and the output upsample convolutions helped remedy this issue. The difficulty of learning consistent weight sets is one of the challenging aspects of this data set and application, but we noticed better reproducibility of transformer-based architectures compared to the CNN-only approaches.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Effect of Receptive Fields and Residuals</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Transformers did introduce a large reduction in required computational complexity at the early stages of our model design, providing a large margin for us to utilize more complex architectures and modules. To take advantage of both transformers and CNNs, we noticed that introducing CNNs before and after the transformers along with residual connections following our design in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Object Detection on Radar Data ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> improved the model’s accuracy. This was in line with the suggested conclusions of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, which was encouraging to explore two main ideas in our experiments. The first is exploring the effects of varying the number of convolution layers, input/output channels, and residual connections before and after the transformers. The second is to assess the effect of varying the receptive fields and the size of the convolutional layers’ kernels. This discussion excludes the first transformer layer of both models due to it being preceded by 3 convolutional downsampling layers. For the first point, using two convolutional layers before the transformer and one after the network provided the best results without residuals. Introducing residuals with element-wise addition and a convolutional layer that equalizes the number of channels, followed by a convolution after the element-wise addition, provided similar performance with a slight decrement in required computational time. We noticed that MaXViTs perform the best without downsampling and the convolution in the residual connection is not necessary and does not improve the performance either with MaXViTs, providing a lighter model in this aspect by removing it. For the second point, varying the kernel sizes of said layers did contribute to better learning of data. Increasing the size of the kernel as we go deeper in the network allows for learning of a more general receptive field that gets improved by the transformer’s global/local attention dynamic. In a similar fashion, establishing residual connections between the input 3D downsampling to the output 3D upsampling compensates for the information loss in propagating through the network and improves the upsampling stream.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Implementation Details</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">We train our models on a single NVIDIA A100 GPU with Adam optimizer and an initial learning rate of <math id="S3.SS4.SSS1.p1.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S3.SS4.SSS1.p1.1.m1.1a"><msup id="S3.SS4.SSS1.p1.1.m1.1.1" xref="S3.SS4.SSS1.p1.1.m1.1.1.cmml"><mn id="S3.SS4.SSS1.p1.1.m1.1.1.2" xref="S3.SS4.SSS1.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S3.SS4.SSS1.p1.1.m1.1.1.3" xref="S3.SS4.SSS1.p1.1.m1.1.1.3.cmml"><mo id="S3.SS4.SSS1.p1.1.m1.1.1.3a" xref="S3.SS4.SSS1.p1.1.m1.1.1.3.cmml">−</mo><mn id="S3.SS4.SSS1.p1.1.m1.1.1.3.2" xref="S3.SS4.SSS1.p1.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.1.m1.1b"><apply id="S3.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.SS4.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS1.p1.1.m1.1.1.2">10</cn><apply id="S3.SS4.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS4.SSS1.p1.1.m1.1.1.3"><minus id="S3.SS4.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.1.1.3"></minus><cn type="integer" id="S3.SS4.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS4.SSS1.p1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.1.m1.1c">10^{-4}</annotation></semantics></math> with step decay.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.1 </span>Dataset Details</h4>

<div id="S4.SS0.SSS1.p1" class="ltx_para">
<p id="S4.SS0.SSS1.p1.9" class="ltx_p">We build and test our model on the CRUW radar data set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. CRUW consists of roughly 400k frames of recorded driving sequences. The data is processed to be represented as Range-Azimuth Heatmaps (RAMap)s, which describe a bird’s-eye view of the scene seen from the ego-vehicle. The <math id="S4.SS0.SSS1.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS0.SSS1.p1.1.m1.1a"><mi id="S4.SS0.SSS1.p1.1.m1.1.1" xref="S4.SS0.SSS1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS1.p1.1.m1.1b"><ci id="S4.SS0.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS1.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS1.p1.1.m1.1c">x</annotation></semantics></math>-axis depicts the azimuth plane, describing the angle, and the <math id="S4.SS0.SSS1.p1.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS0.SSS1.p1.2.m2.1a"><mi id="S4.SS0.SSS1.p1.2.m2.1.1" xref="S4.SS0.SSS1.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS1.p1.2.m2.1b"><ci id="S4.SS0.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS1.p1.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS1.p1.2.m2.1c">y</annotation></semantics></math>-axis depicts the range plane, describing the distance to the object, with the intensity describing the magnitude of the RF signal. These can be described as an image with a resolution of 128<math id="S4.SS0.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS0.SSS1.p1.3.m3.1a"><mo id="S4.SS0.SSS1.p1.3.m3.1.1" xref="S4.SS0.SSS1.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS1.p1.3.m3.1b"><times id="S4.SS0.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS1.p1.3.m3.1c">\times</annotation></semantics></math>128 each, with a sample shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The acquisition setup consists of two 77GHz FMCW antennas that collect 256 chirps every frame (30 frames a second), and only 4 are chosen out of these chirps (0, 64, 128, 192). The details of the data we use and inputs will strictly follow the work done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> which most of our work will use as an evaluation baseline. This work will also focus on the behavior of deep learning modules with radar-based data, which has not been studied extensively in most works regarding radar data. Unlike the model associated with the CRUW data set, RODNet, our aim was to create a deep learning model that uses radar data without any sort of sensor infusion, which was illustrated in the model discussed in Section <a href="#S3" title="3 The Proposed RadarFormer ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The input to the model is a <math id="S4.SS0.SSS1.p1.4.m4.4" class="ltx_Math" alttext="(B,C_{h},H,W)" display="inline"><semantics id="S4.SS0.SSS1.p1.4.m4.4a"><mrow id="S4.SS0.SSS1.p1.4.m4.4.4.1" xref="S4.SS0.SSS1.p1.4.m4.4.4.2.cmml"><mo stretchy="false" id="S4.SS0.SSS1.p1.4.m4.4.4.1.2" xref="S4.SS0.SSS1.p1.4.m4.4.4.2.cmml">(</mo><mi id="S4.SS0.SSS1.p1.4.m4.1.1" xref="S4.SS0.SSS1.p1.4.m4.1.1.cmml">B</mi><mo id="S4.SS0.SSS1.p1.4.m4.4.4.1.3" xref="S4.SS0.SSS1.p1.4.m4.4.4.2.cmml">,</mo><msub id="S4.SS0.SSS1.p1.4.m4.4.4.1.1" xref="S4.SS0.SSS1.p1.4.m4.4.4.1.1.cmml"><mi id="S4.SS0.SSS1.p1.4.m4.4.4.1.1.2" xref="S4.SS0.SSS1.p1.4.m4.4.4.1.1.2.cmml">C</mi><mi id="S4.SS0.SSS1.p1.4.m4.4.4.1.1.3" xref="S4.SS0.SSS1.p1.4.m4.4.4.1.1.3.cmml">h</mi></msub><mo id="S4.SS0.SSS1.p1.4.m4.4.4.1.4" xref="S4.SS0.SSS1.p1.4.m4.4.4.2.cmml">,</mo><mi id="S4.SS0.SSS1.p1.4.m4.2.2" xref="S4.SS0.SSS1.p1.4.m4.2.2.cmml">H</mi><mo id="S4.SS0.SSS1.p1.4.m4.4.4.1.5" xref="S4.SS0.SSS1.p1.4.m4.4.4.2.cmml">,</mo><mi id="S4.SS0.SSS1.p1.4.m4.3.3" xref="S4.SS0.SSS1.p1.4.m4.3.3.cmml">W</mi><mo stretchy="false" id="S4.SS0.SSS1.p1.4.m4.4.4.1.6" xref="S4.SS0.SSS1.p1.4.m4.4.4.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS1.p1.4.m4.4b"><vector id="S4.SS0.SSS1.p1.4.m4.4.4.2.cmml" xref="S4.SS0.SSS1.p1.4.m4.4.4.1"><ci id="S4.SS0.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS1.p1.4.m4.1.1">𝐵</ci><apply id="S4.SS0.SSS1.p1.4.m4.4.4.1.1.cmml" xref="S4.SS0.SSS1.p1.4.m4.4.4.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS1.p1.4.m4.4.4.1.1.1.cmml" xref="S4.SS0.SSS1.p1.4.m4.4.4.1.1">subscript</csymbol><ci id="S4.SS0.SSS1.p1.4.m4.4.4.1.1.2.cmml" xref="S4.SS0.SSS1.p1.4.m4.4.4.1.1.2">𝐶</ci><ci id="S4.SS0.SSS1.p1.4.m4.4.4.1.1.3.cmml" xref="S4.SS0.SSS1.p1.4.m4.4.4.1.1.3">ℎ</ci></apply><ci id="S4.SS0.SSS1.p1.4.m4.2.2.cmml" xref="S4.SS0.SSS1.p1.4.m4.2.2">𝐻</ci><ci id="S4.SS0.SSS1.p1.4.m4.3.3.cmml" xref="S4.SS0.SSS1.p1.4.m4.3.3">𝑊</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS1.p1.4.m4.4c">(B,C_{h},H,W)</annotation></semantics></math> input tensor, with the batch size <math id="S4.SS0.SSS1.p1.5.m5.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS0.SSS1.p1.5.m5.1a"><mi id="S4.SS0.SSS1.p1.5.m5.1.1" xref="S4.SS0.SSS1.p1.5.m5.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS1.p1.5.m5.1b"><ci id="S4.SS0.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS1.p1.5.m5.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS1.p1.5.m5.1c">B</annotation></semantics></math>, the number of channels <math id="S4.SS0.SSS1.p1.6.m6.1" class="ltx_Math" alttext="C_{h}" display="inline"><semantics id="S4.SS0.SSS1.p1.6.m6.1a"><msub id="S4.SS0.SSS1.p1.6.m6.1.1" xref="S4.SS0.SSS1.p1.6.m6.1.1.cmml"><mi id="S4.SS0.SSS1.p1.6.m6.1.1.2" xref="S4.SS0.SSS1.p1.6.m6.1.1.2.cmml">C</mi><mi id="S4.SS0.SSS1.p1.6.m6.1.1.3" xref="S4.SS0.SSS1.p1.6.m6.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS1.p1.6.m6.1b"><apply id="S4.SS0.SSS1.p1.6.m6.1.1.cmml" xref="S4.SS0.SSS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS1.p1.6.m6.1.1.1.cmml" xref="S4.SS0.SSS1.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS0.SSS1.p1.6.m6.1.1.2.cmml" xref="S4.SS0.SSS1.p1.6.m6.1.1.2">𝐶</ci><ci id="S4.SS0.SSS1.p1.6.m6.1.1.3.cmml" xref="S4.SS0.SSS1.p1.6.m6.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS1.p1.6.m6.1c">C_{h}</annotation></semantics></math>, and <math id="S4.SS0.SSS1.p1.7.m7.2" class="ltx_Math" alttext="(H,W)" display="inline"><semantics id="S4.SS0.SSS1.p1.7.m7.2a"><mrow id="S4.SS0.SSS1.p1.7.m7.2.3.2" xref="S4.SS0.SSS1.p1.7.m7.2.3.1.cmml"><mo stretchy="false" id="S4.SS0.SSS1.p1.7.m7.2.3.2.1" xref="S4.SS0.SSS1.p1.7.m7.2.3.1.cmml">(</mo><mi id="S4.SS0.SSS1.p1.7.m7.1.1" xref="S4.SS0.SSS1.p1.7.m7.1.1.cmml">H</mi><mo id="S4.SS0.SSS1.p1.7.m7.2.3.2.2" xref="S4.SS0.SSS1.p1.7.m7.2.3.1.cmml">,</mo><mi id="S4.SS0.SSS1.p1.7.m7.2.2" xref="S4.SS0.SSS1.p1.7.m7.2.2.cmml">W</mi><mo stretchy="false" id="S4.SS0.SSS1.p1.7.m7.2.3.2.3" xref="S4.SS0.SSS1.p1.7.m7.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS1.p1.7.m7.2b"><interval closure="open" id="S4.SS0.SSS1.p1.7.m7.2.3.1.cmml" xref="S4.SS0.SSS1.p1.7.m7.2.3.2"><ci id="S4.SS0.SSS1.p1.7.m7.1.1.cmml" xref="S4.SS0.SSS1.p1.7.m7.1.1">𝐻</ci><ci id="S4.SS0.SSS1.p1.7.m7.2.2.cmml" xref="S4.SS0.SSS1.p1.7.m7.2.2">𝑊</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS1.p1.7.m7.2c">(H,W)</annotation></semantics></math> is the resolution of the RF image. We note that this is the output of the downsampling/upsampling module discussed earlier in Section <a href="#S3.SS3" title="3.3 Use of 2D Information ‣ 3 The Proposed RadarFormer ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. The RF image resolution <math id="S4.SS0.SSS1.p1.8.m8.2" class="ltx_Math" alttext="(H,W)" display="inline"><semantics id="S4.SS0.SSS1.p1.8.m8.2a"><mrow id="S4.SS0.SSS1.p1.8.m8.2.3.2" xref="S4.SS0.SSS1.p1.8.m8.2.3.1.cmml"><mo stretchy="false" id="S4.SS0.SSS1.p1.8.m8.2.3.2.1" xref="S4.SS0.SSS1.p1.8.m8.2.3.1.cmml">(</mo><mi id="S4.SS0.SSS1.p1.8.m8.1.1" xref="S4.SS0.SSS1.p1.8.m8.1.1.cmml">H</mi><mo id="S4.SS0.SSS1.p1.8.m8.2.3.2.2" xref="S4.SS0.SSS1.p1.8.m8.2.3.1.cmml">,</mo><mi id="S4.SS0.SSS1.p1.8.m8.2.2" xref="S4.SS0.SSS1.p1.8.m8.2.2.cmml">W</mi><mo stretchy="false" id="S4.SS0.SSS1.p1.8.m8.2.3.2.3" xref="S4.SS0.SSS1.p1.8.m8.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS1.p1.8.m8.2b"><interval closure="open" id="S4.SS0.SSS1.p1.8.m8.2.3.1.cmml" xref="S4.SS0.SSS1.p1.8.m8.2.3.2"><ci id="S4.SS0.SSS1.p1.8.m8.1.1.cmml" xref="S4.SS0.SSS1.p1.8.m8.1.1">𝐻</ci><ci id="S4.SS0.SSS1.p1.8.m8.2.2.cmml" xref="S4.SS0.SSS1.p1.8.m8.2.2">𝑊</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS1.p1.8.m8.2c">(H,W)</annotation></semantics></math> is fixed at <math id="S4.SS0.SSS1.p1.9.m9.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S4.SS0.SSS1.p1.9.m9.1a"><mn id="S4.SS0.SSS1.p1.9.m9.1.1" xref="S4.SS0.SSS1.p1.9.m9.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS1.p1.9.m9.1b"><cn type="integer" id="S4.SS0.SSS1.p1.9.m9.1.1.cmml" xref="S4.SS0.SSS1.p1.9.m9.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS1.p1.9.m9.1c">128</annotation></semantics></math>. There are 40 sequences reserved for training and 10 sequences for testing. Testing annotations and images are not publicly shared and evaluation of the testing is done on a private evaluation server for the RODNet2021 challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS0.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.2 </span>Evaluation Metrics</h4>

<div id="S4.SS0.SSS2.p1" class="ltx_para">
<p id="S4.SS0.SSS2.p1.1" class="ltx_p">We use the same evaluation metrics as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> throughout all of our evaluations. RODNet uses an object location similarity (OLS) metric that takes the role of Intersection over Union (IoU).
OLS is then passed to a location-based non-maximum suppression (L-NMS) algorithm to generate confidence maps (ConfMaps). ConfMaps in the range-azimuth represent predicted locations for the objects, with multiple channels attributing the location to a class. Similar to previous work for pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, the output is a gaussian heatmap-like prediction with a mean equal to object location and variance attributed to the object class and scale information. Our main evaluation metric will be the average precision (AP) and average recall (AR) calculated through the variation of OLS threshold between 0.5 to 0.9 with steps of 0.05.</p>
</div>
</section>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baselines</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The baseline to the CRUW data set is the RODNet model associated with it, reporting an AP of 77.40% and AR of 80.80% using camera-only (CO) supervision. Instead of using the reported inference times, we instead retrain the model and report inference speed using our equipment to provide a consistent and scalable assessment and comparison. The model reports 61.2 million total trainable parameters (single stack hourglass). The time it takes for a single backpropagation iteration with a window size of 16 for this model on our setup is 1920 ms, and the average inference time is 148 ms. This is associated with 47664 GMAC (<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times 10^{9}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml"><mn id="S4.SS1.p1.1.m1.1.1.3.2" xref="S4.SS1.p1.1.m1.1.1.3.2.cmml">10</mn><mn id="S4.SS1.p1.1.m1.1.1.3.3" xref="S4.SS1.p1.1.m1.1.1.3.3.cmml">9</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">absent</csymbol><apply id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.p1.1.m1.1.1.3.2">10</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3.3">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\times 10^{9}</annotation></semantics></math> multiply-accumulate) operations. The listed parameters will be the main comparison points to our model.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Quantitative Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.6" class="ltx_p">Due to the recency of emerging RF-based data, and the lack of other works to compare to, we compare our proposed models, 2D transformer and RadarFormer, to RODNet considering the latter the state-of-the-art method. Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Quantitative Results ‣ 4 Experiments ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the performance comparisons of mentioned models. The evaluation metric here is AP and AR, each is split into four categories: <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="PL" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑃</ci><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">PL</annotation></semantics></math>, <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="CR" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝐶</ci><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">CR</annotation></semantics></math>, <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="CS" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><times id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></times><ci id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">𝐶</ci><ci id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">CS</annotation></semantics></math>, and <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="HW" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mi id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><times id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1"></times><ci id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">𝐻</ci><ci id="S4.SS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">HW</annotation></semantics></math>, referring to parking lot, campus road, city street, and highway data categories, respectively. Both AP and AR are then averaged into <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="AP_{total}" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mi id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.5.m5.1.1.1" xref="S4.SS2.p1.5.m5.1.1.1.cmml">​</mo><msub id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml"><mi id="S4.SS2.p1.5.m5.1.1.3.2" xref="S4.SS2.p1.5.m5.1.1.3.2.cmml">P</mi><mrow id="S4.SS2.p1.5.m5.1.1.3.3" xref="S4.SS2.p1.5.m5.1.1.3.3.cmml"><mi id="S4.SS2.p1.5.m5.1.1.3.3.2" xref="S4.SS2.p1.5.m5.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.5.m5.1.1.3.3.1" xref="S4.SS2.p1.5.m5.1.1.3.3.1.cmml">​</mo><mi id="S4.SS2.p1.5.m5.1.1.3.3.3" xref="S4.SS2.p1.5.m5.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.5.m5.1.1.3.3.1a" xref="S4.SS2.p1.5.m5.1.1.3.3.1.cmml">​</mo><mi id="S4.SS2.p1.5.m5.1.1.3.3.4" xref="S4.SS2.p1.5.m5.1.1.3.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.5.m5.1.1.3.3.1b" xref="S4.SS2.p1.5.m5.1.1.3.3.1.cmml">​</mo><mi id="S4.SS2.p1.5.m5.1.1.3.3.5" xref="S4.SS2.p1.5.m5.1.1.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.5.m5.1.1.3.3.1c" xref="S4.SS2.p1.5.m5.1.1.3.3.1.cmml">​</mo><mi id="S4.SS2.p1.5.m5.1.1.3.3.6" xref="S4.SS2.p1.5.m5.1.1.3.3.6.cmml">l</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><times id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1"></times><ci id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">𝐴</ci><apply id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m5.1.1.3.1.cmml" xref="S4.SS2.p1.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS2.p1.5.m5.1.1.3.2.cmml" xref="S4.SS2.p1.5.m5.1.1.3.2">𝑃</ci><apply id="S4.SS2.p1.5.m5.1.1.3.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3"><times id="S4.SS2.p1.5.m5.1.1.3.3.1.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3.1"></times><ci id="S4.SS2.p1.5.m5.1.1.3.3.2.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3.2">𝑡</ci><ci id="S4.SS2.p1.5.m5.1.1.3.3.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3.3">𝑜</ci><ci id="S4.SS2.p1.5.m5.1.1.3.3.4.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3.4">𝑡</ci><ci id="S4.SS2.p1.5.m5.1.1.3.3.5.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3.5">𝑎</ci><ci id="S4.SS2.p1.5.m5.1.1.3.3.6.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3.6">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">AP_{total}</annotation></semantics></math> and <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="AR_{total}" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mrow id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mi id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.1" xref="S4.SS2.p1.6.m6.1.1.1.cmml">​</mo><msub id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3.cmml"><mi id="S4.SS2.p1.6.m6.1.1.3.2" xref="S4.SS2.p1.6.m6.1.1.3.2.cmml">R</mi><mrow id="S4.SS2.p1.6.m6.1.1.3.3" xref="S4.SS2.p1.6.m6.1.1.3.3.cmml"><mi id="S4.SS2.p1.6.m6.1.1.3.3.2" xref="S4.SS2.p1.6.m6.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.3.3.1" xref="S4.SS2.p1.6.m6.1.1.3.3.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.3.3.3" xref="S4.SS2.p1.6.m6.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.3.3.1a" xref="S4.SS2.p1.6.m6.1.1.3.3.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.3.3.4" xref="S4.SS2.p1.6.m6.1.1.3.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.3.3.1b" xref="S4.SS2.p1.6.m6.1.1.3.3.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.3.3.5" xref="S4.SS2.p1.6.m6.1.1.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.3.3.1c" xref="S4.SS2.p1.6.m6.1.1.3.3.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.3.3.6" xref="S4.SS2.p1.6.m6.1.1.3.3.6.cmml">l</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><times id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1.1"></times><ci id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">𝐴</ci><apply id="S4.SS2.p1.6.m6.1.1.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.3.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3">subscript</csymbol><ci id="S4.SS2.p1.6.m6.1.1.3.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2">𝑅</ci><apply id="S4.SS2.p1.6.m6.1.1.3.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.3"><times id="S4.SS2.p1.6.m6.1.1.3.3.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3.3.1"></times><ci id="S4.SS2.p1.6.m6.1.1.3.3.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.3.2">𝑡</ci><ci id="S4.SS2.p1.6.m6.1.1.3.3.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.3.3">𝑜</ci><ci id="S4.SS2.p1.6.m6.1.1.3.3.4.cmml" xref="S4.SS2.p1.6.m6.1.1.3.3.4">𝑡</ci><ci id="S4.SS2.p1.6.m6.1.1.3.3.5.cmml" xref="S4.SS2.p1.6.m6.1.1.3.3.5">𝑎</ci><ci id="S4.SS2.p1.6.m6.1.1.3.3.6.cmml" xref="S4.SS2.p1.6.m6.1.1.3.3.6">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">AR_{total}</annotation></semantics></math>. We can see that RadarFormer achieved an AP almost on par with RODNet, with a higher AR. They varied in their performance in different situations, where we noticed that RadarFormer had slightly better performance in the parking lot scenarios, while RODNet performed noticeably better on campus roads. In city streets, RadarFormer had a higher AR than RODNet, but a lower AP, and on highways, RadarFormer outperformed RODNet significantly. Looking at total AP, the models perform comparably the same, but RadarFormer had a tendency to have a higher recall, on average, implying that RadarFormer has a tendency to produce fewer predictions, but an inclination to have higher confidence for the produced predictions. We also note that 2D CNN, which is a 2D version of RODNet that utilized our channel-chirp-time merging module did not compromise the accuracy greatly when compared to the retrained version of RODNet. However, we will see in Section <a href="#S4.SS2.SSS1" title="4.2.1 Model Comparisons ‣ 4.2 Quantitative Results ‣ 4 Experiments ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.1</span></a> how this module reduced the computational cost and size of RODNet to a large degree.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Quantitative comparisons of AP and AR on the CRUW data set with CO supervision, categories are described in Section <a href="#S4.SS2" title="4.2 Quantitative Results ‣ 4 Experiments ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. RadarFormer performed comparable to RODNet, being the accessible and replicable state-of-the-art model for this data set. The exact values of RODNet were not replicable, so we use the values reported in the RODNet challenge server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and report the values trained of the publicly available model (<span id="S4.T1.2.1" class="ltx_text ltx_font_bold">*</span>). We discuss the discrepancy in the results in Section <a href="#S3.SS3" title="3.3 Use of 2D Information ‣ 3 The Proposed RadarFormer ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T1.3.1.1.1.1" class="ltx_text">Method</span></th>
<td id="S4.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Total</td>
<td id="S4.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">PL</td>
<td id="S4.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">CR</td>
<td id="S4.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">CS</td>
<td id="S4.T1.3.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">HW</td>
</tr>
<tr id="S4.T1.3.2.2" class="ltx_tr">
<td id="S4.T1.3.2.2.1" class="ltx_td ltx_align_center">AP</td>
<td id="S4.T1.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r">AR</td>
<td id="S4.T1.3.2.2.3" class="ltx_td ltx_align_center">AP</td>
<td id="S4.T1.3.2.2.4" class="ltx_td ltx_align_center ltx_border_r">AR</td>
<td id="S4.T1.3.2.2.5" class="ltx_td ltx_align_center">AP</td>
<td id="S4.T1.3.2.2.6" class="ltx_td ltx_align_center ltx_border_r">AR</td>
<td id="S4.T1.3.2.2.7" class="ltx_td ltx_align_center">AP</td>
<td id="S4.T1.3.2.2.8" class="ltx_td ltx_align_center ltx_border_r">AR</td>
<td id="S4.T1.3.2.2.9" class="ltx_td ltx_align_center">AP</td>
<td id="S4.T1.3.2.2.10" class="ltx_td ltx_align_center ltx_border_r">AR</td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<th id="S4.T1.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">RODNet<span id="S4.T1.3.3.3.1.1" class="ltx_text ltx_font_bold">*</span>
</th>
<td id="S4.T1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.32</td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.62</td>
<td id="S4.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">94.52</td>
<td id="S4.T1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.59</td>
<td id="S4.T1.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.12</td>
<td id="S4.T1.3.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.50</td>
<td id="S4.T1.3.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.30</td>
<td id="S4.T1.3.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.13</td>
<td id="S4.T1.3.3.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.48</td>
<td id="S4.T1.3.3.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.63</td>
</tr>
<tr id="S4.T1.3.4.4" class="ltx_tr">
<th id="S4.T1.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">RODNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<td id="S4.T1.3.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.4.4.2.1" class="ltx_text ltx_font_bold">77.40</span></td>
<td id="S4.T1.3.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">80.80</td>
<td id="S4.T1.3.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.50</td>
<td id="S4.T1.3.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">96.40</td>
<td id="S4.T1.3.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.4.4.6.1" class="ltx_text ltx_font_bold">75.30</span></td>
<td id="S4.T1.3.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.4.4.7.1" class="ltx_text ltx_font_bold">78.40</span></td>
<td id="S4.T1.3.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.4.4.8.1" class="ltx_text ltx_font_bold">66.10</span></td>
<td id="S4.T1.3.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.70</td>
<td id="S4.T1.3.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.00</td>
<td id="S4.T1.3.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.20</td>
</tr>
<tr id="S4.T1.3.5.5" class="ltx_tr">
<th id="S4.T1.3.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">2D CNN</th>
<td id="S4.T1.3.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.58</td>
<td id="S4.T1.3.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">81.52</td>
<td id="S4.T1.3.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">94.93</td>
<td id="S4.T1.3.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">96.04</td>
<td id="S4.T1.3.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.52</td>
<td id="S4.T1.3.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.56</td>
<td id="S4.T1.3.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.15</td>
<td id="S4.T1.3.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.62</td>
<td id="S4.T1.3.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.23</td>
<td id="S4.T1.3.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.13</td>
</tr>
<tr id="S4.T1.3.6.6" class="ltx_tr">
<th id="S4.T1.3.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">2D Transformer</th>
<td id="S4.T1.3.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.03</td>
<td id="S4.T1.3.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">81.99</td>
<td id="S4.T1.3.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.6.6.4.1" class="ltx_text ltx_font_bold">96.68</span></td>
<td id="S4.T1.3.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.6.6.5.1" class="ltx_text ltx_font_bold">97.55</span></td>
<td id="S4.T1.3.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.51</td>
<td id="S4.T1.3.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.04</td>
<td id="S4.T1.3.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">58.67</td>
<td id="S4.T1.3.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.08</td>
<td id="S4.T1.3.6.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.16</td>
<td id="S4.T1.3.6.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.46</td>
</tr>
<tr id="S4.T1.3.7.7" class="ltx_tr">
<th id="S4.T1.3.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">RadarFormer</th>
<td id="S4.T1.3.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">77.18</td>
<td id="S4.T1.3.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.7.7.3.1" class="ltx_text ltx_font_bold">83.45</span></td>
<td id="S4.T1.3.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">95.88</td>
<td id="S4.T1.3.7.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">96.99</td>
<td id="S4.T1.3.7.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">71.74</td>
<td id="S4.T1.3.7.7.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">77.66</td>
<td id="S4.T1.3.7.7.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">61.19</td>
<td id="S4.T1.3.7.7.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.7.7.9.1" class="ltx_text ltx_font_bold">76.46</span></td>
<td id="S4.T1.3.7.7.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.7.7.10.1" class="ltx_text ltx_font_bold">74.37</span></td>
<td id="S4.T1.3.7.7.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.7.7.11.1" class="ltx_text ltx_font_bold">77.30</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Model Comparisons</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">We further compare the models’ computational cost and inference times using Table <a href="#S4.T2" title="Table 2 ‣ 4.2.1 Model Comparisons ‣ 4.2 Quantitative Results ‣ 4 Experiments ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We note that we use MAC operations instead of floating-point operations (FLOPs) due to PyTorch’s inclination to use MAC operations. The M-Net module used earlier remains constant and its performance is reported separately. The word ‘size’ is used interchangeably with the number of parameters. Despite RODNet having reported a marginally higher accuracy in CO supervision predictions, the model requires almost twenty times more MAC operations than our proposed model, while also being ten times as big in size. This is significant in regards to employing said models on devices that don’t have much computing power. Similarly in the same table, the inference and backpropagation (BP) times are aligned with the MAC discussion, where RadarFormer does provide inferences at roughly half the time of RODNet. We would like to note that these numbers are relative, and they scale up and down based on the used GPU for training and inference. Furthermore, we normalize and take into account the used window sizes and batch sizes for the training and testing. We also point out that reporting the BP time for a single iteration instead of training time per epoch is a more accurate measurement for this data set to factor out the loading time from the storage devices to the CUDA GPU. 2D CNN is a very lightweight and fast model that compromises the accuracy marginally compared to RODNet, while 2D Transformers provides a middle-ground between training and inference time and accuracy, but at the cost of having a large model size and the number of parameters.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparisons of (<math id="S4.T2.2.m1.1" class="ltx_Math" alttext="10^{9}" display="inline"><semantics id="S4.T2.2.m1.1b"><msup id="S4.T2.2.m1.1.1" xref="S4.T2.2.m1.1.1.cmml"><mn id="S4.T2.2.m1.1.1.2" xref="S4.T2.2.m1.1.1.2.cmml">10</mn><mn id="S4.T2.2.m1.1.1.3" xref="S4.T2.2.m1.1.1.3.cmml">9</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><apply id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.m1.1.1.1.cmml" xref="S4.T2.2.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.2.m1.1.1.2.cmml" xref="S4.T2.2.m1.1.1.2">10</cn><cn type="integer" id="S4.T2.2.m1.1.1.3.cmml" xref="S4.T2.2.m1.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">10^{9}</annotation></semantics></math>) multiply-accumulate (GMAC) operations, the number of parameters (No. Parm.) in millions (m), back-propagation (BP) time per iteration, and inference times between the different models (inclusive of M-Net). All time units are in milliseconds (ms). M-Net has no stand-alone inference or BP time since all models use it. The downsampling/upsampling streams are counted in our proposed models. All predictions and inferences were adjusted and normalized to take into account the batch size, window size, and test stride for equal comparisons.</figcaption>
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S4.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">GMACs</th>
<th id="S4.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">No. Parm. (m)</th>
<th id="S4.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">BP Time (ms)</th>
<th id="S4.T2.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Infer. Time (ms)</th>
</tr>
<tr id="S4.T2.3.2.2" class="ltx_tr">
<th id="S4.T2.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">M-Net</th>
<th id="S4.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.805</th>
<th id="S4.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.224</th>
<th id="S4.T2.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">-</th>
<th id="S4.T2.3.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">-</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.3.1" class="ltx_tr">
<td id="S4.T2.3.3.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">RODNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S4.T2.3.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47664</td>
<td id="S4.T2.3.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.22</td>
<td id="S4.T2.3.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1920</td>
<td id="S4.T2.3.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">148.17</td>
</tr>
<tr id="S4.T2.3.4.2" class="ltx_tr">
<td id="S4.T2.3.4.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">2D CNN</td>
<td id="S4.T2.3.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">388.6</td>
<td id="S4.T2.3.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.71</td>
<td id="S4.T2.3.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">330</td>
<td id="S4.T2.3.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">51.92</td>
</tr>
<tr id="S4.T2.3.5.3" class="ltx_tr">
<td id="S4.T2.3.5.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">2D Transformer (ours)</td>
<td id="S4.T2.3.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1950</td>
<td id="S4.T2.3.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.88</td>
<td id="S4.T2.3.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">380</td>
<td id="S4.T2.3.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.38</td>
</tr>
<tr id="S4.T2.3.6.4" class="ltx_tr">
<td id="S4.T2.3.6.4.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">RadarFormer (ours)</td>
<td id="S4.T2.3.6.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2123</td>
<td id="S4.T2.3.6.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6.42</td>
<td id="S4.T2.3.6.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">700</td>
<td id="S4.T2.3.6.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">84.35</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Qualitative Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We refer to Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.3 Qualitative Results ‣ 4 Experiments ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> for samples of the predictions from the mentioned models. It shows the original RGB image, RF image, ground truth (GT), then the prediction heatmap of the models for three samples. We also note that since we do not have access to the images of the test set, the models were re-trained on a 90% split of the training data, and these predictions were generated on the unseen 10% of the data. As discussed earlier and is evident by the relatively high AR of RadarFormer, the model produces predictions with relatively higher confidence when it decides on an object and a class, depicted through the accurate class label and higher brightness of the heatmap. We also notice that RadarFormer performs better in scenes with far objects as can be seen in the third row.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2304.08447/assets/x7.png" id="S4.F5.g1" class="ltx_graphics ltx_img_landscape" width="461" height="150" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Test cases for all models mentioned. Pedestrians, cyclists, and cars are referred to with red, green, and blue, respectively in the prediction and ground truth map. We see that RadarFormer provides predictions with acceptably high confidence when they are presented, in contrast to other models that are prone to occasional false labeling.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We further explore the incorrect prediction cases of RadarFormer and list some of the model’s limitations. Failure is most common due to difficult cases or due to inaccurate annotation of the ground truth, and we show examples of this in Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.3 Qualitative Results ‣ 4 Experiments ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. First, the model tends to generate predictions of objects that it perceives in the RF images but are not present in the ground truth, which can be seen in the first row. We can see a second car, however, the ground truth doesn’t show its presence and is then determined as an incorrect prediction case. We also notice that the model does not generate predictions of objects that are close to other objects within sensor’s view, as can be seen in the second row.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2304.08447/assets/x8.png" id="S4.F6.g1" class="ltx_graphics ltx_img_landscape" width="461" height="178" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Incorrect predictions generated by our proposed model, compared with predictions from RODNet. We note that despite there inaccurate annotations cases like this, the model attempts to predict objects that were not annotated. We can see this evident in the top row where we see predictable objects in the RGB images and the RF images, but not present in the ground truth. The second row is a difficult case where RODNet generated a better prediction than RadarFormer.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Studies</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.2" class="ltx_p"><span id="S4.SS4.p1.2.1" class="ltx_text ltx_font_bold">Data Format and Model Variations</span> We use a window size of <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mn id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><cn type="integer" id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">32</annotation></semantics></math> (<math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mn id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><cn type="integer" id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">32</annotation></semantics></math> frames) in the reported results, except for the RODNet baseline which was configured to 16. Using different window sizes between 8, 16, 24, and 48 did not yield any noticeable improvement to the model and reported degraded performance (1 to 2% less than using 32). This was evident when operating with and without temporal downsampling. Similarly, we varied the patch size of the transformers, the window size of MaXViT, and the MLP sizes. The BP and inference times remain relatively constant, however, the required video ram (VRAM) increases exponentially with larger attention windows without any improvement to the accuracy. Similarly, the size of the window attention for MaXViT should remain a fraction of the image resolution (window size of 7 for an RF image of resolution 128). Using the base window size of 7 yielded consistently higher AP than other counterparts. Increasing the number of neurons in transformer MLP caused the model to not converge to a high enough accuracy compared with a lower number of neurons, so we use an MLP ratio range between 20 and 150, depending on how deep the model goes (smaller ratio for deeper models). Any value higher than 150 yielded degraded performance, and the same applies to values less than 20 (AP maximum of 66%). Lastly, we observed an increase in accuracy from 75.10% to 77.18% when we introduced varying spatial kernels to the convolutions after and before the transformer layers as mentioned in Section <a href="#S3.SS3" title="3.3 Use of 2D Information ‣ 3 The Proposed RadarFormer ‣ RadarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Training Stride</span>
The stride controls how overlapped the data input is. The model’s performance degraded noticeably when we remove the stride and take every unique window size. A jump of around 10% in accuracy is usually observed on different models when an overlap of at least half to 75% of the window size is introduced (e.g. overlap of 24 frames in a 32 window size input).</p>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.4" class="ltx_p"><span id="S4.SS4.p3.4.1" class="ltx_text ltx_font_bold">Learning Rate &amp; Scheduling</span>
The model trains on multiple iterations per epoch, and it is possible to converge to a set of weights that produce acceptable results (71.23% AP) within the first 3 epochs of training at a learning rate of <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><msup id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mn id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS4.p3.1.m1.1.1.3" xref="S4.SS4.p3.1.m1.1.1.3.cmml"><mo id="S4.SS4.p3.1.m1.1.1.3a" xref="S4.SS4.p3.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS4.p3.1.m1.1.1.3.2" xref="S4.SS4.p3.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS4.p3.1.m1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.2">10</cn><apply id="S4.SS4.p3.1.m1.1.1.3.cmml" xref="S4.SS4.p3.1.m1.1.1.3"><minus id="S4.SS4.p3.1.m1.1.1.3.1.cmml" xref="S4.SS4.p3.1.m1.1.1.3"></minus><cn type="integer" id="S4.SS4.p3.1.m1.1.1.3.2.cmml" xref="S4.SS4.p3.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">10^{-4}</annotation></semantics></math>. This learning rate is too large, however, starting with a learning rate of <math id="S4.SS4.p3.2.m2.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S4.SS4.p3.2.m2.1a"><msup id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mn id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">10</mn><mrow id="S4.SS4.p3.2.m2.1.1.3" xref="S4.SS4.p3.2.m2.1.1.3.cmml"><mo id="S4.SS4.p3.2.m2.1.1.3a" xref="S4.SS4.p3.2.m2.1.1.3.cmml">−</mo><mn id="S4.SS4.p3.2.m2.1.1.3.2" xref="S4.SS4.p3.2.m2.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS4.p3.2.m2.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.2">10</cn><apply id="S4.SS4.p3.2.m2.1.1.3.cmml" xref="S4.SS4.p3.2.m2.1.1.3"><minus id="S4.SS4.p3.2.m2.1.1.3.1.cmml" xref="S4.SS4.p3.2.m2.1.1.3"></minus><cn type="integer" id="S4.SS4.p3.2.m2.1.1.3.2.cmml" xref="S4.SS4.p3.2.m2.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">10^{-5}</annotation></semantics></math> proved to be too small. We used multiple scheduling techniques and starting/end points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Starting with a learning rate of <math id="S4.SS4.p3.3.m3.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS4.p3.3.m3.1a"><msup id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml"><mn id="S4.SS4.p3.3.m3.1.1.2" xref="S4.SS4.p3.3.m3.1.1.2.cmml">10</mn><mrow id="S4.SS4.p3.3.m3.1.1.3" xref="S4.SS4.p3.3.m3.1.1.3.cmml"><mo id="S4.SS4.p3.3.m3.1.1.3a" xref="S4.SS4.p3.3.m3.1.1.3.cmml">−</mo><mn id="S4.SS4.p3.3.m3.1.1.3.2" xref="S4.SS4.p3.3.m3.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><apply id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.3.m3.1.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.SS4.p3.3.m3.1.1.2.cmml" xref="S4.SS4.p3.3.m3.1.1.2">10</cn><apply id="S4.SS4.p3.3.m3.1.1.3.cmml" xref="S4.SS4.p3.3.m3.1.1.3"><minus id="S4.SS4.p3.3.m3.1.1.3.1.cmml" xref="S4.SS4.p3.3.m3.1.1.3"></minus><cn type="integer" id="S4.SS4.p3.3.m3.1.1.3.2.cmml" xref="S4.SS4.p3.3.m3.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">10^{-4}</annotation></semantics></math> and ending at <math id="S4.SS4.p3.4.m4.1" class="ltx_Math" alttext="10^{-6}" display="inline"><semantics id="S4.SS4.p3.4.m4.1a"><msup id="S4.SS4.p3.4.m4.1.1" xref="S4.SS4.p3.4.m4.1.1.cmml"><mn id="S4.SS4.p3.4.m4.1.1.2" xref="S4.SS4.p3.4.m4.1.1.2.cmml">10</mn><mrow id="S4.SS4.p3.4.m4.1.1.3" xref="S4.SS4.p3.4.m4.1.1.3.cmml"><mo id="S4.SS4.p3.4.m4.1.1.3a" xref="S4.SS4.p3.4.m4.1.1.3.cmml">−</mo><mn id="S4.SS4.p3.4.m4.1.1.3.2" xref="S4.SS4.p3.4.m4.1.1.3.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.4.m4.1b"><apply id="S4.SS4.p3.4.m4.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.4.m4.1.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1">superscript</csymbol><cn type="integer" id="S4.SS4.p3.4.m4.1.1.2.cmml" xref="S4.SS4.p3.4.m4.1.1.2">10</cn><apply id="S4.SS4.p3.4.m4.1.1.3.cmml" xref="S4.SS4.p3.4.m4.1.1.3"><minus id="S4.SS4.p3.4.m4.1.1.3.1.cmml" xref="S4.SS4.p3.4.m4.1.1.3"></minus><cn type="integer" id="S4.SS4.p3.4.m4.1.1.3.2.cmml" xref="S4.SS4.p3.4.m4.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.4.m4.1c">10^{-6}</annotation></semantics></math> using both step scheduling and cosine annealing yielded the final models.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We introduce a novel transformers-based architecture for deep learning applications on radar frequency images, named RadarFormer. The main novelty lies in the reduction of total computing complexity and training/inference times of the original model by using transformers, a lightweight and efficient deep learning module. We also introduce a channel-chirp-time merging block that contributes to the reduction in computation complexity without compromising accuracy. Our multi-axis attention-based model produces state-of-the-art results while having significantly fewer parameters and inference time compared to the previous state-of-the-art method. The proposed models can pave the way for transformer-based research in radar frequency deep learning research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Angelov, A., Robertson, A., Murray-Smith, R., Fioranelli, F.: Practical
classification of different moving targets using automotive radar and deep
neural networks. IET Radar, Sonar &amp; Navigation <span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">12</span>(10), 1082–1089
(2018). https://doi.org/https://doi.org/10.1049/iet-rsn.2018.0103,
<a target="_blank" href="https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-rsn.2018.0103" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-rsn.2018.0103</a>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bansal, K., Rungta, K., Bharadia, D.: Radsegnet: A reliable approach to radar
camera fusion (2022). https://doi.org/10.48550/ARXIV.2208.03849,
<a target="_blank" href="https://arxiv.org/abs/2208.03849" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2208.03849</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Gall, J.,
Stachniss, C.: Towards 3D LiDAR-based semantic scene understanding of 3D
point cloud sequences: The SemanticKITTI Dataset. The International Journal
on Robotics Research <span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">40</span>(8-9), 959–967 (2021).
https://doi.org/10.1177/02783649211006735

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Cao, P., Xia, W., Ye, M., Zhang, J., Zhou, J.: Radar-id: human identification
based on radar micro-doppler signatures using deep convolutional neural
networks. IET Radar, Sonar &amp; Navigation <span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">12</span>(7), 729–734 (2018).
https://doi.org/https://doi.org/10.1049/iet-rsn.2017.0511,
<a target="_blank" href="https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-rsn.2017.0511" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-rsn.2017.0511</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Capobianco, S., Facheris, L., Cuccoli, F., Marinai, S.: Vehicle classification
based on convolutional networks applied to fmcw radar signals. In: Leuzzi,
F., Ferilli, S. (eds.) Traffic Mining Applied to Police Activities. pp.
115–128. Springer International Publishing, Cham (2018)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Dai, Z., Liu, H., Le, Q.V., Tan, M.: Coatnet: Marrying convolution and
attention for all data sizes. In: Beygelzimer, A., Dauphin, Y., Liang, P.,
Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems
(2021), <a target="_blank" href="https://openreview.net/forum?id=dUk5Foj5CLf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=dUk5Foj5CLf</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
D’Ascoli, S., Touvron, H., Leavitt, M.L., Morcos, A.S., Biroli, G., Sagun, L.:
Convit: Improving vision transformers with soft convolutional inductive
biases. In: Internation Conference on Machine Learning. pp. 2286–2296 (2021)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for
image recognition at scale (2020). https://doi.org/10.48550/ARXIV.2010.11929,
<a target="_blank" href="https://arxiv.org/abs/2010.11929" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2010.11929</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Feng, D., Haase-Schütz, C., Rosenbaum, L., Hertlein, H., Gläser, C., Timm,
F., Wiesbeck, W., Dietmayer, K.: Deep multi-modal object detection and
semantic segmentation for autonomous driving: Datasets, methods, and
challenges. IEEE Transactions on Intelligent Transportation Systems
<span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">22</span>(3), 1341–1360 (2021). https://doi.org/10.1109/TITS.2020.2972974

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Hassanin, M., Anwar, S., Radwan, I., Khan, F.S., Mian, A.: Visual attention
methods in deep learning: An in-depth survey (2022).
https://doi.org/10.48550/ARXIV.2204.07756, <a target="_blank" href="https://arxiv.org/abs/2204.07756" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2204.07756</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B.,
Roth, H.R., Xu, D.: Unetr: Transformers for 3d medical image segmentation.
In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision. pp. 574–584 (2022)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 770–778 (2016). https://doi.org/10.1109/CVPR.2016.90

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks
for mobile vision applications. CoRR <span id="bib.bib13.1.1" class="ltx_text ltx_font_bold">abs/1704.04861</span> (2017),
<a target="_blank" href="http://arxiv.org/abs/1704.04861" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1704.04861</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.:
Transformers in vision: A survey. ACM Comput. Surv. <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">54</span>(10s) (sep
2022). https://doi.org/10.1145/3505244, <a target="_blank" href="https://doi.org/10.1145/3505244" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3505244</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Lahoud, J., Cao, J., Khan, F.S., Cholakkal, H., Anwer, R.M., Khan, S., Yang,
M.H.: 3d vision with transformers: A survey. arXiv preprint arXiv:2208.04309
(2022)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Li, Y., Wu, C.Y., Fan, H., Mangalam, K., Xiong, B., Malik, J., Feichtenhofer,
C.: Mvitv2: Improved multiscale vision transformers for classification and
detection. In: CVPR (2022)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Lim, T.Y., Ansari, A., Major, B., Fontijne, D., Hamilton, M., Gowaikar, R.,
Subramanian, S.: Radar and camera early fusion for vehicle detection in
advanced driver assistance systems. NeurIPS Machine Learning for Autonomous
Driving Workshop (2019)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Lim, T.Y., Markowitz, S.A., Do, M.N.: Radical: A synchronized fmcw radar,
depth, imu and rgb camera data dataset with low-level fmcw radar signals.
IEEE Journal of Selected Topics in Signal Processing <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">15</span>(4),
941–953 (2021). https://doi.org/10.1109/JSTSP.2021.3061270

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang,
Z., Dong, L., Wei, F., Guo, B.: Swin transformer v2: Scaling up capacity and
resolution. In: International Conference on Computer Vision and Pattern
Recognition (CVPR) (2022)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
transformer: Hierarchical vision transformer using shifted windows. In:
Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV) (2021)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: SGDR: Stochastic gradient descent with warm
restarts. In: International Conference on Learning Representations (2017),
<a target="_blank" href="https://openreview.net/forum?id=Skq89Scxx" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=Skq89Scxx</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose
estimation. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) Computer
Vision – ECCV 2016. pp. 483–499. Springer International Publishing, Cham
(2016)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Ouaknine, A., Newson, A., Pérez, P., Tupin, F., Rebut, J.: Multi-view radar
semantic segmentation. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV). pp. 15671–15680 (October 2021)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ouaknine, A., Newson, A., Rebut, J., Tupin, F., Pérez, P.: Carrada dataset:
Camera and automotive radar with range- angle- doppler annotations. In: 2020
25th International Conference on Pattern Recognition (ICPR). pp. 5068–5075
(2021). https://doi.org/10.1109/ICPR48806.2021.9413181

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Pan, Z., Cai, J., Zhuang, B.: Fast vision transformers with hilo attention. In:
NeurIPS (2022)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Peiris, H., Hayat, M., Chen, Z., Egan, G., Harandi, M.: A robust volumetric
transformer for accurate 3d tumor segmentation. In: International Conference
on Medical Image Computing and Computer-Assisted Intervention. pp. 162–172.
Springer (2022)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Redmon, J., Divvala, S.K., Girshick, R.B., Farhadi, A.: You only look once:
Unified, real-time object detection. CoRR <span id="bib.bib27.1.1" class="ltx_text ltx_font_bold">abs/1506.02640</span> (2015),
<a target="_blank" href="http://arxiv.org/abs/1506.02640" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1506.02640</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Schumann, O., Hahn, M., Scheiner, N., Weishaupt, F., Tilly, J.F., Dickmann, J.,
Wöhler, C.: Radarscenes: A real-world radar point cloud data set for
automotive applications. CoRR <span id="bib.bib28.1.1" class="ltx_text ltx_font_bold">abs/2104.02493</span> (2021),
<a target="_blank" href="https://arxiv.org/abs/2104.02493" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2104.02493</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Shaker, A., Maaz, M., Rasheed, H., Khan, S., Yang, M.H., Khan, F.S.: Unetr++:
Delving into efficient and accurate 3d medical image segmentation.
arXiv:2212.04497 (2022)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: 2015 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1–9
(2015). https://doi.org/10.1109/CVPR.2015.7298594

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.:
Maxvit: Multi-axis vision transformer. ECCV (2022)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Vogginger, B., Kreutz, F., López-Randulfe, J., Liu, C., Dietrich, R.,
Gonzalez, H.A., Scholz, D., Reeb, N., Auge, D., Hille, J., Arsalan, M.,
Mirus, F., Grassmann, C., Knoll, A., Mayr, C.: Automotive radar processing
with spiking neural networks: Concepts and challenges. Frontiers in
Neuroscience <span id="bib.bib32.1.1" class="ltx_text ltx_font_bold">16</span> (2022). https://doi.org/10.3389/fnins.2022.851774,
<a target="_blank" href="https://www.frontiersin.org/articles/10.3389/fnins.2022.851774" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.frontiersin.org/articles/10.3389/fnins.2022.851774</a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Wang, Y., Huang, Y.T., Hwang, J.N.: Monocular visual object 3d localization in
road scenes. In: Proceedings of the 27th ACM International Conference on
Multimedia. pp. 917–925. ACM (2019)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Wang, Y., Hwang, J.N., Wang, G., Liu, H., Kim, K.J., Hsu, H.M., Cai, J., Zhang,
H., Jiang, Z., Gu, R.: Rod2021 challenge: A summary for radar object
detection challenge for autonomous driving applications. In: Proceedings of
the 2021 International Conference on Multimedia Retrieval. pp. 553–559
(2021)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Wang, Y., Jiang, Z., Gao, X., Hwang, J.N., Xing, G., Liu, H.: Rodnet: Radar
object detection using cross-modal supervision. In: 2021 IEEE Winter
Conference on Applications of Computer Vision (WACV). pp. 504–513 (2021).
https://doi.org/10.1109/WACV48630.2021.00055

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Wang, Y., Guizilini, V., Zhang, T., Wang, Y., Zhao, H., , Solomon, J.M.:
Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In:
The Conference on Robot Learning (CoRL) (2021)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Xiao, T., Singh, M., Mintun, E., Darrell, T., Dollár, P., Girshick, R.:
Early convolutions help transformers see better. Advances in Neural
Information Processing Systems <span id="bib.bib37.1.1" class="ltx_text ltx_font_bold">34</span>, 30392–30400 (2021)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Yan, X., Gao, J., Li, J., Zhang, R., Li, Z., Huang, R., Cui, S.: Sparse single
sweep lidar point cloud segmentation via learning contextual shape priors
from scene completion. Proceedings of the AAAI Conference on Artificial
Intelligence <span id="bib.bib38.1.1" class="ltx_text ltx_font_bold">35</span>(4), 3101–3109 (May 2021).
https://doi.org/10.1609/aaai.v35i4.16419,
<a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/16419" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ojs.aaai.org/index.php/AAAI/article/view/16419</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Ye, D., Chen, W., Zhou, Z., Xie, Y., Wang, Y., Wang, P., Foroosh, H.:
Lidarmultinet: Unifying lidar semantic segmentation, 3d object detection, and
panoptic segmentation in a single multi-task network (2022).
https://doi.org/10.48550/ARXIV.2206.11428, <a target="_blank" href="https://arxiv.org/abs/2206.11428" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2206.11428</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Yuan, Y., Fu, R., Huang, L., Lin, W., Zhang, C., Chen, X., Wang, J.: Hrformer:
High-resolution transformer for dense prediction. In: NeurIPS (2021)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Yuan, Z., Song, X., Bai, L., Wang, Z., Ouyang, W.: Temporal-channel transformer
for 3d lidar-based video object detection for autonomous driving. IEEE
Transactions on Circuits and Systems for Video Technology <span id="bib.bib41.1.1" class="ltx_text ltx_font_bold">32</span>(4),
2068–2078 (2022). https://doi.org/10.1109/TCSVT.2021.3082763

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Zhang, A., Nowruzi, F.E., Laganiere, R.: Raddet: Range-azimuth-doppler based
radar object detection for dynamic road users. In: 2021 18th Conference on
Robots and Vision (CRV). pp. 95–102 (2021).
https://doi.org/10.1109/CRV52889.2021.00021

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Zhou, Z., Zhang, Y., Foroosh, H.: Panoptic-polarnet: Proposal-free lidar point
cloud panoptic segmentation. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (2021)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.08446" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.08447" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.08447">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.08447" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.08448" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 14:12:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
