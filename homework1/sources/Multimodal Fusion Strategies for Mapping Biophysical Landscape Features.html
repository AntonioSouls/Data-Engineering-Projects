<!DOCTYPE html><html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Multimodal Fusion Strategies for Mapping Biophysical Landscape Features</title>
<!--Generated on Mon Oct  7 08:30:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">




<meta content="multimodal fusion aerial imagery ecology remote sensing rhino middens termite mounds few-shot class imbalance" lang="en" name="keywords">
<base href="https://arxiv.org/html/2410.04833v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2410.04833v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2410.04833v1/#myForm">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2410.04833v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2410.04833v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S1" title="In Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S2" title="In Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S3" title="In Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S4" title="In Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S4.SS0.SSS1" title="In 4 Methods ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.1 </span><span class="ltx_text ltx_font_smallcaps">Early fusion</span>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S4.SS0.SSS2" title="In 4 Methods ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.2 </span><span class="ltx_text ltx_font_smallcaps">Late fusion</span>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S4.SS0.SSS3" title="In 4 Methods ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.3 </span><span class="ltx_text ltx_font_smallcaps">Mixture of Experts</span>.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S5" title="In Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S5.SS0.SSS1" title="In 5 Experiments ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.0.1 </span>Setup.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S5.SS0.SSS2" title="In 5 Experiments ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.0.2 </span>Results.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S6" title="In Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div class="package-alerts ltx_document" role="status" aria-label="Conversion errors have been found">
      <button aria-label="Dismiss alert">
          <span aria-hidden="true"><svg role="presentation" width="20" height="20" viewBox="0 0 44 44" aria-hidden="true" focusable="false">
          <path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
          <path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
          </svg></span>
      </button>
      <p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
          <ul arial-label="Unsupported packages used in this paper">
              <li>failed: axessibility</li><li>failed: orcidlink</li>
          </ul>
      <p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
    </div><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY 4.0</a><div id="watermark-tr">arXiv:2410.04833v1 [cs.CV] 07 Oct 2024</div></div>
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Harvard University, USA </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>University of Copenhagen, Denmark
<br class="ltx_break"><span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span><a class="ltx_ref ltx_href" href="mailto:luciagordon@g.harvard.edu" title="">luciagordon@g.harvard.edu</a></span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Multimodal Fusion Strategies for Mapping Biophysical Landscape Features</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lucia Gordon<span class="ltx_ERROR undefined" id="id1.1.id1">\orcidlink</span>0000-0003-3219-6960
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nico Lang<span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0000-0001-8434-027X
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Catherine Ressijac
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrew Davies<span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0002-0003-1435
</span><span class="ltx_author_notes">11</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id4.id1">Multimodal aerial data are used to monitor natural systems, and machine learning can significantly accelerate the classification of landscape features within such imagery to benefit ecology and conservation. It remains under-explored, however, how these multiple modalities ought to be fused in a deep learning model. As a step towards filling this gap, we study three strategies (<span class="ltx_text ltx_font_smallcaps" id="id4.id1.1">Early fusion</span>, <span class="ltx_text ltx_font_smallcaps" id="id4.id1.2">Late fusion</span>, and <span class="ltx_text ltx_font_smallcaps" id="id4.id1.3">Mixture of Experts</span>) for fusing thermal, RGB, and LiDAR imagery using a dataset of spatially-aligned orthomosaics in these three modalities. In particular, we aim to map three ecologically-relevant biophysical landscape features in African savanna ecosystems: rhino middens, termite mounds, and water. The three fusion strategies differ in whether the modalities are fused early or late, and if late, whether the model learns fixed weights per modality for each class or generates weights for each class adaptively, based on the input. Overall, the three methods have similar macro-averaged performance with <span class="ltx_text ltx_font_smallcaps" id="id4.id1.4">Late fusion</span> achieving an AUC of 0.698, but their per-class performance varies strongly, with <span class="ltx_text ltx_font_smallcaps" id="id4.id1.5">Early fusion</span> achieving the best recall for middens and water and <span class="ltx_text ltx_font_smallcaps" id="id4.id1.6">Mixture of Experts</span> achieving the best recall for mounds.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The code is available at <a class="ltx_ref ltx_href" href="https://github.com/lgordon99/fusion-strategies-eccv" title="">https://github.com/lgordon99/fusion-strategies-eccv</a>.</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>multimodal fusion aerial imagery ecology remote sensing rhino middens termite mounds few-shot class imbalance
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In response to widespread land degradation and biodiversity loss, in its Fifteenth Sustainable Development Goal “Life on Land,” the United Nations calls for protecting, restoring, and promoting the sustainable use and management of terrestrial ecosystems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib3" title="">3</a>]</cite>. Successfully conserving habitats and species necessitates effective ecosystem and biodiversity monitoring&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib10" title="">10</a>]</cite>. Towards this goal, ecologists and protected area managers map biophysical landscape features, often through a combination of remote sensing and ground surveys.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In African savanna ecosystems, three examples of such features of interest are rhino middens, termite mounds, and water.
Rhino middens are communal defecation sites used by rhinos for territorial marking and social communication&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib9" title="">9</a>]</cite>. Termite mounds, composed of soil, termite saliva, and dung, are islands of enhanced nutrient and moisture content in the landscape, facilitating the growth of vegetation, which in turn attracts herbivores&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib8" title="">8</a>]</cite>. Water, which is here defined as a river, stream, or watering hole, is a key resource on which many animal species are dependent. These landscape features are often distributed throughout vast, inaccessible areas, making manual annotation of remotely sensed imagery tedious and a complete ground survey beyond available time and resources. This challenge motivates the use of deep learning (DL) to automate the detection of biophysical landscape features in UAV imagery.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We develop DL models to map these features in aerial imagery taken of a 284-hectare site in Kruger National Park, South Africa in January 2020&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib5" title="">5</a>]</cite>. The UAV was equipped with a thermal camera, an RGB camera, and a LiDAR (light detection and ranging) scanner, which simultaneously collected thermal, RGB, and elevation data at 0.5-m, 0.05-m, and 0.1-m resolution, respectively (see <a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S1.F1" title="In 1 Introduction ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>). Using this multimodal data, we study the question of: <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">How to most effectively fuse multi-resolution thermal, RGB, and LiDAR data in a DL model for multi-class image classification using limited, imbalanced training data?</span> In particular, is it better to perform fusion earlier or later in the feature extraction process? And how should the modalities be weighted when fused later?</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="210" id="S1.F1.g1" src="https://arxiv.org/html/2410.04833v1/x1.png" width="706">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">
Multi-band, multi-resolution aerial imagery collected for a site in Kruger National Park, South Africa.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This work is an extension of Gordon <em class="ltx_emph ltx_font_italic" id="S2.p1.1.1">et al</em>.<span class="ltx_text" id="S2.p1.1.2"></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib5" title="">5</a>]</cite>, which develops an active learning methodology for the detection of rhino middens in this same imagery. While the purpose of that work was to demonstrate the success of a domain-inspired active learning technique, this work explores different fusion methods and architectures and also generalizes to a multi-class setting by considering termite mounds and water in addition to rhino middens.
Burke <em class="ltx_emph ltx_font_italic" id="S2.p1.1.3">et al</em>.<span class="ltx_text" id="S2.p1.1.4"></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib1" title="">1</a>]</cite> summarizes two general fusion strategies. In <em class="ltx_emph ltx_font_italic" id="S2.p1.1.5">early fusion</em>, multiple input modalities are stacked and fed into a single neural network as multiple input channels. In contrast, <em class="ltx_emph ltx_font_italic" id="S2.p1.1.6">late fusion</em> involves using a separate network for each of the modalities to extract modality-specific features. The features extracted by these multiple networks are then concatenated and fed into a prediction layer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Yeh <em class="ltx_emph ltx_font_italic" id="S2.p2.1.1">et al</em>.<span class="ltx_text" id="S2.p2.1.2"></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib12" title="">12</a>]</cite> compares the performance of early and late fusion for predicting wealth across Africa from Landsat satellite imagery and nightlight data. In their early fusion method they stacked the Landsat and nightlight bands and passed them through a single network, while in their late fusion method they used two separate ResNet-18 models, one for the Landsat imagery and one for the nightlight data, to extract features from each of the modalities that were then merged and passed through a fully connected layer. They found that late fusion outperformed early fusion.
Late fusion can be beneficial when different modalities are more or less useful for classifying images belonging to different classes.
For example, Gordon <em class="ltx_emph ltx_font_italic" id="S2.p2.1.3">et al</em>.<span class="ltx_text" id="S2.p2.1.4"></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib5" title="">5</a>]</cite> found that thermal imagery was most useful for detecting rhino middens due to their strong heat signature.
In this dataset, while the height of termite mounds makes them stand out in LiDAR imagery, the same is not true of water, which is at ground level.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">A <em class="ltx_emph ltx_font_italic" id="S2.p3.1.1">mixture of experts</em> in the context of DL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib4" title="">4</a>]</cite> contains separate neural networks, or “experts,” that specialize in different parts of the input space, which here correspond to modalities. The outputs of the different networks are combined using weights specified by a <em class="ltx_emph ltx_font_italic" id="S2.p3.1.2">gating network</em>, which maps each input to a distribution over the experts using a multilayer perceptron (MLP) with a final softmax layer to normalize the resultant weight values. The expert outputs weighted by these gating weights then serve as the final predictions. In contrast to late fusion that has fixed modality-specific weights for each class, using a gating network makes it possible to weight the modalities differently for different inputs. This is useful, for example, if some termite mounds are tall and others are short, meaning that LiDAR will be more useful for the former than the latter.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The dataset consists of spatially aligned thermal, RGB, and LiDAR orthomosaics in GeoTIFF format along with latitude-longitude coordinates for the rhino middens, termite mounds, and water bodies in the site, which we gridify into 20x20-m cells (see <a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S3.F2.sf1" title="In Figure 2 ‣ 3 Dataset ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">2(a)</span></a>).
We use QGIS to assign each grid cell a label corresponding to the aforementioned landscape feature it contains, and it is labeled “empty” if it contains none of them.
Because the band number and resolution of the orthomosaics vary across modalities, the same cell is represented by a different-sized tile in each of the modalities: (40,40) for thermal, (3,400,400) for RGB, and (200,200) for LiDAR.
As this is a geospatial dataset, we perform a spatially-aware train-validation-test partition&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib11" title="">11</a>]</cite> (see <a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S3.F2.sf2" title="In Figure 2 ‣ 3 Dataset ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">2(b)</span></a>), taking the first 50 columns of the grid for training, the next 9 for validation, and the remaining 22 for testing.
To address the severe class imbalance (most cells are empty), within the training set we randomly undersample grid cells in the empty class and oversample cells in the midden and water classes so that there are 88 cells for each class. The validation set contains 704 empty cells, 12 midden cells, 18 mound cells, and 17 water cells. The test set contains 1,036 empty cells, 10 midden cells, 16 mound cells, and 21 water cells. The dataset is proprietary and sensitive due to rhino poaching but could be made available upon request.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="586" id="S3.F2.sf1.g1" src="https://arxiv.org/html/2410.04833v1/x2.png" width="498">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F2.sf1.4.2" style="font-size:90%;">Thermal, RGB, and LiDAR gridded orthomosaics with landscape features: <span class="ltx_text" id="S3.F2.sf1.4.2.1" style="color:#FF0000;">rhino middens<span class="ltx_text" id="S3.F2.sf1.4.2.1.1" style="color:#000000;">, <span class="ltx_text" id="S3.F2.sf1.4.2.1.1.1" style="color:#00FF00;">termite mounds</span>, and <span class="ltx_text" id="S3.F2.sf1.4.2.1.1.2" style="color:#00FFFF;">water</span>.</span></span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="628" id="S3.F2.sf2.g1" src="https://arxiv.org/html/2410.04833v1/x3.png" width="540">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F2.sf2.3.2" style="font-size:90%;">The site is spatially split into training, validation, and testing areas.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Spatial distribution of landscape features (a) and regional data split (b).</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methods</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To address the limited training data (88 cells per class) available, we utilize transfer learning. All our models are based on a ResNet-50&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib6" title="">6</a>]</cite> pretrained on ImageNet-1k&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib2" title="">2</a>]</cite>, a dataset of ground-level RGB images. ResNet-50 takes in three-channel images and outputs a vector of class predictions. We modify the final fully connected layer in the architecture to yield a four-dimensional output to match the four classes in our dataset. We adapt the architecture further in various ways to implement three distinct fusion methods visualized in <a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S4.F3" title="In 4 Methods ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="560" id="S4.F3.sf1.g1" src="https://arxiv.org/html/2410.04833v1/x4.png" width="829">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_smallcaps" id="S4.F3.sf1.4.2" style="font-size:90%;">Early fusion</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="592" id="S4.F3.sf2.g1" src="https://arxiv.org/html/2410.04833v1/x5.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_smallcaps" id="S4.F3.sf2.4.2" style="font-size:90%;">Late fusion</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="355" id="S4.F3.sf3.g1" src="https://arxiv.org/html/2410.04833v1/x6.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.sf3.3.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text ltx_font_smallcaps" id="S4.F3.sf3.4.2" style="font-size:90%;">Mixture of Experts</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">Diagrams for the three fusion methods studied.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S4.SS0.SSS1">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.1 </span><span class="ltx_text ltx_font_smallcaps" id="S4.SS0.SSS1.1.1">Early fusion</span>.</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS1.p1">
<p class="ltx_p" id="S4.SS0.SSS1.p1.1">The <span class="ltx_text ltx_font_smallcaps" id="S4.SS0.SSS1.p1.1.1">early fusion</span> strategy (see <a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S4.F3.sf1" title="In Figure 3 ‣ 4 Methods ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3(a)</span></a>) first involves upsampling the thermal and LiDAR TIFFs using cubic resampling to match the resolution of the RGB TIFF. Each grid cell then corresponds to three tiles (thermal, RGB, LiDAR) of shapes (400,400), (3,400,400), and (400,400), respectively. We concatenate these three tiles to form a (5,400,400) array for each grid cell. Because the pre-trained ResNet-50 architecture accepts a three-channel input, we modify the first convolutional layer to accept five channels and set the weights of the non-RGB channels to the mean of the weights of the RGB channels and rescale all the weights in that first layer by 3/5, following the procedure in Yeh&nbsp;<em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS1.p1.1.2">et al</em>.<span class="ltx_text" id="S4.SS0.SSS1.p1.1.3"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib12" title="">12</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS0.SSS2">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.2 </span><span class="ltx_text ltx_font_smallcaps" id="S4.SS0.SSS2.1.1">Late fusion</span>.</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS2.p1">
<p class="ltx_p" id="S4.SS0.SSS2.p1.1">The <span class="ltx_text ltx_font_smallcaps" id="S4.SS0.SSS2.p1.1.1">Late fusion</span> strategy (see <a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S4.F3.sf2" title="In Figure 3 ‣ 4 Methods ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3(b)</span></a>) passes the tiles of different modalities through separate feature extractors.
Since the thermal and LiDAR tiles have a single band, we modify the first convolutional layer of a ResNet-50 to accept a single-channel input.
We set the weights of that channel to the mean of the weights of the RGB channels and rescale all the weights in that first layer by 3, consistent with the procedure described for <span class="ltx_text ltx_font_smallcaps" id="S4.SS0.SSS2.p1.1.2">Early fusion</span>.
The final fully connected layer of all three networks is modified to output 256 features.
The three sets of 256 features output by the three feature extractors are then concatenated and fed into a final fully connected layer that outputs class predictions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS0.SSS3">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.3 </span><span class="ltx_text ltx_font_smallcaps" id="S4.SS0.SSS3.1.1">Mixture of Experts</span>.</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS3.p1">
<p class="ltx_p" id="S4.SS0.SSS3.p1.1">The <span class="ltx_text ltx_font_smallcaps" id="S4.SS0.SSS3.p1.1.1">Mixture of Experts</span> (MoE, see <a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S4.F3.sf3" title="In Figure 3 ‣ 4 Methods ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3(c)</span></a>) method first passes the tiles of different modalities through separate feature extractors as in <span class="ltx_text ltx_font_smallcaps" id="S4.SS0.SSS3.p1.1.2">Late fusion</span>. Next, the features extracted for each modality are passed through separate fully connected layers that output class predictions. The three sets of 256 features are also concatenated and passed through a gating network consisting of a fully connected layer outputting 256 features, a ReLU function, a fully connected layer with a three-dimensional output (for the three modalities), and a softmax function. The output of the gating network are then normalized weights on each of the modalities.
These gating weights are used to combine the individual expert predictions in a weighted average to yield the final class predictions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S5.SS0.SSS1">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.0.1 </span>Setup.</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS1.p1">
<p class="ltx_p" id="S5.SS0.SSS1.p1.2">We use a batch size of 64 and continue training until the AUC on the validation set has been less than the best AUC for more than 10 consecutive epochs. We normalize all the imagery using the training statistics such that each band in the training imagery has mean 0 and standard deviation 1.
We perform 50 trials for each method, where the random seed for trial <math alttext="i" class="ltx_Math" display="inline" id="S5.SS0.SSS1.p1.1.m1.1"><semantics id="S5.SS0.SSS1.p1.1.m1.1a"><mi id="S5.SS0.SSS1.p1.1.m1.1.1" xref="S5.SS0.SSS1.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS1.p1.1.m1.1b"><ci id="S5.SS0.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS1.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS1.p1.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS1.p1.1.m1.1d">italic_i</annotation></semantics></math> is <math alttext="i" class="ltx_Math" display="inline" id="S5.SS0.SSS1.p1.2.m2.1"><semantics id="S5.SS0.SSS1.p1.2.m2.1a"><mi id="S5.SS0.SSS1.p1.2.m2.1.1" xref="S5.SS0.SSS1.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS1.p1.2.m2.1b"><ci id="S5.SS0.SSS1.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS1.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS1.p1.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS1.p1.2.m2.1d">italic_i</annotation></semantics></math>.
Training for <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS1.p1.2.1">Early fusion</span> and <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS1.p1.2.2">Late fusion</span> utilizes cross entropy loss and <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS1.p1.2.3">Mixture of Experts</span> uses negative log likelihood loss.
We use an Adam optimizer with a learning rate of 0.001 for <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS1.p1.2.4">Early fusion</span> and <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS1.p1.2.5">Late fusion</span>, and 0.0001 for <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS1.p1.2.6">Mixture of Experts</span>.
We tuned these learning rates by selecting the best AUC on the validation set during the training period when using a learning rate of 0.1, 0.01, 0.001, 0.0001, and 0.00001.
All model parameters are fine-tuned during training. All experiments were carried out using one NVIDIA A100 SXM4 40GB graphics card. <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS1.p1.2.7">Early fusion</span> required 40GB of RAM and <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS1.p1.2.8">Late fusion</span> and <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS1.p1.2.9">Mixture of Experts</span> required 20GB.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS0.SSS2">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.0.2 </span>Results.</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS2.p1">
<p class="ltx_p" id="S5.SS0.SSS2.p1.1">The overall performance of the three methods is plotted in <a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S5.F4.sf1" title="In Figure 4 ‣ 5.0.2 Results. ‣ 5 Experiments ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4(a)</span></a>, which shows all three methods have very similar precision and AUC on the test set.
Comparing Figures&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S5.F4.sf2" title="Figure 4(b) ‣ Figure 4 ‣ 5.0.2 Results. ‣ 5 Experiments ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_tag">4(b)</span></a>-<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S5.F4.sf5" title="Figure 4(e) ‣ Figure 4 ‣ 5.0.2 Results. ‣ 5 Experiments ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_tag">4(e)</span></a> shows that even after balancing classes in the training set, precision was by far highest on the empty class for all methods and rather low for all landscape features, which means that all methods lead to a high number of false positives, which could be attributed to the low number of instances available for training for each class.
Recall is highest for termite mounds, with <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS2.p1.1.1">Mixture of Experts</span> achieving the best performance.
We hypothesize that this is because different termite mounds can look very different in the various modalities depending on their height and vegetation, so <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS2.p1.1.2">Mixture of Experts</span>, which applies input-specific modality weights, is well-suited.
We observe a large performance difference between middens and mounds, and mounds’ higher recall could be due to the larger number of distinct images in the training set.
The ranking of the three methods on recall is also reversed for middens and mounds, which could be due to the approximately 3x fewer parameters in the <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS2.p1.1.3">Early fusion</span> model being a better fit to the smaller amount of distinct midden instances in the training data.
<span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS2.p1.1.4">Mixture of Experts</span> does not introduce many more parameters than <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS2.p1.1.5">Late fusion</span>, but the adaptive modality weighting might lead to overfitting for the midden and water classes.
Though <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS2.p1.1.6">Late fusion</span> does not perform best for any of the individual classes, it generalizes best across them, leading to its best AUC overall.
For all three landscape features we see that recall is much higher than precision.
For this application, recall is more important than precision since ecologists would rather have to manually discard some empty images than miss some instances of the landscape features.
Nevertheless, future work could explore data augmentation and alternative class-balancing strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib7" title="">7</a>]</cite> in order to improve the F1 score for all methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S5.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="579" id="S5.F4.sf1.g1" src="https://arxiv.org/html/2410.04833v1/x7.png" width="831">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="563" id="S5.F4.sf2.g1" src="https://arxiv.org/html/2410.04833v1/x8.png" width="832">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="579" id="S5.F4.sf3.g1" src="https://arxiv.org/html/2410.04833v1/x9.png" width="831">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.sf3.2.1.1" style="font-size:90%;">(c)</span> </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F4.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="579" id="S5.F4.sf4.g1" src="https://arxiv.org/html/2410.04833v1/x10.png" width="831">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.sf4.2.1.1" style="font-size:90%;">(d)</span> </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F4.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="579" id="S5.F4.sf5.g1" src="https://arxiv.org/html/2410.04833v1/x11.png" width="831">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.sf5.2.1.1" style="font-size:90%;">(e)</span> </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.3.2" style="font-size:90%;">Performance evaluated for three fusion methods both macro-averaged over all classes (a) and stratified by class (b–e). Mean and two standard errors are displayed.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS2.p2">
<p class="ltx_p" id="S5.SS0.SSS2.p2.1"><span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS2.p2.1.1">Mixture of Experts</span> is the most interpretable among these fusion methods, as we can inspect the gating weights assigned to each of the modalities for test images belonging to different classes, visualized in <a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#S5.T1" title="In 5.0.2 Results. ‣ 5 Experiments ‣ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>.
We see that for midden images, the LiDAR modality is weighted highest.
While middens show slight depressions in the LiDAR imagery due to rhinos’ scraping behavior, their warmth gives them the strongest signal in thermal data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04833v1#bib.bib5" title="">5</a>]</cite>, so MoE’s poor midden recall could be explained by it not having learned to weight thermal highest for this class.
For mound images, the LiDAR modality is weighted highest, even more strongly than for middens.
This is expected because mounds tend to be even taller than middens but often resemble other vegetation in thermal and RGB.
MoE achieves high recall for mounds, so its intuitive modality weighting is consistent with strong performance.
For water images, the thermal modality is weighted highest.
While water’s cooler temperature gives it some signal in thermal data, we expect its blue color visible in the RGB imagery to be an even stronger signal.
Incorporating new data from other sites or using data augmentation techniques could help MoE learn to more appropriately weight the modalities for the midden and water classes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T1.12.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text ltx_font_smallcaps" id="S5.T1.13.2" style="font-size:90%;">Mixture of Experts<span class="ltx_text ltx_font_upright" id="S5.T1.13.2.1"> per-modality gating weights averaged across test images in each class and pooled across trials. Mean and two standard errors are displayed.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.9.10.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T1.9.10.1.1" style="padding-left:8.0pt;padding-right:8.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.9.10.1.2" style="padding-left:8.0pt;padding-right:8.0pt;">Rhino midden</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.9.10.1.3" style="padding-left:8.0pt;padding-right:8.0pt;">Termite mound</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.9.10.1.4" style="padding-left:8.0pt;padding-right:8.0pt;">Water</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T1.3.3.4" style="padding-left:8.0pt;padding-right:8.0pt;">Thermal</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="0.35\pm 0.02" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mrow id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml"><mn id="S5.T1.1.1.1.m1.1.1.2" xref="S5.T1.1.1.1.m1.1.1.2.cmml">0.35</mn><mo id="S5.T1.1.1.1.m1.1.1.1" xref="S5.T1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S5.T1.1.1.1.m1.1.1.3" xref="S5.T1.1.1.1.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><apply id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S5.T1.1.1.1.m1.1.1.2.cmml" type="float" xref="S5.T1.1.1.1.m1.1.1.2">0.35</cn><cn id="S5.T1.1.1.1.m1.1.1.3.cmml" type="float" xref="S5.T1.1.1.1.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">0.35\pm 0.02</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">0.35 ± 0.02</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.2.2" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="0.30\pm 0.03" class="ltx_Math" display="inline" id="S5.T1.2.2.2.m1.1"><semantics id="S5.T1.2.2.2.m1.1a"><mrow id="S5.T1.2.2.2.m1.1.1" xref="S5.T1.2.2.2.m1.1.1.cmml"><mn id="S5.T1.2.2.2.m1.1.1.2" xref="S5.T1.2.2.2.m1.1.1.2.cmml">0.30</mn><mo id="S5.T1.2.2.2.m1.1.1.1" xref="S5.T1.2.2.2.m1.1.1.1.cmml">±</mo><mn id="S5.T1.2.2.2.m1.1.1.3" xref="S5.T1.2.2.2.m1.1.1.3.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m1.1b"><apply id="S5.T1.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.2.2.2.m1.1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S5.T1.2.2.2.m1.1.1.2.cmml" type="float" xref="S5.T1.2.2.2.m1.1.1.2">0.30</cn><cn id="S5.T1.2.2.2.m1.1.1.3.cmml" type="float" xref="S5.T1.2.2.2.m1.1.1.3">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m1.1c">0.30\pm 0.03</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.m1.1d">0.30 ± 0.03</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.3" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathbf{0.40\pm 0.03}" class="ltx_Math" display="inline" id="S5.T1.3.3.3.m1.1"><semantics id="S5.T1.3.3.3.m1.1a"><mrow id="S5.T1.3.3.3.m1.1.1" xref="S5.T1.3.3.3.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" id="S5.T1.3.3.3.m1.1.1.2" mathvariant="bold" xref="S5.T1.3.3.3.m1.1.1.2.cmml">0.40</mn><mo id="S5.T1.3.3.3.m1.1.1.1" xref="S5.T1.3.3.3.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" id="S5.T1.3.3.3.m1.1.1.3" mathvariant="bold" xref="S5.T1.3.3.3.m1.1.1.3.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.m1.1b"><apply id="S5.T1.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1"><csymbol cd="latexml" id="S5.T1.3.3.3.m1.1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S5.T1.3.3.3.m1.1.1.2.cmml" type="float" xref="S5.T1.3.3.3.m1.1.1.2">0.40</cn><cn id="S5.T1.3.3.3.m1.1.1.3.cmml" type="float" xref="S5.T1.3.3.3.m1.1.1.3">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.m1.1c">\mathbf{0.40\pm 0.03}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.m1.1d">bold_0.40 ± bold_0.03</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.6.6.4" style="padding-left:8.0pt;padding-right:8.0pt;">RGB</th>
<td class="ltx_td ltx_align_center" id="S5.T1.4.4.1" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="0.25\pm 0.02" class="ltx_Math" display="inline" id="S5.T1.4.4.1.m1.1"><semantics id="S5.T1.4.4.1.m1.1a"><mrow id="S5.T1.4.4.1.m1.1.1" xref="S5.T1.4.4.1.m1.1.1.cmml"><mn id="S5.T1.4.4.1.m1.1.1.2" xref="S5.T1.4.4.1.m1.1.1.2.cmml">0.25</mn><mo id="S5.T1.4.4.1.m1.1.1.1" xref="S5.T1.4.4.1.m1.1.1.1.cmml">±</mo><mn id="S5.T1.4.4.1.m1.1.1.3" xref="S5.T1.4.4.1.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.1.m1.1b"><apply id="S5.T1.4.4.1.m1.1.1.cmml" xref="S5.T1.4.4.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.4.4.1.m1.1.1.1.cmml" xref="S5.T1.4.4.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S5.T1.4.4.1.m1.1.1.2.cmml" type="float" xref="S5.T1.4.4.1.m1.1.1.2">0.25</cn><cn id="S5.T1.4.4.1.m1.1.1.3.cmml" type="float" xref="S5.T1.4.4.1.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.1.m1.1c">0.25\pm 0.02</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.1.m1.1d">0.25 ± 0.02</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.5.2" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="0.27\pm 0.03" class="ltx_Math" display="inline" id="S5.T1.5.5.2.m1.1"><semantics id="S5.T1.5.5.2.m1.1a"><mrow id="S5.T1.5.5.2.m1.1.1" xref="S5.T1.5.5.2.m1.1.1.cmml"><mn id="S5.T1.5.5.2.m1.1.1.2" xref="S5.T1.5.5.2.m1.1.1.2.cmml">0.27</mn><mo id="S5.T1.5.5.2.m1.1.1.1" xref="S5.T1.5.5.2.m1.1.1.1.cmml">±</mo><mn id="S5.T1.5.5.2.m1.1.1.3" xref="S5.T1.5.5.2.m1.1.1.3.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.2.m1.1b"><apply id="S5.T1.5.5.2.m1.1.1.cmml" xref="S5.T1.5.5.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.5.5.2.m1.1.1.1.cmml" xref="S5.T1.5.5.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S5.T1.5.5.2.m1.1.1.2.cmml" type="float" xref="S5.T1.5.5.2.m1.1.1.2">0.27</cn><cn id="S5.T1.5.5.2.m1.1.1.3.cmml" type="float" xref="S5.T1.5.5.2.m1.1.1.3">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.2.m1.1c">0.27\pm 0.03</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.2.m1.1d">0.27 ± 0.03</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T1.6.6.3" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="0.24\pm 0.03" class="ltx_Math" display="inline" id="S5.T1.6.6.3.m1.1"><semantics id="S5.T1.6.6.3.m1.1a"><mrow id="S5.T1.6.6.3.m1.1.1" xref="S5.T1.6.6.3.m1.1.1.cmml"><mn id="S5.T1.6.6.3.m1.1.1.2" xref="S5.T1.6.6.3.m1.1.1.2.cmml">0.24</mn><mo id="S5.T1.6.6.3.m1.1.1.1" xref="S5.T1.6.6.3.m1.1.1.1.cmml">±</mo><mn id="S5.T1.6.6.3.m1.1.1.3" xref="S5.T1.6.6.3.m1.1.1.3.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.3.m1.1b"><apply id="S5.T1.6.6.3.m1.1.1.cmml" xref="S5.T1.6.6.3.m1.1.1"><csymbol cd="latexml" id="S5.T1.6.6.3.m1.1.1.1.cmml" xref="S5.T1.6.6.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S5.T1.6.6.3.m1.1.1.2.cmml" type="float" xref="S5.T1.6.6.3.m1.1.1.2">0.24</cn><cn id="S5.T1.6.6.3.m1.1.1.3.cmml" type="float" xref="S5.T1.6.6.3.m1.1.1.3">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.3.m1.1c">0.24\pm 0.03</annotation><annotation encoding="application/x-llamapun" id="S5.T1.6.6.3.m1.1d">0.24 ± 0.03</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T1.9.9.4" style="padding-left:8.0pt;padding-right:8.0pt;">LiDAR</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.7.7.1" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathbf{0.40\pm 0.02}" class="ltx_Math" display="inline" id="S5.T1.7.7.1.m1.1"><semantics id="S5.T1.7.7.1.m1.1a"><mrow id="S5.T1.7.7.1.m1.1.1" xref="S5.T1.7.7.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" id="S5.T1.7.7.1.m1.1.1.2" mathvariant="bold" xref="S5.T1.7.7.1.m1.1.1.2.cmml">0.40</mn><mo id="S5.T1.7.7.1.m1.1.1.1" xref="S5.T1.7.7.1.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" id="S5.T1.7.7.1.m1.1.1.3" mathvariant="bold" xref="S5.T1.7.7.1.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.1.m1.1b"><apply id="S5.T1.7.7.1.m1.1.1.cmml" xref="S5.T1.7.7.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.7.7.1.m1.1.1.1.cmml" xref="S5.T1.7.7.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S5.T1.7.7.1.m1.1.1.2.cmml" type="float" xref="S5.T1.7.7.1.m1.1.1.2">0.40</cn><cn id="S5.T1.7.7.1.m1.1.1.3.cmml" type="float" xref="S5.T1.7.7.1.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.1.m1.1c">\mathbf{0.40\pm 0.02}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.7.1.m1.1d">bold_0.40 ± bold_0.02</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.8.8.2" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathbf{0.43\pm 0.04}" class="ltx_Math" display="inline" id="S5.T1.8.8.2.m1.1"><semantics id="S5.T1.8.8.2.m1.1a"><mrow id="S5.T1.8.8.2.m1.1.1" xref="S5.T1.8.8.2.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" id="S5.T1.8.8.2.m1.1.1.2" mathvariant="bold" xref="S5.T1.8.8.2.m1.1.1.2.cmml">0.43</mn><mo id="S5.T1.8.8.2.m1.1.1.1" xref="S5.T1.8.8.2.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" id="S5.T1.8.8.2.m1.1.1.3" mathvariant="bold" xref="S5.T1.8.8.2.m1.1.1.3.cmml">0.04</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.2.m1.1b"><apply id="S5.T1.8.8.2.m1.1.1.cmml" xref="S5.T1.8.8.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.8.8.2.m1.1.1.1.cmml" xref="S5.T1.8.8.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S5.T1.8.8.2.m1.1.1.2.cmml" type="float" xref="S5.T1.8.8.2.m1.1.1.2">0.43</cn><cn id="S5.T1.8.8.2.m1.1.1.3.cmml" type="float" xref="S5.T1.8.8.2.m1.1.1.3">0.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.2.m1.1c">\mathbf{0.43\pm 0.04}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.8.8.2.m1.1d">bold_0.43 ± bold_0.04</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.9.9.3" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="0.36\pm 0.04" class="ltx_Math" display="inline" id="S5.T1.9.9.3.m1.1"><semantics id="S5.T1.9.9.3.m1.1a"><mrow id="S5.T1.9.9.3.m1.1.1" xref="S5.T1.9.9.3.m1.1.1.cmml"><mn id="S5.T1.9.9.3.m1.1.1.2" xref="S5.T1.9.9.3.m1.1.1.2.cmml">0.36</mn><mo id="S5.T1.9.9.3.m1.1.1.1" xref="S5.T1.9.9.3.m1.1.1.1.cmml">±</mo><mn id="S5.T1.9.9.3.m1.1.1.3" xref="S5.T1.9.9.3.m1.1.1.3.cmml">0.04</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.3.m1.1b"><apply id="S5.T1.9.9.3.m1.1.1.cmml" xref="S5.T1.9.9.3.m1.1.1"><csymbol cd="latexml" id="S5.T1.9.9.3.m1.1.1.1.cmml" xref="S5.T1.9.9.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S5.T1.9.9.3.m1.1.1.2.cmml" type="float" xref="S5.T1.9.9.3.m1.1.1.2">0.36</cn><cn id="S5.T1.9.9.3.m1.1.1.3.cmml" type="float" xref="S5.T1.9.9.3.m1.1.1.3">0.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.3.m1.1c">0.36\pm 0.04</annotation><annotation encoding="application/x-llamapun" id="S5.T1.9.9.3.m1.1d">0.36 ± 0.04</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We studied three methods for fusing thermal, RGB, and LiDAR imagery for mapping three biophysical landscape features: rhino middens, termite mounds, and water.
<span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">Early fusion</span> concatenates multimodal tiles of the same resolution and passes them through a 5-channel ResNet. <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.2">Late fusion</span> passes tiles of different modalities through separate ResNet feature extractors before fusing them in a fully connected layer. <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.3">Mixture of Experts</span> uses a gating network to weight the predictions from each modality as a function of the input.
Overall, the results show that while the three methods have similar macro-averaged performance with <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.4">Late fusion</span> achieving an AUC of 0.698, their per-class performance varies strongly, with <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.5">Early fusion</span> achieving the best recall for middens and water and <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.6">Mixture of Experts</span> achieving the best recall for mounds.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We acknowledge South African National Parks for logistical and scientific support, and for permission to perform the study in Kruger National Park.
This work was supported in part by the Pioneer Centre for AI, DNRF grant number P1. L.G. was supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE2140743.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Burke, M., Driscoll, A., Lobell, D.B., Ermon, S.: <a class="ltx_ref ltx_href" href="https://www.science.org/doi/10.1126/science.abe8628" title="">Using satellite imagery to understand and promote sustainable development</a>. Science <span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">371</span>(6535) (2021). https://doi.org/10.1126/science.abe8628

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: <a class="ltx_ref ltx_href" href="https://ieeexplore.ieee.org/document/5206848" title="">ImageNet: A large-scale hierarchical image database</a>. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition. pp. 248–255 (2009). https://doi.org/10.1109/CVPR.2009.5206848

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
of&nbsp;Economic, U.N.D., Affairs, S.: <a class="ltx_ref ltx_href" href="https://sdgs.un.org/goals/goal15" title="">Goal 15: Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss</a> (2023), accessed: 2023-05-24

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Eigen, D., Ranzato, M.A., Sutskever, I.: <a class="ltx_ref ltx_href" href="https://arxiv.org/pdf/1312.4314" title="">Learning Factored Representations in a Deep Mixture of Experts</a>. ICLR (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gordon, L., Behari, N., Collier, S., Bondi-Kelly, E., Killian, J.A., Ressijac, C., Boucher, P., Davies, A., Tambe, M.: <a class="ltx_ref ltx_href" href="https://www.ijcai.org/proceedings/2023/0663.pdf" title="">Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats</a>. In: Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23. pp. 5977–5985. International Joint Conferences on Artificial Intelligence Organization (8 2023). https://doi.org/10.24963/ijcai.2023/663, aI for Good

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: <a class="ltx_ref ltx_href" href="https://arxiv.org/pdf/1512.03385" title="">Deep Residual Learning for Image Recognition</a>. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 770–778 (2016). https://doi.org/10.1109/CVPR.2016.90

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., Kalantidis, Y.: <a class="ltx_ref ltx_href" href="https://arxiv.org/pdf/1910.09217" title="">Decoupling Representation and Classifier for Long-Tailed Recognition</a>. In: Proceedings of the The Eighth International Conference on Learning Representations, ICLR-20. International Conference on Learning Representations (2020). https://doi.org/https://doi.org/10.48550/arXiv.1910.09217

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Loveridge, J.P., Moe, S.R.: <a class="ltx_ref ltx_href" href="https://www.cambridge.org/core/journals/journal-of-tropical-ecology/article/abs/termitaria-as-browsing-hotspots-for-african-megaherbivores-in-miombo-woodland/E2809392DD9ECB34C99A8FD0CA56EA4F" title="">Termitaria as browsing hotspots for African megaherbivores in miombo woodland</a>. Journal of Tropical Ecology <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">20</span>(3), 337–343 (2004). https://doi.org/10.1017/S0266467403001202

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Owen-Smith, R.N., Smith, R.N.O.: <a class="ltx_ref ltx_href" href="http://www.rhinoresourcecenter.com/pdf_files/132/1320739004.pdf" title="">The Behavioural Ecology of the White Rhinoceros</a>. Ph.D. thesis, University of Wisconsin Madison (1973)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Parrish, J.D., Braun, D.P., Unnasch, R.S.: <a class="ltx_ref ltx_href" href="https://academic.oup.com/bioscience/article-pdf/53/9/851/26894780/53-9-851.pdf" title="">Are We Conserving What We Say We Are? Measuring Ecological Integrity within Protected Areas</a>. BioScience <span class="ltx_text ltx_font_bold" id="bib.bib10.1.1">53</span>(9), 851–860 (09 2003). https://doi.org/10.1641/0006-3568(2003)053[0851:AWCWWS]2.0.CO;2

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rolf, E., Klemmer, K., Robinson, C., Kerner, H.: <a class="ltx_ref ltx_href" href="https://arxiv.org/pdf/2402.01444" title="">Mission Critical – Satellite Data is a Distinct Modality in Machine Learning</a> (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yeh, C., Perez, A., Driscoll, A., Azzari, G., Tang, Z., Lobell, D., Ermon, S., Burke, M.: <a class="ltx_ref ltx_href" href="https://www.nature.com/articles/s41467-020-16185-w" title="">Using publicly available satellite imagery and deep learning to understand economic well-being in Africa</a>. Nature Communications <span class="ltx_text ltx_font_bold" id="bib.bib12.1.1">11</span>(2583) (2020)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed; display: none;">Report Issue for Selection</button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body></html>