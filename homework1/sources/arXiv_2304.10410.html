<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.10410] Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review</title><meta property="og:description" content="Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years, enabling vehicles to accurately detect and interpret surrounding environment for safe and efficient…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.10410">

<!--Generated on Thu Feb 29 13:22:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Autonomous driving,  radar-camera fusion,  object detection,  semantic segmentation.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: 
<br class="ltx_break">A Comprehensive Review</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shanliang Yao<math id="id1.1.m1.1" class="ltx_Math" alttext="{}^{\text{1}}" display="inline"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mtext id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><ci id="id1.1.m1.1.1.1a.cmml" xref="id1.1.m1.1.1.1"><mtext mathsize="70%" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{\text{1}}</annotation></semantics></math>, Runwei Guan<math id="id2.2.m2.1" class="ltx_Math" alttext="{}^{\text{1}}" display="inline"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mtext id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><ci id="id2.2.m2.1.1.1a.cmml" xref="id2.2.m2.1.1.1"><mtext mathsize="70%" id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{\text{1}}</annotation></semantics></math>, Xiaoyu Huang<math id="id3.3.m3.1" class="ltx_Math" alttext="{}^{\text{1}}" display="inline"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mtext id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><ci id="id3.3.m3.1.1.1a.cmml" xref="id3.3.m3.1.1.1"><mtext mathsize="70%" id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">{}^{\text{1}}</annotation></semantics></math>, Zhuoxiao Li<math id="id4.4.m4.1" class="ltx_Math" alttext="{}^{\text{1}}" display="inline"><semantics id="id4.4.m4.1a"><msup id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mi id="id4.4.m4.1.1a" xref="id4.4.m4.1.1.cmml"></mi><mtext id="id4.4.m4.1.1.1" xref="id4.4.m4.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><ci id="id4.4.m4.1.1.1a.cmml" xref="id4.4.m4.1.1.1"><mtext mathsize="70%" id="id4.4.m4.1.1.1.cmml" xref="id4.4.m4.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">{}^{\text{1}}</annotation></semantics></math>, Xiangyu Sha<math id="id5.5.m5.1" class="ltx_Math" alttext="{}^{\text{1}}" display="inline"><semantics id="id5.5.m5.1a"><msup id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml"><mi id="id5.5.m5.1.1a" xref="id5.5.m5.1.1.cmml"></mi><mtext id="id5.5.m5.1.1.1" xref="id5.5.m5.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><apply id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1"><ci id="id5.5.m5.1.1.1a.cmml" xref="id5.5.m5.1.1.1"><mtext mathsize="70%" id="id5.5.m5.1.1.1.cmml" xref="id5.5.m5.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">{}^{\text{1}}</annotation></semantics></math>, 
<br class="ltx_break">Yong Yue<math id="id6.6.m6.1" class="ltx_Math" alttext="{}^{\text{2}}" display="inline"><semantics id="id6.6.m6.1a"><msup id="id6.6.m6.1.1" xref="id6.6.m6.1.1.cmml"><mi id="id6.6.m6.1.1a" xref="id6.6.m6.1.1.cmml"></mi><mtext id="id6.6.m6.1.1.1" xref="id6.6.m6.1.1.1a.cmml">2</mtext></msup><annotation-xml encoding="MathML-Content" id="id6.6.m6.1b"><apply id="id6.6.m6.1.1.cmml" xref="id6.6.m6.1.1"><ci id="id6.6.m6.1.1.1a.cmml" xref="id6.6.m6.1.1.1"><mtext mathsize="70%" id="id6.6.m6.1.1.1.cmml" xref="id6.6.m6.1.1.1">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.m6.1c">{}^{\text{2}}</annotation></semantics></math>, Eng Gee Lim<math id="id7.7.m7.1" class="ltx_Math" alttext="{}^{\text{2}}" display="inline"><semantics id="id7.7.m7.1a"><msup id="id7.7.m7.1.1" xref="id7.7.m7.1.1.cmml"><mi id="id7.7.m7.1.1a" xref="id7.7.m7.1.1.cmml"></mi><mtext id="id7.7.m7.1.1.1" xref="id7.7.m7.1.1.1a.cmml">2</mtext></msup><annotation-xml encoding="MathML-Content" id="id7.7.m7.1b"><apply id="id7.7.m7.1.1.cmml" xref="id7.7.m7.1.1"><ci id="id7.7.m7.1.1.1a.cmml" xref="id7.7.m7.1.1.1"><mtext mathsize="70%" id="id7.7.m7.1.1.1.cmml" xref="id7.7.m7.1.1.1">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.m7.1c">{}^{\text{2}}</annotation></semantics></math>, Senior Member, IEEE, Hyungjoon Seo<math id="id8.8.m8.1" class="ltx_Math" alttext="{}^{\text{1}}" display="inline"><semantics id="id8.8.m8.1a"><msup id="id8.8.m8.1.1" xref="id8.8.m8.1.1.cmml"><mi id="id8.8.m8.1.1a" xref="id8.8.m8.1.1.cmml"></mi><mtext id="id8.8.m8.1.1.1" xref="id8.8.m8.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="id8.8.m8.1b"><apply id="id8.8.m8.1.1.cmml" xref="id8.8.m8.1.1"><ci id="id8.8.m8.1.1.1a.cmml" xref="id8.8.m8.1.1.1"><mtext mathsize="70%" id="id8.8.m8.1.1.1.cmml" xref="id8.8.m8.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.8.m8.1c">{}^{\text{1}}</annotation></semantics></math>, Ka Lok Man<math id="id9.9.m9.1" class="ltx_Math" alttext="{}^{\text{2}}" display="inline"><semantics id="id9.9.m9.1a"><msup id="id9.9.m9.1.1" xref="id9.9.m9.1.1.cmml"><mi id="id9.9.m9.1.1a" xref="id9.9.m9.1.1.cmml"></mi><mtext id="id9.9.m9.1.1.1" xref="id9.9.m9.1.1.1a.cmml">2</mtext></msup><annotation-xml encoding="MathML-Content" id="id9.9.m9.1b"><apply id="id9.9.m9.1.1.cmml" xref="id9.9.m9.1.1"><ci id="id9.9.m9.1.1.1a.cmml" xref="id9.9.m9.1.1.1"><mtext mathsize="70%" id="id9.9.m9.1.1.1.cmml" xref="id9.9.m9.1.1.1">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.9.m9.1c">{}^{\text{2}}</annotation></semantics></math>, Xiaohui Zhu<math id="id10.10.m10.2" class="ltx_Math" alttext="{}^{\text{2},\dagger}" display="inline"><semantics id="id10.10.m10.2a"><msup id="id10.10.m10.2.2" xref="id10.10.m10.2.2.cmml"><mi id="id10.10.m10.2.2a" xref="id10.10.m10.2.2.cmml"></mi><mrow id="id10.10.m10.2.2.2.4" xref="id10.10.m10.2.2.2.3.cmml"><mtext id="id10.10.m10.1.1.1.1" xref="id10.10.m10.1.1.1.1a.cmml">2</mtext><mo rspace="0em" id="id10.10.m10.2.2.2.4.1" xref="id10.10.m10.2.2.2.3.cmml">,</mo><mo lspace="0em" id="id10.10.m10.2.2.2.2" xref="id10.10.m10.2.2.2.2.cmml">†</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="id10.10.m10.2b"><apply id="id10.10.m10.2.2.cmml" xref="id10.10.m10.2.2"><list id="id10.10.m10.2.2.2.3.cmml" xref="id10.10.m10.2.2.2.4"><ci id="id10.10.m10.1.1.1.1a.cmml" xref="id10.10.m10.1.1.1.1"><mtext mathsize="70%" id="id10.10.m10.1.1.1.1.cmml" xref="id10.10.m10.1.1.1.1">2</mtext></ci><ci id="id10.10.m10.2.2.2.2.cmml" xref="id10.10.m10.2.2.2.2">†</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.10.m10.2c">{}^{\text{2},\dagger}</annotation></semantics></math>, Yutao Yue<math id="id11.11.m11.2" class="ltx_Math" alttext="{}^{\text{3},\dagger}" display="inline"><semantics id="id11.11.m11.2a"><msup id="id11.11.m11.2.2" xref="id11.11.m11.2.2.cmml"><mi id="id11.11.m11.2.2a" xref="id11.11.m11.2.2.cmml"></mi><mrow id="id11.11.m11.2.2.2.4" xref="id11.11.m11.2.2.2.3.cmml"><mtext id="id11.11.m11.1.1.1.1" xref="id11.11.m11.1.1.1.1a.cmml">3</mtext><mo rspace="0em" id="id11.11.m11.2.2.2.4.1" xref="id11.11.m11.2.2.2.3.cmml">,</mo><mo lspace="0em" id="id11.11.m11.2.2.2.2" xref="id11.11.m11.2.2.2.2.cmml">†</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="id11.11.m11.2b"><apply id="id11.11.m11.2.2.cmml" xref="id11.11.m11.2.2"><list id="id11.11.m11.2.2.2.3.cmml" xref="id11.11.m11.2.2.2.4"><ci id="id11.11.m11.1.1.1.1a.cmml" xref="id11.11.m11.1.1.1.1"><mtext mathsize="70%" id="id11.11.m11.1.1.1.1.cmml" xref="id11.11.m11.1.1.1.1">3</mtext></ci><ci id="id11.11.m11.2.2.2.2.cmml" xref="id11.11.m11.2.2.2.2">†</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.11.m11.2c">{}^{\text{3},\dagger}</annotation></semantics></math>
</span><span class="ltx_author_notes"><math id="id12.12.m1.1" class="ltx_Math" alttext="{}^{\text{1}}" display="inline"><semantics id="id12.12.m1.1a"><msup id="id12.12.m1.1.1" xref="id12.12.m1.1.1.cmml"><mi id="id12.12.m1.1.1a" xref="id12.12.m1.1.1.cmml"></mi><mtext id="id12.12.m1.1.1.1" xref="id12.12.m1.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="id12.12.m1.1b"><apply id="id12.12.m1.1.1.cmml" xref="id12.12.m1.1.1"><ci id="id12.12.m1.1.1.1a.cmml" xref="id12.12.m1.1.1.1"><mtext mathsize="70%" id="id12.12.m1.1.1.1.cmml" xref="id12.12.m1.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.12.m1.1c">{}^{\text{1}}</annotation></semantics></math> Shanliang Yao, Runwei Guan, Xiaoyu Huang, Zhuoxiao Li, Xiangyu Sha and Hyungjoon Seo are with Faculty of Science and Engineering, University of Liverpool, Liverpool, UK. (email: {shanliang.yao, runwei.guan, x.huang42, zhuoxiao.li, sgxsha2, hyungjoon.seo}@liverpool.ac.uk).<math id="id13.13.m1.1" class="ltx_Math" alttext="{}^{\text{2}}" display="inline"><semantics id="id13.13.m1.1a"><msup id="id13.13.m1.1.1" xref="id13.13.m1.1.1.cmml"><mi id="id13.13.m1.1.1a" xref="id13.13.m1.1.1.cmml"></mi><mtext id="id13.13.m1.1.1.1" xref="id13.13.m1.1.1.1a.cmml">2</mtext></msup><annotation-xml encoding="MathML-Content" id="id13.13.m1.1b"><apply id="id13.13.m1.1.1.cmml" xref="id13.13.m1.1.1"><ci id="id13.13.m1.1.1.1a.cmml" xref="id13.13.m1.1.1.1"><mtext mathsize="70%" id="id13.13.m1.1.1.1.cmml" xref="id13.13.m1.1.1.1">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id13.13.m1.1c">{}^{\text{2}}</annotation></semantics></math> Yong Yue, Eng Gee Lim, Ka Lok Man and Xiaohui Zhu are with School of Advanced Technology, Xi’an Jiaotong-Liverpool University, Suzhou, China. (email: {yong.yue, enggee.lim, ka.man, xiaohui.zhu}@xjtlu.edu.cn).<math id="id14.14.m1.1" class="ltx_Math" alttext="{}^{\text{3}}" display="inline"><semantics id="id14.14.m1.1a"><msup id="id14.14.m1.1.1" xref="id14.14.m1.1.1.cmml"><mi id="id14.14.m1.1.1a" xref="id14.14.m1.1.1.cmml"></mi><mtext id="id14.14.m1.1.1.1" xref="id14.14.m1.1.1.1a.cmml">3</mtext></msup><annotation-xml encoding="MathML-Content" id="id14.14.m1.1b"><apply id="id14.14.m1.1.1.cmml" xref="id14.14.m1.1.1"><ci id="id14.14.m1.1.1.1a.cmml" xref="id14.14.m1.1.1.1"><mtext mathsize="70%" id="id14.14.m1.1.1.1.cmml" xref="id14.14.m1.1.1.1">3</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id14.14.m1.1c">{}^{\text{3}}</annotation></semantics></math> Yutao Yue is with Institute of Deep Perception Technology, JITRI, Wuxi, China; XJTLU-JITRI Academy of Industrial Technology, Xi’an Jiaotong-Liverpool University, Suzhou, China; and Department of Mathematical Sciences, University of Liverpool, Liverpool, UK. (email: yueyutao@idpt.org).<sup id="id27.16.id1" class="ltx_sup"><span id="id27.16.id1.1" class="ltx_text ltx_font_italic">†</span></sup> Corresponding author: xiaohui.zhu@xjtlu.edu.cn, yueyutao@idpt.org</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id28.id1" class="ltx_p">Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years, enabling vehicles to accurately detect and interpret surrounding environment for safe and efficient navigation. To achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the perception system.
Among these fused sensors, radars and cameras enable a complementary and cost-effective perception of the surrounding environment regardless of lighting and weather conditions.
This review aims to provide a comprehensive guideline for radar-camera fusion, particularly concentrating on perception tasks related to object detection and semantic segmentation.
Based on the principles of the radar and camera sensors, we delve into the data processing process and representations, followed by an in-depth analysis and summary of radar-camera fusion datasets.
In the review of methodologies in radar-camera fusion, we address interrogative questions, including “why to fuse”, “what to fuse”, “where to fuse”, “when to fuse”, and “how to fuse”, subsequently discussing various challenges and potential research directions within this domain. To ease the retrieval and comparison of datasets and fusion methods, we also provide an interactive website: <a target="_blank" href="https://radar-camera-fusion.github.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://radar-camera-fusion.github.io</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Autonomous driving, radar-camera fusion, object detection, semantic segmentation.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Autonomous driving has excellent potential in mitigating traffic congestion and improving driving safety. Perception, akin to eyes in autonomous driving, constitutes the foundation for successive functions, such as motion prediction, path planning and maneuver control <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. To achieve optimal accuracy and robustness of the perception system, various sensors are integrated into autonomous vehicles, allowing for the utilization of their complementary and redundant characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. However, which sensors to choose and how to fuse the data between different sensors have emerged as challenging issues requiring further exploration.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2304.10410/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="261" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Common scenario of object detection and semantic segmentation in autonomous driving. Boxes and masks represent the results of detection and segmentation, respectively. Dots indicate the location of each radar point, and the darker the dot, the closer the distance to the ego-vehicle. Image is generated from the nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> dataset.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Given the rich semantic information that can be perceived, cameras are widely utilized in autonomous driving for object detection, segmentation and tracking. LiDARs calculate the distance to surrounding objects by measuring the time difference of the laser beam from emission to reception via the objects. The denser the laser layers emitted by a LiDAR sensor, the clearer an object’s three-dimensional (3D) contour.
These complementary features provided by cameras and LiDARs have made LiDAR-camera sensor fusion a hot topic in recent years, and achieved high accuracy in two-dimensional (2D) and 3D object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Despite their strengths, both LiDARs and cameras suffer from the same defect of being sensitive to adverse weather conditions (e.g., rain, fog, snow) that can significantly diminish their field of view and object recognition capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
Moreover, the high cost of LiDAR products has brought certain difficulties in promoting their widespread adoption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Compared to LiDARs and cameras, radars exhibit superior effectiveness under challenging lighting and weather conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Radars can also deliver accurate velocity estimation for all detected objects depending on the Doppler effect without requiring any temporal information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. With these characteristics, radars are widely used in Advanced Driving Assistance Systems (ADAS) applications, including collision avoidance, Adaptive Cruise Control (ACC), Lane Change Assist (LCA) and Automatic Emergency Braking (AEB).
As depicted in Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the integration of radar and camera data in sensor fusion enables a comprehensive perception of the surrounding environment in terms of outlines, colors, textures, ranges, and velocities. Moreover, the fusion system can operate continuously throughout the day regardless of weather and lighting conditions.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Although radar sensors are popularly applied to vehicles, few studies focus on data fusion from radars and cameras. One reason for this is the limitations of radar output data, such as low resolution, sparse point clouds, uncertainty in elevation and clutter effects. Another reason is that up to now, the datasets containing both radar and camera data for autonomous driving applications are insufficient, making it challenging for researchers to conduct in-depth analysis.
Additionally, applying or adapting existing LiDAR-based algorithms to radar point clouds yields poor results due to inherent differences of point clouds between the LiDAR sensor and radar sensor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Radar point clouds are significantly sparser than their LiDAR counterparts, making it inefficient to extract objects’ geometry information using LiDAR-based algorithms.
Although Radar Cross Section (RCS) values in the radar sensor indicate the reflective intensity from the surface of an object, they are easily affected by numerous factors and cannot be used singularly to determine the classification of the target.
In addition, though aggregating multiple radar frames enhances the density of the point clouds, it also causes a delay to the whole system.
In summary, radar-camera fusion perception is significant in autonomous driving as well as challenging in implementation.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS1.5.1.1" class="ltx_text">I-A</span> </span><span id="S1.SS1.6.2" class="ltx_text ltx_font_italic">Related Surveys</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Most sensor fusion surveys focus on LiDAR-camera <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, or the broader field of multi-sensor fusion, including LiDAR, camera, radar and other sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Specifically, in multi-sensor fusion surveys, LiDARs and cameras are still the main research objectives. For example, Feng <span id="S1.SS1.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> conducted a comprehensive survey on deep multi-modal object detection and semantic segmentation for autonomous driving. However, this survey mainly concentrates on fusion methods based on LiDARs and cameras, and briefly mentions some studies combining camera images and radar data.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">To the best of our knowledge, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is the only survey that primarily focuses on radar-camera fusion for object detection in autonomous driving. However, it does not cover the radar-camera fusion dataset or the semantic segmentation task.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS2.5.1.1" class="ltx_text">I-B</span> </span><span id="S1.SS2.6.2" class="ltx_text ltx_font_italic">Contributions</span>
</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">With the limited focus on radar-camera fusion in existing surveys, it is challenging for researchers to gain an overview of this emerging research field. Our survey attempts to narrow this gap by providing a comprehensive review of radar-camera fusion in autonomous driving.
The contributions of our review are summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">To the best of our knowledge, this is the first survey focusing on two fundamental perception problems for radar-camera fusion, namely, object detection and semantic segmentation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present an up-to-date (2019 - 2023) overview of radar-camera fusion datasets and algorithms, and conduct in-depth research on “why to fuse”, “what to fuse”, “where to fuse”, “when to fuse”, and “how to fuse”.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We analyze the critical challenges and open questions in radar-camera fusion, and put forward potential research directions.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We provide an interactive and updated website for better retrieving and comparing the fusion datasets and methods.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section provides background information on radar-camera fusion in autonomous driving.
We first introduce the working principles, sensor characteristics and data representations of the radar and camera sensors. By comparing the characteristics of the two sensors, we aim to demonstrate the importance of radar-camera fusion.
Subsequently, as the perception module leverages data from specific sensors to understand the surroundings, we present basic concepts and highlight representative algorithms for two fundamental and crucial perception tasks: object detection and semantic segmentation.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Radar Sensors</span>
</h3>

<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS1.5.1.1" class="ltx_text">II-A</span>1 </span>Working Principles</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">Radar is the abbreviation of Radio Detection And Ranging, which calculates the range and velocity of the target by transmitting radio waves and receiving the reflected waves from the target <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
In autonomous driving applications, radar typically refers to the MilliMeter-Wave (MMW) radar that works in the millimeter wave band with a wavelength of 1-10mm and frequency of 76-81GHz.
Specifically, the radar equipped in the forward direction, as well as the four corner directions, is usually a Multiple-Input Multiple-Output (MIMO) radar, while the radar on the roof is typically a mechanical rotating radar.
A MIMO radar utilizes multiple antennas and transmitters to simultaneously transmit and receive multiple signals with different frequencies. In contrast, a mechanical rotating radar operates with a single antenna that physically rotates to emit radar signals in different directions.
With multiple antennas and beamforming capabilities, a MIMO radar achieves higher spatial resolution and interference reduction compared to mechanical rotating radar. While a mechanical rotating radar provides better coverage and is simpler in implementation.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p">Based on the Time of Flight (TOF) principle, the radar sensor calculates the range from the object by the time difference between the transmitted and reflected signals. Based on the Doppler principle, when there is a relative movement between the emitted electromagnetic wave and the detected target, the frequency of the returned wave differs from that of the emitted wave. Thus, the target’s relative velocity to the radar can be measured using this frequency difference.
Leveraging the array signal processing method, the azimuth angle is calculated using the signal’s phase difference between parallel antennas. Since the receivers of traditional 3D (range, Doppler velocity and azimuth angle) radar sensors are only lined up in a 2D direction, targets are only detected in 2D horizontal coordinates without vertical height information. Recently, with advancements in radar technologies, 4D (range, Doppler velocity, azimuth angle and elevation angle) radar sensors have been developed with antennas arranged horizontally and vertically, enabling the measurement of elevation information. In addition, 4D is often represented as x, y, z coordinates and Doppler velocity.
</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS2.5.1.1" class="ltx_text">II-A</span>2 </span>Sensor Characteristics</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">In addition to the ability to measure range, Doppler velocity and azimuth angle, electromagnetic waves in the millimeter wave band have low atmospheric attenuation and better penetration of rain, smoke and dust <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. These characteristics make the radar sensor work all day regardless of severe weather conditions.
However, radar sensors still have certain limitations. They exhibit low angular resolution and cannot distinguish between closely located objects. The point clouds generated by radars are sparsely distributed, with only a few points on a pedestrian and a dozen points on a car. These points cannot adequately outline an object’s contours, making it challenging to extract the geometric information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
Doppler radar measurements have a limitation in that they only provide the radial component of velocity. The lack of tangential velocity makes it difficult to estimate the accurate velocity of an object in dynamic scenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
Besides, data produced by radars are noisy, which may arise from diverse sources such as multipath interference, electrical interference and equipment imperfections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Such noise reduces the precision and reliability of radar data, while also increasing the probability of false detections.
Furthermore, radars are weak in the perception of stationary obstacles. Moving targets can be distinguished from the surrounding scene in one dimension of range and velocity. However, radars are highly sensitive to metal, often resulting in strong reflections from stationary objects such as manhole covers on the ground. Strong reflections from stationary objects are not filtered, resulting in a lack of detecting stationary obstacles.</p>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS3.5.1.1" class="ltx_text">II-A</span>3 </span>Data Representations</h4>

<figure id="S2.F2" class="ltx_figure"><img src="/html/2304.10410/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="192" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Generation process of radar data representations.</figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2304.10410/assets/x3.png" id="S2.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="397" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Radar data representations. (a) ADC signal in the format of Simple-Chirp-Antenna tensor. (b) Radar tensor represented by a 3D Range-Azimuth-Doppler tensor. Image is generated from the CARRADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> dataset. (c) Point cloud projected on a 2D image plane. Image is generated from the View-of-Delft <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> dataset. (d) Micro-Doppler signature showing a pedestrian walking. Image is generated from the Open Radar Datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</figcaption>
</figure>
<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">As depicted in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-A3 Data Representations ‣ II-A Radar Sensors ‣ II Background ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the raw output of a radar sensor is the <span id="S2.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_bold">ADC signal</span>, which refers to the output signal of an Analog-to-Digital Converter (ADC).
At this stage, the signal lacks spatial coherence between the values as all the information exists in the time domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Besides, the signal is represented in complex value which contains real part and imaginary part <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.
To represent the ADC signal in a more structured form, it is usually transformed into a 3D Sample-Chirp-Antenna (SCA) tensor, as illustrated in Fig. <a href="#S2.F3" title="Figure 3 ‣ II-A3 Data Representations ‣ II-A Radar Sensors ‣ II Background ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a).
Some researchers apply 3D Fast Fourier Transform (FFT) along the sample, chirp and antenna dimensions to get an image-like representation named <span id="S2.SS1.SSS3.p1.1.2" class="ltx_text ltx_font_bold">radar tensor</span> (Fig. <a href="#S2.F3" title="Figure 3 ‣ II-A3 Data Representations ‣ II-A Radar Sensors ‣ II Background ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b)), describing the spatial pattern of the received echo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
At this stage, the non-coherent combination (e.g., norm calculation) converts ADC signals composed of complex values to radar tensors that consists of real values.
With these three features in Range-Azimuth-Doppler (RAD) coordinates, two forms of radar tensors are formed: one is in 2D including the Range-Azimuth (RA) tensor, Range-Doppler (RD) tensor and Azimuth-Doppler (AD) tensor; the other is the whole 3D RAD tensor, with each side consisting of a 2D tensor.
Furthermore, peak detection is carried out on the radar tensor to filter out clutter, resulting in a sparse point-like representation called the <span id="S2.SS1.SSS3.p1.1.3" class="ltx_text ltx_font_bold">point cloud</span>, as depicted in Fig. <a href="#S2.F3" title="Figure 3 ‣ II-A3 Data Representations ‣ II-A Radar Sensors ‣ II Background ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a). The point cloud provides a spatially intuitive representation better suited for visualization and interpretation, yet it can not accurately indicate the outline information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Constant False Alarm Rate (CFAR) is the most commonly used method for peak detection, which enables the radar system to automatically adjust its sensitivity level to changes in the strength of external interference, thereby maintaining a steady false alarm rate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.
By applying grid mapping methods to point clouds accumulated over a given period, a <span id="S2.SS1.SSS3.p1.1.4" class="ltx_text ltx_font_bold">grid map</span> for identifying static objects is generated. There are two main grid maps: one is the occupancy-based grid map <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, which represents the obstacles and free-space derived from the radar data; the other is the amplitude-based grid map <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, which displays the RCS values for each cell. However, it is essential to note that the sparsity of the point clouds still influences the accuracy of detection and segmentation performed on the grid map.
In addition, some researchers perform Time-Frequency transform after the Range-FFT to obtain the <span id="S2.SS1.SSS3.p1.1.5" class="ltx_text ltx_font_bold">micro-Doppler signature</span>, which is utilized to recognize objects with tiny motion features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. As exemplified in Fig. <a href="#S2.F3" title="Figure 3 ‣ II-A3 Data Representations ‣ II-A Radar Sensors ‣ II Background ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(d), the Doppler frequency of a pedestrian walking shows a periodic variation. This representation enables not only the distinction of different object categories (e.g., pedestrians, bicycles and vehicles), but also the recognition of complex object behaviors, such as gait and gesture recognition.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Camera Sensors</span>
</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.5.1.1" class="ltx_text">II-B</span>1 </span>Working Principles</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">The camera sensor usually consists of a lens, an image sensor, an Image Signal Processor (ISP) and an Input/Output (I/O) interface <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. The lens collects the light reflected from the target and converges it to the image sensor. Then, the image sensor converts light waves into electrical signals and converts electrical signals to digital values via an on-chip ADC. After that, the ISP performs post-processing (e.g., noise reduction) and converts the digital values into a format of RGB data for images or videos. Finally, the image data is transferred and displayed via the I/O interface.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.5.1.1" class="ltx_text">II-B</span>2 </span>Sensor Characteristics</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Cameras capture the rich appearance features of the objects, including colors, shapes and textures. After learning from neural networks, these features can be utilized to identify obstacles, including vehicles, pedestrians, bicycles and various traffic lights.
However, cameras are passive sensors, indicating that the formation of an image requires incident light intake. When the light intake is adversely affected, such as insufficient light at night, extreme weather, water droplets or dust sticking to the lens, the imaging results will be unclear, and object detection performance may be significantly affected <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. Besides, in autonomous driving, it is crucial to identify the distances of obstacles ahead. However, a target in three dimensions in the world coordinate system becomes a 2D target in the image coordinate system after being imaged by the camera sensor, resulting in a loss of distance information.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS3.5.1.1" class="ltx_text">II-B</span>3 </span>Data Representations</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p"><span id="S2.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_bold">Raw data</span> representation is the uncompressed and unprocessed format captured by the camera sensor. It contains all the radiance information that hits each pixel on the camera sensor during image exposure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
After post-processing, a data representation named <span id="S2.SS2.SSS3.p1.1.2" class="ltx_text ltx_font_bold">RGB image</span> is generated, which illustrates an image as a grid of pixels, with each pixel containing a value for each of the red, green and blue color channels.
In addition, some modern cameras applied in autonomous driving are able to generate specific data representations. For instance, a depth camera produces the <span id="S2.SS2.SSS3.p1.1.3" class="ltx_text ltx_font_bold">depth map</span> representation, providing information about the distance to each pixel in the scene <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.
Relying on flood-light flash laser sources, infrared cameras output the <span id="S2.SS2.SSS3.p1.1.4" class="ltx_text ltx_font_bold">infrared image</span> representation, which is able to render perception results in adverse weather and low-light conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
Event cameras are bio-inspired vision sensors that generate an <span id="S2.SS2.SSS3.p1.1.5" class="ltx_text ltx_font_bold">event image</span> representation pertaining to pixel-level changes in brightness. With sub-millisecond latency, high-dynamic range, and robustness to motion blur, event cameras present considerable potential for real-time detection and tracking of objects in time-critical scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Comparison of Radar and Camera Sensors</span>
</h3>

<figure id="S2.F4" class="ltx_figure"><img src="/html/2304.10410/assets/x4.png" id="S2.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="301" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of radar and camera characteristics. In these charts, each characteristic is plotted along one of the line segments radiating from the central point, with closer proximity to the vertex representing higher quality for that characteristic. (a) Fusion characteristics of radar and camera. (b) Radar characteristics. (c) Camera characteristics. </figcaption>
</figure>
<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.2" class="ltx_p">Through our extensive review, a clear and concise figure is designed to compare the characteristics of the two sensors, shown in Fig. <a href="#S2.F4" title="Figure 4 ‣ II-C Comparison of Radar and Camera Sensors ‣ II Background ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Specifically, the radar sensor is an active sensor and measures various information, including ranges, velocities and azimuth angles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.
Nowadays, radars equipped in driver-assistance systems can detect up to 300 meters, with a 140<sup id="S2.SS3.p1.2.1" class="ltx_sup"><span id="S2.SS3.p1.2.1.1" class="ltx_text ltx_font_italic">∘</span></sup> horizontal field of view and a less than 1<sup id="S2.SS3.p1.2.2" class="ltx_sup"><span id="S2.SS3.p1.2.2.1" class="ltx_text ltx_font_italic">∘</span></sup> angular resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>.
In addition, the radar sensor is robust to darkness and extreme weather conditions, allowing it to work throughout the day.
The camera sensor is a passive sensor that provides colors, textures and shapes of objects. With a resolution of up to 2K, the camera sensor performs much better in classification than the radar sensor.
As far as system cost goes, both radars and cameras are relatively cost-effective and are mass-applied in vehicles.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">To sum up, both the radar and camera have their strengths and weaknesses, and they cannot be substituted for each other. The most effective way to ensure adequate information acquisition is mutual integration. Based on their respective characteristics, complementary advantages can improve scenario understanding performance. In addition, when one of the sensors fails, the remaining one can continue working, thus increasing the reliability of the autonomous driving system. Hence, the fusion of radar and camera sensors is critical for perception accuracy and robustness in autonomous driving.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.5.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.6.2" class="ltx_text ltx_font_italic">Perception Tasks</span>
</h3>

<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS1.5.1.1" class="ltx_text">II-D</span>1 </span>Object Detection</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.p1.12" class="ltx_p">The object detection task involves identifying a particular object in a camera image or a radar scan, locating its position and determining its category. Generally speaking, researchers use a rectangular or cubic bounding box to encompass the object. As there is no depth channel in 2D object detection, the rectangular bounding box is expressed as <math id="S2.SS4.SSS1.p1.1.m1.5" class="ltx_Math" alttext="(x,y,h,w,c)" display="inline"><semantics id="S2.SS4.SSS1.p1.1.m1.5a"><mrow id="S2.SS4.SSS1.p1.1.m1.5.6.2" xref="S2.SS4.SSS1.p1.1.m1.5.6.1.cmml"><mo stretchy="false" id="S2.SS4.SSS1.p1.1.m1.5.6.2.1" xref="S2.SS4.SSS1.p1.1.m1.5.6.1.cmml">(</mo><mi id="S2.SS4.SSS1.p1.1.m1.1.1" xref="S2.SS4.SSS1.p1.1.m1.1.1.cmml">x</mi><mo id="S2.SS4.SSS1.p1.1.m1.5.6.2.2" xref="S2.SS4.SSS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.1.m1.2.2" xref="S2.SS4.SSS1.p1.1.m1.2.2.cmml">y</mi><mo id="S2.SS4.SSS1.p1.1.m1.5.6.2.3" xref="S2.SS4.SSS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.1.m1.3.3" xref="S2.SS4.SSS1.p1.1.m1.3.3.cmml">h</mi><mo id="S2.SS4.SSS1.p1.1.m1.5.6.2.4" xref="S2.SS4.SSS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.1.m1.4.4" xref="S2.SS4.SSS1.p1.1.m1.4.4.cmml">w</mi><mo id="S2.SS4.SSS1.p1.1.m1.5.6.2.5" xref="S2.SS4.SSS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.1.m1.5.5" xref="S2.SS4.SSS1.p1.1.m1.5.5.cmml">c</mi><mo stretchy="false" id="S2.SS4.SSS1.p1.1.m1.5.6.2.6" xref="S2.SS4.SSS1.p1.1.m1.5.6.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.1.m1.5b"><vector id="S2.SS4.SSS1.p1.1.m1.5.6.1.cmml" xref="S2.SS4.SSS1.p1.1.m1.5.6.2"><ci id="S2.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1">𝑥</ci><ci id="S2.SS4.SSS1.p1.1.m1.2.2.cmml" xref="S2.SS4.SSS1.p1.1.m1.2.2">𝑦</ci><ci id="S2.SS4.SSS1.p1.1.m1.3.3.cmml" xref="S2.SS4.SSS1.p1.1.m1.3.3">ℎ</ci><ci id="S2.SS4.SSS1.p1.1.m1.4.4.cmml" xref="S2.SS4.SSS1.p1.1.m1.4.4">𝑤</ci><ci id="S2.SS4.SSS1.p1.1.m1.5.5.cmml" xref="S2.SS4.SSS1.p1.1.m1.5.5">𝑐</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.1.m1.5c">(x,y,h,w,c)</annotation></semantics></math>, where <math id="S2.SS4.SSS1.p1.2.m2.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S2.SS4.SSS1.p1.2.m2.2a"><mrow id="S2.SS4.SSS1.p1.2.m2.2.3.2" xref="S2.SS4.SSS1.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S2.SS4.SSS1.p1.2.m2.2.3.2.1" xref="S2.SS4.SSS1.p1.2.m2.2.3.1.cmml">(</mo><mi id="S2.SS4.SSS1.p1.2.m2.1.1" xref="S2.SS4.SSS1.p1.2.m2.1.1.cmml">x</mi><mo id="S2.SS4.SSS1.p1.2.m2.2.3.2.2" xref="S2.SS4.SSS1.p1.2.m2.2.3.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.2.m2.2.2" xref="S2.SS4.SSS1.p1.2.m2.2.2.cmml">y</mi><mo stretchy="false" id="S2.SS4.SSS1.p1.2.m2.2.3.2.3" xref="S2.SS4.SSS1.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.2.m2.2b"><interval closure="open" id="S2.SS4.SSS1.p1.2.m2.2.3.1.cmml" xref="S2.SS4.SSS1.p1.2.m2.2.3.2"><ci id="S2.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS1.p1.2.m2.1.1">𝑥</ci><ci id="S2.SS4.SSS1.p1.2.m2.2.2.cmml" xref="S2.SS4.SSS1.p1.2.m2.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.2.m2.2c">(x,y)</annotation></semantics></math> is the bounding box center, <math id="S2.SS4.SSS1.p1.3.m3.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S2.SS4.SSS1.p1.3.m3.1a"><mi id="S2.SS4.SSS1.p1.3.m3.1.1" xref="S2.SS4.SSS1.p1.3.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.3.m3.1b"><ci id="S2.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.3.m3.1c">h</annotation></semantics></math> and <math id="S2.SS4.SSS1.p1.4.m4.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S2.SS4.SSS1.p1.4.m4.1a"><mi id="S2.SS4.SSS1.p1.4.m4.1.1" xref="S2.SS4.SSS1.p1.4.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.4.m4.1b"><ci id="S2.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS4.SSS1.p1.4.m4.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.4.m4.1c">w</annotation></semantics></math> are the height and width of the bounding box, and <math id="S2.SS4.SSS1.p1.5.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.SS4.SSS1.p1.5.m5.1a"><mi id="S2.SS4.SSS1.p1.5.m5.1.1" xref="S2.SS4.SSS1.p1.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.5.m5.1b"><ci id="S2.SS4.SSS1.p1.5.m5.1.1.cmml" xref="S2.SS4.SSS1.p1.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.5.m5.1c">c</annotation></semantics></math> is the class of the object. While the cubic bounding box for 3D object detection is described as <math id="S2.SS4.SSS1.p1.6.m6.8" class="ltx_Math" alttext="(x,y,z,h,w,l,\theta,c)" display="inline"><semantics id="S2.SS4.SSS1.p1.6.m6.8a"><mrow id="S2.SS4.SSS1.p1.6.m6.8.9.2" xref="S2.SS4.SSS1.p1.6.m6.8.9.1.cmml"><mo stretchy="false" id="S2.SS4.SSS1.p1.6.m6.8.9.2.1" xref="S2.SS4.SSS1.p1.6.m6.8.9.1.cmml">(</mo><mi id="S2.SS4.SSS1.p1.6.m6.1.1" xref="S2.SS4.SSS1.p1.6.m6.1.1.cmml">x</mi><mo id="S2.SS4.SSS1.p1.6.m6.8.9.2.2" xref="S2.SS4.SSS1.p1.6.m6.8.9.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.6.m6.2.2" xref="S2.SS4.SSS1.p1.6.m6.2.2.cmml">y</mi><mo id="S2.SS4.SSS1.p1.6.m6.8.9.2.3" xref="S2.SS4.SSS1.p1.6.m6.8.9.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.6.m6.3.3" xref="S2.SS4.SSS1.p1.6.m6.3.3.cmml">z</mi><mo id="S2.SS4.SSS1.p1.6.m6.8.9.2.4" xref="S2.SS4.SSS1.p1.6.m6.8.9.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.6.m6.4.4" xref="S2.SS4.SSS1.p1.6.m6.4.4.cmml">h</mi><mo id="S2.SS4.SSS1.p1.6.m6.8.9.2.5" xref="S2.SS4.SSS1.p1.6.m6.8.9.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.6.m6.5.5" xref="S2.SS4.SSS1.p1.6.m6.5.5.cmml">w</mi><mo id="S2.SS4.SSS1.p1.6.m6.8.9.2.6" xref="S2.SS4.SSS1.p1.6.m6.8.9.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.6.m6.6.6" xref="S2.SS4.SSS1.p1.6.m6.6.6.cmml">l</mi><mo id="S2.SS4.SSS1.p1.6.m6.8.9.2.7" xref="S2.SS4.SSS1.p1.6.m6.8.9.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.6.m6.7.7" xref="S2.SS4.SSS1.p1.6.m6.7.7.cmml">θ</mi><mo id="S2.SS4.SSS1.p1.6.m6.8.9.2.8" xref="S2.SS4.SSS1.p1.6.m6.8.9.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.6.m6.8.8" xref="S2.SS4.SSS1.p1.6.m6.8.8.cmml">c</mi><mo stretchy="false" id="S2.SS4.SSS1.p1.6.m6.8.9.2.9" xref="S2.SS4.SSS1.p1.6.m6.8.9.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.6.m6.8b"><vector id="S2.SS4.SSS1.p1.6.m6.8.9.1.cmml" xref="S2.SS4.SSS1.p1.6.m6.8.9.2"><ci id="S2.SS4.SSS1.p1.6.m6.1.1.cmml" xref="S2.SS4.SSS1.p1.6.m6.1.1">𝑥</ci><ci id="S2.SS4.SSS1.p1.6.m6.2.2.cmml" xref="S2.SS4.SSS1.p1.6.m6.2.2">𝑦</ci><ci id="S2.SS4.SSS1.p1.6.m6.3.3.cmml" xref="S2.SS4.SSS1.p1.6.m6.3.3">𝑧</ci><ci id="S2.SS4.SSS1.p1.6.m6.4.4.cmml" xref="S2.SS4.SSS1.p1.6.m6.4.4">ℎ</ci><ci id="S2.SS4.SSS1.p1.6.m6.5.5.cmml" xref="S2.SS4.SSS1.p1.6.m6.5.5">𝑤</ci><ci id="S2.SS4.SSS1.p1.6.m6.6.6.cmml" xref="S2.SS4.SSS1.p1.6.m6.6.6">𝑙</ci><ci id="S2.SS4.SSS1.p1.6.m6.7.7.cmml" xref="S2.SS4.SSS1.p1.6.m6.7.7">𝜃</ci><ci id="S2.SS4.SSS1.p1.6.m6.8.8.cmml" xref="S2.SS4.SSS1.p1.6.m6.8.8">𝑐</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.6.m6.8c">(x,y,z,h,w,l,\theta,c)</annotation></semantics></math>, where <math id="S2.SS4.SSS1.p1.7.m7.3" class="ltx_Math" alttext="(x,y,z)" display="inline"><semantics id="S2.SS4.SSS1.p1.7.m7.3a"><mrow id="S2.SS4.SSS1.p1.7.m7.3.4.2" xref="S2.SS4.SSS1.p1.7.m7.3.4.1.cmml"><mo stretchy="false" id="S2.SS4.SSS1.p1.7.m7.3.4.2.1" xref="S2.SS4.SSS1.p1.7.m7.3.4.1.cmml">(</mo><mi id="S2.SS4.SSS1.p1.7.m7.1.1" xref="S2.SS4.SSS1.p1.7.m7.1.1.cmml">x</mi><mo id="S2.SS4.SSS1.p1.7.m7.3.4.2.2" xref="S2.SS4.SSS1.p1.7.m7.3.4.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.7.m7.2.2" xref="S2.SS4.SSS1.p1.7.m7.2.2.cmml">y</mi><mo id="S2.SS4.SSS1.p1.7.m7.3.4.2.3" xref="S2.SS4.SSS1.p1.7.m7.3.4.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.7.m7.3.3" xref="S2.SS4.SSS1.p1.7.m7.3.3.cmml">z</mi><mo stretchy="false" id="S2.SS4.SSS1.p1.7.m7.3.4.2.4" xref="S2.SS4.SSS1.p1.7.m7.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.7.m7.3b"><vector id="S2.SS4.SSS1.p1.7.m7.3.4.1.cmml" xref="S2.SS4.SSS1.p1.7.m7.3.4.2"><ci id="S2.SS4.SSS1.p1.7.m7.1.1.cmml" xref="S2.SS4.SSS1.p1.7.m7.1.1">𝑥</ci><ci id="S2.SS4.SSS1.p1.7.m7.2.2.cmml" xref="S2.SS4.SSS1.p1.7.m7.2.2">𝑦</ci><ci id="S2.SS4.SSS1.p1.7.m7.3.3.cmml" xref="S2.SS4.SSS1.p1.7.m7.3.3">𝑧</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.7.m7.3c">(x,y,z)</annotation></semantics></math> represents the center of the 3D bounding box, <math id="S2.SS4.SSS1.p1.8.m8.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S2.SS4.SSS1.p1.8.m8.1a"><mi id="S2.SS4.SSS1.p1.8.m8.1.1" xref="S2.SS4.SSS1.p1.8.m8.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.8.m8.1b"><ci id="S2.SS4.SSS1.p1.8.m8.1.1.cmml" xref="S2.SS4.SSS1.p1.8.m8.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.8.m8.1c">h</annotation></semantics></math>, <math id="S2.SS4.SSS1.p1.9.m9.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S2.SS4.SSS1.p1.9.m9.1a"><mi id="S2.SS4.SSS1.p1.9.m9.1.1" xref="S2.SS4.SSS1.p1.9.m9.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.9.m9.1b"><ci id="S2.SS4.SSS1.p1.9.m9.1.1.cmml" xref="S2.SS4.SSS1.p1.9.m9.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.9.m9.1c">w</annotation></semantics></math> and <math id="S2.SS4.SSS1.p1.10.m10.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S2.SS4.SSS1.p1.10.m10.1a"><mi id="S2.SS4.SSS1.p1.10.m10.1.1" xref="S2.SS4.SSS1.p1.10.m10.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.10.m10.1b"><ci id="S2.SS4.SSS1.p1.10.m10.1.1.cmml" xref="S2.SS4.SSS1.p1.10.m10.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.10.m10.1c">l</annotation></semantics></math> are the height, width and length of the bounding box, <math id="S2.SS4.SSS1.p1.11.m11.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS4.SSS1.p1.11.m11.1a"><mi id="S2.SS4.SSS1.p1.11.m11.1.1" xref="S2.SS4.SSS1.p1.11.m11.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.11.m11.1b"><ci id="S2.SS4.SSS1.p1.11.m11.1.1.cmml" xref="S2.SS4.SSS1.p1.11.m11.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.11.m11.1c">\theta</annotation></semantics></math> is the object’s orientation, and <math id="S2.SS4.SSS1.p1.12.m12.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.SS4.SSS1.p1.12.m12.1a"><mi id="S2.SS4.SSS1.p1.12.m12.1.1" xref="S2.SS4.SSS1.p1.12.m12.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.12.m12.1b"><ci id="S2.SS4.SSS1.p1.12.m12.1.1.cmml" xref="S2.SS4.SSS1.p1.12.m12.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.12.m12.1c">c</annotation></semantics></math> is its class. Bird’s Eye View (BEV) object detection is a specialized form of 3D object detection focusing on detecting objects from a top-down perspective. In this approach, height information is typically discarded, and objects are represented as 2D bounding boxes on the ground plane.</p>
</div>
<section id="S2.SS4.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Camera-based Object Detection</h5>

<div id="S2.SS4.SSS1.Px1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.Px1.p1.1" class="ltx_p">In autonomous driving, camera-based object detection approaches have been widely used in detecting vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, pedestrians <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, traffic lights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, and traffic signs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
According to different training steps, CNN-based object detection algorithms can be classified into two-stage and one-stage. Two-stage detection algorithms (e.g., R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, SPPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, Fast R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, FPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>) segregate the detection problem into two stages: the first step is to generate region proposals, and the second step is to refine the position and predict the classification of each object. Experimental results of these algorithms are high in precision and recall, but relatively slow in time.
Without the region proposal generation phase, one-stage detection algorithms simultaneously predict the bounding boxes and the probability of classes within these boxes. Thus, one-stage detection algorithms are commonly faster than two-stage detection algorithms, but lower in accuracy. Some highly representative one-stage object detectors include the YOLO series <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> and RetinaNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>.</p>
</div>
<div id="S2.SS4.SSS1.Px1.p2" class="ltx_para">
<p id="S2.SS4.SSS1.Px1.p2.1" class="ltx_p">Exploiting the self-attention mechanism that enables the model to model the contextual features and their correlation, transformer-based methods have emerged as a recent breakthrough compared to CNN-based detectors. Some representative pure transformer detectors include DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, Deformable DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>, RT-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>, WB-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, Swin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> and YOLOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>.
Additionally, plenty of studies have endeavored to accelerate the conventional transformer block by the combination of convolution and self-attention, aggregating the advantages of both CNN and transformer, as exemplified by Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>, EdgeViTs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>, MobileViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>, ViTAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> and Visformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS4.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Radar-based Object Detection</h5>

<div id="S2.SS4.SSS1.Px2.p1" class="ltx_para">
<p id="S2.SS4.SSS1.Px2.p1.1" class="ltx_p">Radar-based object detection approaches have been widely used in detecting vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, pedestrians <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>, <a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> and static objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>.
As radar tensors are image-like representations, researchers generally utilize image-based networks (e.g., ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>, Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, YOLOv4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>) to perform object detection on 2D RA tensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, 2D RD tensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> and 3D RAD tensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>, <a href="#bib.bib96" title="" class="ltx_ref">96</a>, <a href="#bib.bib97" title="" class="ltx_ref">97</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>. Unlike images, radar tensors lack a physical interpretation, thereby presenting difficulties in translating the learned features from image-based algorithms to radar data. Furthermore, applying algorithms to radar tensors in real-time applications poses challenges due to the high-dimensional nature of radar tensors and the presence of noise, interference and clutter.</p>
</div>
<div id="S2.SS4.SSS1.Px2.p2" class="ltx_para">
<p id="S2.SS4.SSS1.Px2.p2.1" class="ltx_p">For radar data in the format of point clouds, various types of point-based networks are utilized to detect objects.
Point-wise methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> directly operate on the raw point clouds and leverage LiDAR-based algorithms, such as PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>, PointNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> and Frustum PointNets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>, to classify the points into distinct object classes.
Grid-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> map the 3D point clouds into grid-like structures, such as 2D image planes or 3D voxel grids. Subsequently, object detection algorithms (e.g., YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>) are applied to the grid representation to identify objects. The grid-based approaches demonstrate efficiency in handling large datasets and are frequently employed in real-time applications.
Graph-based methods (e.g., RadarGNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, Radar-PointGNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>) in radar point cloud object detection employ the Graph Neural Network (GNN), where the points serve as nodes, and the relationships between the points are modeled as edges in the graph. Leveraging graph structures and algorithms, these methods effectively capture the spatial relationships and contextual information among the points, leading to improved detection performance compared to traditional point-wise methods. However, the construction and feature extraction of graphs are complex and computationally intensive, especially when handling large-scale point clouds.</p>
</div>
</section>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS2.5.1.1" class="ltx_text">II-D</span>2 </span>Semantic Segmentation</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.p1.3" class="ltx_p">Semantic segmentation involves clustering the basic components of input data into different semantically relevant regions.
Essentially, it refers to assigning selected labels from a pre-defined set <math id="S2.SS4.SSS2.p1.1.m1.4" class="ltx_Math" alttext="Y=\{y_{1},y_{2},...,y_{k}\}" display="inline"><semantics id="S2.SS4.SSS2.p1.1.m1.4a"><mrow id="S2.SS4.SSS2.p1.1.m1.4.4" xref="S2.SS4.SSS2.p1.1.m1.4.4.cmml"><mi id="S2.SS4.SSS2.p1.1.m1.4.4.5" xref="S2.SS4.SSS2.p1.1.m1.4.4.5.cmml">Y</mi><mo id="S2.SS4.SSS2.p1.1.m1.4.4.4" xref="S2.SS4.SSS2.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S2.SS4.SSS2.p1.1.m1.4.4.3.3" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.4" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.cmml"><mi id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.2" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.2.cmml">y</mi><mn id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.3" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.5" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.cmml"><mi id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.2" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.2.cmml">y</mi><mn id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.3" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.6" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS4.SSS2.p1.1.m1.1.1" xref="S2.SS4.SSS2.p1.1.m1.1.1.cmml">…</mi><mo id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.7" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.cmml"><mi id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.2" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.2.cmml">y</mi><mi id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.3" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.3.cmml">k</mi></msub><mo stretchy="false" id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.8" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.1.m1.4b"><apply id="S2.SS4.SSS2.p1.1.m1.4.4.cmml" xref="S2.SS4.SSS2.p1.1.m1.4.4"><eq id="S2.SS4.SSS2.p1.1.m1.4.4.4.cmml" xref="S2.SS4.SSS2.p1.1.m1.4.4.4"></eq><ci id="S2.SS4.SSS2.p1.1.m1.4.4.5.cmml" xref="S2.SS4.SSS2.p1.1.m1.4.4.5">𝑌</ci><set id="S2.SS4.SSS2.p1.1.m1.4.4.3.4.cmml" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3"><apply id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.2">𝑦</ci><cn type="integer" id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.3.cmml" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.2">𝑦</ci><cn type="integer" id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.3.cmml" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.1.1">…</ci><apply id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.cmml" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.2">𝑦</ci><ci id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.3.cmml" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.3">𝑘</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.1.m1.4c">Y=\{y_{1},y_{2},...,y_{k}\}</annotation></semantics></math> to each pixel in an image-based dataset <math id="S2.SS4.SSS2.p1.2.m2.4" class="ltx_Math" alttext="D_{i}=\{d_{1},d_{2},...,d_{m}\}" display="inline"><semantics id="S2.SS4.SSS2.p1.2.m2.4a"><mrow id="S2.SS4.SSS2.p1.2.m2.4.4" xref="S2.SS4.SSS2.p1.2.m2.4.4.cmml"><msub id="S2.SS4.SSS2.p1.2.m2.4.4.5" xref="S2.SS4.SSS2.p1.2.m2.4.4.5.cmml"><mi id="S2.SS4.SSS2.p1.2.m2.4.4.5.2" xref="S2.SS4.SSS2.p1.2.m2.4.4.5.2.cmml">D</mi><mi id="S2.SS4.SSS2.p1.2.m2.4.4.5.3" xref="S2.SS4.SSS2.p1.2.m2.4.4.5.3.cmml">i</mi></msub><mo id="S2.SS4.SSS2.p1.2.m2.4.4.4" xref="S2.SS4.SSS2.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S2.SS4.SSS2.p1.2.m2.4.4.3.3" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.4" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.4.cmml">{</mo><msub id="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1" xref="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1.2" xref="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1.2.cmml">d</mi><mn id="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1.3" xref="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.5" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2" xref="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2.cmml"><mi id="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2.2" xref="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2.2.cmml">d</mi><mn id="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2.3" xref="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.6" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS4.SSS2.p1.2.m2.1.1" xref="S2.SS4.SSS2.p1.2.m2.1.1.cmml">…</mi><mo id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.7" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3.cmml"><mi id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3.2" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3.2.cmml">d</mi><mi id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3.3" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3.3.cmml">m</mi></msub><mo stretchy="false" id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.8" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.2.m2.4b"><apply id="S2.SS4.SSS2.p1.2.m2.4.4.cmml" xref="S2.SS4.SSS2.p1.2.m2.4.4"><eq id="S2.SS4.SSS2.p1.2.m2.4.4.4.cmml" xref="S2.SS4.SSS2.p1.2.m2.4.4.4"></eq><apply id="S2.SS4.SSS2.p1.2.m2.4.4.5.cmml" xref="S2.SS4.SSS2.p1.2.m2.4.4.5"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.2.m2.4.4.5.1.cmml" xref="S2.SS4.SSS2.p1.2.m2.4.4.5">subscript</csymbol><ci id="S2.SS4.SSS2.p1.2.m2.4.4.5.2.cmml" xref="S2.SS4.SSS2.p1.2.m2.4.4.5.2">𝐷</ci><ci id="S2.SS4.SSS2.p1.2.m2.4.4.5.3.cmml" xref="S2.SS4.SSS2.p1.2.m2.4.4.5.3">𝑖</ci></apply><set id="S2.SS4.SSS2.p1.2.m2.4.4.3.4.cmml" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.3"><apply id="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1.cmml" xref="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1.2">𝑑</ci><cn type="integer" id="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1.3.cmml" xref="S2.SS4.SSS2.p1.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2.cmml" xref="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2.1.cmml" xref="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2.2.cmml" xref="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2.2">𝑑</ci><cn type="integer" id="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2.3.cmml" xref="S2.SS4.SSS2.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS4.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS2.p1.2.m2.1.1">…</ci><apply id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3.cmml" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3.1.cmml" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3.2.cmml" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3.2">𝑑</ci><ci id="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3.3.cmml" xref="S2.SS4.SSS2.p1.2.m2.4.4.3.3.3.3">𝑚</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.2.m2.4c">D_{i}=\{d_{1},d_{2},...,d_{m}\}</annotation></semantics></math> or each point in a point-based dataset <math id="S2.SS4.SSS2.p1.3.m3.4" class="ltx_Math" alttext="D_{p}=\{d_{1},d_{2},...,d_{n}\}" display="inline"><semantics id="S2.SS4.SSS2.p1.3.m3.4a"><mrow id="S2.SS4.SSS2.p1.3.m3.4.4" xref="S2.SS4.SSS2.p1.3.m3.4.4.cmml"><msub id="S2.SS4.SSS2.p1.3.m3.4.4.5" xref="S2.SS4.SSS2.p1.3.m3.4.4.5.cmml"><mi id="S2.SS4.SSS2.p1.3.m3.4.4.5.2" xref="S2.SS4.SSS2.p1.3.m3.4.4.5.2.cmml">D</mi><mi id="S2.SS4.SSS2.p1.3.m3.4.4.5.3" xref="S2.SS4.SSS2.p1.3.m3.4.4.5.3.cmml">p</mi></msub><mo id="S2.SS4.SSS2.p1.3.m3.4.4.4" xref="S2.SS4.SSS2.p1.3.m3.4.4.4.cmml">=</mo><mrow id="S2.SS4.SSS2.p1.3.m3.4.4.3.3" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.4" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.4.cmml">{</mo><msub id="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1" xref="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1.cmml"><mi id="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1.2" xref="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1.2.cmml">d</mi><mn id="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1.3" xref="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.5" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2" xref="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2.cmml"><mi id="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2.2" xref="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2.2.cmml">d</mi><mn id="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2.3" xref="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.6" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS4.SSS2.p1.3.m3.1.1" xref="S2.SS4.SSS2.p1.3.m3.1.1.cmml">…</mi><mo id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.7" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3.cmml"><mi id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3.2" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3.2.cmml">d</mi><mi id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3.3" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.8" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.3.m3.4b"><apply id="S2.SS4.SSS2.p1.3.m3.4.4.cmml" xref="S2.SS4.SSS2.p1.3.m3.4.4"><eq id="S2.SS4.SSS2.p1.3.m3.4.4.4.cmml" xref="S2.SS4.SSS2.p1.3.m3.4.4.4"></eq><apply id="S2.SS4.SSS2.p1.3.m3.4.4.5.cmml" xref="S2.SS4.SSS2.p1.3.m3.4.4.5"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.3.m3.4.4.5.1.cmml" xref="S2.SS4.SSS2.p1.3.m3.4.4.5">subscript</csymbol><ci id="S2.SS4.SSS2.p1.3.m3.4.4.5.2.cmml" xref="S2.SS4.SSS2.p1.3.m3.4.4.5.2">𝐷</ci><ci id="S2.SS4.SSS2.p1.3.m3.4.4.5.3.cmml" xref="S2.SS4.SSS2.p1.3.m3.4.4.5.3">𝑝</ci></apply><set id="S2.SS4.SSS2.p1.3.m3.4.4.3.4.cmml" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.3"><apply id="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1.cmml" xref="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1.1.cmml" xref="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1.2.cmml" xref="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1.2">𝑑</ci><cn type="integer" id="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1.3.cmml" xref="S2.SS4.SSS2.p1.3.m3.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2.cmml" xref="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2.1.cmml" xref="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2.2.cmml" xref="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2.2">𝑑</ci><cn type="integer" id="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2.3.cmml" xref="S2.SS4.SSS2.p1.3.m3.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS4.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS4.SSS2.p1.3.m3.1.1">…</ci><apply id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3.cmml" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3.1.cmml" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3.2.cmml" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3.2">𝑑</ci><ci id="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3.3.cmml" xref="S2.SS4.SSS2.p1.3.m3.4.4.3.3.3.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.3.m3.4c">D_{p}=\{d_{1},d_{2},...,d_{n}\}</annotation></semantics></math>.</p>
</div>
<section id="S2.SS4.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Camera-based Semantic Segmentation</h5>

<div id="S2.SS4.SSS2.Px1.p1" class="ltx_para">
<p id="S2.SS4.SSS2.Px1.p1.1" class="ltx_p">The technique of camera-based semantic segmentation finds widespread application in the fields of free-space segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>, <a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib120" title="" class="ltx_ref">120</a>, <a href="#bib.bib121" title="" class="ltx_ref">121</a>, <a href="#bib.bib122" title="" class="ltx_ref">122</a>, <a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>, lane segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>, <a href="#bib.bib126" title="" class="ltx_ref">126</a>, <a href="#bib.bib127" title="" class="ltx_ref">127</a>, <a href="#bib.bib120" title="" class="ltx_ref">120</a>, <a href="#bib.bib121" title="" class="ltx_ref">121</a>, <a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> and obstacle segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite> in autonomous driving.
Fully Convolutional Network (FCN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite> is a milestone in semantic segmentation as it enables end-to-end training of deep networks for this task. However, due to its failure to account for global contextual information, the obtained segmentation results tend to be coarse. Therefore, the encoder-decoder architecture has emerged to address this shortcoming, represented by SegNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite>, U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> and HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite>. The encoder-decoder architecture typically uses an image classification network as its encoder, gradually reducing the spatial dimensions of the pooling layer. Meanwhile, the decoder gradually restores the details and spatial dimensions for segmentation purposes.</p>
</div>
<div id="S2.SS4.SSS2.Px1.p2" class="ltx_para">
<p id="S2.SS4.SSS2.Px1.p2.1" class="ltx_p">However, for encoder-decoder architectures, high-resolution representations are lost during the encoding process, reducing fine-grained information within the image.
Dilated (or “atrous”) convolution structure is created to avoid decimating the input’s resolution by adding a dilation rate to standard convolutions. This architecture enlarges the receptive field without increasing the parameters and avoids the loss of information caused by repeated pooling. Some notable examples of representative networks which apply dilated convolution structure include DeepLab series <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>, <a href="#bib.bib136" title="" class="ltx_ref">136</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>, ENet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite>, PSPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite>, DUC-HDC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite> and DenseASPP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite>.</p>
</div>
<div id="S2.SS4.SSS2.Px1.p3" class="ltx_para">
<p id="S2.SS4.SSS2.Px1.p3.1" class="ltx_p">CNNs require multiple decoder stacks to map high-level features to the original spatial resolution. In contrast, transformer-based models can be graciously combined with a lightweight transformer decoder for segmentation mask prediction due to their global modeling capability and resolution invariance. Recently, transformer-based segmentation models (e.g., SETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite>, Segmenter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite>, SegFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>, Lawin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite> and MaskFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite>) extract global contextual features based on self-attention and achieve remarkable results.</p>
</div>
</section>
<section id="S2.SS4.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Radar-based Semantic Segmentation</h5>

<div id="S2.SS4.SSS2.Px2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.Px2.p1.1" class="ltx_p">Radar-based semantic segmentation is applied in the fields of vehicle segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite>, pedestrian segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite>, free-space segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite> and static object segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib152" title="" class="ltx_ref">152</a>]</cite> in autonomous driving.
Similar to object detection methods in radar-based applications, network architectures vary depending on specific radar representations. These architectures also incorporate algorithms adapted from the image and point cloud domains to enable efficient processing and analysis of radar data.
Segmentation on radar tensors refers to the process of dividing the tensor into discrete regions or segments based on specific criteria or properties. The goal is to identify and label different parts or objects in the radar RA tensor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite>, RD tensor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite> and RAD semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib148" title="" class="ltx_ref">148</a>, <a href="#bib.bib155" title="" class="ltx_ref">155</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, enabling a more comprehensive understanding of the scene. CNN architectures like DeepLabv3+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite> and U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> possess the ability to extract intricate features and relationships directly from the radar tensor data, thereby facilitating effective segmentation tasks.</p>
</div>
<div id="S2.SS4.SSS2.Px2.p2" class="ltx_para">
<p id="S2.SS4.SSS2.Px2.p2.1" class="ltx_p">For segmentation on radar point clouds, conventional CNN algorithms (e.g., PointNet and PointNet++) can effectively capture the spatial relationships and semantics of individual radar points to classify them into different categories or segments. These algorithms are widely utilized in point-wise semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib157" title="" class="ltx_ref">157</a>, <a href="#bib.bib158" title="" class="ltx_ref">158</a>, <a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite> and grid-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib152" title="" class="ltx_ref">152</a>, <a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite>. However, the initial data transformation involved in these approaches may result in information loss and sparsity in the data representation.
Recent point transformer networks (e.g., Gaussian Radar Transformer (GRT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>) enhance performance for 3D point cloud understanding by elaborating the attention mechanism, which is able to capture complex structures in sparse point clouds.
In addition to basic semantic segmentation, instance-based segmentation methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>, <a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite> not only classify each point in the radar point cloud but also group nearby points together into instances.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Fusion Datasets</span>
</h2>

<figure id="S3.F5" class="ltx_figure"><img src="/html/2304.10410/assets/x5.png" id="S3.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="141" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Overview of radar-camera fusion datasets. Color of the bar donates different radar representations, and height of the bar represents the frame of each dataset.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">High-quality and large-scale data are fundamental for deep learning-based perception algorithms in autonomous driving. Datasets containing data from LiDARs and cameras, such as KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>, Oxford RobotCar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>, ApolloScape <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>, and Waymo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>, have been widely used for LiDAR-camera fusion in autonomous driving. As radar research continues in-depth, dozens of radar and camera datasets have been released in recent years. In this section, we analyze and summarize these datasets designed explicitly for tasks related to object detection and semantic segmentation. Fig. <a href="#S3.F5" title="Figure 5 ‣ III Fusion Datasets ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents clear statistics of radar-camera fusion datasets with their radar representations and dataset sizes. We also provide a table for retrieval and comparison of different datasets (see Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:datasets</span>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Dataset Tasks</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">According to the dimension of bounding boxes and masks, datasets that incorporate radar and camera modalities in object detection and semantic segmentation are categorized into four groups:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">2D object detection:</span> SeeingThroughFog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, CARRADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, Zendar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>, RADIATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>, AIODrive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>, CRUW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>, RaDICaL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite>, RADDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>, FloW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite>, RADIal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, Boreas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> and WaterScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>]</cite>;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">3D object detection:</span> nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, Astyx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>, SeeingThroughFog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, AIODrive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>, VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite>, K-Radar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite> and aiMotive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite>;</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">2D semantic segmentation:</span> CARRADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, RadarScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite> and RADIal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>;</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">3D semantic segmentation:</span> HawkEye <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Regarding dataset tasks, most datasets are oriented toward object detection, whereas comparatively fewer datasets are employed for semantic segmentation tasks. Notably, CARRADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, RadarScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite> and RADIal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> can be applied to both object detection and semantic segmentation tasks. As to those datasets that contribute to multiple tasks, nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is the most widely used dataset in radar-camera fusion algorithms, supporting tasks of detection, tracking, prediction and localization. In addition to object detection, RADIATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite> involves object tracking, scene understanding and SLAM tasks. Moreover, datasets like Zender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite> and Boreas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> are available for localization and odometry.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Sensing Modalities</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">For radar-camera fusion datasets in object detection and semantic segmentation tasks, the data produced by the camera sensor is either a single image or a video over a while, both of which are essentially 2D images. In comparison, data produced by the radar sensor is rich in representations, which can be grouped into ADC signal, radar tensor, and point cloud according to the stages of data processing.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.5.1.1" class="ltx_text">III-B</span>1 </span>ADC Signal</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">As the raw data produced by radar sensors, ADC signals retain all semantic information and can be highly valuable in deep learning applications.
Up to now, only two radar-camera fusion datasets provide raw ADC signal data: RaDICaL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite> and RADIal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>.
RaDICaL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite> is the first dataset providing raw ADC signal data, specialized for object detection tasks involving pedestrians and vehicles. The authors encouraged researchers to further design their own processing methods by providing the raw radar measurements.
RADIal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> is the richest dataset regarding radar data representations, offering not only ADC signals, but also processed data after ADC signals, including radar tensors and point clouds.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.5.1.1" class="ltx_text">III-B</span>2 </span>Radar Tensor</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">After multiple FFTs, radar tensors are obtained from ADC signals. They can be classified into three categories: 2D tensors (e.g., RADIATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>, CRUW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>, FloW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite>), 3D tensors (e.g., CARRADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, Zendar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>, RADDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>, RADIal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>) and 4D tensors (e.g., K-Radar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite>).
Both RADIATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite> and CRUW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite> are in range-azimuth coordinates, presenting the BEV position of objects, while the FloW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite> dataset is in range-Doppler coordinates, illustrating the relationship between range and Doppler velocity of each object.
CARRADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> is the first dataset that combines synchronized stereo RGB images and 3D radar RAD tensors in autonomous driving.
As far as we are aware, K-Radar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite> is the only dataset containing 4D radar tensors, with full information on range, Doppler, azimuth and elevation.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS3.5.1.1" class="ltx_text">III-B</span>3 </span>Point Cloud</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Compared to radar tensors, point clouds serve as a lighter and more intuitive representation of objects. They are also the format of data output from commercial radars.
Conventional 3D radars produce sparse point clouds, such as data in nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, Zender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>, SeeingThroughFog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, HawkEye <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite>, AIODrive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>, RADIal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, FloW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite>, RadarScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite> and aiMotive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> dataset.
In recent years, the radar sensor has advanced from 3D to 4D with improvements in resolution and elevation measurement capabilities. Consequently, public 4D radar-camera fusion datasets are emerging, with examples such as Astyx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>, VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite> and WaterScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>]</cite>.
Although Astyx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite> is the first 4D point cloud dataset, it is limited by the data size, containing only 500 frames. VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite> datasets are improved in terms of data categories and data size, with the former consisting of 13 types and 8,693 frames, and the latter containing eight types and 40k frames. Meanwhile, these two datasets also contain simultaneous LiDAR data, facilitating comparison between the 4D radar point clouds and LiDAR point clouds.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Dataset Categories</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">For autonomous driving, it is critical to identify Vulnerable Road Users (VRU) on roads. Therefore, the most common categories in these datasets are pedestrians, bicycles, and cars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>, <a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>, <a href="#bib.bib179" title="" class="ltx_ref">179</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
Datasets such as nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, AIODrive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>, RadarScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite>, VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and aiMotive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> have studied more than ten categories. nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> provides precision in its classifications, with 23 object categories refining certain ambiguous categories. For instance, the pedestrian category is sub-categorized into groups such as adult and child, while the vehicle category is subclassed into the car, ambulance, police, motorcycle, trailer and truck.
Except for the category of pedestrian, RADIATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite> and RadarScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite> include a class called pedestrian group.
AIODrive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite> and VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> classify stationary objects on the roadside, such as building, road, wall, traffic sign, unused bicycle and bicycle rack.
Furthermore, apart from objects on road surfaces, FloW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite> is a floating waste dataset containing the category of bottle that can be utilized for Unmanned Surface Vehicles (USVs) on water surfaces. WaterScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>]</cite> contains more objects of interest on water surfaces, including static objects such as piers and buoys, and dynamic objects such as ships, boats, vessels, kayaks, and sailors aboard these surface vehicles.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In addition to the primary object categories, some specific attributes of the object are also labeled in some datasets. Examples can be found in nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, in which vehicles are labeled as moving, stopped or parked, while pedestrians are marked as moving or standing.
Besides, in VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> dataset, two types of occlusions (“spatial” and “lighting”) and attributes related to an object’s activity (“stopped”, “moving”, “parked”, “pushed” and “sitting”) are also annotated. All these specific attributes are essential for scene understanding.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Dataset Size</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The reviewed datasets differ significantly in size, ranging from 500 to 1.4 million frames. nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is the largest dataset with 1.4 million images, radar frames and object bounding boxes in 40k keyframes. These data frames are split from 15 hours and 242 kilometers of driving data.
On the other hand, Astyx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite> provides only 500 frames, containing around 3k labeled 3D object annotations.
Others like CRUW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>, CARRADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, RADIATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>, AIODrive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>, SeeingThroughFog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, CRUW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>, RADDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> and RADIal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> all contribute hundreds of thousands of frames.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">In addition to data in frames, some datasets deliver videos for researchers to split keyframes and conduct further research on videos. For example, RadarScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite> offers 158 individual sequences with a total length of over four hours. Similarly, CRUW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite> and CARRADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> datasets provide videos of 3.5 hours and 21.2 minutes, respectively.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Recording Scenarios</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">A rich data collection environment is crucial for training robust models in autonomous driving. Generally, datasets for autonomous driving are collected in road environments like urban streets, country roads, highways and parking lots, which are all represented in datasets like CARRADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, RADIATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>, RadarScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite>, RADlal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, and K-Radar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite>.
However, it is not enough to collect data in common areas. nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, Zendar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>, SeeingThroughFog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, AIODrive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite> and CRUW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite> involve dense traffic and challenging driving situations, including urban roads, residential areas and industrial areas. Moreover, RadarScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite> offers data for selected particular scenarios, such as T-junctions, commercial areas and road works.
All the datasets mentioned above are from outdoor environments, and as to indoor scenarios, HawkEye <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite> and RaDICaL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite> are collected in indoor parking garages. These indoor environments present unique challenges and can help advance research in indoor autonomous vehicle navigation.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">In terms of weather and lighting conditions, related data can be found in nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, SeeingThroughFog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, CARRADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, RADIATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>, K-Radar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite>, AIODrive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>, CRUW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>, TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite>, aiMotive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> and WaterScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>]</cite>.
In particular, SeeingThroughFog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> focuses on extreme weather conditions, like fog, snow and rain. This dataset highlights the importance of data fusion and the redundancy of multiple sensors in adverse weather environments.
In addition to adverse weather conditions, RADIATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>, K-Radar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite> and aiMotive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> involve conditions for the night. AIODrive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>, CRUW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite> and TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite> supply data in specific vision-fail scenarios, such as darkness, bright light and blur, where the images are pretty bad in quality. These datasets provide valuable information about how autonomous driving technology operates in low visibility and low light scenarios.
Boreas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> is a dataset involving data taken from specific routes through one-year repeat collection. It can be leveraged to study the effects of seasonal variation on self-localization and object detection.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">Since it is time-cost and resource-cost for data collection on roads, some researchers have adopted simulated data to generate datasets.
In HawkEye <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite>, raw low-resolution heatmaps are transformed into high-frequency shapes. Additionally, the authors developed a data synthesizer to simulate radar signals from the created 3D point reflector models of cars. Meanwhile, Weng <span id="S3.SS5.p3.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite> used Carla <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> simulator to create different driving scenarios with various sensors. With the annotation data generated by combining and post-processing Carla outputs, they presented AIODrive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>, a large-scale synthetic dataset for all mainstream perception tasks.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Fusion Methodologies</span>
</h2>

<figure id="S4.F6" class="ltx_figure"><img src="/html/2304.10410/assets/x6.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="155" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Methodologies of radar-camera fusion. (a) “What to fuse”: the input modalities for radar-camera fusion, such as RGB images, near-infrared images, point clouds, and radar tensors. (b) “Where to fuse”: the position transformation for radar-camera fusion, including two perspectives: front view and bird’s eye view. (c) “When to fuse”: at which stage the two modalities are fused, encompassing the fusion of final objects, raw data, extracted features and hybrid representations. (d) “How to fuse”: the data alignment and fusion operations, comprising of two types of alignment, namely temporal alignment and spatial alignment, and five types of fusion operations, namely addition, mean, multiplication, concatenation and attention.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we delve into the methodologies of radar-camera fusion related to object detection and semantic segmentation tasks, starting with “why to fuse”, that is, the purpose and advantages of fusion.
Subsequently, we analyze “what to fuse”, covering diverse representations of both radar and camera modalities implicated in fusion.
Next, we investigate “where to fuse”, describing the coordinate relations between the two modalities before fusion.
In the section on “when to fuse”, we categorize the fusion levels and illustrate their differences.
After that, we explore the specifics of “how to fuse”, including temporal-spatial synchronization and fusion operations.
Regarding network architectures for fusion, we categorize them into two architectures: point-based and tensor-based, followed by a more detailed classification of each category and the main ideas in these architectures.
Finally, in the model evaluations section, we review various evaluation metrics and assess the performance of popular methods.
An overview of radar-camera fusion methodologies containing the questions and answers is demonstrated in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV Fusion Methodologies ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Why to Fuse</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The integration of radar and camera sensors for object detection and semantic segmentation is intended to enhance the perception outcomes by capitalizing on the advantages of both sensing modalities.
As illustrated in Fig. <a href="#S2.F4" title="Figure 4 ‣ II-C Comparison of Radar and Camera Sensors ‣ II Background ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> in Section <a href="#S2.SS3" title="II-C Comparison of Radar and Camera Sensors ‣ II Background ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>, the combination of a radar sensor and a camera sensor enables the measurement of rich object attributes such as color, shape, range, and velocity.
In addition, with the ability to perceive in darkness and adverse weather conditions, the fusion of radar and camera can work all day for autonomous driving vehicles.
For autonomous driving tasks, object detection and segmentation results obtained from radar-camera fusion can also assist in object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>, <a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite>, providing accurate environment perception information for the decision-making and control systems.
For other downstream tasks, such as trajectory prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite> and vehicle navigation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite>, radar-camera fusion has successfully demonstrated excellent driving performance in both unseen urban and heavy traffic scenarios.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Numerous studies have also demonstrated that radar-camera fusion improves the accuracy and robustness of the network.
As it is difficult for image-based detectors to detect distant objects, Chadwick <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite> combined a radar sensor and sets of camera sensors in their experiments. Results exceed the performance of the camera detector, as the radar sensor persists in delivering a potent indication of motion for faraway objects.
Major <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> also proved that the velocity dimension derived from the radar sensor could be leveraged to increase detection performance.
Additionally, Nabati and Qi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> utilized radar features (e.g., depth, rotation, velocity) to complement the image features, resulting in an improvement of the overall nuScenes Detection Score (NDS) by more than 12% compared to the SOTA camera-based algorithm including OFT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite>, MonoDIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite> and CenterNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>.
In noisy circumstances, Yadav <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite> discovered that radar data exhibit robustness in detection, and integration of radar data could enhance performance in these challenging scenarios.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">What to Fuse</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The objective of radar-camera fusion is the output data from the radar sensor and camera sensor, which are presented in different modalities at various fusion levels and via different fusion techniques.
For the camera sensor, the output data is typically presented as 2D images. In radar-camera fusion, there are mainly two kinds of images. One type is the RGB image with rich color information, such as images in nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> dataset. The other is the infrared image captured with infrared cameras (including Far Infrared (FIR) and Near Infrared (NIR)), as illustrated in the images from SeeingThroughFog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> dataset. Though in lower resolutions, these images contain specific advantages in temperature differences and night visibility.
The data structure of an image is relatively simple, with low data dimensionality and high correlations between neighboring pixels. The simplicity of this structure allows deep neural networks to learn the fundamental representations of images, thus enabling them to detect objects within the images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">As mentioned in Section <a href="#S2.SS1.SSS3" title="II-A3 Data Representations ‣ II-A Radar Sensors ‣ II Background ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span>3</span></a>, radar data can be classified into different representations depending on the level of processing. ADC Signals, the underlying digital signals of the radar, cannot be marked with the location information of an object. MDS, a Time-Frequency representation, consists of consecutive radar frames and does not correspond to a single image frame. As a result, ADC Signals and MDS are commonly used for identifying the presence of objects and discriminating between different objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>, <a href="#bib.bib192" title="" class="ltx_ref">192</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib193" title="" class="ltx_ref">193</a>]</cite>. With the ability to describe the shape of an object, radar tensors and point clouds are commonly leveraged for object detection and semantic segmentation tasks.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Where to Fuse</span>
</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.5.1.1" class="ltx_text">IV-C</span>1 </span>Front View</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Fusion at the Front View (FV) involves projecting the radar data onto an image plane, where the radar data can be 3D point clouds, partial point cloud information or radar tensors.
Around the projected area, proposals that indicate potential objects are generated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib194" title="" class="ltx_ref">194</a>, <a href="#bib.bib190" title="" class="ltx_ref">190</a>, <a href="#bib.bib195" title="" class="ltx_ref">195</a>, <a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite>. In this way, a large number of non-object regions are excluded, thus reducing the computational burden and increasing recognition speed.
Radar data mapped to the image plane can also be utilized to create feature maps for complementing the image-based features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>, <a href="#bib.bib197" title="" class="ltx_ref">197</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib198" title="" class="ltx_ref">198</a>]</cite>. These methods improve the detection accuracy by leveraging the additional input, including ranges, velocities and RCS values.
Moreover, some researchers project radar point clouds onto the image plane to form a radar pseudo-image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib199" title="" class="ltx_ref">199</a>, <a href="#bib.bib200" title="" class="ltx_ref">200</a>, <a href="#bib.bib201" title="" class="ltx_ref">201</a>]</cite>. For example, In RVNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and SO-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib199" title="" class="ltx_ref">199</a>]</cite>, a pseudo-image named “Sparse Radar Image” is generated from radar data, containing information regarding depth, lateral velocity and longitudinal velocity. Besides, Dong <span id="S4.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib200" title="" class="ltx_ref">200</a>]</cite> projected both radar point clouds and 2D bounding boxes onto the image plane, forming new pseudo-images from camera RGB images. MS-YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib201" title="" class="ltx_ref">201</a>]</cite> generates radar mask maps by a mapping transformation neural network. In each mask map, the boxed area represents the presence of an object, and the gray value of each box indicates the velocity information of that object.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">Projecting radar data onto the image plane assists in providing proposals and features. However, due to the low resolution in the azimuth angle provided by the radar as well as camera calibration errors, projected radar point clouds may deviate from the object. While increasing the Region of Interest (RoI) could potentially address the issue, it results in multiple objects within the same region, and consequently being detected repeatedly, causing confusion in object matching. Moreover, due to the occlusion of objects, the projection of radar data onto the image perspective may be limited.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.5.1.1" class="ltx_text">IV-C</span>2 </span>Bird’s Eye View</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Another fusion position is to convert radar data or camera images into BEV coordinates.
For example, radar point clouds from each frame generate a BEV image of six height maps and one density map in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>.
Besides, Cui <span id="S4.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite> projected radar point clouds to both FV and BEV, and proposed a 3D region proposal network to generate proposals from both camera images and radar BEV images. Compared with generating proposals directly from point clouds, the CNN-based proposal generation approach increases the quality of proposals by leveraging the network’s ability to extract deeper and richer information.
Problems come that BEV images discretize the sensing space into grids, which may lead to the loss of valuable information necessary to refine bounding boxes. To address this issue, Bansal <span id="S4.SS3.SSS2.p1.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>]</cite> added additional point-based features (e.g., velocities, RCS values) to the BEV map.
Simple-BEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite> converts all radar point clouds from multiple radar sensors into BEV coordinates to yield high-dimensional BEV feature maps.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">Apart from projecting radar data into BEV, Inverse Projection Mapping (IPM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib204" title="" class="ltx_ref">204</a>, <a href="#bib.bib205" title="" class="ltx_ref">205</a>]</cite> method can be utilized to convert camera images from FV to BEV with a homography matrix.
For instance, Lim <span id="S4.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib206" title="" class="ltx_ref">206</a>]</cite> transformed the camera images into Cartesian coordinates using IPM and then combined them with 2D radar RA tensors.
In addition, both radar point clouds and camera images are projected onto BEV in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite>, where the independent feature extractors learn shared features.
Consequently, projecting data on BEV offers several advantages over FV, particularly in the case of occlusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite>. Nonetheless, since IPM is based on an assumption of flat road surfaces, it often produces distortions of dynamic objects when applied to real-world scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">When to Fuse</span>
</h3>

<figure id="S4.F7" class="ltx_figure"><img src="/html/2304.10410/assets/x7.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="182" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Taxonomy of radar-camera fusion levels. (a) Object-level fusion. (b) Data-level fusion. (c) Feature-level fusion. (d) Hybrid-level fusion.</figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">When to fuse refers to at which stage the radar and camera data are fused in the network. Based on the occasion of the fusion process, we classify radar-camera fusion levels into object-level, data-level, feature-level and hybrid-level. Fig. <a href="#S4.F7" title="Figure 7 ‣ IV-D When to Fuse ‣ IV Fusion Methodologies ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the overview and difference between the four fusion levels.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS4.SSS1.5.1.1" class="ltx_text">IV-D</span>1 </span>Object-level Fusion</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">For object-level fusion (also known as decision-level fusion or late-level fusion), the independent objects acquired from the radar and camera sensors are fused at a later stage of the network to obtain the final integrated results, as demonstrated in Fig. <a href="#S4.F7" title="Figure 7 ‣ IV-D When to Fuse ‣ IV Fusion Methodologies ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(a).
In object-level fusion, how to match the results from the two different modalities is worth considering.
One way is to calculate the similarity (e.g., location, size, category) and then employ methods such as the Kalman filter, Bayesian theory, Hungarian algorithm and Bipartite Matching to match the outputs.
Another approach involves utilizing the transformation matrix between the radar and camera to determine the position relationships between the two modalities. For example, Jha <span id="S4.SS4.SSS1.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib209" title="" class="ltx_ref">209</a>]</cite> projected the radar detections onto the image plane using the transformation matrix, and then aligned independent detection objects from the two sensors.
Moreover, after completing the association of radar point clouds with camera images, Dong <span id="S4.SS4.SSS1.p1.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib200" title="" class="ltx_ref">200</a>]</cite> proposed AssociationNet for learning the semantic representation information from the two sensors. This network improves the accuracy of association by calculating and minimizing Euclidean distance between the representations from the pair of radar point clouds and image bounding boxes.</p>
</div>
<div id="S4.SS4.SSS1.p2" class="ltx_para">
<p id="S4.SS4.SSS1.p2.1" class="ltx_p">Object-level fusion is commonly used in conventional radars and cameras, which offers high flexibility and modularity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib210" title="" class="ltx_ref">210</a>]</cite>.
However, it also relies heavily on the accuracy of outputs from individual modules. For example, in scenarios where the camera sensor is obstructed, object-level fusion exclusively depends on the final objects detected by the radar sensor.
Besides, rich intermediate features are discarded due to the sensing modality’s weaknesses or errors in the sensors. As a result, object-level fusion methods can only utilize limited information obtained from the detection results.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS4.SSS2.5.1.1" class="ltx_text">IV-D</span>2 </span>Data-level Fusion</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">For data-level fusion (also referred to as low-level fusion or early-level fusion), the raw data or pre-processing data from radar and camera sensors are fused at the early stage of deep learning models, illustrated in Fig. <a href="#S4.F7" title="Figure 7 ‣ IV-D When to Fuse ‣ IV Fusion Methodologies ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(b).
Nobis <span id="S4.SS4.SSS2.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite> fed the concatenated camera and radar point clouds into the network and then employed VGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite> to extract features from the combined data.
Moreover, Bansa <span id="S4.SS4.SSS2.p1.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>]</cite> created a Semantic-Point-Grid (SPG) representation from camera semantic maps, radar point clouds and radar BEV grid maps. In their method, the SPG representation is then fed into SPG encoding to extract semantic information from cameras, aiding in the identification of radar points associated with objects of interest.
Instead of fusing radar point clouds with camera images, Nabati and Qi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> proposed an RRPN, which generates object proposals to narrow the scope of detection on the camera images. However, if there is no radar point on an object, this object will be ignored.
To solve the difficulty of associating radar point clouds with image pixels, Long <span id="S4.SS4.SSS2.p1.1.3" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib213" title="" class="ltx_ref">213</a>]</cite> presented Radar-Camera Pixel Depth Association (RC-PDA), a learned method that associates radar point clouds with nearby image pixels to enhance and densify the radar image.</p>
</div>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<p id="S4.SS4.SSS2.p2.1" class="ltx_p">With the input of raw data, it is possible to exploit complete characteristics and learn a joint representation from these two modalities. However, data-level fusion methods tend to be sensitive to temporal or spatial misalignment within the data. Precise external calibration of the two sensors is essential for data-level fusion.
Besides, as radar data representations are not consistent with the object’s shape, it is difficult to match the radar tensors or radar point clouds with the image pixels.</p>
</div>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS4.SSS3.5.1.1" class="ltx_text">IV-D</span>3 </span>Feature-level Fusion</h4>

<figure id="S4.F8" class="ltx_figure"><img src="/html/2304.10410/assets/x8.png" id="S4.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="261" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Feature heatmaps of two feature-level fusion networks, namely (a) SAF-FCOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite> and (b) BIRANet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite>. Both networks take inputs from camera and radar branches, and extract image features and point cloud features using respective backbones. In the feature fusion stage, SAF-FCOS employs a multiplication operation, while BIRANet performs an element-wise addition operation. Detector heatmaps generated by the Grad-CAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib215" title="" class="ltx_ref">215</a>]</cite> are utilized for the result analysis of each method. To clearly show feature maps, only nine channels are selected for visualization.</figcaption>
</figure>
<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">In feature-level fusion (also called middle-level fusion), features extracted from separate radar data and camera images are combined at an intermediate stage in deep learning-based fusion networks, as shown in Fig. <a href="#S4.F7" title="Figure 7 ‣ IV-D When to Fuse ‣ IV Fusion Methodologies ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(c).
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite>, features from both radar and camera branches are generated by ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> blocks and then fused by concatenation and addition operations.
CenterFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> detects objects by locating their center points in the image using CenterNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>. After that, it utilizes a frustum-based association strategy to accurately match radar detections with objects in the image, generating radar-based feature maps to augment the image features.
SAF-FCOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite> introduces the attention mechanism for weighting different positions of the feature maps. Specifically, it utilizes a Spatial Attention Fusion (SAF) block to merge the feature maps from radar and camera. In the SAF block, the radar image’s feature maps are encoded as a spatial attention weight matrix, which is then applied to all channels to re-weight the feature maps extracted by the camera sensor.
BIRANet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite> uses Concurrent Spatial and Channel Squeeze &amp; Excitation (scSE) blocks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib216" title="" class="ltx_ref">216</a>]</cite> to highlight important spatial features and significant channels. The scSE block acts as attention and adaptively boosts activation of areas where radar point clouds are present while suppressing activation at other locations. This boosted feature map is then fused with the image feature map to improve the performance of the detection network.
Considering feature maps that provide enhanced spatial and channel-wise information, BIRANet exhibits the capability to detect small objects that SAF fails to identify, as illustrated by the detector heatmaps shown in Figure <a href="#S4.F8" title="Figure 8 ‣ IV-D3 Feature-level Fusion ‣ IV-D When to Fuse ‣ IV Fusion Methodologies ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.
In fact, attention maps can be generated from various sensors. Bijelic <span id="S4.SS4.SSS3.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> extended the sensors to RGB camera, gated camera, LiDAR and radar by transforming all sensor data into uniform image coordinates. The feature maps of different sensors are then superimposed together by concatenation and multiplied with the sigmoid-processed entropy map for the final feature output.</p>
</div>
<div id="S4.SS4.SSS3.p2" class="ltx_para">
<p id="S4.SS4.SSS3.p2.1" class="ltx_p">For feature-level fusion, it is possible to design appropriate feature extractors for each modality according to its specific characteristics. Neural networks can also learn features jointly across modalities, making them complementary to each other.
However, it is worth noting that feature extraction and feature fusion do not address scenarios where camera sensors become unreliable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib210" title="" class="ltx_ref">210</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS4.SSS4.5.1.1" class="ltx_text">IV-D</span>4 </span>Hybrid-level Fusion</h4>

<div id="S4.SS4.SSS4.p1" class="ltx_para">
<p id="S4.SS4.SSS4.p1.1" class="ltx_p">Apart from equally fusing final objects, raw data or features from two modalities, some fusion methods combine different stages of data, which we define as hybrid-level fusion, shown in Fig. <a href="#S4.F7" title="Figure 7 ‣ IV-D When to Fuse ‣ IV Fusion Methodologies ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(d).
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref">194</a>]</cite>, radar proposals are first generated from radar point clouds and 3D anchors derived from camera images. Then a Radar Proposal Refinement (RPR) network is proposed to fuse the radar proposals with camera image features, which enables the adjustment of the size and location of the radar proposals in the image. Besides, the RPR network also estimates an objectness score for each radar proposal, as some radar point clouds are caused by background noise.
Similarly, Cui <span id="S4.SS4.SSS4.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite> generated proposals based on camera images and radar BEV point clouds, followed by projecting the proposals onto three feature maps from camera images, radar BEV point clouds and radar FV point clouds. A Self-Supervised Model Adaptation (SSMA) block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib217" title="" class="ltx_ref">217</a>]</cite> is utilized to fuse the proposals with features, which leverages an attention scheme for better correlation.
Furthermore, HRFuser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib218" title="" class="ltx_ref">218</a>]</cite> introduces ideas from HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite> and HRFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib219" title="" class="ltx_ref">219</a>]</cite>, adopting an asymmetric Multi-Window Cross-Attention (MWCA) to fuse the features captured by the RGB camera, LiDAR, radar and gated camera.</p>
</div>
<div id="S4.SS4.SSS4.p2" class="ltx_para">
<p id="S4.SS4.SSS4.p2.1" class="ltx_p">Compared with data-level and feature-level fusion, fusion from both proposals and features leads to more accurate proposals, producing better features for the two-stage network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref">194</a>, <a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite>.
Generally, different modalities have different contributions to radar-camera fusion. One modality dominates, while the other provides supplementary information to refine the features. Thus, hybrid-level fusion takes advantage of different data levels and effectively preserves information at various stages.
However, hybrid-level fusion should consider the importance of different modalities, which also pose implementation challenges. Since most implementations of hybrid-level fusion are based on experience and lack explainability to some extent, conducting numerous ablation experiments is needed to validate the efficiency of hybrid-level fusion. Moreover, models based on hybrid-level fusion typically have more branches in neural networks, dramatically slowing down the inference time.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.5.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.6.2" class="ltx_text ltx_font_italic">How to Fuse</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">In this section, we present how to fuse radar data with camera images.
First of all, the primary consideration is the temporal and spatial alignment between the two sensors.
Then, in fusion operations, we compare five operations and analyze their advantages and disadvantages.</p>
</div>
<section id="S4.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS5.SSS1.5.1.1" class="ltx_text">IV-E</span>1 </span>Data Alignment</h4>

<section id="S4.SS5.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Temporal Alignment</h5>

<div id="S4.SS5.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS5.SSS1.Px1.p1.1" class="ltx_p">Temporal alignment in sensor fusion refers to synchronizing the temporal sequences of data from different sensors.
To obtain high-quality fusion results, the data collected by each sensor must be synchronized with the same time dimension. However, there may be time offsets between these sensors due to the differences in set-up time, crystal oscillator frequencies and measurement latency.
Depending on the object of the temporal alignment methods, we categorize them into two types: estimating temporal latency between sensors and estimating temporal offset within the same frame.</p>
</div>
<div id="S4.SS5.SSS1.Px1.p2" class="ltx_para">
<p id="S4.SS5.SSS1.Px1.p2.1" class="ltx_p"><span id="S4.SS5.SSS1.Px1.p2.1.1" class="ltx_text ltx_font_bold">Estimating Temporal Latency:</span>
Generally, temporal latency consists of the measurement latency between sensors and the drift between different frames.
Measurement latency mainly stems from computer scheduling, measurement acquisition, pre-processing, and communication transfer time. In aligning cycle time, drift is caused by the offset between the internal clock and the Coordinated Universal Time (UTC).</p>
</div>
<div id="S4.SS5.SSS1.Px1.p3" class="ltx_para">
<p id="S4.SS5.SSS1.Px1.p3.1" class="ltx_p">A software-based technique for reducing temporal error is periodically estimating the maximum measurement latency and drift time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib220" title="" class="ltx_ref">220</a>, <a href="#bib.bib221" title="" class="ltx_ref">221</a>, <a href="#bib.bib222" title="" class="ltx_ref">222</a>]</cite>. Another alternative approach is to predict the future latency between sensors using Kalman filters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib223" title="" class="ltx_ref">223</a>]</cite> or Bayesian estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib224" title="" class="ltx_ref">224</a>]</cite> based on prior knowledge of the sensors’ latency.
These methods improve the synchronization result and are suitable for most applications. Since trigger signals for sensors are not initiated simultaneously, there inevitably remains some degree of unknown latency, which can cause variations in acquisition times during data fusion.
Thus, some researchers proposed solutions by combining a hardware controller trigger with the software strategy to reduce the execution time of activation threads in software <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib225" title="" class="ltx_ref">225</a>, <a href="#bib.bib226" title="" class="ltx_ref">226</a>, <a href="#bib.bib227" title="" class="ltx_ref">227</a>]</cite>. These approaches communicate with hardware synchronization components at a low level to eliminate the data acquisition latency. However, standard commercial hardware often lacks hardware synchronization interfaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib228" title="" class="ltx_ref">228</a>]</cite>. When using such methods, the complexity and portability of the system design should be considered.</p>
</div>
<div id="S4.SS5.SSS1.Px1.p4" class="ltx_para">
<p id="S4.SS5.SSS1.Px1.p4.1" class="ltx_p"><span id="S4.SS5.SSS1.Px1.p4.1.1" class="ltx_text ltx_font_bold">Estimating Temporal Offset:</span>
As the temporal offset between sensors directly affects the fusion quality, some studies proposed temporal calibration strategies based on aligning the same objects from camera and radar sensors to extract timestamp offset. For example, Du <span id="S4.SS5.SSS1.Px1.p4.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib229" title="" class="ltx_ref">229</a>]</cite> aligned the frames that a vehicle passes the detection line and then estimated the temporal offset between these two frames. Moreover, some researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>, <a href="#bib.bib231" title="" class="ltx_ref">231</a>, <a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite> suggested employing real-time pre-processing buffers that leverage algorithms like YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> and DBSCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib232" title="" class="ltx_ref">232</a>]</cite> to reorganize the same frame.</p>
</div>
</section>
<section id="S4.SS5.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Spatial Alignment</h5>

<div id="S4.SS5.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS5.SSS1.Px2.p1.1" class="ltx_p">Spatial alignment between radar and camera sensors involves transformation operations that map 3D or 2D radar point clouds to camera image pixels.
As the spatial calibration between radar and camera is a fundamental task for information fusion, several methods of joint calibration have been proposed.
Among these approaches, whether a specially designed calibration target is needed in the calibration process is an important indicator, leading to different calibration design strategies.</p>
</div>
<div id="S4.SS5.SSS1.Px2.p2" class="ltx_para">
<p id="S4.SS5.SSS1.Px2.p2.1" class="ltx_p"><span id="S4.SS5.SSS1.Px2.p2.1.1" class="ltx_text ltx_font_bold">Target-based Approaches:</span>
For target-based calibration approaches, specific calibration targets are utilized so that sensors can get precise locations of the target. These locations estimate the rigid transformation relations between the radar and camera sensors. The triangular corner reflector is the most common choice for radar calibration, reflecting specific RCS values for the positional information.
Moreover, to obtain positions of the calibrated targets from both the radar sensor and camera sensor, some novel designed calibration boards are proposed. For example, a corner reflector and a styrofoam board are combined as the calibration target in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib233" title="" class="ltx_ref">233</a>, <a href="#bib.bib234" title="" class="ltx_ref">234</a>]</cite>. The styrofoam board is applied for visual recognition in camera sensors without affecting the radar signals. Wang <span id="S4.SS5.SSS1.Px2.p2.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib233" title="" class="ltx_ref">233</a>]</cite> designed a calibration board consisting of a styrofoam board with four holes and a corner reflector in the center of these holes. The Perspective-n-Point (PnP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib235" title="" class="ltx_ref">235</a>]</cite> algorithm is then used to extract the 3D location of holes and estimate the location of the corner reflector. Moreover, Peršić <span id="S4.SS5.SSS1.Px2.p2.1.3" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib234" title="" class="ltx_ref">234</a>]</cite> introduced a calibration board consisting of a corner reflector and a styrofoam triangle adorned with a checkerboard, from which both radar and camera sensors can obtain accurate target positional readings. Based on the paired set of image pixels and radar points for the same targets in different locations, the transformation matrix between the radar coordinates and camera coordinates is calculated.</p>
</div>
<div id="S4.SS5.SSS1.Px2.p3" class="ltx_para">
<p id="S4.SS5.SSS1.Px2.p3.1" class="ltx_p"><span id="S4.SS5.SSS1.Px2.p3.1.1" class="ltx_text ltx_font_bold">Target-less Approaches:</span>
On the other hand, target-less calibration approaches do not rely on specific checkerboards, thus improving the portability of calibration. However, the uncertainty of environmental factors when extracting the same features from multiple sensors is a common drawback in target-less calibration methods.
Some researchers utilize precise radar velocity measurements based on the moving objects and the camera pose to implement radar-to-camera extrinsic calibration algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib236" title="" class="ltx_ref">236</a>, <a href="#bib.bib237" title="" class="ltx_ref">237</a>, <a href="#bib.bib238" title="" class="ltx_ref">238</a>, <a href="#bib.bib234" title="" class="ltx_ref">234</a>, <a href="#bib.bib239" title="" class="ltx_ref">239</a>]</cite>.
Besides, machine learning algorithms are also utilized in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib240" title="" class="ltx_ref">240</a>, <a href="#bib.bib241" title="" class="ltx_ref">241</a>, <a href="#bib.bib233" title="" class="ltx_ref">233</a>, <a href="#bib.bib242" title="" class="ltx_ref">242</a>]</cite> to predict calibration parameters based on improving the consistency of radar point clouds and camera images.</p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2304.10410/assets/x9.png" id="S4.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="147" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Chronological overview of radar-camera fusion algorithms. Color of the arrow represents different fusion levels, and shape of the icon represents different network architectures.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS5.SSS2.5.1.1" class="ltx_text">IV-E</span>2 </span>Fusion Operations</h4>

<div id="S4.SS5.SSS2.p1" class="ltx_para">
<p id="S4.SS5.SSS2.p1.1" class="ltx_p">In radar-camera fusion, different fusion operations are used to fuse data from the two modalities.
Specifically, for object-level and data-level fusion networks, a transformation matrix is commonly used to align final objects or raw data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib209" title="" class="ltx_ref">209</a>, <a href="#bib.bib243" title="" class="ltx_ref">243</a>]</cite>.
In contrast, feature-level and hybrid-level fusion networks tend to utilize addition and concatenation operations.
In the <span id="S4.SS5.SSS2.p1.1.1" class="ltx_text ltx_font_bold">addition</span> operation, element-wise features in the feature maps are added. Thus, each channel in the feature map contains more feature information, making the classifier comprehend the feature details.
Similarly to the addition operation, the <span id="S4.SS5.SSS2.p1.1.2" class="ltx_text ltx_font_bold">mean</span> and <span id="S4.SS5.SSS2.p1.1.3" class="ltx_text ltx_font_bold">multiplication</span> operations calculate the average mean and multiplication of the element-wise feature maps, respectively.
In the <span id="S4.SS5.SSS2.p1.1.4" class="ltx_text ltx_font_bold">concatenation</span> operation, the feature maps are flattened into vectors and then concatenated along the rows. The primary objective of the concatenation operation is to enrich feature diversity, enabling the classifier to recognize objects with higher accuracy.</p>
</div>
<div id="S4.SS5.SSS2.p2" class="ltx_para">
<p id="S4.SS5.SSS2.p2.1" class="ltx_p">Given that the features of radars and cameras are heterogeneous, and the above fusion operations are sensitive to changes in the input data, the effectiveness of the modalities in particular scenarios is ignored. For example, the performance of the camera sensor tends to reduce in adverse weather conditions, while the radar sensor continues to work properly. Thus, the <span id="S4.SS5.SSS2.p2.1.1" class="ltx_text ltx_font_bold">attention</span> operation is proposed to re-calculate the weights of the feature maps from two modalities. One example of such an approach is Spatial Attention Fusion (SAF), proposed by Chang <span id="S4.SS5.SSS2.p2.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite>. SAF extracts the spatial attention matrix from the radar images and then employs it to re-weight the feature maps from the image branch.
Other approaches leverage the Mixture of Expert (MoE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib244" title="" class="ltx_ref">244</a>, <a href="#bib.bib245" title="" class="ltx_ref">245</a>]</cite> to extract feature maps from respective expert networks and calculate the attention weights by a gating network. After that, based on these weights, the feature maps are re-assigned to optimize fusion performance.</p>
</div>
</section>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.5.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.6.2" class="ltx_text ltx_font_italic">Network Architectures</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">Generally, networks for radar-camera fusion are structured with dual input branches, where the data from radar and camera is input separately. Depending on the desired fusion stage, raw data, feature maps or final objects are fused in the designed network for fusion results. Based on the representations of radar data, we classify the radar-camera fusion networks in object detection and semantic segmentation tasks into point-based and tensor-based networks. We also provide a chronological overview of radar-camera fusion algorithms in Fig. <a href="#S4.F9" title="Figure 9 ‣ Spatial Alignment ‣ IV-E1 Data Alignment ‣ IV-E How to Fuse ‣ IV Fusion Methodologies ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, and summarize the comparable contents in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:fusion_methods</span>.</p>
</div>
<section id="S4.SS6.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS6.SSS1.5.1.1" class="ltx_text">IV-F</span>1 </span>Point-based Networks</h4>

<div id="S4.SS6.SSS1.p1" class="ltx_para">
<p id="S4.SS6.SSS1.p1.1" class="ltx_p">Point-based Networks take radar point clouds as input. According to the different radar point cloud processing methods, we subdivide the point-based methods into projection-based, pseudo-image-based, voxel-based and BEV-based methods.</p>
</div>
<div id="S4.SS6.SSS1.p2" class="ltx_para">
<p id="S4.SS6.SSS1.p2.1" class="ltx_p"><span id="S4.SS6.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Projection-based Methods:</span>
In point-based radar-camera fusion networks, radar point clouds are mostly projected onto the 2D image plane to provide proposals or features. Then networks such as VGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite>, ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>, U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> and YOLOv4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> are used for feature extraction.
Chadwick <span id="S4.SS6.SSS1.p2.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite> projected the radar point clouds onto the camera plane and generated two kinds of radar images: range image and range-rate image. Then they integrated an additional radar input branch upon the SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> network, and used both concatenation and element-wise addition operations to fuse the radar features after the image block. The branch structure exhibits potential flexibility in re-calculating weights between the camera image and radar representations.
Besides, Meyer and Kuschk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite> generated a BEV image with six height maps and one density map from the point clouds of each frame. The authors also proposed a 3D region proposal network based on VGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite> to predict the position of boxes and the front angle of the detected object.
RVNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, a one-stage object detection network based on the YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, contains two input branches for radar and camera, and two output branches for small obstacles and big obstacles. Specifically, radar point clouds are transformed into sparse radar images in the image coordinate system via the intrinsic matrix from the camera sensor. Each sparse radar image consists of three channels, namely depth, lateral velocity and longitudinal velocity.
Based on RVNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, SO-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib199" title="" class="ltx_ref">199</a>]</cite> is presented, focusing on multi-task learning within a single network. In RVNet, the two output branches are modified for vehicle detection and free-space segmentation.
CRF-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite> projects the radar point clouds onto the image plane and feeds the concatenated camera and radar data into a designed VGG-based network. This network enables learning which layer fusion would yield the best benefits by adjusting the weights to radar features on different layers.
In fact, particular objects in camera images tend to remain undetected in night-time scenarios, even when using standard object detection frameworks like YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>. YOdar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite> involves lowering the score threshold and assigning radar point clouds to image slices, which are then combined through an aggregated output. Finally, a gradient-boosting classifier is employed to minimize the number of false positive predictions, improving the detection accuracy at night conditions.</p>
</div>
<div id="S4.SS6.SSS1.p3" class="ltx_para">
<p id="S4.SS6.SSS1.p3.1" class="ltx_p">In conclusion, projection-based methods in point-based processing leverage radar-to-image projection techniques and various deep learning networks for feature extraction. By leveraging techniques such as radar image generation, multi-stage fusion, and network adaptation, these methods enable robust perception and scene understanding in complex environments, thereby advancing the field of radar-camera fusion networks.</p>
</div>
<div id="S4.SS6.SSS1.p4" class="ltx_para">
<p id="S4.SS6.SSS1.p4.1" class="ltx_p"><span id="S4.SS6.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Pseudo-Image-based Methods:</span>
Since image-based CNN networks cannot directly learn original radar point clouds, some studies convert radar point clouds into radar pseudo-images and then utilize image-based methods to extract features.
Based on the distant object detection method in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite>, Chang <span id="S4.SS6.SSS1.p4.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite> proposed a radar pseudo-image generation model. Apart from transforming radar point clouds from 3D coordinates into 2D camera coordinates, they also converted the depth, longitudinal velocity and lateral velocity to a real pixel value in RGB channels. Then they introduced a Spatial Attention Fusion-based Fully Convolutional One-Stage (SAF-FCOS) network using a SAF block to merge feature maps derived from radar and camera sensors. In the SAF block, features of radar images are encoded as a spatial attention weight matrix, which is employed to re-weight the feature maps from the image branch.
SeeingThroughFog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> introduces a measurement entropy to fuse features from multiple sensors adaptively. Specifically, it applies convolution and sigmoid to the input entropy for a multiplication matrix. The matrix is then utilized to scale the concatenated features from different sensors. This approach adaptively fuses features in the feature extraction stack with the most accurate information.
In CenterFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, a novel frustum-based radar association method is proposed to correlate radar detections with preliminary image results. Notably, the authors generated a heat map using depth and radial velocity channels to produce complementary features for the image. After that, they fed the concatenated features into the regression heads to refine the preliminary detection by re-calculating the object’s depth, rotation, velocity and attributes. Finally, the results from the regression heads are decoded into 3D bounding boxes.</p>
</div>
<div id="S4.SS6.SSS1.p5" class="ltx_para">
<p id="S4.SS6.SSS1.p5.1" class="ltx_p">Overall, pseudo-image-based methods in point-based radar point cloud processing involve the transformation of radar point clouds into radar pseudo-images, which are then processed using image-based techniques. These methods utilize innovative approaches such as spatial attention mechanisms, adaptive feature fusion based on measurement entropy, and frustum-based radar association to enhance the accuracy and robustness of detection results.</p>
</div>
<div id="S4.SS6.SSS1.p6" class="ltx_para">
<p id="S4.SS6.SSS1.p6.1" class="ltx_p"><span id="S4.SS6.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Voxel-based Methods:</span>
Apart from projecting radar point clouds onto the camera plane and transforming them into pseudo-images, some researchers extract features directly from 3D radar point clouds to complement the image features. This approach exploits the rich information from the radar point clouds, but requires more sophisticated processing techniques to handle the high-dimensional and unstructured nature of the data.
In GRIF Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib245" title="" class="ltx_ref">245</a>]</cite>, an FPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> and a Sparse Block Network (SBNet) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib246" title="" class="ltx_ref">246</a>]</cite> are used as radar backbones to achieve superior performance with low computational resources. Specifically, in point cloud processing, GRIF Net converts point clouds into voxels. As the point clouds are sparse and most voxels are empty, it leverages SBNet to convolve only on masked areas, avoiding ineffective blank areas. In the fusion module, RoI features from image and radar feature maps are combined by convolutional MoE, demonstrating the effectiveness of radar sensors in detecting vehicles at longer distances than cameras.
In LXL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib247" title="" class="ltx_ref">247</a>]</cite>, the 4D radar branch produces 3D radar occupancy grids that indicate the occupancy status of radar point clouds. These 3D radar occupancy grids are leveraged together with predicted image depth maps to assist in the transformation of image perspective features to the BEV domain. This integration method effectively aligns image features with radar BEV representations, enabling effective fusion with radar features.</p>
</div>
<div id="S4.SS6.SSS1.p7" class="ltx_para">
<p id="S4.SS6.SSS1.p7.1" class="ltx_p">Above all, voxel-based methods in point-based radar point cloud processing extract features directly from 3D radar point clouds. These methods utilize techniques such as voxelization, sparse convolution, and occupancy grids to handle the high-dimensional and unstructured nature of radar data.
By integrating radar features with image features, these methods demonstrate improved performance in detecting vehicles, especially at longer distances and in scenarios where camera data may be limited. The voxel-based approach allows for effective fusion and alignment of information between radar and camera modalities.</p>
</div>
<div id="S4.SS6.SSS1.p8" class="ltx_para">
<p id="S4.SS6.SSS1.p8.1" class="ltx_p"><span id="S4.SS6.SSS1.p8.1.1" class="ltx_text ltx_font_bold">BEV-based Methods:</span>
Recently, architectures utilizing BEV representations and transformer networks exhibited impressive performance.
Simple-BEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite> focuses on BEV maps from multiple cameras and radars. This method generates a 3D volume with features by projecting 3D coordinates around the ego-vehicle camera images and bilinearly sampling features at projected locations. Later, a BEV feature map is produced by concatenating the 3D features with a rasterized radar image.
CRAFT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib248" title="" class="ltx_ref">248</a>]</cite> refines image proposals by radar point clouds via a Spatio-Contextual Fusion Transformer (SCFT). The SCFT aims to leverage cross-attention layers to exchange spatial and contextual information in BEV, enabling the fusion network to learn where and what information should be extracted from camera and radar modalities.
MVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib249" title="" class="ltx_ref">249</a>]</cite> employs multi-view camera images to obtain semantic-aligned radar features, and subsequently integrates these features in a robust fusion transformer to optimize the cross-modal information interaction.
CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib250" title="" class="ltx_ref">250</a>]</cite> introduces multi-modal deformable attention to tackle the spatial misalignment between radar and camera feature maps. With its aggregated semantic features and accurate BEV representations, CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib250" title="" class="ltx_ref">250</a>]</cite> currently ranks first among all radar-camera fusion detectors in the nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> dataset, being the best approach in 3D radar-camera fusion.
RCFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib251" title="" class="ltx_ref">251</a>]</cite> achieves multi-modal feature fusion under a unified BEV perspective with the input of 4D radar and camera. The Interactive Attention Module (IAM), a key component of RCFusion, is utilized to weight the features of each modality, thus fully exploiting the advantages of both modalities.</p>
</div>
<div id="S4.SS6.SSS1.p9" class="ltx_para">
<p id="S4.SS6.SSS1.p9.1" class="ltx_p">In summary, BEV-based methods in point-based radar point cloud processing leverage BEV representations and transformer networks to achieve impressive performance in radar-camera fusion. These methods incorporate techniques such as refining proposals, cross-modal information interaction, semantic alignment, and attention mechanisms to optimize feature extraction and fusion between radar and camera data. With the ability to handle spatial misalignment and exploit the advantages of top-down perspective, BEV-based methods demonstrate high performance in 3D radar-camera fusion tasks.</p>
</div>
</section>
<section id="S4.SS6.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS6.SSS2.5.1.1" class="ltx_text">IV-F</span>2 </span>Tensor-based Networks</h4>

<div id="S4.SS6.SSS2.p1" class="ltx_para">
<p id="S4.SS6.SSS2.p1.1" class="ltx_p">Due to the potential loss of crucial information concerning objects or the surrounding environment during the processing of radar point clouds after CFAR detection, several researchers have put forward a fusion scheme that involves the fusion of radar tensors with camera images. We categorize these tensor-based networks into cross-supervised-based methods and projection-based methods.</p>
</div>
<div id="S4.SS6.SSS2.p2" class="ltx_para">
<p id="S4.SS6.SSS2.p2.1" class="ltx_p"><span id="S4.SS6.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Cross-Supervised-based Methods:</span>
For radar data in the format of tensors, it is challenging to label the radar data as they are not spatially consistent compared to image data. Thus, some researchers propose cross-modal supervision methods to generate radar labels with the supervision of camera images.
Specifically, RODNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite> is a radar object detection network using a camera-radar fusion strategy to cross-supervised 3D localization of detected objects during the training stage. It takes sequences of RA tensors as input and uses a neural network-based approach to extract the Doppler information. Specifically, to handle multi-chirp merging information and dynamic object motion, RODNet introduces two customized modules, namely M-Net and Temporal Deformable Convolution (TDC).
Moreover, Gao <span id="S4.SS6.SSS2.p2.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> fed sequences of RD tensor, RA tensor and AD tensor into convolutional autoencoders. They proposed a Radar Multiple-Perspectives Convolutional Neural Network (RAMP-CNN) that utilizes the temporal information in the chirps within a single frame, along with the change in spatial information between frames. In RAMP-CNN, features of these three tensors are then fused in a fusion module to generate new range-azimuth features. Compared to RODNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>, RAMP-CNN achieves significant performance and maintains the same detection accuracy in night scenes as in the daytime.
Recently, Jin <span id="S4.SS6.SSS2.p2.1.3" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib252" title="" class="ltx_ref">252</a>]</cite> utilized the segmented camera image with radar customized adaption as the ground truth for training deep neural networks to perform panoptic segmentation on radar data. The proposed network utilizes panoptic segmentation to achieve radar-tailored sensing, including free-space segmentation and object detection, with only radar RA tensor in urban, rural, and highway scenarios.</p>
</div>
<div id="S4.SS6.SSS2.p3" class="ltx_para">
<p id="S4.SS6.SSS2.p3.1" class="ltx_p">In conclusion, cross-supervised-based methods enable radar data to benefit from the rich spatial information available in camera data. Techniques like RODNet, RAMP-CNN, and cross-supervised panoptic segmentation have demonstrated the effectiveness of incorporating multi-modal supervision techniques to enhance object detection and segmentation tasks, thus showing performance improvements in handling the unique characteristics and challenges of radar data.</p>
</div>
<div id="S4.SS6.SSS2.p4" class="ltx_para">
<p id="S4.SS6.SSS2.p4.1" class="ltx_p"><span id="S4.SS6.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Projection-based Methods:</span>
FusionNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib206" title="" class="ltx_ref">206</a>]</cite> converts both radar RA tensors and camera images into Cartesian coordinates, followed by projecting camera images onto the radar plane using a homography transformation. Upon passing through the independent feature extractor branches, features of the two modalities are passed through the additional fusion layers to form a unified feature map.
As it is challenging to fuse radar tensors and camera images in 3D coordinates, Hwang <span id="S4.SS6.SSS2.p4.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib253" title="" class="ltx_ref">253</a>]</cite> proposed a radar-camera matching network named CramNet. CramNet overcomes the uncertainties in the geometric correspondences between the camera and radar through a ray-constrained cross-attention mechanism. Specifically, since a peak in the radar returns usually accompanies the optimal 3D position corresponding to the foreground pixel of an image, CramNet projects radar features along the pixel rays to estimate the depth and refine the 3D locations of camera pixels. Experiments on the RADIATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite> dataset demonstrate that the CramNet outperforms the baseline results from the Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> detector. Additionally, by conducting experiments on the filtering of RA tensors via varying intensity thresholds, radar RA tensors prove to contain more meaningful information for 3D object detection than sparse point clouds.</p>
</div>
<div id="S4.SS6.SSS2.p5" class="ltx_para">
<p id="S4.SS6.SSS2.p5.1" class="ltx_p">To sum up, projection-based fusion methods, such as FusionNet and CramNet, offer practical solutions for integrating radar and camera data by leveraging geometric transformations and novel attention mechanisms. These methods contribute to advancing the integration of multi-modal information and demonstrate promising results in object detection tasks, highlighting the significance of leveraging radar tensors in perception systems.</p>
</div>
</section>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS7.5.1.1" class="ltx_text">IV-G</span> </span><span id="S4.SS7.6.2" class="ltx_text ltx_font_italic">Model Evaluations</span>
</h3>

<section id="S4.SS7.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS7.SSS1.5.1.1" class="ltx_text">IV-G</span>1 </span>Evaluation Metrics</h4>

<div id="S4.SS7.SSS1.p1" class="ltx_para">
<p id="S4.SS7.SSS1.p1.1" class="ltx_p">Various evaluation metrics are adopted or newly proposed to evaluate the performance of radar-camera fusion models, as summarized in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:evaluation_metrics</span>.
Similar to image-based object detection and semantic segmentation tasks, in radar-camera fusion, commonly used evaluation metrics are precision, recall, Average Precision (AP), Average Recall (AR), mean Average Precision (mAP) and mean Intersection over Union (mIoU).
However, these metrics only calculate the prediction accuracy on a given test dataset. Attributes in multi-modal datasets, such as velocity, range, size and orientation, are ignored. Besides, for multi-modal networks, the IoU thresholds should depend on object distance and occlusion, as well as the type of sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S4.SS7.SSS1.p2" class="ltx_para">
<p id="S4.SS7.SSS1.p2.1" class="ltx_p">To overcome these drawbacks, the nuScenes  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> dataset introduces mATE, mASE, mAOE, mAVE and mAAE, which stand for mean average translation, scale, orientation, velocity and attribute errors, respectively. Furthermore, they presented nuScenes Detection Score (NDS), half based on the mAP, half quantifying the previous five metrics.
To evaluate how well a detection result matches the ground truth, Wang <span id="S4.SS7.SSS1.p2.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> defined Object Location Similarity (OLS) that quantifies the correlation between two detections concerning their distance, classes and scale information.
Additionally, some metrics designed for LiDAR object detection are also adopted in radar point clouds. For example, Cui <span id="S4.SS7.SSS1.p2.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite> utilized Average Heading Similarity (AHS) to calculate the accuracy, which is formulated initially to calculate the average orientation angle in 3D LiDAR IoU defined in AVOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS7.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS7.SSS2.5.1.1" class="ltx_text">IV-G</span>2 </span>Performance Evaluation</h4>

<figure id="S4.F10" class="ltx_figure"><img src="/html/2304.10410/assets/x10.png" id="S4.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="359" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Performance of radar-camera fusion methods on the nuScenes <span id="S4.F10.3.1" class="ltx_text ltx_font_italic">test</span> set. The horizontal and vertical axes are mAP and NDS, respectively, and the larger their values, the better the performance.
mAP and NDS are the metrics for evaluating the horizontal and vertical axes, respectively, where greater values indicate better performance.</figcaption>
</figure>
<div id="S4.SS7.SSS2.p1" class="ltx_para">
<p id="S4.SS7.SSS2.p1.1" class="ltx_p">Given that the majority of researchers have employed the nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite> datasets to evaluate the performance of their algorithms, we provide a comprehensive summary of the evaluation metrics and performance outcomes on these dataset in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:evaluation</span>.
We also provide Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-G2 Performance Evaluation ‣ IV-G Model Evaluations ‣ IV Fusion Methodologies ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> to clearly show the performance comparison of radar-camera fusion methods using the complete nuScenes dataset.
CenterFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, the first radar-camera fusion algorithm to operate on the complete nuScenes dataset, achieves a performance outcome of 32.6% on the mAP and 44.9% on the NDS. CenterFusion solved the critical data association problem in radar-camera fusion by proposing a novel frustum-based radar association method, which generates a RoI frustum around objects in 3D space using preliminary detection results, and maps the radar detection to the center of objects on the image. In comparison to CenterNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib254" title="" class="ltx_ref">254</a>]</cite>, which solely relies on image input, CenterFusion delivers a relative increase of 38.1% and 62.1% on the NDS and velocity error metrics, respectively, demonstrating the effectiveness of using radar features and robustness of radar-camera fusion in challenging environments.</p>
</div>
<div id="S4.SS7.SSS2.p2" class="ltx_para">
<p id="S4.SS7.SSS2.p2.1" class="ltx_p">After that, numerous radar-camera fusion algorithms employ CenterFusion as the baseline.
For example, RCBEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite>, a feature-level fusion approach, extracts radar features using a temporal-spatial encoder and transforms image features into BEV representations. Experimental results demonstrate superior feature representation and more accurate 3D object detection outcomes, receiving the mAP and NDS of 40.6% and 48.6%, respectively.
CRAFT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib248" title="" class="ltx_ref">248</a>]</cite> achieves 41.1% mAP and 52.3% NDS on the nuScenes, where most of the gain in performance originates from the improved localization and velocity estimation with the assistance of the spatio-contextual fusion transformer. This transformer exploits both spatial and contextual properties of camera and radar data to detect objects in 3D space more accurately.
CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib250" title="" class="ltx_ref">250</a>]</cite> currently emerges as the top-performing detector among all radar-camera fusion algorithms on the nuScenes dataset with 57.5% mAP and 62.4% NDS, being the best approach in 3D radar-camera fusion. The performance gain of the proposed CRN framework comes from its Radar-assisted View Transformation (RVT), which overcomes the lack of spatial information in an image and transforms perspective view image features to BEV with the help of sparse but accurate radar points. The transformed image features in BEV are then used in the Multi-modal Feature Aggregation (MFA) layers to generate a semantically rich and spatially accurate BEV representation.
For 4D radar-camera fusion, LXL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib247" title="" class="ltx_ref">247</a>]</cite> has 56.31% mAP and 36.32% mAP, ranks first in the VoD and TJ4DRadSet dataset, respectively, being the best approach for 4D radar-camera fusion.</p>
</div>
<div id="S4.SS7.SSS2.p3" class="ltx_para">
<p id="S4.SS7.SSS2.p3.1" class="ltx_p">In general, substantial progress has been made by various algorithms operating on radar-camera fusion datasets. The mAP has improved by 24.9%, and the NDS has increased by 17.5% on the complete nuScenes dataset. On the other hand, the mAP has improved by 18.31% on the VoD dataset. The incorporation of transformer architectures, attention mechanisms, and BEV features are crucial factors that have significantly contributed to enhancing performance outcomes.</p>
</div>
</section>
</section>
<section id="S4.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS8.5.1.1" class="ltx_text">IV-H</span> </span><span id="S4.SS8.6.2" class="ltx_text ltx_font_italic">Summary</span>
</h3>

<div id="S4.SS8.p1" class="ltx_para">
<p id="S4.SS8.p1.1" class="ltx_p">Above all, we present the methodologies of radar-camera fusion related to object detection and semantic segmentation tasks. Through in-depth analysis of the five questions revolving around “why to fuse”, “what to fuse”, “where to fuse”, “when to fuse” and “how to fuse”, we gain insight into the positive benefits of radar-camera fusion when applied on autonomous driving vehicles, including improved accuracy, robustness and redundancy.
As indicated by the summary information in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:fusion_methods</span>, the number of methods designed for semantic segmentation tasks is fewer than that for the object detection task.
On the one hand, as enumerated in Section <a href="#S3.SS1" title="III-A Dataset Tasks ‣ III Fusion Datasets ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a> and Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:datasets</span>, there are fewer public datasets with segmentation annotations than those with detection annotations available for radar-camera fusion.
On the other hand, the intrinsic characteristics of radar data render it more suitable for detection tasks. For example, the capacity for long-distance detection and velocity measurement confers a distinct advantage upon radar data for the effective detection of moving obstacles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite>. Conversely, the sparse and noisy point cloud structure presents significant limitations for semantic segmentation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
<div id="S4.SS8.p2" class="ltx_para">
<p id="S4.SS8.p2.1" class="ltx_p">It is important to note that although radar is not conventionally used for semantic segmentation tasks, it can effectively improve the accuracy and reliability of the fused data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>]</cite>. By providing complementary information, such as depth information, radar data can enhance the performance of semantic segmentation algorithms. Also, radar data can be employed to localize objects in 3D space, which can validate the 2D output generated by semantic segmentation algorithms. However, simple fusion operations of image and radar (e.g., concatenation) for semantic segmentation can distort the semantic structure of the image, essentially adding noise to the image. As a result, this can detrimentally impact convergence rates, causing slow and suboptimal learning of the segmentation model.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Challenges and Research Directions</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">It is a challenging problem to balance the performance of different modalities so that all of them can perform at their best level and thus improve the overall performance.
As described in Section <a href="#S1" title="I Introduction ‣ Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, radar-camera fusion faces numerous challenges.
If these challenges are not properly addressed in sensor perception, they may also affect the subsequent tasks such as localization, prediction, planning and control.
In this section, we focus on improving the accuracy and robustness of radar-camera fusion, with discussions on the critical challenges and possible research directions from two aspects: multi-modal data and multi-modal fusion.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Overview of challenges and research directions.</figcaption>
<table id="S5.T1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.1.1" class="ltx_tr">
<td id="S5.T1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.1.1.1" class="ltx_p" style="width:17.1pt;"><span id="S5.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Topic</span></span>
</span>
</td>
<td id="S5.T1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.2.1.1" class="ltx_p" style="width:69.7pt;"><span id="S5.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Sub Topic</span></span>
</span>
</td>
<td id="S5.T1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.3.1.1" class="ltx_p" style="width:156.5pt;"><span id="S5.T1.1.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Challenges</span></span>
</span>
</td>
<td id="S5.T1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.4.1.1" class="ltx_p" style="width:233.3pt;"><span id="S5.T1.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Research Directions</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.1.2" class="ltx_tr">
<td id="S5.T1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.2.1.1.1" class="ltx_p" style="width:17.1pt;"><span id="S5.T1.1.2.1.1.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T1.1.2.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.6pt;height:63.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:63.4pt;transform:translate(-28.94pt,-28.94pt) rotate(-90deg) ;">
<span id="S5.T1.1.2.1.1.1.1.1.1" class="ltx_p">Multi-modal Data</span>
</span></span></span></span>
</span>
</td>
<td id="S5.T1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.2.2.1.1" class="ltx_p" style="width:69.7pt;"><span id="S5.T1.1.2.2.1.1.1" class="ltx_text" style="font-size:80%;">Data Quality</span></span>
</span>
</td>
<td id="S5.T1.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.2.3.1.1" class="ltx_p" style="width:156.5pt;">
<span id="S5.T1.1.2.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:130.1pt;">
<span id="S5.I1" class="ltx_itemize">
<span id="S5.I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I1.i1.p1" class="ltx_para">
<span id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Sparsity</span></span>
</span></span>
<span id="S5.I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I1.i2.p1" class="ltx_para">
<span id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Inaccuracy</span></span>
</span></span>
<span id="S5.I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I1.i3.p1" class="ltx_para">
<span id="S5.I1.i3.p1.1" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text" style="font-size:80%;">Noise</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S5.T1.1.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.2.4.1.1" class="ltx_p" style="width:233.3pt;">
<span id="S5.T1.1.2.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:238.5pt;">
<span id="S5.I2" class="ltx_itemize">
<span id="S5.I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i1.p1" class="ltx_para">
<span id="S5.I2.i1.p1.1" class="ltx_p"><span id="S5.I2.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Leveraging multiple radar frames to enhance the density</span></span>
</span></span>
<span id="S5.I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i2.p1" class="ltx_para">
<span id="S5.I2.i2.p1.1" class="ltx_p"><span id="S5.I2.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Applying 4D radar sensors with higher resolution</span></span>
</span></span>
<span id="S5.I2.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i3.p1" class="ltx_para">
<span id="S5.I2.i3.p1.1" class="ltx_p"><span id="S5.I2.i3.p1.1.1" class="ltx_text" style="font-size:80%;">Studying distribution of point clouds (e.g., Gaussian distribution)</span></span>
</span></span>
<span id="S5.I2.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i4.p1" class="ltx_para">
<span id="S5.I2.i4.p1.1" class="ltx_p"><span id="S5.I2.i4.p1.1.1" class="ltx_text" style="font-size:80%;">Denoising based on physical characteristics of radar data</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.1.3" class="ltx_tr">
<td id="S5.T1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.3.1.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
<td id="S5.T1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.3.2.1.1" class="ltx_p" style="width:69.7pt;"><span id="S5.T1.1.3.2.1.1.1" class="ltx_text" style="font-size:80%;">Data Diversity</span></span>
</span>
</td>
<td id="S5.T1.1.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.3.3.1.1" class="ltx_p" style="width:156.5pt;">
<span id="S5.T1.1.3.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:130.1pt;">
<span id="S5.I3" class="ltx_itemize">
<span id="S5.I3.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I3.i1.p1" class="ltx_para">
<span id="S5.I3.i1.p1.1" class="ltx_p"><span id="S5.I3.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Small Size</span></span>
</span></span>
<span id="S5.I3.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I3.i2.p1" class="ltx_para">
<span id="S5.I3.i2.p1.1" class="ltx_p"><span id="S5.I3.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Insufficient Conditions</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S5.T1.1.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.3.4.1.1" class="ltx_p" style="width:233.3pt;">
<span id="S5.T1.1.3.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="S5.I4" class="ltx_itemize">
<span id="S5.I4.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I4.i1.p1" class="ltx_para">
<span id="S5.I4.i1.p1.1" class="ltx_p"><span id="S5.I4.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Collecting data from adverse conditions</span></span>
</span></span>
<span id="S5.I4.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I4.i2.p1" class="ltx_para">
<span id="S5.I4.i2.p1.1" class="ltx_p"><span id="S5.I4.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Integrating synthetic data with real-world data</span></span>
</span></span>
<span id="S5.I4.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I4.i3.p1" class="ltx_para">
<span id="S5.I4.i3.p1.1" class="ltx_p"><span id="S5.I4.i3.p1.1.1" class="ltx_text" style="font-size:80%;">Domain adaptation for model generalization with limited data</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.1.4" class="ltx_tr">
<td id="S5.T1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.4.1.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
<td id="S5.T1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.4.2.1.1" class="ltx_p" style="width:69.7pt;"><span id="S5.T1.1.4.2.1.1.1" class="ltx_text" style="font-size:80%;">Data Synchronization</span></span>
</span>
</td>
<td id="S5.T1.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.4.3.1.1" class="ltx_p" style="width:156.5pt;">
<span id="S5.T1.1.4.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:130.1pt;">
<span id="S5.I5" class="ltx_itemize">
<span id="S5.I5.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I5.i1.p1" class="ltx_para">
<span id="S5.I5.i1.p1.1" class="ltx_p"><span id="S5.I5.i1.p1.1.1" class="ltx_text" style="font-size:80%;">High Calibration Requirements</span></span>
</span></span>
<span id="S5.I5.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I5.i2.p1" class="ltx_para">
<span id="S5.I5.i2.p1.1" class="ltx_p"><span id="S5.I5.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Difficulties in Labeling</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S5.T1.1.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.4.4.1.1" class="ltx_p" style="width:233.3pt;">
<span id="S5.T1.1.4.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="S5.I6" class="ltx_itemize">
<span id="S5.I6.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I6.i1.p1" class="ltx_para">
<span id="S5.I6.i1.p1.1" class="ltx_p"><span id="S5.I6.i1.p1.1.1" class="ltx_text" style="font-size:80%;">4D radar-camera calibration</span></span>
</span></span>
<span id="S5.I6.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I6.i2.p1" class="ltx_para">
<span id="S5.I6.i2.p1.1" class="ltx_p"><span id="S5.I6.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Real-time online calibration and correction</span></span>
</span></span>
<span id="S5.I6.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I6.i3.p1" class="ltx_para">
<span id="S5.I6.i3.p1.1" class="ltx_p"><span id="S5.I6.i3.p1.1.1" class="ltx_text" style="font-size:80%;">Auto-labeling to reduce manual labeling</span></span>
</span></span>
<span id="S5.I6.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I6.i4.p1" class="ltx_para">
<span id="S5.I6.i4.p1.1" class="ltx_p"><span id="S5.I6.i4.p1.1.1" class="ltx_text" style="font-size:80%;">Improving labeling efficiency via active learning, domain adaptation, transfer learning, semi-supervised learning</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.1.5" class="ltx_tr">
<td id="S5.T1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.5.1.1.1" class="ltx_p" style="width:17.1pt;"><span id="S5.T1.1.5.1.1.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T1.1.5.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.6pt;height:69pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:69.0pt;transform:translate(-31.74pt,-31.74pt) rotate(-90deg) ;">
<span id="S5.T1.1.5.1.1.1.1.1.1" class="ltx_p">Multi-modal Fusion</span>
</span></span></span></span>
</span>
</td>
<td id="S5.T1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.5.2.1.1" class="ltx_p" style="width:69.7pt;"><span id="S5.T1.1.5.2.1.1.1" class="ltx_text" style="font-size:80%;">Feature Extraction</span></span>
</span>
</td>
<td id="S5.T1.1.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.5.3.1.1" class="ltx_p" style="width:156.5pt;">
<span id="S5.T1.1.5.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:130.1pt;">
<span id="S5.I7" class="ltx_itemize">
<span id="S5.I7.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I7.i1.p1" class="ltx_para">
<span id="S5.I7.i1.p1.1" class="ltx_p"><span id="S5.I7.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Less effective using LiDAR-based or image-based algorithms</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S5.T1.1.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.5.4.1.1" class="ltx_p" style="width:233.3pt;">
<span id="S5.T1.1.5.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="S5.I8" class="ltx_itemize">
<span id="S5.I8.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I8.i1.p1" class="ltx_para">
<span id="S5.I8.i1.p1.1" class="ltx_p"><span id="S5.I8.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Introducing attention mechanisms</span></span>
</span></span>
<span id="S5.I8.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I8.i2.p1" class="ltx_para">
<span id="S5.I8.i2.p1.1" class="ltx_p"><span id="S5.I8.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Using Graph Neural Networks to dig deeper into the relationship between sparse point clouds</span></span>
</span></span>
<span id="S5.I8.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I8.i3.p1" class="ltx_para">
<span id="S5.I8.i3.p1.1" class="ltx_p"><span id="S5.I8.i3.p1.1.1" class="ltx_text" style="font-size:80%;">Applying neural networks to extract radar information instead of traditional FFT operations</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.1.6" class="ltx_tr">
<td id="S5.T1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.6.1.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
<td id="S5.T1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.6.2.1.1" class="ltx_p" style="width:69.7pt;"><span id="S5.T1.1.6.2.1.1.1" class="ltx_text" style="font-size:80%;">Data Association</span></span>
</span>
</td>
<td id="S5.T1.1.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.6.3.1.1" class="ltx_p" style="width:156.5pt;">
<span id="S5.T1.1.6.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:130.1pt;">
<span id="S5.I9" class="ltx_itemize">
<span id="S5.I9.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I9.i1.p1" class="ltx_para">
<span id="S5.I9.i1.p1.1" class="ltx_p"><span id="S5.I9.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Ambiguity in associating radar data with image data</span></span>
</span></span>
<span id="S5.I9.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I9.i2.p1" class="ltx_para">
<span id="S5.I9.i2.p1.1" class="ltx_p"><span id="S5.I9.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Poor association using calibration matrix</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S5.T1.1.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.6.4.1.1" class="ltx_p" style="width:233.3pt;">
<span id="S5.T1.1.6.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="S5.I10" class="ltx_itemize">
<span id="S5.I10.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I10.i1.p1" class="ltx_para">
<span id="S5.I10.i1.p1.1" class="ltx_p"><span id="S5.I10.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Attention-based association with adaptive thresholds</span></span>
</span></span>
<span id="S5.I10.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I10.i2.p1" class="ltx_para">
<span id="S5.I10.i2.p1.1" class="ltx_p"><span id="S5.I10.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Joint state estimation</span></span>
</span></span>
<span id="S5.I10.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I10.i3.p1" class="ltx_para">
<span id="S5.I10.i3.p1.1" class="ltx_p"><span id="S5.I10.i3.p1.1.1" class="ltx_text" style="font-size:80%;">Uncertainty estimation of object tracks</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.1.7" class="ltx_tr">
<td id="S5.T1.1.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.7.1.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
<td id="S5.T1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.7.2.1.1" class="ltx_p" style="width:69.7pt;"><span id="S5.T1.1.7.2.1.1.1" class="ltx_text" style="font-size:80%;">Data Augmentation</span></span>
</span>
</td>
<td id="S5.T1.1.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.7.3.1.1" class="ltx_p" style="width:156.5pt;">
<span id="S5.T1.1.7.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:130.1pt;">
<span id="S5.I11" class="ltx_itemize">
<span id="S5.I11.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I11.i1.p1" class="ltx_para">
<span id="S5.I11.i1.p1.1" class="ltx_p"><span id="S5.I11.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Correlation and interdependence between radar and camera modalities</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S5.T1.1.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.7.4.1.1" class="ltx_p" style="width:233.3pt;">
<span id="S5.T1.1.7.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="S5.I12" class="ltx_itemize">
<span id="S5.I12.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I12.i1.p1" class="ltx_para">
<span id="S5.I12.i1.p1.1" class="ltx_p"><span id="S5.I12.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Joint data augmentation rather than augmenting each modality separately</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.1.8" class="ltx_tr">
<td id="S5.T1.1.8.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.8.1.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
<td id="S5.T1.1.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.8.2.1.1" class="ltx_p" style="width:69.7pt;"><span id="S5.T1.1.8.2.1.1.1" class="ltx_text" style="font-size:80%;">Training Strategies</span></span>
</span>
</td>
<td id="S5.T1.1.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.8.3.1.1" class="ltx_p" style="width:156.5pt;">
<span id="S5.T1.1.8.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:130.1pt;">
<span id="S5.I13" class="ltx_itemize">
<span id="S5.I13.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I13.i1.p1" class="ltx_para">
<span id="S5.I13.i1.p1.1" class="ltx_p"><span id="S5.I13.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Difficulties in training</span></span>
</span></span>
<span id="S5.I13.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I13.i2.p1" class="ltx_para">
<span id="S5.I13.i2.p1.1" class="ltx_p"><span id="S5.I13.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Overfitting for multi-modal model</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S5.T1.1.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.8.4.1.1" class="ltx_p" style="width:233.3pt;">
<span id="S5.T1.1.8.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="S5.I14" class="ltx_itemize">
<span id="S5.I14.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I14.i1.p1" class="ltx_para">
<span id="S5.I14.i1.p1.1" class="ltx_p"><span id="S5.I14.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Weighting operations on loss functions</span></span>
</span></span>
<span id="S5.I14.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I14.i2.p1" class="ltx_para">
<span id="S5.I14.i2.p1.1" class="ltx_p"><span id="S5.I14.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Weights freezing strategies</span></span>
</span></span>
<span id="S5.I14.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I14.i3.p1" class="ltx_para">
<span id="S5.I14.i3.p1.1" class="ltx_p"><span id="S5.I14.i3.p1.1.1" class="ltx_text" style="font-size:80%;">Knowledge distillation on uni-modal features for the multi-modal networks</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.1.9" class="ltx_tr">
<td id="S5.T1.1.9.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.9.1.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
<td id="S5.T1.1.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.9.2.1.1" class="ltx_p" style="width:69.7pt;"><span id="S5.T1.1.9.2.1.1.1" class="ltx_text" style="font-size:80%;">Model Robustness</span></span>
</span>
</td>
<td id="S5.T1.1.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.9.3.1.1" class="ltx_p" style="width:156.5pt;">
<span id="S5.T1.1.9.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:130.1pt;">
<span id="S5.I15" class="ltx_itemize">
<span id="S5.I15.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I15.i1.p1" class="ltx_para">
<span id="S5.I15.i1.p1.1" class="ltx_p"><span id="S5.I15.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Sensor degradation or failure in adverse conditions</span></span>
</span></span>
<span id="S5.I15.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I15.i2.p1" class="ltx_para">
<span id="S5.I15.i2.p1.1" class="ltx_p"><span id="S5.I15.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Unseen driving scenarios</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S5.T1.1.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.9.4.1.1" class="ltx_p" style="width:233.3pt;">
<span id="S5.T1.1.9.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="S5.I16" class="ltx_itemize">
<span id="S5.I16.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I16.i1.p1" class="ltx_para">
<span id="S5.I16.i1.p1.1" class="ltx_p"><span id="S5.I16.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Attention mechanisms</span></span>
</span></span>
<span id="S5.I16.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I16.i2.p1" class="ltx_para">
<span id="S5.I16.i2.p1.1" class="ltx_p"><span id="S5.I16.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Uncertainty estimation</span></span>
</span></span>
<span id="S5.I16.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I16.i3.p1" class="ltx_para">
<span id="S5.I16.i3.p1.1" class="ltx_p"><span id="S5.I16.i3.p1.1.1" class="ltx_text" style="font-size:80%;">Generative models for sensor defects or new scenarios</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.1.10" class="ltx_tr">
<td id="S5.T1.1.10.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.10.1.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
<td id="S5.T1.1.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.10.2.1.1" class="ltx_p" style="width:69.7pt;"><span id="S5.T1.1.10.2.1.1.1" class="ltx_text" style="font-size:80%;">Model Evaluations</span></span>
</span>
</td>
<td id="S5.T1.1.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.10.3.1.1" class="ltx_p" style="width:156.5pt;">
<span id="S5.T1.1.10.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:130.1pt;">
<span id="S5.I17" class="ltx_itemize">
<span id="S5.I17.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I17.i1.p1" class="ltx_para">
<span id="S5.I17.i1.p1.1" class="ltx_p"><span id="S5.I17.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Different selected sub-dataset</span></span>
</span></span>
<span id="S5.I17.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I17.i2.p1" class="ltx_para">
<span id="S5.I17.i2.p1.1" class="ltx_p"><span id="S5.I17.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Unknown objects in the open world</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S5.T1.1.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.10.4.1.1" class="ltx_p" style="width:233.3pt;">
<span id="S5.T1.1.10.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="S5.I18" class="ltx_itemize">
<span id="S5.I18.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I18.i1.p1" class="ltx_para">
<span id="S5.I18.i1.p1.1" class="ltx_p"><span id="S5.I18.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Metrics related to uncertainties</span></span>
</span></span>
<span id="S5.I18.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I18.i2.p1" class="ltx_para">
<span id="S5.I18.i2.p1.1" class="ltx_p"><span id="S5.I18.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Developing visual toolkits for analyzing and optimizing fusion networks</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.1.11" class="ltx_tr">
<td id="S5.T1.1.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.11.1.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
<td id="S5.T1.1.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.11.2.1.1" class="ltx_p" style="width:69.7pt;"><span id="S5.T1.1.11.2.1.1.1" class="ltx_text" style="font-size:80%;">Model Deployment</span></span>
</span>
</td>
<td id="S5.T1.1.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.11.3.1.1" class="ltx_p" style="width:156.5pt;">
<span id="S5.T1.1.11.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:130.1pt;">
<span id="S5.I19" class="ltx_itemize">
<span id="S5.I19.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I19.i1.p1" class="ltx_para">
<span id="S5.I19.i1.p1.1" class="ltx_p"><span id="S5.I19.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Edge devices with limited computational resources</span></span>
</span></span>
<span id="S5.I19.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I19.i2.p1" class="ltx_para">
<span id="S5.I19.i2.p1.1" class="ltx_p"><span id="S5.I19.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Balancing the importance of different tasks</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S5.T1.1.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T1.1.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.11.4.1.1" class="ltx_p" style="width:233.3pt;">
<span id="S5.T1.1.11.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="S5.I20" class="ltx_itemize">
<span id="S5.I20.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I20.i1.p1" class="ltx_para">
<span id="S5.I20.i1.p1.1" class="ltx_p"><span id="S5.I20.i1.p1.1.1" class="ltx_text" style="font-size:80%;">Lightweight models and acceleration schemes (e.g., pruning and quantization)</span></span>
</span></span>
<span id="S5.I20.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I20.i2.p1" class="ltx_para">
<span id="S5.I20.i2.p1.1" class="ltx_p"><span id="S5.I20.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Fusion-based multi-task perception and panoptic perception</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
</table>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Multi-modal Data</span>
</h3>

<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS1.5.1.1" class="ltx_text">V-A</span>1 </span>Data Quality</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">Unlike uni-modal data, multi-modal data requires consideration of each modality’s native characteristics.
The information within an image is structured and regular, with partial information being associated with the whole image. In contrast, the spatial information embodied in radar point clouds tends to be disordered.
As a result, handling radar data poses a more significant challenge in the context of radar-camera fusion. We categorize these challenges related to data quality into three directions: sparsity, inaccuracy, and noise.</p>
</div>
<section id="S5.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Sparsity</h5>

<div id="S5.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.Px1.p1.1" class="ltx_p">The sparsity of radar point clouds poses a challenge for neural networks to learn features effectively. Besides, as these point clouds do not comprehensively represent an object’s shape, a fixed-size bounding box approach would be impractical.
To address the issue of sparsity, researchers usually combine multiple frames (from 0.25 seconds to 1 second) of radar data to get denser point clouds, making it conducive to improving accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib211" title="" class="ltx_ref">211</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib245" title="" class="ltx_ref">245</a>, <a href="#bib.bib213" title="" class="ltx_ref">213</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite>. However, the time-dependent approach also causes system delays.
Nowadays, the 4D radar sensor is a potential research direction as it produces denser point clouds, reaching hundreds of points on a car. The spatial distribution of objects is effectively represented in 4D radar datasets, including Astyx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>, VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite>.
Experiments also indicate that the 4D radar is helpful in radar detection. For example, Zheng <span id="S5.SS1.SSS1.Px1.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite> proved that 4D radar has potential in 3D perception as the points get dense. Palffy <span id="S5.SS1.SSS1.Px1.p1.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> pointed out that the additional elevation data increases object detection performance (from 31.9% to 38.0% in mAP) in their VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> dataset.</p>
</div>
</section>
<section id="S5.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Inaccuracy</h5>

<div id="S5.SS1.SSS1.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p1.1" class="ltx_p">Aside from the sparsity of radar point clouds, the points may not be located at the object’s center, but may be at any corner of the object or even outside of it <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>.
To make the radar points located on or close to the object, Chadwick <span id="S5.SS1.SSS1.Px2.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite> marked each radar point on the image as a small circle instead of a single pixel. Nabati and Qi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> proposed RRPN to generate several anchors with different sizes and aspect ratios centered at the points of interest. These translated anchors are employed to achieve more precise bounding boxes where the points of interest are mapped to the object’s right, left, or bottom.</p>
</div>
<div id="S5.SS1.SSS1.Px2.p2" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p2.2" class="ltx_p">Researchers have employed column and pillar expansion techniques to improve the accuracy of radar point clouds in the vertical dimension.
For example, Nobis <span id="S5.SS1.SSS1.Px2.p2.2.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite> assumed a height extension of three meters on each radar detection to associate camera pixels with radar data.
Nabati and Qi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> used pillar expansion to expand each radar point to a fixed-size pillar. In their experiment, the size of a pillar is set to <math id="S5.SS1.SSS1.Px2.p2.1.m1.3" class="ltx_Math" alttext="[0.2,0.2,1.5]" display="inline"><semantics id="S5.SS1.SSS1.Px2.p2.1.m1.3a"><mrow id="S5.SS1.SSS1.Px2.p2.1.m1.3.4.2" xref="S5.SS1.SSS1.Px2.p2.1.m1.3.4.1.cmml"><mo stretchy="false" id="S5.SS1.SSS1.Px2.p2.1.m1.3.4.2.1" xref="S5.SS1.SSS1.Px2.p2.1.m1.3.4.1.cmml">[</mo><mn id="S5.SS1.SSS1.Px2.p2.1.m1.1.1" xref="S5.SS1.SSS1.Px2.p2.1.m1.1.1.cmml">0.2</mn><mo id="S5.SS1.SSS1.Px2.p2.1.m1.3.4.2.2" xref="S5.SS1.SSS1.Px2.p2.1.m1.3.4.1.cmml">,</mo><mn id="S5.SS1.SSS1.Px2.p2.1.m1.2.2" xref="S5.SS1.SSS1.Px2.p2.1.m1.2.2.cmml">0.2</mn><mo id="S5.SS1.SSS1.Px2.p2.1.m1.3.4.2.3" xref="S5.SS1.SSS1.Px2.p2.1.m1.3.4.1.cmml">,</mo><mn id="S5.SS1.SSS1.Px2.p2.1.m1.3.3" xref="S5.SS1.SSS1.Px2.p2.1.m1.3.3.cmml">1.5</mn><mo stretchy="false" id="S5.SS1.SSS1.Px2.p2.1.m1.3.4.2.4" xref="S5.SS1.SSS1.Px2.p2.1.m1.3.4.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.Px2.p2.1.m1.3b"><list id="S5.SS1.SSS1.Px2.p2.1.m1.3.4.1.cmml" xref="S5.SS1.SSS1.Px2.p2.1.m1.3.4.2"><cn type="float" id="S5.SS1.SSS1.Px2.p2.1.m1.1.1.cmml" xref="S5.SS1.SSS1.Px2.p2.1.m1.1.1">0.2</cn><cn type="float" id="S5.SS1.SSS1.Px2.p2.1.m1.2.2.cmml" xref="S5.SS1.SSS1.Px2.p2.1.m1.2.2">0.2</cn><cn type="float" id="S5.SS1.SSS1.Px2.p2.1.m1.3.3.cmml" xref="S5.SS1.SSS1.Px2.p2.1.m1.3.3">1.5</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.Px2.p2.1.m1.3c">[0.2,0.2,1.5]</annotation></semantics></math> meters along the <math id="S5.SS1.SSS1.Px2.p2.2.m2.3" class="ltx_Math" alttext="[x,y,z]" display="inline"><semantics id="S5.SS1.SSS1.Px2.p2.2.m2.3a"><mrow id="S5.SS1.SSS1.Px2.p2.2.m2.3.4.2" xref="S5.SS1.SSS1.Px2.p2.2.m2.3.4.1.cmml"><mo stretchy="false" id="S5.SS1.SSS1.Px2.p2.2.m2.3.4.2.1" xref="S5.SS1.SSS1.Px2.p2.2.m2.3.4.1.cmml">[</mo><mi id="S5.SS1.SSS1.Px2.p2.2.m2.1.1" xref="S5.SS1.SSS1.Px2.p2.2.m2.1.1.cmml">x</mi><mo id="S5.SS1.SSS1.Px2.p2.2.m2.3.4.2.2" xref="S5.SS1.SSS1.Px2.p2.2.m2.3.4.1.cmml">,</mo><mi id="S5.SS1.SSS1.Px2.p2.2.m2.2.2" xref="S5.SS1.SSS1.Px2.p2.2.m2.2.2.cmml">y</mi><mo id="S5.SS1.SSS1.Px2.p2.2.m2.3.4.2.3" xref="S5.SS1.SSS1.Px2.p2.2.m2.3.4.1.cmml">,</mo><mi id="S5.SS1.SSS1.Px2.p2.2.m2.3.3" xref="S5.SS1.SSS1.Px2.p2.2.m2.3.3.cmml">z</mi><mo stretchy="false" id="S5.SS1.SSS1.Px2.p2.2.m2.3.4.2.4" xref="S5.SS1.SSS1.Px2.p2.2.m2.3.4.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.Px2.p2.2.m2.3b"><list id="S5.SS1.SSS1.Px2.p2.2.m2.3.4.1.cmml" xref="S5.SS1.SSS1.Px2.p2.2.m2.3.4.2"><ci id="S5.SS1.SSS1.Px2.p2.2.m2.1.1.cmml" xref="S5.SS1.SSS1.Px2.p2.2.m2.1.1">𝑥</ci><ci id="S5.SS1.SSS1.Px2.p2.2.m2.2.2.cmml" xref="S5.SS1.SSS1.Px2.p2.2.m2.2.2">𝑦</ci><ci id="S5.SS1.SSS1.Px2.p2.2.m2.3.3.cmml" xref="S5.SS1.SSS1.Px2.p2.2.m2.3.3">𝑧</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.Px2.p2.2.m2.3c">[x,y,z]</annotation></semantics></math> directions.
Notably, the column size should be different for different types of objects.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite>, the authors utilized a clustering approach to group ground truth bounding boxes of vehicles into three distinct height categories. Following this, radar points are assigned a scale based on boundary edge values in each category.</p>
</div>
<div id="S5.SS1.SSS1.Px2.p3" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p3.1" class="ltx_p">In our opinion, column or pillar expansion is effective but still hardly convincing. The distribution of point clouds is a direction worth investigating.
For example, Stäcker <span id="S5.SS1.SSS1.Px2.p3.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib255" title="" class="ltx_ref">255</a>]</cite> assumed a Gaussian distribution to measure the azimuth angle. According to the resulting Gaussian density curve, they generated denser radar point clouds, horizontally distributed over multiple pixels.</p>
</div>
</section>
<section id="S5.SS1.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Noise</h5>

<div id="S5.SS1.SSS1.Px3.p1" class="ltx_para">
<p id="S5.SS1.SSS1.Px3.p1.1" class="ltx_p">Actually, the radar sensor returns noisy data from irrelevant objects, including ghost objects, ground detections and even multi-radar mutual interference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib256" title="" class="ltx_ref">256</a>]</cite>. The noise from radars could cause the detection of fake targets, thereby limiting the accuracy of radar-based detection or segmentation.</p>
</div>
<div id="S5.SS1.SSS1.Px3.p2" class="ltx_para">
<p id="S5.SS1.SSS1.Px3.p2.1" class="ltx_p">Conventional methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib257" title="" class="ltx_ref">257</a>, <a href="#bib.bib258" title="" class="ltx_ref">258</a>, <a href="#bib.bib259" title="" class="ltx_ref">259</a>, <a href="#bib.bib260" title="" class="ltx_ref">260</a>]</cite> for automotive radar denoising are typically based on CFAR and peak detection algorithms, which exhibit poor generalization capabilities. Recently, deep learning methods have provided a key solution to the challenges associated with automotive radar data denoising.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib261" title="" class="ltx_ref">261</a>]</cite>, a deep neural network is proposed to enhance the target peaks on RA tensors. Rock <span id="S5.SS1.SSS1.Px3.p2.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib262" title="" class="ltx_ref">262</a>]</cite> analyzed the quantization of CNN-based denoising autoencoder for radar interference mitigation on radar RD tensors to guarantee real-time inference on low-performance equipment.
Dubey <span id="S5.SS1.SSS1.Px3.p2.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib263" title="" class="ltx_ref">263</a>]</cite> realized the multi-radar mutual interference and object detection on RD tensors simultaneously with one one-stage CNN-based neural network.
Moreover, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib264" title="" class="ltx_ref">264</a>, <a href="#bib.bib265" title="" class="ltx_ref">265</a>, <a href="#bib.bib266" title="" class="ltx_ref">266</a>, <a href="#bib.bib267" title="" class="ltx_ref">267</a>, <a href="#bib.bib268" title="" class="ltx_ref">268</a>]</cite>, fully-convolution networks are widely proposed and applied to interference mitigation on RD tensors.</p>
</div>
<div id="S5.SS1.SSS1.Px3.p3" class="ltx_para">
<p id="S5.SS1.SSS1.Px3.p3.1" class="ltx_p">However, conventional radars generally do not provide access to the radar tensor, highlighting the importance of noise mitigation techniques at the level of radar point clouds.
For example, Nobis <span id="S5.SS1.SSS1.Px3.p3.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite> designed a ground-truth filter to remove radar detections outside of the 3D ground truth bounding boxes. Cheng <span id="S5.SS1.SSS1.Px3.p3.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib269" title="" class="ltx_ref">269</a>]</cite> proposed a cross-modal radar point detector through the assistance of LiDAR, which could also remove the noisy points.
Essentially, noise mitigation at the point cloud level is semantic segmentation of the point cloud, whereby a semantic segmentation model is leveraged to assign a category to each point.
Numerous studies focus on point cloud segmentation, including PointMLP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib270" title="" class="ltx_ref">270</a>]</cite>, PointNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib271" title="" class="ltx_ref">271</a>]</cite>, Point Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib272" title="" class="ltx_ref">272</a>]</cite> and Point Cloud Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib273" title="" class="ltx_ref">273</a>]</cite>. Notably, Point Transformer and Point Cloud Transformer introduce the self-attention mechanism within their point cloud processing networks to capture contextual features. However, it is worth noting that these approaches still rely on the advantages provided by two essential modules derived from PointNet++: Set Abstraction (SA) and Multi-Scale Grouping (MSG).
Recently, a revolutionary non-parametric point cloud segmentation model called Point-NN has been proposed. Point-NN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib274" title="" class="ltx_ref">274</a>]</cite> elegantly assembles farthest point sampling, K nearest neighbor, pooling, trigonometric position encodings and similarity measurement, thereby achieving SOTA performances on several benchmarks with superior performance to any other point cloud processing models.</p>
</div>
<div id="S5.SS1.SSS1.Px3.p4" class="ltx_para">
<p id="S5.SS1.SSS1.Px3.p4.1" class="ltx_p">In all, regardless of the stage of denoising in radar tensors or point clouds, a dataset with high-quality annotations is necessary. For the noise removal on radar tensors, researchers may adopt image denoising and restoration principles. For removing noisy point clouds, constructing features based on the physical characteristics of point clouds to guide models separate targets from clutters makes sense. As radar point clouds are sparse and inaccurate, modeling the inherent uncertainty is an open question, which can aid in effectively distinguishing targets from noise.</p>
</div>
</section>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS2.5.1.1" class="ltx_text">V-A</span>2 </span>Data Diversity</h4>

<section id="S5.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Small Size</h5>

<div id="S5.SS1.SSS2.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS2.Px1.p1.1" class="ltx_p">Deep learning models rely on large amounts of training data to achieve high levels of accuracy.
However, multi-modal datasets consisting of both radar and camera data are much smaller than uni-modal image data. For instance, compared to the ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib275" title="" class="ltx_ref">275</a>, <a href="#bib.bib276" title="" class="ltx_ref">276</a>]</cite> dataset with over 14 million images and over 20k classes, the largest radar-camera fusion dataset to date named CRUW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite> has only 400k frames and 260k objects.
Furthermore, regarding category distribution, most labels are vehicles, while pedestrians and bicycles are far less prevalent. The imbalance in these categories’ distribution may result in overfitting designed deep learning networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Insufficient Conditions</h5>

<div id="S5.SS1.SSS2.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.Px2.p1.1" class="ltx_p">In real scenarios, 360-degree perception of the surrounding environment is critical in autonomous driving, requiring multiple cameras and radars to work together. Besides, the multi-modal dataset also needs to consider complex weather conditions (e.g., rain, fog, snow) and complex road conditions (e.g., blocked roads, rural paths, intersections), all of which are time-consuming and labor-consuming tasks.</p>
</div>
<div id="S5.SS1.SSS2.Px2.p2" class="ltx_para">
<p id="S5.SS1.SSS2.Px2.p2.1" class="ltx_p">Some studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite> generate synthetic data via simulation tools (e.g., Carla <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>).
Researchers can freely match different sensors and generate different driving conditions with these tools, especially in complex and dangerous scenarios.
However, it is also worth noting that although simulators can generate a variety of virtual datasets, the simulated data cannot completely replace the data from real scenarios. Moreover, exploring the appropriate methodology for integrating synthetic data with real-world data is a critical area of inquiry warranting further investigation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib277" title="" class="ltx_ref">277</a>]</cite>.</p>
</div>
<div id="S5.SS1.SSS2.Px2.p3" class="ltx_para">
<p id="S5.SS1.SSS2.Px2.p3.1" class="ltx_p">Domain adaptation is also a valuable research direction that aims to leverage knowledge learned from a related domain with adequate labeled data. Although domain adaptation has been applied in radar data, including radar tensor reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib278" title="" class="ltx_ref">278</a>]</cite>, human sensing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib279" title="" class="ltx_ref">279</a>]</cite>, human activity recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib280" title="" class="ltx_ref">280</a>]</cite> and gesture recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib281" title="" class="ltx_ref">281</a>]</cite>, it has not been employed in radar-camera fusion till now.</p>
</div>
</section>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS3.5.1.1" class="ltx_text">V-A</span>3 </span>Data Synchronization</h4>

<section id="S5.SS1.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">High Calibration Requirements</h5>

<div id="S5.SS1.SSS3.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS3.Px1.p1.1" class="ltx_p">For radar-camera fusion systems, well-calibrated sensors are the prerequisite.
In multi-sensor calibration, LiDAR sensors are typically employed as an essential intermediary component. The LiDAR sensor is calibrated separately from the camera sensor and radar sensor, and then a transformation matrix between the radar and camera can be calculated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>, <a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite>.
Although numerous approaches (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib282" title="" class="ltx_ref">282</a>, <a href="#bib.bib195" title="" class="ltx_ref">195</a>, <a href="#bib.bib283" title="" class="ltx_ref">283</a>]</cite>) are proposed for calibration between radars and the cameras, the accuracy of the calibration remains a challenge due to the inaccurate and vulnerable radar returns.
Besides, as far as we know, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite> are the only methods for 4D radar and camera calibration. As 4D radar technologies are developing rapidly, we believe that 4D radar calibration is a potential direction, and more finds will be proposed in the future.</p>
</div>
<div id="S5.SS1.SSS3.Px1.p2" class="ltx_para">
<p id="S5.SS1.SSS3.Px1.p2.1" class="ltx_p">In real scenarios, extrinsic calibration parameters between radar and camera sensors may change from vehicle vibration. Besides, different sampling frequencies of the radar and camera may produce a particular temporal difference between the data from each sensor. The temporal difference would cause data inconsistency, especially when the ego-car or targets move at high speed. Therefore, real-time online calibration and correction are essential research directions in the future.</p>
</div>
</section>
<section id="S5.SS1.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Difficulties in Labeling</h5>

<div id="S5.SS1.SSS3.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS3.Px2.p1.1" class="ltx_p">The process of labeling data is labor-intensive and time-consuming, especially when dealing with multi-modal data. This is particularly true for radar-camera fusion, where the physical shapes of objects cannot be discerned directly from the radar data representation.
Auto-labeling radar data is a potential research direction to address the challenge of laborious data labeling. Actually, labels for radar data can be calculated based on the corresponding ground truth from camera images and the extrinsic matrix between the radar sensor and the camera sensor. But the problem is that applying this labeling approach for radar data is not perfect, as radar targets may not always be located in the ground truth from images.
Sengupta <span id="S5.SS1.SSS3.Px2.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib284" title="" class="ltx_ref">284</a>]</cite> proposed a camera-aided method for automatically labeling radar point clouds, leveraging a pre-trained YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> network and the Hungarian algorithm for enhanced accuracy and efficiency.
However, despite the potential advantages of auto-labeling radar data, filtering out noisy data around the object of interest is still challenging.</p>
</div>
<div id="S5.SS1.SSS3.Px2.p2" class="ltx_para">
<p id="S5.SS1.SSS3.Px2.p2.1" class="ltx_p">For camera image labeling, it is worth considering how to select appropriate labeling data for reducing labor costs.
Active learning is a supervised learning method that aims to select the smallest possible training set to achieve the desired data efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib285" title="" class="ltx_ref">285</a>, <a href="#bib.bib286" title="" class="ltx_ref">286</a>]</cite>. The active learning network iteratively queries the most informative samples from the human labelers in an unlabeled data pool and then updates the weights for the network. This approach leads to equivalent performance with less labeled training data, reducing human labeling efforts.
Experiments from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib287" title="" class="ltx_ref">287</a>]</cite> indicate that using only about 40% of the data in the training set leads to the same classification results as the completely supervised reference experiment.
Furthermore, many other methods would also be used to reduce the burden of data labeling, such as domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib288" title="" class="ltx_ref">288</a>, <a href="#bib.bib289" title="" class="ltx_ref">289</a>, <a href="#bib.bib278" title="" class="ltx_ref">278</a>]</cite>, transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib290" title="" class="ltx_ref">290</a>]</cite>, semi-supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib291" title="" class="ltx_ref">291</a>]</cite> and lifelong learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib292" title="" class="ltx_ref">292</a>]</cite>.</p>
</div>
</section>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Multi-modal Fusion</span>
</h3>

<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS1.5.1.1" class="ltx_text">V-B</span>1 </span>Feature Extraction</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">Applying LiDAR-based feature extraction algorithms to radar modality is less effective due to the inherent sparsity of radar point clouds.
As an example, PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib293" title="" class="ltx_ref">293</a>]</cite> algorithm converts LiDAR point clouds into pillars and then extracts features from each pillar. When this algorithm is adapted to radar point clouds, there may be few or even no points in a pillar, which makes it hard to extract features. In fact, results in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib294" title="" class="ltx_ref">294</a>]</cite> also indicate that the average precision of radar point clouds using PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib293" title="" class="ltx_ref">293</a>]</cite> is much lower than using SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> and YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> detectors.</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p">As PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib293" title="" class="ltx_ref">293</a>]</cite> focus on local features, the attention mechanism is a potential research direction to extract global features to improve accuracy. For example, Radar Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib295" title="" class="ltx_ref">295</a>]</cite> incorporates both vector attention and scalar attention mechanisms to effectively leverage spatial information, Doppler information, and reflection intensity information from radar point clouds. By integrating local attention features and global attention features, Radar Transformer achieves deep integration of radar information.
RPFA-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib296" title="" class="ltx_ref">296</a>]</cite> leverages the self-attention mechanism to extract global features (e.g., orientation) from point clouds. These global features enable the network to perform more efficient and effective regression of key object parameters (e.g., heading angle), thereby enhancing the accuracy and reliability of object detection.
In addition, Gaussian Radar Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> employs attentive upsampling and downsampling modules to enlarge the receptive field and capture distinctive spatial correlations, effectively addressing the challenge of capturing long-range dependencies in radar data.
The attention-based techniques and multi-task learning used in HARadNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> also lead to a significant performance improvement in the classification and detection.</p>
</div>
<div id="S5.SS2.SSS1.p3" class="ltx_para">
<p id="S5.SS2.SSS1.p3.1" class="ltx_p">To dig deeper into the relationship between sparse point clouds, GNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib297" title="" class="ltx_ref">297</a>]</cite> is a promising research direction in which each point is considered as a node, and edges are the relationship between the points.
In Radar-PointGNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, GNN adopted for feature extraction of radar point clouds demonstrates that the graph representation produces more effective object proposals than other point cloud encoders by mapping radar point clouds to contextual representations. RadarGNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> indicates that GNNs can operate on unstructured and unordered data, obtaining both point features and point-pair features embedded in the edges of the graph. Thus, compared to voxelization operations, GNN eliminates the information loss from the sparse radar point clouds.
GNN also shows its advantages in detection from RA tensors. The Graph Tensor Radar Network (GTR-Net) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib298" title="" class="ltx_ref">298</a>]</cite> architecture utilizes graph convolutional operations to aggregate information across the point cloud nodes. The process involves weighting the features of connected nodes based on their respective edge weights. In this way, it improves the defective sparse points by aggregating relevant information and thus leads to better performance.</p>
</div>
<div id="S5.SS2.SSS1.p4" class="ltx_para">
<p id="S5.SS2.SSS1.p4.1" class="ltx_p">Another potential research opportunity is using neural networks to extract radar information instead of traditional FFT operations, which can reduce the computational requirements that consume most of the operations and simplify the data flow in the embedded implementation.
For example, in RODNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, FFT operations are only performed in sample and antenna dimensions, while the chirp dimension remains to get the range-azimuth-chirp tensor. Then a neural network is employed to process the chirp dimension for extracting Doppler features, enabling end-to-end training of radar features in-depth within the deep learning framework.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS2.5.1.1" class="ltx_text">V-B</span>2 </span>Data Association</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">Another significant challenge is the ambiguity in associating radar data with image data, as they are heterogeneous. The typical way is to project radar data onto the image plane and then bind the data in the same position through a calibration matrix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib299" title="" class="ltx_ref">299</a>]</cite>.
However, direct projection results in poor association with the objects’ centers. As aforementioned, radar data is sparse, inaccurate and noisy, making poor association at either the object-level or data-level fusion.</p>
</div>
<div id="S5.SS2.SSS2.p2" class="ltx_para">
<p id="S5.SS2.SSS2.p2.1" class="ltx_p">Thus, associating image data with radar data is an open question.
Nabati and Qi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref">194</a>]</cite> proposed a Radar Proposal Refinement (RPR) network to match proposals from radars and cameras.
Later, they integrated the detection boxes and pillar expansion through frustum association in CenterFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, allowing for the mapping of radar detections to the centers of objects and mitigating the issue of overlapping.
Dong <span id="S5.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib200" title="" class="ltx_ref">200</a>]</cite> used AssociationNet to learn the semantic representation information and associate radar point clouds and image bounding boxes by Euclidean distance.
For associating the semantics to radar point clouds, Bansal <span id="S5.SS2.SSS2.p2.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>]</cite> proposed a representation named Semantic-Point-Grid (SPG), which encodes semantic information from camera images into radar point clouds to identify camera pixel correspondences for each radar point.</p>
</div>
<div id="S5.SS2.SSS2.p3" class="ltx_para">
<p id="S5.SS2.SSS2.p3.1" class="ltx_p">In our opinion, a potential approach to associate radar data with image data is the attention-based association with adaptive thresholds. For example, Radar-Camera Pixel Depth Association (RC-PDA) is proposed to filter out occluded radar returns and enhance the projected radar depth map by generating a confidence measure for these associations in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib213" title="" class="ltx_ref">213</a>]</cite>.
Soft Polar Association (SPA) is proposed to associate radar point clouds around the image proposals in polar coordinates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib248" title="" class="ltx_ref">248</a>]</cite>. In order to overcome background clutter, it utilized consecutive cross attention-based encoder layers to integrate image proposal features and radar point features.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS3.5.1.1" class="ltx_text">V-B</span>3 </span>Data Augmentation</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">Numerous data augmentation methods have been proposed to increase the quantity and diversity of data samples, thus preventing network overfitting and enhancing model generalization.
For radar data in the form of point clouds, data augmentations such as random rotation, scaling, and flipping shifting are applied to enrich the diversity of samples in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib176" title="" class="ltx_ref">176</a>, <a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite>.
In addition, since radar tensors can be treated as images, existing image-based data augmentation algorithms (e.g., horizontal flipping, translating in range, interpolating, mixing) are tested in experiments and proved to be effective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>.</p>
</div>
<div id="S5.SS2.SSS3.p2" class="ltx_para">
<p id="S5.SS2.SSS3.p2.1" class="ltx_p">However, all these data augmentation methods above are based only on the radar modality. In radar-camera fusion perception, designing effective data augmentation methods needs to consider the correlation and interdependence between radar and camera modalities, which means joint data augmentation methods are necessary rather than augmenting each modality separately. Otherwise, the model will learn from incorrect data, whose physical properties are unreliable. For example, when the coordinates of radar data and image are aligned, applying Cutmix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib300" title="" class="ltx_ref">300</a>]</cite> on the image and radar feature maps will undoubtedly destroy the target features (e.g., azimuth and elevation) obtained by the radar sensor, leading to incorrect model inferences. Therefore, designing joint data augmentation algorithms for the unique radar representations combined with image modality remains a significant challenge.</p>
</div>
</section>
<section id="S5.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS4.5.1.1" class="ltx_text">V-B</span>4 </span>Training Strategies</h4>

<div id="S5.SS2.SSS4.p1" class="ltx_para">
<p id="S5.SS2.SSS4.p1.1" class="ltx_p">Since a multi-modal network has additional input information, it should match or outperform the uni-modal network. However, this is not always the case. A multi-modal network is often prone to overfitting and tends to learn to ignore one branch if the hyper-parameters set for training are more suitable for the other branch.
Wang <span id="S5.SS2.SSS4.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib301" title="" class="ltx_ref">301</a>]</cite> argued that the rates of overfitting and generalization vary across different modalities, and training a multi-modal network using the uni-modal training strategy may not be optimal for the overall network.</p>
</div>
<div id="S5.SS2.SSS4.p2" class="ltx_para">
<p id="S5.SS2.SSS4.p2.1" class="ltx_p">A feasible approach to balance the performance is to add loss functions for each modality. In this way, after one modality converges, the remaining modality can still be generalized.
Besides, weighting operations on loss functions could be more beneficial to adapt to the learning rates of each modality.
In recent studies, Wang <span id="S5.SS2.SSS4.p2.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib301" title="" class="ltx_ref">301</a>]</cite> proposed Gradient-Blending, which computes an optimal blending of modalities based on their overfitting behaviors. Although this method achieves SOTA accuracy on audio and vision benchmarks, the idea has yet to be applied in radar and camera modalities.
Moreover, dropout operation helps overcome overfitting. Nobis <span id="S5.SS2.SSS4.p2.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite> introduced BlackIn by deactivating camera image data. The lack of camera input data forces the network to rely more on sparse radar data for specific potential values.</p>
</div>
<div id="S5.SS2.SSS4.p3" class="ltx_para">
<p id="S5.SS2.SSS4.p3.1" class="ltx_p">Fine-tuning a multi-modal network over pre-trained uni-modal encoders can also outperform fusion from scratch. Lim <span id="S5.SS2.SSS4.p3.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib206" title="" class="ltx_ref">206</a>]</cite> utilized the weights freezing strategy to train a single branch network using the optimal training hyper-parameters. These weights were subsequently loaded into the corresponding branches to train the fusion network. Experimental results indicate that the best strategy is to train the camera branch in advance and then train the entire network with the gradient propagation disabled through the camera branch.
Recently, knowledge distillation has shown its performance in multi-modal networks by distilling the pre-trained uni-modal features to the multi-modal networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib302" title="" class="ltx_ref">302</a>, <a href="#bib.bib303" title="" class="ltx_ref">303</a>]</cite>. It could also be a potential research direction in radar-camera fusion.</p>
</div>
</section>
<section id="S5.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS5.5.1.1" class="ltx_text">V-B</span>5 </span>Model Robustness</h4>

<div id="S5.SS2.SSS5.p1" class="ltx_para">
<p id="S5.SS2.SSS5.p1.1" class="ltx_p">Another challenge is how to guarantee the model’s robustness when the sensors are degraded, or the autonomous driving vehicles enter into adverse or unseen driving scenarios.
Most reviewed methods focus on the accuracy of public datasets, while only a few consider sensor failure with only one modality as the input data.
In RadSegNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>]</cite>, the SPG encoding independently extracts information from cameras and radar, as well as encodes semantic information from camera images into radar point clouds. Thus in scenarios where the camera input becomes unreliable, the SPG encoding method maintains reliable operation using radar data alone.
Bijelic <span id="S5.SS2.SSS5.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> introduced an entropy channel for each sensor stream and a feature fusion architecture to exchange features, which still work in unseen weather conditions and sensor failures.
Moreover, the attention mechanism is also an effective choice for guiding mixed information from different sensors to fuse features of multiple modalities, as well as handle original features from a single modality. For example, attention maps leverage features learned from different sensors to predict the importance of specific parameters in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib304" title="" class="ltx_ref">304</a>]</cite>.</p>
</div>
<div id="S5.SS2.SSS5.p2" class="ltx_para">
<p id="S5.SS2.SSS5.p2.1" class="ltx_p">It is essential to focus not only on the accuracy of the predicted outcomes, but also on the degree of certainty the model has about them.
Uncertainty is a potential direction that can be used to handle unseen driving scenarios. Specifically, a multi-modal network should present higher uncertainty against unseen objects. The Bayesian neural network is a valuable choice for calculating uncertainty. It utilizes a prior distribution of network weights to infer the posterior distribution, thereby estimating the probability associated with a given prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib305" title="" class="ltx_ref">305</a>]</cite>.
In radar-camera fusion, YOdar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite> is an uncertainty-based method in which uncertainty combines outputs of radar and camera networks with a gradient-boosting classifier. Experimental results show that YOdar increases performance significantly at night scenes.</p>
</div>
<div id="S5.SS2.SSS5.p3" class="ltx_para">
<p id="S5.SS2.SSS5.p3.1" class="ltx_p">Another way that may be useful to increase the networks’ robustness is generative models. They can detect sensor defects or new scenarios an autonomous vehicle has never entered.
Wheeler <span id="S5.SS2.SSS5.p3.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib306" title="" class="ltx_ref">306</a>]</cite> described a methodology for constructing stochastic automotive radar models based on deep learning with adversarial loss connected to real-world data. The resulting model exhibits fundamental radar effects while maintaining real-time capability.
Lekic and Babic  <span id="S5.SS2.SSS5.p3.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib307" title="" class="ltx_ref">307</a>]</cite> introduced a Conditional Multi-Generator Generative Adversarial Network (CMGGAN) to generate pseudo-images containing all the surrounding objects detected by the radar sensor.
In our opinion, designing specific deep generative models for radar-camera fusion is an interesting open question.</p>
</div>
</section>
<section id="S5.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS6.5.1.1" class="ltx_text">V-B</span>6 </span>Model Evaluations</h4>

<div id="S5.SS2.SSS6.p1" class="ltx_para">
<p id="S5.SS2.SSS6.p1.1" class="ltx_p">Most researchers utilize the nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> dataset to evaluate the performance of their algorithms.
However, the selected sub-dataset and the evaluation metrics are different, leading to a lack of direct comparisons. As summarized in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:evaluation</span>, some methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib199" title="" class="ltx_ref">199</a>, <a href="#bib.bib214" title="" class="ltx_ref">214</a>, <a href="#bib.bib197" title="" class="ltx_ref">197</a>, <a href="#bib.bib243" title="" class="ltx_ref">243</a>]</cite> use portions of the nuScenes dataset for training, validating and testing, while some others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib194" title="" class="ltx_ref">194</a>, <a href="#bib.bib245" title="" class="ltx_ref">245</a>, <a href="#bib.bib255" title="" class="ltx_ref">255</a>]</cite> exploit data collected by part of sensors within the nuScenes dataset. In addition, some researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>, <a href="#bib.bib190" title="" class="ltx_ref">190</a>, <a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite> do not clarify which section of the nuScenes dataset they utilized in their experiments.</p>
</div>
<div id="S5.SS2.SSS6.p2" class="ltx_para">
<p id="S5.SS2.SSS6.p2.1" class="ltx_p">In terms of evaluation metrics, even though some studies provide results using AP and mAP metrics, the type and value of the threshold are different. Besides, only a few works provide information on the inference time, which is also calculated by authors on their own devices and lacks uniform hardware measurements.
In our opinion, since the nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> dataset has been used to evaluate the performance of major algorithms, researchers should validate the performance of their algorithms on the same IoU, metrics and sub-dataset. This would enable a more meaningful and direct comparison of the results obtained from various studies.</p>
</div>
<div id="S5.SS2.SSS6.p3" class="ltx_para">
<p id="S5.SS2.SSS6.p3.1" class="ltx_p">Moreover, standard evaluation metrics are not specifically designed for situations where sensors are defective. Metrics related to uncertainties, such as Probability-based Detection Quality (PDQ) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib308" title="" class="ltx_ref">308</a>]</cite>, may be helpful in radar-camera fusion to compare the robustness of different algorithms.
Radar-camera fusion also faces the challenges of unknown objects in the open world. In such scenarios, evaluation metrics proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib309" title="" class="ltx_ref">309</a>, <a href="#bib.bib310" title="" class="ltx_ref">310</a>]</cite> can be utilized for open-set objects in radar-camera fusion.</p>
</div>
<div id="S5.SS2.SSS6.p4" class="ltx_para">
<p id="S5.SS2.SSS6.p4.1" class="ltx_p">Furthermore, visualization evaluation techniques are a potential research direction for analyzing and optimizing radar-camera fusion networks. Several methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib311" title="" class="ltx_ref">311</a>, <a href="#bib.bib312" title="" class="ltx_ref">312</a>]</cite> have been proposed for interpreting and understanding deep neural networks. However, to the best of our knowledge, there has yet to be an investigation of visual analytics in radar-camera fusion. How to design the visual toolkits for radar-camera fusion networks is still an open and challenging question.</p>
</div>
</section>
<section id="S5.SS2.SSS7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS7.5.1.1" class="ltx_text">V-B</span>7 </span>Model Deployment</h4>

<div id="S5.SS2.SSS7.p1" class="ltx_para">
<p id="S5.SS2.SSS7.p1.1" class="ltx_p">Radar-camera fusion holds significant potential in practical autonomous driving vehicles, where models of radar-camera fusion are deployed on edge devices.
Compared with high computational servers, edge devices are often equipped with limited computational resources in memory, bandwidth, Graphics Processing Unit (GPU) and Central Processing Unit (CPU). Nevertheless, they still need to meet the low-latency and high-performance requirements. Currently,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib313" title="" class="ltx_ref">313</a>]</cite> is the only work that reports on fusion output speed, reaching 11 Hz on an NVIDIA Jetson AGX TX2.
The results of fusion algorithms on edge devices are an open question, and how to improve the computational efficiency is worth considering. Some network acceleration schemes (e.g., pruning and quantization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib314" title="" class="ltx_ref">314</a>, <a href="#bib.bib315" title="" class="ltx_ref">315</a>]</cite>) are good choices to be applied to radar-camera fusion models.</p>
</div>
<div id="S5.SS2.SSS7.p2" class="ltx_para">
<p id="S5.SS2.SSS7.p2.1" class="ltx_p">It is valuable to implement multiple tasks in a uni-model for real applications. In multi-task learning, the knowledge learned during training for one task can be shared and used to improve performance on the other tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Besides, by sharing model features between multiple tasks, the overall number of parameters and computations can be reduced, making it more efficient in real-time autonomous driving applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref">194</a>, <a href="#bib.bib316" title="" class="ltx_ref">316</a>]</cite>.
Multi-task in radar-camera fusion is still in the preliminary stage, and we believe the multi-task approach in radar-camera fusion is a potential research direction. Nonetheless, combining multiple tasks into a unified optimization objective results in a complex optimization problem, especially when the tasks are related but have different performance metrics. Finding a set of hyper-parameters that can effectively balance the importance of different tasks is challenging.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">With the rapid development of autonomous driving, radar-camera fusion, a multi-modal and all-weather solution, is gaining more attention in both academic research and industrial applications. This review investigates and discusses radar-camera fusion studies on object detection and semantic segmentation tasks.
Starting with the working principles of radar and camera sensors, we gradually introduce the importance of radar-camera fusion in autonomous driving perception.
Through the analysis of radar signal processing, we gain a deep understanding of radar representations, which also provides fundamental support for the radar-camera fusion datasets.
As to fusion methodologies, we delve into various fusion methods and explore questions about “why to fuse”, “what to fuse”, “where to fuse”, “when to fuse” and “how to fuse”.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Based on the current radar-camera fusion datasets and methods, we discuss the critical challenges and raise possible research directions involving multi-modal data and multi-modal fusion.
In general, radar-camera fusion is moving towards data representations containing rich information. On the one hand, representations such as ADC signals and radar tensors provide more potential information, which is valuable for multi-modal fusion. On the other hand, the new 4D radar sensors provide denser point clouds and higher resolutions, which will become a new trend in autonomous driving.
Fusion approaches are evolving towards customizing radar algorithms based on particular radar characteristics. Additionally, methods on multi-frames and multi-tasks in radar-camera fusion are expected in future works.
Above all, we hope that our survey serves as a comprehensive reference for researchers and practitioners in developing robust perception in radar-camera fusion.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research was funded by the Suzhou Municipal Key Laboratory for Intelligent Virtual Engineering (SZS2022004), the Key Programme Special Fund of XJTLU (KSF-A-19), the Suzhou Science and Technology Project (SYG202122), the Research Development Fund of XJTLU (RDF-19-02-23) and Jiangsu Engineering Research Center for Data Science and Cognitive Computing.
This work received financial support from Jiangsu Industrial Technology Research Institute (JITRI) and Wuxi National Hi-Tech District (WND).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. Paden, M. Čáp, S. Z. Yong, D. Yershov, and E. Frazzoli, “A survey
of motion planning and control techniques for self-driving urban vehicles,”
<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Vehicles</em>, vol. 1, no. 1, pp. 33–55,
2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y. Xiao, F. Codevilla, A. Gurram, O. Urfalioglu, and A. M. López,
“Multimodal end-to-end autonomous driving,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Intelligent Transportation Systems</em>, vol. 23, no. 1, pp. 537–547, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D. Feng, C. Haase-Schütz, L. Rosenbaum, H. Hertlein, C. Glaeser, F. Timm,
W. Wiesbeck, and K. Dietmayer, “Deep multi-modal object detection and
semantic segmentation for autonomous driving: Datasets, methods, and
challenges,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>,
vol. 22, no. 3, pp. 1341–1360, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Liang, B. Yang, Y. Chen, R. Hu, and R. Urtasun, “Multi-task multi-sensor
fusion for 3d object detection,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 7345–7353.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for
autonomous driving,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition</em>, 2020, pp. 11 621–11 631.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object detection
network for autonomous driving,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition</em>, 2017, pp. 1907–1915.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander, “Joint 3d
proposal generation and object detection from view aggregation,” in
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</em>.   IEEE, 2018, pp. 1–8.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
B. Li, T. Zhang, and T. Xia, “Vehicle detection from 3d lidar using fully
convolutional network,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1608.07916</em>, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R. Nabati and H. Qi, “Rrpn: Radar region proposal network for object detection
in autonomous vehicles,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on
Image Processing (ICIP)</em>.   IEEE, 2019,
pp. 3093–3097.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. Zhang, S. A. Candra, K. Vetter, and A. Zakhor, “Sensor fusion for semantic
segmentation of urban scenes,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2015 IEEE international conference
on robotics and automation (ICRA)</em>.   IEEE, 2015, pp. 1850–1857.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R. Yin, Y. Cheng, H. Wu, Y. Song, B. Yu, and R. Niu, “Fusionlane: Multi-sensor
fusion for lane marking semantic segmentation using deep neural networks,”
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 23,
no. 2, pp. 1543–1553, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Asvadi, P. Girao, P. Peixoto, and U. Nunes, “3d object tracking using rgb
and lidar data,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2016 IEEE 19th International Conference on
Intelligent Transportation Systems (ITSC)</em>.   IEEE, 2016, pp. 1255–1260.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y. Fang, H. Zhao, H. Zha, X. Zhao, and W. Yao, “Camera and lidar fusion for
on-road vehicle tracking with reinforcement learning,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2019 IEEE
Intelligent Vehicles Symposium (IV)</em>.   IEEE, 2019, pp. 1723–1730.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
K. Yoneda, N. Suganuma, R. Yanase, and M. Aldibaja, “Automated driving
recognition technologies for adverse weather conditions,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IATSS
research</em>, vol. 43, no. 4, pp. 253–262, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
P. Li, P. Wang, K. Berntorp, and H. Liu, “Exploiting temporal relations on
radar perception for autonomous driving,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp.
17 071–17 080.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
B. Major, D. Fontijne, A. Ansari, R. T. Sukhavasi, R. Gowaikar, M. Hamilton,
S. Lee, S. Grzechnik, and S. Subramanian, “Vehicle detection with automotive
radar using deep learning on range-azimuth-doppler tensors,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2019
IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</em>.   IEEE, 2019, pp. 924–932.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y. Wang, Z. Jiang, Y. Li, J.-N. Hwang, G. Xing, and H. Liu, “Rodnet: A
real-time radar object detection network cross-supervised by camera-radar
fused object 3d localization,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in
Signal Processing</em>, vol. 15, no. 4, pp. 954–967, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
R. Nabati and H. Qi, “Centerfusion: Center-based radar and camera fusion for
3d object detection,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference
on Applications of Computer Vision</em>, 2021, pp. 1527–1536.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby, and
A. Mouzakitis, “A survey on 3d object detection methods for autonomous
driving applications,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation
Systems</em>, vol. 20, no. 10, pp. 3782–3795, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Cui, R. Chen, W. Chu, L. Chen, D. Tian, Y. Li, and D. Cao, “Deep learning
for image and point cloud fusion in autonomous driving: A review,”
<em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 23,
no. 2, pp. 722–739, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
K. Huang, B. Shi, X. Li, X. Li, S. Huang, and Y. Li, “Multi-modal sensor
fusion for auto driving perception: A survey,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2202.02703</em>, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. Jusoh and S. Almajali, “A systematic review on fusion techniques and
approaches used in applications,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp.
14 424–14 439, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Z. Wang, Y. Wu, and Q. Niu, “Multi-sensor fusion in automated driving: A
survey,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Ieee Access</em>, vol. 8, pp. 2847–2868, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Z. Wei, F. Zhang, S. Chang, Y. Liu, H. Wu, and Z. Feng, “Mmwave radar and
vision fusion for object detection in autonomous driving: A review,”
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 22, no. 7, p. 2542, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
C. Iovescu and S. Rao, “The fundamentals of millimeter wave sensors,”
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Texas Instruments</em>, pp. 1–8, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
R. Appleby and R. N. Anderton, “Millimeter-wave and submillimeter-wave imaging
for security and surveillance,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, vol. 95,
no. 8, pp. 1683–1690, 2007.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
P. Fritsche, S. Kueppers, G. Briese, and B. Wagner, “Radar and lidar
sensorfusion in low visibility environments.” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ICINCO (2)</em>, 2016,
pp. 30–36.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
V. John and S. Mita, “Rvnet: Deep sensor fusion of monocular camera and radar
for image-based obstacle detection in challenging environments,” in
<em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Image and Video Technology: 9th Pacific-Rim Symposium, PSIVT 2019,
Sydney, NSW, Australia, November 18–22, 2019, Proceedings 9</em>.   Springer, 2019, pp. 351–364.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Y. Long, D. Morris, X. Liu, M. Castro, P. Chakravarty, and P. Narayanan,
“Full-velocity radar returns by radar-camera fusion,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp.
16 198–16 207.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y. Li, J. Deng, Y. Zhang, J. Ji, H. Li, and Y. Zhang, “Ezfusion: A close look
at the integration of lidar, millimeter-wave radar, and camera for accurate
3d object detection and tracking,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation
Letters</em>, vol. 7, no. 4, pp. 11 182–11 189, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
H. Griffiths, L. Cohen, S. Watts, E. Mokole, C. Baker, M. Wicks, and S. Blunt,
“Radar spectrum engineering and management: Technical and regulatory
issues,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, vol. 103, no. 1, pp. 85–102, 2014.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. Kopp, D. Kellner, A. Piroli, and K. Dietmayer, “Tackling clutter in radar
data–label generation and detection using pointnet++,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2303.09530</em>, 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Ouaknine, A. Newson, J. Rebut, F. Tupin, and P. Pérez, “Carrada
dataset: Camera and automotive radar with range-angle-doppler annotations,”
in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2020 25th International Conference on Pattern Recognition
(ICPR)</em>.   IEEE, 2021, pp. 5068–5075.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
A. Palffy, E. Pool, S. Baratam, J. F. Kooij, and D. M. Gavrila, “Multi-class
road user detection with 3+ 1d radar in the view-of-delft dataset,”
<em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 7, no. 2, pp. 4961–4968,
2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
D. Gusland, J. M. Christiansen, B. Torvik, F. Fioranelli, S. Z. Gurbuz, and
M. Ritchie, “Open radar initiative: Large scale dataset for benchmarking of
micro-doppler recognition algorithms,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Radar Conference
(RadarConf21)</em>.   IEEE, 2021, pp. 1–6.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
F. E. Nowruzi, D. Kolhatkar, P. Kapoor, F. Al Hassanat, E. J. Heravi,
R. Laganiere, J. Rebut, and W. Malik, “Deep open space segmentation using
automotive radar,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">2020 IEEE MTT-S International Conference on
Microwaves for Intelligent Mobility (ICMIM)</em>.   IEEE, 2020, pp. 1–4.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
B. Yang, I. Khatri, M. Happold, and C. Chen, “Adcnet: End-to-end perception
with raw radar adc data,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.11420</em>, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
O. Schumann, M. Hahn, J. Dickmann, and C. Wöhler, “Semantic segmentation
on radar point clouds,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">2018 21st International Conference on
Information Fusion (FUSION)</em>.   IEEE,
2018, pp. 2179–2186.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
F. Fent, P. Bauerschmidt, and M. Lienkamp, “Radargnn: Transformation invariant
graph neural network for radar-based perception,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023,
pp. 182–191.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
H. Rohling, “Radar cfar thresholding in clutter and multiple target
situations,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on aerospace and electronic systems</em>,
no. 4, pp. 608–621, 1983.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
P. P. Gandhi and S. A. Kassam, “Analysis of cfar processors in nonhomogeneous
background,” <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Aerospace and Electronic systems</em>,
vol. 24, no. 4, pp. 427–445, 1988.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
K. Werber, M. Rapp, J. Klappstein, M. Hahn, J. Dickmann, K. Dietmayer, and
C. Waldschmidt, “Automotive radar gridmap representations,” in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">2015
IEEE MTT-S International Conference on Microwaves for Intelligent Mobility
(ICMIM)</em>.   IEEE, 2015, pp. 1–4.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
L. Sless, B. El Shlomo, G. Cohen, and S. Oron, “Road scene understanding by
occupancy grid learning from sparse radar clusters using semantic
segmentation,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference
on Computer Vision Workshops</em>, 2019, pp. 0–0.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
O. Schumann, J. Lombacher, M. Hahn, C. Wöhler, and J. Dickmann, “Scene
understanding with automotive radar,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent
Vehicles</em>, vol. 5, no. 2, pp. 188–203, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
J. Lombacher, M. Hahn, J. Dickmann, and C. Wöhler, “Detection of
arbitrarily rotated parked cars based on radar sensors,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">2015 16th
International Radar Symposium (IRS)</em>.   IEEE, 2015, pp. 180–185.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
S. Chen, W. He, J. Ren, and X. Jiang, “Attention-based dual-stream vision
transformer for radar gait recognition,” in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em>.   IEEE, 2022, pp. 3668–3672.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Wikipedia contributors, “Image sensor — Wikipedia, the free
encyclopedia,” 2023, [Online; accessed 10-April-2023]. [Online]. Available:
<a target="_blank" href="https://en.wikipedia.org/w/index.php?title=Image_sensor&amp;oldid=1146373856" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://en.wikipedia.org/w/index.php?title=Image_sensor&amp;oldid=1146373856</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
C. Michaelis, B. Mitzkus, R. Geirhos, E. Rusak, O. Bringmann, A. S. Ecker,
M. Bethge, and W. Brendel, “Benchmarking robustness in object detection:
Autonomous driving when winter is coming,” <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1907.07484</em>, 2019.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
X. Xu, Y. Ma, and W. Sun, “Towards real scene super-resolution with raw
images,” in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 2019, pp. 1723–1731.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
X. Liu, K. Shi, Z. Wang, and J. Chen, “Exploit camera raw data for video
super-resolution via hidden markov model inference,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions
on Image Processing</em>, vol. 30, pp. 2127–2140, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
H. Song, W. Choi, and H. Kim, “Robust vision-based relative-localization
approach using an rgb-depth camera and lidar sensor fusion,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Industrial Electronics</em>, vol. 63, no. 6, pp. 3725–3736,
2016.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
S. Inbar and O. David, “Laser gated camera imaging system and method,” May 27
2008, uS Patent 7,379,164.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
M. Bijelic, T. Gruber, F. Mannan, F. Kraus, W. Ritter, K. Dietmayer, and
F. Heide, “Seeing through fog without seeing fog: Deep multimodal sensor
fusion in unseen adverse weather,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2020, pp.
11 682–11 692.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
G. Gallego, T. Delbrück, G. Orchard, C. Bartolozzi, B. Taba, A. Censi,
S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Event-based vision: A survey,” <em id="bib.bib54.2.2" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis
and machine intelligence</em>, vol. 44, no. 1, pp. 154–180, 2020.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
F. Rosique, P. J. Navarro, C. Fernández, and A. Padilla, “A systematic
review of perception system and simulators for autonomous vehicles
research,” <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 19, no. 3, p. 648, 2019.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
S. Saponara and B. Neri, “Radar sensor signal acquisition and multidimensional
fft processing for surveillance applications in transport systems,”
<em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Instrumentation and Measurement</em>, vol. 66, no. 4,
pp. 604–615, 2017.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
S. Sun, A. P. Petropulu, and H. V. Poor, “Mimo radar for advanced
driver-assistance systems and autonomous driving: Advantages and
challenges,” <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, vol. 37, no. 4, pp.
98–117, 2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
B. Li, “3d fully convolutional network for vehicle detection in point cloud,”
in <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">2017 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</em>.   IEEE, 2017, pp.
1513–1518.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
X. Chen, K. Kundu, Z. Zhang, H. Ma, S. Fidler, and R. Urtasun, “Monocular 3d
object detection for autonomous driving,” in <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2016, pp. 2147–2156.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
S. Zhang, R. Benenson, M. Omran, J. Hosang, and B. Schiele, “Towards reaching
human performance in pedestrian detection,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on
pattern analysis and machine intelligence</em>, vol. 40, no. 4, pp. 973–986,
2017.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
L. Zhang, L. Lin, X. Liang, and K. He, “Is faster r-cnn doing well for
pedestrian detection?” in <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings,
Part II 14</em>.   Springer, 2016, pp.
443–457.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
X. Chen, K. Kundu, Y. Zhu, H. Ma, S. Fidler, and R. Urtasun, “3d object
proposals using stereo imagery for accurate object class detection,”
<em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em>,
vol. 40, no. 5, pp. 1259–1272, 2017.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
M. Weber, P. Wolf, and J. M. Zöllner, “Deeptlr: A single deep
convolutional network for detection and classification of traffic lights,”
in <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">2016 IEEE intelligent vehicles symposium (IV)</em>.   IEEE, 2016, pp. 342–348.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
J. Müller and K. Dietmayer, “Detecting traffic lights by single shot
detection,” in <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">2018 21st International Conference on Intelligent
Transportation Systems (ITSC)</em>.   IEEE,
2018, pp. 266–273.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
M. Bach, S. Reuter, and K. Dietmayer, “Multi-camera traffic light recognition
using a classifying labeled multi-bernoulli filter,” in <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">2017 IEEE
Intelligent Vehicles Symposium (IV)</em>.   IEEE, 2017, pp. 1045–1051.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
K. Behrendt, L. Novak, and R. Botros, “A deep learning approach to traffic
lights: Detection, tracking, and classification,” in <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">2017 IEEE
International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2017, pp. 1370–1377.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Z. Zhu, D. Liang, S. Zhang, X. Huang, B. Li, and S. Hu, “Traffic-sign
detection and classification in the wild,” in <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2016, pp. 2110–2118.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
H. S. Lee and K. Kim, “Simultaneous traffic sign detection and boundary
estimation using convolutional neural network,” <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Intelligent Transportation Systems</em>, vol. 19, no. 5, pp. 1652–1663, 2018.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
H. Luo, Y. Yang, B. Tong, F. Wu, and B. Fan, “Traffic sign recognition using a
multi-task convolutional neural network,” <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Intelligent Transportation Systems</em>, vol. 19, no. 4, pp. 1100–1111, 2017.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
for accurate object detection and semantic segmentation,” in
<em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2014, pp. 580–587.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep
convolutional networks for visual recognition,” <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Pattern Analysis and Machine Intelligence</em>, vol. 37, no. 9, pp. 1904–1916,
2015.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
R. Girshick, “Fast r-cnn,” in <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international
conference on computer vision</em>, 2015, pp. 1440–1448.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, vol. 28, 2015.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
“Feature pyramid networks for object detection,” in <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE conference on computer vision and pattern recognition</em>, 2017, pp.
2117–2125.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
Unified, real-time object detection,” in <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2016, pp. 779–788.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,” in
<em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2017, pp. 7263–7271.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1804.02767</em>, 2018.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, “Yolov4: Optimal speed and
accuracy of object detection,” <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.10934</em>, 2020.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
G. Jocher, “YOLOv5 by Ultralytics,” 5 2020. [Online]. Available:
<a target="_blank" href="https://github.com/ultralytics/yolov5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ultralytics/yolov5</a>

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
C. Li, L. Li, H. Jiang, K. Weng, Y. Geng, L. Li, Z. Ke, Q. Li, M. Cheng, W. Nie
<em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Yolov6: A single-stage object detection framework for
industrial applications,” <em id="bib.bib80.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.02976</em>, 2022.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
C.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao, “Yolov7: Trainable
bag-of-freebies sets new state-of-the-art for real-time object detectors,”
in <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</em>, 2023, pp. 7464–7475.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
G. Jocher, A. Chaurasia, and J. Qiu, “YOLO by Ultralytics,” Jan. 2023.
[Online]. Available: <a target="_blank" href="https://github.com/ultralytics/ultralytics" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ultralytics/ultralytics</a>

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“Ssd: Single shot multibox detector,” in <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2016:
14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016,
Proceedings, Part I 14</em>.   Springer,
2016, pp. 21–37.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
dense object detection,” in <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international
conference on computer vision</em>, 2017, pp. 2980–2988.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
“End-to-end object detection with transformers,” in <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">European
conference on computer vision</em>.   Springer International Publishing Cham, 2020, pp. 213–229.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr: Deformable
transformers for end-to-end object detection,” <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2010.04159</em>, 2020.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
W. Lv, S. Xu, Y. Zhao, G. Wang, J. Wei, C. Cui, Y. Du, Q. Dang, and Y. Liu,
“Detrs beat yolos on real-time object detection,” 2023.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
F. Liu, H. Wei, W. Zhao, G. Li, J. Peng, and Z. Li, “Wb-detr:
transformer-based detector without backbone,” in <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 2979–2987.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin
transformer: Hierarchical vision transformer using shifted windows,” in
<em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer
vision</em>, 2021, pp. 10 012–10 022.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Y. Fang, B. Liao, X. Wang, J. Fang, J. Qi, R. Wu, J. Niu, and W. Liu, “You
only look at one sequence: Rethinking transformer in vision through object
detection,” <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
vol. 34, pp. 26 183–26 197, 2021.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Z. Peng, W. Huang, S. Gu, L. Xie, Y. Wang, J. Jiao, and Q. Ye, “Conformer:
Local features coupling global representations for visual recognition,” in
<em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer
vision</em>, 2021, pp. 367–376.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
J. Pan, A. Bulat, F. Tan, X. Zhu, L. Dudziak, H. Li, G. Tzimiropoulos, and
B. Martinez, “Edgevits: Competing light-weight cnns on mobile devices with
vision transformers,” in <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer
Vision</em>.   Springer Nature Switzerland
Cham, 2022, pp. 294–311.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
S. Mehta and M. Rastegari, “Mobilevit: light-weight, general-purpose, and
mobile-friendly vision transformer,” <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.02178</em>,
2021.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Y. Xu, Q. Zhang, J. Zhang, and D. Tao, “Vitae: Vision transformer advanced by
exploring intrinsic inductive bias,” <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, vol. 34, pp. 28 522–28 535, 2021.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Z. Chen, L. Xie, J. Niu, X. Liu, L. Wei, and Q. Tian, “Visformer: The
vision-friendly transformer,” in <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
international conference on computer vision</em>, 2021, pp. 589–598.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
A. Palffy, J. Dong, J. F. Kooij, and D. M. Gavrila, “Cnn based road user
detection using the 3d radar cube,” <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation
Letters</em>, vol. 5, no. 2, pp. 1263–1270, 2020.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
A. Zhang, F. E. Nowruzi, and R. Laganiere, “Raddet: Range-azimuth-doppler
based radar object detection for dynamic road users,” in <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">2021 18th
Conference on Robots and Vision (CRV)</em>.   IEEE, 2021, pp. 95–102.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
P. Svenningsson, F. Fioranelli, and A. Yarovoy, “Radar-pointgnn: Graph based
object recognition for unstructured radar point-cloud data,” in <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">2021
IEEE Radar Conference (RadarConf21)</em>.   IEEE, 2021, pp. 1–6.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision
and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
X. Gao, G. Xing, S. Roy, and H. Liu, “Experiments with mmwave automotive radar
test-bed,” in <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">2019 53rd Asilomar conference on signals, systems, and
computers</em>.   IEEE, 2019, pp. 1–6.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
X. Dong, P. Wang, P. Zhang, and L. Liu, “Probabilistic oriented object
detection in automotive radar,” in <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition Workshops</em>, 2020, pp.
102–103.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
W. Ng, G. Wang, Z. Lin, B. J. Dutta <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Range-doppler detection in
automotive radar with deep learning,” in <em id="bib.bib102.2.2" class="ltx_emph ltx_font_italic">2020 International Joint
Conference on Neural Networks (IJCNN)</em>.   IEEE, 2020, pp. 1–8.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
C. Decourt, R. VanRullen, D. Salle, and T. Oberlin, “Darod: A deep automotive
radar object detector on range-doppler maps,” in <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">2022 IEEE Intelligent
Vehicles Symposium (IV)</em>.   IEEE, 2022,
pp. 112–118.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
X. Gao, G. Xing, S. Roy, and H. Liu, “Ramp-cnn: A novel neural network for
enhanced automotive radar object recognition,” <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>,
vol. 21, no. 4, pp. 5119–5132, 2020.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
J. Rebut, A. Ouaknine, W. Malik, and P. Pérez, “Raw high-definition radar
for multi-task learning,” in <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, 2022, pp. 17 021–17 030.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
A. Danzer, T. Griebel, M. Bach, and K. Dietmayer, “2d car detection in radar
data with pointnets,” in <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Intelligent Transportation Systems
Conference (ITSC)</em>.   IEEE, 2019, pp.
61–66.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
J. F. Tilly, S. Haag, O. Schumann, F. Weishaupt, B. Duraisamy, J. Dickmann, and
M. Fritzsche, “Detection and tracking on automotive radar data with deep
learning,” in <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">2020 IEEE 23rd International Conference on Information
Fusion (FUSION)</em>.   IEEE, 2020, pp.
1–7.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
N. Scheiner, F. Kraus, F. Wei, B. Phan, F. Mannan, N. Appenrodt, W. Ritter,
J. Dickmann, K. Dietmayer, B. Sick <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Seeing around street
corners: Non-line-of-sight detection and tracking in-the-wild using doppler
radar,” in <em id="bib.bib108.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 2020, pp. 2068–2077.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
A. Dubey, A. Santra, J. Fuchs, M. Lübke, R. Weigel, and F. Lurz,
“Haradnet: Anchor-free target detection for radar point clouds using
hierarchical attention and multi-task learning,” <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">Machine Learning with
Applications</em>, vol. 8, p. 100275, 2022.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
B. Tan, Z. Ma, X. Zhu, S. Li, L. Zheng, S. Chen, L. Huang, and J. Bai, “3d
object detection for multi-frame 4d automotive millimeter-wave radar point
cloud,” <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>, 2022.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on point
sets for 3d classification and segmentation,” in <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE conference on computer vision and pattern recognition</em>, 2017, pp.
652–660.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
feature learning on point sets in a metric space,” <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for 3d
object detection from rgb-d data,” in <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2018, pp. 918–927.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
M. Dreher, E. Erçelik, T. Bänziger, and A. Knoll, “Radar-based 2d
car detection using deep neural networks,” in <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">2020 IEEE 23rd
International Conference on Intelligent Transportation Systems (ITSC)</em>.   IEEE, 2020, pp. 1–8.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
D. Köhler, M. Quach, M. Ulrich, F. Meinl, B. Bischoff, and H. Blume,
“Improved multi-scale grid rendering of point clouds for radar object
detection networks,” <em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15836</em>, 2023.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
J. Liu, Q. Zhao, W. Xiong, T. Huang, Q.-L. Han, and B. Zhu, “Smurf: Spatial
multi-representation fusion for 3d object detection with 4d imaging radar,”
<em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.10784</em>, 2023.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud based 3d
object detection,” in <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>, 2018, pp. 4490–4499.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
S. Tsutsui, T. Kerola, and S. Saito, “Distantly supervised road
segmentation,” in <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on
Computer Vision Workshops</em>, 2017, pp. 174–181.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
S. Tsutsui, T. Kerola, S. Saito, and D. J. Crandall, “Minimizing supervision
for free-space segmentation,” in <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops</em>, 2018, pp. 988–997.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Y.-C. Chan, Y.-C. Lin, and P.-C. Chen, “Lane mark and drivable area detection
using a novel instance segmentation scheme,” in <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/SICE
International Symposium on System Integration (SII)</em>.   IEEE, 2019, pp. 502–506.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Y. Qian, J. M. Dolan, and M. Yang, “Dlt-net: Joint detection of drivable
areas, lane lines, and traffic objects,” <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Intelligent Transportation Systems</em>, vol. 21, no. 11, pp. 4670–4679, 2019.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
H. Wang, R. Fan, P. Cai, and M. Liu, “Sne-roadseg+: Rethinking depth-normal
translation and deep supervision for freespace detection,” in <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">2021
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS)</em>.   IEEE, 2021, pp. 1140–1145.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
M.-E. Shao, M. A. Haq, D.-Q. Gao, P. Chondro, and S.-J. Ruan, “Semantic
segmentation for free space and lane based on grid-based interest point
detection,” <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>,
vol. 23, no. 7, pp. 8498–8512, 2021.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
J. Kim and C. Park, “End-to-end ego lane estimation based on sequential
transfer learning for self-driving cars,” in <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition workshops</em>, 2017, pp.
30–38.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
D. Neven, B. De Brabandere, S. Georgoulis, M. Proesmans, and L. Van Gool,
“Towards end-to-end lane detection: an instance segmentation approach,” in
<em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">2018 IEEE intelligent vehicles symposium (IV)</em>.   IEEE, 2018, pp. 286–291.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Z. Wang, W. Ren, and Q. Qiu, “Lanenet: Real-time lane detection networks for
autonomous driving,” <em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.01726</em>, 2018.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
Q. Zou, H. Jiang, Q. Dai, Y. Yue, L. Chen, and Q. Wang, “Robust lane detection
from continuous driving scenes using deep neural networks,” <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Vehicular Technology</em>, vol. 69, no. 1, pp. 41–54, 2019.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
X. Chen, Y. Liu, and K. Achuthan, “Wodis: Water obstacle detection network
based on image segmentation for autonomous surface vehicles in maritime
environments,” <em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Instrumentation and Measurement</em>,
vol. 70, pp. 1–13, 2021.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
B. Bovcon and M. Kristan, “Wasr—a water segmentation and refinement maritime
obstacle detection network,” <em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Cybernetics</em>,
vol. 52, no. 12, pp. 12 661–12 674, 2021.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
B. Bovcon, J. Muhovič, D. Vranac, D. Mozetič, J. Perš, and
M. Kristan, “Mods—a usv-oriented object detection and obstacle
segmentation benchmark,” <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent
Transportation Systems</em>, vol. 23, no. 8, pp. 13 403–13 418, 2021.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
semantic segmentation,” in <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on
computer vision and pattern recognition</em>, 2015, pp. 3431–3440.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
encoder-decoder architecture for image segmentation,” <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 39, no. 12,
pp. 2481–2495, 2017.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for
biomedical image segmentation,” in <em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and
Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III 18</em>.   Springer, 2015, pp. 234–241.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Y. Yuan, X. Chen, and J. Wang, “Object-contextual representations for semantic
segmentation,” in <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16</em>.   Springer, 2020, pp. 173–190.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Semantic
image segmentation with deep convolutional nets and fully connected crfs,”
<em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.7062</em>, 2014.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected crfs,” <em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and
Machine Intelligence</em>, vol. 40, no. 4, pp. 834–848, 2017.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous
convolution for semantic image segmentation,” <em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1706.05587</em>, 2017.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-decoder
with atrous separable convolution for semantic image segmentation,” in
<em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision (ECCV)</em>,
2018, pp. 801–818.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep neural
network architecture for real-time semantic segmentation,” <em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1606.02147</em>, 2016.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
in <em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2017, pp. 2881–2890.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and G. Cottrell,
“Understanding convolution for semantic segmentation,” in <em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">2018 IEEE
winter conference on applications of computer vision (WACV)</em>.   Ieee, 2018, pp. 1451–1460.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, “Denseaspp for semantic
segmentation in street scenes,” in <em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference
on computer vision and pattern recognition</em>, 2018, pp. 3684–3692.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang,
P. H. Torr <em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Rethinking semantic segmentation from a
sequence-to-sequence perspective with transformers,” in <em id="bib.bib143.2.2" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition</em>, 2021,
pp. 6881–6890.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
R. Strudel, R. Garcia, I. Laptev, and C. Schmid, “Segmenter: Transformer for
semantic segmentation,” in <em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international
conference on computer vision</em>, 2021, pp. 7262–7272.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer:
Simple and efficient design for semantic segmentation with transformers,”
<em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 34, pp.
12 077–12 090, 2021.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
H. Yan, C. Zhang, and M. Wu, “Lawin transformer: Improving semantic
segmentation transformer with multi-scale representations via large window
attention,” <em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.01615</em>, 2022.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
B. Cheng, A. Schwing, and A. Kirillov, “Per-pixel classification is not all
you need for semantic segmentation,” <em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, vol. 34, pp. 17 864–17 875, 2021.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
A. Ouaknine, A. Newson, P. Pérez, F. Tupin, and J. Rebut, “Multi-view
radar semantic segmentation,” in <em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2021, pp. 15 671–15 680.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
S. T. Isele, F. Klein, M. Brosowsky, and J. M. Zöllner, “Learning
semantics on radar point-clouds,” in <em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Intelligent Vehicles
Symposium (IV)</em>.   IEEE, 2021, pp.
810–817.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
F. E. Nowruzi, D. Kolhatkar, P. Kapoor, E. J. Heravi, F. A. Hassanat,
R. Laganiere, J. Rebut, and W. Malik, “Polarnet: Accelerated deep open space
segmentation using automotive radar in polar domain,” <em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2103.03387</em>, 2021.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
J. Lombacher, K. Laudt, M. Hahn, J. Dickmann, and C. Wöhler, “Semantic
radar grids,” in <em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">2017 IEEE intelligent vehicles symposium (IV)</em>.   IEEE, 2017, pp. 1170–1175.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
R. Prophet, G. Li, C. Sturm, and M. Vossiek, “Semantic segmentation on
automotive radar maps,” in <em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Intelligent Vehicles Symposium
(IV)</em>.   IEEE, 2019, pp. 756–763.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
P. Kaul, D. De Martini, M. Gadd, and P. Newman, “Rss-net: Weakly-supervised
multi-class semantic segmentation with fmcw radar,” in <em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
Intelligent Vehicles Symposium (IV)</em>.   IEEE, 2020, pp. 431–436.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
I. Orr, M. Cohen, and Z. Zalevsky, “High-resolution radar road segmentation
using weakly supervised learning,” <em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>,
vol. 3, no. 3, pp. 239–246, 2021.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
L. Zhang, X. Zhang, Y. Zhang, Y. Guo, Y. Chen, X. Huang, and Z. Ma, “Peakconv:
Learning peak receptive field for radar semantic segmentation,” in
<em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2023, pp. 17 577–17 586.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
Z. Feng, S. Zhang, M. Kunert, and W. Wiesbeck, “Point cloud segmentation with
a high-resolution automotive radar,” in <em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">AmE 2019-Automotive meets
Electronics; 10th GMM-Symposium</em>.   VDE,
2019, pp. 1–5.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
O. Schumann, M. Hahn, J. Dickmann, and C. Wöhler, “Supervised clustering
for radar applications: On the way to radar instance segmentation,” in
<em id="bib.bib157.1.1" class="ltx_emph ltx_font_italic">2018 IEEE MTT-S International Conference on Microwaves for Intelligent
Mobility (ICMIM)</em>.   IEEE, 2018, pp.
1–4.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
F. Nobis, F. Fent, J. Betz, and M. Lienkamp, “Kernel point convolution lstm
networks for radar point cloud segmentation,” <em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>,
vol. 11, no. 6, p. 2599, 2021.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
J. Liu, W. Xiong, L. Bai, Y. Xia, T. Huang, W. Ouyang, and B. Zhu, “Deep
instance segmentation with automotive radar detection points,” <em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Intelligent Vehicles</em>, vol. 8, no. 1, pp. 84–94, 2022.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
R. Prophet, A. Deligiannis, J.-C. Fuentes-Michel, I. Weber, and M. Vossiek,
“Semantic segmentation on 3d occupancy grids for automotive radar,”
<em id="bib.bib160.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp. 197 917–197 930, 2020.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
M. Zeller, J. Behley, M. Heidingsfeld, and C. Stachniss, “Gaussian radar
transformer for semantic segmentation in noisy radar data,” <em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">IEEE
Robotics and Automation Letters</em>, vol. 8, no. 1, pp. 344–351, 2022.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
W. Xiong, J. Liu, Y. Xia, T. Huang, B. Zhu, and W. Xiang, “Contrastive
learning for automotive mmwave radar detection points based instance
segmentation,” in <em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">2022 IEEE 25th International Conference on
Intelligent Transportation Systems (ITSC)</em>.   IEEE, 2022, pp. 1255–1261.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? the
kitti vision benchmark suite,” in <em id="bib.bib163.1.1" class="ltx_emph ltx_font_italic">2012 IEEE conference on computer
vision and pattern recognition</em>.   IEEE,
2012, pp. 3354–3361.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 year, 1000 km: The oxford
robotcar dataset,” <em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">The International Journal of Robotics Research</em>,
vol. 36, no. 1, pp. 3–15, 2017.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
X. Huang, P. Wang, X. Cheng, D. Zhou, Q. Geng, and R. Yang, “The apolloscape
open dataset for autonomous driving and its application,” <em id="bib.bib165.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 42, no. 10,
pp. 2702–2719, 2019.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo,
Y. Zhou, Y. Chai, B. Caine <em id="bib.bib166.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Scalability in perception for
autonomous driving: Waymo open dataset,” in <em id="bib.bib166.2.2" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp.
2446–2454.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
M. Mostajabi, C. M. Wang, D. Ranjan, and G. Hsyu, “High-resolution radar
dataset for semi-supervised learning of dynamic objects,” in
<em id="bib.bib167.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops</em>, 2020, pp. 100–101.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
M. Sheeny, E. De Pellegrin, S. Mukherjee, A. Ahrabian, S. Wang, and A. Wallace,
“Radiate: A radar dataset for automotive perception in bad weather,” in
<em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Robotics and Automation
(ICRA)</em>.   IEEE, 2021, pp. 1–7.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
X. Weng, Y. Man, J. Park, Y. Yuan, M. O’Toole, and K. M. Kitani, “All-in-one
drive: A comprehensive perception dataset with high-density long-range point
clouds,” 2023.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Y. Wang, G. Wang, H.-M. Hsu, H. Liu, and J.-N. Hwang, “Rethinking of radar’s
role: A camera-radar dataset and systematic annotator via coordinate
alignment,” in <em id="bib.bib170.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</em>, 2021, pp. 2815–2824.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
T.-Y. Lim, S. A. Markowitz, and M. N. Do, “Radical: A synchronized fmcw radar,
depth, imu and rgb camera data dataset with low-level fmcw radar signals,”
<em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol. 15, no. 4,
pp. 941–953, 2021.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
Y. Cheng, J. Zhu, M. Jiang, J. Fu, C. Pang, P. Wang, K. Sankaran, O. Onabola,
Y. Liu, D. Liu <em id="bib.bib172.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Flow: A dataset and benchmark for floating
waste detection in inland waters,” in <em id="bib.bib172.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2021, pp. 10 953–10 962.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
K. Burnett, D. J. Yoon, Y. Wu, A. Z. Li, H. Zhang, S. Lu, J. Qian, W.-K. Tseng,
A. Lambert, K. Y. Leung <em id="bib.bib173.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Boreas: A multi-season autonomous
driving dataset,” <em id="bib.bib173.2.2" class="ltx_emph ltx_font_italic">The International Journal of Robotics Research</em>,
vol. 42, no. 1-2, pp. 33–42, 2023.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
S. Yao, R. Guan, Z. Wu, Y. Ni, Z. Zhang, Z. Huang, X. Zhu, Y. Yue, Y. Yue,
H. Seo <em id="bib.bib174.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Waterscenes: A multi-task 4d radar-camera fusion
dataset and benchmark for autonomous driving on water surfaces,” <em id="bib.bib174.2.2" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2307.06505</em>, 2023.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
M. Meyer and G. Kuschk, “Deep learning based 3d object detection for
automotive radar and camera,” in <em id="bib.bib175.1.1" class="ltx_emph ltx_font_italic">2019 16th European Radar Conference
(EuRAD)</em>.   IEEE, 2019, pp. 133–136.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
L. Zheng, Z. Ma, X. Zhu, B. Tan, S. Li, K. Long, W. Sun, S. Chen, L. Zhang,
M. Wan <em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Tj4dradset: A 4d radar dataset for autonomous
driving,” in <em id="bib.bib176.2.2" class="ltx_emph ltx_font_italic">2022 IEEE 25th International Conference on Intelligent
Transportation Systems (ITSC)</em>.   IEEE,
2022, pp. 493–498.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
D.-H. Paek, S.-H. Kong, and K. T. Wijaya, “K-radar: 4d radar object detection
dataset and benchmark for autonomous driving in various weather conditions,”
<em id="bib.bib177.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.08171</em>, 2022.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
T. Matuszka, I. Barton, Á. Butykai, P. Hajas, D. Kiss, D. Kovács,
S. Kunsági-Máté, P. Lengyel, G. Németh, L. Pető
<em id="bib.bib178.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “aimotive dataset: A multimodal dataset for robust autonomous
driving with long-range perception,” <em id="bib.bib178.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.09445</em>,
2022.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
O. Schumann, M. Hahn, N. Scheiner, F. Weishaupt, J. F. Tilly, J. Dickmann, and
C. Wöhler, “Radarscenes: A real-world radar point cloud data set for
automotive applications,” in <em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">2021 IEEE 24th International Conference
on Information Fusion (FUSION)</em>.   IEEE,
2021, pp. 1–8.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
J. Guan, S. Madani, S. Jog, S. Gupta, and H. Hassanieh, “Through fog
high-resolution imaging using millimeter wave radar,” in <em id="bib.bib180.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020,
pp. 11 464–11 473.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla: An open
urban driving simulator,” in <em id="bib.bib181.1.1" class="ltx_emph ltx_font_italic">Conference on robot learning</em>.   PMLR, 2017, pp. 1–16.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Cai, H. Wang, L. Chen, H. Gao, Y. Jia, and Y. Li, “Robust target
recognition and tracking of self-driving cars with radar and camera
information fusion under severe weather conditions,” <em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions
on Intelligent Transportation Systems</em>, vol. 23, no. 7, pp. 6640–6653, 2021.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
R. Nabati, L. Harris, and H. Qi, “Cftrack: Center-based radar and camera
fusion for 3d multi-object tracking,” in <em id="bib.bib183.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Intelligent
Vehicles Symposium Workshops (IV Workshops)</em>.   IEEE, 2021, pp. 243–248.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
A. Benterki, M. Boukhnifer, V. Judalet, and C. Maaoui, “Artificial
intelligence for vehicle behavior anticipation: Hybrid approach based on
maneuver classification and trajectory prediction,” <em id="bib.bib184.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>,
vol. 8, pp. 56 992–57 002, 2020.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
P. Cai, S. Wang, Y. Sun, and M. Liu, “Probabilistic end-to-end vehicle
navigation in complex dynamic environments with multimodal sensor fusion,”
<em id="bib.bib185.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 5, no. 3, pp. 4218–4224,
2020.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
S. Chadwick, W. Maddern, and P. Newman, “Distant vehicle detection using radar
and vision,” in <em id="bib.bib186.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Robotics and
Automation (ICRA)</em>.   IEEE, 2019, pp.
8311–8317.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
T. Roddick, A. Kendall, and R. Cipolla, “Orthographic feature transform for
monocular 3d object detection,” <em id="bib.bib187.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.08188</em>,
2018.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
A. Simonelli, S. R. Bulo, L. Porzi, M. López-Antequera, and
P. Kontschieder, “Disentangling monocular 3d object detection,” in
<em id="bib.bib188.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer
Vision</em>, 2019, pp. 1991–1999.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
X. Zhou, D. Wang, and P. Krähenbühl, “Objects as points,” <em id="bib.bib189.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1904.07850</em>, 2019.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
R. Yadav, A. Vierling, and K. Berns, “Radar+ rgb fusion for robust object
detection in autonomous vehicle,” in <em id="bib.bib190.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International
Conference on Image Processing (ICIP)</em>.   IEEE, 2020, pp. 1986–1990.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
P. Zhao, C. X. Lu, B. Wang, N. Trigoni, and A. Markham, “Cubelearn: End-to-end
learning for human motion recognition from raw mmwave radar signals,”
<em id="bib.bib191.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, 2023.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
T. Stadelmayer, A. Santra, R. Weigel, and F. Lurz, “Data-driven radar
processing using a parametric convolutional neural network for human activity
classification,” <em id="bib.bib192.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>, vol. 21, no. 17, pp.
19 529–19 540, 2021.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
Y. Kim and T. Moon, “Human detection and activity classification based on
micro-doppler signatures using deep convolutional neural networks,”
<em id="bib.bib193.1.1" class="ltx_emph ltx_font_italic">IEEE geoscience and remote sensing letters</em>, vol. 13, no. 1, pp. 8–12,
2015.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
R. Nabati and H. Qi, “Radar-camera sensor fusion for joint object detection
and distance estimation in autonomous vehicles,” <em id="bib.bib194.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2009.08428</em>, 2020.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
H. Cui, J. Wu, J. Zhang, G. Chowdhary, and W. R. Norris, “3d detection and
tracking for on-road vehicles with a monovision camera and dual low-cost 4d
mmwave radars,” in <em id="bib.bib195.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Intelligent Transportation
Systems Conference (ITSC)</em>.   IEEE,
2021, pp. 2931–2937.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
L.-q. Li and Y.-l. Xie, “A feature pyramid fusion detection algorithm based on
radar and camera sensor,” in <em id="bib.bib196.1.1" class="ltx_emph ltx_font_italic">2020 15th IEEE International Conference
on Signal Processing (ICSP)</em>, vol. 1.   IEEE, 2020, pp. 366–370.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
K. Kowol, M. Rottmann, S. Bracke, and H. Gottschalk, “Yodar: Uncertainty-based
sensor fusion for vehicle detection with camera and radar sensors,”
<em id="bib.bib197.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.03320</em>, 2020.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
Y. Cheng, H. Xu, and Y. Liu, “Robust small object detection on the water
surface through fusion of camera and millimeter wave radar,” in
<em id="bib.bib198.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer
Vision</em>, 2021, pp. 15 263–15 272.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
V. John, M. Nithilan, S. Mita, H. Tehrani, R. Sudheesh, and P. Lalu, “So-net:
Joint semantic segmentation and obstacle detection using deep fusion of
monocular camera and radar,” in <em id="bib.bib199.1.1" class="ltx_emph ltx_font_italic">Image and Video Technology: PSIVT 2019
International Workshops, Sydney, NSW, Australia, November 18–22, 2019,
Revised Selected Papers 9</em>.   Springer,
2020, pp. 138–148.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
X. Dong, B. Zhuang, Y. Mao, and L. Liu, “Radar camera fusion via
representation learning in autonomous driving,” in <em id="bib.bib200.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp.
1672–1681.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
Y. Song, Z. Xie, X. Wang, and Y. Zou, “Ms-yolo: Object detection based on
yolov5 optimized fusion millimeter-wave radar and machine vision,”
<em id="bib.bib201.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>, vol. 22, no. 15, pp. 15 435–15 447, 2022.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
K. Bansal, K. Rungta, and D. Bharadia, “Radsegnet: A reliable approach to
radar camera fusion,” <em id="bib.bib202.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.03849</em>, 2022.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
A. W. Harley, Z. Fang, J. Li, R. Ambrus, and K. Fragkiadaki, “Simple-bev: What
really matters for multi-sensor bev perception?” in <em id="bib.bib203.1.1" class="ltx_emph ltx_font_italic">2023 IEEE
International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2023, pp. 2759–2765.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
M. Bertozz, A. Broggi, and A. Fascioli, “Stereo inverse perspective mapping:
theory and applications,” <em id="bib.bib204.1.1" class="ltx_emph ltx_font_italic">Image and vision computing</em>, vol. 16, no. 8,
pp. 585–590, 1998.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
M. Oliveira, V. Santos, and A. D. Sappa, “Multimodal inverse perspective
mapping,” <em id="bib.bib205.1.1" class="ltx_emph ltx_font_italic">Information Fusion</em>, vol. 24, pp. 108–121, 2015.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
T.-Y. Lim, A. Ansari, B. Major, D. Fontijne, M. Hamilton, R. Gowaikar, and
S. Subramanian, “Radar and camera early fusion for vehicle detection in
advanced driver assistance systems,” in <em id="bib.bib206.1.1" class="ltx_emph ltx_font_italic">Machine learning for
autonomous driving workshop at the 33rd conference on neural information
processing systems</em>, vol. 2, no. 7, 2019.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
T. Zhou, J. Chen, Y. Shi, K. Jiang, M. Yang, and D. Yang, “Bridging the view
disparity between radar and camera features for multi-modal fusion 3d object
detection,” <em id="bib.bib207.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Vehicles</em>, vol. 8, no. 2,
pp. 1523–1535, 2023.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Q. Weinberger,
“Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object
detection for autonomous driving,” in <em id="bib.bib208.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 8445–8453.

</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
H. Jha, V. Lodhi, and D. Chakravarty, “Object detection and identification
using vision and radar data fusion system for ground-based navigation,” in
<em id="bib.bib209.1.1" class="ltx_emph ltx_font_italic">2019 6th International Conference on Signal Processing and Integrated
Networks (SPIN)</em>.   IEEE, 2019, pp.
590–593.

</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
J. Kim, Y. Kim, and D. Kum, “Low-level sensor fusion network for 3d vehicle
detection using radar range-azimuth heatmap and monocular image,” in
<em id="bib.bib210.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Asian Conference on Computer Vision</em>, 2020.

</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
F. Nobis, M. Geisslinger, M. Weber, J. Betz, and M. Lienkamp, “A deep
learning-based radar and camera sensor fusion architecture for object
detection,” in <em id="bib.bib211.1.1" class="ltx_emph ltx_font_italic">2019 Sensor Data Fusion: Trends, Solutions,
Applications (SDF)</em>.   IEEE, 2019, pp.
1–7.

</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” <em id="bib.bib212.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.1556</em>, 2014.

</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
Y. Long, D. Morris, X. Liu, M. Castro, P. Chakravarty, and P. Narayanan,
“Radar-camera pixel depth association for depth completion,” in
<em id="bib.bib213.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2021, pp. 12 507–12 516.

</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
S. Chang, Y. Zhang, F. Zhang, X. Zhao, S. Huang, Z. Feng, and Z. Wei, “Spatial
attention fusion for obstacle detection using mmwave radar and vision
sensor,” <em id="bib.bib214.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 20, no. 4, p. 956, 2020.

</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
“Grad-cam: Visual explanations from deep networks via gradient-based
localization,” in <em id="bib.bib215.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on
computer vision</em>, 2017, pp. 618–626.

</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
A. G. Roy, N. Navab, and C. Wachinger, “Concurrent spatial and channel
‘squeeze &amp; excitation’in fully convolutional networks,” in
<em id="bib.bib216.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer Assisted Intervention–MICCAI
2018: 21st International Conference, Granada, Spain, September 16-20, 2018,
Proceedings, Part I</em>.   Springer, 2018,
pp. 421–429.

</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
A. Valada, R. Mohan, and W. Burgard, “Self-supervised model adaptation for
multimodal semantic segmentation,” <em id="bib.bib217.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer
Vision</em>, vol. 128, no. 5, pp. 1239–1285, 2020.

</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
T. Broedermann, C. Sakaridis, D. Dai, and L. Van Gool, “Hrfuser: A
multi-resolution sensor fusion architecture for 2d object detection,”
<em id="bib.bib218.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.15157</em>, 2022.

</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
Y. Yuan, R. Fu, L. Huang, W. Lin, C. Zhang, X. Chen, and J. Wang, “Hrformer:
High-resolution transformer for dense prediction,” <em id="bib.bib219.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2110.09408</em>, 2021.

</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
E. Olson, “A passive solution to the sensor synchronization problem,” in
<em id="bib.bib220.1.1" class="ltx_emph ltx_font_italic">2010 IEEE/RSJ International Conference on Intelligent Robots and
Systems</em>.   IEEE, 2010, pp. 1059–1064.

</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
A. Westenberger, T. Huck, M. Fritzsche, T. Schwarz, and K. Dietmayer,
“Temporal synchronization in multi-sensor fusion for future driver
assistance systems,” in <em id="bib.bib221.1.1" class="ltx_emph ltx_font_italic">2011 IEEE International Symposium on Precision
Clock Synchronization for Measurement, Control and Communication</em>.   IEEE, 2011, pp. 93–98.

</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
P. Furgale, J. Rehder, and R. Siegwart, “Unified temporal and spatial
calibration for multi-sensor systems,” in <em id="bib.bib222.1.1" class="ltx_emph ltx_font_italic">2013 IEEE/RSJ International
Conference on Intelligent Robots and Systems</em>.   IEEE, 2013, pp. 1280–1286.

</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
T. Huck, A. Westenberger, M. Fritzsche, T. Schwarz, and K. Dietmayer, “Precise
timestamping and temporal synchronization in multi-sensor fusion,” in
<em id="bib.bib223.1.1" class="ltx_emph ltx_font_italic">2011 IEEE intelligent vehicles symposium (IV)</em>.   IEEE, 2011, pp. 242–247.

</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
J. E. Guivant, S. Marden, and K. Pereida, “Distributed multi sensor data
fusion for autonomous 3d mapping,” in <em id="bib.bib224.1.1" class="ltx_emph ltx_font_italic">2012 International Conference on
Indoor Positioning and Indoor Navigation (IPIN)</em>.   IEEE, 2012, pp. 1–11.

</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
J. Steinbaeck, C. Steger, E. Brenner, and N. Druml, “A hybrid timestamping
approach for multi-sensor perception systems,” in <em id="bib.bib225.1.1" class="ltx_emph ltx_font_italic">2020 23rd Euromicro
Conference on Digital System Design (DSD)</em>.   IEEE, 2020, pp. 447–454.

</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
A. English, P. Ross, D. Ball, B. Upcroft, and P. Corke, “Triggersync: A time
synchronisation tool,” in <em id="bib.bib226.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on
Robotics and Automation (ICRA)</em>.   IEEE,
2015, pp. 6220–6226.

</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock">
L. Lu, C. Zhang, Y. Liu, W. Zhang, and Y. Xia, “Ieee 1588-based general and
precise time synchronization method for multiple sensors,” in <em id="bib.bib227.1.1" class="ltx_emph ltx_font_italic">2019
IEEE International Conference on Robotics and Biomimetics (ROBIO)</em>.   IEEE, 2019, pp. 2427–2432.

</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock">
M. Huber, M. Schlegel, and G. Klinker, “Temporal calibration in multisensor
tracking setups,” in <em id="bib.bib228.1.1" class="ltx_emph ltx_font_italic">2009 8th IEEE International Symposium on Mixed
and Augmented Reality</em>.   IEEE, 2009,
pp. 195–196.

</span>
</li>
<li id="bib.bib229" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock">
Y. Du, B. Qin, C. Zhao, Y. Zhu, J. Cao, and Y. Ji, “A novel spatio-temporal
synchronization method of roadside asynchronous mmw radar-camera for sensor
fusion,” <em id="bib.bib229.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>,
vol. 23, no. 11, pp. 22 278–22 289, 2021.

</span>
</li>
<li id="bib.bib230" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock">
Y. Fu, D. Tian, X. Duan, J. Zhou, P. Lang, C. Lin, and X. You, “A
camera–radar fusion method based on edge computing,” in <em id="bib.bib230.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
International Conference on Edge Computing (EDGE)</em>.   IEEE, 2020, pp. 9–14.

</span>
</li>
<li id="bib.bib231" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock">
F. Liu, J. Sparbert, and C. Stiller, “Immpda vehicle tracking system using
asynchronous sensor fusion of radar and vision,” in <em id="bib.bib231.1.1" class="ltx_emph ltx_font_italic">2008 IEEE
Intelligent Vehicles Symposium</em>.   IEEE,
2008, pp. 168–173.

</span>
</li>
<li id="bib.bib232" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock">
M. Ester, H.-P. Kriegel, J. Sander, X. Xu <em id="bib.bib232.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A density-based
algorithm for discovering clusters in large spatial databases with noise,”
in <em id="bib.bib232.2.2" class="ltx_emph ltx_font_italic">kdd</em>, vol. 96, no. 34, 1996, pp. 226–231.

</span>
</li>
<li id="bib.bib233" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock">
L. Wang, Z. Zhang, X. Di, and J. Tian, “A roadside camera-radar sensing fusion
system for intelligent transportation,” in <em id="bib.bib233.1.1" class="ltx_emph ltx_font_italic">2020 17th European Radar
Conference (EuRAD)</em>.   IEEE, 2021, pp.
282–285.

</span>
</li>
<li id="bib.bib234" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock">
J. Peršić, L. Petrović, I. Marković, and I. Petrović,
“Spatiotemporal multisensor calibration via gaussian processes moving target
tracking,” <em id="bib.bib234.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Robotics</em>, vol. 37, no. 5, pp.
1401–1415, 2021.

</span>
</li>
<li id="bib.bib235" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[235]</span>
<span class="ltx_bibblock">
S. Li, C. Xu, and M. Xie, “A robust o (n) solution to the perspective-n-point
problem,” <em id="bib.bib235.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, vol. 34, no. 7, pp. 1444–1450, 2012.

</span>
</li>
<li id="bib.bib236" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[236]</span>
<span class="ltx_bibblock">
E. Wise, J. Peršić, C. Grebe, I. Petrović, and J. Kelly, “A
continuous-time approach for 3d radar-to-camera extrinsic calibration,” in
<em id="bib.bib236.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Robotics and Automation
(ICRA)</em>.   IEEE, 2021, pp.
13 164–13 170.

</span>
</li>
<li id="bib.bib237" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[237]</span>
<span class="ltx_bibblock">
J. Peršić, L. Petrović, I. Marković, and I. Petrović,
“Online multi-sensor calibration based on moving object tracking,”
<em id="bib.bib237.1.1" class="ltx_emph ltx_font_italic">Advanced Robotics</em>, vol. 35, no. 3-4, pp. 130–140, 2021.

</span>
</li>
<li id="bib.bib238" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[238]</span>
<span class="ltx_bibblock">
K. Qiu, T. Qin, J. Pan, S. Liu, and S. Shen, “Real-time temporal and
rotational calibration of heterogeneous sensors using motion correlation
analysis,” <em id="bib.bib238.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Robotics</em>, vol. 37, no. 2, pp.
587–602, 2020.

</span>
</li>
<li id="bib.bib239" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[239]</span>
<span class="ltx_bibblock">
X.-p. Guo, J.-s. Du, J. Gao, and W. Wang, “Pedestrian detection based on
fusion of millimeter wave radar and vision,” in <em id="bib.bib239.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
2018 International Conference on Artificial Intelligence and Pattern
Recognition</em>, 2018, pp. 38–42.

</span>
</li>
<li id="bib.bib240" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[240]</span>
<span class="ltx_bibblock">
G. Iyer, R. K. Ram, J. K. Murthy, and K. M. Krishna, “Calibnet: Geometrically
supervised extrinsic calibration using 3d spatial transformer networks,” in
<em id="bib.bib240.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</em>.   IEEE, 2018, pp.
1110–1117.

</span>
</li>
<li id="bib.bib241" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[241]</span>
<span class="ltx_bibblock">
C. Schöller, M. Schnettler, A. Krämmer, G. Hinz, M. Bakovic,
M. Güzet, and A. Knoll, “Targetless rotational auto-calibration of radar
and camera for intelligent transportation systems,” in <em id="bib.bib241.1.1" class="ltx_emph ltx_font_italic">2019 IEEE
Intelligent Transportation Systems Conference (ITSC)</em>.   IEEE, 2019, pp. 3934–3941.

</span>
</li>
<li id="bib.bib242" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[242]</span>
<span class="ltx_bibblock">
G. Zhao, J. Hu, S. You, and C.-C. J. Kuo, “Calibdnn: multimodal sensor
calibration for perception using deep neural networks,” in <em id="bib.bib242.1.1" class="ltx_emph ltx_font_italic">Signal
Processing, Sensor/Information Fusion, and Target Recognition XXX</em>, vol.
11756.   SPIE, 2021, pp. 324–335.

</span>
</li>
<li id="bib.bib243" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[243]</span>
<span class="ltx_bibblock">
F. Nobis, E. Shafiei, P. Karle, J. Betz, and M. Lienkamp, “Radar voxel fusion
for 3d object detection,” <em id="bib.bib243.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol. 11, no. 12, p. 5598,
2021.

</span>
</li>
<li id="bib.bib244" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[244]</span>
<span class="ltx_bibblock">
R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, “Adaptive mixtures
of local experts,” <em id="bib.bib244.1.1" class="ltx_emph ltx_font_italic">Neural computation</em>, vol. 3, no. 1, pp. 79–87,
1991.

</span>
</li>
<li id="bib.bib245" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[245]</span>
<span class="ltx_bibblock">
Y. Kim, J. W. Choi, and D. Kum, “Grif net: Gated region of interest fusion
network for robust 3d object detection from radar point cloud and monocular
image,” in <em id="bib.bib245.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS)</em>.   IEEE,
2020, pp. 10 857–10 864.

</span>
</li>
<li id="bib.bib246" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[246]</span>
<span class="ltx_bibblock">
M. Ren, A. Pokrovsky, B. Yang, and R. Urtasun, “Sbnet: Sparse blocks network
for fast inference,” in <em id="bib.bib246.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>, 2018, pp. 8711–8720.

</span>
</li>
<li id="bib.bib247" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[247]</span>
<span class="ltx_bibblock">
W. Xiong, J. Liu, T. Huang, Q.-L. Han, Y. Xia, and B. Zhu, “Lxl: Lidar
exclusive lean 3d object detection with 4d imaging radar and camera fusion,”
<em id="bib.bib247.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.00724</em>, 2023.

</span>
</li>
<li id="bib.bib248" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[248]</span>
<span class="ltx_bibblock">
Y. Kim, S. Kim, J. W. Choi, and D. Kum, “Craft: Camera-radar 3d object
detection with spatio-contextual fusion transformer,” in <em id="bib.bib248.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the AAAI Conference on Artificial Intelligence</em>, vol. 37, no. 1, 2023, pp.
1160–1168.

</span>
</li>
<li id="bib.bib249" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[249]</span>
<span class="ltx_bibblock">
Z. Wu, G. Chen, Y. Gan, L. Wang, and J. Pu, “Mvfusion: Multi-view 3d object
detection with semantic-aligned radar and camera fusion,” <em id="bib.bib249.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2302.10511</em>, 2023.

</span>
</li>
<li id="bib.bib250" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[250]</span>
<span class="ltx_bibblock">
Y. Kim, S. Kim, J. Shin, J. W. Choi, and D. Kum, “Crn: Camera radar net for
accurate, robust, efficient 3d perception,” <em id="bib.bib250.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2304.00670</em>, 2023.

</span>
</li>
<li id="bib.bib251" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[251]</span>
<span class="ltx_bibblock">
L. Zheng, S. Li, B. Tan, L. Yang, S. Chen, L. Huang, J. Bai, X. Zhu, and Z. Ma,
“Rcfusion: Fusing 4d radar and camera with bird’s-eye view features for 3d
object detection,” <em id="bib.bib251.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Instrumentation and
Measurement</em>, 2023.

</span>
</li>
<li id="bib.bib252" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[252]</span>
<span class="ltx_bibblock">
Y. Jin, A. Deligiannis, J.-C. Fuentes-Michel, and M. Vossiek, “Cross-modal
supervision-based multitask learning with automotive radar raw data,”
<em id="bib.bib252.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Vehicles</em>, 2023.

</span>
</li>
<li id="bib.bib253" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[253]</span>
<span class="ltx_bibblock">
J.-J. Hwang, H. Kretzschmar, J. Manela, S. Rafferty, N. Armstrong-Crews,
T. Chen, and D. Anguelov, “Cramnet: Camera-radar fusion with ray-constrained
cross-attention for robust 3d object detection,” in <em id="bib.bib253.1.1" class="ltx_emph ltx_font_italic">European
Conference on Computer Vision</em>.   Springer Nature Switzerland Cham, 2022, pp. 388–405.

</span>
</li>
<li id="bib.bib254" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[254]</span>
<span class="ltx_bibblock">
K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, “Centernet: Keypoint
triplets for object detection,” in <em id="bib.bib254.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
international conference on computer vision</em>, 2019, pp. 6569–6578.

</span>
</li>
<li id="bib.bib255" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[255]</span>
<span class="ltx_bibblock">
L. Stäcker, P. Heidenreich, J. Rambach, and D. Stricker, “Fusion point
pruning for optimized 2d object detection with radar-camera fusion,” in
<em id="bib.bib255.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision</em>, 2022, pp. 3087–3094.

</span>
</li>
<li id="bib.bib256" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[256]</span>
<span class="ltx_bibblock">
C. X. Lu, S. Rosa, P. Zhao, B. Wang, C. Chen, J. A. Stankovic, N. Trigoni, and
A. Markham, “See through smoke: robust indoor mapping with low-cost mmwave
radar,” in <em id="bib.bib256.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 18th International Conference on Mobile
Systems, Applications, and Services</em>, 2020, pp. 14–27.

</span>
</li>
<li id="bib.bib257" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[257]</span>
<span class="ltx_bibblock">
J. Wang, “Cfar-based interference mitigation for fmcw automotive radar
systems,” <em id="bib.bib257.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>,
vol. 23, no. 8, pp. 12 229–12 238, 2021.

</span>
</li>
<li id="bib.bib258" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[258]</span>
<span class="ltx_bibblock">
R. Zhang and S. Cao, “Support vector machines for classification of automotive
radar interference,” in <em id="bib.bib258.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Radar Conference
(RadarConf18)</em>.   IEEE, 2018, pp.
0366–0371.

</span>
</li>
<li id="bib.bib259" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[259]</span>
<span class="ltx_bibblock">
F. Jin and S. Cao, “Automotive radar interference mitigation using adaptive
noise canceller,” <em id="bib.bib259.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Vehicular Technology</em>, vol. 68,
no. 4, pp. 3747–3754, 2019.

</span>
</li>
<li id="bib.bib260" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[260]</span>
<span class="ltx_bibblock">
M. Alhumaidi and M. Wintermantel, “Interference avoidance and mitigation in
automotive radar,” in <em id="bib.bib260.1.1" class="ltx_emph ltx_font_italic">2020 17th European Radar Conference
(EuRAD)</em>.   IEEE, 2021, pp. 172–175.

</span>
</li>
<li id="bib.bib261" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[261]</span>
<span class="ltx_bibblock">
C. Schüßler, M. Hoffmann, I. Ullmann, R. Ebelt, and M. Vossiek, “Deep
learning based image enhancement for automotive radar trained with an
advanced virtual sensor,” <em id="bib.bib261.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 10, pp. 40 419–40 431,
2022.

</span>
</li>
<li id="bib.bib262" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[262]</span>
<span class="ltx_bibblock">
J. Rock, W. Roth, M. Toth, P. Meissner, and F. Pernkopf, “Resource-efficient
deep neural networks for automotive radar interference mitigation,”
<em id="bib.bib262.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol. 15, no. 4,
pp. 927–940, 2021.

</span>
</li>
<li id="bib.bib263" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[263]</span>
<span class="ltx_bibblock">
A. Dubey, J. Fuchs, V. Madhavan, M. Lübke, R. Weigel, and F. Lurz, “Region
based single-stage interference mitigation and target detection,” in
<em id="bib.bib263.1.1" class="ltx_emph ltx_font_italic">2020 IEEE Radar Conference (RadarConf20)</em>.   IEEE, 2020, pp. 1–5.

</span>
</li>
<li id="bib.bib264" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[264]</span>
<span class="ltx_bibblock">
N.-C. Ristea, A. Anghel, and R. T. Ionescu, “Fully convolutional neural
networks for automotive radar interference mitigation,” in <em id="bib.bib264.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
92nd Vehicular Technology Conference (VTC2020-Fall)</em>.   IEEE, 2020, pp. 1–5.

</span>
</li>
<li id="bib.bib265" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[265]</span>
<span class="ltx_bibblock">
J. Fuchs, A. Dubey, M. Lübke, R. Weigel, and F. Lurz, “Automotive radar
interference mitigation using a convolutional autoencoder,” in <em id="bib.bib265.1.1" class="ltx_emph ltx_font_italic">2020
IEEE International Radar Conference (RADAR)</em>.   IEEE, 2020, pp. 315–320.

</span>
</li>
<li id="bib.bib266" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[266]</span>
<span class="ltx_bibblock">
S. Chen, J. Taghia, T. Fei, U. Kühnau, N. Pohl, and R. Martin, “A dnn
autoencoder for automotive radar interference mitigation,” in <em id="bib.bib266.1.1" class="ltx_emph ltx_font_italic">ICASSP
2021-2021 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</em>.   IEEE, 2021, pp.
4065–4069.

</span>
</li>
<li id="bib.bib267" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[267]</span>
<span class="ltx_bibblock">
S. Chen, J. Taghia, U. Kühnau, N. Pohl, and R. Martin, “A two-stage dnn
model with mask-gated convolution for automotive radar interference detection
and mitigation,” <em id="bib.bib267.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>, vol. 22, no. 12, pp.
12 017–12 027, 2022.

</span>
</li>
<li id="bib.bib268" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[268]</span>
<span class="ltx_bibblock">
M. L. L. de Oliveira and M. J. Bekooij, “Deep convolutional autoencoder
applied for noise reduction in range-doppler maps of fmcw radars,” in
<em id="bib.bib268.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Radar Conference (RADAR)</em>.   IEEE, 2020, pp. 630–635.

</span>
</li>
<li id="bib.bib269" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[269]</span>
<span class="ltx_bibblock">
Y. Cheng, J. Su, H. Chen, and Y. Liu, “A new automotive radar 4d point clouds
detector by using deep learning,” in <em id="bib.bib269.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em>.   IEEE, 2021, pp. 8398–8402.

</span>
</li>
<li id="bib.bib270" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[270]</span>
<span class="ltx_bibblock">
X. Ma, C. Qin, H. You, H. Ran, and Y. Fu, “Rethinking network design and local
geometry in point cloud: A simple residual mlp framework,” <em id="bib.bib270.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2202.07123</em>, 2022.

</span>
</li>
<li id="bib.bib271" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[271]</span>
<span class="ltx_bibblock">
G. Qian, Y. Li, H. Peng, J. Mai, H. Hammoud, M. Elhoseiny, and B. Ghanem,
“Pointnext: Revisiting pointnet++ with improved training and scaling
strategies,” <em id="bib.bib271.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
vol. 35, pp. 23 192–23 204, 2022.

</span>
</li>
<li id="bib.bib272" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[272]</span>
<span class="ltx_bibblock">
H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point transformer,” in
<em id="bib.bib272.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer
vision</em>, 2021, pp. 16 259–16 268.

</span>
</li>
<li id="bib.bib273" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[273]</span>
<span class="ltx_bibblock">
M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu, “Pct:
Point cloud transformer,” <em id="bib.bib273.1.1" class="ltx_emph ltx_font_italic">Computational Visual Media</em>, vol. 7, pp.
187–199, 2021.

</span>
</li>
<li id="bib.bib274" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[274]</span>
<span class="ltx_bibblock">
R. Zhang, L. Wang, Y. Wang, P. Gao, H. Li, and J. Shi, “Parameter is not all
you need: Starting from non-parametric networks for 3d point cloud
analysis,” <em id="bib.bib274.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08134</em>, 2023.

</span>
</li>
<li id="bib.bib275" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[275]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A
large-scale hierarchical image database,” in <em id="bib.bib275.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on
computer vision and pattern recognition</em>.   Ieee, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib276" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[276]</span>
<span class="ltx_bibblock">
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein <em id="bib.bib276.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Imagenet large scale
visual recognition challenge,” <em id="bib.bib276.2.2" class="ltx_emph ltx_font_italic">International journal of computer
vision</em>, vol. 115, pp. 211–252, 2015.

</span>
</li>
<li id="bib.bib277" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[277]</span>
<span class="ltx_bibblock">
F. E. Nowruzi, P. Kapoor, D. Kolhatkar, F. A. Hassanat, R. Laganiere, and
J. Rebut, “How much real data do we actually need: Analyzing object
detection performance using synthetic and real data,” <em id="bib.bib277.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1907.07061</em>, 2019.

</span>
</li>
<li id="bib.bib278" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[278]</span>
<span class="ltx_bibblock">
M. Stephan, T. Stadelmayer, A. Santra, G. Fischer, R. Weigel, and F. Lurz,
“Radar image reconstruction from raw adc data using parametric variational
autoencoder with domain adaptation,” in <em id="bib.bib278.1.1" class="ltx_emph ltx_font_italic">2020 25th International
Conference on Pattern Recognition (ICPR)</em>.   IEEE, 2021, pp. 9529–9536.

</span>
</li>
<li id="bib.bib279" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[279]</span>
<span class="ltx_bibblock">
T. Li, L. Fan, Y. Yuan, and D. Katabi, “Unsupervised learning for human
sensing using radio signals,” in <em id="bib.bib279.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision</em>, 2022, pp. 3288–3297.

</span>
</li>
<li id="bib.bib280" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[280]</span>
<span class="ltx_bibblock">
X. Li, Y. He, F. Fioranelli, and X. Jing, “Semisupervised human activity
recognition with radar micro-doppler signatures,” <em id="bib.bib280.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Geoscience and Remote Sensing</em>, vol. 60, pp. 1–12, 2021.

</span>
</li>
<li id="bib.bib281" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[281]</span>
<span class="ltx_bibblock">
Y. Li, D. Zhang, J. Chen, J. Wan, D. Zhang, Y. Hu, Q. Sun, and Y. Chen,
“Towards domain-independent and real-time gesture recognition using mmwave
signal,” <em id="bib.bib281.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Mobile Computing</em>, 2022.

</span>
</li>
<li id="bib.bib282" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[282]</span>
<span class="ltx_bibblock">
J. Domhof, J. F. Kooij, and D. M. Gavrila, “A joint extrinsic calibration tool
for radar, camera and lidar,” <em id="bib.bib282.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent
Vehicles</em>, vol. 6, no. 3, pp. 571–582, 2021.

</span>
</li>
<li id="bib.bib283" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[283]</span>
<span class="ltx_bibblock">
J. Zhang, S. Zhang, G. Peng, H. Zhang, and D. Wang, “3dradar2thermalcalib:
Accurate extrinsic calibration between a 3d mmwave radar and a thermal camera
using a spherical-trihedral,” in <em id="bib.bib283.1.1" class="ltx_emph ltx_font_italic">2022 IEEE 25th International
Conference on Intelligent Transportation Systems (ITSC)</em>.   IEEE, 2022, pp. 2744–2749.

</span>
</li>
<li id="bib.bib284" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[284]</span>
<span class="ltx_bibblock">
A. Sengupta, A. Yoshizawa, and S. Cao, “Automatic radar-camera dataset
generation for sensor-fusion applications,” <em id="bib.bib284.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and
Automation Letters</em>, vol. 7, no. 2, pp. 2875–2882, 2022.

</span>
</li>
<li id="bib.bib285" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[285]</span>
<span class="ltx_bibblock">
P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, B. B. Gupta, X. Chen, and
X. Wang, “A survey of deep active learning,” <em id="bib.bib285.1.1" class="ltx_emph ltx_font_italic">ACM computing surveys
(CSUR)</em>, vol. 54, no. 9, pp. 1–40, 2021.

</span>
</li>
<li id="bib.bib286" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[286]</span>
<span class="ltx_bibblock">
X. Zhan, Q. Wang, K.-h. Huang, H. Xiong, D. Dou, and A. B. Chan, “A
comparative survey of deep active learning,” <em id="bib.bib286.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2203.13450</em>, 2022.

</span>
</li>
<li id="bib.bib287" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[287]</span>
<span class="ltx_bibblock">
T. Winterling, J. Lombacher, M. Hahn, J. Dickmann, and C. Wöhler,
“Optimizing labelling on radar-based grid maps using active learning,” in
<em id="bib.bib287.1.1" class="ltx_emph ltx_font_italic">2017 18th International Radar Symposium (IRS)</em>.   IEEE, 2017, pp. 1–6.

</span>
</li>
<li id="bib.bib288" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[288]</span>
<span class="ltx_bibblock">
V. M. Patel, R. Gopalan, R. Li, and R. Chellappa, “Visual domain adaptation: A
survey of recent advances,” <em id="bib.bib288.1.1" class="ltx_emph ltx_font_italic">IEEE signal processing magazine</em>, vol. 32,
no. 3, pp. 53–69, 2015.

</span>
</li>
<li id="bib.bib289" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[289]</span>
<span class="ltx_bibblock">
Y. Chen, W. Li, X. Chen, and L. V. Gool, “Learning semantic segmentation from
synthetic data: A geometrically guided input-output adaptation approach,” in
<em id="bib.bib289.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition</em>, 2019, pp. 1841–1850.

</span>
</li>
<li id="bib.bib290" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[290]</span>
<span class="ltx_bibblock">
S. J. Pan and Q. Yang, “A survey on transfer learning,” <em id="bib.bib290.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on knowledge and data engineering</em>, vol. 22, no. 10, pp.
1345–1359, 2009.

</span>
</li>
<li id="bib.bib291" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[291]</span>
<span class="ltx_bibblock">
D. P. Kingma, S. Mohamed, D. Jimenez Rezende, and M. Welling, “Semi-supervised
learning with deep generative models,” <em id="bib.bib291.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, vol. 27, 2014.

</span>
</li>
<li id="bib.bib292" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[292]</span>
<span class="ltx_bibblock">
G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, “Continual
lifelong learning with neural networks: A review,” <em id="bib.bib292.1.1" class="ltx_emph ltx_font_italic">Neural networks</em>,
vol. 113, pp. 54–71, 2019.

</span>
</li>
<li id="bib.bib293" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[293]</span>
<span class="ltx_bibblock">
A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,
“Pointpillars: Fast encoders for object detection from point clouds,” in
<em id="bib.bib293.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition</em>, 2019, pp. 12 697–12 705.

</span>
</li>
<li id="bib.bib294" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[294]</span>
<span class="ltx_bibblock">
N. Scheiner, F. Kraus, N. Appenrodt, J. Dickmann, and B. Sick, “Object
detection for automotive radar point clouds–a comparison,” <em id="bib.bib294.1.1" class="ltx_emph ltx_font_italic">AI
Perspectives</em>, vol. 3, no. 1, pp. 1–23, 2021.

</span>
</li>
<li id="bib.bib295" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[295]</span>
<span class="ltx_bibblock">
J. Bai, L. Zheng, S. Li, B. Tan, S. Chen, and L. Huang, “Radar transformer: An
object classification network based on 4d mmw imaging radar,”
<em id="bib.bib295.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 21, no. 11, p. 3854, 2021.

</span>
</li>
<li id="bib.bib296" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[296]</span>
<span class="ltx_bibblock">
B. Xu, X. Zhang, L. Wang, X. Hu, Z. Li, S. Pan, J. Li, and Y. Deng, “Rpfa-net:
A 4d radar pillar feature attention network for 3d object detection,” in
<em id="bib.bib296.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Intelligent Transportation Systems Conference
(ITSC)</em>.   IEEE, 2021, pp. 3061–3066.

</span>
</li>
<li id="bib.bib297" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[297]</span>
<span class="ltx_bibblock">
F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, “The
graph neural network model,” <em id="bib.bib297.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks</em>,
vol. 20, no. 1, pp. 61–80, 2008.

</span>
</li>
<li id="bib.bib298" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[298]</span>
<span class="ltx_bibblock">
M. Meyer, G. Kuschk, and S. Tomforde, “Graph convolutional networks for 3d
object detection on radar data,” in <em id="bib.bib298.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2021, pp. 3060–3069.

</span>
</li>
<li id="bib.bib299" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[299]</span>
<span class="ltx_bibblock">
J. Bai, S. Li, L. Huang, and H. Chen, “Robust detection and tracking method
for moving object based on radar and camera data fusion,” <em id="bib.bib299.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors
Journal</em>, vol. 21, no. 9, pp. 10 761–10 774, 2021.

</span>
</li>
<li id="bib.bib300" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[300]</span>
<span class="ltx_bibblock">
S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, “Cutmix:
Regularization strategy to train strong classifiers with localizable
features,” in <em id="bib.bib300.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on
computer vision</em>, 2019, pp. 6023–6032.

</span>
</li>
<li id="bib.bib301" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[301]</span>
<span class="ltx_bibblock">
W. Wang, D. Tran, and M. Feiszli, “What makes training multi-modal
classification networks hard?” in <em id="bib.bib301.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>, 2020, pp.
12 695–12 705.

</span>
</li>
<li id="bib.bib302" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[302]</span>
<span class="ltx_bibblock">
Q. Dou, Q. Liu, P. A. Heng, and B. Glocker, “Unpaired multi-modal segmentation
via knowledge distillation,” <em id="bib.bib302.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>,
vol. 39, no. 7, pp. 2415–2425, 2020.

</span>
</li>
<li id="bib.bib303" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[303]</span>
<span class="ltx_bibblock">
C. Du, T. Li, Y. Liu, Z. Wen, T. Hua, Y. Wang, and H. Zhao, “Improving
multi-modal learning with uni-modal teachers,” <em id="bib.bib303.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2106.11059</em>, 2021.

</span>
</li>
<li id="bib.bib304" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[304]</span>
<span class="ltx_bibblock">
W. Zhang, H. Zhou, S. Sun, Z. Wang, J. Shi, and C. C. Loy, “Robust
multi-modality multi-object tracking,” in <em id="bib.bib304.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2019, pp. 2365–2374.

</span>
</li>
<li id="bib.bib305" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[305]</span>
<span class="ltx_bibblock">
D. J. MacKay, “A practical bayesian framework for backpropagation networks,”
<em id="bib.bib305.1.1" class="ltx_emph ltx_font_italic">Neural computation</em>, vol. 4, no. 3, pp. 448–472, 1992.

</span>
</li>
<li id="bib.bib306" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[306]</span>
<span class="ltx_bibblock">
T. A. Wheeler, M. Holder, H. Winner, and M. J. Kochenderfer, “Deep stochastic
radar models,” in <em id="bib.bib306.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Intelligent Vehicles Symposium
(IV)</em>.   IEEE, 2017, pp. 47–53.

</span>
</li>
<li id="bib.bib307" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[307]</span>
<span class="ltx_bibblock">
V. Lekic and Z. Babic, “Automotive radar and camera fusion using generative
adversarial networks,” <em id="bib.bib307.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, vol.
184, pp. 1–8, 2019.

</span>
</li>
<li id="bib.bib308" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[308]</span>
<span class="ltx_bibblock">
D. Hall, F. Dayoub, J. Skinner, H. Zhang, D. Miller, P. Corke, G. Carneiro,
A. Angelova, and N. Sünderhauf, “Probabilistic object detection:
Definition and evaluation,” in <em id="bib.bib308.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision</em>, 2020, pp. 1031–1040.

</span>
</li>
<li id="bib.bib309" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[309]</span>
<span class="ltx_bibblock">
A. Dhamija, M. Gunther, J. Ventura, and T. Boult, “The overlooked elephant of
object detection: Open set,” in <em id="bib.bib309.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision</em>, 2020, pp. 1021–1030.

</span>
</li>
<li id="bib.bib310" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[310]</span>
<span class="ltx_bibblock">
K. Joseph, S. Khan, F. S. Khan, and V. N. Balasubramanian, “Towards open world
object detection,” in <em id="bib.bib310.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition</em>, 2021, pp. 5830–5840.

</span>
</li>
<li id="bib.bib311" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[311]</span>
<span class="ltx_bibblock">
G. Montavon, W. Samek, and K.-R. Müller, “Methods for interpreting and
understanding deep neural networks,” <em id="bib.bib311.1.1" class="ltx_emph ltx_font_italic">Digital signal processing</em>,
vol. 73, pp. 1–15, 2018.

</span>
</li>
<li id="bib.bib312" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[312]</span>
<span class="ltx_bibblock">
F. Hohman, M. Kahng, R. Pienta, and D. H. Chau, “Visual analytics in deep
learning: An interrogative survey for the next frontiers,” <em id="bib.bib312.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Visualization and Computer Graphics</em>, vol. 25, no. 8, pp.
2674–2693, 2018.

</span>
</li>
<li id="bib.bib313" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[313]</span>
<span class="ltx_bibblock">
H. Yu, F. Zhang, P. Huang, C. Wang, and L. Yuanhao, “Autonomous obstacle
avoidance for uav based on fusion of radar and monocular camera,” in
<em id="bib.bib313.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</em>.   IEEE, 2020, pp.
5954–5961.

</span>
</li>
<li id="bib.bib314" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[314]</span>
<span class="ltx_bibblock">
S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep neural
networks with pruning, trained quantization and huffman coding,” <em id="bib.bib314.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1510.00149</em>, 2015.

</span>
</li>
<li id="bib.bib315" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[315]</span>
<span class="ltx_bibblock">
Y. Cheng, D. Wang, P. Zhou, and T. Zhang, “A survey of model compression and
acceleration for deep neural networks,” <em id="bib.bib315.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1710.09282</em>, 2017.

</span>
</li>
<li id="bib.bib316" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[316]</span>
<span class="ltx_bibblock">
Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. L. Rus, and S. Han, “Bevfusion:
Multi-task multi-sensor fusion with unified bird’s-eye view representation,”
in <em id="bib.bib316.1.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Robotics and Automation
(ICRA)</em>.   IEEE, 2023, pp. 2774–2781.

</span>
</li>
<li id="bib.bib317" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[317]</span>
<span class="ltx_bibblock">
M. Meyer and G. Kuschk, “Automotive radar dataset for deep learning based 3d
object detection,” in <em id="bib.bib317.1.1" class="ltx_emph ltx_font_italic">2019 16th european radar conference
(EuRAD)</em>.   IEEE, 2019, pp. 129–132.

</span>
</li>
<li id="bib.bib318" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[318]</span>
<span class="ltx_bibblock">
Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage
object detection,” in <em id="bib.bib318.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international
conference on computer vision</em>, 2019, pp. 9627–9636.

</span>
</li>
<li id="bib.bib319" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[319]</span>
<span class="ltx_bibblock">
F. Yu, D. Wang, E. Shelhamer, and T. Darrell, “Deep layer aggregation,” in
<em id="bib.bib319.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2018, pp. 2403–2412.

</span>
</li>
<li id="bib.bib320" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[320]</span>
<span class="ltx_bibblock">
X. Chen, T. Zhang, Y. Wang, Y. Wang, and H. Zhao, “Futr3d: A unified sensor
fusion framework for 3d detection,” in <em id="bib.bib320.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 172–181.

</span>
</li>
<li id="bib.bib321" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[321]</span>
<span class="ltx_bibblock">
F. Drews, D. Feng, F. Faion, L. Rosenbaum, M. Ulrich, and C. Gläser,
“Deepfusion: A robust and modular 3d object detector for lidars, cameras and
radars,” in <em id="bib.bib321.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS)</em>.   IEEE,
2022, pp. 560–567.

</span>
</li>
<li id="bib.bib322" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[322]</span>
<span class="ltx_bibblock">
Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A convnet
for the 2020s,” in <em id="bib.bib322.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition</em>, 2022, pp. 11 976–11 986.

</span>
</li>
<li id="bib.bib323" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[323]</span>
<span class="ltx_bibblock">
R. Guan, S. Yao, X. Zhu, K. L. Man, E. G. Lim, J. Smith, Y. Yue, and Y. Yue,
“Achelous: A fast unified water-surface panoptic perception framework based
on fusion of monocular camera and 4d mmwave radar,” <em id="bib.bib323.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2307.07102</em>, 2023.

</span>
</li>
</ul>
</section>
<figure id="id16" class="ltx_float biography">
<table id="id16.1" class="ltx_tabular">
<tr id="id16.1.1" class="ltx_tr">
<td id="id16.1.1.1" class="ltx_td"><img src="/html/2304.10410/assets/photo/Yao.jpg" id="id16.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="89" height="125" alt="[Uncaptioned image]"></td>
<td id="id16.1.1.2" class="ltx_td">
<span id="id16.1.1.2.1" class="ltx_inline-block">
<span id="id16.1.1.2.1.1" class="ltx_p"><span id="id16.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Shanliang Yao</span>  (Student Member, IEEE) received the B.E. degree in 2016 from the School of Computer Science and Technology, Soochow University, Suzhou, China, and the M.S. degree in 2021 from the Faculty of Science and Engineering, University of Liverpool, Liverpool, U.K. He is currently a joint Ph.D. student of University of Liverpool, Xi’an Jiaotong-Liverpool University and Institute of Deep Perception Technology, Jiangsu Industrial Technology Research Institute. His current research is centered on multi-modal perception using deep learning approach for autonomous driving. He is also interested in robotics, autonomous vehicles and intelligent transportation systems.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id17" class="ltx_float biography">
<table id="id17.1" class="ltx_tabular">
<tr id="id17.1.1" class="ltx_tr">
<td id="id17.1.1.1" class="ltx_td"><img src="/html/2304.10410/assets/photo/Guan.jpg" id="id17.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="90" height="125" alt="[Uncaptioned image]"></td>
<td id="id17.1.1.2" class="ltx_td">
<span id="id17.1.1.2.1" class="ltx_inline-block">
<span id="id17.1.1.2.1.1" class="ltx_p"><span id="id17.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Runwei Guan</span>  (Student Member, IEEE) received his M.S. degree in Data Science from University of Southampton, Southampton, United Kingdom, in 2021. He is currently a joint Ph.D. student of University of Liverpool, Xi’an Jiaotong-Liverpool University and Institute of Deep Perception Technology, Jiangsu Industrial Technology Research Institute. His research interests include visual grounding, panoptic perception based on the fusion of radar and camera, lightweight neural network, multi-task learning and statistical machine learning. He serves as the peer reviewer of IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, Engineering Applications of Artificial Intelligence, Journal of Supercomputing, IJCNN, etc.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id18" class="ltx_float biography">
<table id="id18.1" class="ltx_tabular">
<tr id="id18.1.1" class="ltx_tr">
<td id="id18.1.1.1" class="ltx_td"><img src="/html/2304.10410/assets/photo/Huang.jpg" id="id18.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="89" height="125" alt="[Uncaptioned image]"></td>
<td id="id18.1.1.2" class="ltx_td">
<span id="id18.1.1.2.1" class="ltx_inline-block">
<span id="id18.1.1.2.1.1" class="ltx_p"><span id="id18.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Xiaoyu Huang</span>  (Student Member, IEEE) received the B.S. degree in electronic science and technology from Nanjing University of Information Science &amp; Technology in 2019. In 2022, he received the M.S. degree from the University of Liverpool and Xi’an Jiaotong-Liverpool University joint program, where he is currently working toward the Ph.D. degree with the School of Advanced Technology. His research interests include computer vision, pattern recognition, deep learning, and image processing.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id19" class="ltx_float biography">
<table id="id19.1" class="ltx_tabular">
<tr id="id19.1.1" class="ltx_tr">
<td id="id19.1.1.1" class="ltx_td"><img src="/html/2304.10410/assets/photo/Li.png" id="id19.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="99" height="125" alt="[Uncaptioned image]"></td>
<td id="id19.1.1.2" class="ltx_td">
<span id="id19.1.1.2.1" class="ltx_inline-block">
<span id="id19.1.1.2.1.1" class="ltx_p"><span id="id19.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Zhuoxiao Li</span>  (Student Member, IEEE) received the MSc degree in Information Systems (2020) from the Information School, University of Sheffield, Sheffield, UK. After that, he joined the VR+ Culture Lab of the School of Information Management, Sun Yat-sen University as a research assistant. He is currently a Ph.D student at the Department of Computer Science at the University of Liverpool. His current main research direction is the combination of unmanned surface vehicle and virtual reality/augmented reality. He is also interested in deep learning applications in geographic information systems and remote sensing.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id20" class="ltx_float biography">
<table id="id20.1" class="ltx_tabular">
<tr id="id20.1.1" class="ltx_tr">
<td id="id20.1.1.1" class="ltx_td"><img src="/html/2304.10410/assets/photo/Sha.jpg" id="id20.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="89" height="125" alt="[Uncaptioned image]"></td>
<td id="id20.1.1.2" class="ltx_td">
<span id="id20.1.1.2.1" class="ltx_inline-block">
<span id="id20.1.1.2.1.1" class="ltx_p"><span id="id20.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Xiangyu Sha</span>  (Student Member, IEEE) is currently working toward the B.S. degree in computer science from University of Liverpool, United Kingdom. She will complete her undergraduate study in 2024. Her research interests include computer vision, virtual reality, extended reality, robotics simulation and sensor fusion in autonomous driving vehicles. She has won full scholarships for two consecutive years and participated the Programme and Poster Competition as a Summer Undergraduate Research Fellow in 2022.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id21" class="ltx_float biography">
<table id="id21.1" class="ltx_tabular">
<tr id="id21.1.1" class="ltx_tr">
<td id="id21.1.1.1" class="ltx_td"><img src="/html/2304.10410/assets/photo/yongyue.png" id="id21.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="91" height="125" alt="[Uncaptioned image]"></td>
<td id="id21.1.1.2" class="ltx_td">
<span id="id21.1.1.2.1" class="ltx_inline-block">
<span id="id21.1.1.2.1.1" class="ltx_p"><span id="id21.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Yong Yue</span> 
Fellow of Institution of Engineering and Technology (FIET), received the B.Eng. degree in mechanical engineering from Northeastern University, Shenyang, China, in 1982, and the Ph.D. degree in computer aided design from Heriot-Watt University, Edinburgh, U.K., in 1994. He worked in the industry for eight years and followed experience in academia with the University of Nottingham, Cardiff University, and the University of Bedfordshire, U.K. He is currently a Professor and Director with the Virtual Engineering Centre, Xi’an Jiaotong-Liverpool University, Suzhou, China. His current research interests include computer graphics, virtual reality, and robot navigation.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id22" class="ltx_float biography">
<table id="id22.1" class="ltx_tabular">
<tr id="id22.1.1" class="ltx_tr">
<td id="id22.1.1.1" class="ltx_td"><img src="/html/2304.10410/assets/photo/lim.jpg" id="id22.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="123" alt="[Uncaptioned image]"></td>
<td id="id22.1.1.2" class="ltx_td">
<span id="id22.1.1.2.1" class="ltx_inline-block">
<span id="id22.1.1.2.1.1" class="ltx_p"><span id="id22.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Eng Gee Lim</span> 
(Senior Member, IEEE) received the B.Eng. (Hons.) and Ph.D. degrees in Electrical and Electronic Engineering (EEE) from Northumbria University, Newcastle, U.K., in 1998 and 2002,
respectively. He worked for Andrew Ltd., Coventry, U.K., a leading communications systems company from 2002 to 2007. Since 2007, he has been with Xi’an Jiaotong–Liverpool University, Suzhou, China, where he was the Head of the EEE Department, and the University Dean of research and graduate studies. He is currently the School Dean of Advanced Technology, the Director of the AI University Research Centre, and a Professor with the Department of EEE. He has authored or coauthored over 100 refereed international journals and conference papers. His research interests are artificial intelligence (AI), robotics, AI+ health care, international standard (ISO/IEC) in robotics, antennas, RF/microwave engineering, EM measurements/simulations, energy harvesting, power/energy transfer, smart-grid communication, and wireless communication networks for smart and green cities. He is a Charted Engineer and a fellow of The Institution of Engineering and Technology (IET) and Engineers Australia. He is also a Senior Fellow of Higher Education Academy (HEA).</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id23" class="ltx_float biography">
<table id="id23.1" class="ltx_tabular">
<tr id="id23.1.1" class="ltx_tr">
<td id="id23.1.1.1" class="ltx_td"><img src="/html/2304.10410/assets/photo/Seo.jpg" id="id23.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="91" height="125" alt="[Uncaptioned image]"></td>
<td id="id23.1.1.2" class="ltx_td">
<span id="id23.1.1.2.1" class="ltx_inline-block">
<span id="id23.1.1.2.1.1" class="ltx_p"><span id="id23.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Hyungjoon Seo</span>  (Member, IEEE), received the bachelor’s degree in civil engineering from Korea University, Seoul, South Korea, in 2007, and the Ph.D. degree in geotechnical engineering from Korea University in 2013. In 2013, he worked as a research professor in Korea University. He served as a visiting scholar at University of Cambridge, Cambridge, UK, and he worked for engineering department in University of Cambridge as a research associate from 2014 to 2016. In August 2016, he got an assistant professor position in the Department of Civil Engineering at the Xi’an Jiaotong Liverpool University (XJTLU), China. He has been an assistant professor at the University of Liverpool, UK, from 2020. His research interests are monitoring using artificial intelligence and SMART monitoring system for infrastructure, soil-structure interaction (tunneling, slope stability, pile), Antarctic survey and freezing ground.
Hyungjoon is the director of the CSMI (Centre for SMART Monitoring Infrastructure), CSMI is collaborating with University of Cambridge, University of Oxford, University of Bath, UC Berkeley University, Nanjing University, and Tongji University on SMART monitoring. He presented a keynote speech at the 15th European Conference on Soil Mechanics and Geotechnical Engineering in 2015. He is currently appointed editor of the CivilEng journal and organized two international conferences. He has published more than 50 scientific papers including a book on Geotechnical Engineering and SMART monitoring.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id24" class="ltx_float biography">
<table id="id24.1" class="ltx_tabular">
<tr id="id24.1.1" class="ltx_tr">
<td id="id24.1.1.1" class="ltx_td"><img src="/html/2304.10410/assets/photo/kalok.jpg" id="id24.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="100" height="125" alt="[Uncaptioned image]"></td>
<td id="id24.1.1.2" class="ltx_td">
<span id="id24.1.1.2.1" class="ltx_inline-block">
<span id="id24.1.1.2.1.1" class="ltx_p"><span id="id24.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Ka Lok Man</span> 
(Member, IEEE), received the Dr.Eng. degree in electronic engineering from the Politecnico di Torino, Turin, Italy, in 1998, and the Ph.D. degree in computer science from Technische Universiteit Eindhoven, Eindhoven, The Netherlands, in 2006. He is currently a Professor in Computer Science and Software Engineering with Xi’an Jiaotong-Liverpool University, Suzhou, China. His research interests include formal methods and process algebras, embedded system design and testing, and photovoltaics.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id25" class="ltx_float biography">
<table id="id25.1" class="ltx_tabular">
<tr id="id25.1.1" class="ltx_tr">
<td id="id25.1.1.1" class="ltx_td"><img src="/html/2304.10410/assets/photo/xiaohuizhu.png" id="id25.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="90" height="125" alt="[Uncaptioned image]"></td>
<td id="id25.1.1.2" class="ltx_td">
<span id="id25.1.1.2.1" class="ltx_inline-block">
<span id="id25.1.1.2.1.1" class="ltx_p"><span id="id25.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Xiaohui Zhu</span> 
(Member, IEEE) received his Ph.D. from the University of Liverpool, UK in 2019. He is currently an assistant professor, Ph.D. supervisor and Programme Director with the Department of Computing, School of Advanced Technology, Xi’an Jiaotong-Liverpool University. He focuses on advanced techniques related to autonomous driving, including sensor-fusion perception, fast path planning, autonomous navigation and multi-vehicle collaborative scheduling.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id26" class="ltx_float biography">
<table id="id26.1" class="ltx_tabular">
<tr id="id26.1.1" class="ltx_tr">
<td id="id26.1.1.1" class="ltx_td"><img src="/html/2304.10410/assets/photo/yutaoyue.jpg" id="id26.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="94" height="125" alt="[Uncaptioned image]"></td>
<td id="id26.1.1.2" class="ltx_td">
<span id="id26.1.1.2.1" class="ltx_inline-block">
<span id="id26.1.1.2.1.1" class="ltx_p"><span id="id26.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Yutao Yue</span> 
(Member, IEEE) was born in Qingzhou, Shandong, China, in 1982. He received the B.S. degree in applied physics from the University of Science and Technology of China, in 2004, and the M.S. and Ph.D. degrees in computational physics from Purdue University, USA, in 2006 and 2010, respectively. From 2011 to 2017, he worked as a Senior Scientist with the Shenzhen Kuang-Chi Institute of Advanced Technology and a Team Leader of the Guangdong “Zhujiang Plan” 3rd Introduced Innovation Scientific Research Team. From 2017 to 2018, he was a Research Associate Professor with the Southern University of Science and Technology, China. Since 2018, he has been the Founder and the Director of the Institute of Deep Perception Technology, JITRI, Jiangsu, China. Since 2020, he has been working as an Honorary Recognized Ph.D. Advisor of the University of Liverpool, U.K., and Xi’an Jiaotong-Liverpool University, China. He is the co-inventor of over 300 granted patents of China, USA, and Europe. He is also the author of over 20 journals and conference papers. His research interests include computational modeling, radar vision fusion, perception and cognition cooperation, artificial intelligence theory, and electromagnetic field modulation. Dr. Yue was a recipient of the Wu WenJun Artificial Intelligence Science and Technology Award in 2020.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="Sx1.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>OVERVIEW OF RADAR-CAMERA FUSION DATASETS</figcaption>
<table id="Sx1.T2.3" class="ltx_tabular">
<thead class="ltx_thead">
<tr id="Sx1.T2.3.1.1" class="ltx_tr">
<th id="Sx1.T2.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T2.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.1.1.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Name</span></span>
</span>
</th>
<th id="Sx1.T2.3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T2.3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.1.1.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Year</span></span>
</span>
</th>
<th id="Sx1.T2.3.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T2.3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.1.1.3.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T2.3.1.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Tasks</span></span>
</span>
</th>
<th id="Sx1.T2.3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T2.3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.1.1.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Annotations</span></span>
</span>
</th>
<th id="Sx1.T2.3.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T2.3.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.1.1.5.1.1" class="ltx_p" style="width:85.4pt;"><span id="Sx1.T2.3.1.1.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Data Representations</span></span>
</span>
</th>
<th id="Sx1.T2.3.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T2.3.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.1.1.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.1.1.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Categories</span></span>
</span>
</th>
<th id="Sx1.T2.3.1.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T2.3.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.1.1.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.1.1.7.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Size</span></span>
</span>
</th>
<th id="Sx1.T2.3.1.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T2.3.1.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.1.1.8.1.1" class="ltx_p" style="width:88.2pt;"><span id="Sx1.T2.3.1.1.8.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Link</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx1.T2.3.2.1" class="ltx_tr">
<td id="Sx1.T2.3.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.2.1.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.2.1.1.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.2.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T2.3.2.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.2.1.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.2.1.2.1.1.1" class="ltx_text" style="font-size:70%;">2019</span></span>
</span>
</td>
<td id="Sx1.T2.3.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.2.1.3.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T2.3.2.1.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.2.1.3.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.2.1.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T2.3.2.1.3.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.2.1.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Object Tracking</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.2.1.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.2.1.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.2.1.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.2.1.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.2.1.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.2.1.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.2.1.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.2.1.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.2.1.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.2.1.6.1.1.1" class="ltx_text" style="font-size:70%;">23 classes (Vehicle, Pedestrian, Bicycle, Movable Object, Static Object, etc.)</span></span>
</span>
</td>
<td id="Sx1.T2.3.2.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.2.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.2.1.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.2.1.7.1.1.1" class="ltx_text" style="font-size:70%;">1000 scenes, 1.4M boxes in 40k frames, 5.5 hours</span></span>
</span>
</td>
<td id="Sx1.T2.3.2.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.2.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.2.1.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://www.nuscenes.org/nuscenes" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://www.nuscenes.org/nuscenes</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.3.2" class="ltx_tr">
<td id="Sx1.T2.3.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.3.2.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.3.2.1.1.1.1" class="ltx_text" style="font-size:70%;">Astyx </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.3.2.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib317" title="" class="ltx_ref">317</a><span id="Sx1.T2.3.3.2.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.3.2.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.3.2.2.1.1.1" class="ltx_text" style="font-size:70%;">2019</span></span>
</span>
</td>
<td id="Sx1.T2.3.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.3.2.3.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T2.3.3.2.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T2.3.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.3.2.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.3.2.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.3.2.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.3.2.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.3.2.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.3.2.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.3.2.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.3.2.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.3.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.3.2.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.3.2.6.1.1.1" class="ltx_text" style="font-size:70%;">7 classes (Bus, Car, Cyclist, Motorcyclist, Person, Trailer, Truck)</span></span>
</span>
</td>
<td id="Sx1.T2.3.3.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.3.2.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.3.2.7.1.1.1" class="ltx_text" style="font-size:70%;">500 frames, around 3000 labeled objects</span></span>
</span>
</td>
<td id="Sx1.T2.3.3.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.3.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.3.2.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="http://www.astyx.net" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">http://www.astyx.net</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.4.3" class="ltx_tr">
<td id="Sx1.T2.3.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.4.3.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.4.3.1.1.1.1" class="ltx_text" style="font-size:70%;">SeeingThroughFog </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.4.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="Sx1.T2.3.4.3.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.4.3.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.4.3.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T2.3.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.4.3.3.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T2.3.4.3.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T2.3.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.4.3.4.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T2.3.4.3.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:173.4pt;">
<span id="Sx1.T2.3.4.3.4.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.4.3.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level,</span></span>
<span id="Sx1.T2.3.4.3.4.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.4.3.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.4.3.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.4.3.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:173.4pt;">
<span id="Sx1.T2.3.4.3.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.4.3.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image,</span></span>
<span id="Sx1.T2.3.4.3.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.4.3.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">gated image, thermal image;</span></span>
<span id="Sx1.T2.3.4.3.5.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.4.3.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.4.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.4.3.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.4.3.6.1.1.1" class="ltx_text" style="font-size:70%;">4 classes (Passenger Car, Large Vehicle, Pedestrian, Ridable Vehicle)</span></span>
</span>
</td>
<td id="Sx1.T2.3.4.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.4.3.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.4.3.7.1.1.1" class="ltx_text" style="font-size:70%;">12k samples in real-world driving scenes and 1.5k samples in controlled weather conditions within a fog chamber, 100k objects</span></span>
</span>
</td>
<td id="Sx1.T2.3.4.3.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.4.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.4.3.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://www.uni-ulm.de/en/in/driveu/projects/dense-datasets" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://www.uni-ulm.de/en/in/driveu/projects/dense-datasets</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.5.4" class="ltx_tr">
<td id="Sx1.T2.3.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.5.4.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.5.4.1.1.1.1" class="ltx_text" style="font-size:70%;">CARRADA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.5.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="Sx1.T2.3.5.4.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.5.4.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.5.4.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T2.3.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.5.4.3.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T2.3.5.4.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.5.4.3.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.5.4.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T2.3.5.4.3.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.5.4.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Semantic Segmentation,</span></span>
<span id="Sx1.T2.3.5.4.3.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.5.4.3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Object Tracking,</span></span>
<span id="Sx1.T2.3.5.4.3.1.1.1.4" class="ltx_p"><span id="Sx1.T2.3.5.4.3.1.1.1.4.1" class="ltx_text" style="font-size:70%;">Scene Understanding</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.5.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.5.4.4.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T2.3.5.4.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.5.4.4.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.5.4.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level,</span></span>
<span id="Sx1.T2.3.5.4.4.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.5.4.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">2D pixel-level</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.5.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.5.4.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.5.4.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.5.4.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.5.4.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.5.4.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.5.4.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: range-azimuth tensor,</span></span>
<span id="Sx1.T2.3.5.4.5.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.5.4.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">range-Doppler tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.5.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.5.4.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.5.4.6.1.1.1" class="ltx_text" style="font-size:70%;">3 classes (Pedestrian, Car, Cyclist)</span></span>
</span>
</td>
<td id="Sx1.T2.3.5.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.5.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.5.4.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.5.4.7.1.1.1" class="ltx_text" style="font-size:70%;">12,666 frames, 78 instances, 7,139 annotated frames with instances, 23GB synchronized camera and radar views</span></span>
</span>
</td>
<td id="Sx1.T2.3.5.4.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.5.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.5.4.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://arthurouaknine.github.io/codeanddata/carrada" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://arthurouaknine.github.io/codeanddata/carrada</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.6.5" class="ltx_tr">
<td id="Sx1.T2.3.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.6.5.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.6.5.1.1.1.1" class="ltx_text" style="font-size:70%;">HawkEye </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.6.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib180" title="" class="ltx_ref">180</a><span id="Sx1.T2.3.6.5.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.6.5.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.6.5.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T2.3.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.6.5.3.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T2.3.6.5.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.6.5.3.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.6.5.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Semantic Segmentation</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.6.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.6.5.4.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T2.3.6.5.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.6.5.4.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.6.5.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">3D point-level</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.6.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.6.5.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.6.5.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.6.5.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.6.5.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.6.5.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.6.5.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.6.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.6.5.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.6.5.6.1.1.1" class="ltx_text" style="font-size:70%;">9 classes of cars (Sub-compact, Compact, Mid-sized, Full-sized, Sport, SUV, Jeep, Van, Truck)</span></span>
</span>
</td>
<td id="Sx1.T2.3.6.5.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.6.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.6.5.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.6.5.7.1.1.1" class="ltx_text" style="font-size:70%;">3k images, 4k scenes, 120 car models</span></span>
</span>
</td>
<td id="Sx1.T2.3.6.5.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.6.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.6.5.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://jguan.page/HawkEye" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://jguan.page/HawkEye</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.7.6" class="ltx_tr">
<td id="Sx1.T2.3.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.7.6.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.7.6.1.1.1.1" class="ltx_text" style="font-size:70%;">Zendar </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.7.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib167" title="" class="ltx_ref">167</a><span id="Sx1.T2.3.7.6.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.7.6.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.7.6.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T2.3.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.7.6.3.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T2.3.7.6.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.7.6.3.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.7.6.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T2.3.7.6.3.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.7.6.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Mapping,</span></span>
<span id="Sx1.T2.3.7.6.3.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.7.6.3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Localization</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.7.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.7.6.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.7.6.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.7.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.7.6.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.7.6.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.7.6.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.7.6.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.7.6.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.7.6.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: range-Doppler tensor,</span></span>
<span id="Sx1.T2.3.7.6.5.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.7.6.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">range-azimuth tensor,</span></span>
<span id="Sx1.T2.3.7.6.5.1.1.1.4" class="ltx_p"><span id="Sx1.T2.3.7.6.5.1.1.1.4.1" class="ltx_text" style="font-size:70%;">point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.7.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.7.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.7.6.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.7.6.6.1.1.1" class="ltx_text" style="font-size:70%;">1 class (Car)</span></span>
</span>
</td>
<td id="Sx1.T2.3.7.6.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.7.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.7.6.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.7.6.7.1.1.1" class="ltx_text" style="font-size:70%;">Over 11k moving cars labeled in 27 diverse scenes with over 40k automatically generated labels</span></span>
</span>
</td>
<td id="Sx1.T2.3.7.6.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.7.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.7.6.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="http://zendar.io/dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">http://zendar.io/dataset</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.8.7" class="ltx_tr">
<td id="Sx1.T2.3.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.8.7.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.8.7.1.1.1.1" class="ltx_text" style="font-size:70%;">RADIATE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.8.7.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib168" title="" class="ltx_ref">168</a><span id="Sx1.T2.3.8.7.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.8.7.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.8.7.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T2.3.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.8.7.3.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T2.3.8.7.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T2.3.8.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.8.7.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.8.7.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.8.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.8.7.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.8.7.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.8.7.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.8.7.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.8.7.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.8.7.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: range-azimuth tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.8.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.8.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.8.7.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.8.7.6.1.1.1" class="ltx_text" style="font-size:70%;">8 classes (Car, Van, Bus, Truck, Motorbike, Bicycle, Pedestrian, A group of pedestrians)</span></span>
</span>
</td>
<td id="Sx1.T2.3.8.7.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.8.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.8.7.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.8.7.7.1.1.1" class="ltx_text" style="font-size:70%;">200k bounding boxes over 44k radar frames</span></span>
</span>
</td>
<td id="Sx1.T2.3.8.7.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.8.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.8.7.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="http://pro.hw.ac.uk/radiate" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">http://pro.hw.ac.uk/radiate</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.9.8" class="ltx_tr">
<td id="Sx1.T2.3.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.9.8.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.9.8.1.1.1.1" class="ltx_text" style="font-size:70%;">AIODrive </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.9.8.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib169" title="" class="ltx_ref">169</a><span id="Sx1.T2.3.9.8.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.9.8.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.9.8.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T2.3.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.9.8.3.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T2.3.9.8.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.9.8.3.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.9.8.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T2.3.9.8.3.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.9.8.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Semantic Segmentation,</span></span>
<span id="Sx1.T2.3.9.8.3.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.9.8.3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Object Tracking,</span></span>
<span id="Sx1.T2.3.9.8.3.1.1.1.4" class="ltx_p"><span id="Sx1.T2.3.9.8.3.1.1.1.4.1" class="ltx_text" style="font-size:70%;">Trajectory Prediction,</span></span>
<span id="Sx1.T2.3.9.8.3.1.1.1.5" class="ltx_p"><span id="Sx1.T2.3.9.8.3.1.1.1.5.1" class="ltx_text" style="font-size:70%;">Depth Estimation</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.9.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.9.8.4.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T2.3.9.8.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.9.8.4.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.9.8.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level,</span></span>
<span id="Sx1.T2.3.9.8.4.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.9.8.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.9.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.9.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.9.8.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.9.8.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.9.8.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.9.8.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.9.8.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.9.8.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.9.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.9.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.9.8.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.9.8.6.1.1.1" class="ltx_text" style="font-size:70%;">11 classes (Vehicle, Pedestrian, Vegetation, Building, Road, Sidewalk, Wall, Traffic Sign, Pole and Fence)</span></span>
</span>
</td>
<td id="Sx1.T2.3.9.8.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.9.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.9.8.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.9.8.7.1.1.1" class="ltx_text" style="font-size:70%;">500k annotated images for five camera viewpoints, 100k annotated frames for radar sensor</span></span>
</span>
</td>
<td id="Sx1.T2.3.9.8.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.9.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.9.8.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="http://www.aiodrive.org" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">http://www.aiodrive.org</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.10.9" class="ltx_tr">
<td id="Sx1.T2.3.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.10.9.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.10.9.1.1.1.1" class="ltx_text" style="font-size:70%;">CRUW </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.10.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib122" title="" class="ltx_ref">122</a><span id="Sx1.T2.3.10.9.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.10.9.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.10.9.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T2.3.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.10.9.3.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T2.3.10.9.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T2.3.10.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.10.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.10.9.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.10.9.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.10.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.10.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.10.9.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.10.9.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.10.9.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.10.9.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.10.9.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.10.9.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: range-azimuth tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.10.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.10.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.10.9.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.10.9.6.1.1.1" class="ltx_text" style="font-size:70%;">3 classes (Pedestrian, Cyclist, Car)</span></span>
</span>
</td>
<td id="Sx1.T2.3.10.9.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.10.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.10.9.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.10.9.7.1.1.1" class="ltx_text" style="font-size:70%;">400k frames, 260k objects, 3.5 hours</span></span>
</span>
</td>
<td id="Sx1.T2.3.10.9.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.10.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.10.9.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https:/www.cruwdataset.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https:/www.cruwdataset.org/</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.11.10" class="ltx_tr">
<td id="Sx1.T2.3.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.11.10.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.11.10.1.1.1.1" class="ltx_text" style="font-size:70%;">RaDICaL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.11.10.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib171" title="" class="ltx_ref">171</a><span id="Sx1.T2.3.11.10.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.11.10.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.11.10.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T2.3.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.11.10.3.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T2.3.11.10.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T2.3.11.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.11.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.11.10.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.11.10.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.11.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.11.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.11.10.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.11.10.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.11.10.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.11.10.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image,</span></span>
<span id="Sx1.T2.3.11.10.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.11.10.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">RGB-D image;</span></span>
<span id="Sx1.T2.3.11.10.5.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.11.10.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Radar: ADC signal</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.11.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.11.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.11.10.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.11.10.6.1.1.1" class="ltx_text" style="font-size:70%;">2 classes (Car, Pedestrian)</span></span>
</span>
</td>
<td id="Sx1.T2.3.11.10.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.11.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.11.10.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.11.10.7.1.1.1" class="ltx_text" style="font-size:70%;">393k frames</span></span>
</span>
</td>
<td id="Sx1.T2.3.11.10.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.11.10.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.11.10.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://publish.illinois.edu/radicaldata" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://publish.illinois.edu/radicaldata</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.12.11" class="ltx_tr">
<td id="Sx1.T2.3.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.12.11.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.12.11.1.1.1.1" class="ltx_text" style="font-size:70%;">RadarScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.12.11.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib179" title="" class="ltx_ref">179</a><span id="Sx1.T2.3.12.11.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.12.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.12.11.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.12.11.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T2.3.12.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.12.11.3.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T2.3.12.11.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.12.11.3.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.12.11.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Semantic Segmentation,</span></span>
<span id="Sx1.T2.3.12.11.3.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.12.11.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Object Tracking</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.12.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.12.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.12.11.4.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T2.3.12.11.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.12.11.4.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.12.11.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">2D point-level</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.12.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.12.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.12.11.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.12.11.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.12.11.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.12.11.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.12.11.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.12.11.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.12.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.12.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.12.11.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.12.11.6.1.1.1" class="ltx_text" style="font-size:70%;">11 classes (Car, Large Vehicle, Truck, Bus, Train, Bicycle, Motorized Two-wheeler, Pedestrian, Pedestrian Group, Animal, Other)</span></span>
</span>
</td>
<td id="Sx1.T2.3.12.11.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.12.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.12.11.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.12.11.7.1.1.1" class="ltx_text" style="font-size:70%;">40,208 frames, 158 individual sequences, 118.9M radar points</span></span>
</span>
</td>
<td id="Sx1.T2.3.12.11.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.12.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.12.11.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://radar-scenes.com" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://radar-scenes.com</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.13.12" class="ltx_tr">
<td id="Sx1.T2.3.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.13.12.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.13.12.1.1.1.1" class="ltx_text" style="font-size:70%;">RADDet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.13.12.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib97" title="" class="ltx_ref">97</a><span id="Sx1.T2.3.13.12.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.13.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.13.12.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.13.12.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T2.3.13.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.13.12.3.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T2.3.13.12.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T2.3.13.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.13.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.13.12.4.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T2.3.13.12.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.13.12.4.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.13.12.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level,</span></span>
<span id="Sx1.T2.3.13.12.4.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.13.12.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.13.12.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.13.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.13.12.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.13.12.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.13.12.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.13.12.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.13.12.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.13.12.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: range-azimuth-Doppler</span></span>
<span id="Sx1.T2.3.13.12.5.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.13.12.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.13.12.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.13.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.13.12.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.13.12.6.1.1.1" class="ltx_text" style="font-size:70%;">6 classes (Person, Bicycle, Car, Motorcycle, Bus, Truck)</span></span>
</span>
</td>
<td id="Sx1.T2.3.13.12.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.13.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.13.12.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.13.12.7.1.1.1" class="ltx_text" style="font-size:70%;">10,158 frames</span></span>
</span>
</td>
<td id="Sx1.T2.3.13.12.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.13.12.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.13.12.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://github.com/ZhangAoCanada/RADDet" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/ZhangAoCanada/RADDet</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.14.13" class="ltx_tr">
<td id="Sx1.T2.3.14.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.14.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.14.13.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.14.13.1.1.1.1" class="ltx_text" style="font-size:70%;">FloW </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.14.13.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib172" title="" class="ltx_ref">172</a><span id="Sx1.T2.3.14.13.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.14.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.14.13.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.14.13.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T2.3.14.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.14.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.14.13.3.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T2.3.14.13.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T2.3.14.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.14.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.14.13.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.14.13.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.14.13.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.14.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.14.13.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.14.13.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.14.13.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.14.13.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.14.13.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.14.13.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: range-Doppler tensor,</span></span>
<span id="Sx1.T2.3.14.13.5.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.14.13.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.14.13.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.14.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.14.13.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.14.13.6.1.1.1" class="ltx_text" style="font-size:70%;">1 class (Bottle)</span></span>
</span>
</td>
<td id="Sx1.T2.3.14.13.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.14.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.14.13.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.14.13.7.1.1.1" class="ltx_text" style="font-size:70%;">4k frames</span></span>
</span>
</td>
<td id="Sx1.T2.3.14.13.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.14.13.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.14.13.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://github.com/ORCA-Uboat/FloW-Dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/ORCA-Uboat/FloW-Dataset</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.15.14" class="ltx_tr">
<td id="Sx1.T2.3.15.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.15.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.15.14.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.15.14.1.1.1.1" class="ltx_text" style="font-size:70%;">RADIal </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.15.14.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib105" title="" class="ltx_ref">105</a><span id="Sx1.T2.3.15.14.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.15.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.15.14.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.15.14.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T2.3.15.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.15.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.15.14.3.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T2.3.15.14.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.15.14.3.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.15.14.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T2.3.15.14.3.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.15.14.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Semantic Segmentation</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.15.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.15.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.15.14.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.15.14.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.15.14.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.15.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.15.14.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.15.14.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.15.14.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.15.14.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.15.14.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.15.14.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: ADC signal,</span></span>
<span id="Sx1.T2.3.15.14.5.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.15.14.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">range-azimuth-Doppler tensor,</span></span>
<span id="Sx1.T2.3.15.14.5.1.1.1.4" class="ltx_p"><span id="Sx1.T2.3.15.14.5.1.1.1.4.1" class="ltx_text" style="font-size:70%;">range-azimuth tensor,</span></span>
<span id="Sx1.T2.3.15.14.5.1.1.1.5" class="ltx_p"><span id="Sx1.T2.3.15.14.5.1.1.1.5.1" class="ltx_text" style="font-size:70%;">range-Doppler tensor,</span></span>
<span id="Sx1.T2.3.15.14.5.1.1.1.6" class="ltx_p"><span id="Sx1.T2.3.15.14.5.1.1.1.6.1" class="ltx_text" style="font-size:70%;">point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.15.14.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.15.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.15.14.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.15.14.6.1.1.1" class="ltx_text" style="font-size:70%;">1 class (Vehicle)</span></span>
</span>
</td>
<td id="Sx1.T2.3.15.14.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.15.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.15.14.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.15.14.7.1.1.1" class="ltx_text" style="font-size:70%;">8,252 frames are labelled with 9,550 vehicle</span></span>
</span>
</td>
<td id="Sx1.T2.3.15.14.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.15.14.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.15.14.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://github.com/valeoai/RADIal" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/valeoai/RADIal</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.16.15" class="ltx_tr">
<td id="Sx1.T2.3.16.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.16.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.16.15.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.16.15.1.1.1.1" class="ltx_text" style="font-size:70%;">VoD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.16.15.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="Sx1.T2.3.16.15.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.16.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.16.15.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.16.15.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T2.3.16.15.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.16.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.16.15.3.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T2.3.16.15.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T2.3.16.15.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.16.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.16.15.4.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T2.3.16.15.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.16.15.4.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.16.15.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level,</span></span>
<span id="Sx1.T2.3.16.15.4.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.16.15.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.16.15.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.16.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.16.15.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.16.15.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.16.15.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.16.15.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.16.15.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.16.15.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.16.15.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.16.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.16.15.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.16.15.6.1.1.1" class="ltx_text" style="font-size:70%;">13 classes (Car, Pedestrian, Cyclist, Rider, Unused Bicycle, Bicycle Rack, Human Depiction, Moped or Scooter, Motor, Ride Other, Vehicle Other, Truck, Ride Uncertain)</span></span>
</span>
</td>
<td id="Sx1.T2.3.16.15.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.16.15.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.16.15.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.16.15.7.1.1.1" class="ltx_text" style="font-size:70%;">8693 frames, 123,106 annotations of both moving and static objects, including 26,587 pedestrian, 10,800 cyclist and 26,949 car labels</span></span>
</span>
</td>
<td id="Sx1.T2.3.16.15.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.16.15.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.16.15.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://tudelft-iv.github.io/view-of-delft-dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://tudelft-iv.github.io/view-of-delft-dataset</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.17.16" class="ltx_tr">
<td id="Sx1.T2.3.17.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.17.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.17.16.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.17.16.1.1.1.1" class="ltx_text" style="font-size:70%;">Boreas </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.17.16.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib173" title="" class="ltx_ref">173</a><span id="Sx1.T2.3.17.16.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.17.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.17.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.17.16.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.17.16.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T2.3.17.16.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.17.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.17.16.3.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T2.3.17.16.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.17.16.3.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.17.16.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T2.3.17.16.3.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.17.16.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Localization,</span></span>
<span id="Sx1.T2.3.17.16.3.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.17.16.3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Odometry</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.17.16.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.17.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.17.16.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.17.16.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.17.16.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.17.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.17.16.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.17.16.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.17.16.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.17.16.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.17.16.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.17.16.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: range-azimuth tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.17.16.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.17.16.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.17.16.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.17.16.6.1.1.1" class="ltx_text" style="font-size:70%;">4 classes (Car, Pedestrian, Cyclist, Misc)</span></span>
</span>
</td>
<td id="Sx1.T2.3.17.16.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.17.16.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.17.16.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.17.16.7.1.1.1" class="ltx_text" style="font-size:70%;">7.1k frames for detection, over 350km of driving data, 326,180 unique 3D box annotations</span></span>
</span>
</td>
<td id="Sx1.T2.3.17.16.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.17.16.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.17.16.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://www.boreas.utias.utoronto.ca" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://www.boreas.utias.utoronto.ca</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.18.17" class="ltx_tr">
<td id="Sx1.T2.3.18.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.18.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.18.17.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.18.17.1.1.1.1" class="ltx_text" style="font-size:70%;">TJ4DRadSet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.18.17.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib176" title="" class="ltx_ref">176</a><span id="Sx1.T2.3.18.17.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.18.17.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.18.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.18.17.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.18.17.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T2.3.18.17.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.18.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.18.17.3.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T2.3.18.17.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.18.17.3.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.18.17.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T2.3.18.17.3.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.18.17.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Object Tracking</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.18.17.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.18.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.18.17.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.18.17.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.18.17.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.18.17.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.18.17.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.18.17.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.18.17.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.18.17.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.18.17.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.18.17.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.18.17.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.18.17.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.18.17.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.18.17.6.1.1.1" class="ltx_text" style="font-size:70%;">8 classes (Car, Pedestrian, Cyclist, Bus, Motorcyclist, Truck, Engineering Vehicle, Tricyclist)</span></span>
</span>
</td>
<td id="Sx1.T2.3.18.17.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.18.17.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.18.17.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.18.17.7.1.1.1" class="ltx_text" style="font-size:70%;">40k frames in total, 7757 frames within 44 consecutive sequences</span></span>
</span>
</td>
<td id="Sx1.T2.3.18.17.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.18.17.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.18.17.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://github.com/TJRadarLab/TJ4DRadSet" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/TJRadarLab/TJ4DRadSet</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.19.18" class="ltx_tr">
<td id="Sx1.T2.3.19.18.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.19.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.19.18.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.19.18.1.1.1.1" class="ltx_text" style="font-size:70%;">K-Radar </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.19.18.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib177" title="" class="ltx_ref">177</a><span id="Sx1.T2.3.19.18.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.19.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.19.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.19.18.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.19.18.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T2.3.19.18.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.19.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.19.18.3.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T2.3.19.18.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.19.18.3.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.19.18.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T2.3.19.18.3.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.19.18.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Object Tracking,</span></span>
<span id="Sx1.T2.3.19.18.3.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.19.18.3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">SLAM</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.19.18.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.19.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.19.18.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.19.18.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.19.18.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.19.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.19.18.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.19.18.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.19.18.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.19.18.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.19.18.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.19.18.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: range-azimuth-Doppler</span></span>
<span id="Sx1.T2.3.19.18.5.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.19.18.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.19.18.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.19.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.19.18.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.19.18.6.1.1.1" class="ltx_text" style="font-size:70%;">5 classes (Pedestrian, Motorbike, Bicycle, Sedan, Bus or Truck)</span></span>
</span>
</td>
<td id="Sx1.T2.3.19.18.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.19.18.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.19.18.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.19.18.7.1.1.1" class="ltx_text" style="font-size:70%;">35k frames of 4D radar tensor</span></span>
</span>
</td>
<td id="Sx1.T2.3.19.18.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.19.18.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.19.18.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://github.com/kaist-avelab/k-radar" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/kaist-avelab/k-radar</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.20.19" class="ltx_tr">
<td id="Sx1.T2.3.20.19.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.20.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.20.19.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.20.19.1.1.1.1" class="ltx_text" style="font-size:70%;">aiMotive </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.20.19.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib178" title="" class="ltx_ref">178</a><span id="Sx1.T2.3.20.19.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.20.19.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.20.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.20.19.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.20.19.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T2.3.20.19.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.20.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.20.19.3.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T2.3.20.19.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T2.3.20.19.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.20.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.20.19.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.20.19.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.20.19.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.20.19.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.20.19.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.20.19.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.20.19.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.20.19.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.20.19.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.20.19.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.20.19.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.20.19.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.20.19.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.20.19.6.1.1.1" class="ltx_text" style="font-size:70%;">14 classes (Pedestrian, Car, Bus, Truck, Van, Motorcycle, Pickup, Rider, Bicycle, Trailer, Train, Shopping Cart, Other Object)</span></span>
</span>
</td>
<td id="Sx1.T2.3.20.19.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.20.19.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.20.19.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.20.19.7.1.1.1" class="ltx_text" style="font-size:70%;">26,583 frames, 425k objects</span></span>
</span>
</td>
<td id="Sx1.T2.3.20.19.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T2.3.20.19.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.20.19.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://github.com/aimotive/aimotive_dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/aimotive/aimotive_dataset</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T2.3.21.20" class="ltx_tr">
<td id="Sx1.T2.3.21.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T2.3.21.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.21.20.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T2.3.21.20.1.1.1.1" class="ltx_text" style="font-size:70%;">WaterScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T2.3.21.20.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib174" title="" class="ltx_ref">174</a><span id="Sx1.T2.3.21.20.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T2.3.21.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T2.3.21.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.21.20.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T2.3.21.20.2.1.1.1" class="ltx_text" style="font-size:70%;">2023</span></span>
</span>
</td>
<td id="Sx1.T2.3.21.20.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T2.3.21.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.21.20.3.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T2.3.21.20.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.21.20.3.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.21.20.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T2.3.21.20.3.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.21.20.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Instance Segmentation,</span></span>
<span id="Sx1.T2.3.21.20.3.1.1.1.3" class="ltx_p"><span id="Sx1.T2.3.21.20.3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Semantic Segmentation,</span></span>
<span id="Sx1.T2.3.21.20.3.1.1.1.4" class="ltx_p"><span id="Sx1.T2.3.21.20.3.1.1.1.4.1" class="ltx_text" style="font-size:70%;">Free-space Segmentation,</span></span>
<span id="Sx1.T2.3.21.20.3.1.1.1.5" class="ltx_p"><span id="Sx1.T2.3.21.20.3.1.1.1.5.1" class="ltx_text" style="font-size:70%;">Waterline Segmentation,</span></span>
<span id="Sx1.T2.3.21.20.3.1.1.1.6" class="ltx_p"><span id="Sx1.T2.3.21.20.3.1.1.1.6.1" class="ltx_text" style="font-size:70%;">Panoptic Perception</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.21.20.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T2.3.21.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.21.20.4.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T2.3.21.20.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T2.3.21.20.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T2.3.21.20.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.21.20.5.1.1" class="ltx_p" style="width:85.4pt;">
<span id="Sx1.T2.3.21.20.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T2.3.21.20.5.1.1.1.1" class="ltx_p"><span id="Sx1.T2.3.21.20.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T2.3.21.20.5.1.1.1.2" class="ltx_p"><span id="Sx1.T2.3.21.20.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T2.3.21.20.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T2.3.21.20.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.21.20.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.21.20.6.1.1.1" class="ltx_text" style="font-size:70%;">7 classes (Pier, Buoy, Sailor, Ship, Boat, Vessel, Kayak)</span></span>
</span>
</td>
<td id="Sx1.T2.3.21.20.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T2.3.21.20.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.21.20.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="Sx1.T2.3.21.20.7.1.1.1" class="ltx_text" style="font-size:70%;">54,120 frames, 202k objects</span></span>
</span>
</td>
<td id="Sx1.T2.3.21.20.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T2.3.21.20.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T2.3.21.20.8.1.1" class="ltx_p" style="width:88.2pt;"><a target="_blank" href="https://waterscenes.github.io" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://waterscenes.github.io</a></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="Sx1.T3" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>SUMMARY OF RADAR-CAMERA FUSION METHODS</figcaption>
<table id="Sx1.T3.3" class="ltx_tabular">
<thead class="ltx_thead">
<tr id="Sx1.T3.3.1.1" class="ltx_tr">
<th id="Sx1.T3.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T3.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.1.1.1.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T3.3.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Reference</span></span>
</span>
</th>
<th id="Sx1.T3.3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T3.3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.1.1.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Year</span></span>
</span>
</th>
<th id="Sx1.T3.3.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T3.3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.1.1.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.1.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Task</span></span>
</span>
</th>
<th id="Sx1.T3.3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T3.3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.1.1.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Annotations</span></span>
</span>
</th>
<th id="Sx1.T3.3.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T3.3.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.1.1.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.1.1.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Categories</span></span>
</span>
</th>
<th id="Sx1.T3.3.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T3.3.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.1.1.6.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.1.1.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Modalities</span></span>
</span>
</th>
<th id="Sx1.T3.3.1.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T3.3.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.1.1.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.1.1.7.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Network Architecture</span></span>
</span>
</th>
<th id="Sx1.T3.3.1.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T3.3.1.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.1.1.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.1.1.8.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Fusion Level</span></span>
</span>
</th>
<th id="Sx1.T3.3.1.1.9" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T3.3.1.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.1.1.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.1.1.9.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Fusion Operation</span></span>
</span>
</th>
<th id="Sx1.T3.3.1.1.10" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T3.3.1.1.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.1.1.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.1.1.10.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Dataset</span></span>
</span>
</th>
<th id="Sx1.T3.3.1.1.11" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="Sx1.T3.3.1.1.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.1.1.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.1.1.11.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Source Code</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx1.T3.3.2.1" class="ltx_tr">
<td id="Sx1.T3.3.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.2.1.1.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T3.3.2.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Chadwick </span><span id="Sx1.T3.3.2.1.1.1.1.2" class="ltx_text ltx_font_italic" style="font-size:70%;">et al</span><span id="Sx1.T3.3.2.1.1.1.1.3" class="ltx_text" style="font-size:70%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.2.1.1.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib186" title="" class="ltx_ref">186</a><span id="Sx1.T3.3.2.1.1.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.2.1.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.2.1.2.1.1.1" class="ltx_text" style="font-size:70%;">2019</span></span>
</span>
</td>
<td id="Sx1.T3.3.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.2.1.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.2.1.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.2.1.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.2.1.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.2.1.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.2.1.5.1.1.1" class="ltx_text" style="font-size:70%;">Vehicle</span></span>
</span>
</td>
<td id="Sx1.T3.3.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.2.1.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.2.1.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.2.1.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.2.1.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.2.1.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.2.1.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.2.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.2.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.2.1.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.2.1.7.1.1.1" class="ltx_text" style="font-size:70%;">One-stage network based on ResNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.2.1.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib99" title="" class="ltx_ref">99</a><span id="Sx1.T3.3.2.1.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.2.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.2.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.2.1.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.2.1.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.2.1.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.2.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.2.1.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.2.1.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.2.1.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.2.1.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Addition;</span></span>
<span id="Sx1.T3.3.2.1.9.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.2.1.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.2.1.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.2.1.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.2.1.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.2.1.10.1.1.1" class="ltx_text" style="font-size:70%;">Self-recorded</span></span>
</span>
</td>
<td id="Sx1.T3.3.2.1.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.2.1.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.2.1.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.2.1.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.3.2" class="ltx_tr">
<td id="Sx1.T3.3.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.3.2.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.3.2.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.3.2.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.3.2.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">RRPN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.3.2.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="Sx1.T3.3.3.2.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.3.2.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.3.2.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.3.2.2.1.1.1" class="ltx_text" style="font-size:70%;">2019</span></span>
</span>
</td>
<td id="Sx1.T3.3.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.3.2.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.3.2.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.3.2.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.3.2.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.3.2.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.3.2.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck, Person, Motorcycle, Bicycle, Bus</span></span>
</span>
</td>
<td id="Sx1.T3.3.3.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.3.2.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.3.2.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.3.2.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.3.2.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.3.2.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.3.2.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.3.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.3.2.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.3.2.7.1.1.1" class="ltx_text" style="font-size:70%;">RRPN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.3.2.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="Sx1.T3.3.3.2.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.3.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.3.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.3.2.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.3.2.8.1.1.1" class="ltx_text" style="font-size:70%;">Data-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.3.2.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.3.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.3.2.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.3.2.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.3.2.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.3.2.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Transformation matrix</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.3.2.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.3.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.3.2.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.3.2.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.3.2.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.3.2.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.3.2.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.3.2.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.3.2.11.1.1" class="ltx_p" style="width:56.9pt;"><a target="_blank" href="https://github.com/mrnabati/RRPN" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/mrnabati/RRPN</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.4.3" class="ltx_tr">
<td id="Sx1.T3.3.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.4.3.1.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T3.3.4.3.1.1.1.1" class="ltx_text" style="font-size:70%;">Jha </span><span id="Sx1.T3.3.4.3.1.1.1.2" class="ltx_text ltx_font_italic" style="font-size:70%;">et al</span><span id="Sx1.T3.3.4.3.1.1.1.3" class="ltx_text" style="font-size:70%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.4.3.1.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib209" title="" class="ltx_ref">209</a><span id="Sx1.T3.3.4.3.1.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.4.3.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.4.3.2.1.1.1" class="ltx_text" style="font-size:70%;">2019</span></span>
</span>
</td>
<td id="Sx1.T3.3.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.4.3.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.4.3.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.4.3.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.4.3.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.4.3.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.4.3.5.1.1.1" class="ltx_text" style="font-size:70%;">Pedestrian, Lamp Post</span></span>
</span>
</td>
<td id="Sx1.T3.3.4.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.4.3.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.4.3.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.4.3.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.4.3.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.4.3.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.4.3.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.4.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.4.3.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.4.3.7.1.1.1" class="ltx_text" style="font-size:70%;">YOLOv3 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.4.3.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib77" title="" class="ltx_ref">77</a><span id="Sx1.T3.3.4.3.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.4.3.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.4.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.4.3.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.4.3.8.1.1.1" class="ltx_text" style="font-size:70%;">Object-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.4.3.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.4.3.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.4.3.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.4.3.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.4.3.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.4.3.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Transformation matrix</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.4.3.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.4.3.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.4.3.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.4.3.10.1.1.1" class="ltx_text" style="font-size:70%;">Self-recorded</span></span>
</span>
</td>
<td id="Sx1.T3.3.4.3.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.4.3.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.4.3.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.4.3.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.5.4" class="ltx_tr">
<td id="Sx1.T3.3.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.5.4.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.5.4.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.5.4.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.5.4.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">CMGGAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.5.4.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib307" title="" class="ltx_ref">307</a><span id="Sx1.T3.3.5.4.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.5.4.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.5.4.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.5.4.2.1.1.1" class="ltx_text" style="font-size:70%;">2019</span></span>
</span>
</td>
<td id="Sx1.T3.3.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.5.4.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.5.4.3.1.1.1" class="ltx_text" style="font-size:70%;">Semantic Segmentation</span></span>
</span>
</td>
<td id="Sx1.T3.3.5.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.5.4.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.5.4.4.1.1.1" class="ltx_text" style="font-size:70%;">2D point-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.5.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.5.4.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.5.4.5.1.1.1" class="ltx_text" style="font-size:70%;">Free Space</span></span>
</span>
</td>
<td id="Sx1.T3.3.5.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.5.4.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.5.4.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.5.4.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.5.4.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.5.4.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.5.4.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: grid map</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.5.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.5.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.5.4.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.5.4.7.1.1.1" class="ltx_text" style="font-size:70%;">CMGGAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.5.4.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib307" title="" class="ltx_ref">307</a><span id="Sx1.T3.3.5.4.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.5.4.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.5.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.5.4.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.5.4.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.5.4.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.5.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.5.4.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.5.4.9.1.1.1" class="ltx_text" style="font-size:70%;">Addition</span></span>
</span>
</td>
<td id="Sx1.T3.3.5.4.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.5.4.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.5.4.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.5.4.10.1.1.1" class="ltx_text" style="font-size:70%;">Self-recorded</span></span>
</span>
</td>
<td id="Sx1.T3.3.5.4.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.5.4.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.5.4.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.5.4.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.6.5" class="ltx_tr">
<td id="Sx1.T3.3.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.6.5.1.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T3.3.6.5.1.1.1.1" class="ltx_text" style="font-size:70%;">Meyer and Kuschk </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.6.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib317" title="" class="ltx_ref">317</a><span id="Sx1.T3.3.6.5.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.6.5.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.6.5.2.1.1.1" class="ltx_text" style="font-size:70%;">2019</span></span>
</span>
</td>
<td id="Sx1.T3.3.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.6.5.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.6.5.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.6.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.6.5.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.6.5.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.6.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.6.5.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.6.5.5.1.1.1" class="ltx_text" style="font-size:70%;">Car</span></span>
</span>
</td>
<td id="Sx1.T3.3.6.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.6.5.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.6.5.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.6.5.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.6.5.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.6.5.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.6.5.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.6.5.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.6.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.6.5.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.6.5.7.1.1.1" class="ltx_text" style="font-size:70%;">A 3D region proposal network based on VGG </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.6.5.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib212" title="" class="ltx_ref">212</a><span id="Sx1.T3.3.6.5.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.6.5.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.6.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.6.5.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.6.5.8.1.1.1" class="ltx_text" style="font-size:70%;">Data-Level</span></span>
</span>
</td>
<td id="Sx1.T3.3.6.5.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.6.5.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.6.5.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.6.5.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.6.5.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.6.5.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Transformation matrix</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.6.5.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.6.5.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.6.5.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.6.5.10.1.1.1" class="ltx_text" style="font-size:70%;">Astyx </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.6.5.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib317" title="" class="ltx_ref">317</a><span id="Sx1.T3.3.6.5.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.6.5.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.6.5.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.6.5.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.6.5.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.7.6" class="ltx_tr">
<td id="Sx1.T3.3.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.7.6.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.7.6.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.7.6.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.7.6.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">RVNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.7.6.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="Sx1.T3.3.7.6.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.7.6.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.7.6.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.7.6.2.1.1.1" class="ltx_text" style="font-size:70%;">2019</span></span>
</span>
</td>
<td id="Sx1.T3.3.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.7.6.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.7.6.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.7.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.7.6.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.7.6.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.7.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.7.6.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.7.6.5.1.1.1" class="ltx_text" style="font-size:70%;">Vehicle, Pedestrian, Two-wheelers, Objects (movable objects and debris)</span></span>
</span>
</td>
<td id="Sx1.T3.3.7.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.7.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.7.6.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.7.6.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.7.6.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.7.6.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.7.6.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.7.6.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.7.6.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.7.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.7.6.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.7.6.7.1.1.1" class="ltx_text" style="font-size:70%;">RVNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.7.6.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="Sx1.T3.3.7.6.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.7.6.7.1.1.4" class="ltx_text" style="font-size:70%;"> based on YOLOv3 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.7.6.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib77" title="" class="ltx_ref">77</a><span id="Sx1.T3.3.7.6.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.7.6.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.7.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.7.6.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.7.6.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.7.6.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.7.6.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.7.6.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.7.6.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.7.6.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.7.6.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.7.6.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.7.6.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.7.6.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.7.6.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.7.6.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.7.6.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.7.6.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.7.6.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.8.7" class="ltx_tr">
<td id="Sx1.T3.3.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.8.7.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.8.7.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.8.7.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.8.7.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">FusionNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.8.7.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib206" title="" class="ltx_ref">206</a><span id="Sx1.T3.3.8.7.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.8.7.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.8.7.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.8.7.2.1.1.1" class="ltx_text" style="font-size:70%;">2019</span></span>
</span>
</td>
<td id="Sx1.T3.3.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.8.7.3.1.1" class="ltx_p" style="width:68.3pt;">
<span id="Sx1.T3.3.8.7.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.8.7.3.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.8.7.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T3.3.8.7.3.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.8.7.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Object Classification</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.8.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.8.7.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.8.7.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.8.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.8.7.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.8.7.5.1.1.1" class="ltx_text" style="font-size:70%;">Vehicle</span></span>
</span>
</td>
<td id="Sx1.T3.3.8.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.8.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.8.7.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.8.7.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.8.7.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.8.7.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.8.7.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.8.7.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: range-azimuth</span></span>
<span id="Sx1.T3.3.8.7.6.1.1.1.3" class="ltx_p"><span id="Sx1.T3.3.8.7.6.1.1.1.3.1" class="ltx_text" style="font-size:70%;">tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.8.7.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.8.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.8.7.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.8.7.7.1.1.1" class="ltx_text" style="font-size:70%;">FusionNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.8.7.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib206" title="" class="ltx_ref">206</a><span id="Sx1.T3.3.8.7.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.8.7.7.1.1.4" class="ltx_text" style="font-size:70%;"> inspired by SSD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.8.7.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib83" title="" class="ltx_ref">83</a><span id="Sx1.T3.3.8.7.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.8.7.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.8.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.8.7.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.8.7.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.8.7.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.8.7.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.8.7.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.8.7.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.8.7.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.8.7.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.8.7.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.8.7.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.8.7.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.8.7.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.8.7.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.8.7.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.8.7.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.8.7.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.9.8" class="ltx_tr">
<td id="Sx1.T3.3.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.9.8.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.9.8.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.9.8.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.9.8.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">SO-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.9.8.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib199" title="" class="ltx_ref">199</a><span id="Sx1.T3.3.9.8.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.9.8.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.9.8.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.9.8.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T3.3.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.9.8.3.1.1" class="ltx_p" style="width:68.3pt;">
<span id="Sx1.T3.3.9.8.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.9.8.3.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.9.8.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T3.3.9.8.3.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.9.8.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Semantic Segmentation</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.9.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.9.8.4.1.1" class="ltx_p" style="width:42.7pt;">
<span id="Sx1.T3.3.9.8.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.9.8.4.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.9.8.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level,</span></span>
<span id="Sx1.T3.3.9.8.4.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.9.8.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">2D pixel-level</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.9.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.9.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.9.8.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.9.8.5.1.1.1" class="ltx_text" style="font-size:70%;">Vehicle, Free Space</span></span>
</span>
</td>
<td id="Sx1.T3.3.9.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.9.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.9.8.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.9.8.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.9.8.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.9.8.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.9.8.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.9.8.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.9.8.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.9.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.9.8.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.9.8.7.1.1.1" class="ltx_text" style="font-size:70%;">SO-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.9.8.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib199" title="" class="ltx_ref">199</a><span id="Sx1.T3.3.9.8.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.9.8.7.1.1.4" class="ltx_text" style="font-size:70%;"> based on the RVNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.9.8.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="Sx1.T3.3.9.8.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.9.8.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.9.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.9.8.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.9.8.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.9.8.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.9.8.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.9.8.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.9.8.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.9.8.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.9.8.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.9.8.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.9.8.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.9.8.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.9.8.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.9.8.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.9.8.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.9.8.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.9.8.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.10.9" class="ltx_tr">
<td id="Sx1.T3.3.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.10.9.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.10.9.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.10.9.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.10.9.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">SAF-FCOS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.10.9.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib214" title="" class="ltx_ref">214</a><span id="Sx1.T3.3.10.9.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.10.9.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.10.9.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.10.9.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T3.3.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.10.9.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.10.9.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.10.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.10.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.10.9.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.10.9.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.10.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.10.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.10.9.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.10.9.5.1.1.1" class="ltx_text" style="font-size:70%;">Bicycle, Car, Motorcycle, Bus, Train, Truck</span></span>
</span>
</td>
<td id="Sx1.T3.3.10.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.10.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.10.9.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.10.9.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.10.9.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.10.9.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.10.9.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.10.9.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.10.9.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.10.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.10.9.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.10.9.7.1.1.1" class="ltx_text" style="font-size:70%;">SAF-FCOS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.10.9.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib214" title="" class="ltx_ref">214</a><span id="Sx1.T3.3.10.9.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.10.9.7.1.1.4" class="ltx_text" style="font-size:70%;"> based on FCOS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.10.9.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib318" title="" class="ltx_ref">318</a><span id="Sx1.T3.3.10.9.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.10.9.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.10.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.10.9.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.10.9.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.10.9.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.10.9.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.10.9.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.10.9.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.10.9.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.10.9.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Addition;</span></span>
<span id="Sx1.T3.3.10.9.9.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.10.9.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Multiplication</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.10.9.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.10.9.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.10.9.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.10.9.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.10.9.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.10.9.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.10.9.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.10.9.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.10.9.11.1.1" class="ltx_p" style="width:56.9pt;"><a target="_blank" href="https://github.com/Singingkettle/SAF-FCOS" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/Singingkettle/SAF-FCOS</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.11.10" class="ltx_tr">
<td id="Sx1.T3.3.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.11.10.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.11.10.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.11.10.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.11.10.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">CRF-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.11.10.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib211" title="" class="ltx_ref">211</a><span id="Sx1.T3.3.11.10.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.11.10.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.11.10.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.11.10.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T3.3.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.11.10.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.11.10.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.11.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.11.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.11.10.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.11.10.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.11.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.11.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.11.10.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.11.10.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Bus, Motorcycle, Truck, Trailer, Bicycle, Human</span></span>
</span>
</td>
<td id="Sx1.T3.3.11.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.11.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.11.10.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.11.10.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.11.10.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.11.10.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.11.10.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.11.10.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.11.10.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.11.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.11.10.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.11.10.7.1.1.1" class="ltx_text" style="font-size:70%;">CRF-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.11.10.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib211" title="" class="ltx_ref">211</a><span id="Sx1.T3.3.11.10.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.11.10.7.1.1.4" class="ltx_text" style="font-size:70%;"> based on RetinaNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.11.10.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib84" title="" class="ltx_ref">84</a><span id="Sx1.T3.3.11.10.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.11.10.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.11.10.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.11.10.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.11.10.8.1.1.1" class="ltx_text" style="font-size:70%;">Data-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.11.10.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.11.10.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.11.10.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.11.10.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.11.10.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.11.10.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.11.10.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.11.10.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.11.10.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.11.10.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.11.10.10.1.1.4" class="ltx_text" style="font-size:70%;">, Self-recorded</span></span>
</span>
</td>
<td id="Sx1.T3.3.11.10.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.11.10.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.11.10.11.1.1" class="ltx_p" style="width:56.9pt;"><a target="_blank" href="https://github.com/TUMFTM/CameraRadarFusionNet" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/TUMFTM/CameraRadarFusionNet</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.12.11" class="ltx_tr">
<td id="Sx1.T3.3.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.12.11.1.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T3.3.12.11.1.1.1.1" class="ltx_text" style="font-size:70%;">Bijelic </span><span id="Sx1.T3.3.12.11.1.1.1.2" class="ltx_text ltx_font_italic" style="font-size:70%;">et al</span><span id="Sx1.T3.3.12.11.1.1.1.3" class="ltx_text" style="font-size:70%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.12.11.1.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="Sx1.T3.3.12.11.1.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.12.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.12.11.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.12.11.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T3.3.12.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.12.11.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.12.11.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.12.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.12.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.12.11.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.12.11.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.12.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.12.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.12.11.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.12.11.5.1.1.1" class="ltx_text" style="font-size:70%;">Vehicle</span></span>
</span>
</td>
<td id="Sx1.T3.3.12.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.12.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.12.11.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.12.11.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.12.11.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.12.11.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.12.11.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.12.11.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.12.11.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.12.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.12.11.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.12.11.7.1.1.1" class="ltx_text" style="font-size:70%;">A modiﬁed VGG </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.12.11.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib212" title="" class="ltx_ref">212</a><span id="Sx1.T3.3.12.11.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.12.11.7.1.1.4" class="ltx_text" style="font-size:70%;"> backbone and SSD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.12.11.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib83" title="" class="ltx_ref">83</a><span id="Sx1.T3.3.12.11.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.12.11.7.1.1.7" class="ltx_text" style="font-size:70%;"> blocks</span></span>
</span>
</td>
<td id="Sx1.T3.3.12.11.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.12.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.12.11.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.12.11.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.12.11.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.12.11.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.12.11.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.12.11.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.12.11.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.12.11.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation;</span></span>
<span id="Sx1.T3.3.12.11.9.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.12.11.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Attention</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.12.11.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.12.11.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.12.11.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.12.11.10.1.1.1" class="ltx_text" style="font-size:70%;">DENSE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.12.11.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="Sx1.T3.3.12.11.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.12.11.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.12.11.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.12.11.11.1.1" class="ltx_p" style="width:56.9pt;"><a target="_blank" href="https://github.com/princeton-computational-imaging/SeeingThroughFog" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/princeton-computational-imaging/SeeingThroughFog</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.13.12" class="ltx_tr">
<td id="Sx1.T3.3.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.13.12.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.13.12.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.13.12.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.13.12.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">BIRANet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.13.12.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib190" title="" class="ltx_ref">190</a><span id="Sx1.T3.3.13.12.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.13.12.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.13.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.13.12.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.13.12.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T3.3.13.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.13.12.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.13.12.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.13.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.13.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.13.12.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.13.12.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.13.12.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.13.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.13.12.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.13.12.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck, Person, Motorcycle, Bicycle, Bus</span></span>
</span>
</td>
<td id="Sx1.T3.3.13.12.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.13.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.13.12.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.13.12.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.13.12.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.13.12.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.13.12.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.13.12.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.13.12.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.13.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.13.12.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.13.12.7.1.1.1" class="ltx_text" style="font-size:70%;">RANet and BIRANet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.13.12.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib190" title="" class="ltx_ref">190</a><span id="Sx1.T3.3.13.12.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.13.12.7.1.1.4" class="ltx_text" style="font-size:70%;"> based on ResNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.13.12.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib99" title="" class="ltx_ref">99</a><span id="Sx1.T3.3.13.12.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.13.12.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.13.12.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.13.12.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.13.12.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.13.12.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.13.12.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.13.12.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.13.12.9.1.1.1" class="ltx_text" style="font-size:70%;">Addition</span></span>
</span>
</td>
<td id="Sx1.T3.3.13.12.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.13.12.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.13.12.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.13.12.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.13.12.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.13.12.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.13.12.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.13.12.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.13.12.11.1.1" class="ltx_p" style="width:56.9pt;"><a target="_blank" href="https://github.com/RituYadav92/Radar-RGB-Attentive-Multimodal-Object-Detection" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/RituYadav92/Radar-RGB-Attentive-Multimodal-Object-Detection</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.14.13" class="ltx_tr">
<td id="Sx1.T3.3.14.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.14.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.14.13.1.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T3.3.14.13.1.1.1.1" class="ltx_text" style="font-size:70%;">Nabati and Qi </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.14.13.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib194" title="" class="ltx_ref">194</a><span id="Sx1.T3.3.14.13.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.14.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.14.13.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.14.13.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T3.3.14.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.14.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.14.13.3.1.1" class="ltx_p" style="width:68.3pt;">
<span id="Sx1.T3.3.14.13.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.14.13.3.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.14.13.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T3.3.14.13.3.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.14.13.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Depth Estimation</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.14.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.14.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.14.13.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.14.13.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.14.13.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.14.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.14.13.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.14.13.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck, Person, Motorcycle, Bicycle, Bus</span></span>
</span>
</td>
<td id="Sx1.T3.3.14.13.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.14.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.14.13.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.14.13.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.14.13.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.14.13.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.14.13.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.14.13.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.14.13.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.14.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.14.13.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.14.13.7.1.1.1" class="ltx_text" style="font-size:70%;">FPN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.14.13.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib74" title="" class="ltx_ref">74</a><span id="Sx1.T3.3.14.13.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.14.13.7.1.1.4" class="ltx_text" style="font-size:70%;"> with ResNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.14.13.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib99" title="" class="ltx_ref">99</a><span id="Sx1.T3.3.14.13.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.14.13.7.1.1.7" class="ltx_text" style="font-size:70%;"> as backbone, and RPN in Faster R-CNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.14.13.7.1.1.8.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib73" title="" class="ltx_ref">73</a><span id="Sx1.T3.3.14.13.7.1.1.9.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.14.13.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.14.13.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.14.13.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.14.13.8.1.1.1" class="ltx_text" style="font-size:70%;">Hybrid-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.14.13.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.14.13.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.14.13.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.14.13.9.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T3.3.14.13.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.14.13.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.14.13.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.14.13.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.14.13.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.14.13.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.14.13.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.14.13.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.14.13.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.14.13.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.15.14" class="ltx_tr">
<td id="Sx1.T3.3.15.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.15.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.15.14.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.15.14.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.15.14.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.15.14.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">YOdar </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.15.14.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib197" title="" class="ltx_ref">197</a><span id="Sx1.T3.3.15.14.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.15.14.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.15.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.15.14.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.15.14.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T3.3.15.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.15.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.15.14.3.1.1" class="ltx_p" style="width:68.3pt;">
<span id="Sx1.T3.3.15.14.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.15.14.3.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.15.14.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.15.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.15.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.15.14.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.15.14.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.15.14.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.15.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.15.14.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.15.14.5.1.1.1" class="ltx_text" style="font-size:70%;">Vehicle</span></span>
</span>
</td>
<td id="Sx1.T3.3.15.14.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.15.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.15.14.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.15.14.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.15.14.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.15.14.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.15.14.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.15.14.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.15.14.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.15.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.15.14.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.15.14.7.1.1.1" class="ltx_text" style="font-size:70%;">YOdar </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.15.14.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib197" title="" class="ltx_ref">197</a><span id="Sx1.T3.3.15.14.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.15.14.7.1.1.4" class="ltx_text" style="font-size:70%;"> based on YOLOv3 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.15.14.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib77" title="" class="ltx_ref">77</a><span id="Sx1.T3.3.15.14.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.15.14.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.15.14.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.15.14.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.15.14.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.15.14.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.15.14.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.15.14.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.15.14.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.15.14.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.15.14.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.15.14.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.15.14.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.15.14.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.15.14.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.15.14.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.15.14.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.15.14.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.15.14.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.16.15" class="ltx_tr">
<td id="Sx1.T3.3.16.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.16.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.16.15.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.16.15.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.16.15.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.16.15.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">CenterFusion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.16.15.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="Sx1.T3.3.16.15.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.16.15.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.16.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.16.15.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.16.15.2.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="Sx1.T3.3.16.15.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.16.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.16.15.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.16.15.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.16.15.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.16.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.16.15.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.16.15.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.16.15.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.16.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.16.15.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.16.15.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck, Bus, Trailer, Pedestrian, Barrier, Motorcycle, Bicycle, Traffic Cone</span></span>
</span>
</td>
<td id="Sx1.T3.3.16.15.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.16.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.16.15.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.16.15.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.16.15.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.16.15.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.16.15.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.16.15.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.16.15.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.16.15.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.16.15.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.16.15.7.1.1.1" class="ltx_text" style="font-size:70%;">CenterNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.16.15.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib189" title="" class="ltx_ref">189</a><span id="Sx1.T3.3.16.15.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.16.15.7.1.1.4" class="ltx_text" style="font-size:70%;"> with the DLA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.16.15.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib319" title="" class="ltx_ref">319</a><span id="Sx1.T3.3.16.15.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.16.15.7.1.1.7" class="ltx_text" style="font-size:70%;"> backbone</span></span>
</span>
</td>
<td id="Sx1.T3.3.16.15.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.16.15.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.16.15.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.16.15.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.16.15.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.16.15.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.16.15.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.16.15.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.16.15.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.16.15.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.16.15.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.16.15.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.16.15.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.16.15.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.16.15.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.16.15.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.16.15.11.1.1" class="ltx_p" style="width:56.9pt;"><a target="_blank" href="https://github.com/mrnabati/CenterFusion" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/mrnabati/CenterFusion</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.17.16" class="ltx_tr">
<td id="Sx1.T3.3.17.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.17.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.17.16.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.17.16.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.17.16.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.17.16.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">RODNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.17.16.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="Sx1.T3.3.17.16.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.17.16.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.17.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.17.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.17.16.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.17.16.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T3.3.17.16.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.17.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.17.16.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.17.16.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.17.16.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.17.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.17.16.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.17.16.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.17.16.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.17.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.17.16.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.17.16.5.1.1.1" class="ltx_text" style="font-size:70%;">Pedestrian, Cyclist, Car</span></span>
</span>
</td>
<td id="Sx1.T3.3.17.16.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.17.16.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.17.16.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.17.16.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.17.16.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.17.16.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.17.16.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.17.16.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: range-azimuth</span></span>
<span id="Sx1.T3.3.17.16.6.1.1.1.3" class="ltx_p"><span id="Sx1.T3.3.17.16.6.1.1.1.3.1" class="ltx_text" style="font-size:70%;">tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.17.16.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.17.16.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.17.16.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.17.16.7.1.1.1" class="ltx_text" style="font-size:70%;">RODNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.17.16.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="Sx1.T3.3.17.16.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.17.16.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.17.16.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.17.16.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.17.16.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.17.16.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.17.16.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.17.16.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.17.16.9.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T3.3.17.16.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.17.16.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.17.16.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.17.16.10.1.1.1" class="ltx_text" style="font-size:70%;">CRUW </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.17.16.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib170" title="" class="ltx_ref">170</a><span id="Sx1.T3.3.17.16.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.17.16.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.17.16.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.17.16.11.1.1" class="ltx_p" style="width:56.9pt;"><a target="_blank" href="https://github.com/yizhou-wang/RODNet" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/yizhou-wang/RODNet</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.18.17" class="ltx_tr">
<td id="Sx1.T3.3.18.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.18.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.18.17.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.18.17.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.18.17.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.18.17.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">RAMP-CNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.18.17.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib104" title="" class="ltx_ref">104</a><span id="Sx1.T3.3.18.17.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.18.17.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.18.17.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.18.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.18.17.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.18.17.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T3.3.18.17.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.18.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.18.17.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.18.17.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.18.17.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.18.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.18.17.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.18.17.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.18.17.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.18.17.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.18.17.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.18.17.5.1.1.1" class="ltx_text" style="font-size:70%;">Pedestrian, Cyclist, Car</span></span>
</span>
</td>
<td id="Sx1.T3.3.18.17.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.18.17.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.18.17.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.18.17.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.18.17.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.18.17.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.18.17.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.18.17.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar:</span></span>
<span id="Sx1.T3.3.18.17.6.1.1.1.3" class="ltx_p"><span id="Sx1.T3.3.18.17.6.1.1.1.3.1" class="ltx_text" style="font-size:70%;">range-azimuth-Doppler</span></span>
<span id="Sx1.T3.3.18.17.6.1.1.1.4" class="ltx_p"><span id="Sx1.T3.3.18.17.6.1.1.1.4.1" class="ltx_text" style="font-size:70%;">tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.18.17.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.18.17.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.18.17.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.18.17.7.1.1.1" class="ltx_text" style="font-size:70%;">RAMP-CNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.18.17.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib104" title="" class="ltx_ref">104</a><span id="Sx1.T3.3.18.17.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.18.17.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.18.17.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.18.17.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.18.17.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.18.17.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.18.17.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.18.17.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.18.17.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.18.17.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.18.17.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.18.17.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.18.17.10.1.1.1" class="ltx_text" style="font-size:70%;">CRUW </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.18.17.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib170" title="" class="ltx_ref">170</a><span id="Sx1.T3.3.18.17.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.18.17.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.18.17.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.18.17.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.18.17.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.19.18" class="ltx_tr">
<td id="Sx1.T3.3.19.18.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.19.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.19.18.1.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T3.3.19.18.1.1.1.1" class="ltx_text" style="font-size:70%;">Li and Xie </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.19.18.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib196" title="" class="ltx_ref">196</a><span id="Sx1.T3.3.19.18.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.19.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.19.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.19.18.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.19.18.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T3.3.19.18.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.19.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.19.18.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.19.18.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.19.18.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.19.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.19.18.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.19.18.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.19.18.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.19.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.19.18.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.19.18.5.1.1.1" class="ltx_text" style="font-size:70%;">Vehicle</span></span>
</span>
</td>
<td id="Sx1.T3.3.19.18.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.19.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.19.18.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.19.18.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.19.18.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.19.18.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.19.18.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.19.18.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.19.18.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.19.18.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.19.18.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.19.18.7.1.1.1" class="ltx_text" style="font-size:70%;">A network based on YOLOv3 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.19.18.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib77" title="" class="ltx_ref">77</a><span id="Sx1.T3.3.19.18.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.19.18.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.19.18.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.19.18.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.19.18.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.19.18.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.19.18.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.19.18.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.19.18.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.19.18.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.19.18.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation;</span></span>
<span id="Sx1.T3.3.19.18.9.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.19.18.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Multiplication</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.19.18.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.19.18.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.19.18.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.19.18.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.19.18.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.19.18.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.19.18.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.19.18.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.19.18.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.19.18.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.20.19" class="ltx_tr">
<td id="Sx1.T3.3.20.19.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.20.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.20.19.1.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T3.3.20.19.1.1.1.1" class="ltx_text" style="font-size:70%;">Kim </span><span id="Sx1.T3.3.20.19.1.1.1.2" class="ltx_text ltx_font_italic" style="font-size:70%;">et al</span><span id="Sx1.T3.3.20.19.1.1.1.3" class="ltx_text" style="font-size:70%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.20.19.1.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib210" title="" class="ltx_ref">210</a><span id="Sx1.T3.3.20.19.1.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.20.19.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.20.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.20.19.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.20.19.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T3.3.20.19.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.20.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.20.19.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.20.19.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.20.19.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.20.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.20.19.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.20.19.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.20.19.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.20.19.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.20.19.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.20.19.5.1.1.1" class="ltx_text" style="font-size:70%;">Vehicle</span></span>
</span>
</td>
<td id="Sx1.T3.3.20.19.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.20.19.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.20.19.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.20.19.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.20.19.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.20.19.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.20.19.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.20.19.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: range-azimuth</span></span>
<span id="Sx1.T3.3.20.19.6.1.1.1.3" class="ltx_p"><span id="Sx1.T3.3.20.19.6.1.1.1.3.1" class="ltx_text" style="font-size:70%;">tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.20.19.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.20.19.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.20.19.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.20.19.7.1.1.1" class="ltx_text" style="font-size:70%;">A network based on VGG </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.20.19.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib212" title="" class="ltx_ref">212</a><span id="Sx1.T3.3.20.19.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.20.19.7.1.1.4" class="ltx_text" style="font-size:70%;"> and FPN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.20.19.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib74" title="" class="ltx_ref">74</a><span id="Sx1.T3.3.20.19.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.20.19.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.20.19.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.20.19.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.20.19.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.20.19.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.20.19.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.20.19.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.20.19.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.20.19.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.20.19.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.20.19.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.20.19.10.1.1.1" class="ltx_text" style="font-size:70%;">Self-recorded</span></span>
</span>
</td>
<td id="Sx1.T3.3.20.19.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.20.19.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.20.19.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.20.19.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.21.20" class="ltx_tr">
<td id="Sx1.T3.3.21.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.21.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.21.20.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.21.20.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.21.20.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.21.20.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">AssociationNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.21.20.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib200" title="" class="ltx_ref">200</a><span id="Sx1.T3.3.21.20.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.21.20.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.21.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.21.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.21.20.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.21.20.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T3.3.21.20.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.21.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.21.20.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.21.20.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.21.20.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.21.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.21.20.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.21.20.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.21.20.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.21.20.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.21.20.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.21.20.5.1.1.1" class="ltx_text" style="font-size:70%;">Vehicle</span></span>
</span>
</td>
<td id="Sx1.T3.3.21.20.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.21.20.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.21.20.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.21.20.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.21.20.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.21.20.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.21.20.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.21.20.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.21.20.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.21.20.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.21.20.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.21.20.7.1.1.1" class="ltx_text" style="font-size:70%;">AssociationNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.21.20.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib200" title="" class="ltx_ref">200</a><span id="Sx1.T3.3.21.20.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.21.20.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.21.20.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.21.20.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.21.20.8.1.1.1" class="ltx_text" style="font-size:70%;">Object-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.21.20.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.21.20.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.21.20.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.21.20.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.21.20.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.21.20.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Transformation matrix;</span></span>
<span id="Sx1.T3.3.21.20.9.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.21.20.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.21.20.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.21.20.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.21.20.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.21.20.10.1.1.1" class="ltx_text" style="font-size:70%;">Self-recorded</span></span>
</span>
</td>
<td id="Sx1.T3.3.21.20.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.21.20.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.21.20.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.21.20.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.22.21" class="ltx_tr">
<td id="Sx1.T3.3.22.21.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.22.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.22.21.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.22.21.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.22.21.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.22.21.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">RVF-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.22.21.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib243" title="" class="ltx_ref">243</a><span id="Sx1.T3.3.22.21.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.22.21.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.22.21.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.22.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.22.21.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.22.21.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T3.3.22.21.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.22.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.22.21.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.22.21.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.22.21.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.22.21.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.22.21.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.22.21.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.22.21.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.22.21.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.22.21.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.22.21.5.1.1.1" class="ltx_text" style="font-size:70%;">Car</span></span>
</span>
</td>
<td id="Sx1.T3.3.22.21.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.22.21.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.22.21.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.22.21.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.22.21.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.22.21.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.22.21.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.22.21.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.22.21.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.22.21.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.22.21.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.22.21.7.1.1.1" class="ltx_text" style="font-size:70%;">RVF-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.22.21.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib243" title="" class="ltx_ref">243</a><span id="Sx1.T3.3.22.21.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.22.21.7.1.1.4" class="ltx_text" style="font-size:70%;"> based on VoxelNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.22.21.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib117" title="" class="ltx_ref">117</a><span id="Sx1.T3.3.22.21.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.22.21.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.22.21.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.22.21.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.22.21.8.1.1.1" class="ltx_text" style="font-size:70%;">Data-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.22.21.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.22.21.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.22.21.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.22.21.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.22.21.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.22.21.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.22.21.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.22.21.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.22.21.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.22.21.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.22.21.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.22.21.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.22.21.11.1.1" class="ltx_p" style="width:56.9pt;"><a target="_blank" href="https://github.com/TUMFTM/RadarVoxelFusionNet" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/TUMFTM/RadarVoxelFusionNet</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.23.22" class="ltx_tr">
<td id="Sx1.T3.3.23.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.23.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.23.22.1.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T3.3.23.22.1.1.1.1" class="ltx_text" style="font-size:70%;">Cui </span><span id="Sx1.T3.3.23.22.1.1.1.2" class="ltx_text ltx_font_italic" style="font-size:70%;">et al</span><span id="Sx1.T3.3.23.22.1.1.1.3" class="ltx_text" style="font-size:70%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.23.22.1.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib195" title="" class="ltx_ref">195</a><span id="Sx1.T3.3.23.22.1.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.23.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.23.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.23.22.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.23.22.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T3.3.23.22.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.23.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.23.22.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.23.22.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.23.22.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.23.22.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.23.22.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.23.22.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.23.22.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.23.22.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.23.22.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.23.22.5.1.1.1" class="ltx_text" style="font-size:70%;">Car</span></span>
</span>
</td>
<td id="Sx1.T3.3.23.22.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.23.22.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.23.22.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.23.22.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.23.22.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.23.22.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.23.22.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.23.22.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.23.22.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.23.22.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.23.22.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.23.22.7.1.1.1" class="ltx_text" style="font-size:70%;">CNN with SSMA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.23.22.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib217" title="" class="ltx_ref">217</a><span id="Sx1.T3.3.23.22.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.23.22.7.1.1.4" class="ltx_text" style="font-size:70%;"> block</span></span>
</span>
</td>
<td id="Sx1.T3.3.23.22.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.23.22.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.23.22.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.23.22.8.1.1.1" class="ltx_text" style="font-size:70%;">Hybrid-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.23.22.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.23.22.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.23.22.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.23.22.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.23.22.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.23.22.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.23.22.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.23.22.10.1.1.1" class="ltx_text" style="font-size:70%;">Astyx </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.23.22.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib317" title="" class="ltx_ref">317</a><span id="Sx1.T3.3.23.22.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.23.22.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.23.22.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.23.22.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.23.22.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.24.23" class="ltx_tr">
<td id="Sx1.T3.3.24.23.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.24.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.24.23.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.24.23.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.24.23.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.24.23.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">RISFNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.24.23.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib198" title="" class="ltx_ref">198</a><span id="Sx1.T3.3.24.23.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.24.23.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.24.23.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.24.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.24.23.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.24.23.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T3.3.24.23.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.24.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.24.23.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.24.23.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.24.23.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.24.23.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.24.23.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.24.23.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.24.23.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.24.23.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.24.23.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.24.23.5.1.1.1" class="ltx_text" style="font-size:70%;">Bottle</span></span>
</span>
</td>
<td id="Sx1.T3.3.24.23.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.24.23.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.24.23.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.24.23.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.24.23.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.24.23.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.24.23.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.24.23.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.24.23.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.24.23.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.24.23.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.24.23.7.1.1.1" class="ltx_text" style="font-size:70%;">RISFNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.24.23.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib198" title="" class="ltx_ref">198</a><span id="Sx1.T3.3.24.23.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.24.23.7.1.1.4" class="ltx_text" style="font-size:70%;"> based on CSPdarknet53 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.24.23.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib78" title="" class="ltx_ref">78</a><span id="Sx1.T3.3.24.23.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.24.23.7.1.1.7" class="ltx_text" style="font-size:70%;"> and VGG </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.24.23.7.1.1.8.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib212" title="" class="ltx_ref">212</a><span id="Sx1.T3.3.24.23.7.1.1.9.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.24.23.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.24.23.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.24.23.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.24.23.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.24.23.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.24.23.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.24.23.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.24.23.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.24.23.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.24.23.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation;</span></span>
<span id="Sx1.T3.3.24.23.9.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.24.23.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Addition;</span></span>
<span id="Sx1.T3.3.24.23.9.1.1.1.3" class="ltx_p"><span id="Sx1.T3.3.24.23.9.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Multiplication</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.24.23.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.24.23.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.24.23.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.24.23.10.1.1.1" class="ltx_text" style="font-size:70%;">FloW </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.24.23.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib172" title="" class="ltx_ref">172</a><span id="Sx1.T3.3.24.23.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.24.23.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.24.23.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.24.23.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.24.23.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.25.24" class="ltx_tr">
<td id="Sx1.T3.3.25.24.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.25.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.25.24.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.25.24.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.25.24.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.25.24.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">GRIF Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.25.24.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib245" title="" class="ltx_ref">245</a><span id="Sx1.T3.3.25.24.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.25.24.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.25.24.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.25.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.25.24.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.25.24.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="Sx1.T3.3.25.24.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.25.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.25.24.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.25.24.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.25.24.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.25.24.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.25.24.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.25.24.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.25.24.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.25.24.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.25.24.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.25.24.5.1.1.1" class="ltx_text" style="font-size:70%;">Vehicle</span></span>
</span>
</td>
<td id="Sx1.T3.3.25.24.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.25.24.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.25.24.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.25.24.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.25.24.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.25.24.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.25.24.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.25.24.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.25.24.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.25.24.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.25.24.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.25.24.7.1.1.1" class="ltx_text" style="font-size:70%;">GRIF Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.25.24.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib245" title="" class="ltx_ref">245</a><span id="Sx1.T3.3.25.24.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.25.24.7.1.1.4" class="ltx_text" style="font-size:70%;"> based on FPN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.25.24.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib74" title="" class="ltx_ref">74</a><span id="Sx1.T3.3.25.24.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.25.24.7.1.1.7" class="ltx_text" style="font-size:70%;"> and SBNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.25.24.7.1.1.8.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib246" title="" class="ltx_ref">246</a><span id="Sx1.T3.3.25.24.7.1.1.9.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.25.24.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.25.24.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.25.24.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.25.24.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature level</span></span>
</span>
</td>
<td id="Sx1.T3.3.25.24.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.25.24.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.25.24.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.25.24.9.1.1.1" class="ltx_text" style="font-size:70%;">Attention</span></span>
</span>
</td>
<td id="Sx1.T3.3.25.24.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.25.24.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.25.24.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.25.24.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.25.24.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.25.24.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.25.24.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.25.24.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.25.24.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.25.24.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.26.25" class="ltx_tr">
<td id="Sx1.T3.3.26.25.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.26.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.26.25.1.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T3.3.26.25.1.1.1.1" class="ltx_text" style="font-size:70%;">Stäcker  </span><span id="Sx1.T3.3.26.25.1.1.1.2" class="ltx_text ltx_font_italic" style="font-size:70%;">et al</span><span id="Sx1.T3.3.26.25.1.1.1.3" class="ltx_text" style="font-size:70%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.26.25.1.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib255" title="" class="ltx_ref">255</a><span id="Sx1.T3.3.26.25.1.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.26.25.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.26.25.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.26.25.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.26.25.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T3.3.26.25.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.26.25.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.26.25.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.26.25.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.26.25.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.26.25.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.26.25.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.26.25.4.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.26.25.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.26.25.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.26.25.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.26.25.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Person, Truck, Bicycle, Motorcycle</span></span>
</span>
</td>
<td id="Sx1.T3.3.26.25.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.26.25.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.26.25.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.26.25.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.26.25.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.26.25.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.26.25.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.26.25.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.26.25.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.26.25.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.26.25.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.26.25.7.1.1.1" class="ltx_text" style="font-size:70%;">A network based on RetinaNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.26.25.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib84" title="" class="ltx_ref">84</a><span id="Sx1.T3.3.26.25.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.26.25.7.1.1.4" class="ltx_text" style="font-size:70%;"> architecture with a ResNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.26.25.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib99" title="" class="ltx_ref">99</a><span id="Sx1.T3.3.26.25.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.26.25.7.1.1.7" class="ltx_text" style="font-size:70%;"> backbone</span></span>
</span>
</td>
<td id="Sx1.T3.3.26.25.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.26.25.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.26.25.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.26.25.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature level</span></span>
</span>
</td>
<td id="Sx1.T3.3.26.25.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.26.25.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.26.25.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.26.25.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.26.25.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.26.25.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Addition,</span></span>
<span id="Sx1.T3.3.26.25.9.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.26.25.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.26.25.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.26.25.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.26.25.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.26.25.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.26.25.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.26.25.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.26.25.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.26.25.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.26.25.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.26.25.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.27.26" class="ltx_tr">
<td id="Sx1.T3.3.27.26.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.27.26.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.27.26.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.27.26.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.27.26.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.27.26.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">FUTR3D </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.27.26.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib320" title="" class="ltx_ref">320</a><span id="Sx1.T3.3.27.26.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.27.26.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.27.26.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.27.26.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.27.26.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.27.26.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T3.3.27.26.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.27.26.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.27.26.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.27.26.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.27.26.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.27.26.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.27.26.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.27.26.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.27.26.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.27.26.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.27.26.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.27.26.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck, Bus, Pedestrian, Barrier, Trailer, Construction Vehicle, Motorcycle, Bicycle, Traffic cone</span></span>
</span>
</td>
<td id="Sx1.T3.3.27.26.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.27.26.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.27.26.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.27.26.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.27.26.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.27.26.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.27.26.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.27.26.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.27.26.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.27.26.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.27.26.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.27.26.7.1.1.1" class="ltx_text" style="font-size:70%;">FUTR3D </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.27.26.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib207" title="" class="ltx_ref">207</a><span id="Sx1.T3.3.27.26.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.27.26.7.1.1.4" class="ltx_text" style="font-size:70%;"> with ResNet-101 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.27.26.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib99" title="" class="ltx_ref">99</a><span id="Sx1.T3.3.27.26.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.27.26.7.1.1.7" class="ltx_text" style="font-size:70%;"> as backbone and FPN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.27.26.7.1.1.8.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib74" title="" class="ltx_ref">74</a><span id="Sx1.T3.3.27.26.7.1.1.9.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.27.26.7.1.1.10" class="ltx_text" style="font-size:70%;"> as neck</span></span>
</span>
</td>
<td id="Sx1.T3.3.27.26.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.27.26.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.27.26.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.27.26.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.27.26.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.27.26.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.27.26.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.27.26.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.27.26.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.27.26.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.27.26.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.27.26.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.27.26.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.27.26.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.27.26.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.27.26.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.27.26.11.1.1" class="ltx_p" style="width:56.9pt;"><a target="_blank" href="https://github.com/Tsinghua-MARS-Lab/futr3d" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/Tsinghua-MARS-Lab/futr3d</a></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.28.27" class="ltx_tr">
<td id="Sx1.T3.3.28.27.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.28.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.28.27.1.1.1" class="ltx_p" style="width:54.1pt;"><span id="Sx1.T3.3.28.27.1.1.1.1" class="ltx_text" style="font-size:70%;">Simple-BEV </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.28.27.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib203" title="" class="ltx_ref">203</a><span id="Sx1.T3.3.28.27.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.28.27.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.28.27.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.28.27.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.28.27.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T3.3.28.27.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.28.27.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.28.27.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.28.27.3.1.1.1" class="ltx_text" style="font-size:70%;">Semantic Segmentation</span></span>
</span>
</td>
<td id="Sx1.T3.3.28.27.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.28.27.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.28.27.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.28.27.4.1.1.1" class="ltx_text" style="font-size:70%;">2D pixel-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.28.27.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.28.27.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.28.27.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.28.27.5.1.1.1" class="ltx_text" style="font-size:70%;">Vehicle, Background</span></span>
</span>
</td>
<td id="Sx1.T3.3.28.27.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.28.27.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.28.27.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.28.27.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.28.27.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.28.27.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.28.27.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.28.27.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.28.27.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.28.27.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.28.27.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.28.27.7.1.1.1" class="ltx_text" style="font-size:70%;">A network with a ResNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.28.27.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib99" title="" class="ltx_ref">99</a><span id="Sx1.T3.3.28.27.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.28.27.7.1.1.4" class="ltx_text" style="font-size:70%;"> backbone</span></span>
</span>
</td>
<td id="Sx1.T3.3.28.27.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.28.27.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.28.27.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.28.27.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.28.27.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.28.27.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.28.27.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.28.27.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.28.27.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.28.27.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.28.27.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.28.27.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.28.27.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.28.27.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.28.27.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.28.27.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.28.27.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.28.27.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.29.28" class="ltx_tr">
<td id="Sx1.T3.3.29.28.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.29.28.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.29.28.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.29.28.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.29.28.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.29.28.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">RadSegNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.29.28.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib202" title="" class="ltx_ref">202</a><span id="Sx1.T3.3.29.28.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.29.28.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.29.28.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.29.28.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.29.28.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.29.28.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T3.3.29.28.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.29.28.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.29.28.3.1.1" class="ltx_p" style="width:68.3pt;">
<span id="Sx1.T3.3.29.28.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.29.28.3.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.29.28.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T3.3.29.28.3.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.29.28.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Semantic Segmentation</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.29.28.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.29.28.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.29.28.4.1.1" class="ltx_p" style="width:42.7pt;">
<span id="Sx1.T3.3.29.28.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.29.28.4.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.29.28.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level,</span></span>
<span id="Sx1.T3.3.29.28.4.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.29.28.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">2D pixel-level</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.29.28.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.29.28.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.29.28.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.29.28.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck</span></span>
</span>
</td>
<td id="Sx1.T3.3.29.28.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.29.28.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.29.28.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.29.28.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.29.28.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.29.28.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.29.28.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.29.28.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud,</span></span>
<span id="Sx1.T3.3.29.28.6.1.1.1.3" class="ltx_p"><span id="Sx1.T3.3.29.28.6.1.1.1.3.1" class="ltx_text" style="font-size:70%;">range-azimuth</span></span>
<span id="Sx1.T3.3.29.28.6.1.1.1.4" class="ltx_p"><span id="Sx1.T3.3.29.28.6.1.1.1.4.1" class="ltx_text" style="font-size:70%;">tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.29.28.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.29.28.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.29.28.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.29.28.7.1.1.1" class="ltx_text" style="font-size:70%;">RadSegNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.29.28.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib202" title="" class="ltx_ref">202</a><span id="Sx1.T3.3.29.28.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.29.28.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.29.28.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.29.28.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.29.28.8.1.1.1" class="ltx_text" style="font-size:70%;">Data-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.29.28.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.29.28.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.29.28.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.29.28.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.29.28.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.29.28.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.29.28.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.29.28.10.1.1.1" class="ltx_text" style="font-size:70%;">Astyx </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.29.28.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib317" title="" class="ltx_ref">317</a><span id="Sx1.T3.3.29.28.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.29.28.10.1.1.4" class="ltx_text" style="font-size:70%;">, RADIATE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.29.28.10.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib168" title="" class="ltx_ref">168</a><span id="Sx1.T3.3.29.28.10.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.29.28.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.29.28.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.29.28.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.29.28.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.30.29" class="ltx_tr">
<td id="Sx1.T3.3.30.29.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.30.29.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.30.29.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.30.29.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.30.29.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.30.29.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">RCBEV </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.30.29.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib207" title="" class="ltx_ref">207</a><span id="Sx1.T3.3.30.29.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.30.29.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.30.29.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.30.29.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.30.29.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.30.29.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T3.3.30.29.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.30.29.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.30.29.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.30.29.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.30.29.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.30.29.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.30.29.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.30.29.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.30.29.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.30.29.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.30.29.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.30.29.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck, Bus, Pedestrian, Barrier, Trailer, Construction Vehicle, Motorcycle, Bicycle, Traffic cone</span></span>
</span>
</td>
<td id="Sx1.T3.3.30.29.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.30.29.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.30.29.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.30.29.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.30.29.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.30.29.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.30.29.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.30.29.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.30.29.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.30.29.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.30.29.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.30.29.7.1.1.1" class="ltx_text" style="font-size:70%;">RCBEV </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.30.29.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib207" title="" class="ltx_ref">207</a><span id="Sx1.T3.3.30.29.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.30.29.7.1.1.4" class="ltx_text" style="font-size:70%;"> with Swin Transformer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.30.29.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib89" title="" class="ltx_ref">89</a><span id="Sx1.T3.3.30.29.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.30.29.7.1.1.7" class="ltx_text" style="font-size:70%;"> as backbone and FPN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.30.29.7.1.1.8.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib74" title="" class="ltx_ref">74</a><span id="Sx1.T3.3.30.29.7.1.1.9.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.30.29.7.1.1.10" class="ltx_text" style="font-size:70%;"> as neck</span></span>
</span>
</td>
<td id="Sx1.T3.3.30.29.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.30.29.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.30.29.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.30.29.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.30.29.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.30.29.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.30.29.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.30.29.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.30.29.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.30.29.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.30.29.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.30.29.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.30.29.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.30.29.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.30.29.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.30.29.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.30.29.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.30.29.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.31.30" class="ltx_tr">
<td id="Sx1.T3.3.31.30.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.31.30.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.31.30.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.31.30.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.31.30.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.31.30.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">CRAFT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.31.30.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib248" title="" class="ltx_ref">248</a><span id="Sx1.T3.3.31.30.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.31.30.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.31.30.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.31.30.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.31.30.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.31.30.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T3.3.31.30.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.31.30.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.31.30.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.31.30.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.31.30.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.31.30.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.31.30.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.31.30.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.31.30.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.31.30.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.31.30.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.31.30.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck, Bus, Pedestrian, Barrier, Trailer, Construction Vehicle, Motorcycle, Bicycle, Traffic cone</span></span>
</span>
</td>
<td id="Sx1.T3.3.31.30.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.31.30.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.31.30.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.31.30.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.31.30.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.31.30.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.31.30.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.31.30.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.31.30.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.31.30.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.31.30.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.31.30.7.1.1.1" class="ltx_text" style="font-size:70%;">CRAFT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.31.30.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib248" title="" class="ltx_ref">248</a><span id="Sx1.T3.3.31.30.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.31.30.7.1.1.4" class="ltx_text" style="font-size:70%;"> based on DLA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.31.30.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib319" title="" class="ltx_ref">319</a><span id="Sx1.T3.3.31.30.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.31.30.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.31.30.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.31.30.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.31.30.8.1.1.1" class="ltx_text" style="font-size:70%;">Data-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.31.30.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.31.30.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.31.30.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.31.30.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.31.30.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.31.30.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.31.30.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.31.30.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.31.30.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.31.30.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.31.30.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.31.30.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.31.30.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.31.30.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.32.31" class="ltx_tr">
<td id="Sx1.T3.3.32.31.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.32.31.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.32.31.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.32.31.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.32.31.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.32.31.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">DeepFusion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.32.31.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib321" title="" class="ltx_ref">321</a><span id="Sx1.T3.3.32.31.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.32.31.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.32.31.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.32.31.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.32.31.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.32.31.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T3.3.32.31.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.32.31.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.32.31.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.32.31.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.32.31.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.32.31.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.32.31.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.32.31.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.32.31.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.32.31.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.32.31.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.32.31.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck, Bus, Pedestrian, Barrier, Trailer, Construction Vehicle, Motorcycle, Bicycle, Traffic cone</span></span>
</span>
</td>
<td id="Sx1.T3.3.32.31.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.32.31.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.32.31.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.32.31.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.32.31.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.32.31.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.32.31.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.32.31.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.32.31.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.32.31.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.32.31.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.32.31.7.1.1.1" class="ltx_text" style="font-size:70%;">DeepFusion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.32.31.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib321" title="" class="ltx_ref">321</a><span id="Sx1.T3.3.32.31.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.32.31.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.32.31.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.32.31.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.32.31.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.32.31.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.32.31.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.32.31.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.32.31.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.32.31.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.32.31.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.32.31.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.32.31.10.1.1.1" class="ltx_text" style="font-size:70%;">Self-recorded, nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.32.31.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.32.31.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.32.31.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.32.31.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.32.31.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.32.31.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.33.32" class="ltx_tr">
<td id="Sx1.T3.3.33.32.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.33.32.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.33.32.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.33.32.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.33.32.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.33.32.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">CramNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.33.32.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib253" title="" class="ltx_ref">253</a><span id="Sx1.T3.3.33.32.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.33.32.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.33.32.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.33.32.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.33.32.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.33.32.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="Sx1.T3.3.33.32.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.33.32.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.33.32.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.33.32.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.33.32.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.33.32.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.33.32.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.33.32.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.33.32.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.33.32.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.33.32.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.33.32.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Van, Truck, Bus, Motorbike, Bicycle</span></span>
</span>
</td>
<td id="Sx1.T3.3.33.32.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.33.32.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.33.32.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.33.32.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.33.32.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.33.32.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.33.32.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.33.32.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar:</span></span>
<span id="Sx1.T3.3.33.32.6.1.1.1.3" class="ltx_p"><span id="Sx1.T3.3.33.32.6.1.1.1.3.1" class="ltx_text" style="font-size:70%;">range-azimuth tensor</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.33.32.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.33.32.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.33.32.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.33.32.7.1.1.1" class="ltx_text" style="font-size:70%;">CramNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.33.32.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib253" title="" class="ltx_ref">253</a><span id="Sx1.T3.3.33.32.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.33.32.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.33.32.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.33.32.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.33.32.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.33.32.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.33.32.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.33.32.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.33.32.9.1.1.1" class="ltx_text" style="font-size:70%;">Attention</span></span>
</span>
</td>
<td id="Sx1.T3.3.33.32.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.33.32.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.33.32.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.33.32.10.1.1.1" class="ltx_text" style="font-size:70%;">RADIATE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.33.32.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib168" title="" class="ltx_ref">168</a><span id="Sx1.T3.3.33.32.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.33.32.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.33.32.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.33.32.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.33.32.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.34.33" class="ltx_tr">
<td id="Sx1.T3.3.34.33.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.34.33.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.34.33.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.34.33.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.34.33.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.34.33.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">MVFusion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.34.33.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib249" title="" class="ltx_ref">249</a><span id="Sx1.T3.3.34.33.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.34.33.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.34.33.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.34.33.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.34.33.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.34.33.2.1.1.1" class="ltx_text" style="font-size:70%;">2023</span></span>
</span>
</td>
<td id="Sx1.T3.3.34.33.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.34.33.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.34.33.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.34.33.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.34.33.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.34.33.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.34.33.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.34.33.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.34.33.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.34.33.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.34.33.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.34.33.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck, Bus, Pedestrian, Barrier, Trailer, Construction Vehicle, Motorcycle, Bicycle, Traffic cone</span></span>
</span>
</td>
<td id="Sx1.T3.3.34.33.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.34.33.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.34.33.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.34.33.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.34.33.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.34.33.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.34.33.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.34.33.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.34.33.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.34.33.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.34.33.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.34.33.7.1.1.1" class="ltx_text" style="font-size:70%;">MVFusion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.34.33.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib249" title="" class="ltx_ref">249</a><span id="Sx1.T3.3.34.33.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.34.33.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.34.33.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.34.33.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.34.33.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.34.33.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.34.33.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.34.33.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.34.33.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.34.33.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.34.33.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Addition;</span></span>
<span id="Sx1.T3.3.34.33.9.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.34.33.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.34.33.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.34.33.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.34.33.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.34.33.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.34.33.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.34.33.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.34.33.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.34.33.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.34.33.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.34.33.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.35.34" class="ltx_tr">
<td id="Sx1.T3.3.35.34.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.35.34.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.35.34.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.35.34.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.35.34.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.35.34.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">CRN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.35.34.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib250" title="" class="ltx_ref">250</a><span id="Sx1.T3.3.35.34.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.35.34.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.35.34.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.35.34.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.35.34.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.35.34.2.1.1.1" class="ltx_text" style="font-size:70%;">2023</span></span>
</span>
</td>
<td id="Sx1.T3.3.35.34.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.35.34.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.35.34.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.35.34.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.35.34.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.35.34.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.35.34.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.35.34.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.35.34.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.35.34.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.35.34.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.35.34.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck, Bus, Pedestrian, Barrier, Trailer, Construction Vehicle, Motorcycle, Bicycle, Traffic cone</span></span>
</span>
</td>
<td id="Sx1.T3.3.35.34.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.35.34.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.35.34.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.35.34.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.35.34.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.35.34.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.35.34.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.35.34.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.35.34.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.35.34.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.35.34.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.35.34.7.1.1.1" class="ltx_text" style="font-size:70%;">CRN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.35.34.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib250" title="" class="ltx_ref">250</a><span id="Sx1.T3.3.35.34.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.35.34.7.1.1.4" class="ltx_text" style="font-size:70%;"> based on ResNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.35.34.7.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib99" title="" class="ltx_ref">99</a><span id="Sx1.T3.3.35.34.7.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.35.34.7.1.1.7" class="ltx_text" style="font-size:70%;">, ConvNeXt </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.35.34.7.1.1.8.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib322" title="" class="ltx_ref">322</a><span id="Sx1.T3.3.35.34.7.1.1.9.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.35.34.7.1.1.10" class="ltx_text" style="font-size:70%;"> and FPN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.35.34.7.1.1.11.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib74" title="" class="ltx_ref">74</a><span id="Sx1.T3.3.35.34.7.1.1.12.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.35.34.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.35.34.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.35.34.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.35.34.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.35.34.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.35.34.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.35.34.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.35.34.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.35.34.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.35.34.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.35.34.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.35.34.10.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.35.34.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T3.3.35.34.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.35.34.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.35.34.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.35.34.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.35.34.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.36.35" class="ltx_tr">
<td id="Sx1.T3.3.36.35.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.36.35.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.36.35.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.36.35.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.36.35.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.36.35.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">RCFusion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.36.35.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib251" title="" class="ltx_ref">251</a><span id="Sx1.T3.3.36.35.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.36.35.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.36.35.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.36.35.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.36.35.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.36.35.2.1.1.1" class="ltx_text" style="font-size:70%;">2023</span></span>
</span>
</td>
<td id="Sx1.T3.3.36.35.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.36.35.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.36.35.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.36.35.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.36.35.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.36.35.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.36.35.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.36.35.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.36.35.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.36.35.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.36.35.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.36.35.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Truck, Pedestrian, Cyclist</span></span>
</span>
</td>
<td id="Sx1.T3.3.36.35.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.36.35.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.36.35.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.36.35.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.36.35.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.36.35.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.36.35.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.36.35.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.36.35.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.36.35.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.36.35.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.36.35.7.1.1.1" class="ltx_text" style="font-size:70%;">RCFusion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.36.35.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib251" title="" class="ltx_ref">251</a><span id="Sx1.T3.3.36.35.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.36.35.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.36.35.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.36.35.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.36.35.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.36.35.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.36.35.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.36.35.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.36.35.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.36.35.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.36.35.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation;</span></span>
<span id="Sx1.T3.3.36.35.9.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.36.35.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Multiplication;</span></span>
<span id="Sx1.T3.3.36.35.9.1.1.1.3" class="ltx_p"><span id="Sx1.T3.3.36.35.9.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Attention</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.36.35.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.36.35.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.36.35.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.36.35.10.1.1.1" class="ltx_text" style="font-size:70%;">VoD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.36.35.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="Sx1.T3.3.36.35.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.36.35.10.1.1.4" class="ltx_text" style="font-size:70%;">,
TJ4DRadSet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.36.35.10.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib176" title="" class="ltx_ref">176</a><span id="Sx1.T3.3.36.35.10.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.36.35.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.36.35.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.36.35.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.36.35.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.37.36" class="ltx_tr">
<td id="Sx1.T3.3.37.36.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.37.36.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.37.36.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.37.36.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.37.36.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.37.36.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">LXL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.37.36.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib247" title="" class="ltx_ref">247</a><span id="Sx1.T3.3.37.36.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.37.36.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.37.36.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.37.36.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.37.36.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.37.36.2.1.1.1" class="ltx_text" style="font-size:70%;">2023</span></span>
</span>
</td>
<td id="Sx1.T3.3.37.36.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.37.36.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.37.36.3.1.1" class="ltx_p" style="width:68.3pt;"><span id="Sx1.T3.3.37.36.3.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection</span></span>
</span>
</td>
<td id="Sx1.T3.3.37.36.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.37.36.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.37.36.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.37.36.4.1.1.1" class="ltx_text" style="font-size:70%;">3D box-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.37.36.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.37.36.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.37.36.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.37.36.5.1.1.1" class="ltx_text" style="font-size:70%;">Car, Pedestrian, Cyclist</span></span>
</span>
</td>
<td id="Sx1.T3.3.37.36.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.37.36.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.37.36.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.37.36.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.37.36.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.37.36.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.37.36.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.37.36.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.37.36.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.37.36.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.37.36.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.37.36.7.1.1.1" class="ltx_text" style="font-size:70%;">LXL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.37.36.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib247" title="" class="ltx_ref">247</a><span id="Sx1.T3.3.37.36.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.37.36.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.37.36.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.37.36.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.37.36.8.1.1.1" class="ltx_text" style="font-size:70%;">Feature-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.37.36.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.37.36.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.37.36.9.1.1" class="ltx_p" style="width:62.6pt;">
<span id="Sx1.T3.3.37.36.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.37.36.9.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.37.36.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation;</span></span>
<span id="Sx1.T3.3.37.36.9.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.37.36.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Multiplication</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.37.36.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.37.36.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.37.36.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.37.36.10.1.1.1" class="ltx_text" style="font-size:70%;">VoD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.37.36.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="Sx1.T3.3.37.36.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.37.36.10.1.1.4" class="ltx_text" style="font-size:70%;">,
TJ4DRadSet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.37.36.10.1.1.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib176" title="" class="ltx_ref">176</a><span id="Sx1.T3.3.37.36.10.1.1.6.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.37.36.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T3.3.37.36.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.37.36.11.1.1" class="ltx_p" style="width:56.9pt;"><span id="Sx1.T3.3.37.36.11.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T3.3.38.37" class="ltx_tr">
<td id="Sx1.T3.3.38.37.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T3.3.38.37.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.38.37.1.1.1" class="ltx_p" style="width:54.1pt;">
<span id="Sx1.T3.3.38.37.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.38.37.1.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.38.37.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Achelous </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.38.37.1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib323" title="" class="ltx_ref">323</a><span id="Sx1.T3.3.38.37.1.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="Sx1.T3.3.38.37.1.1.1.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.38.37.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T3.3.38.37.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.38.37.2.1.1" class="ltx_p" style="width:17.1pt;"><span id="Sx1.T3.3.38.37.2.1.1.1" class="ltx_text" style="font-size:70%;">2023</span></span>
</span>
</td>
<td id="Sx1.T3.3.38.37.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T3.3.38.37.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.38.37.3.1.1" class="ltx_p" style="width:68.3pt;">
<span id="Sx1.T3.3.38.37.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.38.37.3.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.38.37.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Object Detection,</span></span>
<span id="Sx1.T3.3.38.37.3.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.38.37.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Semantic Segmentation,</span></span>
<span id="Sx1.T3.3.38.37.3.1.1.1.3" class="ltx_p"><span id="Sx1.T3.3.38.37.3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Free-space Segmentation,</span></span>
<span id="Sx1.T3.3.38.37.3.1.1.1.4" class="ltx_p"><span id="Sx1.T3.3.38.37.3.1.1.1.4.1" class="ltx_text" style="font-size:70%;">Waterline Segmentation,</span></span>
<span id="Sx1.T3.3.38.37.3.1.1.1.5" class="ltx_p"><span id="Sx1.T3.3.38.37.3.1.1.1.5.1" class="ltx_text" style="font-size:70%;">Point Cloud Segmentation</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.38.37.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T3.3.38.37.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.38.37.4.1.1" class="ltx_p" style="width:42.7pt;">
<span id="Sx1.T3.3.38.37.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.38.37.4.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.38.37.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">2D box-level,</span></span>
<span id="Sx1.T3.3.38.37.4.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.38.37.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">2D pixel-level,</span></span>
<span id="Sx1.T3.3.38.37.4.1.1.1.3" class="ltx_p"><span id="Sx1.T3.3.38.37.4.1.1.1.3.1" class="ltx_text" style="font-size:70%;">3D point-level</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.38.37.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T3.3.38.37.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.38.37.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T3.3.38.37.5.1.1.1" class="ltx_text" style="font-size:70%;">Pier, Buoy, Sailor, Ship, Boat, Vessel, Kayak</span></span>
</span>
</td>
<td id="Sx1.T3.3.38.37.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T3.3.38.37.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.38.37.6.1.1" class="ltx_p" style="width:59.8pt;">
<span id="Sx1.T3.3.38.37.6.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T3.3.38.37.6.1.1.1.1" class="ltx_p"><span id="Sx1.T3.3.38.37.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Camera: RGB image;</span></span>
<span id="Sx1.T3.3.38.37.6.1.1.1.2" class="ltx_p"><span id="Sx1.T3.3.38.37.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Radar: point cloud</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T3.3.38.37.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T3.3.38.37.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.38.37.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.38.37.7.1.1.1" class="ltx_text" style="font-size:70%;">Achelous </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.38.37.7.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib323" title="" class="ltx_ref">323</a><span id="Sx1.T3.3.38.37.7.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.38.37.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T3.3.38.37.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.38.37.8.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx1.T3.3.38.37.8.1.1.1" class="ltx_text" style="font-size:70%;">Data-level</span></span>
</span>
</td>
<td id="Sx1.T3.3.38.37.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T3.3.38.37.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.38.37.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="Sx1.T3.3.38.37.9.1.1.1" class="ltx_text" style="font-size:70%;">Concatenation</span></span>
</span>
</td>
<td id="Sx1.T3.3.38.37.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T3.3.38.37.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.38.37.10.1.1" class="ltx_p" style="width:42.7pt;"><span id="Sx1.T3.3.38.37.10.1.1.1" class="ltx_text" style="font-size:70%;">WaterScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T3.3.38.37.10.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib174" title="" class="ltx_ref">174</a><span id="Sx1.T3.3.38.37.10.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T3.3.38.37.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T3.3.38.37.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T3.3.38.37.11.1.1" class="ltx_p" style="width:56.9pt;"><a target="_blank" href="https://github.com/GuanRunwei/Achelous" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/GuanRunwei/Achelous</a></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="Sx1.T4" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>SUMMARY OF RADAR-CAMERA FUSION EVALUATION METRICS</figcaption>
<table id="Sx1.T4.3" class="ltx_tabular">
<thead class="ltx_thead">
<tr id="Sx1.T4.3.1.1" class="ltx_tr">
<th id="Sx1.T4.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt"><span id="Sx1.T4.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Metric</span></th>
<th id="Sx1.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt"><span id="Sx1.T4.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Formula</span></th>
<th id="Sx1.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt"><span id="Sx1.T4.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Definition</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx1.T4.3.2.1" class="ltx_tr">
<td id="Sx1.T4.3.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.2.1.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.2.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Accuracy</span></span>
</span>
</td>
<td id="Sx1.T4.3.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.2.1.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="Sx1.E1" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E1.m1.1" class="ltx_Math" alttext="\mathrm{Accuracy}=\frac{\mathrm{TP+TN}}{\mathrm{TP+TN+FP+FN}}" display="block"><semantics id="Sx1.E1.m1.1a"><mrow id="Sx1.E1.m1.1.1" xref="Sx1.E1.m1.1.1.cmml"><mi mathsize="70%" id="Sx1.E1.m1.1.1.2" xref="Sx1.E1.m1.1.1.2.cmml">Accuracy</mi><mo mathsize="70%" id="Sx1.E1.m1.1.1.1" xref="Sx1.E1.m1.1.1.1.cmml">=</mo><mfrac id="Sx1.E1.m1.1.1.3" xref="Sx1.E1.m1.1.1.3.cmml"><mrow id="Sx1.E1.m1.1.1.3.2" xref="Sx1.E1.m1.1.1.3.2.cmml"><mi mathsize="70%" id="Sx1.E1.m1.1.1.3.2.2" xref="Sx1.E1.m1.1.1.3.2.2.cmml">TP</mi><mo mathsize="70%" id="Sx1.E1.m1.1.1.3.2.1" xref="Sx1.E1.m1.1.1.3.2.1.cmml">+</mo><mi mathsize="70%" id="Sx1.E1.m1.1.1.3.2.3" xref="Sx1.E1.m1.1.1.3.2.3.cmml">TN</mi></mrow><mrow id="Sx1.E1.m1.1.1.3.3" xref="Sx1.E1.m1.1.1.3.3.cmml"><mi mathsize="70%" id="Sx1.E1.m1.1.1.3.3.2" xref="Sx1.E1.m1.1.1.3.3.2.cmml">TP</mi><mo mathsize="70%" id="Sx1.E1.m1.1.1.3.3.1" xref="Sx1.E1.m1.1.1.3.3.1.cmml">+</mo><mi mathsize="70%" id="Sx1.E1.m1.1.1.3.3.3" xref="Sx1.E1.m1.1.1.3.3.3.cmml">TN</mi><mo mathsize="70%" id="Sx1.E1.m1.1.1.3.3.1a" xref="Sx1.E1.m1.1.1.3.3.1.cmml">+</mo><mi mathsize="70%" id="Sx1.E1.m1.1.1.3.3.4" xref="Sx1.E1.m1.1.1.3.3.4.cmml">FP</mi><mo mathsize="70%" id="Sx1.E1.m1.1.1.3.3.1b" xref="Sx1.E1.m1.1.1.3.3.1.cmml">+</mo><mi mathsize="70%" id="Sx1.E1.m1.1.1.3.3.5" xref="Sx1.E1.m1.1.1.3.3.5.cmml">FN</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E1.m1.1b"><apply id="Sx1.E1.m1.1.1.cmml" xref="Sx1.E1.m1.1.1"><eq id="Sx1.E1.m1.1.1.1.cmml" xref="Sx1.E1.m1.1.1.1"></eq><ci id="Sx1.E1.m1.1.1.2.cmml" xref="Sx1.E1.m1.1.1.2">Accuracy</ci><apply id="Sx1.E1.m1.1.1.3.cmml" xref="Sx1.E1.m1.1.1.3"><divide id="Sx1.E1.m1.1.1.3.1.cmml" xref="Sx1.E1.m1.1.1.3"></divide><apply id="Sx1.E1.m1.1.1.3.2.cmml" xref="Sx1.E1.m1.1.1.3.2"><plus id="Sx1.E1.m1.1.1.3.2.1.cmml" xref="Sx1.E1.m1.1.1.3.2.1"></plus><ci id="Sx1.E1.m1.1.1.3.2.2.cmml" xref="Sx1.E1.m1.1.1.3.2.2">TP</ci><ci id="Sx1.E1.m1.1.1.3.2.3.cmml" xref="Sx1.E1.m1.1.1.3.2.3">TN</ci></apply><apply id="Sx1.E1.m1.1.1.3.3.cmml" xref="Sx1.E1.m1.1.1.3.3"><plus id="Sx1.E1.m1.1.1.3.3.1.cmml" xref="Sx1.E1.m1.1.1.3.3.1"></plus><ci id="Sx1.E1.m1.1.1.3.3.2.cmml" xref="Sx1.E1.m1.1.1.3.3.2">TP</ci><ci id="Sx1.E1.m1.1.1.3.3.3.cmml" xref="Sx1.E1.m1.1.1.3.3.3">TN</ci><ci id="Sx1.E1.m1.1.1.3.3.4.cmml" xref="Sx1.E1.m1.1.1.3.3.4">FP</ci><ci id="Sx1.E1.m1.1.1.3.3.5.cmml" xref="Sx1.E1.m1.1.1.3.3.5">FN</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E1.m1.1c">\mathrm{Accuracy}=\frac{\mathrm{TP+TN}}{\mathrm{TP+TN+FP+FN}}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.2.1.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.2.1.3.1.1.1" class="ltx_text" style="font-size:70%;">             
Accuracy is the number of correct predictions over all predictions.</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T4.3.3.2" class="ltx_tr">
<td id="Sx1.T4.3.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.3.2.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.3.2.1.1.1.1" class="ltx_text" style="font-size:70%;">
Precision</span></span>
</span>
</td>
<td id="Sx1.T4.3.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.3.2.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="Sx1.E2" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E2.m1.1" class="ltx_Math" alttext="\mathrm{Precision}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}" display="block"><semantics id="Sx1.E2.m1.1a"><mrow id="Sx1.E2.m1.1.1" xref="Sx1.E2.m1.1.1.cmml"><mi mathsize="70%" id="Sx1.E2.m1.1.1.2" xref="Sx1.E2.m1.1.1.2.cmml">Precision</mi><mo mathsize="70%" id="Sx1.E2.m1.1.1.1" xref="Sx1.E2.m1.1.1.1.cmml">=</mo><mfrac id="Sx1.E2.m1.1.1.3" xref="Sx1.E2.m1.1.1.3.cmml"><mi mathsize="70%" id="Sx1.E2.m1.1.1.3.2" xref="Sx1.E2.m1.1.1.3.2.cmml">TP</mi><mrow id="Sx1.E2.m1.1.1.3.3" xref="Sx1.E2.m1.1.1.3.3.cmml"><mi mathsize="70%" id="Sx1.E2.m1.1.1.3.3.2" xref="Sx1.E2.m1.1.1.3.3.2.cmml">TP</mi><mo mathsize="70%" id="Sx1.E2.m1.1.1.3.3.1" xref="Sx1.E2.m1.1.1.3.3.1.cmml">+</mo><mi mathsize="70%" id="Sx1.E2.m1.1.1.3.3.3" xref="Sx1.E2.m1.1.1.3.3.3.cmml">FP</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E2.m1.1b"><apply id="Sx1.E2.m1.1.1.cmml" xref="Sx1.E2.m1.1.1"><eq id="Sx1.E2.m1.1.1.1.cmml" xref="Sx1.E2.m1.1.1.1"></eq><ci id="Sx1.E2.m1.1.1.2.cmml" xref="Sx1.E2.m1.1.1.2">Precision</ci><apply id="Sx1.E2.m1.1.1.3.cmml" xref="Sx1.E2.m1.1.1.3"><divide id="Sx1.E2.m1.1.1.3.1.cmml" xref="Sx1.E2.m1.1.1.3"></divide><ci id="Sx1.E2.m1.1.1.3.2.cmml" xref="Sx1.E2.m1.1.1.3.2">TP</ci><apply id="Sx1.E2.m1.1.1.3.3.cmml" xref="Sx1.E2.m1.1.1.3.3"><plus id="Sx1.E2.m1.1.1.3.3.1.cmml" xref="Sx1.E2.m1.1.1.3.3.1"></plus><ci id="Sx1.E2.m1.1.1.3.3.2.cmml" xref="Sx1.E2.m1.1.1.3.3.2">TP</ci><ci id="Sx1.E2.m1.1.1.3.3.3.cmml" xref="Sx1.E2.m1.1.1.3.3.3">FP</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E2.m1.1c">\mathrm{Precision}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.3.2.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.3.2.3.1.1.1" class="ltx_text" style="font-size:70%;">
              Precision is the fraction of true positive among total predicted positive.</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T4.3.4.3" class="ltx_tr">
<td id="Sx1.T4.3.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.4.3.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.4.3.1.1.1.1" class="ltx_text" style="font-size:70%;">
Recall</span></span>
</span>
</td>
<td id="Sx1.T4.3.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.4.3.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="Sx1.E3" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E3.m1.1" class="ltx_Math" alttext="\mathrm{Recall}=\frac{\mathrm{TP}}{\mathrm{TP+FN}}" display="block"><semantics id="Sx1.E3.m1.1a"><mrow id="Sx1.E3.m1.1.1" xref="Sx1.E3.m1.1.1.cmml"><mi mathsize="70%" id="Sx1.E3.m1.1.1.2" xref="Sx1.E3.m1.1.1.2.cmml">Recall</mi><mo mathsize="70%" id="Sx1.E3.m1.1.1.1" xref="Sx1.E3.m1.1.1.1.cmml">=</mo><mfrac id="Sx1.E3.m1.1.1.3" xref="Sx1.E3.m1.1.1.3.cmml"><mi mathsize="70%" id="Sx1.E3.m1.1.1.3.2" xref="Sx1.E3.m1.1.1.3.2.cmml">TP</mi><mrow id="Sx1.E3.m1.1.1.3.3" xref="Sx1.E3.m1.1.1.3.3.cmml"><mi mathsize="70%" id="Sx1.E3.m1.1.1.3.3.2" xref="Sx1.E3.m1.1.1.3.3.2.cmml">TP</mi><mo mathsize="70%" id="Sx1.E3.m1.1.1.3.3.1" xref="Sx1.E3.m1.1.1.3.3.1.cmml">+</mo><mi mathsize="70%" id="Sx1.E3.m1.1.1.3.3.3" xref="Sx1.E3.m1.1.1.3.3.3.cmml">FN</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E3.m1.1b"><apply id="Sx1.E3.m1.1.1.cmml" xref="Sx1.E3.m1.1.1"><eq id="Sx1.E3.m1.1.1.1.cmml" xref="Sx1.E3.m1.1.1.1"></eq><ci id="Sx1.E3.m1.1.1.2.cmml" xref="Sx1.E3.m1.1.1.2">Recall</ci><apply id="Sx1.E3.m1.1.1.3.cmml" xref="Sx1.E3.m1.1.1.3"><divide id="Sx1.E3.m1.1.1.3.1.cmml" xref="Sx1.E3.m1.1.1.3"></divide><ci id="Sx1.E3.m1.1.1.3.2.cmml" xref="Sx1.E3.m1.1.1.3.2">TP</ci><apply id="Sx1.E3.m1.1.1.3.3.cmml" xref="Sx1.E3.m1.1.1.3.3"><plus id="Sx1.E3.m1.1.1.3.3.1.cmml" xref="Sx1.E3.m1.1.1.3.3.1"></plus><ci id="Sx1.E3.m1.1.1.3.3.2.cmml" xref="Sx1.E3.m1.1.1.3.3.2">TP</ci><ci id="Sx1.E3.m1.1.1.3.3.3.cmml" xref="Sx1.E3.m1.1.1.3.3.3">FN</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E3.m1.1c">\mathrm{Recall}=\frac{\mathrm{TP}}{\mathrm{TP+FN}}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.4.3.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.4.3.3.1.1.1" class="ltx_text" style="font-size:70%;">             
Recall is the fraction of true positive over all actual positive in the dataset.</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T4.3.5.4" class="ltx_tr">
<td id="Sx1.T4.3.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.5.4.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.5.4.1.1.1.1" class="ltx_text" style="font-size:70%;">
F1-Score</span></span>
</span>
</td>
<td id="Sx1.T4.3.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.5.4.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="Sx1.E4" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E4.m1.1" class="ltx_Math" alttext="\text{F1-Score}=2*\frac{\mathrm{Precision*Recall}}{\mathrm{Precision+Recall}}" display="block"><semantics id="Sx1.E4.m1.1a"><mrow id="Sx1.E4.m1.1.1" xref="Sx1.E4.m1.1.1.cmml"><mtext mathsize="70%" id="Sx1.E4.m1.1.1.2" xref="Sx1.E4.m1.1.1.2a.cmml">F1-Score</mtext><mo mathsize="70%" id="Sx1.E4.m1.1.1.1" xref="Sx1.E4.m1.1.1.1.cmml">=</mo><mrow id="Sx1.E4.m1.1.1.3" xref="Sx1.E4.m1.1.1.3.cmml"><mn mathsize="70%" id="Sx1.E4.m1.1.1.3.2" xref="Sx1.E4.m1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" mathsize="70%" rspace="0.222em" id="Sx1.E4.m1.1.1.3.1" xref="Sx1.E4.m1.1.1.3.1.cmml">∗</mo><mfrac id="Sx1.E4.m1.1.1.3.3" xref="Sx1.E4.m1.1.1.3.3.cmml"><mrow id="Sx1.E4.m1.1.1.3.3.2" xref="Sx1.E4.m1.1.1.3.3.2.cmml"><mi mathsize="70%" id="Sx1.E4.m1.1.1.3.3.2.2" xref="Sx1.E4.m1.1.1.3.3.2.2.cmml">Precision</mi><mo lspace="0.222em" mathsize="70%" rspace="0.222em" id="Sx1.E4.m1.1.1.3.3.2.1" xref="Sx1.E4.m1.1.1.3.3.2.1.cmml">∗</mo><mi mathsize="70%" id="Sx1.E4.m1.1.1.3.3.2.3" xref="Sx1.E4.m1.1.1.3.3.2.3.cmml">Recall</mi></mrow><mrow id="Sx1.E4.m1.1.1.3.3.3" xref="Sx1.E4.m1.1.1.3.3.3.cmml"><mi mathsize="70%" id="Sx1.E4.m1.1.1.3.3.3.2" xref="Sx1.E4.m1.1.1.3.3.3.2.cmml">Precision</mi><mo mathsize="70%" id="Sx1.E4.m1.1.1.3.3.3.1" xref="Sx1.E4.m1.1.1.3.3.3.1.cmml">+</mo><mi mathsize="70%" id="Sx1.E4.m1.1.1.3.3.3.3" xref="Sx1.E4.m1.1.1.3.3.3.3.cmml">Recall</mi></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E4.m1.1b"><apply id="Sx1.E4.m1.1.1.cmml" xref="Sx1.E4.m1.1.1"><eq id="Sx1.E4.m1.1.1.1.cmml" xref="Sx1.E4.m1.1.1.1"></eq><ci id="Sx1.E4.m1.1.1.2a.cmml" xref="Sx1.E4.m1.1.1.2"><mtext mathsize="70%" id="Sx1.E4.m1.1.1.2.cmml" xref="Sx1.E4.m1.1.1.2">F1-Score</mtext></ci><apply id="Sx1.E4.m1.1.1.3.cmml" xref="Sx1.E4.m1.1.1.3"><times id="Sx1.E4.m1.1.1.3.1.cmml" xref="Sx1.E4.m1.1.1.3.1"></times><cn type="integer" id="Sx1.E4.m1.1.1.3.2.cmml" xref="Sx1.E4.m1.1.1.3.2">2</cn><apply id="Sx1.E4.m1.1.1.3.3.cmml" xref="Sx1.E4.m1.1.1.3.3"><divide id="Sx1.E4.m1.1.1.3.3.1.cmml" xref="Sx1.E4.m1.1.1.3.3"></divide><apply id="Sx1.E4.m1.1.1.3.3.2.cmml" xref="Sx1.E4.m1.1.1.3.3.2"><times id="Sx1.E4.m1.1.1.3.3.2.1.cmml" xref="Sx1.E4.m1.1.1.3.3.2.1"></times><ci id="Sx1.E4.m1.1.1.3.3.2.2.cmml" xref="Sx1.E4.m1.1.1.3.3.2.2">Precision</ci><ci id="Sx1.E4.m1.1.1.3.3.2.3.cmml" xref="Sx1.E4.m1.1.1.3.3.2.3">Recall</ci></apply><apply id="Sx1.E4.m1.1.1.3.3.3.cmml" xref="Sx1.E4.m1.1.1.3.3.3"><plus id="Sx1.E4.m1.1.1.3.3.3.1.cmml" xref="Sx1.E4.m1.1.1.3.3.3.1"></plus><ci id="Sx1.E4.m1.1.1.3.3.3.2.cmml" xref="Sx1.E4.m1.1.1.3.3.3.2">Precision</ci><ci id="Sx1.E4.m1.1.1.3.3.3.3.cmml" xref="Sx1.E4.m1.1.1.3.3.3.3">Recall</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E4.m1.1c">\text{F1-Score}=2*\frac{\mathrm{Precision*Recall}}{\mathrm{Precision+Recall}}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.5.4.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.5.4.3.1.1.1" class="ltx_text" style="font-size:70%;">             
</span>
<span id="Sx1.T4.3.5.4.3.1.1.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:238.5pt;">
<span id="Sx1.T4.3.5.4.3.1.1.2.1" class="ltx_p"><span id="Sx1.T4.3.5.4.3.1.1.2.1.1" class="ltx_text" style="font-size:70%;">F1-Score is the harmonic mean of precision and recall, describing a balance between precision and recall.</span></span>
</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T4.3.6.5" class="ltx_tr">
<td id="Sx1.T4.3.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.6.5.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.6.5.1.1.1.1" class="ltx_text" style="font-size:70%;">
Average Precision (AP)</span></span>
</span>
</td>
<td id="Sx1.T4.3.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.6.5.2.1.1" class="ltx_p" style="width:227.6pt;"><span id="Sx1.T4.3.6.5.2.1.1.1" class="ltx_text" style="font-size:70%;">
</span>
<span id="Sx1.E5" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E5.m1.1" class="ltx_Math" alttext="\mathrm{AP}={\int_{0}^{1}\mathrm{precision}(r)\mathrm{d}r}" display="block"><semantics id="Sx1.E5.m1.1a"><mrow id="Sx1.E5.m1.1.2" xref="Sx1.E5.m1.1.2.cmml"><mi mathsize="70%" id="Sx1.E5.m1.1.2.2" xref="Sx1.E5.m1.1.2.2.cmml">AP</mi><mo mathsize="70%" rspace="0.111em" id="Sx1.E5.m1.1.2.1" xref="Sx1.E5.m1.1.2.1.cmml">=</mo><mrow id="Sx1.E5.m1.1.2.3" xref="Sx1.E5.m1.1.2.3.cmml"><msubsup id="Sx1.E5.m1.1.2.3.1" xref="Sx1.E5.m1.1.2.3.1.cmml"><mo maxsize="70%" minsize="70%" stretchy="true" id="Sx1.E5.m1.1.2.3.1.2.2" xref="Sx1.E5.m1.1.2.3.1.2.2.cmml">∫</mo><mn mathsize="70%" id="Sx1.E5.m1.1.2.3.1.2.3" xref="Sx1.E5.m1.1.2.3.1.2.3.cmml">0</mn><mn mathsize="70%" id="Sx1.E5.m1.1.2.3.1.3" xref="Sx1.E5.m1.1.2.3.1.3.cmml">1</mn></msubsup><mrow id="Sx1.E5.m1.1.2.3.2" xref="Sx1.E5.m1.1.2.3.2.cmml"><mi mathsize="70%" id="Sx1.E5.m1.1.2.3.2.2" xref="Sx1.E5.m1.1.2.3.2.2.cmml">precision</mi><mo lspace="0em" rspace="0em" id="Sx1.E5.m1.1.2.3.2.1" xref="Sx1.E5.m1.1.2.3.2.1.cmml">​</mo><mrow id="Sx1.E5.m1.1.2.3.2.3.2" xref="Sx1.E5.m1.1.2.3.2.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E5.m1.1.2.3.2.3.2.1" xref="Sx1.E5.m1.1.2.3.2.cmml">(</mo><mi mathsize="70%" id="Sx1.E5.m1.1.1" xref="Sx1.E5.m1.1.1.cmml">r</mi><mo maxsize="70%" minsize="70%" id="Sx1.E5.m1.1.2.3.2.3.2.2" xref="Sx1.E5.m1.1.2.3.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="Sx1.E5.m1.1.2.3.2.1a" xref="Sx1.E5.m1.1.2.3.2.1.cmml">​</mo><mrow id="Sx1.E5.m1.1.2.3.2.4" xref="Sx1.E5.m1.1.2.3.2.4.cmml"><mo mathsize="70%" rspace="0em" id="Sx1.E5.m1.1.2.3.2.4.1" xref="Sx1.E5.m1.1.2.3.2.4.1.cmml">d</mo><mi mathsize="70%" id="Sx1.E5.m1.1.2.3.2.4.2" xref="Sx1.E5.m1.1.2.3.2.4.2.cmml">r</mi></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E5.m1.1b"><apply id="Sx1.E5.m1.1.2.cmml" xref="Sx1.E5.m1.1.2"><eq id="Sx1.E5.m1.1.2.1.cmml" xref="Sx1.E5.m1.1.2.1"></eq><ci id="Sx1.E5.m1.1.2.2.cmml" xref="Sx1.E5.m1.1.2.2">AP</ci><apply id="Sx1.E5.m1.1.2.3.cmml" xref="Sx1.E5.m1.1.2.3"><apply id="Sx1.E5.m1.1.2.3.1.cmml" xref="Sx1.E5.m1.1.2.3.1"><csymbol cd="ambiguous" id="Sx1.E5.m1.1.2.3.1.1.cmml" xref="Sx1.E5.m1.1.2.3.1">superscript</csymbol><apply id="Sx1.E5.m1.1.2.3.1.2.cmml" xref="Sx1.E5.m1.1.2.3.1"><csymbol cd="ambiguous" id="Sx1.E5.m1.1.2.3.1.2.1.cmml" xref="Sx1.E5.m1.1.2.3.1">subscript</csymbol><int id="Sx1.E5.m1.1.2.3.1.2.2.cmml" xref="Sx1.E5.m1.1.2.3.1.2.2"></int><cn type="integer" id="Sx1.E5.m1.1.2.3.1.2.3.cmml" xref="Sx1.E5.m1.1.2.3.1.2.3">0</cn></apply><cn type="integer" id="Sx1.E5.m1.1.2.3.1.3.cmml" xref="Sx1.E5.m1.1.2.3.1.3">1</cn></apply><apply id="Sx1.E5.m1.1.2.3.2.cmml" xref="Sx1.E5.m1.1.2.3.2"><times id="Sx1.E5.m1.1.2.3.2.1.cmml" xref="Sx1.E5.m1.1.2.3.2.1"></times><ci id="Sx1.E5.m1.1.2.3.2.2.cmml" xref="Sx1.E5.m1.1.2.3.2.2">precision</ci><ci id="Sx1.E5.m1.1.1.cmml" xref="Sx1.E5.m1.1.1">𝑟</ci><apply id="Sx1.E5.m1.1.2.3.2.4.cmml" xref="Sx1.E5.m1.1.2.3.2.4"><csymbol cd="latexml" id="Sx1.E5.m1.1.2.3.2.4.1.cmml" xref="Sx1.E5.m1.1.2.3.2.4.1">differential-d</csymbol><ci id="Sx1.E5.m1.1.2.3.2.4.2.cmml" xref="Sx1.E5.m1.1.2.3.2.4.2">𝑟</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E5.m1.1c">\mathrm{AP}={\int_{0}^{1}\mathrm{precision}(r)\mathrm{d}r}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.6.5.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.6.5.3.1.1.1" class="ltx_text" style="font-size:70%;">             
</span>
<span id="Sx1.T4.3.6.5.3.1.1.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:238.5pt;">
<span id="Sx1.T4.3.6.5.3.1.1.2.1" class="ltx_p"><span id="Sx1.T4.3.6.5.3.1.1.2.1.1" class="ltx_text" style="font-size:70%;">AP is the precision averaged over all recall values between 0 and 1 for a single class. It is the area under the Precision-Recall curve.</span></span>
<span id="Sx1.I1" class="ltx_itemize">
<span id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I1.i1.p1" class="ltx_para">
<span id="Sx1.I1.i1.p1.1" class="ltx_p"><math id="Sx1.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="Sx1.I1.i1.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I1.i1.p1.1.m1.1.1" xref="Sx1.I1.i1.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="Sx1.I1.i1.p1.1.m1.1b"><ci id="Sx1.I1.i1.p1.1.m1.1.1.cmml" xref="Sx1.I1.i1.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I1.i1.p1.1.m1.1c">r</annotation></semantics></math><span id="Sx1.I1.i1.p1.1.1" class="ltx_text" style="font-size:70%;">: recall value</span></span>
</span></span>
<span id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I1.i2.p1" class="ltx_para">
<span id="Sx1.I1.i2.p1.2" class="ltx_p"><span id="Sx1.I1.i2.p1.2.1" class="ltx_text" style="font-size:70%;">precision(</span><math id="Sx1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="Sx1.I1.i2.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I1.i2.p1.1.m1.1.1" xref="Sx1.I1.i2.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="Sx1.I1.i2.p1.1.m1.1b"><ci id="Sx1.I1.i2.p1.1.m1.1.1.cmml" xref="Sx1.I1.i2.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I1.i2.p1.1.m1.1c">r</annotation></semantics></math><span id="Sx1.I1.i2.p1.2.2" class="ltx_text" style="font-size:70%;">): the precision at recall value of </span><math id="Sx1.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="Sx1.I1.i2.p1.2.m2.1a"><mi mathsize="70%" id="Sx1.I1.i2.p1.2.m2.1.1" xref="Sx1.I1.i2.p1.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="Sx1.I1.i2.p1.2.m2.1b"><ci id="Sx1.I1.i2.p1.2.m2.1.1.cmml" xref="Sx1.I1.i2.p1.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I1.i2.p1.2.m2.1c">r</annotation></semantics></math><span id="Sx1.I1.i2.p1.2.3" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T4.3.7.6" class="ltx_tr">
<td id="Sx1.T4.3.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.7.6.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.7.6.1.1.1.1" class="ltx_text" style="font-size:70%;">
Average Recall (AR)</span></span>
</span>
</td>
<td id="Sx1.T4.3.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.7.6.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="Sx1.E6" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E6.m1.2" class="ltx_Math" alttext="\mathrm{AR}=2\int_{0.5}^{1}\operatorname{recall}(o)\mathrm{d}o" display="block"><semantics id="Sx1.E6.m1.2a"><mrow id="Sx1.E6.m1.2.3" xref="Sx1.E6.m1.2.3.cmml"><mi mathsize="70%" id="Sx1.E6.m1.2.3.2" xref="Sx1.E6.m1.2.3.2.cmml">AR</mi><mo mathsize="70%" id="Sx1.E6.m1.2.3.1" xref="Sx1.E6.m1.2.3.1.cmml">=</mo><mrow id="Sx1.E6.m1.2.3.3" xref="Sx1.E6.m1.2.3.3.cmml"><mn mathsize="70%" id="Sx1.E6.m1.2.3.3.2" xref="Sx1.E6.m1.2.3.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="Sx1.E6.m1.2.3.3.1" xref="Sx1.E6.m1.2.3.3.1.cmml">​</mo><mrow id="Sx1.E6.m1.2.3.3.3" xref="Sx1.E6.m1.2.3.3.3.cmml"><msubsup id="Sx1.E6.m1.2.3.3.3.1" xref="Sx1.E6.m1.2.3.3.3.1.cmml"><mo maxsize="70%" minsize="70%" stretchy="true" id="Sx1.E6.m1.2.3.3.3.1.2.2" xref="Sx1.E6.m1.2.3.3.3.1.2.2.cmml">∫</mo><mn mathsize="70%" id="Sx1.E6.m1.2.3.3.3.1.2.3" xref="Sx1.E6.m1.2.3.3.3.1.2.3.cmml">0.5</mn><mn mathsize="70%" id="Sx1.E6.m1.2.3.3.3.1.3" xref="Sx1.E6.m1.2.3.3.3.1.3.cmml">1</mn></msubsup><mrow id="Sx1.E6.m1.2.3.3.3.2" xref="Sx1.E6.m1.2.3.3.3.2.cmml"><mrow id="Sx1.E6.m1.2.3.3.3.2.2.2" xref="Sx1.E6.m1.2.3.3.3.2.2.1.cmml"><mi mathsize="70%" id="Sx1.E6.m1.1.1" xref="Sx1.E6.m1.1.1.cmml">recall</mi><mo id="Sx1.E6.m1.2.3.3.3.2.2.2a" xref="Sx1.E6.m1.2.3.3.3.2.2.1.cmml">⁡</mo><mrow id="Sx1.E6.m1.2.3.3.3.2.2.2.1" xref="Sx1.E6.m1.2.3.3.3.2.2.1.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E6.m1.2.3.3.3.2.2.2.1.1" xref="Sx1.E6.m1.2.3.3.3.2.2.1.cmml">(</mo><mi mathsize="70%" id="Sx1.E6.m1.2.2" xref="Sx1.E6.m1.2.2.cmml">o</mi><mo maxsize="70%" minsize="70%" id="Sx1.E6.m1.2.3.3.3.2.2.2.1.2" xref="Sx1.E6.m1.2.3.3.3.2.2.1.cmml">)</mo></mrow></mrow><mo lspace="0em" rspace="0em" id="Sx1.E6.m1.2.3.3.3.2.1" xref="Sx1.E6.m1.2.3.3.3.2.1.cmml">​</mo><mrow id="Sx1.E6.m1.2.3.3.3.2.3" xref="Sx1.E6.m1.2.3.3.3.2.3.cmml"><mo mathsize="70%" rspace="0em" id="Sx1.E6.m1.2.3.3.3.2.3.1" xref="Sx1.E6.m1.2.3.3.3.2.3.1.cmml">d</mo><mi mathsize="70%" id="Sx1.E6.m1.2.3.3.3.2.3.2" xref="Sx1.E6.m1.2.3.3.3.2.3.2.cmml">o</mi></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E6.m1.2b"><apply id="Sx1.E6.m1.2.3.cmml" xref="Sx1.E6.m1.2.3"><eq id="Sx1.E6.m1.2.3.1.cmml" xref="Sx1.E6.m1.2.3.1"></eq><ci id="Sx1.E6.m1.2.3.2.cmml" xref="Sx1.E6.m1.2.3.2">AR</ci><apply id="Sx1.E6.m1.2.3.3.cmml" xref="Sx1.E6.m1.2.3.3"><times id="Sx1.E6.m1.2.3.3.1.cmml" xref="Sx1.E6.m1.2.3.3.1"></times><cn type="integer" id="Sx1.E6.m1.2.3.3.2.cmml" xref="Sx1.E6.m1.2.3.3.2">2</cn><apply id="Sx1.E6.m1.2.3.3.3.cmml" xref="Sx1.E6.m1.2.3.3.3"><apply id="Sx1.E6.m1.2.3.3.3.1.cmml" xref="Sx1.E6.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="Sx1.E6.m1.2.3.3.3.1.1.cmml" xref="Sx1.E6.m1.2.3.3.3.1">superscript</csymbol><apply id="Sx1.E6.m1.2.3.3.3.1.2.cmml" xref="Sx1.E6.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="Sx1.E6.m1.2.3.3.3.1.2.1.cmml" xref="Sx1.E6.m1.2.3.3.3.1">subscript</csymbol><int id="Sx1.E6.m1.2.3.3.3.1.2.2.cmml" xref="Sx1.E6.m1.2.3.3.3.1.2.2"></int><cn type="float" id="Sx1.E6.m1.2.3.3.3.1.2.3.cmml" xref="Sx1.E6.m1.2.3.3.3.1.2.3">0.5</cn></apply><cn type="integer" id="Sx1.E6.m1.2.3.3.3.1.3.cmml" xref="Sx1.E6.m1.2.3.3.3.1.3">1</cn></apply><apply id="Sx1.E6.m1.2.3.3.3.2.cmml" xref="Sx1.E6.m1.2.3.3.3.2"><times id="Sx1.E6.m1.2.3.3.3.2.1.cmml" xref="Sx1.E6.m1.2.3.3.3.2.1"></times><apply id="Sx1.E6.m1.2.3.3.3.2.2.1.cmml" xref="Sx1.E6.m1.2.3.3.3.2.2.2"><ci id="Sx1.E6.m1.1.1.cmml" xref="Sx1.E6.m1.1.1">recall</ci><ci id="Sx1.E6.m1.2.2.cmml" xref="Sx1.E6.m1.2.2">𝑜</ci></apply><apply id="Sx1.E6.m1.2.3.3.3.2.3.cmml" xref="Sx1.E6.m1.2.3.3.3.2.3"><csymbol cd="latexml" id="Sx1.E6.m1.2.3.3.3.2.3.1.cmml" xref="Sx1.E6.m1.2.3.3.3.2.3.1">differential-d</csymbol><ci id="Sx1.E6.m1.2.3.3.3.2.3.2.cmml" xref="Sx1.E6.m1.2.3.3.3.2.3.2">𝑜</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E6.m1.2c">\mathrm{AR}=2\int_{0.5}^{1}\operatorname{recall}(o)\mathrm{d}o</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.7.6.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.7.6.3.1.1.1" class="ltx_text" style="font-size:70%;">             
</span>
<span id="Sx1.T4.3.7.6.3.1.1.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:238.5pt;">
<span id="Sx1.T4.3.7.6.3.1.1.2.1" class="ltx_p"><span id="Sx1.T4.3.7.6.3.1.1.2.1.1" class="ltx_text" style="font-size:70%;">AR is the average of all recalls at IoU thresholds from 0.5 to 1.0. It is twice the area under the Recall-IoU curve.</span></span>
<span id="Sx1.I2" class="ltx_itemize">
<span id="Sx1.I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I2.i1.p1" class="ltx_para">
<span id="Sx1.I2.i1.p1.1" class="ltx_p"><math id="Sx1.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="o" display="inline"><semantics id="Sx1.I2.i1.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I2.i1.p1.1.m1.1.1" xref="Sx1.I2.i1.p1.1.m1.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="Sx1.I2.i1.p1.1.m1.1b"><ci id="Sx1.I2.i1.p1.1.m1.1.1.cmml" xref="Sx1.I2.i1.p1.1.m1.1.1">𝑜</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I2.i1.p1.1.m1.1c">o</annotation></semantics></math><span id="Sx1.I2.i1.p1.1.1" class="ltx_text" style="font-size:70%;">: the IoU overlap</span></span>
</span></span>
<span id="Sx1.I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I2.i2.p1" class="ltx_para">
<span id="Sx1.I2.i2.p1.2" class="ltx_p"><span id="Sx1.I2.i2.p1.2.1" class="ltx_text" style="font-size:70%;">recall(</span><math id="Sx1.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="o" display="inline"><semantics id="Sx1.I2.i2.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I2.i2.p1.1.m1.1.1" xref="Sx1.I2.i2.p1.1.m1.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="Sx1.I2.i2.p1.1.m1.1b"><ci id="Sx1.I2.i2.p1.1.m1.1.1.cmml" xref="Sx1.I2.i2.p1.1.m1.1.1">𝑜</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I2.i2.p1.1.m1.1c">o</annotation></semantics></math><span id="Sx1.I2.i2.p1.2.2" class="ltx_text" style="font-size:70%;">): the recall at IoU value of </span><math id="Sx1.I2.i2.p1.2.m2.1" class="ltx_Math" alttext="o" display="inline"><semantics id="Sx1.I2.i2.p1.2.m2.1a"><mi mathsize="70%" id="Sx1.I2.i2.p1.2.m2.1.1" xref="Sx1.I2.i2.p1.2.m2.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="Sx1.I2.i2.p1.2.m2.1b"><ci id="Sx1.I2.i2.p1.2.m2.1.1.cmml" xref="Sx1.I2.i2.p1.2.m2.1.1">𝑜</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I2.i2.p1.2.m2.1c">o</annotation></semantics></math><span id="Sx1.I2.i2.p1.2.3" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T4.3.8.7" class="ltx_tr">
<td id="Sx1.T4.3.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.8.7.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.8.7.1.1.1.1" class="ltx_text" style="font-size:70%;">
Frame Per Second (FPS)</span></span>
</span>
</td>
<td id="Sx1.T4.3.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.8.7.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="Sx1.E7" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E7.m1.1" class="ltx_Math" alttext="\mathrm{FPS}=\frac{\mathrm{m}}{\mathrm{s}}" display="block"><semantics id="Sx1.E7.m1.1a"><mrow id="Sx1.E7.m1.1.1" xref="Sx1.E7.m1.1.1.cmml"><mi mathsize="70%" id="Sx1.E7.m1.1.1.2" xref="Sx1.E7.m1.1.1.2.cmml">FPS</mi><mo mathsize="70%" id="Sx1.E7.m1.1.1.1" xref="Sx1.E7.m1.1.1.1.cmml">=</mo><mfrac id="Sx1.E7.m1.1.1.3" xref="Sx1.E7.m1.1.1.3.cmml"><mi mathsize="70%" mathvariant="normal" id="Sx1.E7.m1.1.1.3.2" xref="Sx1.E7.m1.1.1.3.2.cmml">m</mi><mi mathsize="70%" mathvariant="normal" id="Sx1.E7.m1.1.1.3.3" xref="Sx1.E7.m1.1.1.3.3.cmml">s</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E7.m1.1b"><apply id="Sx1.E7.m1.1.1.cmml" xref="Sx1.E7.m1.1.1"><eq id="Sx1.E7.m1.1.1.1.cmml" xref="Sx1.E7.m1.1.1.1"></eq><ci id="Sx1.E7.m1.1.1.2.cmml" xref="Sx1.E7.m1.1.1.2">FPS</ci><apply id="Sx1.E7.m1.1.1.3.cmml" xref="Sx1.E7.m1.1.1.3"><divide id="Sx1.E7.m1.1.1.3.1.cmml" xref="Sx1.E7.m1.1.1.3"></divide><ci id="Sx1.E7.m1.1.1.3.2.cmml" xref="Sx1.E7.m1.1.1.3.2">m</ci><ci id="Sx1.E7.m1.1.1.3.3.cmml" xref="Sx1.E7.m1.1.1.3.3">s</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E7.m1.1c">\mathrm{FPS}=\frac{\mathrm{m}}{\mathrm{s}}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.8.7.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.8.7.3.1.1.1" class="ltx_text" style="font-size:70%;">             
</span>
<span id="Sx1.T4.3.8.7.3.1.1.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:238.5pt;">
<span id="Sx1.T4.3.8.7.3.1.1.2.1" class="ltx_p"><span id="Sx1.T4.3.8.7.3.1.1.2.1.1" class="ltx_text" style="font-size:70%;">FPS is a measure of how many images the model processes per second.</span></span>
<span id="Sx1.I3" class="ltx_itemize">
<span id="Sx1.I3.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I3.i1.p1" class="ltx_para">
<span id="Sx1.I3.i1.p1.1" class="ltx_p"><math id="Sx1.I3.i1.p1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="Sx1.I3.i1.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I3.i1.p1.1.m1.1.1" xref="Sx1.I3.i1.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Sx1.I3.i1.p1.1.m1.1b"><ci id="Sx1.I3.i1.p1.1.m1.1.1.cmml" xref="Sx1.I3.i1.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I3.i1.p1.1.m1.1c">m</annotation></semantics></math><span id="Sx1.I3.i1.p1.1.1" class="ltx_text" style="font-size:70%;">: the number of images</span></span>
</span></span>
<span id="Sx1.I3.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I3.i2.p1" class="ltx_para">
<span id="Sx1.I3.i2.p1.1" class="ltx_p"><math id="Sx1.I3.i2.p1.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="Sx1.I3.i2.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I3.i2.p1.1.m1.1.1" xref="Sx1.I3.i2.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Sx1.I3.i2.p1.1.m1.1b"><ci id="Sx1.I3.i2.p1.1.m1.1.1.cmml" xref="Sx1.I3.i2.p1.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I3.i2.p1.1.m1.1c">s</annotation></semantics></math><span id="Sx1.I3.i2.p1.1.1" class="ltx_text" style="font-size:70%;">: total seconds consumed</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T4.3.9.8" class="ltx_tr">
<td id="Sx1.T4.3.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.9.8.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.9.8.1.1.1.1" class="ltx_text" style="font-size:70%;">
Mean Average Precision (mAP)</span></span>
</span>
</td>
<td id="Sx1.T4.3.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.9.8.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="Sx1.E8" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E8.m1.1" class="ltx_Math" alttext="\mathrm{mAP}={\frac{1}{N}{\sum_{i=1}^{N}\operatorname{AP_{i}}}}" display="block"><semantics id="Sx1.E8.m1.1a"><mrow id="Sx1.E8.m1.1.1" xref="Sx1.E8.m1.1.1.cmml"><mi mathsize="70%" id="Sx1.E8.m1.1.1.2" xref="Sx1.E8.m1.1.1.2.cmml">mAP</mi><mo mathsize="70%" id="Sx1.E8.m1.1.1.1" xref="Sx1.E8.m1.1.1.1.cmml">=</mo><mrow id="Sx1.E8.m1.1.1.3" xref="Sx1.E8.m1.1.1.3.cmml"><mfrac id="Sx1.E8.m1.1.1.3.2" xref="Sx1.E8.m1.1.1.3.2.cmml"><mn mathsize="70%" id="Sx1.E8.m1.1.1.3.2.2" xref="Sx1.E8.m1.1.1.3.2.2.cmml">1</mn><mi mathsize="70%" id="Sx1.E8.m1.1.1.3.2.3" xref="Sx1.E8.m1.1.1.3.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="Sx1.E8.m1.1.1.3.1" xref="Sx1.E8.m1.1.1.3.1.cmml">​</mo><mrow id="Sx1.E8.m1.1.1.3.3" xref="Sx1.E8.m1.1.1.3.3.cmml"><munderover id="Sx1.E8.m1.1.1.3.3.1" xref="Sx1.E8.m1.1.1.3.3.1.cmml"><mo maxsize="70%" minsize="70%" movablelimits="false" stretchy="true" id="Sx1.E8.m1.1.1.3.3.1.2.2" xref="Sx1.E8.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="Sx1.E8.m1.1.1.3.3.1.2.3" xref="Sx1.E8.m1.1.1.3.3.1.2.3.cmml"><mi mathsize="70%" id="Sx1.E8.m1.1.1.3.3.1.2.3.2" xref="Sx1.E8.m1.1.1.3.3.1.2.3.2.cmml">i</mi><mo mathsize="70%" id="Sx1.E8.m1.1.1.3.3.1.2.3.1" xref="Sx1.E8.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn mathsize="70%" id="Sx1.E8.m1.1.1.3.3.1.2.3.3" xref="Sx1.E8.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi mathsize="70%" id="Sx1.E8.m1.1.1.3.3.1.3" xref="Sx1.E8.m1.1.1.3.3.1.3.cmml">N</mi></munderover><msub id="Sx1.E8.m1.1.1.3.3.2" xref="Sx1.E8.m1.1.1.3.3.2.cmml"><mi mathsize="70%" id="Sx1.E8.m1.1.1.3.3.2.2" xref="Sx1.E8.m1.1.1.3.3.2.2.cmml">AP</mi><mi mathsize="70%" mathvariant="normal" id="Sx1.E8.m1.1.1.3.3.2.3" xref="Sx1.E8.m1.1.1.3.3.2.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E8.m1.1b"><apply id="Sx1.E8.m1.1.1.cmml" xref="Sx1.E8.m1.1.1"><eq id="Sx1.E8.m1.1.1.1.cmml" xref="Sx1.E8.m1.1.1.1"></eq><ci id="Sx1.E8.m1.1.1.2.cmml" xref="Sx1.E8.m1.1.1.2">mAP</ci><apply id="Sx1.E8.m1.1.1.3.cmml" xref="Sx1.E8.m1.1.1.3"><times id="Sx1.E8.m1.1.1.3.1.cmml" xref="Sx1.E8.m1.1.1.3.1"></times><apply id="Sx1.E8.m1.1.1.3.2.cmml" xref="Sx1.E8.m1.1.1.3.2"><divide id="Sx1.E8.m1.1.1.3.2.1.cmml" xref="Sx1.E8.m1.1.1.3.2"></divide><cn type="integer" id="Sx1.E8.m1.1.1.3.2.2.cmml" xref="Sx1.E8.m1.1.1.3.2.2">1</cn><ci id="Sx1.E8.m1.1.1.3.2.3.cmml" xref="Sx1.E8.m1.1.1.3.2.3">𝑁</ci></apply><apply id="Sx1.E8.m1.1.1.3.3.cmml" xref="Sx1.E8.m1.1.1.3.3"><apply id="Sx1.E8.m1.1.1.3.3.1.cmml" xref="Sx1.E8.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="Sx1.E8.m1.1.1.3.3.1.1.cmml" xref="Sx1.E8.m1.1.1.3.3.1">superscript</csymbol><apply id="Sx1.E8.m1.1.1.3.3.1.2.cmml" xref="Sx1.E8.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="Sx1.E8.m1.1.1.3.3.1.2.1.cmml" xref="Sx1.E8.m1.1.1.3.3.1">subscript</csymbol><sum id="Sx1.E8.m1.1.1.3.3.1.2.2.cmml" xref="Sx1.E8.m1.1.1.3.3.1.2.2"></sum><apply id="Sx1.E8.m1.1.1.3.3.1.2.3.cmml" xref="Sx1.E8.m1.1.1.3.3.1.2.3"><eq id="Sx1.E8.m1.1.1.3.3.1.2.3.1.cmml" xref="Sx1.E8.m1.1.1.3.3.1.2.3.1"></eq><ci id="Sx1.E8.m1.1.1.3.3.1.2.3.2.cmml" xref="Sx1.E8.m1.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="Sx1.E8.m1.1.1.3.3.1.2.3.3.cmml" xref="Sx1.E8.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="Sx1.E8.m1.1.1.3.3.1.3.cmml" xref="Sx1.E8.m1.1.1.3.3.1.3">𝑁</ci></apply><apply id="Sx1.E8.m1.1.1.3.3.2.cmml" xref="Sx1.E8.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="Sx1.E8.m1.1.1.3.3.2.1.cmml" xref="Sx1.E8.m1.1.1.3.3.2">subscript</csymbol><ci id="Sx1.E8.m1.1.1.3.3.2.2.cmml" xref="Sx1.E8.m1.1.1.3.3.2.2">AP</ci><ci id="Sx1.E8.m1.1.1.3.3.2.3.cmml" xref="Sx1.E8.m1.1.1.3.3.2.3">i</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E8.m1.1c">\mathrm{mAP}={\frac{1}{N}{\sum_{i=1}^{N}\operatorname{AP_{i}}}}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.9.8.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.9.8.3.1.1.1" class="ltx_text" style="font-size:70%;">             
</span>
<span id="Sx1.T4.3.9.8.3.1.1.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:238.5pt;">
<span id="Sx1.T4.3.9.8.3.1.1.2.1" class="ltx_p"><span id="Sx1.T4.3.9.8.3.1.1.2.1.1" class="ltx_text" style="font-size:70%;">mAP is the average value of AP, that is, the average of the area under the Precision-Recall curve of each category.</span></span>
<span id="Sx1.I4" class="ltx_itemize">
<span id="Sx1.I4.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I4.i1.p1" class="ltx_para">
<span id="Sx1.I4.i1.p1.1" class="ltx_p"><math id="Sx1.I4.i1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Sx1.I4.i1.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I4.i1.p1.1.m1.1.1" xref="Sx1.I4.i1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Sx1.I4.i1.p1.1.m1.1b"><ci id="Sx1.I4.i1.p1.1.m1.1.1.cmml" xref="Sx1.I4.i1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I4.i1.p1.1.m1.1c">N</annotation></semantics></math><span id="Sx1.I4.i1.p1.1.1" class="ltx_text" style="font-size:70%;">: the number of classes</span></span>
</span></span>
<span id="Sx1.I4.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I4.i2.p1" class="ltx_para">
<span id="Sx1.I4.i2.p1.2" class="ltx_p"><math id="Sx1.I4.i2.p1.1.m1.1" class="ltx_Math" alttext="AP_{i}" display="inline"><semantics id="Sx1.I4.i2.p1.1.m1.1a"><mrow id="Sx1.I4.i2.p1.1.m1.1.1" xref="Sx1.I4.i2.p1.1.m1.1.1.cmml"><mi mathsize="70%" id="Sx1.I4.i2.p1.1.m1.1.1.2" xref="Sx1.I4.i2.p1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx1.I4.i2.p1.1.m1.1.1.1" xref="Sx1.I4.i2.p1.1.m1.1.1.1.cmml">​</mo><msub id="Sx1.I4.i2.p1.1.m1.1.1.3" xref="Sx1.I4.i2.p1.1.m1.1.1.3.cmml"><mi mathsize="70%" id="Sx1.I4.i2.p1.1.m1.1.1.3.2" xref="Sx1.I4.i2.p1.1.m1.1.1.3.2.cmml">P</mi><mi mathsize="70%" id="Sx1.I4.i2.p1.1.m1.1.1.3.3" xref="Sx1.I4.i2.p1.1.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx1.I4.i2.p1.1.m1.1b"><apply id="Sx1.I4.i2.p1.1.m1.1.1.cmml" xref="Sx1.I4.i2.p1.1.m1.1.1"><times id="Sx1.I4.i2.p1.1.m1.1.1.1.cmml" xref="Sx1.I4.i2.p1.1.m1.1.1.1"></times><ci id="Sx1.I4.i2.p1.1.m1.1.1.2.cmml" xref="Sx1.I4.i2.p1.1.m1.1.1.2">𝐴</ci><apply id="Sx1.I4.i2.p1.1.m1.1.1.3.cmml" xref="Sx1.I4.i2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="Sx1.I4.i2.p1.1.m1.1.1.3.1.cmml" xref="Sx1.I4.i2.p1.1.m1.1.1.3">subscript</csymbol><ci id="Sx1.I4.i2.p1.1.m1.1.1.3.2.cmml" xref="Sx1.I4.i2.p1.1.m1.1.1.3.2">𝑃</ci><ci id="Sx1.I4.i2.p1.1.m1.1.1.3.3.cmml" xref="Sx1.I4.i2.p1.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I4.i2.p1.1.m1.1c">AP_{i}</annotation></semantics></math><span id="Sx1.I4.i2.p1.2.1" class="ltx_text" style="font-size:70%;">: AP value of the </span><math id="Sx1.I4.i2.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="Sx1.I4.i2.p1.2.m2.1a"><mi mathsize="70%" id="Sx1.I4.i2.p1.2.m2.1.1" xref="Sx1.I4.i2.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="Sx1.I4.i2.p1.2.m2.1b"><ci id="Sx1.I4.i2.p1.2.m2.1.1.cmml" xref="Sx1.I4.i2.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I4.i2.p1.2.m2.1c">i</annotation></semantics></math><span id="Sx1.I4.i2.p1.2.2" class="ltx_text" style="font-size:70%;">th class</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T4.3.10.9" class="ltx_tr">
<td id="Sx1.T4.3.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.10.9.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.10.9.1.1.1.1" class="ltx_text" style="font-size:70%;">Mean Intersection over Union (mIoU)</span></span>
</span>
</td>
<td id="Sx1.T4.3.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.10.9.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="Sx1.E9" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E9.m1.1" class="ltx_Math" alttext="\mathrm{mIoU}=\frac{1}{N+1}\sum_{i=0}^{N}\frac{\mathrm{TP}}{\mathrm{FN+FP+TP}}" display="block"><semantics id="Sx1.E9.m1.1a"><mrow id="Sx1.E9.m1.1.1" xref="Sx1.E9.m1.1.1.cmml"><mi mathsize="70%" id="Sx1.E9.m1.1.1.2" xref="Sx1.E9.m1.1.1.2.cmml">mIoU</mi><mo mathsize="70%" id="Sx1.E9.m1.1.1.1" xref="Sx1.E9.m1.1.1.1.cmml">=</mo><mrow id="Sx1.E9.m1.1.1.3" xref="Sx1.E9.m1.1.1.3.cmml"><mfrac id="Sx1.E9.m1.1.1.3.2" xref="Sx1.E9.m1.1.1.3.2.cmml"><mn mathsize="70%" id="Sx1.E9.m1.1.1.3.2.2" xref="Sx1.E9.m1.1.1.3.2.2.cmml">1</mn><mrow id="Sx1.E9.m1.1.1.3.2.3" xref="Sx1.E9.m1.1.1.3.2.3.cmml"><mi mathsize="70%" id="Sx1.E9.m1.1.1.3.2.3.2" xref="Sx1.E9.m1.1.1.3.2.3.2.cmml">N</mi><mo mathsize="70%" id="Sx1.E9.m1.1.1.3.2.3.1" xref="Sx1.E9.m1.1.1.3.2.3.1.cmml">+</mo><mn mathsize="70%" id="Sx1.E9.m1.1.1.3.2.3.3" xref="Sx1.E9.m1.1.1.3.2.3.3.cmml">1</mn></mrow></mfrac><mo lspace="0em" rspace="0em" id="Sx1.E9.m1.1.1.3.1" xref="Sx1.E9.m1.1.1.3.1.cmml">​</mo><mrow id="Sx1.E9.m1.1.1.3.3" xref="Sx1.E9.m1.1.1.3.3.cmml"><munderover id="Sx1.E9.m1.1.1.3.3.1" xref="Sx1.E9.m1.1.1.3.3.1.cmml"><mo maxsize="70%" minsize="70%" movablelimits="false" stretchy="true" id="Sx1.E9.m1.1.1.3.3.1.2.2" xref="Sx1.E9.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="Sx1.E9.m1.1.1.3.3.1.2.3" xref="Sx1.E9.m1.1.1.3.3.1.2.3.cmml"><mi mathsize="70%" id="Sx1.E9.m1.1.1.3.3.1.2.3.2" xref="Sx1.E9.m1.1.1.3.3.1.2.3.2.cmml">i</mi><mo mathsize="70%" id="Sx1.E9.m1.1.1.3.3.1.2.3.1" xref="Sx1.E9.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn mathsize="70%" id="Sx1.E9.m1.1.1.3.3.1.2.3.3" xref="Sx1.E9.m1.1.1.3.3.1.2.3.3.cmml">0</mn></mrow><mi mathsize="70%" id="Sx1.E9.m1.1.1.3.3.1.3" xref="Sx1.E9.m1.1.1.3.3.1.3.cmml">N</mi></munderover><mfrac id="Sx1.E9.m1.1.1.3.3.2" xref="Sx1.E9.m1.1.1.3.3.2.cmml"><mi mathsize="70%" id="Sx1.E9.m1.1.1.3.3.2.2" xref="Sx1.E9.m1.1.1.3.3.2.2.cmml">TP</mi><mrow id="Sx1.E9.m1.1.1.3.3.2.3" xref="Sx1.E9.m1.1.1.3.3.2.3.cmml"><mi mathsize="70%" id="Sx1.E9.m1.1.1.3.3.2.3.2" xref="Sx1.E9.m1.1.1.3.3.2.3.2.cmml">FN</mi><mo mathsize="70%" id="Sx1.E9.m1.1.1.3.3.2.3.1" xref="Sx1.E9.m1.1.1.3.3.2.3.1.cmml">+</mo><mi mathsize="70%" id="Sx1.E9.m1.1.1.3.3.2.3.3" xref="Sx1.E9.m1.1.1.3.3.2.3.3.cmml">FP</mi><mo mathsize="70%" id="Sx1.E9.m1.1.1.3.3.2.3.1a" xref="Sx1.E9.m1.1.1.3.3.2.3.1.cmml">+</mo><mi mathsize="70%" id="Sx1.E9.m1.1.1.3.3.2.3.4" xref="Sx1.E9.m1.1.1.3.3.2.3.4.cmml">TP</mi></mrow></mfrac></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E9.m1.1b"><apply id="Sx1.E9.m1.1.1.cmml" xref="Sx1.E9.m1.1.1"><eq id="Sx1.E9.m1.1.1.1.cmml" xref="Sx1.E9.m1.1.1.1"></eq><ci id="Sx1.E9.m1.1.1.2.cmml" xref="Sx1.E9.m1.1.1.2">mIoU</ci><apply id="Sx1.E9.m1.1.1.3.cmml" xref="Sx1.E9.m1.1.1.3"><times id="Sx1.E9.m1.1.1.3.1.cmml" xref="Sx1.E9.m1.1.1.3.1"></times><apply id="Sx1.E9.m1.1.1.3.2.cmml" xref="Sx1.E9.m1.1.1.3.2"><divide id="Sx1.E9.m1.1.1.3.2.1.cmml" xref="Sx1.E9.m1.1.1.3.2"></divide><cn type="integer" id="Sx1.E9.m1.1.1.3.2.2.cmml" xref="Sx1.E9.m1.1.1.3.2.2">1</cn><apply id="Sx1.E9.m1.1.1.3.2.3.cmml" xref="Sx1.E9.m1.1.1.3.2.3"><plus id="Sx1.E9.m1.1.1.3.2.3.1.cmml" xref="Sx1.E9.m1.1.1.3.2.3.1"></plus><ci id="Sx1.E9.m1.1.1.3.2.3.2.cmml" xref="Sx1.E9.m1.1.1.3.2.3.2">𝑁</ci><cn type="integer" id="Sx1.E9.m1.1.1.3.2.3.3.cmml" xref="Sx1.E9.m1.1.1.3.2.3.3">1</cn></apply></apply><apply id="Sx1.E9.m1.1.1.3.3.cmml" xref="Sx1.E9.m1.1.1.3.3"><apply id="Sx1.E9.m1.1.1.3.3.1.cmml" xref="Sx1.E9.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="Sx1.E9.m1.1.1.3.3.1.1.cmml" xref="Sx1.E9.m1.1.1.3.3.1">superscript</csymbol><apply id="Sx1.E9.m1.1.1.3.3.1.2.cmml" xref="Sx1.E9.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="Sx1.E9.m1.1.1.3.3.1.2.1.cmml" xref="Sx1.E9.m1.1.1.3.3.1">subscript</csymbol><sum id="Sx1.E9.m1.1.1.3.3.1.2.2.cmml" xref="Sx1.E9.m1.1.1.3.3.1.2.2"></sum><apply id="Sx1.E9.m1.1.1.3.3.1.2.3.cmml" xref="Sx1.E9.m1.1.1.3.3.1.2.3"><eq id="Sx1.E9.m1.1.1.3.3.1.2.3.1.cmml" xref="Sx1.E9.m1.1.1.3.3.1.2.3.1"></eq><ci id="Sx1.E9.m1.1.1.3.3.1.2.3.2.cmml" xref="Sx1.E9.m1.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="Sx1.E9.m1.1.1.3.3.1.2.3.3.cmml" xref="Sx1.E9.m1.1.1.3.3.1.2.3.3">0</cn></apply></apply><ci id="Sx1.E9.m1.1.1.3.3.1.3.cmml" xref="Sx1.E9.m1.1.1.3.3.1.3">𝑁</ci></apply><apply id="Sx1.E9.m1.1.1.3.3.2.cmml" xref="Sx1.E9.m1.1.1.3.3.2"><divide id="Sx1.E9.m1.1.1.3.3.2.1.cmml" xref="Sx1.E9.m1.1.1.3.3.2"></divide><ci id="Sx1.E9.m1.1.1.3.3.2.2.cmml" xref="Sx1.E9.m1.1.1.3.3.2.2">TP</ci><apply id="Sx1.E9.m1.1.1.3.3.2.3.cmml" xref="Sx1.E9.m1.1.1.3.3.2.3"><plus id="Sx1.E9.m1.1.1.3.3.2.3.1.cmml" xref="Sx1.E9.m1.1.1.3.3.2.3.1"></plus><ci id="Sx1.E9.m1.1.1.3.3.2.3.2.cmml" xref="Sx1.E9.m1.1.1.3.3.2.3.2">FN</ci><ci id="Sx1.E9.m1.1.1.3.3.2.3.3.cmml" xref="Sx1.E9.m1.1.1.3.3.2.3.3">FP</ci><ci id="Sx1.E9.m1.1.1.3.3.2.3.4.cmml" xref="Sx1.E9.m1.1.1.3.3.2.3.4">TP</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E9.m1.1c">\mathrm{mIoU}=\frac{1}{N+1}\sum_{i=0}^{N}\frac{\mathrm{TP}}{\mathrm{FN+FP+TP}}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.10.9.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.10.9.3.1.1.1" class="ltx_text" style="font-size:70%;">             
</span>
<span id="Sx1.T4.3.10.9.3.1.1.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:238.5pt;">
<span id="Sx1.T4.3.10.9.3.1.1.2.1" class="ltx_p"><span id="Sx1.T4.3.10.9.3.1.1.2.1.1" class="ltx_text" style="font-size:70%;">IoU is the overlap between the predicted value and the ground truth divided by the area of union.
Then, mIoU is the average value of IoU over all classes.</span></span>
</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T4.3.11.10" class="ltx_tr">
<td id="Sx1.T4.3.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.11.10.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.11.10.1.1.1.1" class="ltx_text" style="font-size:70%;">
nuScenes Detection Score (NDS) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T4.3.11.10.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="Sx1.T4.3.11.10.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T4.3.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.11.10.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="Sx1.E10" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E10.m1.4" class="ltx_Math" alttext="\mathrm{NDS}=\frac{1}{10}\left[5\mathrm{mAP}+\sum_{\mathrm{mTP}\in\mathbb{TP}}(1-\min(1,\mathrm{mTP}))\right]" display="block"><semantics id="Sx1.E10.m1.4a"><mrow id="Sx1.E10.m1.4.4" xref="Sx1.E10.m1.4.4.cmml"><mi mathsize="70%" id="Sx1.E10.m1.4.4.3" xref="Sx1.E10.m1.4.4.3.cmml">NDS</mi><mo mathsize="70%" id="Sx1.E10.m1.4.4.2" xref="Sx1.E10.m1.4.4.2.cmml">=</mo><mrow id="Sx1.E10.m1.4.4.1" xref="Sx1.E10.m1.4.4.1.cmml"><mfrac id="Sx1.E10.m1.4.4.1.3" xref="Sx1.E10.m1.4.4.1.3.cmml"><mn mathsize="70%" id="Sx1.E10.m1.4.4.1.3.2" xref="Sx1.E10.m1.4.4.1.3.2.cmml">1</mn><mn mathsize="70%" id="Sx1.E10.m1.4.4.1.3.3" xref="Sx1.E10.m1.4.4.1.3.3.cmml">10</mn></mfrac><mo lspace="0em" rspace="0em" id="Sx1.E10.m1.4.4.1.2" xref="Sx1.E10.m1.4.4.1.2.cmml">​</mo><mrow id="Sx1.E10.m1.4.4.1.1.1" xref="Sx1.E10.m1.4.4.1.1.2.cmml"><mo id="Sx1.E10.m1.4.4.1.1.1.2" xref="Sx1.E10.m1.4.4.1.1.2.1.cmml">[</mo><mrow id="Sx1.E10.m1.4.4.1.1.1.1" xref="Sx1.E10.m1.4.4.1.1.1.1.cmml"><mrow id="Sx1.E10.m1.4.4.1.1.1.1.3" xref="Sx1.E10.m1.4.4.1.1.1.1.3.cmml"><mn mathsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.3.2" xref="Sx1.E10.m1.4.4.1.1.1.1.3.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="Sx1.E10.m1.4.4.1.1.1.1.3.1" xref="Sx1.E10.m1.4.4.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" mathvariant="normal" id="Sx1.E10.m1.4.4.1.1.1.1.3.3" xref="Sx1.E10.m1.4.4.1.1.1.1.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="Sx1.E10.m1.4.4.1.1.1.1.3.1a" xref="Sx1.E10.m1.4.4.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" mathvariant="normal" id="Sx1.E10.m1.4.4.1.1.1.1.3.4" xref="Sx1.E10.m1.4.4.1.1.1.1.3.4.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx1.E10.m1.4.4.1.1.1.1.3.1b" xref="Sx1.E10.m1.4.4.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" mathvariant="normal" id="Sx1.E10.m1.4.4.1.1.1.1.3.5" xref="Sx1.E10.m1.4.4.1.1.1.1.3.5.cmml">P</mi></mrow><mo mathsize="70%" rspace="0.055em" id="Sx1.E10.m1.4.4.1.1.1.1.2" xref="Sx1.E10.m1.4.4.1.1.1.1.2.cmml">+</mo><mrow id="Sx1.E10.m1.4.4.1.1.1.1.1" xref="Sx1.E10.m1.4.4.1.1.1.1.1.cmml"><munder id="Sx1.E10.m1.4.4.1.1.1.1.1.2" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.cmml"><mo maxsize="70%" minsize="70%" movablelimits="false" rspace="0em" stretchy="true" id="Sx1.E10.m1.4.4.1.1.1.1.1.2.2" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.cmml"><mi mathsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.2" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.2.cmml">mTP</mi><mo mathsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.1" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.1.cmml">∈</mo><mrow id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.cmml"><mi mathsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.2" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.2.cmml">𝕋</mi><mo lspace="0em" rspace="0em" id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.1" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi mathsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.3" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.3.cmml">ℙ</mi></mrow></mrow></munder><mrow id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.2" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mn mathsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.2" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo mathsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.1" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.2" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml"><mi mathsize="70%" id="Sx1.E10.m1.1.1" xref="Sx1.E10.m1.1.1.cmml">min</mi><mo id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.2a" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml">⁡</mo><mrow id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.2.1" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.2.1.1" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml">(</mo><mn mathsize="70%" id="Sx1.E10.m1.2.2" xref="Sx1.E10.m1.2.2.cmml">1</mn><mo mathsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.2.1.2" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml">,</mo><mi mathsize="70%" id="Sx1.E10.m1.3.3" xref="Sx1.E10.m1.3.3.cmml">mTP</mi><mo maxsize="70%" minsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.2.1.3" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo maxsize="70%" minsize="70%" id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.3" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="Sx1.E10.m1.4.4.1.1.1.3" xref="Sx1.E10.m1.4.4.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E10.m1.4b"><apply id="Sx1.E10.m1.4.4.cmml" xref="Sx1.E10.m1.4.4"><eq id="Sx1.E10.m1.4.4.2.cmml" xref="Sx1.E10.m1.4.4.2"></eq><ci id="Sx1.E10.m1.4.4.3.cmml" xref="Sx1.E10.m1.4.4.3">NDS</ci><apply id="Sx1.E10.m1.4.4.1.cmml" xref="Sx1.E10.m1.4.4.1"><times id="Sx1.E10.m1.4.4.1.2.cmml" xref="Sx1.E10.m1.4.4.1.2"></times><apply id="Sx1.E10.m1.4.4.1.3.cmml" xref="Sx1.E10.m1.4.4.1.3"><divide id="Sx1.E10.m1.4.4.1.3.1.cmml" xref="Sx1.E10.m1.4.4.1.3"></divide><cn type="integer" id="Sx1.E10.m1.4.4.1.3.2.cmml" xref="Sx1.E10.m1.4.4.1.3.2">1</cn><cn type="integer" id="Sx1.E10.m1.4.4.1.3.3.cmml" xref="Sx1.E10.m1.4.4.1.3.3">10</cn></apply><apply id="Sx1.E10.m1.4.4.1.1.2.cmml" xref="Sx1.E10.m1.4.4.1.1.1"><csymbol cd="latexml" id="Sx1.E10.m1.4.4.1.1.2.1.cmml" xref="Sx1.E10.m1.4.4.1.1.1.2">delimited-[]</csymbol><apply id="Sx1.E10.m1.4.4.1.1.1.1.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1"><plus id="Sx1.E10.m1.4.4.1.1.1.1.2.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.2"></plus><apply id="Sx1.E10.m1.4.4.1.1.1.1.3.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.3"><times id="Sx1.E10.m1.4.4.1.1.1.1.3.1.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.3.1"></times><cn type="integer" id="Sx1.E10.m1.4.4.1.1.1.1.3.2.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.3.2">5</cn><ci id="Sx1.E10.m1.4.4.1.1.1.1.3.3.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.3.3">m</ci><ci id="Sx1.E10.m1.4.4.1.1.1.1.3.4.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.3.4">A</ci><ci id="Sx1.E10.m1.4.4.1.1.1.1.3.5.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.3.5">P</ci></apply><apply id="Sx1.E10.m1.4.4.1.1.1.1.1.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1"><apply id="Sx1.E10.m1.4.4.1.1.1.1.1.2.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx1.E10.m1.4.4.1.1.1.1.1.2.1.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2">subscript</csymbol><sum id="Sx1.E10.m1.4.4.1.1.1.1.1.2.2.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.2"></sum><apply id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3"><in id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.1.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.1"></in><ci id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.2.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.2">mTP</ci><apply id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3"><times id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.1.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.1"></times><ci id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.2.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.2">𝕋</ci><ci id="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.3.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.2.3.3.3">ℙ</ci></apply></apply></apply><apply id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1"><minus id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.2">1</cn><apply id="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx1.E10.m1.4.4.1.1.1.1.1.1.1.1.3.2"><min id="Sx1.E10.m1.1.1.cmml" xref="Sx1.E10.m1.1.1"></min><cn type="integer" id="Sx1.E10.m1.2.2.cmml" xref="Sx1.E10.m1.2.2">1</cn><ci id="Sx1.E10.m1.3.3.cmml" xref="Sx1.E10.m1.3.3">mTP</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E10.m1.4c">\mathrm{NDS}=\frac{1}{10}\left[5\mathrm{mAP}+\sum_{\mathrm{mTP}\in\mathbb{TP}}(1-\min(1,\mathrm{mTP}))\right]</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></span></span></span>
</span><span id="Sx1.T4.3.11.10.2.1.1.1" class="ltx_text" style="font-size:70%;">
</span>
<span id="Sx1.E11" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E11.m1.1" class="ltx_Math" alttext="\mathrm{mTP}=\frac{1}{|\mathbb{C}|}\sum_{c\in\mathbb{C}}\mathrm{TP}_{c}" display="block"><semantics id="Sx1.E11.m1.1a"><mrow id="Sx1.E11.m1.1.2" xref="Sx1.E11.m1.1.2.cmml"><mi mathsize="70%" id="Sx1.E11.m1.1.2.2" xref="Sx1.E11.m1.1.2.2.cmml">mTP</mi><mo mathsize="70%" id="Sx1.E11.m1.1.2.1" xref="Sx1.E11.m1.1.2.1.cmml">=</mo><mrow id="Sx1.E11.m1.1.2.3" xref="Sx1.E11.m1.1.2.3.cmml"><mfrac id="Sx1.E11.m1.1.1" xref="Sx1.E11.m1.1.1.cmml"><mn mathsize="70%" id="Sx1.E11.m1.1.1.3" xref="Sx1.E11.m1.1.1.3.cmml">1</mn><mrow id="Sx1.E11.m1.1.1.1.3" xref="Sx1.E11.m1.1.1.1.2.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E11.m1.1.1.1.3.1" xref="Sx1.E11.m1.1.1.1.2.1.cmml">|</mo><mi mathsize="70%" id="Sx1.E11.m1.1.1.1.1" xref="Sx1.E11.m1.1.1.1.1.cmml">ℂ</mi><mo maxsize="70%" minsize="70%" id="Sx1.E11.m1.1.1.1.3.2" xref="Sx1.E11.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="Sx1.E11.m1.1.2.3.1" xref="Sx1.E11.m1.1.2.3.1.cmml">​</mo><mrow id="Sx1.E11.m1.1.2.3.2" xref="Sx1.E11.m1.1.2.3.2.cmml"><munder id="Sx1.E11.m1.1.2.3.2.1" xref="Sx1.E11.m1.1.2.3.2.1.cmml"><mo maxsize="70%" minsize="70%" movablelimits="false" stretchy="true" id="Sx1.E11.m1.1.2.3.2.1.2" xref="Sx1.E11.m1.1.2.3.2.1.2.cmml">∑</mo><mrow id="Sx1.E11.m1.1.2.3.2.1.3" xref="Sx1.E11.m1.1.2.3.2.1.3.cmml"><mi mathsize="70%" id="Sx1.E11.m1.1.2.3.2.1.3.2" xref="Sx1.E11.m1.1.2.3.2.1.3.2.cmml">c</mi><mo mathsize="70%" id="Sx1.E11.m1.1.2.3.2.1.3.1" xref="Sx1.E11.m1.1.2.3.2.1.3.1.cmml">∈</mo><mi mathsize="70%" id="Sx1.E11.m1.1.2.3.2.1.3.3" xref="Sx1.E11.m1.1.2.3.2.1.3.3.cmml">ℂ</mi></mrow></munder><msub id="Sx1.E11.m1.1.2.3.2.2" xref="Sx1.E11.m1.1.2.3.2.2.cmml"><mi mathsize="70%" id="Sx1.E11.m1.1.2.3.2.2.2" xref="Sx1.E11.m1.1.2.3.2.2.2.cmml">TP</mi><mi mathsize="70%" id="Sx1.E11.m1.1.2.3.2.2.3" xref="Sx1.E11.m1.1.2.3.2.2.3.cmml">c</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E11.m1.1b"><apply id="Sx1.E11.m1.1.2.cmml" xref="Sx1.E11.m1.1.2"><eq id="Sx1.E11.m1.1.2.1.cmml" xref="Sx1.E11.m1.1.2.1"></eq><ci id="Sx1.E11.m1.1.2.2.cmml" xref="Sx1.E11.m1.1.2.2">mTP</ci><apply id="Sx1.E11.m1.1.2.3.cmml" xref="Sx1.E11.m1.1.2.3"><times id="Sx1.E11.m1.1.2.3.1.cmml" xref="Sx1.E11.m1.1.2.3.1"></times><apply id="Sx1.E11.m1.1.1.cmml" xref="Sx1.E11.m1.1.1"><divide id="Sx1.E11.m1.1.1.2.cmml" xref="Sx1.E11.m1.1.1"></divide><cn type="integer" id="Sx1.E11.m1.1.1.3.cmml" xref="Sx1.E11.m1.1.1.3">1</cn><apply id="Sx1.E11.m1.1.1.1.2.cmml" xref="Sx1.E11.m1.1.1.1.3"><abs id="Sx1.E11.m1.1.1.1.2.1.cmml" xref="Sx1.E11.m1.1.1.1.3.1"></abs><ci id="Sx1.E11.m1.1.1.1.1.cmml" xref="Sx1.E11.m1.1.1.1.1">ℂ</ci></apply></apply><apply id="Sx1.E11.m1.1.2.3.2.cmml" xref="Sx1.E11.m1.1.2.3.2"><apply id="Sx1.E11.m1.1.2.3.2.1.cmml" xref="Sx1.E11.m1.1.2.3.2.1"><csymbol cd="ambiguous" id="Sx1.E11.m1.1.2.3.2.1.1.cmml" xref="Sx1.E11.m1.1.2.3.2.1">subscript</csymbol><sum id="Sx1.E11.m1.1.2.3.2.1.2.cmml" xref="Sx1.E11.m1.1.2.3.2.1.2"></sum><apply id="Sx1.E11.m1.1.2.3.2.1.3.cmml" xref="Sx1.E11.m1.1.2.3.2.1.3"><in id="Sx1.E11.m1.1.2.3.2.1.3.1.cmml" xref="Sx1.E11.m1.1.2.3.2.1.3.1"></in><ci id="Sx1.E11.m1.1.2.3.2.1.3.2.cmml" xref="Sx1.E11.m1.1.2.3.2.1.3.2">𝑐</ci><ci id="Sx1.E11.m1.1.2.3.2.1.3.3.cmml" xref="Sx1.E11.m1.1.2.3.2.1.3.3">ℂ</ci></apply></apply><apply id="Sx1.E11.m1.1.2.3.2.2.cmml" xref="Sx1.E11.m1.1.2.3.2.2"><csymbol cd="ambiguous" id="Sx1.E11.m1.1.2.3.2.2.1.cmml" xref="Sx1.E11.m1.1.2.3.2.2">subscript</csymbol><ci id="Sx1.E11.m1.1.2.3.2.2.2.cmml" xref="Sx1.E11.m1.1.2.3.2.2.2">TP</ci><ci id="Sx1.E11.m1.1.2.3.2.2.3.cmml" xref="Sx1.E11.m1.1.2.3.2.2.3">𝑐</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E11.m1.1c">\mathrm{mTP}=\frac{1}{|\mathbb{C}|}\sum_{c\in\mathbb{C}}\mathrm{TP}_{c}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.11.10.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.11.10.3.1.1.1" class="ltx_text" style="font-size:70%;">             
</span>
<span id="Sx1.T4.3.11.10.3.1.1.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:238.5pt;">
<span id="Sx1.T4.3.11.10.3.1.1.2.1" class="ltx_p"><span id="Sx1.T4.3.11.10.3.1.1.2.1.1" class="ltx_text" style="font-size:70%;">NDS is a weighted sum of mAP and five TP metrics.</span></span>
<span id="Sx1.I5" class="ltx_itemize">
<span id="Sx1.I5.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I5.i1.p1" class="ltx_para">
<span id="Sx1.I5.i1.p1.1" class="ltx_p"><span id="Sx1.I5.i1.p1.1.1" class="ltx_text" style="font-size:70%;">mAP: mean Average Precision over all classes</span></span>
</span></span>
<span id="Sx1.I5.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I5.i2.p1" class="ltx_para">
<span id="Sx1.I5.i2.p1.1" class="ltx_p"><math id="Sx1.I5.i2.p1.1.m1.1" class="ltx_Math" alttext="\mathbb{TP}" display="inline"><semantics id="Sx1.I5.i2.p1.1.m1.1a"><mrow id="Sx1.I5.i2.p1.1.m1.1.1" xref="Sx1.I5.i2.p1.1.m1.1.1.cmml"><mi mathsize="70%" id="Sx1.I5.i2.p1.1.m1.1.1.2" xref="Sx1.I5.i2.p1.1.m1.1.1.2.cmml">𝕋</mi><mo lspace="0em" rspace="0em" id="Sx1.I5.i2.p1.1.m1.1.1.1" xref="Sx1.I5.i2.p1.1.m1.1.1.1.cmml">​</mo><mi mathsize="70%" id="Sx1.I5.i2.p1.1.m1.1.1.3" xref="Sx1.I5.i2.p1.1.m1.1.1.3.cmml">ℙ</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx1.I5.i2.p1.1.m1.1b"><apply id="Sx1.I5.i2.p1.1.m1.1.1.cmml" xref="Sx1.I5.i2.p1.1.m1.1.1"><times id="Sx1.I5.i2.p1.1.m1.1.1.1.cmml" xref="Sx1.I5.i2.p1.1.m1.1.1.1"></times><ci id="Sx1.I5.i2.p1.1.m1.1.1.2.cmml" xref="Sx1.I5.i2.p1.1.m1.1.1.2">𝕋</ci><ci id="Sx1.I5.i2.p1.1.m1.1.1.3.cmml" xref="Sx1.I5.i2.p1.1.m1.1.1.3">ℙ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I5.i2.p1.1.m1.1c">\mathbb{TP}</annotation></semantics></math><span id="Sx1.I5.i2.p1.1.1" class="ltx_text" style="font-size:70%;">: the set of the five mean True Positive metrics, including box location, size, orientation, attributes, and velocity</span></span>
</span></span>
<span id="Sx1.I5.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I5.i3.p1" class="ltx_para">
<span id="Sx1.I5.i3.p1.1" class="ltx_p"><math id="Sx1.I5.i3.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{mTP}" display="inline"><semantics id="Sx1.I5.i3.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I5.i3.p1.1.m1.1.1" xref="Sx1.I5.i3.p1.1.m1.1.1.cmml">mTP</mi><annotation-xml encoding="MathML-Content" id="Sx1.I5.i3.p1.1.m1.1b"><ci id="Sx1.I5.i3.p1.1.m1.1.1.cmml" xref="Sx1.I5.i3.p1.1.m1.1.1">mTP</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I5.i3.p1.1.m1.1c">\mathrm{mTP}</annotation></semantics></math><span id="Sx1.I5.i3.p1.1.1" class="ltx_text" style="font-size:70%;">: the mean True Positive over all classes</span></span>
</span></span>
<span id="Sx1.I5.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I5.i4.p1" class="ltx_para">
<span id="Sx1.I5.i4.p1.1" class="ltx_p"><math id="Sx1.I5.i4.p1.1.m1.1" class="ltx_Math" alttext="\mathbb{C}" display="inline"><semantics id="Sx1.I5.i4.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I5.i4.p1.1.m1.1.1" xref="Sx1.I5.i4.p1.1.m1.1.1.cmml">ℂ</mi><annotation-xml encoding="MathML-Content" id="Sx1.I5.i4.p1.1.m1.1b"><ci id="Sx1.I5.i4.p1.1.m1.1.1.cmml" xref="Sx1.I5.i4.p1.1.m1.1.1">ℂ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I5.i4.p1.1.m1.1c">\mathbb{C}</annotation></semantics></math><span id="Sx1.I5.i4.p1.1.1" class="ltx_text" style="font-size:70%;">: the set of classes</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T4.3.12.11" class="ltx_tr">
<td id="Sx1.T4.3.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.12.11.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.12.11.1.1.1.1" class="ltx_text" style="font-size:70%;">
Object Location Similarity (OLS) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T4.3.12.11.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="Sx1.T4.3.12.11.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T4.3.12.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.12.11.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="Sx1.E12" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E12.m1.2" class="ltx_Math" alttext="\mathrm{OLS}=\exp\left\{\frac{-d^{2}}{2\left(s\kappa_{cls}\right)^{2}}\right\}" display="block"><semantics id="Sx1.E12.m1.2a"><mrow id="Sx1.E12.m1.2.3" xref="Sx1.E12.m1.2.3.cmml"><mi mathsize="70%" id="Sx1.E12.m1.2.3.2" xref="Sx1.E12.m1.2.3.2.cmml">OLS</mi><mo mathsize="70%" id="Sx1.E12.m1.2.3.1" xref="Sx1.E12.m1.2.3.1.cmml">=</mo><mrow id="Sx1.E12.m1.2.3.3.2" xref="Sx1.E12.m1.2.3.3.1.cmml"><mi mathsize="70%" id="Sx1.E12.m1.2.2" xref="Sx1.E12.m1.2.2.cmml">exp</mi><mo id="Sx1.E12.m1.2.3.3.2a" xref="Sx1.E12.m1.2.3.3.1.cmml">⁡</mo><mrow id="Sx1.E12.m1.2.3.3.2.1" xref="Sx1.E12.m1.2.3.3.1.cmml"><mo id="Sx1.E12.m1.2.3.3.2.1.1" xref="Sx1.E12.m1.2.3.3.1.cmml">{</mo><mfrac id="Sx1.E12.m1.1.1" xref="Sx1.E12.m1.1.1.cmml"><mrow id="Sx1.E12.m1.1.1.3" xref="Sx1.E12.m1.1.1.3.cmml"><mo mathsize="70%" id="Sx1.E12.m1.1.1.3a" xref="Sx1.E12.m1.1.1.3.cmml">−</mo><msup id="Sx1.E12.m1.1.1.3.2" xref="Sx1.E12.m1.1.1.3.2.cmml"><mi mathsize="70%" id="Sx1.E12.m1.1.1.3.2.2" xref="Sx1.E12.m1.1.1.3.2.2.cmml">d</mi><mn mathsize="70%" id="Sx1.E12.m1.1.1.3.2.3" xref="Sx1.E12.m1.1.1.3.2.3.cmml">2</mn></msup></mrow><mrow id="Sx1.E12.m1.1.1.1" xref="Sx1.E12.m1.1.1.1.cmml"><mn mathsize="70%" id="Sx1.E12.m1.1.1.1.3" xref="Sx1.E12.m1.1.1.1.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="Sx1.E12.m1.1.1.1.2" xref="Sx1.E12.m1.1.1.1.2.cmml">​</mo><msup id="Sx1.E12.m1.1.1.1.1" xref="Sx1.E12.m1.1.1.1.1.cmml"><mrow id="Sx1.E12.m1.1.1.1.1.1.1" xref="Sx1.E12.m1.1.1.1.1.1.1.1.cmml"><mo id="Sx1.E12.m1.1.1.1.1.1.1.2" xref="Sx1.E12.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx1.E12.m1.1.1.1.1.1.1.1" xref="Sx1.E12.m1.1.1.1.1.1.1.1.cmml"><mi mathsize="70%" id="Sx1.E12.m1.1.1.1.1.1.1.1.2" xref="Sx1.E12.m1.1.1.1.1.1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="Sx1.E12.m1.1.1.1.1.1.1.1.1" xref="Sx1.E12.m1.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="Sx1.E12.m1.1.1.1.1.1.1.1.3" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.cmml"><mi mathsize="70%" id="Sx1.E12.m1.1.1.1.1.1.1.1.3.2" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.2.cmml">κ</mi><mrow id="Sx1.E12.m1.1.1.1.1.1.1.1.3.3" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.cmml"><mi mathsize="70%" id="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.2" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.1" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi mathsize="70%" id="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.3" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.1a" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi mathsize="70%" id="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.4" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.4.cmml">s</mi></mrow></msub></mrow><mo id="Sx1.E12.m1.1.1.1.1.1.1.3" xref="Sx1.E12.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn mathsize="70%" id="Sx1.E12.m1.1.1.1.1.3" xref="Sx1.E12.m1.1.1.1.1.3.cmml">2</mn></msup></mrow></mfrac><mo id="Sx1.E12.m1.2.3.3.2.1.2" xref="Sx1.E12.m1.2.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E12.m1.2b"><apply id="Sx1.E12.m1.2.3.cmml" xref="Sx1.E12.m1.2.3"><eq id="Sx1.E12.m1.2.3.1.cmml" xref="Sx1.E12.m1.2.3.1"></eq><ci id="Sx1.E12.m1.2.3.2.cmml" xref="Sx1.E12.m1.2.3.2">OLS</ci><apply id="Sx1.E12.m1.2.3.3.1.cmml" xref="Sx1.E12.m1.2.3.3.2"><exp id="Sx1.E12.m1.2.2.cmml" xref="Sx1.E12.m1.2.2"></exp><apply id="Sx1.E12.m1.1.1.cmml" xref="Sx1.E12.m1.1.1"><divide id="Sx1.E12.m1.1.1.2.cmml" xref="Sx1.E12.m1.1.1"></divide><apply id="Sx1.E12.m1.1.1.3.cmml" xref="Sx1.E12.m1.1.1.3"><minus id="Sx1.E12.m1.1.1.3.1.cmml" xref="Sx1.E12.m1.1.1.3"></minus><apply id="Sx1.E12.m1.1.1.3.2.cmml" xref="Sx1.E12.m1.1.1.3.2"><csymbol cd="ambiguous" id="Sx1.E12.m1.1.1.3.2.1.cmml" xref="Sx1.E12.m1.1.1.3.2">superscript</csymbol><ci id="Sx1.E12.m1.1.1.3.2.2.cmml" xref="Sx1.E12.m1.1.1.3.2.2">𝑑</ci><cn type="integer" id="Sx1.E12.m1.1.1.3.2.3.cmml" xref="Sx1.E12.m1.1.1.3.2.3">2</cn></apply></apply><apply id="Sx1.E12.m1.1.1.1.cmml" xref="Sx1.E12.m1.1.1.1"><times id="Sx1.E12.m1.1.1.1.2.cmml" xref="Sx1.E12.m1.1.1.1.2"></times><cn type="integer" id="Sx1.E12.m1.1.1.1.3.cmml" xref="Sx1.E12.m1.1.1.1.3">2</cn><apply id="Sx1.E12.m1.1.1.1.1.cmml" xref="Sx1.E12.m1.1.1.1.1"><csymbol cd="ambiguous" id="Sx1.E12.m1.1.1.1.1.2.cmml" xref="Sx1.E12.m1.1.1.1.1">superscript</csymbol><apply id="Sx1.E12.m1.1.1.1.1.1.1.1.cmml" xref="Sx1.E12.m1.1.1.1.1.1.1"><times id="Sx1.E12.m1.1.1.1.1.1.1.1.1.cmml" xref="Sx1.E12.m1.1.1.1.1.1.1.1.1"></times><ci id="Sx1.E12.m1.1.1.1.1.1.1.1.2.cmml" xref="Sx1.E12.m1.1.1.1.1.1.1.1.2">𝑠</ci><apply id="Sx1.E12.m1.1.1.1.1.1.1.1.3.cmml" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx1.E12.m1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Sx1.E12.m1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.2">𝜅</ci><apply id="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.3"><times id="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.1.cmml" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.1"></times><ci id="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.2.cmml" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.2">𝑐</ci><ci id="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.3.cmml" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.3">𝑙</ci><ci id="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.4.cmml" xref="Sx1.E12.m1.1.1.1.1.1.1.1.3.3.4">𝑠</ci></apply></apply></apply><cn type="integer" id="Sx1.E12.m1.1.1.1.1.3.cmml" xref="Sx1.E12.m1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E12.m1.2c">\mathrm{OLS}=\exp\left\{\frac{-d^{2}}{2\left(s\kappa_{cls}\right)^{2}}\right\}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.12.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T4.3.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.12.11.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.12.11.3.1.1.1" class="ltx_text" style="font-size:70%;">             
</span>
<span id="Sx1.T4.3.12.11.3.1.1.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:238.5pt;">
<span id="Sx1.T4.3.12.11.3.1.1.2.1" class="ltx_p"><span id="Sx1.T4.3.12.11.3.1.1.2.1.1" class="ltx_text" style="font-size:70%;">OLS describes the correlation between two detections related to distance, classes and scale information.</span></span>
<span id="Sx1.I6" class="ltx_itemize">
<span id="Sx1.I6.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I6.i1.p1" class="ltx_para">
<span id="Sx1.I6.i1.p1.1" class="ltx_p"><math id="Sx1.I6.i1.p1.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="Sx1.I6.i1.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I6.i1.p1.1.m1.1.1" xref="Sx1.I6.i1.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="Sx1.I6.i1.p1.1.m1.1b"><ci id="Sx1.I6.i1.p1.1.m1.1.1.cmml" xref="Sx1.I6.i1.p1.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I6.i1.p1.1.m1.1c">d</annotation></semantics></math><span id="Sx1.I6.i1.p1.1.1" class="ltx_text" style="font-size:70%;">: distance between two points in an RA tensor</span></span>
</span></span>
<span id="Sx1.I6.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I6.i2.p1" class="ltx_para">
<span id="Sx1.I6.i2.p1.1" class="ltx_p"><math id="Sx1.I6.i2.p1.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="Sx1.I6.i2.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I6.i2.p1.1.m1.1.1" xref="Sx1.I6.i2.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Sx1.I6.i2.p1.1.m1.1b"><ci id="Sx1.I6.i2.p1.1.m1.1.1.cmml" xref="Sx1.I6.i2.p1.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I6.i2.p1.1.m1.1c">s</annotation></semantics></math><span id="Sx1.I6.i2.p1.1.1" class="ltx_text" style="font-size:70%;">: the distance between the object and the radar sensor, indicating object scale information</span></span>
</span></span>
<span id="Sx1.I6.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I6.i3.p1" class="ltx_para">
<span id="Sx1.I6.i3.p1.2" class="ltx_p"><math id="Sx1.I6.i3.p1.1.m1.1" class="ltx_Math" alttext="\kappa_{cls}" display="inline"><semantics id="Sx1.I6.i3.p1.1.m1.1a"><msub id="Sx1.I6.i3.p1.1.m1.1.1" xref="Sx1.I6.i3.p1.1.m1.1.1.cmml"><mi mathsize="70%" id="Sx1.I6.i3.p1.1.m1.1.1.2" xref="Sx1.I6.i3.p1.1.m1.1.1.2.cmml">κ</mi><mrow id="Sx1.I6.i3.p1.1.m1.1.1.3" xref="Sx1.I6.i3.p1.1.m1.1.1.3.cmml"><mi mathsize="70%" id="Sx1.I6.i3.p1.1.m1.1.1.3.2" xref="Sx1.I6.i3.p1.1.m1.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx1.I6.i3.p1.1.m1.1.1.3.1" xref="Sx1.I6.i3.p1.1.m1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="Sx1.I6.i3.p1.1.m1.1.1.3.3" xref="Sx1.I6.i3.p1.1.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="Sx1.I6.i3.p1.1.m1.1.1.3.1a" xref="Sx1.I6.i3.p1.1.m1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="Sx1.I6.i3.p1.1.m1.1.1.3.4" xref="Sx1.I6.i3.p1.1.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx1.I6.i3.p1.1.m1.1b"><apply id="Sx1.I6.i3.p1.1.m1.1.1.cmml" xref="Sx1.I6.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx1.I6.i3.p1.1.m1.1.1.1.cmml" xref="Sx1.I6.i3.p1.1.m1.1.1">subscript</csymbol><ci id="Sx1.I6.i3.p1.1.m1.1.1.2.cmml" xref="Sx1.I6.i3.p1.1.m1.1.1.2">𝜅</ci><apply id="Sx1.I6.i3.p1.1.m1.1.1.3.cmml" xref="Sx1.I6.i3.p1.1.m1.1.1.3"><times id="Sx1.I6.i3.p1.1.m1.1.1.3.1.cmml" xref="Sx1.I6.i3.p1.1.m1.1.1.3.1"></times><ci id="Sx1.I6.i3.p1.1.m1.1.1.3.2.cmml" xref="Sx1.I6.i3.p1.1.m1.1.1.3.2">𝑐</ci><ci id="Sx1.I6.i3.p1.1.m1.1.1.3.3.cmml" xref="Sx1.I6.i3.p1.1.m1.1.1.3.3">𝑙</ci><ci id="Sx1.I6.i3.p1.1.m1.1.1.3.4.cmml" xref="Sx1.I6.i3.p1.1.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I6.i3.p1.1.m1.1c">\kappa_{cls}</annotation></semantics></math><span id="Sx1.I6.i3.p1.2.1" class="ltx_text" style="font-size:70%;">: a constant value that donates the error tolerance for each class </span><math id="Sx1.I6.i3.p1.2.m2.1" class="ltx_Math" alttext="cls" display="inline"><semantics id="Sx1.I6.i3.p1.2.m2.1a"><mrow id="Sx1.I6.i3.p1.2.m2.1.1" xref="Sx1.I6.i3.p1.2.m2.1.1.cmml"><mi mathsize="70%" id="Sx1.I6.i3.p1.2.m2.1.1.2" xref="Sx1.I6.i3.p1.2.m2.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx1.I6.i3.p1.2.m2.1.1.1" xref="Sx1.I6.i3.p1.2.m2.1.1.1.cmml">​</mo><mi mathsize="70%" id="Sx1.I6.i3.p1.2.m2.1.1.3" xref="Sx1.I6.i3.p1.2.m2.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="Sx1.I6.i3.p1.2.m2.1.1.1a" xref="Sx1.I6.i3.p1.2.m2.1.1.1.cmml">​</mo><mi mathsize="70%" id="Sx1.I6.i3.p1.2.m2.1.1.4" xref="Sx1.I6.i3.p1.2.m2.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx1.I6.i3.p1.2.m2.1b"><apply id="Sx1.I6.i3.p1.2.m2.1.1.cmml" xref="Sx1.I6.i3.p1.2.m2.1.1"><times id="Sx1.I6.i3.p1.2.m2.1.1.1.cmml" xref="Sx1.I6.i3.p1.2.m2.1.1.1"></times><ci id="Sx1.I6.i3.p1.2.m2.1.1.2.cmml" xref="Sx1.I6.i3.p1.2.m2.1.1.2">𝑐</ci><ci id="Sx1.I6.i3.p1.2.m2.1.1.3.cmml" xref="Sx1.I6.i3.p1.2.m2.1.1.3">𝑙</ci><ci id="Sx1.I6.i3.p1.2.m2.1.1.4.cmml" xref="Sx1.I6.i3.p1.2.m2.1.1.4">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I6.i3.p1.2.m2.1c">cls</annotation></semantics></math><span id="Sx1.I6.i3.p1.2.2" class="ltx_text" style="font-size:70%;">, which can be calculated based on the average object size of that class</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T4.3.13.12" class="ltx_tr">
<td id="Sx1.T4.3.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T4.3.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.13.12.1.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T4.3.13.12.1.1.1.1" class="ltx_text" style="font-size:70%;">
Average Heading Similarity (AHS) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T4.3.13.12.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib163" title="" class="ltx_ref">163</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="Sx1.T4.3.13.12.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T4.3.13.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T4.3.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.13.12.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="Sx1.E13" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E13.m1.5" class="ltx_Math" alttext="\mathrm{AHS}=\frac{1}{11}\sum_{r\in\{0,0.1,\ldots,1\}}\max_{\tilde{r}:\tilde{r}\geq r}\mathrm{s}(\tilde{r})" display="block"><semantics id="Sx1.E13.m1.5a"><mrow id="Sx1.E13.m1.5.6" xref="Sx1.E13.m1.5.6.cmml"><mi mathsize="70%" id="Sx1.E13.m1.5.6.2" xref="Sx1.E13.m1.5.6.2.cmml">AHS</mi><mo mathsize="70%" id="Sx1.E13.m1.5.6.1" xref="Sx1.E13.m1.5.6.1.cmml">=</mo><mrow id="Sx1.E13.m1.5.6.3" xref="Sx1.E13.m1.5.6.3.cmml"><mfrac id="Sx1.E13.m1.5.6.3.2" xref="Sx1.E13.m1.5.6.3.2.cmml"><mn mathsize="70%" id="Sx1.E13.m1.5.6.3.2.2" xref="Sx1.E13.m1.5.6.3.2.2.cmml">1</mn><mn mathsize="70%" id="Sx1.E13.m1.5.6.3.2.3" xref="Sx1.E13.m1.5.6.3.2.3.cmml">11</mn></mfrac><mo lspace="0em" rspace="0em" id="Sx1.E13.m1.5.6.3.1" xref="Sx1.E13.m1.5.6.3.1.cmml">​</mo><mrow id="Sx1.E13.m1.5.6.3.3" xref="Sx1.E13.m1.5.6.3.3.cmml"><munder id="Sx1.E13.m1.5.6.3.3.1" xref="Sx1.E13.m1.5.6.3.3.1.cmml"><mo maxsize="70%" minsize="70%" movablelimits="false" stretchy="true" id="Sx1.E13.m1.5.6.3.3.1.2" xref="Sx1.E13.m1.5.6.3.3.1.2.cmml">∑</mo><mrow id="Sx1.E13.m1.4.4.4" xref="Sx1.E13.m1.4.4.4.cmml"><mi mathsize="70%" id="Sx1.E13.m1.4.4.4.6" xref="Sx1.E13.m1.4.4.4.6.cmml">r</mi><mo mathsize="70%" id="Sx1.E13.m1.4.4.4.5" xref="Sx1.E13.m1.4.4.4.5.cmml">∈</mo><mrow id="Sx1.E13.m1.4.4.4.7.2" xref="Sx1.E13.m1.4.4.4.7.1.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E13.m1.4.4.4.7.2.1" xref="Sx1.E13.m1.4.4.4.7.1.cmml">{</mo><mn mathsize="70%" id="Sx1.E13.m1.1.1.1.1" xref="Sx1.E13.m1.1.1.1.1.cmml">0</mn><mo mathsize="70%" id="Sx1.E13.m1.4.4.4.7.2.2" xref="Sx1.E13.m1.4.4.4.7.1.cmml">,</mo><mn mathsize="70%" id="Sx1.E13.m1.2.2.2.2" xref="Sx1.E13.m1.2.2.2.2.cmml">0.1</mn><mo mathsize="70%" id="Sx1.E13.m1.4.4.4.7.2.3" xref="Sx1.E13.m1.4.4.4.7.1.cmml">,</mo><mi mathsize="70%" mathvariant="normal" id="Sx1.E13.m1.3.3.3.3" xref="Sx1.E13.m1.3.3.3.3.cmml">…</mi><mo mathsize="70%" id="Sx1.E13.m1.4.4.4.7.2.4" xref="Sx1.E13.m1.4.4.4.7.1.cmml">,</mo><mn mathsize="70%" id="Sx1.E13.m1.4.4.4.4" xref="Sx1.E13.m1.4.4.4.4.cmml">1</mn><mo maxsize="70%" minsize="70%" id="Sx1.E13.m1.4.4.4.7.2.5" xref="Sx1.E13.m1.4.4.4.7.1.cmml">}</mo></mrow></mrow></munder><mrow id="Sx1.E13.m1.5.6.3.3.2" xref="Sx1.E13.m1.5.6.3.3.2.cmml"><mrow id="Sx1.E13.m1.5.6.3.3.2.2" xref="Sx1.E13.m1.5.6.3.3.2.2.cmml"><munder id="Sx1.E13.m1.5.6.3.3.2.2.1" xref="Sx1.E13.m1.5.6.3.3.2.2.1.cmml"><mi mathsize="70%" id="Sx1.E13.m1.5.6.3.3.2.2.1.2" xref="Sx1.E13.m1.5.6.3.3.2.2.1.2.cmml">max</mi><mrow id="Sx1.E13.m1.5.6.3.3.2.2.1.3" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.cmml"><mover accent="true" id="Sx1.E13.m1.5.6.3.3.2.2.1.3.2" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.2.cmml"><mi mathsize="70%" id="Sx1.E13.m1.5.6.3.3.2.2.1.3.2.2" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.2.2.cmml">r</mi><mo mathsize="70%" id="Sx1.E13.m1.5.6.3.3.2.2.1.3.2.1" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.2.1.cmml">~</mo></mover><mo lspace="0.278em" mathsize="70%" rspace="0.278em" id="Sx1.E13.m1.5.6.3.3.2.2.1.3.1" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.1.cmml">:</mo><mrow id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.cmml"><mover accent="true" id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2.cmml"><mi mathsize="70%" id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2.2" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2.2.cmml">r</mi><mo mathsize="70%" id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2.1" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2.1.cmml">~</mo></mover><mo mathsize="70%" id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.1" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.1.cmml">≥</mo><mi mathsize="70%" id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.3" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.3.cmml">r</mi></mrow></mrow></munder><mo lspace="0.167em" id="Sx1.E13.m1.5.6.3.3.2.2a" xref="Sx1.E13.m1.5.6.3.3.2.2.cmml">⁡</mo><mi mathsize="70%" mathvariant="normal" id="Sx1.E13.m1.5.6.3.3.2.2.2" xref="Sx1.E13.m1.5.6.3.3.2.2.2.cmml">s</mi></mrow><mo lspace="0em" rspace="0em" id="Sx1.E13.m1.5.6.3.3.2.1" xref="Sx1.E13.m1.5.6.3.3.2.1.cmml">​</mo><mrow id="Sx1.E13.m1.5.6.3.3.2.3.2" xref="Sx1.E13.m1.5.5.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E13.m1.5.6.3.3.2.3.2.1" xref="Sx1.E13.m1.5.5.cmml">(</mo><mover accent="true" id="Sx1.E13.m1.5.5" xref="Sx1.E13.m1.5.5.cmml"><mi mathsize="70%" id="Sx1.E13.m1.5.5.2" xref="Sx1.E13.m1.5.5.2.cmml">r</mi><mo mathsize="70%" id="Sx1.E13.m1.5.5.1" xref="Sx1.E13.m1.5.5.1.cmml">~</mo></mover><mo maxsize="70%" minsize="70%" id="Sx1.E13.m1.5.6.3.3.2.3.2.2" xref="Sx1.E13.m1.5.5.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E13.m1.5b"><apply id="Sx1.E13.m1.5.6.cmml" xref="Sx1.E13.m1.5.6"><eq id="Sx1.E13.m1.5.6.1.cmml" xref="Sx1.E13.m1.5.6.1"></eq><ci id="Sx1.E13.m1.5.6.2.cmml" xref="Sx1.E13.m1.5.6.2">AHS</ci><apply id="Sx1.E13.m1.5.6.3.cmml" xref="Sx1.E13.m1.5.6.3"><times id="Sx1.E13.m1.5.6.3.1.cmml" xref="Sx1.E13.m1.5.6.3.1"></times><apply id="Sx1.E13.m1.5.6.3.2.cmml" xref="Sx1.E13.m1.5.6.3.2"><divide id="Sx1.E13.m1.5.6.3.2.1.cmml" xref="Sx1.E13.m1.5.6.3.2"></divide><cn type="integer" id="Sx1.E13.m1.5.6.3.2.2.cmml" xref="Sx1.E13.m1.5.6.3.2.2">1</cn><cn type="integer" id="Sx1.E13.m1.5.6.3.2.3.cmml" xref="Sx1.E13.m1.5.6.3.2.3">11</cn></apply><apply id="Sx1.E13.m1.5.6.3.3.cmml" xref="Sx1.E13.m1.5.6.3.3"><apply id="Sx1.E13.m1.5.6.3.3.1.cmml" xref="Sx1.E13.m1.5.6.3.3.1"><csymbol cd="ambiguous" id="Sx1.E13.m1.5.6.3.3.1.1.cmml" xref="Sx1.E13.m1.5.6.3.3.1">subscript</csymbol><sum id="Sx1.E13.m1.5.6.3.3.1.2.cmml" xref="Sx1.E13.m1.5.6.3.3.1.2"></sum><apply id="Sx1.E13.m1.4.4.4.cmml" xref="Sx1.E13.m1.4.4.4"><in id="Sx1.E13.m1.4.4.4.5.cmml" xref="Sx1.E13.m1.4.4.4.5"></in><ci id="Sx1.E13.m1.4.4.4.6.cmml" xref="Sx1.E13.m1.4.4.4.6">𝑟</ci><set id="Sx1.E13.m1.4.4.4.7.1.cmml" xref="Sx1.E13.m1.4.4.4.7.2"><cn type="integer" id="Sx1.E13.m1.1.1.1.1.cmml" xref="Sx1.E13.m1.1.1.1.1">0</cn><cn type="float" id="Sx1.E13.m1.2.2.2.2.cmml" xref="Sx1.E13.m1.2.2.2.2">0.1</cn><ci id="Sx1.E13.m1.3.3.3.3.cmml" xref="Sx1.E13.m1.3.3.3.3">…</ci><cn type="integer" id="Sx1.E13.m1.4.4.4.4.cmml" xref="Sx1.E13.m1.4.4.4.4">1</cn></set></apply></apply><apply id="Sx1.E13.m1.5.6.3.3.2.cmml" xref="Sx1.E13.m1.5.6.3.3.2"><times id="Sx1.E13.m1.5.6.3.3.2.1.cmml" xref="Sx1.E13.m1.5.6.3.3.2.1"></times><apply id="Sx1.E13.m1.5.6.3.3.2.2.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2"><apply id="Sx1.E13.m1.5.6.3.3.2.2.1.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1"><csymbol cd="ambiguous" id="Sx1.E13.m1.5.6.3.3.2.2.1.1.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1">subscript</csymbol><max id="Sx1.E13.m1.5.6.3.3.2.2.1.2.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.2"></max><apply id="Sx1.E13.m1.5.6.3.3.2.2.1.3.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3"><ci id="Sx1.E13.m1.5.6.3.3.2.2.1.3.1.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.1">:</ci><apply id="Sx1.E13.m1.5.6.3.3.2.2.1.3.2.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.2"><ci id="Sx1.E13.m1.5.6.3.3.2.2.1.3.2.1.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.2.1">~</ci><ci id="Sx1.E13.m1.5.6.3.3.2.2.1.3.2.2.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.2.2">𝑟</ci></apply><apply id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3"><geq id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.1.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.1"></geq><apply id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2"><ci id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2.1.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2.1">~</ci><ci id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2.2.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.2.2">𝑟</ci></apply><ci id="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.3.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.1.3.3.3">𝑟</ci></apply></apply></apply><ci id="Sx1.E13.m1.5.6.3.3.2.2.2.cmml" xref="Sx1.E13.m1.5.6.3.3.2.2.2">s</ci></apply><apply id="Sx1.E13.m1.5.5.cmml" xref="Sx1.E13.m1.5.6.3.3.2.3.2"><ci id="Sx1.E13.m1.5.5.1.cmml" xref="Sx1.E13.m1.5.5.1">~</ci><ci id="Sx1.E13.m1.5.5.2.cmml" xref="Sx1.E13.m1.5.5.2">𝑟</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E13.m1.5c">\mathrm{AHS}=\frac{1}{11}\sum_{r\in\{0,0.1,\ldots,1\}}\max_{\tilde{r}:\tilde{r}\geq r}\mathrm{s}(\tilde{r})</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></span></span></span>
</span><span id="Sx1.T4.3.13.12.2.1.1.1" class="ltx_text" style="font-size:70%;">
</span>
<span id="Sx1.E14" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="Sx1.E14.m1.5" class="ltx_Math" alttext="\mathrm{s}(r)=\frac{1}{|\mathcal{D}(r)|}\sum_{i\in\mathcal{D}(r)}\frac{1+\cos\Delta_{\theta}^{(i)}}{2}\delta_{i}" display="block"><semantics id="Sx1.E14.m1.5a"><mrow id="Sx1.E14.m1.5.6" xref="Sx1.E14.m1.5.6.cmml"><mrow id="Sx1.E14.m1.5.6.2" xref="Sx1.E14.m1.5.6.2.cmml"><mi mathsize="70%" mathvariant="normal" id="Sx1.E14.m1.5.6.2.2" xref="Sx1.E14.m1.5.6.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="Sx1.E14.m1.5.6.2.1" xref="Sx1.E14.m1.5.6.2.1.cmml">​</mo><mrow id="Sx1.E14.m1.5.6.2.3.2" xref="Sx1.E14.m1.5.6.2.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E14.m1.5.6.2.3.2.1" xref="Sx1.E14.m1.5.6.2.cmml">(</mo><mi mathsize="70%" id="Sx1.E14.m1.5.5" xref="Sx1.E14.m1.5.5.cmml">r</mi><mo maxsize="70%" minsize="70%" id="Sx1.E14.m1.5.6.2.3.2.2" xref="Sx1.E14.m1.5.6.2.cmml">)</mo></mrow></mrow><mo mathsize="70%" id="Sx1.E14.m1.5.6.1" xref="Sx1.E14.m1.5.6.1.cmml">=</mo><mrow id="Sx1.E14.m1.5.6.3" xref="Sx1.E14.m1.5.6.3.cmml"><mfrac id="Sx1.E14.m1.2.2" xref="Sx1.E14.m1.2.2.cmml"><mn mathsize="70%" id="Sx1.E14.m1.2.2.4" xref="Sx1.E14.m1.2.2.4.cmml">1</mn><mrow id="Sx1.E14.m1.2.2.2.2" xref="Sx1.E14.m1.2.2.2.3.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E14.m1.2.2.2.2.2" xref="Sx1.E14.m1.2.2.2.3.1.cmml">|</mo><mrow id="Sx1.E14.m1.2.2.2.2.1" xref="Sx1.E14.m1.2.2.2.2.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="Sx1.E14.m1.2.2.2.2.1.2" xref="Sx1.E14.m1.2.2.2.2.1.2.cmml">𝒟</mi><mo lspace="0em" rspace="0em" id="Sx1.E14.m1.2.2.2.2.1.1" xref="Sx1.E14.m1.2.2.2.2.1.1.cmml">​</mo><mrow id="Sx1.E14.m1.2.2.2.2.1.3.2" xref="Sx1.E14.m1.2.2.2.2.1.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E14.m1.2.2.2.2.1.3.2.1" xref="Sx1.E14.m1.2.2.2.2.1.cmml">(</mo><mi mathsize="70%" id="Sx1.E14.m1.1.1.1.1" xref="Sx1.E14.m1.1.1.1.1.cmml">r</mi><mo maxsize="70%" minsize="70%" id="Sx1.E14.m1.2.2.2.2.1.3.2.2" xref="Sx1.E14.m1.2.2.2.2.1.cmml">)</mo></mrow></mrow><mo maxsize="70%" minsize="70%" id="Sx1.E14.m1.2.2.2.2.3" xref="Sx1.E14.m1.2.2.2.3.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="Sx1.E14.m1.5.6.3.1" xref="Sx1.E14.m1.5.6.3.1.cmml">​</mo><mrow id="Sx1.E14.m1.5.6.3.2" xref="Sx1.E14.m1.5.6.3.2.cmml"><munder id="Sx1.E14.m1.5.6.3.2.1" xref="Sx1.E14.m1.5.6.3.2.1.cmml"><mo maxsize="70%" minsize="70%" movablelimits="false" stretchy="true" id="Sx1.E14.m1.5.6.3.2.1.2" xref="Sx1.E14.m1.5.6.3.2.1.2.cmml">∑</mo><mrow id="Sx1.E14.m1.3.3.1" xref="Sx1.E14.m1.3.3.1.cmml"><mi mathsize="70%" id="Sx1.E14.m1.3.3.1.3" xref="Sx1.E14.m1.3.3.1.3.cmml">i</mi><mo mathsize="70%" id="Sx1.E14.m1.3.3.1.2" xref="Sx1.E14.m1.3.3.1.2.cmml">∈</mo><mrow id="Sx1.E14.m1.3.3.1.4" xref="Sx1.E14.m1.3.3.1.4.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="Sx1.E14.m1.3.3.1.4.2" xref="Sx1.E14.m1.3.3.1.4.2.cmml">𝒟</mi><mo lspace="0em" rspace="0em" id="Sx1.E14.m1.3.3.1.4.1" xref="Sx1.E14.m1.3.3.1.4.1.cmml">​</mo><mrow id="Sx1.E14.m1.3.3.1.4.3.2" xref="Sx1.E14.m1.3.3.1.4.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E14.m1.3.3.1.4.3.2.1" xref="Sx1.E14.m1.3.3.1.4.cmml">(</mo><mi mathsize="70%" id="Sx1.E14.m1.3.3.1.1" xref="Sx1.E14.m1.3.3.1.1.cmml">r</mi><mo maxsize="70%" minsize="70%" id="Sx1.E14.m1.3.3.1.4.3.2.2" xref="Sx1.E14.m1.3.3.1.4.cmml">)</mo></mrow></mrow></mrow></munder><mrow id="Sx1.E14.m1.5.6.3.2.2" xref="Sx1.E14.m1.5.6.3.2.2.cmml"><mfrac id="Sx1.E14.m1.4.4" xref="Sx1.E14.m1.4.4.cmml"><mrow id="Sx1.E14.m1.4.4.1" xref="Sx1.E14.m1.4.4.1.cmml"><mn mathsize="70%" id="Sx1.E14.m1.4.4.1.3" xref="Sx1.E14.m1.4.4.1.3.cmml">1</mn><mo mathsize="70%" id="Sx1.E14.m1.4.4.1.2" xref="Sx1.E14.m1.4.4.1.2.cmml">+</mo><mrow id="Sx1.E14.m1.4.4.1.4" xref="Sx1.E14.m1.4.4.1.4.cmml"><mi mathsize="70%" id="Sx1.E14.m1.4.4.1.4.1" xref="Sx1.E14.m1.4.4.1.4.1.cmml">cos</mi><mo lspace="0.167em" id="Sx1.E14.m1.4.4.1.4a" xref="Sx1.E14.m1.4.4.1.4.cmml">⁡</mo><msubsup id="Sx1.E14.m1.4.4.1.4.2" xref="Sx1.E14.m1.4.4.1.4.2.cmml"><mi mathsize="70%" mathvariant="normal" id="Sx1.E14.m1.4.4.1.4.2.2.2" xref="Sx1.E14.m1.4.4.1.4.2.2.2.cmml">Δ</mi><mi mathsize="70%" id="Sx1.E14.m1.4.4.1.4.2.2.3" xref="Sx1.E14.m1.4.4.1.4.2.2.3.cmml">θ</mi><mrow id="Sx1.E14.m1.4.4.1.1.1.3" xref="Sx1.E14.m1.4.4.1.4.2.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.E14.m1.4.4.1.1.1.3.1" xref="Sx1.E14.m1.4.4.1.4.2.cmml">(</mo><mi mathsize="70%" id="Sx1.E14.m1.4.4.1.1.1.1" xref="Sx1.E14.m1.4.4.1.1.1.1.cmml">i</mi><mo maxsize="70%" minsize="70%" id="Sx1.E14.m1.4.4.1.1.1.3.2" xref="Sx1.E14.m1.4.4.1.4.2.cmml">)</mo></mrow></msubsup></mrow></mrow><mn mathsize="70%" id="Sx1.E14.m1.4.4.3" xref="Sx1.E14.m1.4.4.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="Sx1.E14.m1.5.6.3.2.2.1" xref="Sx1.E14.m1.5.6.3.2.2.1.cmml">​</mo><msub id="Sx1.E14.m1.5.6.3.2.2.2" xref="Sx1.E14.m1.5.6.3.2.2.2.cmml"><mi mathsize="70%" id="Sx1.E14.m1.5.6.3.2.2.2.2" xref="Sx1.E14.m1.5.6.3.2.2.2.2.cmml">δ</mi><mi mathsize="70%" id="Sx1.E14.m1.5.6.3.2.2.2.3" xref="Sx1.E14.m1.5.6.3.2.2.2.3.cmml">i</mi></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx1.E14.m1.5b"><apply id="Sx1.E14.m1.5.6.cmml" xref="Sx1.E14.m1.5.6"><eq id="Sx1.E14.m1.5.6.1.cmml" xref="Sx1.E14.m1.5.6.1"></eq><apply id="Sx1.E14.m1.5.6.2.cmml" xref="Sx1.E14.m1.5.6.2"><times id="Sx1.E14.m1.5.6.2.1.cmml" xref="Sx1.E14.m1.5.6.2.1"></times><ci id="Sx1.E14.m1.5.6.2.2.cmml" xref="Sx1.E14.m1.5.6.2.2">s</ci><ci id="Sx1.E14.m1.5.5.cmml" xref="Sx1.E14.m1.5.5">𝑟</ci></apply><apply id="Sx1.E14.m1.5.6.3.cmml" xref="Sx1.E14.m1.5.6.3"><times id="Sx1.E14.m1.5.6.3.1.cmml" xref="Sx1.E14.m1.5.6.3.1"></times><apply id="Sx1.E14.m1.2.2.cmml" xref="Sx1.E14.m1.2.2"><divide id="Sx1.E14.m1.2.2.3.cmml" xref="Sx1.E14.m1.2.2"></divide><cn type="integer" id="Sx1.E14.m1.2.2.4.cmml" xref="Sx1.E14.m1.2.2.4">1</cn><apply id="Sx1.E14.m1.2.2.2.3.cmml" xref="Sx1.E14.m1.2.2.2.2"><abs id="Sx1.E14.m1.2.2.2.3.1.cmml" xref="Sx1.E14.m1.2.2.2.2.2"></abs><apply id="Sx1.E14.m1.2.2.2.2.1.cmml" xref="Sx1.E14.m1.2.2.2.2.1"><times id="Sx1.E14.m1.2.2.2.2.1.1.cmml" xref="Sx1.E14.m1.2.2.2.2.1.1"></times><ci id="Sx1.E14.m1.2.2.2.2.1.2.cmml" xref="Sx1.E14.m1.2.2.2.2.1.2">𝒟</ci><ci id="Sx1.E14.m1.1.1.1.1.cmml" xref="Sx1.E14.m1.1.1.1.1">𝑟</ci></apply></apply></apply><apply id="Sx1.E14.m1.5.6.3.2.cmml" xref="Sx1.E14.m1.5.6.3.2"><apply id="Sx1.E14.m1.5.6.3.2.1.cmml" xref="Sx1.E14.m1.5.6.3.2.1"><csymbol cd="ambiguous" id="Sx1.E14.m1.5.6.3.2.1.1.cmml" xref="Sx1.E14.m1.5.6.3.2.1">subscript</csymbol><sum id="Sx1.E14.m1.5.6.3.2.1.2.cmml" xref="Sx1.E14.m1.5.6.3.2.1.2"></sum><apply id="Sx1.E14.m1.3.3.1.cmml" xref="Sx1.E14.m1.3.3.1"><in id="Sx1.E14.m1.3.3.1.2.cmml" xref="Sx1.E14.m1.3.3.1.2"></in><ci id="Sx1.E14.m1.3.3.1.3.cmml" xref="Sx1.E14.m1.3.3.1.3">𝑖</ci><apply id="Sx1.E14.m1.3.3.1.4.cmml" xref="Sx1.E14.m1.3.3.1.4"><times id="Sx1.E14.m1.3.3.1.4.1.cmml" xref="Sx1.E14.m1.3.3.1.4.1"></times><ci id="Sx1.E14.m1.3.3.1.4.2.cmml" xref="Sx1.E14.m1.3.3.1.4.2">𝒟</ci><ci id="Sx1.E14.m1.3.3.1.1.cmml" xref="Sx1.E14.m1.3.3.1.1">𝑟</ci></apply></apply></apply><apply id="Sx1.E14.m1.5.6.3.2.2.cmml" xref="Sx1.E14.m1.5.6.3.2.2"><times id="Sx1.E14.m1.5.6.3.2.2.1.cmml" xref="Sx1.E14.m1.5.6.3.2.2.1"></times><apply id="Sx1.E14.m1.4.4.cmml" xref="Sx1.E14.m1.4.4"><divide id="Sx1.E14.m1.4.4.2.cmml" xref="Sx1.E14.m1.4.4"></divide><apply id="Sx1.E14.m1.4.4.1.cmml" xref="Sx1.E14.m1.4.4.1"><plus id="Sx1.E14.m1.4.4.1.2.cmml" xref="Sx1.E14.m1.4.4.1.2"></plus><cn type="integer" id="Sx1.E14.m1.4.4.1.3.cmml" xref="Sx1.E14.m1.4.4.1.3">1</cn><apply id="Sx1.E14.m1.4.4.1.4.cmml" xref="Sx1.E14.m1.4.4.1.4"><cos id="Sx1.E14.m1.4.4.1.4.1.cmml" xref="Sx1.E14.m1.4.4.1.4.1"></cos><apply id="Sx1.E14.m1.4.4.1.4.2.cmml" xref="Sx1.E14.m1.4.4.1.4.2"><csymbol cd="ambiguous" id="Sx1.E14.m1.4.4.1.4.2.1.cmml" xref="Sx1.E14.m1.4.4.1.4.2">superscript</csymbol><apply id="Sx1.E14.m1.4.4.1.4.2.2.cmml" xref="Sx1.E14.m1.4.4.1.4.2"><csymbol cd="ambiguous" id="Sx1.E14.m1.4.4.1.4.2.2.1.cmml" xref="Sx1.E14.m1.4.4.1.4.2">subscript</csymbol><ci id="Sx1.E14.m1.4.4.1.4.2.2.2.cmml" xref="Sx1.E14.m1.4.4.1.4.2.2.2">Δ</ci><ci id="Sx1.E14.m1.4.4.1.4.2.2.3.cmml" xref="Sx1.E14.m1.4.4.1.4.2.2.3">𝜃</ci></apply><ci id="Sx1.E14.m1.4.4.1.1.1.1.cmml" xref="Sx1.E14.m1.4.4.1.1.1.1">𝑖</ci></apply></apply></apply><cn type="integer" id="Sx1.E14.m1.4.4.3.cmml" xref="Sx1.E14.m1.4.4.3">2</cn></apply><apply id="Sx1.E14.m1.5.6.3.2.2.2.cmml" xref="Sx1.E14.m1.5.6.3.2.2.2"><csymbol cd="ambiguous" id="Sx1.E14.m1.5.6.3.2.2.2.1.cmml" xref="Sx1.E14.m1.5.6.3.2.2.2">subscript</csymbol><ci id="Sx1.E14.m1.5.6.3.2.2.2.2.cmml" xref="Sx1.E14.m1.5.6.3.2.2.2.2">𝛿</ci><ci id="Sx1.E14.m1.5.6.3.2.2.2.3.cmml" xref="Sx1.E14.m1.5.6.3.2.2.2.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.E14.m1.5c">\mathrm{s}(r)=\frac{1}{|\mathcal{D}(r)|}\sum_{i\in\mathcal{D}(r)}\frac{1+\cos\Delta_{\theta}^{(i)}}{2}\delta_{i}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></span></span></span>
</span></span>
</span>
</td>
<td id="Sx1.T4.3.13.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T4.3.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T4.3.13.12.3.1.1" class="ltx_p" style="width:313.0pt;"><span id="Sx1.T4.3.13.12.3.1.1.1" class="ltx_text" style="font-size:70%;">             
</span>
<span id="Sx1.T4.3.13.12.3.1.1.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:238.5pt;">
<span id="Sx1.T4.3.13.12.3.1.1.2.1" class="ltx_p"><span id="Sx1.T4.3.13.12.3.1.1.2.1.1" class="ltx_text" style="font-size:70%;">AHS is the average orientation accuracy in 3D IOU and global orientation angle.</span></span>
<span id="Sx1.I7" class="ltx_itemize">
<span id="Sx1.I7.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I7.i1.p1" class="ltx_para">
<span id="Sx1.I7.i1.p1.1" class="ltx_p"><math id="Sx1.I7.i1.p1.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="Sx1.I7.i1.p1.1.m1.1a"><mi mathsize="70%" id="Sx1.I7.i1.p1.1.m1.1.1" xref="Sx1.I7.i1.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="Sx1.I7.i1.p1.1.m1.1b"><ci id="Sx1.I7.i1.p1.1.m1.1.1.cmml" xref="Sx1.I7.i1.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I7.i1.p1.1.m1.1c">r</annotation></semantics></math><span id="Sx1.I7.i1.p1.1.1" class="ltx_text" style="font-size:70%;">: recall value</span></span>
</span></span>
<span id="Sx1.I7.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I7.i2.p1" class="ltx_para">
<span id="Sx1.I7.i2.p1.2" class="ltx_p"><math id="Sx1.I7.i2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}(r)" display="inline"><semantics id="Sx1.I7.i2.p1.1.m1.1a"><mrow id="Sx1.I7.i2.p1.1.m1.1.2" xref="Sx1.I7.i2.p1.1.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="Sx1.I7.i2.p1.1.m1.1.2.2" xref="Sx1.I7.i2.p1.1.m1.1.2.2.cmml">𝒟</mi><mo lspace="0em" rspace="0em" id="Sx1.I7.i2.p1.1.m1.1.2.1" xref="Sx1.I7.i2.p1.1.m1.1.2.1.cmml">​</mo><mrow id="Sx1.I7.i2.p1.1.m1.1.2.3.2" xref="Sx1.I7.i2.p1.1.m1.1.2.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.I7.i2.p1.1.m1.1.2.3.2.1" xref="Sx1.I7.i2.p1.1.m1.1.2.cmml">(</mo><mi mathsize="70%" id="Sx1.I7.i2.p1.1.m1.1.1" xref="Sx1.I7.i2.p1.1.m1.1.1.cmml">r</mi><mo maxsize="70%" minsize="70%" id="Sx1.I7.i2.p1.1.m1.1.2.3.2.2" xref="Sx1.I7.i2.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx1.I7.i2.p1.1.m1.1b"><apply id="Sx1.I7.i2.p1.1.m1.1.2.cmml" xref="Sx1.I7.i2.p1.1.m1.1.2"><times id="Sx1.I7.i2.p1.1.m1.1.2.1.cmml" xref="Sx1.I7.i2.p1.1.m1.1.2.1"></times><ci id="Sx1.I7.i2.p1.1.m1.1.2.2.cmml" xref="Sx1.I7.i2.p1.1.m1.1.2.2">𝒟</ci><ci id="Sx1.I7.i2.p1.1.m1.1.1.cmml" xref="Sx1.I7.i2.p1.1.m1.1.1">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I7.i2.p1.1.m1.1c">\mathcal{D}(r)</annotation></semantics></math><span id="Sx1.I7.i2.p1.2.1" class="ltx_text" style="font-size:70%;">: all object detections at recall rate </span><math id="Sx1.I7.i2.p1.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="Sx1.I7.i2.p1.2.m2.1a"><mi mathsize="70%" id="Sx1.I7.i2.p1.2.m2.1.1" xref="Sx1.I7.i2.p1.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="Sx1.I7.i2.p1.2.m2.1b"><ci id="Sx1.I7.i2.p1.2.m2.1.1.cmml" xref="Sx1.I7.i2.p1.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I7.i2.p1.2.m2.1c">r</annotation></semantics></math><span id="Sx1.I7.i2.p1.2.2" class="ltx_text" style="font-size:70%;"></span></span>
</span></span>
<span id="Sx1.I7.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I7.i3.p1" class="ltx_para">
<span id="Sx1.I7.i3.p1.2" class="ltx_p"><math id="Sx1.I7.i3.p1.1.m1.1" class="ltx_Math" alttext="\Delta_{\theta}^{(i)}" display="inline"><semantics id="Sx1.I7.i3.p1.1.m1.1a"><msubsup id="Sx1.I7.i3.p1.1.m1.1.2" xref="Sx1.I7.i3.p1.1.m1.1.2.cmml"><mi mathsize="70%" mathvariant="normal" id="Sx1.I7.i3.p1.1.m1.1.2.2.2" xref="Sx1.I7.i3.p1.1.m1.1.2.2.2.cmml">Δ</mi><mi mathsize="70%" id="Sx1.I7.i3.p1.1.m1.1.2.2.3" xref="Sx1.I7.i3.p1.1.m1.1.2.2.3.cmml">θ</mi><mrow id="Sx1.I7.i3.p1.1.m1.1.1.1.3" xref="Sx1.I7.i3.p1.1.m1.1.2.cmml"><mo maxsize="70%" minsize="70%" id="Sx1.I7.i3.p1.1.m1.1.1.1.3.1" xref="Sx1.I7.i3.p1.1.m1.1.2.cmml">(</mo><mi mathsize="70%" id="Sx1.I7.i3.p1.1.m1.1.1.1.1" xref="Sx1.I7.i3.p1.1.m1.1.1.1.1.cmml">i</mi><mo maxsize="70%" minsize="70%" id="Sx1.I7.i3.p1.1.m1.1.1.1.3.2" xref="Sx1.I7.i3.p1.1.m1.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="Sx1.I7.i3.p1.1.m1.1b"><apply id="Sx1.I7.i3.p1.1.m1.1.2.cmml" xref="Sx1.I7.i3.p1.1.m1.1.2"><csymbol cd="ambiguous" id="Sx1.I7.i3.p1.1.m1.1.2.1.cmml" xref="Sx1.I7.i3.p1.1.m1.1.2">superscript</csymbol><apply id="Sx1.I7.i3.p1.1.m1.1.2.2.cmml" xref="Sx1.I7.i3.p1.1.m1.1.2"><csymbol cd="ambiguous" id="Sx1.I7.i3.p1.1.m1.1.2.2.1.cmml" xref="Sx1.I7.i3.p1.1.m1.1.2">subscript</csymbol><ci id="Sx1.I7.i3.p1.1.m1.1.2.2.2.cmml" xref="Sx1.I7.i3.p1.1.m1.1.2.2.2">Δ</ci><ci id="Sx1.I7.i3.p1.1.m1.1.2.2.3.cmml" xref="Sx1.I7.i3.p1.1.m1.1.2.2.3">𝜃</ci></apply><ci id="Sx1.I7.i3.p1.1.m1.1.1.1.1.cmml" xref="Sx1.I7.i3.p1.1.m1.1.1.1.1">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I7.i3.p1.1.m1.1c">\Delta_{\theta}^{(i)}</annotation></semantics></math><span id="Sx1.I7.i3.p1.2.1" class="ltx_text" style="font-size:70%;">: difference in global orientation of detection </span><math id="Sx1.I7.i3.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="Sx1.I7.i3.p1.2.m2.1a"><mi mathsize="70%" id="Sx1.I7.i3.p1.2.m2.1.1" xref="Sx1.I7.i3.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="Sx1.I7.i3.p1.2.m2.1b"><ci id="Sx1.I7.i3.p1.2.m2.1.1.cmml" xref="Sx1.I7.i3.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I7.i3.p1.2.m2.1c">i</annotation></semantics></math><span id="Sx1.I7.i3.p1.2.2" class="ltx_text" style="font-size:70%;"> as determined by the estimated and ground truth orientation</span></span>
</span></span>
<span id="Sx1.I7.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="Sx1.I7.i4.p1" class="ltx_para">
<span id="Sx1.I7.i4.p1.2" class="ltx_p"><math id="Sx1.I7.i4.p1.1.m1.1" class="ltx_Math" alttext="\delta_{i}" display="inline"><semantics id="Sx1.I7.i4.p1.1.m1.1a"><msub id="Sx1.I7.i4.p1.1.m1.1.1" xref="Sx1.I7.i4.p1.1.m1.1.1.cmml"><mi mathsize="70%" id="Sx1.I7.i4.p1.1.m1.1.1.2" xref="Sx1.I7.i4.p1.1.m1.1.1.2.cmml">δ</mi><mi mathsize="70%" id="Sx1.I7.i4.p1.1.m1.1.1.3" xref="Sx1.I7.i4.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx1.I7.i4.p1.1.m1.1b"><apply id="Sx1.I7.i4.p1.1.m1.1.1.cmml" xref="Sx1.I7.i4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx1.I7.i4.p1.1.m1.1.1.1.cmml" xref="Sx1.I7.i4.p1.1.m1.1.1">subscript</csymbol><ci id="Sx1.I7.i4.p1.1.m1.1.1.2.cmml" xref="Sx1.I7.i4.p1.1.m1.1.1.2">𝛿</ci><ci id="Sx1.I7.i4.p1.1.m1.1.1.3.cmml" xref="Sx1.I7.i4.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I7.i4.p1.1.m1.1c">\delta_{i}</annotation></semantics></math><span id="Sx1.I7.i4.p1.2.1" class="ltx_text" style="font-size:70%;">: whether detection </span><math id="Sx1.I7.i4.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="Sx1.I7.i4.p1.2.m2.1a"><mi mathsize="70%" id="Sx1.I7.i4.p1.2.m2.1.1" xref="Sx1.I7.i4.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="Sx1.I7.i4.p1.2.m2.1b"><ci id="Sx1.I7.i4.p1.2.m2.1.1.cmml" xref="Sx1.I7.i4.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.I7.i4.p1.2.m2.1c">i</annotation></semantics></math><span id="Sx1.I7.i4.p1.2.2" class="ltx_text" style="font-size:70%;"> is assigned to a ground truth bounding box</span></span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="Sx1.T5" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE V: </span>PERFORMANCE OVERVIEW OF RADAR-CAMERA METHODS</figcaption>
<table id="Sx1.T5.17" class="ltx_tabular">
<thead class="ltx_thead">
<tr id="Sx1.T5.17.18.1" class="ltx_tr">
<th id="Sx1.T5.17.18.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="Sx1.T5.17.18.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Method</span></th>
<th id="Sx1.T5.17.18.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="Sx1.T5.17.18.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Dataset</span></th>
<th id="Sx1.T5.17.18.1.3" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="Sx1.T5.17.18.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Metrics</span></th>
<th id="Sx1.T5.17.18.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="Sx1.T5.17.18.1.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Threshold</span></th>
<th id="Sx1.T5.17.18.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" rowspan="2">
<span id="Sx1.T5.17.18.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.18.1.5.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.18.1.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Sub-dataset</span></span>
</span>
</th>
<th id="Sx1.T5.17.18.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" rowspan="2">
<span id="Sx1.T5.17.18.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.18.1.6.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.18.1.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Hardware</span></span>
</span>
</th>
</tr>
<tr id="Sx1.T5.17.19.2" class="ltx_tr">
<th id="Sx1.T5.17.19.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="Sx1.T5.17.19.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.19.2.1.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.19.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">AP/AR</span></span>
</span>
</th>
<th id="Sx1.T5.17.19.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="Sx1.T5.17.19.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.19.2.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.19.2.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">mAP</span></span>
</span>
</th>
<th id="Sx1.T5.17.19.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="Sx1.T5.17.19.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.19.2.3.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T5.17.19.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Others</span></span>
</span>
</th>
<th id="Sx1.T5.17.19.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="Sx1.T5.17.19.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.19.2.4.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.19.2.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Inference Time</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx1.T5.4.4" class="ltx_tr">
<td id="Sx1.T5.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.4.4.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.4.4.5.1.1.1" class="ltx_text" style="font-size:70%;">RRPN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.4.4.5.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="Sx1.T5.4.4.5.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.4.4.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.4.4.6.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.4.4.4.4" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.4.4.4.4.4" class="ltx_p" style="width:74.0pt;">
<span id="Sx1.T5.4.4.4.4.4.4" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.4.4.4.4.4.4.5" class="ltx_p"><span id="Sx1.T5.4.4.4.4.4.4.5.1" class="ltx_text" style="font-size:70%;">AP(NS-F): 43.0</span></span>
<span id="Sx1.T5.1.1.1.1.1.1.1" class="ltx_p"><span id="Sx1.T5.1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.1.1.1.1.1.1.1.2" class="ltx_sup"><span id="Sx1.T5.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.50</span></sup><span id="Sx1.T5.1.1.1.1.1.1.1.3" class="ltx_text" style="font-size:70%;">(NS-F): 64.9</span></span>
<span id="Sx1.T5.2.2.2.2.2.2.2" class="ltx_p"><span id="Sx1.T5.2.2.2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.2.2.2.2.2.2.2.2" class="ltx_sup"><span id="Sx1.T5.2.2.2.2.2.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.75</span></sup><span id="Sx1.T5.2.2.2.2.2.2.2.3" class="ltx_text" style="font-size:70%;">(NS-F): 48.5</span></span>
<span id="Sx1.T5.4.4.4.4.4.4.6" class="ltx_p"><span id="Sx1.T5.4.4.4.4.4.4.6.1" class="ltx_text" style="font-size:70%;">AP (NS-FB): 35.5</span></span>
<span id="Sx1.T5.3.3.3.3.3.3.3" class="ltx_p"><span id="Sx1.T5.3.3.3.3.3.3.3.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.3.3.3.3.3.3.3.2" class="ltx_sup"><span id="Sx1.T5.3.3.3.3.3.3.3.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.50</span></sup><span id="Sx1.T5.3.3.3.3.3.3.3.3" class="ltx_text" style="font-size:70%;">(NS-FB): 59.0</span></span>
<span id="Sx1.T5.4.4.4.4.4.4.4" class="ltx_p"><span id="Sx1.T5.4.4.4.4.4.4.4.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.4.4.4.4.4.4.4.2" class="ltx_sup"><span id="Sx1.T5.4.4.4.4.4.4.4.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.75</span></sup><span id="Sx1.T5.4.4.4.4.4.4.4.3" class="ltx_text" style="font-size:70%;">(NS-FB): 37.0</span></span>
<span id="Sx1.T5.4.4.4.4.4.4.7" class="ltx_p"><span id="Sx1.T5.4.4.4.4.4.4.7.1" class="ltx_text" style="font-size:70%;">AR(NS-F): 48.6</span></span>
<span id="Sx1.T5.4.4.4.4.4.4.8" class="ltx_p"><span id="Sx1.T5.4.4.4.4.4.4.8.1" class="ltx_text" style="font-size:70%;">AR(NS-FB): 42.1</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.4.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.4.4.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.4.4.7.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.4.4.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.4.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.4.4.8.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T5.4.4.8.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.4.4.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.4.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.4.4.9.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.4.4.9.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.4.4.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.4.4.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.4.4.10.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.4.4.10.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.5, 0.75}</span></span>
</span>
</td>
<td id="Sx1.T5.4.4.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.4.4.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.4.4.11.1.1" class="ltx_p" style="width:102.4pt;">
<span id="Sx1.T5.4.4.11.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:69.4pt;">
<span id="Sx1.T5.4.4.11.1.1.1.1" class="ltx_p"><span id="Sx1.T5.4.4.11.1.1.1.1.1" class="ltx_text" style="font-size:70%;">a) NS-F sub-dataset: from front camera and front radar only, with 23k samples</span></span>
<span id="Sx1.T5.4.4.11.1.1.1.2" class="ltx_p"><span id="Sx1.T5.4.4.11.1.1.1.2.1" class="ltx_text" style="font-size:70%;">b) NS-FB sub-dataset: from the rear camera and two rear radars, with 45k samples</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.4.4.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.4.4.12.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.4.4.12.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.4.4.12.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.20.1" class="ltx_tr">
<td id="Sx1.T5.17.20.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.20.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.20.1.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.20.1.1.1.1.1" class="ltx_text" style="font-size:70%;">RVNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.20.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="Sx1.T5.17.20.1.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.20.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.20.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.20.1.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.20.1.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.20.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.20.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.20.1.3.1.1" class="ltx_p" style="width:74.0pt;">
<span id="Sx1.T5.17.20.1.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.20.1.3.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.20.1.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">AP(Cycle): 20.0</span></span>
<span id="Sx1.T5.17.20.1.3.1.1.1.2" class="ltx_p"><span id="Sx1.T5.17.20.1.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">AP(Pedestrian): 14.0</span></span>
<span id="Sx1.T5.17.20.1.3.1.1.1.3" class="ltx_p"><span id="Sx1.T5.17.20.1.3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">AP(Vehicle): 59.0</span></span>
<span id="Sx1.T5.17.20.1.3.1.1.1.4" class="ltx_p"><span id="Sx1.T5.17.20.1.3.1.1.1.4.1" class="ltx_text" style="font-size:70%;">AP(Obstacle): 26.0</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.20.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.20.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.20.1.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.20.1.4.1.1.1" class="ltx_text" style="font-size:70%;">25.0</span></span>
</span>
</td>
<td id="Sx1.T5.17.20.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.20.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.20.1.5.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T5.17.20.1.5.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.20.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.20.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.20.1.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.20.1.6.1.1.1" class="ltx_text" style="font-size:70%;">17 ms</span></span>
</span>
</td>
<td id="Sx1.T5.17.20.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.20.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.20.1.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.20.1.7.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.5}</span></span>
</span>
</td>
<td id="Sx1.T5.17.20.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.20.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.20.1.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.20.1.8.1.1.1" class="ltx_text" style="font-size:70%;">Samples from front camera and front radar</span></span>
</span>
</td>
<td id="Sx1.T5.17.20.1.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.20.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.20.1.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.20.1.9.1.1.1" class="ltx_text" style="font-size:70%;">One NVIDIA GeForce 1080 GPU</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.21.2" class="ltx_tr">
<td id="Sx1.T5.17.21.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.21.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.21.2.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.21.2.1.1.1.1" class="ltx_text" style="font-size:70%;">SO-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.21.2.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib199" title="" class="ltx_ref">199</a><span id="Sx1.T5.17.21.2.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.21.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.21.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.21.2.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.21.2.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.21.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.21.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.21.2.3.1.1" class="ltx_p" style="width:74.0pt;">
<span id="Sx1.T5.17.21.2.3.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.21.2.3.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.21.2.3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">AP(detection): 42.3</span></span>
<span id="Sx1.T5.17.21.2.3.1.1.1.2" class="ltx_p"><span id="Sx1.T5.17.21.2.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">AP(segmentation): 99.1</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.21.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.21.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.21.2.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.21.2.4.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.21.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.21.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.21.2.5.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T5.17.21.2.5.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.21.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.21.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.21.2.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.21.2.6.1.1.1" class="ltx_text" style="font-size:70%;">25 ms</span></span>
</span>
</td>
<td id="Sx1.T5.17.21.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.21.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.21.2.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.21.2.7.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.5}</span></span>
</span>
</td>
<td id="Sx1.T5.17.21.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.21.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.21.2.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.21.2.8.1.1.1" class="ltx_text" style="font-size:70%;">Samples under rainy and nighttime conditions (308 pairs for training and 114 pairs for testing)</span></span>
</span>
</td>
<td id="Sx1.T5.17.21.2.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.21.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.21.2.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.21.2.9.1.1.1" class="ltx_text" style="font-size:70%;">One NVIDIA GeForce 1080 GPU</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.6.6" class="ltx_tr">
<td id="Sx1.T5.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.6.6.3.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.6.6.3.1.1.1" class="ltx_text" style="font-size:70%;">SAF-FCOS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.6.6.3.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib214" title="" class="ltx_ref">214</a><span id="Sx1.T5.6.6.3.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.6.6.4.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.6.6.4.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.6.6.2.2" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.6.6.2.2.2" class="ltx_p" style="width:74.0pt;">
<span id="Sx1.T5.6.6.2.2.2.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.6.6.2.2.2.2.3" class="ltx_p"><span id="Sx1.T5.6.6.2.2.2.2.3.1" class="ltx_text" style="font-size:70%;">AP: 72.4</span></span>
<span id="Sx1.T5.5.5.1.1.1.1.1" class="ltx_p"><span id="Sx1.T5.5.5.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.5.5.1.1.1.1.1.2" class="ltx_sup"><span id="Sx1.T5.5.5.1.1.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.50</span></sup><span id="Sx1.T5.5.5.1.1.1.1.1.3" class="ltx_text" style="font-size:70%;">: 90.0</span></span>
<span id="Sx1.T5.6.6.2.2.2.2.2" class="ltx_p"><span id="Sx1.T5.6.6.2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.6.6.2.2.2.2.2.2" class="ltx_sup"><span id="Sx1.T5.6.6.2.2.2.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.75</span></sup><span id="Sx1.T5.6.6.2.2.2.2.2.3" class="ltx_text" style="font-size:70%;">: 79.3</span></span>
<span id="Sx1.T5.6.6.2.2.2.2.4" class="ltx_p"><span id="Sx1.T5.6.6.2.2.2.2.4.1" class="ltx_text" style="font-size:70%;">AR: 79.0</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.6.6.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.6.6.5.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.6.6.6" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="Sx1.T5.6.6.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.6.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.6.6.7.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.6.6.7.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.6.6.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.6.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.6.6.8.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.6.6.8.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.5, 0.75, 0.5-0.95}</span></span>
</span>
</td>
<td id="Sx1.T5.6.6.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.6.6.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.6.6.9.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.6.6.9.1.1.1" class="ltx_text" style="font-size:70%;">A total of 34,149 radar-camera pairs</span></span>
</span>
</td>
<td id="Sx1.T5.6.6.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.6.6.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.6.6.10.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.6.6.10.1.1.1" class="ltx_text" style="font-size:70%;">Eight NVIDIA GeForce GTX 1080Ti GPUs</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.22.3" class="ltx_tr">
<td id="Sx1.T5.17.22.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.22.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.22.3.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.22.3.1.1.1.1" class="ltx_text" style="font-size:70%;">CRF-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.22.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib211" title="" class="ltx_ref">211</a><span id="Sx1.T5.17.22.3.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.22.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.22.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.22.3.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.22.3.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.22.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.22.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.22.3.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.22.3.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.22.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.22.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.22.3.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.22.3.4.1.1.1" class="ltx_text" style="font-size:70%;">55.23</span></span>
</span>
</td>
<td id="Sx1.T5.17.22.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.22.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.22.3.5.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T5.17.22.3.5.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.22.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.22.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.22.3.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.22.3.6.1.1.1" class="ltx_text" style="font-size:70%;">43 ms</span></span>
</span>
</td>
<td id="Sx1.T5.17.22.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.22.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.22.3.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.22.3.7.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.22.3.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.22.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.22.3.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.22.3.8.1.1.1" class="ltx_text" style="font-size:70%;">Merge the original 23 object classes into seven classes</span></span>
</span>
</td>
<td id="Sx1.T5.17.22.3.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.22.3.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.22.3.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.22.3.9.1.1.1" class="ltx_text" style="font-size:70%;">One NVIDIA Titan XP GPU</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.9.9" class="ltx_tr">
<td id="Sx1.T5.9.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.9.9.4.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.9.9.4.1.1.1" class="ltx_text" style="font-size:70%;">BIRANet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.9.9.4.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib190" title="" class="ltx_ref">190</a><span id="Sx1.T5.9.9.4.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.9.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.9.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.9.9.5.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.9.9.5.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.9.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.9.9.3.3" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.9.9.3.3.3" class="ltx_p" style="width:74.0pt;">
<span id="Sx1.T5.9.9.3.3.3.3" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.9.9.3.3.3.3.4" class="ltx_p"><span id="Sx1.T5.9.9.3.3.3.3.4.1" class="ltx_text" style="font-size:70%;">AP: 72.3</span></span>
<span id="Sx1.T5.7.7.1.1.1.1.1" class="ltx_p"><span id="Sx1.T5.7.7.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.7.7.1.1.1.1.1.2" class="ltx_sup"><span id="Sx1.T5.7.7.1.1.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.50</span></sup><span id="Sx1.T5.7.7.1.1.1.1.1.3" class="ltx_text" style="font-size:70%;">: 88.9</span></span>
<span id="Sx1.T5.8.8.2.2.2.2.2" class="ltx_p"><span id="Sx1.T5.8.8.2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.8.8.2.2.2.2.2.2" class="ltx_sup"><span id="Sx1.T5.8.8.2.2.2.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.75</span></sup><span id="Sx1.T5.8.8.2.2.2.2.2.3" class="ltx_text" style="font-size:70%;">: 84.3</span></span>
<span id="Sx1.T5.9.9.3.3.3.3.3" class="ltx_p"><span id="Sx1.T5.9.9.3.3.3.3.3.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.9.9.3.3.3.3.3.2" class="ltx_sup"><span id="Sx1.T5.9.9.3.3.3.3.3.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.85</span></sup><span id="Sx1.T5.9.9.3.3.3.3.3.3" class="ltx_text" style="font-size:70%;">: 65.7</span></span>
<span id="Sx1.T5.9.9.3.3.3.3.5" class="ltx_p"><span id="Sx1.T5.9.9.3.3.3.3.5.1" class="ltx_text" style="font-size:70%;">AR: 75.3</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.9.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.9.9.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.9.9.6.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.9.9.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.9.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.9.9.7.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T5.9.9.7.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.9.9.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.9.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.9.9.8.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.9.9.8.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.9.9.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.9.9.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.9.9.9.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.9.9.9.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.5, 0.75, 0.85}</span></span>
</span>
</td>
<td id="Sx1.T5.9.9.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.9.9.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.9.9.10.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.9.9.10.1.1.1" class="ltx_text" style="font-size:70%;">Merged relevant classes into six classes</span></span>
</span>
</td>
<td id="Sx1.T5.9.9.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.9.9.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.9.9.11.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.9.9.11.1.1.1" class="ltx_text" style="font-size:70%;">One NVIDIA Titan Pascal GPU</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.11.11" class="ltx_tr">
<td id="Sx1.T5.11.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.11.11.3.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.11.11.3.1.1.1" class="ltx_text" style="font-size:70%;">Nabati and Qi </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.11.11.3.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib194" title="" class="ltx_ref">194</a><span id="Sx1.T5.11.11.3.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.11.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.11.11.4.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.11.11.4.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.11.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.11.11.2.2" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.11.11.2.2.2" class="ltx_p" style="width:74.0pt;">
<span id="Sx1.T5.11.11.2.2.2.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.11.11.2.2.2.2.3" class="ltx_p"><span id="Sx1.T5.11.11.2.2.2.2.3.1" class="ltx_text" style="font-size:70%;">AP: 35.6</span></span>
<span id="Sx1.T5.10.10.1.1.1.1.1" class="ltx_p"><span id="Sx1.T5.10.10.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.10.10.1.1.1.1.1.2" class="ltx_sup"><span id="Sx1.T5.10.10.1.1.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.50</span></sup><span id="Sx1.T5.10.10.1.1.1.1.1.3" class="ltx_text" style="font-size:70%;">: 60.53</span></span>
<span id="Sx1.T5.11.11.2.2.2.2.2" class="ltx_p"><span id="Sx1.T5.11.11.2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.11.11.2.2.2.2.2.2" class="ltx_sup"><span id="Sx1.T5.11.11.2.2.2.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.75</span></sup><span id="Sx1.T5.11.11.2.2.2.2.2.3" class="ltx_text" style="font-size:70%;">: 37.38</span></span>
<span id="Sx1.T5.11.11.2.2.2.2.4" class="ltx_p"><span id="Sx1.T5.11.11.2.2.2.2.4.1" class="ltx_text" style="font-size:70%;">AR: 42.1</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.11.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.11.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.11.11.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.11.11.5.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.11.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.11.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.11.11.6.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T5.11.11.6.1.1.1" class="ltx_text" style="font-size:70%;">MAE: 2.65</span></span>
</span>
</td>
<td id="Sx1.T5.11.11.7" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="Sx1.T5.11.11.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.11.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.11.11.8.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.11.11.8.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.5, 0.75}</span></span>
</span>
</td>
<td id="Sx1.T5.11.11.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.11.11.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.11.11.9.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.11.11.9.1.1.1" class="ltx_text" style="font-size:70%;">Samples from front and rear cameras together with all radars</span></span>
</span>
</td>
<td id="Sx1.T5.11.11.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.11.11.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.11.11.10.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.11.11.10.1.1.1" class="ltx_text" style="font-size:70%;">Two NVIDIA Quadro P6000 GPUs</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.23.4" class="ltx_tr">
<td id="Sx1.T5.17.23.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.23.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.23.4.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.23.4.1.1.1.1" class="ltx_text" style="font-size:70%;">YOdar </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.23.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib197" title="" class="ltx_ref">197</a><span id="Sx1.T5.17.23.4.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.23.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.23.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.23.4.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.23.4.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.23.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.23.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.23.4.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.23.4.3.1.1.1" class="ltx_text" style="font-size:70%;">AP: 43.1</span></span>
</span>
</td>
<td id="Sx1.T5.17.23.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.23.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.23.4.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.23.4.4.1.1.1" class="ltx_text" style="font-size:70%;">39.4</span></span>
</span>
</td>
<td id="Sx1.T5.17.23.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.23.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.23.4.5.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T5.17.23.4.5.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.23.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.23.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.23.4.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.23.4.6.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.23.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.23.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.23.4.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.23.4.7.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.23.4.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.23.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.23.4.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.23.4.8.1.1.1" class="ltx_text" style="font-size:70%;">Samples from nuScenes (29,853 frames for training, 3,289 frames for validation and 1,006 frames for testing)</span></span>
</span>
</td>
<td id="Sx1.T5.17.23.4.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.23.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.23.4.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.23.4.9.1.1.1" class="ltx_text" style="font-size:70%;">One NVIDIA Quadro P6000 GPU</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.24.5" class="ltx_tr">
<td id="Sx1.T5.17.24.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.24.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.24.5.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.24.5.1.1.1.1" class="ltx_text" style="font-size:70%;">CenterFusion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.24.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="Sx1.T5.17.24.5.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.24.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.24.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.24.5.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.24.5.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.24.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.24.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.24.5.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.24.5.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.24.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.24.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.24.5.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.24.5.4.1.1.1" class="ltx_text" style="font-size:70%;">32.6</span></span>
</span>
</td>
<td id="Sx1.T5.17.24.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.24.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.24.5.5.1.1" class="ltx_p" style="width:51.2pt;">
<span id="Sx1.T5.17.24.5.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.24.5.5.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.24.5.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">NDS: 44.9</span></span>
<span id="Sx1.T5.17.24.5.5.1.1.1.2" class="ltx_p"><span id="Sx1.T5.17.24.5.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">mATE: 63.1</span></span>
<span id="Sx1.T5.17.24.5.5.1.1.1.3" class="ltx_p"><span id="Sx1.T5.17.24.5.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">mASE: 26.1</span></span>
<span id="Sx1.T5.17.24.5.5.1.1.1.4" class="ltx_p"><span id="Sx1.T5.17.24.5.5.1.1.1.4.1" class="ltx_text" style="font-size:70%;">mAOE: 51.6</span></span>
<span id="Sx1.T5.17.24.5.5.1.1.1.5" class="ltx_p"><span id="Sx1.T5.17.24.5.5.1.1.1.5.1" class="ltx_text" style="font-size:70%;">mAVE: 61.4</span></span>
<span id="Sx1.T5.17.24.5.5.1.1.1.6" class="ltx_p"><span id="Sx1.T5.17.24.5.5.1.1.1.6.1" class="ltx_text" style="font-size:70%;">mAAE: 11.5</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.24.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.24.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.24.5.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.24.5.6.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.24.5.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.24.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.24.5.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.24.5.7.1.1.1" class="ltx_text" style="font-size:70%;">Distance={0.5, 1, 2, 4}</span></span>
</span>
</td>
<td id="Sx1.T5.17.24.5.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.24.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.24.5.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.24.5.8.1.1.1" class="ltx_text" style="font-size:70%;">Complete nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.24.5.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.24.5.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.24.5.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.24.5.9.1.1.1" class="ltx_text" style="font-size:70%;">Two NVIDIA P5000 GPUs</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.13.13" class="ltx_tr">
<td id="Sx1.T5.13.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.13.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.13.13.3.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.13.13.3.1.1.1" class="ltx_text" style="font-size:70%;">Li and Xie </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.13.13.3.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib196" title="" class="ltx_ref">196</a><span id="Sx1.T5.13.13.3.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.13.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.13.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.13.13.4.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.13.13.4.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.13.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.13.13.2.2" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.13.13.2.2.2" class="ltx_p" style="width:74.0pt;">
<span id="Sx1.T5.13.13.2.2.2.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.13.13.2.2.2.2.3" class="ltx_p"><span id="Sx1.T5.13.13.2.2.2.2.3.1" class="ltx_text" style="font-size:70%;">AP: 24.3</span></span>
<span id="Sx1.T5.12.12.1.1.1.1.1" class="ltx_p"><span id="Sx1.T5.12.12.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.12.12.1.1.1.1.1.2" class="ltx_sup"><span id="Sx1.T5.12.12.1.1.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.50</span></sup><span id="Sx1.T5.12.12.1.1.1.1.1.3" class="ltx_text" style="font-size:70%;">: 48.4</span></span>
<span id="Sx1.T5.13.13.2.2.2.2.2" class="ltx_p"><span id="Sx1.T5.13.13.2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.13.13.2.2.2.2.2.2" class="ltx_sup"><span id="Sx1.T5.13.13.2.2.2.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">.75</span></sup><span id="Sx1.T5.13.13.2.2.2.2.2.3" class="ltx_text" style="font-size:70%;">: 22.3</span></span>
<span id="Sx1.T5.13.13.2.2.2.2.4" class="ltx_p"><span id="Sx1.T5.13.13.2.2.2.2.4.1" class="ltx_text" style="font-size:70%;">AR: 33.7</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.13.13.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.13.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.13.13.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.13.13.5.1.1.1" class="ltx_text" style="font-size:70%;">48.4</span></span>
</span>
</td>
<td id="Sx1.T5.13.13.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.13.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.13.13.6.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T5.13.13.6.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.13.13.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.13.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.13.13.7.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.13.13.7.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.13.13.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.13.13.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.13.13.8.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.13.13.8.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.5, 0.75}</span></span>
</span>
</td>
<td id="Sx1.T5.13.13.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.13.13.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.13.13.9.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.13.13.9.1.1.1" class="ltx_text" style="font-size:70%;">Dataset is randomly divided into a training set, a validation set, and a testing set according to the ratio of 6:2:2</span></span>
</span>
</td>
<td id="Sx1.T5.13.13.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.13.13.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.13.13.10.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.13.13.10.1.1.1" class="ltx_text" style="font-size:70%;">One NVIDIA GeForce GTX 1080Ti GPU</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.25.6" class="ltx_tr">
<td id="Sx1.T5.17.25.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.25.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.25.6.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.25.6.1.1.1.1" class="ltx_text" style="font-size:70%;">RVF-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.25.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib243" title="" class="ltx_ref">243</a><span id="Sx1.T5.17.25.6.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.25.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.25.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.25.6.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.25.6.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.25.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.25.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.25.6.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.25.6.3.1.1.1" class="ltx_text" style="font-size:70%;">AP: 54.86</span></span>
</span>
</td>
<td id="Sx1.T5.17.25.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.25.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.25.6.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.25.6.4.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.25.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.25.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.25.6.5.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T5.17.25.6.5.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.25.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.25.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.25.6.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.25.6.6.1.1.1" class="ltx_text" style="font-size:70%;">44 ms</span></span>
</span>
</td>
<td id="Sx1.T5.17.25.6.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.25.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.25.6.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.25.6.7.1.1.1" class="ltx_text" style="font-size:70%;">Distance={0.5}</span></span>
</span>
</td>
<td id="Sx1.T5.17.25.6.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.25.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.25.6.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.25.6.8.1.1.1" class="ltx_text" style="font-size:70%;">Samples under rainy and nighttime conditions</span></span>
</span>
</td>
<td id="Sx1.T5.17.25.6.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.25.6.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.25.6.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.25.6.9.1.1.1" class="ltx_text" style="font-size:70%;">One NVIDIA Titan XP GPU</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.17" class="ltx_tr">
<td id="Sx1.T5.17.17.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.17.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.17.5.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.17.5.1.1.1" class="ltx_text" style="font-size:70%;">GRIF Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.17.5.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib245" title="" class="ltx_ref">245</a><span id="Sx1.T5.17.17.5.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.17.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.17.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.17.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.17.6.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.17.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.17.4.4" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.17.4.4.4" class="ltx_p" style="width:74.0pt;">
<span id="Sx1.T5.17.17.4.4.4.4" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.14.14.1.1.1.1.1" class="ltx_p"><span id="Sx1.T5.14.14.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.14.14.1.1.1.1.1.2" class="ltx_sup"><span id="Sx1.T5.14.14.1.1.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">0.5m</span></sup><span id="Sx1.T5.14.14.1.1.1.1.1.3" class="ltx_text" style="font-size:70%;">: 44.1</span></span>
<span id="Sx1.T5.15.15.2.2.2.2.2" class="ltx_p"><span id="Sx1.T5.15.15.2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.15.15.2.2.2.2.2.2" class="ltx_sup"><span id="Sx1.T5.15.15.2.2.2.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">1m</span></sup><span id="Sx1.T5.15.15.2.2.2.2.2.3" class="ltx_text" style="font-size:70%;">: 66.5</span></span>
<span id="Sx1.T5.16.16.3.3.3.3.3" class="ltx_p"><span id="Sx1.T5.16.16.3.3.3.3.3.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.16.16.3.3.3.3.3.2" class="ltx_sup"><span id="Sx1.T5.16.16.3.3.3.3.3.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">2m</span></sup><span id="Sx1.T5.16.16.3.3.3.3.3.3" class="ltx_text" style="font-size:70%;">: 71.9</span></span>
<span id="Sx1.T5.17.17.4.4.4.4.4" class="ltx_p"><span id="Sx1.T5.17.17.4.4.4.4.4.1" class="ltx_text" style="font-size:70%;">AP</span><sup id="Sx1.T5.17.17.4.4.4.4.4.2" class="ltx_sup"><span id="Sx1.T5.17.17.4.4.4.4.4.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">4m</span></sup><span id="Sx1.T5.17.17.4.4.4.4.4.3" class="ltx_text" style="font-size:70%;">: 74.9</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.17.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.17.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.17.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.17.7.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.17.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.17.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.17.8.1.1" class="ltx_p" style="width:51.2pt;"><span id="Sx1.T5.17.17.8.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.17.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.17.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.17.9.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.17.9.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.17.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.17.10.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.17.10.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.17.10.1.1.1" class="ltx_text" style="font-size:70%;">Distance={0.5, 1, 2, 4}</span></span>
</span>
</td>
<td id="Sx1.T5.17.17.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.17.11.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.17.11.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.17.11.1.1.1" class="ltx_text" style="font-size:70%;">One front camera and 3 front radars</span></span>
</span>
</td>
<td id="Sx1.T5.17.17.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.17.12.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.17.12.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.17.12.1.1.1" class="ltx_text" style="font-size:70%;">One NVIDIA GeForce GTX 1080Ti GPU</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.26.7" class="ltx_tr">
<td id="Sx1.T5.17.26.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.26.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.26.7.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.26.7.1.1.1.1" class="ltx_text" style="font-size:70%;">Stäcker </span><span id="Sx1.T5.17.26.7.1.1.1.2" class="ltx_text ltx_font_italic" style="font-size:70%;">et al</span><span id="Sx1.T5.17.26.7.1.1.1.3" class="ltx_text" style="font-size:70%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.26.7.1.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib255" title="" class="ltx_ref">255</a><span id="Sx1.T5.17.26.7.1.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.26.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.26.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.26.7.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.26.7.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.26.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.26.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.26.7.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.26.7.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.26.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.26.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.26.7.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.26.7.4.1.1.1" class="ltx_text" style="font-size:70%;">36.78</span></span>
</span>
</td>
<td id="Sx1.T5.17.26.7.5" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="Sx1.T5.17.26.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.26.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.26.7.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.26.7.6.1.1.1" class="ltx_text" style="font-size:70%;">36.7 ms</span></span>
</span>
</td>
<td id="Sx1.T5.17.26.7.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.26.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.26.7.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.26.7.7.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.5}</span></span>
</span>
</td>
<td id="Sx1.T5.17.26.7.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.26.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.26.7.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.26.7.8.1.1.1" class="ltx_text" style="font-size:70%;">Sample from front camera and radar</span></span>
</span>
</td>
<td id="Sx1.T5.17.26.7.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.26.7.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.26.7.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.26.7.9.1.1.1" class="ltx_text" style="font-size:70%;">One NVIDIA GeForce RTX 2080 GPU</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.27.8" class="ltx_tr">
<td id="Sx1.T5.17.27.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.27.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.27.8.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.27.8.1.1.1.1" class="ltx_text" style="font-size:70%;">FUTR3D </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.27.8.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib320" title="" class="ltx_ref">320</a><span id="Sx1.T5.17.27.8.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.27.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.27.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.27.8.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.27.8.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.27.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.27.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.27.8.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.27.8.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.27.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.27.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.27.8.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.27.8.4.1.1.1" class="ltx_text" style="font-size:70%;">39.9</span></span>
</span>
</td>
<td id="Sx1.T5.17.27.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.27.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.27.8.5.1.1" class="ltx_p" style="width:51.2pt;">
<span id="Sx1.T5.17.27.8.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.27.8.5.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.27.8.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">NDS: 50.8</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.27.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.27.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.27.8.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.27.8.6.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.27.8.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.27.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.27.8.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.27.8.7.1.1.1" class="ltx_text" style="font-size:70%;">Distance={0.5, 1, 2, 4}</span></span>
</span>
</td>
<td id="Sx1.T5.17.27.8.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.27.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.27.8.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.27.8.8.1.1.1" class="ltx_text" style="font-size:70%;">Complete nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.27.8.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.27.8.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.27.8.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.27.8.9.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.28.9" class="ltx_tr">
<td id="Sx1.T5.17.28.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.28.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.28.9.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.28.9.1.1.1.1" class="ltx_text" style="font-size:70%;">RCBEV </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.28.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib207" title="" class="ltx_ref">207</a><span id="Sx1.T5.17.28.9.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.28.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.28.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.28.9.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.28.9.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.28.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.28.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.28.9.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.28.9.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.28.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.28.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.28.9.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.28.9.4.1.1.1" class="ltx_text" style="font-size:70%;">40.6</span></span>
</span>
</td>
<td id="Sx1.T5.17.28.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.28.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.28.9.5.1.1" class="ltx_p" style="width:51.2pt;">
<span id="Sx1.T5.17.28.9.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.28.9.5.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.28.9.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">NDS: 45.6</span></span>
<span id="Sx1.T5.17.28.9.5.1.1.1.2" class="ltx_p"><span id="Sx1.T5.17.28.9.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">mATE: 48.4</span></span>
<span id="Sx1.T5.17.28.9.5.1.1.1.3" class="ltx_p"><span id="Sx1.T5.17.28.9.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">mASE: 25.7</span></span>
<span id="Sx1.T5.17.28.9.5.1.1.1.4" class="ltx_p"><span id="Sx1.T5.17.28.9.5.1.1.1.4.1" class="ltx_text" style="font-size:70%;">mAOE: 58.7</span></span>
<span id="Sx1.T5.17.28.9.5.1.1.1.5" class="ltx_p"><span id="Sx1.T5.17.28.9.5.1.1.1.5.1" class="ltx_text" style="font-size:70%;">mAVE: 70.2</span></span>
<span id="Sx1.T5.17.28.9.5.1.1.1.6" class="ltx_p"><span id="Sx1.T5.17.28.9.5.1.1.1.6.1" class="ltx_text" style="font-size:70%;">mAAE: 14.0</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.28.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.28.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.28.9.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.28.9.6.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.28.9.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.28.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.28.9.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.28.9.7.1.1.1" class="ltx_text" style="font-size:70%;">Distance={0.5, 1, 2, 4}</span></span>
</span>
</td>
<td id="Sx1.T5.17.28.9.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.28.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.28.9.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.28.9.8.1.1.1" class="ltx_text" style="font-size:70%;">Complete nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.28.9.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.28.9.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.28.9.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.28.9.9.1.1.1" class="ltx_text" style="font-size:70%;">Four NVIDIA GeForce GTX 3090 GPUs</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.29.10" class="ltx_tr">
<td id="Sx1.T5.17.29.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.29.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.29.10.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.29.10.1.1.1.1" class="ltx_text" style="font-size:70%;">CRAFT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.29.10.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib248" title="" class="ltx_ref">248</a><span id="Sx1.T5.17.29.10.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.29.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.29.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.29.10.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.29.10.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.29.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.29.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.29.10.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.29.10.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.29.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.29.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.29.10.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.29.10.4.1.1.1" class="ltx_text" style="font-size:70%;">41.1</span></span>
</span>
</td>
<td id="Sx1.T5.17.29.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.29.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.29.10.5.1.1" class="ltx_p" style="width:51.2pt;">
<span id="Sx1.T5.17.29.10.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.29.10.5.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.29.10.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">NDS: 52.3</span></span>
<span id="Sx1.T5.17.29.10.5.1.1.1.2" class="ltx_p"><span id="Sx1.T5.17.29.10.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">mATE: 46.7</span></span>
<span id="Sx1.T5.17.29.10.5.1.1.1.3" class="ltx_p"><span id="Sx1.T5.17.29.10.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">mASE: 26.8</span></span>
<span id="Sx1.T5.17.29.10.5.1.1.1.4" class="ltx_p"><span id="Sx1.T5.17.29.10.5.1.1.1.4.1" class="ltx_text" style="font-size:70%;">mAOE: 45.3</span></span>
<span id="Sx1.T5.17.29.10.5.1.1.1.5" class="ltx_p"><span id="Sx1.T5.17.29.10.5.1.1.1.5.1" class="ltx_text" style="font-size:70%;">mAVE: 51.9</span></span>
<span id="Sx1.T5.17.29.10.5.1.1.1.6" class="ltx_p"><span id="Sx1.T5.17.29.10.5.1.1.1.6.1" class="ltx_text" style="font-size:70%;">mAAE: 11.4</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.29.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.29.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.29.10.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.29.10.6.1.1.1" class="ltx_text" style="font-size:70%;">4.1 FPS</span></span>
</span>
</td>
<td id="Sx1.T5.17.29.10.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.29.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.29.10.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.29.10.7.1.1.1" class="ltx_text" style="font-size:70%;">Distance={0.5, 1, 2, 4}</span></span>
</span>
</td>
<td id="Sx1.T5.17.29.10.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.29.10.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.29.10.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.29.10.8.1.1.1" class="ltx_text" style="font-size:70%;">Complete nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.29.10.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.29.10.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.29.10.9.1.1" class="ltx_p" style="width:96.7pt;">
<span id="Sx1.T5.17.29.10.9.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.29.10.9.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.29.10.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Training: four NVIDIA GeForce</span></span>
<span id="Sx1.T5.17.29.10.9.1.1.1.2" class="ltx_p"><span id="Sx1.T5.17.29.10.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">RTX 3090 GPUs;</span></span>
<span id="Sx1.T5.17.29.10.9.1.1.1.3" class="ltx_p"><span id="Sx1.T5.17.29.10.9.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Testing: one RTX 3090 GPU</span></span>
</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.30.11" class="ltx_tr">
<td id="Sx1.T5.17.30.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.30.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.30.11.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.30.11.1.1.1.1" class="ltx_text" style="font-size:70%;">MVFusion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.30.11.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib249" title="" class="ltx_ref">249</a><span id="Sx1.T5.17.30.11.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.30.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.30.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.30.11.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.30.11.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.30.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.30.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.30.11.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.30.11.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.30.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.30.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.30.11.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.30.11.4.1.1.1" class="ltx_text" style="font-size:70%;">45.3</span></span>
</span>
</td>
<td id="Sx1.T5.17.30.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.30.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.30.11.5.1.1" class="ltx_p" style="width:51.2pt;">
<span id="Sx1.T5.17.30.11.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.30.11.5.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.30.11.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">NDS: 51.7</span></span>
<span id="Sx1.T5.17.30.11.5.1.1.1.2" class="ltx_p"><span id="Sx1.T5.17.30.11.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">mATE: 56.9</span></span>
<span id="Sx1.T5.17.30.11.5.1.1.1.3" class="ltx_p"><span id="Sx1.T5.17.30.11.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">mASE: 24.6</span></span>
<span id="Sx1.T5.17.30.11.5.1.1.1.4" class="ltx_p"><span id="Sx1.T5.17.30.11.5.1.1.1.4.1" class="ltx_text" style="font-size:70%;">mAOE: 37.9</span></span>
<span id="Sx1.T5.17.30.11.5.1.1.1.5" class="ltx_p"><span id="Sx1.T5.17.30.11.5.1.1.1.5.1" class="ltx_text" style="font-size:70%;">mAVE: 78.1</span></span>
<span id="Sx1.T5.17.30.11.5.1.1.1.6" class="ltx_p"><span id="Sx1.T5.17.30.11.5.1.1.1.6.1" class="ltx_text" style="font-size:70%;">mAAE: 12.8</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.30.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.30.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.30.11.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.30.11.6.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.30.11.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.30.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.30.11.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.30.11.7.1.1.1" class="ltx_text" style="font-size:70%;">Distance={0.5, 1, 2, 4}</span></span>
</span>
</td>
<td id="Sx1.T5.17.30.11.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.30.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.30.11.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.30.11.8.1.1.1" class="ltx_text" style="font-size:70%;">Complete nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.30.11.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.30.11.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.30.11.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.30.11.9.1.1.1" class="ltx_text" style="font-size:70%;">Eight NVIDIA RTX A6000 GPUs</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.31.12" class="ltx_tr">
<td id="Sx1.T5.17.31.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.31.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.31.12.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.31.12.1.1.1.1" class="ltx_text" style="font-size:70%;">CRN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx1.T5.17.31.12.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib250" title="" class="ltx_ref">250</a><span id="Sx1.T5.17.31.12.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="Sx1.T5.17.31.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.31.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.31.12.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.31.12.2.1.1.1" class="ltx_text" style="font-size:70%;">nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.31.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.31.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.31.12.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.31.12.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.31.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.31.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.31.12.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.31.12.4.1.1.1" class="ltx_text" style="font-size:70%;">57.5</span></span>
</span>
</td>
<td id="Sx1.T5.17.31.12.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.31.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.31.12.5.1.1" class="ltx_p" style="width:51.2pt;">
<span id="Sx1.T5.17.31.12.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.31.12.5.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.31.12.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">NDS: 62.4</span></span>
<span id="Sx1.T5.17.31.12.5.1.1.1.2" class="ltx_p"><span id="Sx1.T5.17.31.12.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">mATE: 46.0</span></span>
<span id="Sx1.T5.17.31.12.5.1.1.1.3" class="ltx_p"><span id="Sx1.T5.17.31.12.5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">mASE: 27.3</span></span>
<span id="Sx1.T5.17.31.12.5.1.1.1.4" class="ltx_p"><span id="Sx1.T5.17.31.12.5.1.1.1.4.1" class="ltx_text" style="font-size:70%;">mAOE: 44.3</span></span>
<span id="Sx1.T5.17.31.12.5.1.1.1.5" class="ltx_p"><span id="Sx1.T5.17.31.12.5.1.1.1.5.1" class="ltx_text" style="font-size:70%;">mAVE: 35.2</span></span>
<span id="Sx1.T5.17.31.12.5.1.1.1.6" class="ltx_p"><span id="Sx1.T5.17.31.12.5.1.1.1.6.1" class="ltx_text" style="font-size:70%;">mAAE: 18.0</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.31.12.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.31.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.31.12.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.31.12.6.1.1.1" class="ltx_text" style="font-size:70%;">7.2 FPS</span></span>
</span>
</td>
<td id="Sx1.T5.17.31.12.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.31.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.31.12.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.31.12.7.1.1.1" class="ltx_text" style="font-size:70%;">Distance={0.5, 1, 2, 4}</span></span>
</span>
</td>
<td id="Sx1.T5.17.31.12.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.31.12.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.31.12.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.31.12.8.1.1.1" class="ltx_text" style="font-size:70%;">Complete nuScenes</span></span>
</span>
</td>
<td id="Sx1.T5.17.31.12.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.31.12.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.31.12.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.31.12.9.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.32.13" class="ltx_tr">
<td id="Sx1.T5.17.32.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="Sx1.T5.17.32.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.32.13.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.32.13.1.1.1.1" class="ltx_text" style="font-size:70%;">RCFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib251" title="" class="ltx_ref">251</a>]</cite></span></span>
</span>
</td>
<td id="Sx1.T5.17.32.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.32.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.32.13.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.32.13.2.1.1.1" class="ltx_text" style="font-size:70%;">VoD</span></span>
</span>
</td>
<td id="Sx1.T5.17.32.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.32.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.32.13.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.32.13.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.32.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.32.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.32.13.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.32.13.4.1.1.1" class="ltx_text" style="font-size:70%;">49.65</span></span>
</span>
</td>
<td id="Sx1.T5.17.32.13.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.32.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.32.13.5.1.1" class="ltx_p" style="width:51.2pt;">
<span id="Sx1.T5.17.32.13.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.32.13.5.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.32.13.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.32.13.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.32.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.32.13.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.32.13.6.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.32.13.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.32.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.32.13.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.32.13.7.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.25, 0.5}</span></span>
</span>
</td>
<td id="Sx1.T5.17.32.13.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.32.13.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.32.13.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.32.13.8.1.1.1" class="ltx_text" style="font-size:70%;">Complete VoD</span></span>
</span>
</td>
<td id="Sx1.T5.17.32.13.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="Sx1.T5.17.32.13.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.32.13.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.32.13.9.1.1.1" class="ltx_text" style="font-size:70%;">NVIDIA GeForce RTX 3090 GPUs</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.33.14" class="ltx_tr">
<td id="Sx1.T5.17.33.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.33.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.33.14.1.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.33.14.1.1.1.1" class="ltx_text" style="font-size:70%;">TJ4DRadSet</span></span>
</span>
</td>
<td id="Sx1.T5.17.33.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.33.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.33.14.2.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.33.14.2.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.33.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.33.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.33.14.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.33.14.3.1.1.1" class="ltx_text" style="font-size:70%;">33.85</span></span>
</span>
</td>
<td id="Sx1.T5.17.33.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.33.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.33.14.4.1.1" class="ltx_p" style="width:51.2pt;">
<span id="Sx1.T5.17.33.14.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.33.14.4.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.33.14.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">BEV mAP: 39.76</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.33.14.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.33.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.33.14.5.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.33.14.5.1.1.1" class="ltx_text" style="font-size:70%;">10.8 FPS</span></span>
</span>
</td>
<td id="Sx1.T5.17.33.14.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.33.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.33.14.6.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.33.14.6.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.25, 0.5}</span></span>
</span>
</td>
<td id="Sx1.T5.17.33.14.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.33.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.33.14.7.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.33.14.7.1.1.1" class="ltx_text" style="font-size:70%;">Complete TJ4DRadSet</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.34.15" class="ltx_tr">
<td id="Sx1.T5.17.34.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" rowspan="2">
<span id="Sx1.T5.17.34.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.34.15.1.1.1" class="ltx_p" style="width:59.8pt;"><span id="Sx1.T5.17.34.15.1.1.1.1" class="ltx_text" style="font-size:70%;">LXL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib247" title="" class="ltx_ref">247</a>]</cite></span></span>
</span>
</td>
<td id="Sx1.T5.17.34.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.34.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.34.15.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.34.15.2.1.1.1" class="ltx_text" style="font-size:70%;">VoD</span></span>
</span>
</td>
<td id="Sx1.T5.17.34.15.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.34.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.34.15.3.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.34.15.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.34.15.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.34.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.34.15.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.34.15.4.1.1.1" class="ltx_text" style="font-size:70%;">56.31</span></span>
</span>
</td>
<td id="Sx1.T5.17.34.15.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.34.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.34.15.5.1.1" class="ltx_p" style="width:51.2pt;">
<span id="Sx1.T5.17.34.15.5.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.34.15.5.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.34.15.5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.34.15.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.34.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.34.15.6.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.34.15.6.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.34.15.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.34.15.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.34.15.7.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.34.15.7.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.25, 0.5}</span></span>
</span>
</td>
<td id="Sx1.T5.17.34.15.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="Sx1.T5.17.34.15.8.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.34.15.8.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.34.15.8.1.1.1" class="ltx_text" style="font-size:70%;">Complete VoD</span></span>
</span>
</td>
<td id="Sx1.T5.17.34.15.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" rowspan="2">
<span id="Sx1.T5.17.34.15.9.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.34.15.9.1.1" class="ltx_p" style="width:96.7pt;"><span id="Sx1.T5.17.34.15.9.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="Sx1.T5.17.35.16" class="ltx_tr">
<td id="Sx1.T5.17.35.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T5.17.35.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.35.16.1.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.35.16.1.1.1.1" class="ltx_text" style="font-size:70%;">TJ4DRadSet</span></span>
</span>
</td>
<td id="Sx1.T5.17.35.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T5.17.35.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.35.16.2.1.1" class="ltx_p" style="width:74.0pt;"><span id="Sx1.T5.17.35.16.2.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.35.16.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T5.17.35.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.35.16.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="Sx1.T5.17.35.16.3.1.1.1" class="ltx_text" style="font-size:70%;">36.32</span></span>
</span>
</td>
<td id="Sx1.T5.17.35.16.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T5.17.35.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.35.16.4.1.1" class="ltx_p" style="width:51.2pt;">
<span id="Sx1.T5.17.35.16.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="Sx1.T5.17.35.16.4.1.1.1.1" class="ltx_p"><span id="Sx1.T5.17.35.16.4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">BEV mAP: 41.20</span></span>
</span></span>
</span>
</td>
<td id="Sx1.T5.17.35.16.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T5.17.35.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.35.16.5.1.1" class="ltx_p" style="width:37.0pt;"><span id="Sx1.T5.17.35.16.5.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="Sx1.T5.17.35.16.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T5.17.35.16.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.35.16.6.1.1" class="ltx_p" style="width:79.7pt;"><span id="Sx1.T5.17.35.16.6.1.1.1" class="ltx_text" style="font-size:70%;">IoU={0.25, 0.5}</span></span>
</span>
</td>
<td id="Sx1.T5.17.35.16.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="Sx1.T5.17.35.16.7.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.T5.17.35.16.7.1.1" class="ltx_p" style="width:102.4pt;"><span id="Sx1.T5.17.35.16.7.1.1.1" class="ltx_text" style="font-size:70%;">Complete TJ4DRadSet</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.10409" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.10410" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.10410">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.10410" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.10411" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 13:22:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
