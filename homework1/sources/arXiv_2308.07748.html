<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.07748] Exploiting Sparsity in Automotive Radar Object Detection Networks</title><meta property="og:description" content="Having precise perception of the environment is crucial for ensuring the secure and reliable functioning of autonomous driving systems.
Radar object detection networks are one fundamental part of such systems.
CNN-baseâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Exploiting Sparsity in Automotive Radar Object Detection Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Exploiting Sparsity in Automotive Radar Object Detection Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.07748">

<!--Generated on Wed Feb 28 12:26:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Exploiting Sparsity in Automotive Radar Object Detection Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marius Lippke <sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>, Maurice Quach<sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">1</span></sup>, Sascha Braun<sup id="id13.13.id3" class="ltx_sup"><span id="id13.13.id3.1" class="ltx_text ltx_font_italic">3</span></sup>, Daniel KÃ¶hler<sup id="id14.14.id4" class="ltx_sup"><span id="id14.14.id4.1" class="ltx_text ltx_font_italic">3</span></sup>, Michael Ulrich<sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">1</span></sup>, Bastian Bischoff<sup id="id16.16.id6" class="ltx_sup"><span id="id16.16.id6.1" class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break">and Wei Yap Tan<sup id="id17.17.id7" class="ltx_sup"><span id="id17.17.id7.1" class="ltx_text ltx_font_italic">2</span></sup>
</span><span class="ltx_author_notes"><sup id="id18.18.id1" class="ltx_sup"><span id="id18.18.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Marius Lippke, Maurice Quach, Michael Ulrich and Bastian Bischoff are with Robert Bosch GmbH, Corporate Research, Germany
<br class="ltx_break"><span id="id19.19.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">marius.lippke@de.bosch.com</span><sup id="id20.20.id1" class="ltx_sup"><span id="id20.20.id1.1" class="ltx_text ltx_font_italic">2</span></sup>Marius Lippke and Wei Yap Tan are with Mannheim University of Applied Sciences, Institute of Information Technology, Germany<sup id="id21.21.id1" class="ltx_sup"><span id="id21.21.id1.1" class="ltx_text ltx_font_italic">3</span></sup> Sascha Braun and Daniel KÃ¶hler are with Robert Bosch GmbH, Cross-Domain Computing Solutions, Germany</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id22.id1" class="ltx_p"><span id="id22.id1.1" class="ltx_text">Having precise perception of the environment is crucial for ensuring the secure and reliable functioning of autonomous driving systems.
Radar object detection networks are one fundamental part of such systems.
CNN-based object detectors showed good performance in this context, but they require large compute resources.
This paper investigates sparse convolutional object detection networks, which combine powerful grid-based detection with low compute resources.
We investigate radar specific challenges and propose sparse kernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as remedies for the grid rendering and sparse backbone architectures.
We evaluate our SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by 5.89% and the previous state of the art by 4.19% in Car AP4.0.
Moreover, SKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Having precise awareness of the environment is essential to ensure secure and dependable functioning of autonomous driving as well as driver assistance systems.
Radar is an important sensor modality alongside cameras and lidar due to its price and weather robustness.
Recently, object detection networks are used for this purpose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, also in combination with other sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
Often, grid-based (CNN) radar object detection networks achieve a better performance than point-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Research in lidar object detection strengthens this observation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which can be seen as a hint for future, high-resolution radar systems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Grid-based object detectors typically render the radar point cloud to a 2D birds-eye-view (BEV) or 3D Cartesian grid before the features are processed using convolutional layers.
As a result, multiple radar points fall into the same cell or voxel, while many other cells remain empty.
Previous research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> focuses on the first issue of grid rendering, while this paper investigates the latter issue of sparsity in the grid structure.
These effects are particularly severe for sparse point clouds, such as those obtained from radar sensors, and increase with the field of view (FOV) or grid resolution.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Sparse convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and submanifold convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> are already applied for lidar object detection networks.
Similarly to lidar, radar point clouds have an inherently sparse structure and sparse CNNs can reduce computational complexity substantially.
In addition, sparse CNN processing improves the object detection performance, as will be shown in this paper.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">For this purpose, we propose a novel sparse grid rendering module <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">sparse kernel point pillars</span> (SKPP) and a novel sparse CNN block <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">dual point voxel convolutions</span> (DPVC), which can be used in the backbone of the detection network.
SKPP extends existing grid rendering techniques, such as kernel point convolution BEV rendering (KPBEV, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>), to sparse grids (SKPBEV).
Further, SKPP allows combining multiple feature extractors from different grid rendering techniques (multigrid rendering).
DPVC blocks consists of two branches, one using submanifold sparse convolutions and the other using kernel point convolutions (KPConv).
After each block, the features of the two branches are merged.
By doing so, the network can learn from the data, which processing (submanifold convolution or KPConv) is more suitable for the representation of this particular block.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The contributions of this paper are the following:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">To the best of our knowledge, this is the first investigation of sparse CNNs for radar object detection.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Novel building blocks SKPP and DVPC, which are particularly suitable for sparse radar detectors.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">An ablation study for the proposed components on the nuScenes benchmark.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2308.07748/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="275" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">
Object detectors based on sparse CNNs often rely on a single grid rendering method, which can lead an information loss.
We propose the novel SKPP-DPVCN architecture for radar-only object detection.
First, we introduce a multigrid rendering module SKPP that uses sparse KPBEV and sparse PointPillars to learn more expressive features.
In addition, we address the limitations of submanifold sparse convolution limitations to extract local and disconnected features in sparse grids by introducing DPVCN.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Radar Object Detection</span>
</h3>

<figure id="S2.F2" class="ltx_figure"><img src="/html/2308.07748/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="322" height="368" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.4.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.2.1" class="ltx_text" style="font-size:90%;">SKPBEV extracts features at each reference point <math id="S2.F2.2.1.m1.1" class="ltx_Math" alttext="N_{a}" display="inline"><semantics id="S2.F2.2.1.m1.1b"><msub id="S2.F2.2.1.m1.1.1" xref="S2.F2.2.1.m1.1.1.cmml"><mi id="S2.F2.2.1.m1.1.1.2" xref="S2.F2.2.1.m1.1.1.2.cmml">N</mi><mi id="S2.F2.2.1.m1.1.1.3" xref="S2.F2.2.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.2.1.m1.1c"><apply id="S2.F2.2.1.m1.1.1.cmml" xref="S2.F2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.2.1.m1.1.1.1.cmml" xref="S2.F2.2.1.m1.1.1">subscript</csymbol><ci id="S2.F2.2.1.m1.1.1.2.cmml" xref="S2.F2.2.1.m1.1.1.2">ğ‘</ci><ci id="S2.F2.2.1.m1.1.1.3.cmml" xref="S2.F2.2.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.2.1.m1.1d">N_{a}</annotation></semantics></math> (centers of occupied cells) using KPConvs. The features then lie on a sparse grid.</span></figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2308.07748/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">SKPP first renders the point cloud into two sparse grids based on SPP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and the proposed SKPBEV. Finally, the features of both grids are normalized and added into a single sparse grid.</span></figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Previous studies on radar object detection can be categorized based on the type of input data. Indeed, the research community has shown increasing interest in approaches based on radar spectra instead of radar point clouds. While 1D radar velocity spectra or 2D spectrograms (Doppler) are commonly employed for object classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, object detection networks typically rely on 3D radar spectra <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, incorporating range, radial velocity, and azimuth angle dimensions. These spectra are often transformed into single <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> or multiple <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> 2D projections to facilitate the application of 2D CNNs. Most literature in this field utilizes CNNs due to the natural grid-based representation of spectrum data, although exceptions exist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Alternatively, radar object detection can also use point clouds as input for detection networks. These approaches are typically divided into point-based and grid-based methods. Models such as PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and PointNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, aim to directly aggregate information from point clouds. In the radar domain, these approaches have been utilized for tasks such as classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Recent research has also explored advanced techniques for extracting features at the point level, including graph and kernel point convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> or transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
On the other hand, grid-based methods transform the point cloud into a structured grid format. This enables the utilization of CNNs to extract features effectively. For this purpose, variations of the YOLO architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> or feature pyramid networks [11], <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> have been employed in studies such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, to detect objects in birdâ€™s-eye-view (BEV) projections of the radar point cloud. In other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> has been used to learn a more abstract grid representation of the point cloud. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> presents a hybrid architecture that combines grid-based methods with point-based preprocessing to enhance feature learning from point clouds and improve detection performance.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Grid Rendering of Point Clouds</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Methods for converting irregular and sparse point clouds into regular and dense grid representations play a crucial role in 3D object detection models that utilize CNNs for feature extraction. These methods can be categorized based on the resulting grid representation and the type of encoding employed to obtain cell-wise features.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">For instance, MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and PIXOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> utilize handcrafted features like intensity, density, and height maps to generate a BEV representation from point clouds. In contrast, VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> leverage learnable feature extractors that apply simplified PointNets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to points within the same grid cell.
KPBEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> on the other hand leverages the descriptive power of kernel point convolutions to encode local information during grid rendering in a multiscale manner.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Sparse Convolution Backbones</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, a spatially sparse convolution technique is introduced, decreasing complexity of 3D convolutions by only considering active sites.
However, sparsity in deeper layers is lost, thus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> suggests submanifold sparse convolutions (SSC), which preserve the sparsity of the output and significantly increases the computation speed. Additionally, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, submanifold convolutions are employed for 3D semantic segmentation.
Building upon this approach, SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> introduces hierarchical encoders that progressively downsample the initial feature map using sparse convolutions before feeding it into a convolutional detection backbone. To better process sparse point clouds, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposes sparse PointPillars to obtain a sparse grid and further process it with a sparse convolution backbone.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Proposed Method</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Overview</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this paper, we introduce novel multigrid-rendering module Sparse Kernel Point Pillars (SKPP), that effectively encodes point features into a sparse grid by leveraging the strengths of both PointPillars and KPBEV methodologies, enhancing the scalability and performance of radar object detection networks. Furthermore, we propose Dual Point Voxel ConvNet (DPVCN), a hybrid grid-based backbone architecture, which efficiently processes the sparse grid by exploiting the sparse grid and point cloud duality.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Definitions</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We define a dense 2D grid with <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="N_{x}\times N_{y}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><msub id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.2.2.cmml">N</mi><mi id="S3.SS2.p1.1.m1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.2.3.cmml">x</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">Ã—</mo><msub id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">N</mi><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">y</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><apply id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2">ğ‘</ci><ci id="S3.SS2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3">ğ‘¥</ci></apply><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ğ‘</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">ğ‘¦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">N_{x}\times N_{y}</annotation></semantics></math> cells</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="G=I\times J" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">G</mi><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">I</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">Ã—</mo><mi id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml">J</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">ğº</ci><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><times id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">ğ¼</ci><ci id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">ğ½</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">G=I\times J</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.5" class="ltx_p">where <math id="S3.SS2.p1.2.m1.3" class="ltx_Math" alttext="I=\{0,\dots,N_{x-1}\}" display="inline"><semantics id="S3.SS2.p1.2.m1.3a"><mrow id="S3.SS2.p1.2.m1.3.3" xref="S3.SS2.p1.2.m1.3.3.cmml"><mi id="S3.SS2.p1.2.m1.3.3.3" xref="S3.SS2.p1.2.m1.3.3.3.cmml">I</mi><mo id="S3.SS2.p1.2.m1.3.3.2" xref="S3.SS2.p1.2.m1.3.3.2.cmml">=</mo><mrow id="S3.SS2.p1.2.m1.3.3.1.1" xref="S3.SS2.p1.2.m1.3.3.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.2.m1.3.3.1.1.2" xref="S3.SS2.p1.2.m1.3.3.1.2.cmml">{</mo><mn id="S3.SS2.p1.2.m1.1.1" xref="S3.SS2.p1.2.m1.1.1.cmml">0</mn><mo id="S3.SS2.p1.2.m1.3.3.1.1.3" xref="S3.SS2.p1.2.m1.3.3.1.2.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p1.2.m1.2.2" xref="S3.SS2.p1.2.m1.2.2.cmml">â€¦</mi><mo id="S3.SS2.p1.2.m1.3.3.1.1.4" xref="S3.SS2.p1.2.m1.3.3.1.2.cmml">,</mo><msub id="S3.SS2.p1.2.m1.3.3.1.1.1" xref="S3.SS2.p1.2.m1.3.3.1.1.1.cmml"><mi id="S3.SS2.p1.2.m1.3.3.1.1.1.2" xref="S3.SS2.p1.2.m1.3.3.1.1.1.2.cmml">N</mi><mrow id="S3.SS2.p1.2.m1.3.3.1.1.1.3" xref="S3.SS2.p1.2.m1.3.3.1.1.1.3.cmml"><mi id="S3.SS2.p1.2.m1.3.3.1.1.1.3.2" xref="S3.SS2.p1.2.m1.3.3.1.1.1.3.2.cmml">x</mi><mo id="S3.SS2.p1.2.m1.3.3.1.1.1.3.1" xref="S3.SS2.p1.2.m1.3.3.1.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS2.p1.2.m1.3.3.1.1.1.3.3" xref="S3.SS2.p1.2.m1.3.3.1.1.1.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S3.SS2.p1.2.m1.3.3.1.1.5" xref="S3.SS2.p1.2.m1.3.3.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m1.3b"><apply id="S3.SS2.p1.2.m1.3.3.cmml" xref="S3.SS2.p1.2.m1.3.3"><eq id="S3.SS2.p1.2.m1.3.3.2.cmml" xref="S3.SS2.p1.2.m1.3.3.2"></eq><ci id="S3.SS2.p1.2.m1.3.3.3.cmml" xref="S3.SS2.p1.2.m1.3.3.3">ğ¼</ci><set id="S3.SS2.p1.2.m1.3.3.1.2.cmml" xref="S3.SS2.p1.2.m1.3.3.1.1"><cn type="integer" id="S3.SS2.p1.2.m1.1.1.cmml" xref="S3.SS2.p1.2.m1.1.1">0</cn><ci id="S3.SS2.p1.2.m1.2.2.cmml" xref="S3.SS2.p1.2.m1.2.2">â€¦</ci><apply id="S3.SS2.p1.2.m1.3.3.1.1.1.cmml" xref="S3.SS2.p1.2.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m1.3.3.1.1.1.1.cmml" xref="S3.SS2.p1.2.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m1.3.3.1.1.1.2.cmml" xref="S3.SS2.p1.2.m1.3.3.1.1.1.2">ğ‘</ci><apply id="S3.SS2.p1.2.m1.3.3.1.1.1.3.cmml" xref="S3.SS2.p1.2.m1.3.3.1.1.1.3"><minus id="S3.SS2.p1.2.m1.3.3.1.1.1.3.1.cmml" xref="S3.SS2.p1.2.m1.3.3.1.1.1.3.1"></minus><ci id="S3.SS2.p1.2.m1.3.3.1.1.1.3.2.cmml" xref="S3.SS2.p1.2.m1.3.3.1.1.1.3.2">ğ‘¥</ci><cn type="integer" id="S3.SS2.p1.2.m1.3.3.1.1.1.3.3.cmml" xref="S3.SS2.p1.2.m1.3.3.1.1.1.3.3">1</cn></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m1.3c">I=\{0,\dots,N_{x-1}\}</annotation></semantics></math> and <math id="S3.SS2.p1.3.m2.3" class="ltx_Math" alttext="J=\{0,\dots,N_{y-1}\}" display="inline"><semantics id="S3.SS2.p1.3.m2.3a"><mrow id="S3.SS2.p1.3.m2.3.3" xref="S3.SS2.p1.3.m2.3.3.cmml"><mi id="S3.SS2.p1.3.m2.3.3.3" xref="S3.SS2.p1.3.m2.3.3.3.cmml">J</mi><mo id="S3.SS2.p1.3.m2.3.3.2" xref="S3.SS2.p1.3.m2.3.3.2.cmml">=</mo><mrow id="S3.SS2.p1.3.m2.3.3.1.1" xref="S3.SS2.p1.3.m2.3.3.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.3.m2.3.3.1.1.2" xref="S3.SS2.p1.3.m2.3.3.1.2.cmml">{</mo><mn id="S3.SS2.p1.3.m2.1.1" xref="S3.SS2.p1.3.m2.1.1.cmml">0</mn><mo id="S3.SS2.p1.3.m2.3.3.1.1.3" xref="S3.SS2.p1.3.m2.3.3.1.2.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p1.3.m2.2.2" xref="S3.SS2.p1.3.m2.2.2.cmml">â€¦</mi><mo id="S3.SS2.p1.3.m2.3.3.1.1.4" xref="S3.SS2.p1.3.m2.3.3.1.2.cmml">,</mo><msub id="S3.SS2.p1.3.m2.3.3.1.1.1" xref="S3.SS2.p1.3.m2.3.3.1.1.1.cmml"><mi id="S3.SS2.p1.3.m2.3.3.1.1.1.2" xref="S3.SS2.p1.3.m2.3.3.1.1.1.2.cmml">N</mi><mrow id="S3.SS2.p1.3.m2.3.3.1.1.1.3" xref="S3.SS2.p1.3.m2.3.3.1.1.1.3.cmml"><mi id="S3.SS2.p1.3.m2.3.3.1.1.1.3.2" xref="S3.SS2.p1.3.m2.3.3.1.1.1.3.2.cmml">y</mi><mo id="S3.SS2.p1.3.m2.3.3.1.1.1.3.1" xref="S3.SS2.p1.3.m2.3.3.1.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS2.p1.3.m2.3.3.1.1.1.3.3" xref="S3.SS2.p1.3.m2.3.3.1.1.1.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S3.SS2.p1.3.m2.3.3.1.1.5" xref="S3.SS2.p1.3.m2.3.3.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m2.3b"><apply id="S3.SS2.p1.3.m2.3.3.cmml" xref="S3.SS2.p1.3.m2.3.3"><eq id="S3.SS2.p1.3.m2.3.3.2.cmml" xref="S3.SS2.p1.3.m2.3.3.2"></eq><ci id="S3.SS2.p1.3.m2.3.3.3.cmml" xref="S3.SS2.p1.3.m2.3.3.3">ğ½</ci><set id="S3.SS2.p1.3.m2.3.3.1.2.cmml" xref="S3.SS2.p1.3.m2.3.3.1.1"><cn type="integer" id="S3.SS2.p1.3.m2.1.1.cmml" xref="S3.SS2.p1.3.m2.1.1">0</cn><ci id="S3.SS2.p1.3.m2.2.2.cmml" xref="S3.SS2.p1.3.m2.2.2">â€¦</ci><apply id="S3.SS2.p1.3.m2.3.3.1.1.1.cmml" xref="S3.SS2.p1.3.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m2.3.3.1.1.1.1.cmml" xref="S3.SS2.p1.3.m2.3.3.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m2.3.3.1.1.1.2.cmml" xref="S3.SS2.p1.3.m2.3.3.1.1.1.2">ğ‘</ci><apply id="S3.SS2.p1.3.m2.3.3.1.1.1.3.cmml" xref="S3.SS2.p1.3.m2.3.3.1.1.1.3"><minus id="S3.SS2.p1.3.m2.3.3.1.1.1.3.1.cmml" xref="S3.SS2.p1.3.m2.3.3.1.1.1.3.1"></minus><ci id="S3.SS2.p1.3.m2.3.3.1.1.1.3.2.cmml" xref="S3.SS2.p1.3.m2.3.3.1.1.1.3.2">ğ‘¦</ci><cn type="integer" id="S3.SS2.p1.3.m2.3.3.1.1.1.3.3.cmml" xref="S3.SS2.p1.3.m2.3.3.1.1.1.3.3">1</cn></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m2.3c">J=\{0,\dots,N_{y-1}\}</annotation></semantics></math>.
Subsequently, we define a sparse grid <math id="S3.SS2.p1.4.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p1.4.m3.1a"><mi id="S3.SS2.p1.4.m3.1.1" xref="S3.SS2.p1.4.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m3.1b"><ci id="S3.SS2.p1.4.m3.1.1.cmml" xref="S3.SS2.p1.4.m3.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m3.1c">S</annotation></semantics></math> where <math id="S3.SS2.p1.5.m4.1" class="ltx_Math" alttext="S\subseteq G" display="inline"><semantics id="S3.SS2.p1.5.m4.1a"><mrow id="S3.SS2.p1.5.m4.1.1" xref="S3.SS2.p1.5.m4.1.1.cmml"><mi id="S3.SS2.p1.5.m4.1.1.2" xref="S3.SS2.p1.5.m4.1.1.2.cmml">S</mi><mo id="S3.SS2.p1.5.m4.1.1.1" xref="S3.SS2.p1.5.m4.1.1.1.cmml">âŠ†</mo><mi id="S3.SS2.p1.5.m4.1.1.3" xref="S3.SS2.p1.5.m4.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m4.1b"><apply id="S3.SS2.p1.5.m4.1.1.cmml" xref="S3.SS2.p1.5.m4.1.1"><subset id="S3.SS2.p1.5.m4.1.1.1.cmml" xref="S3.SS2.p1.5.m4.1.1.1"></subset><ci id="S3.SS2.p1.5.m4.1.1.2.cmml" xref="S3.SS2.p1.5.m4.1.1.2">ğ‘†</ci><ci id="S3.SS2.p1.5.m4.1.1.3.cmml" xref="S3.SS2.p1.5.m4.1.1.3">ğº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m4.1c">S\subseteq G</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To define features over a sparse grid, we adopt the notation</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="f_{S}\colon S\to\mathbb{R}^{d}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2.cmml">f</mi><mi id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml">S</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">:</mo><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">S</mi><mo stretchy="false" id="S3.E2.m1.1.1.3.1" xref="S3.E2.m1.1.1.3.1.cmml">â†’</mo><msup id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml">â„</mi><mi id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml">d</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><ci id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1">:</ci><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">ğ‘“</ci><ci id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3">ğ‘†</ci></apply><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><ci id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1">â†’</ci><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">ğ‘†</ci><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3">superscript</csymbol><ci id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2">â„</ci><ci id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">f_{S}\colon S\to\mathbb{R}^{d}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.8" class="ltx_p">to represent <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">d</annotation></semantics></math>-dimensional features over a sparse grid <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">S</annotation></semantics></math>.
For instance, <math id="S3.SS2.p3.3.m3.2" class="ltx_Math" alttext="f(i,j)" display="inline"><semantics id="S3.SS2.p3.3.m3.2a"><mrow id="S3.SS2.p3.3.m3.2.3" xref="S3.SS2.p3.3.m3.2.3.cmml"><mi id="S3.SS2.p3.3.m3.2.3.2" xref="S3.SS2.p3.3.m3.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.3.m3.2.3.1" xref="S3.SS2.p3.3.m3.2.3.1.cmml">â€‹</mo><mrow id="S3.SS2.p3.3.m3.2.3.3.2" xref="S3.SS2.p3.3.m3.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p3.3.m3.2.3.3.2.1" xref="S3.SS2.p3.3.m3.2.3.3.1.cmml">(</mo><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">i</mi><mo id="S3.SS2.p3.3.m3.2.3.3.2.2" xref="S3.SS2.p3.3.m3.2.3.3.1.cmml">,</mo><mi id="S3.SS2.p3.3.m3.2.2" xref="S3.SS2.p3.3.m3.2.2.cmml">j</mi><mo stretchy="false" id="S3.SS2.p3.3.m3.2.3.3.2.3" xref="S3.SS2.p3.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.2b"><apply id="S3.SS2.p3.3.m3.2.3.cmml" xref="S3.SS2.p3.3.m3.2.3"><times id="S3.SS2.p3.3.m3.2.3.1.cmml" xref="S3.SS2.p3.3.m3.2.3.1"></times><ci id="S3.SS2.p3.3.m3.2.3.2.cmml" xref="S3.SS2.p3.3.m3.2.3.2">ğ‘“</ci><interval closure="open" id="S3.SS2.p3.3.m3.2.3.3.1.cmml" xref="S3.SS2.p3.3.m3.2.3.3.2"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">ğ‘–</ci><ci id="S3.SS2.p3.3.m3.2.2.cmml" xref="S3.SS2.p3.3.m3.2.2">ğ‘—</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.2c">f(i,j)</annotation></semantics></math> represents the features at occupied cell <math id="S3.SS2.p3.4.m4.2" class="ltx_Math" alttext="(i,j)\in S" display="inline"><semantics id="S3.SS2.p3.4.m4.2a"><mrow id="S3.SS2.p3.4.m4.2.3" xref="S3.SS2.p3.4.m4.2.3.cmml"><mrow id="S3.SS2.p3.4.m4.2.3.2.2" xref="S3.SS2.p3.4.m4.2.3.2.1.cmml"><mo stretchy="false" id="S3.SS2.p3.4.m4.2.3.2.2.1" xref="S3.SS2.p3.4.m4.2.3.2.1.cmml">(</mo><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">i</mi><mo id="S3.SS2.p3.4.m4.2.3.2.2.2" xref="S3.SS2.p3.4.m4.2.3.2.1.cmml">,</mo><mi id="S3.SS2.p3.4.m4.2.2" xref="S3.SS2.p3.4.m4.2.2.cmml">j</mi><mo stretchy="false" id="S3.SS2.p3.4.m4.2.3.2.2.3" xref="S3.SS2.p3.4.m4.2.3.2.1.cmml">)</mo></mrow><mo id="S3.SS2.p3.4.m4.2.3.1" xref="S3.SS2.p3.4.m4.2.3.1.cmml">âˆˆ</mo><mi id="S3.SS2.p3.4.m4.2.3.3" xref="S3.SS2.p3.4.m4.2.3.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.2b"><apply id="S3.SS2.p3.4.m4.2.3.cmml" xref="S3.SS2.p3.4.m4.2.3"><in id="S3.SS2.p3.4.m4.2.3.1.cmml" xref="S3.SS2.p3.4.m4.2.3.1"></in><interval closure="open" id="S3.SS2.p3.4.m4.2.3.2.1.cmml" xref="S3.SS2.p3.4.m4.2.3.2.2"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">ğ‘–</ci><ci id="S3.SS2.p3.4.m4.2.2.cmml" xref="S3.SS2.p3.4.m4.2.2">ğ‘—</ci></interval><ci id="S3.SS2.p3.4.m4.2.3.3.cmml" xref="S3.SS2.p3.4.m4.2.3.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.2c">(i,j)\in S</annotation></semantics></math>.
Similarly, we adopt the notation <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="f_{G}" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><msub id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml">f</mi><mi id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2">ğ‘“</ci><ci id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">ğº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">f_{G}</annotation></semantics></math> when operating over a dense grid <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mi id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">G</annotation></semantics></math>.
We assume in this paper that <math id="S3.SS2.p3.7.m7.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p3.7.m7.1a"><mi id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><ci id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">S</annotation></semantics></math> contains the occupied cells in <math id="S3.SS2.p3.8.m8.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS2.p3.8.m8.1a"><mi id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><ci id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">G</annotation></semantics></math> given an input point cloud.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Multigrid Rendering: Sparse Kernel Point Pillars (SKPP)</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Sparse grids are an efficient alternative to dense grids when processing sparse point clouds.
Typical grid projection approaches such as PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> projects the point cloud onto the dense grid <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">G</annotation></semantics></math>, where each cell represents a pillar. Within each grid-cell, PointPillars aggregates the features of the points using a PointNet-based network. However, this feature aggregation process may lead to information loss, as the original spatial arrangement of the points within a cell is discarded, potentially impacting the modelâ€™s ability to capture fine-grained details.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">More recent grid-rendering methods such as KPBEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> improves information flow between points in order to learn more expressive features of local neighborhoods.
Specifically, KPBEV includes points from adjacent cells and uses the descriptive capabilities of Kernel Point Convolution (KPConv), which enables it to better capture local context.
For each occupied grid cell, an reference point placed in the center of the grid cell.
Then, for each reference point, the neighboring points in the input point cloud are retrieved and their features are aggregated with a KPConv.
Finally, the features are back scattered into a dense grid <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">G</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.3" class="ltx_p">However, <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">G</annotation></semantics></math> scales linearly with the number of cells and thus scales quadratically with the distance.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, a sparse formulation of PointPillars (SPP) elides the feature back scattering step to <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">G</annotation></semantics></math> and uses a sparse grid <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mi id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><ci id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">S</annotation></semantics></math> to scale lineraly with the number of occupied cells.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.2" class="ltx_p">The proposed SKPBEV elides the feature back scattering step of the KPBEV to <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">G</annotation></semantics></math> as well and uses a sparse grid <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mi id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">S</annotation></semantics></math> instead.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2308.07748/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="257" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Structure of a DPVC block. The sparse input grid is processed by a branch of submanifold sparse convolution layer (left) and another branch with KPConvs. Each convolution layer in both branches is followed by a batch normalisation and ReLU activation function. The normalized features for each branch are added, resulting in a single sparse grid as output.</span></figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2308.07748/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="163" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.6.3.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.4.2" class="ltx_text" style="font-size:90%;">The proposed SKPP-DPVCN first renders the input point cloud into a sparse grid <math id="S3.F5.3.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.F5.3.1.m1.1b"><mi id="S3.F5.3.1.m1.1.1" xref="S3.F5.3.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.F5.3.1.m1.1c"><ci id="S3.F5.3.1.m1.1.1.cmml" xref="S3.F5.3.1.m1.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.3.1.m1.1d">S</annotation></semantics></math> with features <math id="S3.F5.4.2.m2.2" class="ltx_Math" alttext="f_{S,\operatorname{SKPP}}" display="inline"><semantics id="S3.F5.4.2.m2.2b"><msub id="S3.F5.4.2.m2.2.3" xref="S3.F5.4.2.m2.2.3.cmml"><mi id="S3.F5.4.2.m2.2.3.2" xref="S3.F5.4.2.m2.2.3.2.cmml">f</mi><mrow id="S3.F5.4.2.m2.2.2.2.4" xref="S3.F5.4.2.m2.2.2.2.3.cmml"><mi id="S3.F5.4.2.m2.1.1.1.1" xref="S3.F5.4.2.m2.1.1.1.1.cmml">S</mi><mo id="S3.F5.4.2.m2.2.2.2.4.1" xref="S3.F5.4.2.m2.2.2.2.3.cmml">,</mo><mi id="S3.F5.4.2.m2.2.2.2.2" xref="S3.F5.4.2.m2.2.2.2.2.cmml">SKPP</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F5.4.2.m2.2c"><apply id="S3.F5.4.2.m2.2.3.cmml" xref="S3.F5.4.2.m2.2.3"><csymbol cd="ambiguous" id="S3.F5.4.2.m2.2.3.1.cmml" xref="S3.F5.4.2.m2.2.3">subscript</csymbol><ci id="S3.F5.4.2.m2.2.3.2.cmml" xref="S3.F5.4.2.m2.2.3.2">ğ‘“</ci><list id="S3.F5.4.2.m2.2.2.2.3.cmml" xref="S3.F5.4.2.m2.2.2.2.4"><ci id="S3.F5.4.2.m2.1.1.1.1.cmml" xref="S3.F5.4.2.m2.1.1.1.1">ğ‘†</ci><ci id="S3.F5.4.2.m2.2.2.2.2.cmml" xref="S3.F5.4.2.m2.2.2.2.2">SKPP</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.4.2.m2.2d">f_{S,\operatorname{SKPP}}</annotation></semantics></math>. Then, the features are processed by the DPVCN backbone to extract feature maps where each DPVC block downsamples by a factor of two. The FPN blocks upsample the feature map and apply a SSC block. Convolutional detection heads for different classes predict OBB proposals. Additionally, NMS is applied as a post-processing step.</span></figcaption>
</figure>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.3" class="ltx_p">Although the two methods SKPBEV and SPP are fundamentally different in the feature aggregation step, the features <math id="S3.SS3.p5.1.m1.2" class="ltx_Math" alttext="f_{S,\operatorname{SKPBEV}}" display="inline"><semantics id="S3.SS3.p5.1.m1.2a"><msub id="S3.SS3.p5.1.m1.2.3" xref="S3.SS3.p5.1.m1.2.3.cmml"><mi id="S3.SS3.p5.1.m1.2.3.2" xref="S3.SS3.p5.1.m1.2.3.2.cmml">f</mi><mrow id="S3.SS3.p5.1.m1.2.2.2.4" xref="S3.SS3.p5.1.m1.2.2.2.3.cmml"><mi id="S3.SS3.p5.1.m1.1.1.1.1" xref="S3.SS3.p5.1.m1.1.1.1.1.cmml">S</mi><mo id="S3.SS3.p5.1.m1.2.2.2.4.1" xref="S3.SS3.p5.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2" xref="S3.SS3.p5.1.m1.2.2.2.2.cmml">SKPBEV</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.2b"><apply id="S3.SS3.p5.1.m1.2.3.cmml" xref="S3.SS3.p5.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.2.3.1.cmml" xref="S3.SS3.p5.1.m1.2.3">subscript</csymbol><ci id="S3.SS3.p5.1.m1.2.3.2.cmml" xref="S3.SS3.p5.1.m1.2.3.2">ğ‘“</ci><list id="S3.SS3.p5.1.m1.2.2.2.3.cmml" xref="S3.SS3.p5.1.m1.2.2.2.4"><ci id="S3.SS3.p5.1.m1.1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1">ğ‘†</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2">SKPBEV</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.2c">f_{S,\operatorname{SKPBEV}}</annotation></semantics></math> lie over the same sparse grid <math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><mi id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">S</annotation></semantics></math> as <math id="S3.SS3.p5.3.m3.2" class="ltx_Math" alttext="f_{S,\operatorname{SPP}}" display="inline"><semantics id="S3.SS3.p5.3.m3.2a"><msub id="S3.SS3.p5.3.m3.2.3" xref="S3.SS3.p5.3.m3.2.3.cmml"><mi id="S3.SS3.p5.3.m3.2.3.2" xref="S3.SS3.p5.3.m3.2.3.2.cmml">f</mi><mrow id="S3.SS3.p5.3.m3.2.2.2.4" xref="S3.SS3.p5.3.m3.2.2.2.3.cmml"><mi id="S3.SS3.p5.3.m3.1.1.1.1" xref="S3.SS3.p5.3.m3.1.1.1.1.cmml">S</mi><mo id="S3.SS3.p5.3.m3.2.2.2.4.1" xref="S3.SS3.p5.3.m3.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p5.3.m3.2.2.2.2" xref="S3.SS3.p5.3.m3.2.2.2.2.cmml">SPP</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.2b"><apply id="S3.SS3.p5.3.m3.2.3.cmml" xref="S3.SS3.p5.3.m3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.2.3.1.cmml" xref="S3.SS3.p5.3.m3.2.3">subscript</csymbol><ci id="S3.SS3.p5.3.m3.2.3.2.cmml" xref="S3.SS3.p5.3.m3.2.3.2">ğ‘“</ci><list id="S3.SS3.p5.3.m3.2.2.2.3.cmml" xref="S3.SS3.p5.3.m3.2.2.2.4"><ci id="S3.SS3.p5.3.m3.1.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1">ğ‘†</ci><ci id="S3.SS3.p5.3.m3.2.2.2.2.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2">SPP</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.2c">f_{S,\operatorname{SPP}}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.2" class="ltx_p">We first propose a general multi-grid rendering framework as follows.
Let <math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="F_{S}" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><msub id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml"><mi id="S3.SS3.p6.1.m1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.2.cmml">F</mi><mi id="S3.SS3.p6.1.m1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><apply id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.1.m1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p6.1.m1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.2">ğ¹</ci><ci id="S3.SS3.p6.1.m1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">F_{S}</annotation></semantics></math> be a set of features from grid rendering methods that render a given point cloud into a corresponding sparse grid <math id="S3.SS3.p6.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS3.p6.2.m2.1a"><mi id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><ci id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">S</annotation></semantics></math>.
Then, we formulate multi-grid (MG) rendering as follows</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="f_{S,MG}=\operatorname*{\scalerel*{\square}{\sum}}_{f_{S}\in F_{S}}f_{S}" display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.3" xref="S3.E3.m1.2.3.cmml"><msub id="S3.E3.m1.2.3.2" xref="S3.E3.m1.2.3.2.cmml"><mi id="S3.E3.m1.2.3.2.2" xref="S3.E3.m1.2.3.2.2.cmml">f</mi><mrow id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">S</mi><mo id="S3.E3.m1.2.2.2.2.2" xref="S3.E3.m1.2.2.2.3.cmml">,</mo><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.1.cmml"><mi id="S3.E3.m1.2.2.2.2.1.2" xref="S3.E3.m1.2.2.2.2.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.E3.m1.2.2.2.2.1.3" xref="S3.E3.m1.2.2.2.2.1.3.cmml">G</mi></mrow></mrow></msub><mo id="S3.E3.m1.2.3.1" xref="S3.E3.m1.2.3.1.cmml">=</mo><mrow id="S3.E3.m1.2.3.3" xref="S3.E3.m1.2.3.3.cmml"><munder id="S3.E3.m1.2.3.3.1" xref="S3.E3.m1.2.3.3.1.cmml"><mrow id="S3.E3.m1.2.3.3.1.2" xref="S3.E3.m1.2.3.3.1.2.cmml"><mrow id="S3.E3.m1.2.3.3.1.2.2" xref="S3.E3.m1.2.3.3.1.2.2.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.E3.m1.2.3.3.1.2.2.2" xref="S3.E3.m1.2.3.3.1.2.2.2b.cmml"><mtext id="S3.E3.m1.2.3.3.1.2.2.2a" xref="S3.E3.m1.2.3.3.1.2.2.2b.cmml">\scalerel</mtext></merror><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.2.3.3.1.2.2.1" xref="S3.E3.m1.2.3.3.1.2.2.1.cmml">âˆ—</mo><mi mathvariant="normal" id="S3.E3.m1.2.3.3.1.2.2.3" xref="S3.E3.m1.2.3.3.1.2.2.3.cmml">â–¡</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.3.3.1.2.1" xref="S3.E3.m1.2.3.3.1.2.1.cmml">â€‹</mo><mo movablelimits="false" id="S3.E3.m1.2.3.3.1.2.3" xref="S3.E3.m1.2.3.3.1.2.3.cmml">âˆ‘</mo></mrow><mrow id="S3.E3.m1.2.3.3.1.3" xref="S3.E3.m1.2.3.3.1.3.cmml"><msub id="S3.E3.m1.2.3.3.1.3.2" xref="S3.E3.m1.2.3.3.1.3.2.cmml"><mi id="S3.E3.m1.2.3.3.1.3.2.2" xref="S3.E3.m1.2.3.3.1.3.2.2.cmml">f</mi><mi id="S3.E3.m1.2.3.3.1.3.2.3" xref="S3.E3.m1.2.3.3.1.3.2.3.cmml">S</mi></msub><mo id="S3.E3.m1.2.3.3.1.3.1" xref="S3.E3.m1.2.3.3.1.3.1.cmml">âˆˆ</mo><msub id="S3.E3.m1.2.3.3.1.3.3" xref="S3.E3.m1.2.3.3.1.3.3.cmml"><mi id="S3.E3.m1.2.3.3.1.3.3.2" xref="S3.E3.m1.2.3.3.1.3.3.2.cmml">F</mi><mi id="S3.E3.m1.2.3.3.1.3.3.3" xref="S3.E3.m1.2.3.3.1.3.3.3.cmml">S</mi></msub></mrow></munder><mo id="S3.E3.m1.2.3.3a" xref="S3.E3.m1.2.3.3.cmml">â¡</mo><msub id="S3.E3.m1.2.3.3.2" xref="S3.E3.m1.2.3.3.2.cmml"><mi id="S3.E3.m1.2.3.3.2.2" xref="S3.E3.m1.2.3.3.2.2.cmml">f</mi><mi id="S3.E3.m1.2.3.3.2.3" xref="S3.E3.m1.2.3.3.2.3.cmml">S</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.3.cmml" xref="S3.E3.m1.2.3"><eq id="S3.E3.m1.2.3.1.cmml" xref="S3.E3.m1.2.3.1"></eq><apply id="S3.E3.m1.2.3.2.cmml" xref="S3.E3.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.3.2.1.cmml" xref="S3.E3.m1.2.3.2">subscript</csymbol><ci id="S3.E3.m1.2.3.2.2.cmml" xref="S3.E3.m1.2.3.2.2">ğ‘“</ci><list id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2"><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">ğ‘†</ci><apply id="S3.E3.m1.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1"><times id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1"></times><ci id="S3.E3.m1.2.2.2.2.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.2">ğ‘€</ci><ci id="S3.E3.m1.2.2.2.2.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.3">ğº</ci></apply></list></apply><apply id="S3.E3.m1.2.3.3.cmml" xref="S3.E3.m1.2.3.3"><apply id="S3.E3.m1.2.3.3.1.cmml" xref="S3.E3.m1.2.3.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.3.3.1.1.cmml" xref="S3.E3.m1.2.3.3.1">subscript</csymbol><apply id="S3.E3.m1.2.3.3.1.2.cmml" xref="S3.E3.m1.2.3.3.1.2"><times id="S3.E3.m1.2.3.3.1.2.1.cmml" xref="S3.E3.m1.2.3.3.1.2.1"></times><apply id="S3.E3.m1.2.3.3.1.2.2.cmml" xref="S3.E3.m1.2.3.3.1.2.2"><times id="S3.E3.m1.2.3.3.1.2.2.1.cmml" xref="S3.E3.m1.2.3.3.1.2.2.1"></times><ci id="S3.E3.m1.2.3.3.1.2.2.2b.cmml" xref="S3.E3.m1.2.3.3.1.2.2.2"><merror class="ltx_ERROR undefined undefined" id="S3.E3.m1.2.3.3.1.2.2.2.cmml" xref="S3.E3.m1.2.3.3.1.2.2.2"><mtext id="S3.E3.m1.2.3.3.1.2.2.2a.cmml" xref="S3.E3.m1.2.3.3.1.2.2.2">\scalerel</mtext></merror></ci><ci id="S3.E3.m1.2.3.3.1.2.2.3.cmml" xref="S3.E3.m1.2.3.3.1.2.2.3">â–¡</ci></apply><sum id="S3.E3.m1.2.3.3.1.2.3.cmml" xref="S3.E3.m1.2.3.3.1.2.3"></sum></apply><apply id="S3.E3.m1.2.3.3.1.3.cmml" xref="S3.E3.m1.2.3.3.1.3"><in id="S3.E3.m1.2.3.3.1.3.1.cmml" xref="S3.E3.m1.2.3.3.1.3.1"></in><apply id="S3.E3.m1.2.3.3.1.3.2.cmml" xref="S3.E3.m1.2.3.3.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.3.3.1.3.2.1.cmml" xref="S3.E3.m1.2.3.3.1.3.2">subscript</csymbol><ci id="S3.E3.m1.2.3.3.1.3.2.2.cmml" xref="S3.E3.m1.2.3.3.1.3.2.2">ğ‘“</ci><ci id="S3.E3.m1.2.3.3.1.3.2.3.cmml" xref="S3.E3.m1.2.3.3.1.3.2.3">ğ‘†</ci></apply><apply id="S3.E3.m1.2.3.3.1.3.3.cmml" xref="S3.E3.m1.2.3.3.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.3.3.1.3.3.1.cmml" xref="S3.E3.m1.2.3.3.1.3.3">subscript</csymbol><ci id="S3.E3.m1.2.3.3.1.3.3.2.cmml" xref="S3.E3.m1.2.3.3.1.3.3.2">ğ¹</ci><ci id="S3.E3.m1.2.3.3.1.3.3.3.cmml" xref="S3.E3.m1.2.3.3.1.3.3.3">ğ‘†</ci></apply></apply></apply><apply id="S3.E3.m1.2.3.3.2.cmml" xref="S3.E3.m1.2.3.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.3.3.2.1.cmml" xref="S3.E3.m1.2.3.3.2">subscript</csymbol><ci id="S3.E3.m1.2.3.3.2.2.cmml" xref="S3.E3.m1.2.3.3.2.2">ğ‘“</ci><ci id="S3.E3.m1.2.3.3.2.3.cmml" xref="S3.E3.m1.2.3.3.2.3">ğ‘†</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">f_{S,MG}=\operatorname*{\scalerel*{\square}{\sum}}_{f_{S}\in F_{S}}f_{S}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p6.5" class="ltx_p">where <math id="S3.SS3.p6.3.m1.1" class="ltx_Math" alttext="\operatorname*{\scalerel*{\square}{\sum}}" display="inline"><semantics id="S3.SS3.p6.3.m1.1a"><mrow id="S3.SS3.p6.3.m1.1.1" xref="S3.SS3.p6.3.m1.1.1.cmml"><mrow id="S3.SS3.p6.3.m1.1.1.2" xref="S3.SS3.p6.3.m1.1.1.2.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS3.p6.3.m1.1.1.2.2" xref="S3.SS3.p6.3.m1.1.1.2.2b.cmml"><mtext id="S3.SS3.p6.3.m1.1.1.2.2a" xref="S3.SS3.p6.3.m1.1.1.2.2b.cmml">\scalerel</mtext></merror><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p6.3.m1.1.1.2.1" xref="S3.SS3.p6.3.m1.1.1.2.1.cmml">âˆ—</mo><mi mathvariant="normal" id="S3.SS3.p6.3.m1.1.1.2.3" xref="S3.SS3.p6.3.m1.1.1.2.3.cmml">â–¡</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS3.p6.3.m1.1.1.1" xref="S3.SS3.p6.3.m1.1.1.1.cmml">â€‹</mo><mo id="S3.SS3.p6.3.m1.1.1.3" xref="S3.SS3.p6.3.m1.1.1.3.cmml">âˆ‘</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.3.m1.1b"><apply id="S3.SS3.p6.3.m1.1.1.cmml" xref="S3.SS3.p6.3.m1.1.1"><times id="S3.SS3.p6.3.m1.1.1.1.cmml" xref="S3.SS3.p6.3.m1.1.1.1"></times><apply id="S3.SS3.p6.3.m1.1.1.2.cmml" xref="S3.SS3.p6.3.m1.1.1.2"><times id="S3.SS3.p6.3.m1.1.1.2.1.cmml" xref="S3.SS3.p6.3.m1.1.1.2.1"></times><ci id="S3.SS3.p6.3.m1.1.1.2.2b.cmml" xref="S3.SS3.p6.3.m1.1.1.2.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS3.p6.3.m1.1.1.2.2.cmml" xref="S3.SS3.p6.3.m1.1.1.2.2"><mtext id="S3.SS3.p6.3.m1.1.1.2.2a.cmml" xref="S3.SS3.p6.3.m1.1.1.2.2">\scalerel</mtext></merror></ci><ci id="S3.SS3.p6.3.m1.1.1.2.3.cmml" xref="S3.SS3.p6.3.m1.1.1.2.3">â–¡</ci></apply><sum id="S3.SS3.p6.3.m1.1.1.3.cmml" xref="S3.SS3.p6.3.m1.1.1.3"></sum></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.3.m1.1c">\operatorname*{\scalerel*{\square}{\sum}}</annotation></semantics></math> is an aggregation operator.
We note that typical grid rendering methods are a special case of multi-grid rendering where <math id="S3.SS3.p6.4.m2.1" class="ltx_Math" alttext="|F_{S}|=1" display="inline"><semantics id="S3.SS3.p6.4.m2.1a"><mrow id="S3.SS3.p6.4.m2.1.1" xref="S3.SS3.p6.4.m2.1.1.cmml"><mrow id="S3.SS3.p6.4.m2.1.1.1.1" xref="S3.SS3.p6.4.m2.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p6.4.m2.1.1.1.1.2" xref="S3.SS3.p6.4.m2.1.1.1.2.1.cmml">|</mo><msub id="S3.SS3.p6.4.m2.1.1.1.1.1" xref="S3.SS3.p6.4.m2.1.1.1.1.1.cmml"><mi id="S3.SS3.p6.4.m2.1.1.1.1.1.2" xref="S3.SS3.p6.4.m2.1.1.1.1.1.2.cmml">F</mi><mi id="S3.SS3.p6.4.m2.1.1.1.1.1.3" xref="S3.SS3.p6.4.m2.1.1.1.1.1.3.cmml">S</mi></msub><mo stretchy="false" id="S3.SS3.p6.4.m2.1.1.1.1.3" xref="S3.SS3.p6.4.m2.1.1.1.2.1.cmml">|</mo></mrow><mo id="S3.SS3.p6.4.m2.1.1.2" xref="S3.SS3.p6.4.m2.1.1.2.cmml">=</mo><mn id="S3.SS3.p6.4.m2.1.1.3" xref="S3.SS3.p6.4.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.4.m2.1b"><apply id="S3.SS3.p6.4.m2.1.1.cmml" xref="S3.SS3.p6.4.m2.1.1"><eq id="S3.SS3.p6.4.m2.1.1.2.cmml" xref="S3.SS3.p6.4.m2.1.1.2"></eq><apply id="S3.SS3.p6.4.m2.1.1.1.2.cmml" xref="S3.SS3.p6.4.m2.1.1.1.1"><abs id="S3.SS3.p6.4.m2.1.1.1.2.1.cmml" xref="S3.SS3.p6.4.m2.1.1.1.1.2"></abs><apply id="S3.SS3.p6.4.m2.1.1.1.1.1.cmml" xref="S3.SS3.p6.4.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.4.m2.1.1.1.1.1.1.cmml" xref="S3.SS3.p6.4.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p6.4.m2.1.1.1.1.1.2.cmml" xref="S3.SS3.p6.4.m2.1.1.1.1.1.2">ğ¹</ci><ci id="S3.SS3.p6.4.m2.1.1.1.1.1.3.cmml" xref="S3.SS3.p6.4.m2.1.1.1.1.1.3">ğ‘†</ci></apply></apply><cn type="integer" id="S3.SS3.p6.4.m2.1.1.3.cmml" xref="S3.SS3.p6.4.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.4.m2.1c">|F_{S}|=1</annotation></semantics></math> and <math id="S3.SS3.p6.5.m3.1" class="ltx_Math" alttext="\operatorname*{\scalerel*{\square}{\sum}}=\sum" display="inline"><semantics id="S3.SS3.p6.5.m3.1a"><mrow id="S3.SS3.p6.5.m3.1.1" xref="S3.SS3.p6.5.m3.1.1.cmml"><mrow id="S3.SS3.p6.5.m3.1.1.2" xref="S3.SS3.p6.5.m3.1.1.2.cmml"><mrow id="S3.SS3.p6.5.m3.1.1.2.2" xref="S3.SS3.p6.5.m3.1.1.2.2.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS3.p6.5.m3.1.1.2.2.2" xref="S3.SS3.p6.5.m3.1.1.2.2.2b.cmml"><mtext id="S3.SS3.p6.5.m3.1.1.2.2.2a" xref="S3.SS3.p6.5.m3.1.1.2.2.2b.cmml">\scalerel</mtext></merror><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p6.5.m3.1.1.2.2.1" xref="S3.SS3.p6.5.m3.1.1.2.2.1.cmml">âˆ—</mo><mi mathvariant="normal" id="S3.SS3.p6.5.m3.1.1.2.2.3" xref="S3.SS3.p6.5.m3.1.1.2.2.3.cmml">â–¡</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS3.p6.5.m3.1.1.2.1" xref="S3.SS3.p6.5.m3.1.1.2.1.cmml">â€‹</mo><mo id="S3.SS3.p6.5.m3.1.1.2.3" xref="S3.SS3.p6.5.m3.1.1.2.3.cmml">âˆ‘</mo></mrow><mo lspace="0.111em" rspace="0.111em" id="S3.SS3.p6.5.m3.1.1.1" xref="S3.SS3.p6.5.m3.1.1.1.cmml">=</mo><mo id="S3.SS3.p6.5.m3.1.1.3" xref="S3.SS3.p6.5.m3.1.1.3.cmml">âˆ‘</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.5.m3.1b"><apply id="S3.SS3.p6.5.m3.1.1.cmml" xref="S3.SS3.p6.5.m3.1.1"><eq id="S3.SS3.p6.5.m3.1.1.1.cmml" xref="S3.SS3.p6.5.m3.1.1.1"></eq><apply id="S3.SS3.p6.5.m3.1.1.2.cmml" xref="S3.SS3.p6.5.m3.1.1.2"><times id="S3.SS3.p6.5.m3.1.1.2.1.cmml" xref="S3.SS3.p6.5.m3.1.1.2.1"></times><apply id="S3.SS3.p6.5.m3.1.1.2.2.cmml" xref="S3.SS3.p6.5.m3.1.1.2.2"><times id="S3.SS3.p6.5.m3.1.1.2.2.1.cmml" xref="S3.SS3.p6.5.m3.1.1.2.2.1"></times><ci id="S3.SS3.p6.5.m3.1.1.2.2.2b.cmml" xref="S3.SS3.p6.5.m3.1.1.2.2.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS3.p6.5.m3.1.1.2.2.2.cmml" xref="S3.SS3.p6.5.m3.1.1.2.2.2"><mtext id="S3.SS3.p6.5.m3.1.1.2.2.2a.cmml" xref="S3.SS3.p6.5.m3.1.1.2.2.2">\scalerel</mtext></merror></ci><ci id="S3.SS3.p6.5.m3.1.1.2.2.3.cmml" xref="S3.SS3.p6.5.m3.1.1.2.2.3">â–¡</ci></apply><sum id="S3.SS3.p6.5.m3.1.1.2.3.cmml" xref="S3.SS3.p6.5.m3.1.1.2.3"></sum></apply><sum id="S3.SS3.p6.5.m3.1.1.3.cmml" xref="S3.SS3.p6.5.m3.1.1.3"></sum></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.5.m3.1c">\operatorname*{\scalerel*{\square}{\sum}}=\sum</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.2" class="ltx_p">To further improve information flow from point clouds to sparse grids, we propose a novel sparse multi-grid rendering module Sparse Kernel Point Pillars (SKPP), based on the proposed SKPBEV and SPP to render both features into the same sparse grid <math id="S3.SS3.p7.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS3.p7.1.m1.1a"><mi id="S3.SS3.p7.1.m1.1.1" xref="S3.SS3.p7.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.1.m1.1b"><ci id="S3.SS3.p7.1.m1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.1.m1.1c">S</annotation></semantics></math>.
Specifically, SKPP is an instance of multi-grid rendering with resulting features <math id="S3.SS3.p7.2.m2.2" class="ltx_Math" alttext="f_{S,\operatorname{SKPP}}" display="inline"><semantics id="S3.SS3.p7.2.m2.2a"><msub id="S3.SS3.p7.2.m2.2.3" xref="S3.SS3.p7.2.m2.2.3.cmml"><mi id="S3.SS3.p7.2.m2.2.3.2" xref="S3.SS3.p7.2.m2.2.3.2.cmml">f</mi><mrow id="S3.SS3.p7.2.m2.2.2.2.4" xref="S3.SS3.p7.2.m2.2.2.2.3.cmml"><mi id="S3.SS3.p7.2.m2.1.1.1.1" xref="S3.SS3.p7.2.m2.1.1.1.1.cmml">S</mi><mo id="S3.SS3.p7.2.m2.2.2.2.4.1" xref="S3.SS3.p7.2.m2.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p7.2.m2.2.2.2.2" xref="S3.SS3.p7.2.m2.2.2.2.2.cmml">SKPP</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.2.m2.2b"><apply id="S3.SS3.p7.2.m2.2.3.cmml" xref="S3.SS3.p7.2.m2.2.3"><csymbol cd="ambiguous" id="S3.SS3.p7.2.m2.2.3.1.cmml" xref="S3.SS3.p7.2.m2.2.3">subscript</csymbol><ci id="S3.SS3.p7.2.m2.2.3.2.cmml" xref="S3.SS3.p7.2.m2.2.3.2">ğ‘“</ci><list id="S3.SS3.p7.2.m2.2.2.2.3.cmml" xref="S3.SS3.p7.2.m2.2.2.2.4"><ci id="S3.SS3.p7.2.m2.1.1.1.1.cmml" xref="S3.SS3.p7.2.m2.1.1.1.1">ğ‘†</ci><ci id="S3.SS3.p7.2.m2.2.2.2.2.cmml" xref="S3.SS3.p7.2.m2.2.2.2.2">SKPP</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.2.m2.2c">f_{S,\operatorname{SKPP}}</annotation></semantics></math> where</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="F_{S}=\set{f_{S,\mathrm{\operatorname{SPP}}},f_{S,\operatorname{SKPBEV}}}" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.2" xref="S3.E4.m1.1.2.cmml"><msub id="S3.E4.m1.1.2.2" xref="S3.E4.m1.1.2.2.cmml"><mi id="S3.E4.m1.1.2.2.2" xref="S3.E4.m1.1.2.2.2.cmml">F</mi><mi id="S3.E4.m1.1.2.2.3" xref="S3.E4.m1.1.2.2.3.cmml">S</mi></msub><mo id="S3.E4.m1.1.2.1" xref="S3.E4.m1.1.2.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.2.cmml">{</mo><mrow id="S3.E4.m1.1.1.1.1.6" xref="S3.E4.m1.1.1.1.1.7.cmml"><msub id="S3.E4.m1.1.1.1.1.5.1" xref="S3.E4.m1.1.1.1.1.5.1.cmml"><mi id="S3.E4.m1.1.1.1.1.5.1.2" xref="S3.E4.m1.1.1.1.1.5.1.2.cmml">f</mi><mrow id="S3.E4.m1.1.1.1.1.2.2.4" xref="S3.E4.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml">S</mi><mo id="S3.E4.m1.1.1.1.1.2.2.4.1" xref="S3.E4.m1.1.1.1.1.2.2.3.cmml">,</mo><mi id="S3.E4.m1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.2.2.2.cmml">SPP</mi></mrow></msub><mo id="S3.E4.m1.1.1.1.1.6.3" xref="S3.E4.m1.1.1.1.1.7.cmml">,</mo><msub id="S3.E4.m1.1.1.1.1.6.2" xref="S3.E4.m1.1.1.1.1.6.2.cmml"><mi id="S3.E4.m1.1.1.1.1.6.2.2" xref="S3.E4.m1.1.1.1.1.6.2.2.cmml">f</mi><mrow id="S3.E4.m1.1.1.1.1.4.2.4" xref="S3.E4.m1.1.1.1.1.4.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.1.1" xref="S3.E4.m1.1.1.1.1.3.1.1.cmml">S</mi><mo id="S3.E4.m1.1.1.1.1.4.2.4.1" xref="S3.E4.m1.1.1.1.1.4.2.3.cmml">,</mo><mi id="S3.E4.m1.1.1.1.1.4.2.2" xref="S3.E4.m1.1.1.1.1.4.2.2.cmml">SKPBEV</mi></mrow></msub></mrow><mo stretchy="false" id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.2.cmml" xref="S3.E4.m1.1.2"><eq id="S3.E4.m1.1.2.1.cmml" xref="S3.E4.m1.1.2.1"></eq><apply id="S3.E4.m1.1.2.2.cmml" xref="S3.E4.m1.1.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.2.2.1.cmml" xref="S3.E4.m1.1.2.2">subscript</csymbol><ci id="S3.E4.m1.1.2.2.2.cmml" xref="S3.E4.m1.1.2.2.2">ğ¹</ci><ci id="S3.E4.m1.1.2.2.3.cmml" xref="S3.E4.m1.1.2.2.3">ğ‘†</ci></apply><set id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.3"><list id="S3.E4.m1.1.1.1.1.7.cmml" xref="S3.E4.m1.1.1.1.1.6"><apply id="S3.E4.m1.1.1.1.1.5.1.cmml" xref="S3.E4.m1.1.1.1.1.5.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.5.1.1.cmml" xref="S3.E4.m1.1.1.1.1.5.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.5.1.2.cmml" xref="S3.E4.m1.1.1.1.1.5.1.2">ğ‘“</ci><list id="S3.E4.m1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.4"><ci id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1">ğ‘†</ci><ci id="S3.E4.m1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2">SPP</ci></list></apply><apply id="S3.E4.m1.1.1.1.1.6.2.cmml" xref="S3.E4.m1.1.1.1.1.6.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.6.2.1.cmml" xref="S3.E4.m1.1.1.1.1.6.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.6.2.2.cmml" xref="S3.E4.m1.1.1.1.1.6.2.2">ğ‘“</ci><list id="S3.E4.m1.1.1.1.1.4.2.3.cmml" xref="S3.E4.m1.1.1.1.1.4.2.4"><ci id="S3.E4.m1.1.1.1.1.3.1.1.cmml" xref="S3.E4.m1.1.1.1.1.3.1.1">ğ‘†</ci><ci id="S3.E4.m1.1.1.1.1.4.2.2.cmml" xref="S3.E4.m1.1.1.1.1.4.2.2">SKPBEV</ci></list></apply></list></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">F_{S}=\set{f_{S,\mathrm{\operatorname{SPP}}},f_{S,\operatorname{SKPBEV}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p7.3" class="ltx_p">and</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.2" class="ltx_Math" alttext="\operatorname*{\scalerel*{\square}{\sum}}_{f_{S}\in F_{S}}f_{S}=\sum_{f_{S}\in F_{S}}\operatorname{BN}(f_{S})" display="block"><semantics id="S3.E5.m1.2a"><mrow id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml"><mrow id="S3.E5.m1.2.2.3" xref="S3.E5.m1.2.2.3.cmml"><munder id="S3.E5.m1.2.2.3.1" xref="S3.E5.m1.2.2.3.1.cmml"><mrow id="S3.E5.m1.2.2.3.1.2" xref="S3.E5.m1.2.2.3.1.2.cmml"><mrow id="S3.E5.m1.2.2.3.1.2.2" xref="S3.E5.m1.2.2.3.1.2.2.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.E5.m1.2.2.3.1.2.2.2" xref="S3.E5.m1.2.2.3.1.2.2.2b.cmml"><mtext id="S3.E5.m1.2.2.3.1.2.2.2a" xref="S3.E5.m1.2.2.3.1.2.2.2b.cmml">\scalerel</mtext></merror><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m1.2.2.3.1.2.2.1" xref="S3.E5.m1.2.2.3.1.2.2.1.cmml">âˆ—</mo><mi mathvariant="normal" id="S3.E5.m1.2.2.3.1.2.2.3" xref="S3.E5.m1.2.2.3.1.2.2.3.cmml">â–¡</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.3.1.2.1" xref="S3.E5.m1.2.2.3.1.2.1.cmml">â€‹</mo><mo movablelimits="false" id="S3.E5.m1.2.2.3.1.2.3" xref="S3.E5.m1.2.2.3.1.2.3.cmml">âˆ‘</mo></mrow><mrow id="S3.E5.m1.2.2.3.1.3" xref="S3.E5.m1.2.2.3.1.3.cmml"><msub id="S3.E5.m1.2.2.3.1.3.2" xref="S3.E5.m1.2.2.3.1.3.2.cmml"><mi id="S3.E5.m1.2.2.3.1.3.2.2" xref="S3.E5.m1.2.2.3.1.3.2.2.cmml">f</mi><mi id="S3.E5.m1.2.2.3.1.3.2.3" xref="S3.E5.m1.2.2.3.1.3.2.3.cmml">S</mi></msub><mo id="S3.E5.m1.2.2.3.1.3.1" xref="S3.E5.m1.2.2.3.1.3.1.cmml">âˆˆ</mo><msub id="S3.E5.m1.2.2.3.1.3.3" xref="S3.E5.m1.2.2.3.1.3.3.cmml"><mi id="S3.E5.m1.2.2.3.1.3.3.2" xref="S3.E5.m1.2.2.3.1.3.3.2.cmml">F</mi><mi id="S3.E5.m1.2.2.3.1.3.3.3" xref="S3.E5.m1.2.2.3.1.3.3.3.cmml">S</mi></msub></mrow></munder><mo lspace="0.167em" id="S3.E5.m1.2.2.3a" xref="S3.E5.m1.2.2.3.cmml">â¡</mo><msub id="S3.E5.m1.2.2.3.2" xref="S3.E5.m1.2.2.3.2.cmml"><mi id="S3.E5.m1.2.2.3.2.2" xref="S3.E5.m1.2.2.3.2.2.cmml">f</mi><mi id="S3.E5.m1.2.2.3.2.3" xref="S3.E5.m1.2.2.3.2.3.cmml">S</mi></msub></mrow><mo rspace="0.111em" id="S3.E5.m1.2.2.2" xref="S3.E5.m1.2.2.2.cmml">=</mo><mrow id="S3.E5.m1.2.2.1" xref="S3.E5.m1.2.2.1.cmml"><munder id="S3.E5.m1.2.2.1.2" xref="S3.E5.m1.2.2.1.2.cmml"><mo movablelimits="false" id="S3.E5.m1.2.2.1.2.2" xref="S3.E5.m1.2.2.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E5.m1.2.2.1.2.3" xref="S3.E5.m1.2.2.1.2.3.cmml"><msub id="S3.E5.m1.2.2.1.2.3.2" xref="S3.E5.m1.2.2.1.2.3.2.cmml"><mi id="S3.E5.m1.2.2.1.2.3.2.2" xref="S3.E5.m1.2.2.1.2.3.2.2.cmml">f</mi><mi id="S3.E5.m1.2.2.1.2.3.2.3" xref="S3.E5.m1.2.2.1.2.3.2.3.cmml">S</mi></msub><mo id="S3.E5.m1.2.2.1.2.3.1" xref="S3.E5.m1.2.2.1.2.3.1.cmml">âˆˆ</mo><msub id="S3.E5.m1.2.2.1.2.3.3" xref="S3.E5.m1.2.2.1.2.3.3.cmml"><mi id="S3.E5.m1.2.2.1.2.3.3.2" xref="S3.E5.m1.2.2.1.2.3.3.2.cmml">F</mi><mi id="S3.E5.m1.2.2.1.2.3.3.3" xref="S3.E5.m1.2.2.1.2.3.3.3.cmml">S</mi></msub></mrow></munder><mrow id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.1.2.cmml"><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">BN</mi><mo id="S3.E5.m1.2.2.1.1.1a" xref="S3.E5.m1.2.2.1.1.2.cmml">â¡</mo><mrow id="S3.E5.m1.2.2.1.1.1.1" xref="S3.E5.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.2.cmml">(</mo><msub id="S3.E5.m1.2.2.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.2.cmml">f</mi><mi id="S3.E5.m1.2.2.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.3.cmml">S</mi></msub><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.2b"><apply id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2"><eq id="S3.E5.m1.2.2.2.cmml" xref="S3.E5.m1.2.2.2"></eq><apply id="S3.E5.m1.2.2.3.cmml" xref="S3.E5.m1.2.2.3"><apply id="S3.E5.m1.2.2.3.1.cmml" xref="S3.E5.m1.2.2.3.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.3.1.1.cmml" xref="S3.E5.m1.2.2.3.1">subscript</csymbol><apply id="S3.E5.m1.2.2.3.1.2.cmml" xref="S3.E5.m1.2.2.3.1.2"><times id="S3.E5.m1.2.2.3.1.2.1.cmml" xref="S3.E5.m1.2.2.3.1.2.1"></times><apply id="S3.E5.m1.2.2.3.1.2.2.cmml" xref="S3.E5.m1.2.2.3.1.2.2"><times id="S3.E5.m1.2.2.3.1.2.2.1.cmml" xref="S3.E5.m1.2.2.3.1.2.2.1"></times><ci id="S3.E5.m1.2.2.3.1.2.2.2b.cmml" xref="S3.E5.m1.2.2.3.1.2.2.2"><merror class="ltx_ERROR undefined undefined" id="S3.E5.m1.2.2.3.1.2.2.2.cmml" xref="S3.E5.m1.2.2.3.1.2.2.2"><mtext id="S3.E5.m1.2.2.3.1.2.2.2a.cmml" xref="S3.E5.m1.2.2.3.1.2.2.2">\scalerel</mtext></merror></ci><ci id="S3.E5.m1.2.2.3.1.2.2.3.cmml" xref="S3.E5.m1.2.2.3.1.2.2.3">â–¡</ci></apply><sum id="S3.E5.m1.2.2.3.1.2.3.cmml" xref="S3.E5.m1.2.2.3.1.2.3"></sum></apply><apply id="S3.E5.m1.2.2.3.1.3.cmml" xref="S3.E5.m1.2.2.3.1.3"><in id="S3.E5.m1.2.2.3.1.3.1.cmml" xref="S3.E5.m1.2.2.3.1.3.1"></in><apply id="S3.E5.m1.2.2.3.1.3.2.cmml" xref="S3.E5.m1.2.2.3.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.3.1.3.2.1.cmml" xref="S3.E5.m1.2.2.3.1.3.2">subscript</csymbol><ci id="S3.E5.m1.2.2.3.1.3.2.2.cmml" xref="S3.E5.m1.2.2.3.1.3.2.2">ğ‘“</ci><ci id="S3.E5.m1.2.2.3.1.3.2.3.cmml" xref="S3.E5.m1.2.2.3.1.3.2.3">ğ‘†</ci></apply><apply id="S3.E5.m1.2.2.3.1.3.3.cmml" xref="S3.E5.m1.2.2.3.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.3.1.3.3.1.cmml" xref="S3.E5.m1.2.2.3.1.3.3">subscript</csymbol><ci id="S3.E5.m1.2.2.3.1.3.3.2.cmml" xref="S3.E5.m1.2.2.3.1.3.3.2">ğ¹</ci><ci id="S3.E5.m1.2.2.3.1.3.3.3.cmml" xref="S3.E5.m1.2.2.3.1.3.3.3">ğ‘†</ci></apply></apply></apply><apply id="S3.E5.m1.2.2.3.2.cmml" xref="S3.E5.m1.2.2.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.3.2.1.cmml" xref="S3.E5.m1.2.2.3.2">subscript</csymbol><ci id="S3.E5.m1.2.2.3.2.2.cmml" xref="S3.E5.m1.2.2.3.2.2">ğ‘“</ci><ci id="S3.E5.m1.2.2.3.2.3.cmml" xref="S3.E5.m1.2.2.3.2.3">ğ‘†</ci></apply></apply><apply id="S3.E5.m1.2.2.1.cmml" xref="S3.E5.m1.2.2.1"><apply id="S3.E5.m1.2.2.1.2.cmml" xref="S3.E5.m1.2.2.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.2.1.cmml" xref="S3.E5.m1.2.2.1.2">subscript</csymbol><sum id="S3.E5.m1.2.2.1.2.2.cmml" xref="S3.E5.m1.2.2.1.2.2"></sum><apply id="S3.E5.m1.2.2.1.2.3.cmml" xref="S3.E5.m1.2.2.1.2.3"><in id="S3.E5.m1.2.2.1.2.3.1.cmml" xref="S3.E5.m1.2.2.1.2.3.1"></in><apply id="S3.E5.m1.2.2.1.2.3.2.cmml" xref="S3.E5.m1.2.2.1.2.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.2.3.2.1.cmml" xref="S3.E5.m1.2.2.1.2.3.2">subscript</csymbol><ci id="S3.E5.m1.2.2.1.2.3.2.2.cmml" xref="S3.E5.m1.2.2.1.2.3.2.2">ğ‘“</ci><ci id="S3.E5.m1.2.2.1.2.3.2.3.cmml" xref="S3.E5.m1.2.2.1.2.3.2.3">ğ‘†</ci></apply><apply id="S3.E5.m1.2.2.1.2.3.3.cmml" xref="S3.E5.m1.2.2.1.2.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.2.3.3.1.cmml" xref="S3.E5.m1.2.2.1.2.3.3">subscript</csymbol><ci id="S3.E5.m1.2.2.1.2.3.3.2.cmml" xref="S3.E5.m1.2.2.1.2.3.3.2">ğ¹</ci><ci id="S3.E5.m1.2.2.1.2.3.3.3.cmml" xref="S3.E5.m1.2.2.1.2.3.3.3">ğ‘†</ci></apply></apply></apply><apply id="S3.E5.m1.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1"><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">BN</ci><apply id="S3.E5.m1.2.2.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.2">ğ‘“</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.3">ğ‘†</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.2c">\operatorname*{\scalerel*{\square}{\sum}}_{f_{S}\in F_{S}}f_{S}=\sum_{f_{S}\in F_{S}}\operatorname{BN}(f_{S})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p7.4" class="ltx_p">where the aggregation first applies batch normalization (BN) before summing features.
Weâ€™ve found empirically that BN before summation improves performance.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Dual Point Voxel Convolution (DPVC) Blocks</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Regular sparse convolution suffers from the dilation of all sparse features, leading to increased computational burden <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. Furthermore, it diminishes sparsity and blurs feature distinctions, which undermines its efficacy in distinguishing target objects from background features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">In contrast, submanifold sparse convolutions confine output features to the input, alleviating the computational issue but sacrificing information flow, particularly for spatially disconnected features.
While this approach is appropriate for conventional 2D convolutions operating on regularly and uniformly distributed points such as image pixels, it is less suitable for unordered sparse point clouds such as radar point clouds.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">We address this problem in submanifold sparse convolution (SSC) networks by improving information flow of spatially disconnected features.
We achieve this by exploiting the dual nature of sparse grids as they can be considered simultaneously as a grid and a point cloud.
Indeed, the set of indices <math id="S3.SS4.p3.1.m1.2" class="ltx_Math" alttext="(i,j)\in S" display="inline"><semantics id="S3.SS4.p3.1.m1.2a"><mrow id="S3.SS4.p3.1.m1.2.3" xref="S3.SS4.p3.1.m1.2.3.cmml"><mrow id="S3.SS4.p3.1.m1.2.3.2.2" xref="S3.SS4.p3.1.m1.2.3.2.1.cmml"><mo stretchy="false" id="S3.SS4.p3.1.m1.2.3.2.2.1" xref="S3.SS4.p3.1.m1.2.3.2.1.cmml">(</mo><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">i</mi><mo id="S3.SS4.p3.1.m1.2.3.2.2.2" xref="S3.SS4.p3.1.m1.2.3.2.1.cmml">,</mo><mi id="S3.SS4.p3.1.m1.2.2" xref="S3.SS4.p3.1.m1.2.2.cmml">j</mi><mo stretchy="false" id="S3.SS4.p3.1.m1.2.3.2.2.3" xref="S3.SS4.p3.1.m1.2.3.2.1.cmml">)</mo></mrow><mo id="S3.SS4.p3.1.m1.2.3.1" xref="S3.SS4.p3.1.m1.2.3.1.cmml">âˆˆ</mo><mi id="S3.SS4.p3.1.m1.2.3.3" xref="S3.SS4.p3.1.m1.2.3.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.2b"><apply id="S3.SS4.p3.1.m1.2.3.cmml" xref="S3.SS4.p3.1.m1.2.3"><in id="S3.SS4.p3.1.m1.2.3.1.cmml" xref="S3.SS4.p3.1.m1.2.3.1"></in><interval closure="open" id="S3.SS4.p3.1.m1.2.3.2.1.cmml" xref="S3.SS4.p3.1.m1.2.3.2.2"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">ğ‘–</ci><ci id="S3.SS4.p3.1.m1.2.2.cmml" xref="S3.SS4.p3.1.m1.2.2">ğ‘—</ci></interval><ci id="S3.SS4.p3.1.m1.2.3.3.cmml" xref="S3.SS4.p3.1.m1.2.3.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.2c">(i,j)\in S</annotation></semantics></math> is a sparse grid but can also be considered as a point cloud with integer coordinates.
Therefore, we propose a novel Dual Point Voxel Convolution (DPVC) block which computes submanifold sparse convolutions, which operate on sparse grids, and KPConvs, which operate on point clouds, in parallel in order to extract more representative features. First, we extend all occupied grid cells of the input sparse grid by zero-initialising their features to the eight connected non-occupied neighbouring sites. This voxel padding allows the following convolution operations to diffuse the features. Then, we consider two branches of computation within each DPVC block (Fig. <a href="#S3.F4" title="Figure 4 â€£ III-C Multigrid Rendering: Sparse Kernel Point Pillars (SKPP) â€£ III Proposed Method â€£ Exploiting Sparsity in Automotive Radar Object Detection Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). In the first branch, we start with two 3x3 SSC layers to effectively diffuse information throughout the sparse grid, followed by BN and ReLU non-linearity. Finally, a last BN is applied. In the second branch, we process the input data with two KPConv layers, each followed by BN and ReLU non-linearity. Here, a BN is applied last as well. Lastly, the results of both branches are added.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.4.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.5.2" class="ltx_text ltx_font_italic">Dual Point Voxel ConvNet (DPVCN)</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">The original PointPillars backbone processes a dense pseudo-image with a feature pyramid network backbone<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Our empirical findings indicate that this backbone architecture exhibits poor performance when applied to radar data, which aligns with the observations made in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. In addition, it only processes dense grids which scale poorly with increased ranges.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Instead, we propose Dual Point Voxel ConvNet (DPVCN) (Fig. <a href="#S3.F5" title="Figure 5 â€£ III-C Multigrid Rendering: Sparse Kernel Point Pillars (SKPP) â€£ III Proposed Method â€£ Exploiting Sparsity in Automotive Radar Object Detection Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), employing our aforementioned DPVC block for each basic block of the network.
We use a size two max-pooling operation when downsampling. We use a voxel unpooling as the opposite operation to restore the high grid resolution. Weâ€™ve found empirically that using the DPVC blocks only in the encoder performs best.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2308.07748/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="368" height="543" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;">Images of the front camera and corresponding qualitative results of SPP and SKPP-DPVCN in BEV perspective. The results show the radar point cloud as colored points (color encodes radial velocity), lidar points in grey, ground truth boxes as solid rectangles and predicted boxes as dotted rectangles. The results illustrates the improved detection performance of SKPP-DPVCN, as both models detects cars that are missed by the SPP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> baseline model.</span></figcaption>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.4.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.5.2" class="ltx_text ltx_font_italic">SKPP-DPVCN</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Finally, the full architecture of the proposed SKPP-DPVCN (see fig. <a href="#S3.F5" title="Figure 5 â€£ III-C Multigrid Rendering: Sparse Kernel Point Pillars (SKPP) â€£ III Proposed Method â€£ Exploiting Sparsity in Automotive Radar Object Detection Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) fuses the benefits from the proposed SKPP grid rendering and DPVCN backbone network into a single model.
The resulting model improves information flow from point cloud to the grid, scales well to farther distances with sparse grids and sparse convolutions and exhibits strong feature extraction capabilities by exploiting the duality between sparse grids and point clouds.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">As the final step, detection heads process the output feature maps at different scales. We use a coarser scale for larger objects and a finer scale for small objects such as vulnerable road users (VRUs). Non-maximum suppression (NMS) is employed to refine oriented bounding boxes (OBBs) for various class categories.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">EXPERIMENTS</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Experimental Setup</span>
</h3>

<figure id="S4.T1" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.8.1.1" class="ltx_text" style="font-size:90%;">Table I</span>: </span><span id="S4.T1.9.2" class="ltx_text" style="font-size:90%;">Ablation of the different architectures for class <span id="S4.T1.9.2.1" class="ltx_text ltx_font_italic">car</span> on the nuScenes test set.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T1.10" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S4.T1.10.1" class="ltx_text" style="font-size:90%;">The proposed multigrid-rendering module SKPP significantly outperforms our implementation of SPP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and SKPBEV in terms of AP4.0, mAP, ATE and ASE at a slightly lower frame rate. Our new proposed DPVCN backbone also outperforms the SSCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> backbone in terms of AP4.0 and mAP at a lower frame rate but comes with higher computational costs. The proposed SKPP-DPVCN outperforms all other methods in terms of AP4.0, mAP, ASE and ATE.</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T1.5" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.5.5" class="ltx_tr">
<th id="S4.T1.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T1.5.5.6.1" class="ltx_text ltx_font_bold">Grid-Rendering</span></th>
<th id="S4.T1.5.5.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T1.5.5.7.1" class="ltx_text ltx_font_bold">Backbone</span></th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.1" class="ltx_text ltx_font_bold">AP4.0 Car [%] <math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T1.2.2.2.1" class="ltx_text ltx_font_bold">mAP Car</span>[%] <math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.3.3.3.1" class="ltx_text ltx_font_bold">ASE [m] <math id="S4.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.1.m1.1b"><ci id="S4.T1.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
<th id="S4.T1.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.4.4.4.1" class="ltx_text ltx_font_bold">AOE [deg.] <math id="S4.T1.4.4.4.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.4.4.4.1.m1.1a"><mo stretchy="false" id="S4.T1.4.4.4.1.m1.1.1" xref="S4.T1.4.4.4.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.1.m1.1b"><ci id="S4.T1.4.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.4.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
<th id="S4.T1.5.5.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.5.5.5.1" class="ltx_text ltx_font_bold">FPS [Hz] <math id="S4.T1.5.5.5.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.5.5.5.1.m1.1a"><mo stretchy="false" id="S4.T1.5.5.5.1.m1.1.1" xref="S4.T1.5.5.5.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.1.m1.1b"><ci id="S4.T1.5.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.5.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.5.6.1" class="ltx_tr">
<th id="S4.T1.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.5.6.1.1.1" class="ltx_text">SPP-like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> (baseline)</span></th>
<th id="S4.T1.5.6.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SSCN-like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> (baseline)</th>
<td id="S4.T1.5.6.1.3" class="ltx_td ltx_align_center ltx_border_t">42.98</td>
<td id="S4.T1.5.6.1.4" class="ltx_td ltx_align_center ltx_border_t">24.29</td>
<td id="S4.T1.5.6.1.5" class="ltx_td ltx_align_center ltx_border_t">0.495</td>
<td id="S4.T1.5.6.1.6" class="ltx_td ltx_align_center ltx_border_t">39.26</td>
<td id="S4.T1.5.6.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T1.5.6.1.7.1" class="ltx_text ltx_font_bold">61.92</span></td>
</tr>
<tr id="S4.T1.5.7.2" class="ltx_tr">
<th id="S4.T1.5.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.5.7.2.1.1" class="ltx_text">SKPBEV (ours)</span></th>
<th id="S4.T1.5.7.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">SSCN-like<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> (baseline)</th>
<td id="S4.T1.5.7.2.3" class="ltx_td ltx_align_center">43.10</td>
<td id="S4.T1.5.7.2.4" class="ltx_td ltx_align_center">24.39</td>
<td id="S4.T1.5.7.2.5" class="ltx_td ltx_align_center">0.509</td>
<td id="S4.T1.5.7.2.6" class="ltx_td ltx_align_center">39.47</td>
<td id="S4.T1.5.7.2.7" class="ltx_td ltx_nopad_r ltx_align_center">60.16</td>
</tr>
<tr id="S4.T1.5.8.3" class="ltx_tr">
<th id="S4.T1.5.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.5.8.3.1.1" class="ltx_text">SKPP (ours)</span></th>
<th id="S4.T1.5.8.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">SSCN-like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> (baseline)</th>
<td id="S4.T1.5.8.3.3" class="ltx_td ltx_align_center">43.71</td>
<td id="S4.T1.5.8.3.4" class="ltx_td ltx_align_center">24.90</td>
<td id="S4.T1.5.8.3.5" class="ltx_td ltx_align_center">0.477</td>
<td id="S4.T1.5.8.3.6" class="ltx_td ltx_align_center">38.44</td>
<td id="S4.T1.5.8.3.7" class="ltx_td ltx_nopad_r ltx_align_center">57.97</td>
</tr>
<tr id="S4.T1.5.9.4" class="ltx_tr">
<th id="S4.T1.5.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.5.9.4.1.1" class="ltx_text">SPP-like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> (baseline)</span></th>
<th id="S4.T1.5.9.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">DPVCN (ours)</th>
<td id="S4.T1.5.9.4.3" class="ltx_td ltx_align_center">44.47</td>
<td id="S4.T1.5.9.4.4" class="ltx_td ltx_align_center">25.20</td>
<td id="S4.T1.5.9.4.5" class="ltx_td ltx_align_center">0.458</td>
<td id="S4.T1.5.9.4.6" class="ltx_td ltx_align_center">38.68</td>
<td id="S4.T1.5.9.4.7" class="ltx_td ltx_nopad_r ltx_align_center">31.98</td>
</tr>
<tr id="S4.T1.5.10.5" class="ltx_tr">
<th id="S4.T1.5.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.5.10.5.1.1" class="ltx_text">SKPBEV</span></th>
<th id="S4.T1.5.10.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">DPVCN (ours)</th>
<td id="S4.T1.5.10.5.3" class="ltx_td ltx_align_center">44.78</td>
<td id="S4.T1.5.10.5.4" class="ltx_td ltx_align_center">25.32</td>
<td id="S4.T1.5.10.5.5" class="ltx_td ltx_align_center">0.460</td>
<td id="S4.T1.5.10.5.6" class="ltx_td ltx_align_center">38.04</td>
<td id="S4.T1.5.10.5.7" class="ltx_td ltx_nopad_r ltx_align_center">30.21</td>
</tr>
<tr id="S4.T1.5.11.6" class="ltx_tr">
<th id="S4.T1.5.11.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.5.11.6.1.1" class="ltx_text">SKPP (ours)</span></th>
<th id="S4.T1.5.11.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">DPVCN (ours)</th>
<td id="S4.T1.5.11.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.11.6.3.1" class="ltx_text ltx_font_bold">45.51</span></td>
<td id="S4.T1.5.11.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.11.6.4.1" class="ltx_text ltx_font_bold">25.98</span></td>
<td id="S4.T1.5.11.6.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.11.6.5.1" class="ltx_text ltx_font_bold">0.389</span></td>
<td id="S4.T1.5.11.6.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.11.6.6.1" class="ltx_text ltx_font_bold">37.01</span></td>
<td id="S4.T1.5.11.6.7" class="ltx_td ltx_nopad_r ltx_align_center">31.48</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluate different tradeoffs between detection performance, computational complexity and true positive metrics with variants of SKPP-DPVCN. Specifically, we consider the dimensions of the grid rendering method (sparse PointPillars, sparse KPBEV or SKPP) and the backbone architecture whether the baseline submanifold sparse convolution (SSCN) backbone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> or our DPVCN is used. Indeed, we find in our experiments that the combination of SKPP grid rendering and DPVCN significantly improves detection performance compared to other variants. We utilize the official evaluation toolkit and metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. In the nuScenes benchmark, average precision (AP) metrics are utilized to assess the detection performance, considering various matching thresholds (0.5, 1, 2, and 4 meters) between the ground truth and predictions. For our evaluation we focus on the average precision for a matching threshold of 4 m (AP4.0). Furthermore, Average Scale Error (ASE) quantifies the intersection over union (IOU) after aligning orientation and translation (<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="1-IoU" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">1</mn><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">âˆ’</mo><mrow id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml"><mi id="S4.SS1.p1.1.m1.1.1.3.2" xref="S4.SS1.p1.1.m1.1.1.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.3.1" xref="S4.SS1.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p1.1.m1.1.1.3.3" xref="S4.SS1.p1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.3.1a" xref="S4.SS1.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p1.1.m1.1.1.3.4" xref="S4.SS1.p1.1.m1.1.1.3.4.cmml">U</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><minus id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></minus><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">1</cn><apply id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><times id="S4.SS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3.1"></times><ci id="S4.SS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.p1.1.m1.1.1.3.2">ğ¼</ci><ci id="S4.SS1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3.3">ğ‘œ</ci><ci id="S4.SS1.p1.1.m1.1.1.3.4.cmml" xref="S4.SS1.p1.1.m1.1.1.3.4">ğ‘ˆ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">1-IoU</annotation></semantics></math>) and Average Orientation Error (AOE) quantifies the smalles yaw angle difference between pr8ediction and ground truth.
Similar to previsous research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, we focus on the AP4.0 for the class car because other objects are hardly distinguishable using only the low resolution radar in the nuScenes dataset. Consequently, numerous objects either lack reflections entirely or exhibit only a minimal number of reflections. Moreover, the sparsity of and the absence of elevation in nuScenesâ€™ radar data hinder the accurate classification of certain objects. This limitation results in confusion when distinguishing between classes like buses and trucks.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Models</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We evaluate the performance of the proposed SKPP-DPVCN method and perform an ablation study to quantify the impact of each individual contribution. To this end, we consider four different evaluation settings:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">SPP (baseline)</span>: Our re-implementation from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> which utilizes the voxelization feature encoder of PointPillars to convert the point cloud into a vectorized sparse BEV feature map which is processed by a SSCN backbone network.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">SKPBEV (ours)</span>: A sparse version of the KPBEV feature encoder with a SSCN backbone network.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">SKPP (ours)</span>: A multigrid-rendering method based on SPP and SKPBEV to render the features into a sparse BEV feature map which is processed by a SSCN backbone network.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">SPP-DPVCN (ours)</span>: The baseline sparse PointPillars with our DPVCN backbone network.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">SKPBEV-DPVCN (ours)</span>: Our SKPBEV in combination with our DPVCN backbone</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p"><span id="S4.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">SKPP-DPVCN (ours)</span>: Our high performance setting which combines the SKPP multigrid-rendering module with our DPVCN backbone network.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Experiment parameters</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.9" class="ltx_p">The input to the network is a radar point cloud that is aggregated over seven consecutive measurements. Each point in the cloud contains the 2D Cartesian coordinates <math id="S4.SS3.p1.1.m1.2" class="ltx_Math" alttext="x,y" display="inline"><semantics id="S4.SS3.p1.1.m1.2a"><mrow id="S4.SS3.p1.1.m1.2.3.2" xref="S4.SS3.p1.1.m1.2.3.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">x</mi><mo id="S4.SS3.p1.1.m1.2.3.2.1" xref="S4.SS3.p1.1.m1.2.3.1.cmml">,</mo><mi id="S4.SS3.p1.1.m1.2.2" xref="S4.SS3.p1.1.m1.2.2.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.2b"><list id="S4.SS3.p1.1.m1.2.3.1.cmml" xref="S4.SS3.p1.1.m1.2.3.2"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">ğ‘¥</ci><ci id="S4.SS3.p1.1.m1.2.2.cmml" xref="S4.SS3.p1.1.m1.2.2">ğ‘¦</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.2c">x,y</annotation></semantics></math>, the ego-motion compensated radial velocity <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="v_{r}" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><msub id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">v</mi><mi id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">ğ‘£</ci><ci id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">v_{r}</annotation></semantics></math> and the RCS. We perform object detection on a grid ranging from -60 to 60 meters in both the x- and y-direction with an initial cell size of <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="s_{0}=0.5\ m" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><msub id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml"><mi id="S4.SS3.p1.3.m3.1.1.2.2" xref="S4.SS3.p1.3.m3.1.1.2.2.cmml">s</mi><mn id="S4.SS3.p1.3.m3.1.1.2.3" xref="S4.SS3.p1.3.m3.1.1.2.3.cmml">0</mn></msub><mo id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">=</mo><mrow id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3.cmml"><mn id="S4.SS3.p1.3.m3.1.1.3.2" xref="S4.SS3.p1.3.m3.1.1.3.2.cmml">0.5</mn><mo lspace="0.500em" rspace="0em" id="S4.SS3.p1.3.m3.1.1.3.1" xref="S4.SS3.p1.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS3.p1.3.m3.1.1.3.3" xref="S4.SS3.p1.3.m3.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><eq id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1"></eq><apply id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.1.1.2.1.cmml" xref="S4.SS3.p1.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS3.p1.3.m3.1.1.2.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2.2">ğ‘ </ci><cn type="integer" id="S4.SS3.p1.3.m3.1.1.2.3.cmml" xref="S4.SS3.p1.3.m3.1.1.2.3">0</cn></apply><apply id="S4.SS3.p1.3.m3.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3"><times id="S4.SS3.p1.3.m3.1.1.3.1.cmml" xref="S4.SS3.p1.3.m3.1.1.3.1"></times><cn type="float" id="S4.SS3.p1.3.m3.1.1.3.2.cmml" xref="S4.SS3.p1.3.m3.1.1.3.2">0.5</cn><ci id="S4.SS3.p1.3.m3.1.1.3.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3.3">ğ‘š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">s_{0}=0.5\ m</annotation></semantics></math>. In order to process the input point cloud, all layers prior to the detection backbone utilize <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="F_{out}=32" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mrow id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml"><msub id="S4.SS3.p1.4.m4.1.1.2" xref="S4.SS3.p1.4.m4.1.1.2.cmml"><mi id="S4.SS3.p1.4.m4.1.1.2.2" xref="S4.SS3.p1.4.m4.1.1.2.2.cmml">F</mi><mrow id="S4.SS3.p1.4.m4.1.1.2.3" xref="S4.SS3.p1.4.m4.1.1.2.3.cmml"><mi id="S4.SS3.p1.4.m4.1.1.2.3.2" xref="S4.SS3.p1.4.m4.1.1.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.4.m4.1.1.2.3.1" xref="S4.SS3.p1.4.m4.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.SS3.p1.4.m4.1.1.2.3.3" xref="S4.SS3.p1.4.m4.1.1.2.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.4.m4.1.1.2.3.1a" xref="S4.SS3.p1.4.m4.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.SS3.p1.4.m4.1.1.2.3.4" xref="S4.SS3.p1.4.m4.1.1.2.3.4.cmml">t</mi></mrow></msub><mo id="S4.SS3.p1.4.m4.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.4.m4.1.1.3" xref="S4.SS3.p1.4.m4.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><apply id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1"><eq id="S4.SS3.p1.4.m4.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1"></eq><apply id="S4.SS3.p1.4.m4.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.4.m4.1.1.2.1.cmml" xref="S4.SS3.p1.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS3.p1.4.m4.1.1.2.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2.2">ğ¹</ci><apply id="S4.SS3.p1.4.m4.1.1.2.3.cmml" xref="S4.SS3.p1.4.m4.1.1.2.3"><times id="S4.SS3.p1.4.m4.1.1.2.3.1.cmml" xref="S4.SS3.p1.4.m4.1.1.2.3.1"></times><ci id="S4.SS3.p1.4.m4.1.1.2.3.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2.3.2">ğ‘œ</ci><ci id="S4.SS3.p1.4.m4.1.1.2.3.3.cmml" xref="S4.SS3.p1.4.m4.1.1.2.3.3">ğ‘¢</ci><ci id="S4.SS3.p1.4.m4.1.1.2.3.4.cmml" xref="S4.SS3.p1.4.m4.1.1.2.3.4">ğ‘¡</ci></apply></apply><cn type="integer" id="S4.SS3.p1.4.m4.1.1.3.cmml" xref="S4.SS3.p1.4.m4.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">F_{out}=32</annotation></semantics></math> channels. Whenever we use SKPBEV, we use 15 kernel points with a convolution radius of <math id="S4.SS3.p1.5.m5.1" class="ltx_Math" alttext="1.5\ m" display="inline"><semantics id="S4.SS3.p1.5.m5.1a"><mrow id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml"><mn id="S4.SS3.p1.5.m5.1.1.2" xref="S4.SS3.p1.5.m5.1.1.2.cmml">1.5</mn><mo lspace="0.500em" rspace="0em" id="S4.SS3.p1.5.m5.1.1.1" xref="S4.SS3.p1.5.m5.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.5.m5.1.1.3" xref="S4.SS3.p1.5.m5.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><apply id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1"><times id="S4.SS3.p1.5.m5.1.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1.1"></times><cn type="float" id="S4.SS3.p1.5.m5.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2">1.5</cn><ci id="S4.SS3.p1.5.m5.1.1.3.cmml" xref="S4.SS3.p1.5.m5.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">1.5\ m</annotation></semantics></math> and whenever we use KPConv in the DPVCN, we use a convolution radius of <math id="S4.SS3.p1.6.m6.1" class="ltx_Math" alttext="3.75\ m" display="inline"><semantics id="S4.SS3.p1.6.m6.1a"><mrow id="S4.SS3.p1.6.m6.1.1" xref="S4.SS3.p1.6.m6.1.1.cmml"><mn id="S4.SS3.p1.6.m6.1.1.2" xref="S4.SS3.p1.6.m6.1.1.2.cmml">3.75</mn><mo lspace="0.500em" rspace="0em" id="S4.SS3.p1.6.m6.1.1.1" xref="S4.SS3.p1.6.m6.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.6.m6.1.1.3" xref="S4.SS3.p1.6.m6.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m6.1b"><apply id="S4.SS3.p1.6.m6.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1"><times id="S4.SS3.p1.6.m6.1.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1.1"></times><cn type="float" id="S4.SS3.p1.6.m6.1.1.2.cmml" xref="S4.SS3.p1.6.m6.1.1.2">3.75</cn><ci id="S4.SS3.p1.6.m6.1.1.3.cmml" xref="S4.SS3.p1.6.m6.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m6.1c">3.75\ m</annotation></semantics></math>. For the detection backbones SSCN and DPVCN, we use <math id="S4.SS3.p1.7.m7.7" class="ltx_Math" alttext="F_{out,encoder}=\{72,96,128,146,160\}" display="inline"><semantics id="S4.SS3.p1.7.m7.7a"><mrow id="S4.SS3.p1.7.m7.7.8" xref="S4.SS3.p1.7.m7.7.8.cmml"><msub id="S4.SS3.p1.7.m7.7.8.2" xref="S4.SS3.p1.7.m7.7.8.2.cmml"><mi id="S4.SS3.p1.7.m7.7.8.2.2" xref="S4.SS3.p1.7.m7.7.8.2.2.cmml">F</mi><mrow id="S4.SS3.p1.7.m7.2.2.2.2" xref="S4.SS3.p1.7.m7.2.2.2.3.cmml"><mrow id="S4.SS3.p1.7.m7.1.1.1.1.1" xref="S4.SS3.p1.7.m7.1.1.1.1.1.cmml"><mi id="S4.SS3.p1.7.m7.1.1.1.1.1.2" xref="S4.SS3.p1.7.m7.1.1.1.1.1.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.7.m7.1.1.1.1.1.1" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.7.m7.1.1.1.1.1.3" xref="S4.SS3.p1.7.m7.1.1.1.1.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.7.m7.1.1.1.1.1.1a" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.7.m7.1.1.1.1.1.4" xref="S4.SS3.p1.7.m7.1.1.1.1.1.4.cmml">t</mi></mrow><mo id="S4.SS3.p1.7.m7.2.2.2.2.3" xref="S4.SS3.p1.7.m7.2.2.2.3.cmml">,</mo><mrow id="S4.SS3.p1.7.m7.2.2.2.2.2" xref="S4.SS3.p1.7.m7.2.2.2.2.2.cmml"><mi id="S4.SS3.p1.7.m7.2.2.2.2.2.2" xref="S4.SS3.p1.7.m7.2.2.2.2.2.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.7.m7.2.2.2.2.2.1" xref="S4.SS3.p1.7.m7.2.2.2.2.2.1.cmml">â€‹</mo><mi id="S4.SS3.p1.7.m7.2.2.2.2.2.3" xref="S4.SS3.p1.7.m7.2.2.2.2.2.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.7.m7.2.2.2.2.2.1a" xref="S4.SS3.p1.7.m7.2.2.2.2.2.1.cmml">â€‹</mo><mi id="S4.SS3.p1.7.m7.2.2.2.2.2.4" xref="S4.SS3.p1.7.m7.2.2.2.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.7.m7.2.2.2.2.2.1b" xref="S4.SS3.p1.7.m7.2.2.2.2.2.1.cmml">â€‹</mo><mi id="S4.SS3.p1.7.m7.2.2.2.2.2.5" xref="S4.SS3.p1.7.m7.2.2.2.2.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.7.m7.2.2.2.2.2.1c" xref="S4.SS3.p1.7.m7.2.2.2.2.2.1.cmml">â€‹</mo><mi id="S4.SS3.p1.7.m7.2.2.2.2.2.6" xref="S4.SS3.p1.7.m7.2.2.2.2.2.6.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.7.m7.2.2.2.2.2.1d" xref="S4.SS3.p1.7.m7.2.2.2.2.2.1.cmml">â€‹</mo><mi id="S4.SS3.p1.7.m7.2.2.2.2.2.7" xref="S4.SS3.p1.7.m7.2.2.2.2.2.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.7.m7.2.2.2.2.2.1e" xref="S4.SS3.p1.7.m7.2.2.2.2.2.1.cmml">â€‹</mo><mi id="S4.SS3.p1.7.m7.2.2.2.2.2.8" xref="S4.SS3.p1.7.m7.2.2.2.2.2.8.cmml">r</mi></mrow></mrow></msub><mo id="S4.SS3.p1.7.m7.7.8.1" xref="S4.SS3.p1.7.m7.7.8.1.cmml">=</mo><mrow id="S4.SS3.p1.7.m7.7.8.3.2" xref="S4.SS3.p1.7.m7.7.8.3.1.cmml"><mo stretchy="false" id="S4.SS3.p1.7.m7.7.8.3.2.1" xref="S4.SS3.p1.7.m7.7.8.3.1.cmml">{</mo><mn id="S4.SS3.p1.7.m7.3.3" xref="S4.SS3.p1.7.m7.3.3.cmml">72</mn><mo id="S4.SS3.p1.7.m7.7.8.3.2.2" xref="S4.SS3.p1.7.m7.7.8.3.1.cmml">,</mo><mn id="S4.SS3.p1.7.m7.4.4" xref="S4.SS3.p1.7.m7.4.4.cmml">96</mn><mo id="S4.SS3.p1.7.m7.7.8.3.2.3" xref="S4.SS3.p1.7.m7.7.8.3.1.cmml">,</mo><mn id="S4.SS3.p1.7.m7.5.5" xref="S4.SS3.p1.7.m7.5.5.cmml">128</mn><mo id="S4.SS3.p1.7.m7.7.8.3.2.4" xref="S4.SS3.p1.7.m7.7.8.3.1.cmml">,</mo><mn id="S4.SS3.p1.7.m7.6.6" xref="S4.SS3.p1.7.m7.6.6.cmml">146</mn><mo id="S4.SS3.p1.7.m7.7.8.3.2.5" xref="S4.SS3.p1.7.m7.7.8.3.1.cmml">,</mo><mn id="S4.SS3.p1.7.m7.7.7" xref="S4.SS3.p1.7.m7.7.7.cmml">160</mn><mo stretchy="false" id="S4.SS3.p1.7.m7.7.8.3.2.6" xref="S4.SS3.p1.7.m7.7.8.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.7.m7.7b"><apply id="S4.SS3.p1.7.m7.7.8.cmml" xref="S4.SS3.p1.7.m7.7.8"><eq id="S4.SS3.p1.7.m7.7.8.1.cmml" xref="S4.SS3.p1.7.m7.7.8.1"></eq><apply id="S4.SS3.p1.7.m7.7.8.2.cmml" xref="S4.SS3.p1.7.m7.7.8.2"><csymbol cd="ambiguous" id="S4.SS3.p1.7.m7.7.8.2.1.cmml" xref="S4.SS3.p1.7.m7.7.8.2">subscript</csymbol><ci id="S4.SS3.p1.7.m7.7.8.2.2.cmml" xref="S4.SS3.p1.7.m7.7.8.2.2">ğ¹</ci><list id="S4.SS3.p1.7.m7.2.2.2.3.cmml" xref="S4.SS3.p1.7.m7.2.2.2.2"><apply id="S4.SS3.p1.7.m7.1.1.1.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1"><times id="S4.SS3.p1.7.m7.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1"></times><ci id="S4.SS3.p1.7.m7.1.1.1.1.1.2.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1.2">ğ‘œ</ci><ci id="S4.SS3.p1.7.m7.1.1.1.1.1.3.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1.3">ğ‘¢</ci><ci id="S4.SS3.p1.7.m7.1.1.1.1.1.4.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1.4">ğ‘¡</ci></apply><apply id="S4.SS3.p1.7.m7.2.2.2.2.2.cmml" xref="S4.SS3.p1.7.m7.2.2.2.2.2"><times id="S4.SS3.p1.7.m7.2.2.2.2.2.1.cmml" xref="S4.SS3.p1.7.m7.2.2.2.2.2.1"></times><ci id="S4.SS3.p1.7.m7.2.2.2.2.2.2.cmml" xref="S4.SS3.p1.7.m7.2.2.2.2.2.2">ğ‘’</ci><ci id="S4.SS3.p1.7.m7.2.2.2.2.2.3.cmml" xref="S4.SS3.p1.7.m7.2.2.2.2.2.3">ğ‘›</ci><ci id="S4.SS3.p1.7.m7.2.2.2.2.2.4.cmml" xref="S4.SS3.p1.7.m7.2.2.2.2.2.4">ğ‘</ci><ci id="S4.SS3.p1.7.m7.2.2.2.2.2.5.cmml" xref="S4.SS3.p1.7.m7.2.2.2.2.2.5">ğ‘œ</ci><ci id="S4.SS3.p1.7.m7.2.2.2.2.2.6.cmml" xref="S4.SS3.p1.7.m7.2.2.2.2.2.6">ğ‘‘</ci><ci id="S4.SS3.p1.7.m7.2.2.2.2.2.7.cmml" xref="S4.SS3.p1.7.m7.2.2.2.2.2.7">ğ‘’</ci><ci id="S4.SS3.p1.7.m7.2.2.2.2.2.8.cmml" xref="S4.SS3.p1.7.m7.2.2.2.2.2.8">ğ‘Ÿ</ci></apply></list></apply><set id="S4.SS3.p1.7.m7.7.8.3.1.cmml" xref="S4.SS3.p1.7.m7.7.8.3.2"><cn type="integer" id="S4.SS3.p1.7.m7.3.3.cmml" xref="S4.SS3.p1.7.m7.3.3">72</cn><cn type="integer" id="S4.SS3.p1.7.m7.4.4.cmml" xref="S4.SS3.p1.7.m7.4.4">96</cn><cn type="integer" id="S4.SS3.p1.7.m7.5.5.cmml" xref="S4.SS3.p1.7.m7.5.5">128</cn><cn type="integer" id="S4.SS3.p1.7.m7.6.6.cmml" xref="S4.SS3.p1.7.m7.6.6">146</cn><cn type="integer" id="S4.SS3.p1.7.m7.7.7.cmml" xref="S4.SS3.p1.7.m7.7.7">160</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.7.m7.7c">F_{out,encoder}=\{72,96,128,146,160\}</annotation></semantics></math> channels for the encoder blocks. Additionally, we augment the RCS by adding a value <math id="S4.SS3.p1.8.m8.1" class="ltx_Math" alttext="\Delta RCS" display="inline"><semantics id="S4.SS3.p1.8.m8.1a"><mrow id="S4.SS3.p1.8.m8.1.1" xref="S4.SS3.p1.8.m8.1.1.cmml"><mi mathvariant="normal" id="S4.SS3.p1.8.m8.1.1.2" xref="S4.SS3.p1.8.m8.1.1.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.8.m8.1.1.1" xref="S4.SS3.p1.8.m8.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.8.m8.1.1.3" xref="S4.SS3.p1.8.m8.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.8.m8.1.1.1a" xref="S4.SS3.p1.8.m8.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.8.m8.1.1.4" xref="S4.SS3.p1.8.m8.1.1.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.8.m8.1.1.1b" xref="S4.SS3.p1.8.m8.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.8.m8.1.1.5" xref="S4.SS3.p1.8.m8.1.1.5.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.8.m8.1b"><apply id="S4.SS3.p1.8.m8.1.1.cmml" xref="S4.SS3.p1.8.m8.1.1"><times id="S4.SS3.p1.8.m8.1.1.1.cmml" xref="S4.SS3.p1.8.m8.1.1.1"></times><ci id="S4.SS3.p1.8.m8.1.1.2.cmml" xref="S4.SS3.p1.8.m8.1.1.2">Î”</ci><ci id="S4.SS3.p1.8.m8.1.1.3.cmml" xref="S4.SS3.p1.8.m8.1.1.3">ğ‘…</ci><ci id="S4.SS3.p1.8.m8.1.1.4.cmml" xref="S4.SS3.p1.8.m8.1.1.4">ğ¶</ci><ci id="S4.SS3.p1.8.m8.1.1.5.cmml" xref="S4.SS3.p1.8.m8.1.1.5">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.8.m8.1c">\Delta RCS</annotation></semantics></math> from a normal distribution with zero mean and a standard deviation of <math id="S4.SS3.p1.9.m9.1" class="ltx_Math" alttext="\sigma=0.7" display="inline"><semantics id="S4.SS3.p1.9.m9.1a"><mrow id="S4.SS3.p1.9.m9.1.1" xref="S4.SS3.p1.9.m9.1.1.cmml"><mi id="S4.SS3.p1.9.m9.1.1.2" xref="S4.SS3.p1.9.m9.1.1.2.cmml">Ïƒ</mi><mo id="S4.SS3.p1.9.m9.1.1.1" xref="S4.SS3.p1.9.m9.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.9.m9.1.1.3" xref="S4.SS3.p1.9.m9.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.9.m9.1b"><apply id="S4.SS3.p1.9.m9.1.1.cmml" xref="S4.SS3.p1.9.m9.1.1"><eq id="S4.SS3.p1.9.m9.1.1.1.cmml" xref="S4.SS3.p1.9.m9.1.1.1"></eq><ci id="S4.SS3.p1.9.m9.1.1.2.cmml" xref="S4.SS3.p1.9.m9.1.1.2">ğœ</ci><cn type="float" id="S4.SS3.p1.9.m9.1.1.3.cmml" xref="S4.SS3.p1.9.m9.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.9.m9.1c">\sigma=0.7</annotation></semantics></math> to each reflection point. For each model variant, we conduct 5 trials and compute the average of metrics to accommodate for training stochasticity. Each model is trained for 30 epochs using a batch size of 32.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Tab. <a href="#S4.T1" title="Table I â€£ IV-A Experimental Setup â€£ IV EXPERIMENTS â€£ Exploiting Sparsity in Automotive Radar Object Detection Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> shows the quantitative results of the different methods on the nuScenes validation set. We see that our SKPP-DPVCN has a relative AP4.0 improvement of 5.89% and an absolute improvement of 2.53% compared to the SPP-like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> baseline module. Simultaneously, we see that in relative terms the ASE improves by 21.41% and the AOE by 5.73% relatively to the baseline. Furthermore, we find that the framerate deteriorates from 61.92 Hz to 31.48 Hz.
Tab. <a href="#S5.T2" title="Table II â€£ V Results â€£ Exploiting Sparsity in Automotive Radar Object Detection Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows that SKPP-DPVCN also outperforms the dense state-of-the-art radar object detection model KPPillarsBEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> by 4.19% AP4.0.
As an ablation study, we analyse the effects of both the SKPP multigrid rendering module and the DPVCN backbone separately. Comparing the SKPP module with the SPP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, it achieves an absolute AP4.0 increase of 0.73%. At the same time, the frame rate hardly deteriorates (Table <a href="#S4.T1" title="Table I â€£ IV-A Experimental Setup â€£ IV EXPERIMENTS â€£ Exploiting Sparsity in Automotive Radar Object Detection Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>) and both ASE and AOE improve by 3.64% and 2.08%, respectively. This performance improvement comes at no additional cost. SKPP benefits from the inherent advantages of both grid rendering methods.</p>
</div>
<figure id="S5.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.6.1.1" class="ltx_text" style="font-size:90%;">Table II</span>: </span><span id="S5.T2.7.2" class="ltx_text" style="font-size:90%;">Benchmark of the different architectures for class <span id="S5.T2.7.2.1" class="ltx_text ltx_font_italic">car</span> on the nuScenes test set.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.T2.8" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S5.T2.8.1" class="ltx_text" style="font-size:90%;">The proposed SKPP-DPVCN significantly outperforms the dense methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> in terms of AP4.0, ASE and AOE.</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S5.T2.3" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.3.3" class="ltx_tr">
<th id="S5.T2.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T2.3.3.4.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1" class="ltx_text ltx_font_bold">AP4.0 Car [%]<math id="S5.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T2.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S5.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.2.2.2.1" class="ltx_text ltx_font_bold">ASE [m]<math id="S5.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.2.2.2.1.m1.1a"><mo stretchy="false" id="S5.T2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
<th id="S5.T2.3.3.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.3.3.3.1" class="ltx_text ltx_font_bold">AOE [deg.]<math id="S5.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.3.3.3.1.m1.1a"><mo stretchy="false" id="S5.T2.3.3.3.1.m1.1.1" xref="S5.T2.3.3.3.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.1.m1.1b"><ci id="S5.T2.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.3.4.1" class="ltx_tr">
<th id="S5.T2.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T2.3.4.1.1.1" class="ltx_text">PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></span></th>
<td id="S5.T2.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t">38.31</td>
<td id="S5.T2.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t">0.48</td>
<td id="S5.T2.3.4.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">38.48</td>
</tr>
<tr id="S5.T2.3.5.2" class="ltx_tr">
<th id="S5.T2.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.3.5.2.1.1" class="ltx_text">KPPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span></th>
<td id="S5.T2.3.5.2.2" class="ltx_td ltx_align_center">40.80</td>
<td id="S5.T2.3.5.2.3" class="ltx_td ltx_align_center">0.5</td>
<td id="S5.T2.3.5.2.4" class="ltx_td ltx_nopad_r ltx_align_center">37.87</td>
</tr>
<tr id="S5.T2.3.6.3" class="ltx_tr">
<th id="S5.T2.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.3.6.3.1.1" class="ltx_text">KPBEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></span></th>
<td id="S5.T2.3.6.3.2" class="ltx_td ltx_align_center">41.21</td>
<td id="S5.T2.3.6.3.3" class="ltx_td ltx_align_center">0.49</td>
<td id="S5.T2.3.6.3.4" class="ltx_td ltx_nopad_r ltx_align_center">38.49</td>
</tr>
<tr id="S5.T2.3.7.4" class="ltx_tr">
<th id="S5.T2.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.3.7.4.1.1" class="ltx_text">KPPillarsBEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></span></th>
<td id="S5.T2.3.7.4.2" class="ltx_td ltx_align_center">43.68</td>
<td id="S5.T2.3.7.4.3" class="ltx_td ltx_align_center">0.44</td>
<td id="S5.T2.3.7.4.4" class="ltx_td ltx_nopad_r ltx_align_center">37.31</td>
</tr>
<tr id="S5.T2.3.8.5" class="ltx_tr">
<th id="S5.T2.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.3.8.5.1.1" class="ltx_text">SKPP (ours)</span></th>
<td id="S5.T2.3.8.5.2" class="ltx_td ltx_align_center">43.71</td>
<td id="S5.T2.3.8.5.3" class="ltx_td ltx_align_center">0.48</td>
<td id="S5.T2.3.8.5.4" class="ltx_td ltx_nopad_r ltx_align_center">38.44</td>
</tr>
<tr id="S5.T2.3.9.6" class="ltx_tr">
<th id="S5.T2.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.3.9.6.1.1" class="ltx_text">SKPP-DPVCN (ours)</span></th>
<td id="S5.T2.3.9.6.2" class="ltx_td ltx_align_center"><span id="S5.T2.3.9.6.2.1" class="ltx_text ltx_font_bold">45.51</span></td>
<td id="S5.T2.3.9.6.3" class="ltx_td ltx_align_center"><span id="S5.T2.3.9.6.3.1" class="ltx_text ltx_font_bold">0.39</span></td>
<td id="S5.T2.3.9.6.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.3.9.6.4.1" class="ltx_text ltx_font_bold">37.01</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The new DPVCN backbone with SPP-like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> grid-rendering achieves an absolute improvement of AP4.0 of 1.49% compared to the baseline SSCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Additionally, a relative improvement of ASE and AOE by 7.47% and 1.48% respectively is obtained. However, the frame rate decreases to 31.48 Hz.
The trade-offs between detection performance and computational complexity are shown in Tab. <a href="#S4.T1" title="Table I â€£ IV-A Experimental Setup â€£ IV EXPERIMENTS â€£ Exploiting Sparsity in Automotive Radar Object Detection Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. In summary, both SKPP and DPVCN contribute to the performance of radar object detection networks. The modularity of our proposed SKPP-DPVCN helps in designing suitable architectures that balance detection performance and interference time. When maximum detection performance is desired, the proposed multiscale SKPP-DPVCN architecture is the best choice and achieves an AP4.0 of 45.51% in our experiments. If an increase in compute resources is undesirable, the SKPP multigrid rendering module significantly increases detection performance with negligible increase in interference time.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Qualitative results of the high detection performance of this method can be found in Fig. <a href="#S3.F6" title="Figure 6 â€£ III-E Dual Point Voxel ConvNet (DPVCN) â€£ III Proposed Method â€£ Exploiting Sparsity in Automotive Radar Object Detection Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, showing the results from SKPP and SKPP-DPVCN in comparison to the SPP baseline based on a scene in the nuScenes validation set. In a scenario with limited computational resources, SKPP may be advantageous as it achieves significantly better detection performance at the same inference speed. Additionally, a further increase in detection performance is seen with SKPP-DPVCN, although this comes with a higher interference speed.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In conclusion, we have shed light on the limitations of grid-based approaches and highlighted the importance of preserving essential information of the point cloud data. By introducing the novel multigrid-rendering method SKPP, we have improve detection performance and have overcome the tradeoffs faced by previous models.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Furthermore, our exploration of the duality between sparse grids and point clouds has led to the development of the DPVCN backbone architecture. This innovative approach leverages the descriptive power of KPConvs and SSCs to effectively extract information from spatially disconnected features, addressing the challenge posed by unstructured sparse radar data.
We have proposed SKPP-DPVCN which outperforms the current state-of-the-art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for radar object detection on nuScenes and surpassing the baseline model. We have also performed an ablation study showing the individual contributions of SKPP and DPVCN.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
P.Â Svenningsson, F.Â Fioranelli, and A.Â Yarovoy, â€œRadar-pointgnn: Graph based
object recognition for unstructured radar point-cloud data,â€ in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2021
IEEE Radar Conference (RadarConf21)</em>.Â Â Â IEEE, 2021, pp. 1â€“6.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
F.Â Drews <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œDeepfusion: A robust and modular 3d object detector
for lidars, cameras and radars,â€ in <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">2022 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS)</em>.Â Â Â IEEE, 2022, pp. 560â€“567.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
K.Â Lei <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œHvdetfusion: A simple and robust camera-radar fusion
framework,â€ <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.11323</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B.Â Xu <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œRpfa-net: A 4d radar pillar feature attention network
for 3d object detection,â€ in <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">2021 IEEE International Intelligent
Transportation Systems Conference (ITSC)</em>.Â Â Â IEEE Press, 2021, p. 3061â€“3066.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C.Â M., <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition and Machine
Learning</em>, 1stÂ ed., ser. Information Science and Statistics.Â Â Â New York, NY: Springer, Aug. 2006.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M.Â Ulrich <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œImproved Orientation Estimation and Detection
with Hybrid Object Detection Networks for Automotive Radar,â€ in
<em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">2022 IEEE 25th International Conference on Intelligent
Transportation Systems (ITSC)</em>, Oct. 2022, pp. 111â€“117.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
X.Â Chen <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œMulti-view 3d object detection network for autonomous
driving,â€ in <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition</em>, 2017, pp. 1907â€“1915.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y.Â Zhou <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œEnd-to-end multi-view fusion for 3d object detection
in lidar point clouds,â€ PMLR, pp. 923â€“932, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
B.Â Yang, W.Â Luo, and R.Â Urtasun, â€œPixor: Real-time 3d object detection from
point clouds,â€ in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition</em>, 2018, pp. 7652â€“7660.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A.Â H. Lang <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œPointpillars: Fast encoders for object detection
from point clouds,â€ in <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition</em>, 2019, pp. 12â€‰697â€“12â€‰705.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
D.Â Koehler <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œImproved multi-scale grid rendering of point clouds
for radar object detection networks,â€ in <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">Fusion conference</em>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J.Â Deng <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œVoxel r-cnn: Towards high performance voxel-based 3d
object detection,â€ <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, vol.Â 35, no.Â 2, pp. 1201â€“1209, May 2021. [Online]. Available:
https://ojs.aaai.org/index.php/AAAI/article/view/16207

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
B.Â Graham, M.Â Engelcke, and L.Â Van DerÂ Maaten, â€œ3d semantic segmentation with
submanifold sparse convolutional networks,â€ in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2018, pp. 9224â€“9232.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
K.Â Vedder and E.Â Eaton, â€œSparse pointpillars: Maintaining and exploiting input
sparsity to improve runtime on embedded systems,â€ in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS)</em>.Â Â Â IEEE, 2022, pp. 2025â€“2031.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M.Â Ulrich <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œPerson recognition based on micro-doppler and
thermal infrared camera fusion for firefighting,â€ in <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">2018 21st
International Conference on Information Fusion (FUSION)</em>, 2018, pp. 919â€“926.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M.Â Ulrich and B.Â Yang, â€œShort-duration doppler spectrogram for person
recognition with a handheld radar,â€ in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2018 26th European Signal
Processing Conference (EUSIPCO)</em>, 2018, pp. 1227â€“1231.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A.Â Palffy <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œCNN based road user detection using the 3d radar
cube,â€ <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol.Â 5, no.Â 2, pp.
1263â€“1270, apr 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
D.Â Brodeski, I.Â Bilik, and R.Â Giryes, â€œDeep radar detector,â€ in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2019
IEEE Radar Conference (RadarConf)</em>, 2019, pp. 1â€“6.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T.-Y. Lim <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œRadar and camera early fusion for vehicle detection
in advanced driver assistance systems,â€ in <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">Machine Learning for
Autonomous Driving Workshop at the 33rd Conference on Neural Information
Processing Systems</em>, vol.Â 2, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M.Â Meyer, G.Â Kuschk, and S.Â Tomforde, â€œGraph convolutional networks for 3d
object detection on radar data,â€ in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2021, pp. 3060â€“3069.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
C.Â R. Qi <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œPointnet: Deep learning on point sets for 3d
classification and segmentation,â€ in <em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2017, pp. 652â€“660.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
â€”â€”, â€œPointnet++: Deep hierarchical feature learning on point sets in a
metric space,â€ <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
vol.Â 30, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M.Â Ulrich, C.Â GlÃ¤ser, and F.Â Timm, â€œDeepreflecs: Deep learning for automotive
object classification with radar reflections,â€ in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Radar
Conference (RadarConf21)</em>, 2021, pp. 1â€“6.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
O.Â Schumann <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œSemantic segmentation on radar point clouds,â€ in
<em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">2018 21st International Conference on Information Fusion
(FUSION)</em>.Â Â Â IEEE, 2018, pp. 2179â€“2186.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
B.Â Xu <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œRpfa-net: a 4d radar pillar feature attention network
for 3d object detection,â€ in <em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">2021 IEEE International Intelligent
Transportation Systems Conference (ITSC)</em>, 2021, pp. 3061â€“3066.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A.Â Danzer <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œ2d car detection in radar data with pointnets,â€ in
<em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</em>, 2019,
p. 61â€“66.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
K.Â Bansal <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œPointillism: Accurate 3d bounding box estimation
with multi-radars,â€ in <em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">Proceedings of the 18th Conference on Embedded
Networked Sensor Systems</em>, 2020, pp. 340â€“353.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
F.Â Nobis <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œKernel point convolution lstm networks for radar
point cloud segmentation,â€ <em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol.Â 11, no.Â 6, p. 2599,
2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J.Â Bai <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œRadar transformer: An object classification network
based on 4d MMW imaging radar,â€ <em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic">Sensors</em>, vol.Â 21, no.Â 11, p. 3854,
Jun. 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J.Â Redmon <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œYou only look once: Unified, real-time object
detection,â€ in <em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2016, pp. 779â€“788.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M.Â Meyer and G.Â Kuschk, â€œDeep learning based 3d object detection for
automotive radar and camera,â€ in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">2019 16th European Radar Conference
(EuRAD)</em>.Â Â Â IEEE, 2019, pp. 133â€“136.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T.-Y. Lin <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œFeature pyramid networks for object detection,â€ in
<em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2017, pp. 2117â€“2125.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
M.Â Dreher <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œRadar-based 2d car detection using deep neural
networks,â€ in <em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">2020 IEEE 23rd International Conference on Intelligent
Transportation Systems (ITSC)</em>, 2020, pp. 1â€“8.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
B.Â Tan <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œTracking of multiple static and dynamic targets for 4d
automotive millimeter-wave radar point cloud in urban environments,â€
<em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic">Remote Sensing</em>, vol.Â 15, no.Â 11, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
A.Â Palffy <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œMulti-class road user detection with 3+1d radar in
the view-of-delft dataset,â€ <em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>,
vol.Â 7, pp. 4961â€“4968, 04 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Y.Â Zhou and O.Â Tuzel, â€œVoxelnet: End-to-end learning for point cloud based 3d
object detection,â€ in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>, 2018, pp. 4490â€“4499.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
B.Â Graham, â€œSpatially-sparse convolutional neural networks. arxiv 2014,â€
<em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.6070</em>, 2014.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
â€”â€”, â€œSparse 3d convolutional neural networks,â€ <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1505.02890</em>, 2015.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
B.Â Graham and L.Â VanÂ der Maaten, â€œSubmanifold sparse convolutional networks,â€
<em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.01307</em>, 2017.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Y.Â Yan, Y.Â Mao, and B.Â Li, â€œSECOND: Sparsely embedded convolutional
detection,â€ <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol.Â 18, no.Â 10, p. 3337, Oct. 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
S.Â Shi <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œPv-rcnn: Point-voxel feature set abstraction for 3d
object detection,â€ in <em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition</em>, 2020, pp. 10â€‰529â€“10â€‰538.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Y.Â Yan, Y.Â Mao, and B.Â Li, â€œSecond: Sparsely embedded convolutional
detection,â€ <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol.Â 18, no.Â 10, p. 3337, 2018.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
N.Â Scheiner <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œObject detection for automotive radar point
cloudsâ€“a comparison,â€ <em id="bib.bib43.2.2" class="ltx_emph ltx_font_italic">AI Perspectives</em>, vol.Â 3, no.Â 1, pp. 1â€“23,
2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
H.Â Caesar <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œnuscenes: A multimodal dataset for autonomous
driving,â€ in <em id="bib.bib44.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, 2020, pp. 11â€‰621â€“11â€‰631.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
P.Â Svenningsson, F.Â Fioranelli, and A.Â Yarovoy, â€œRadar-pointgnn: Graph based
object recognition for unstructured radar point-cloud data,â€ in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">2021
IEEE Radar Conference (RadarConf21)</em>, 2021, pp. 1â€“6.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.07747" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.07748" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.07748">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.07748" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.07749" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 12:26:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
