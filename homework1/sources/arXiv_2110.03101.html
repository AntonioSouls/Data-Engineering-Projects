<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2110.03101] SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap</title><meta property="og:description" content="Autonomous vision-based spaceborne navigation is an enabling technology for future on-orbit servicing and space logistics missions. While computer vision in general has benefited from Machine Learning (ML), training an…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2110.03101">

<!--Generated on Sat Mar  2 02:53:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tae Ha Park
<br class="ltx_break">Department of Aeronautics &amp; Astronautics
<br class="ltx_break">Stanford University
<br class="ltx_break">496 Lomita Mall
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Stanford
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> CA 94305
<br class="ltx_break">tpark94@stanford.edu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marcus Märtens
<br class="ltx_break">Advanced Concepts Team
<br class="ltx_break">European Space Agency
<br class="ltx_break">2201 AZ Noordwijk
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> The Netherlands
<br class="ltx_break">marcus.maertens@esa.int
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gurvan Lecuyer
<br class="ltx_break">Advanced Concepts Team
<br class="ltx_break">European Space Agency
<br class="ltx_break">2201 AZ Noordwijk
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> The Netherlands
<br class="ltx_break">gurvan.lecuyer@esa.int
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dario Izzo
<br class="ltx_break">Advanced Concepts Team
<br class="ltx_break">European Space Agency
<br class="ltx_break">2201 AZ Noordwijk
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> The Netherlands
<br class="ltx_break">dario.izzo@esa.int
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Simone D’Amico
<br class="ltx_break">Department of Aeronautics &amp; Astronautics
<br class="ltx_break">Stanford University
<br class="ltx_break">496 Lomita Mall
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Stanford
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> CA 94305
<br class="ltx_break">damicos@stanford.edu
</span><span class="ltx_author_notes"><span id="id1.1.1" class="ltx_text" style="font-size:80%;">978-1-6654-3760-8/22/<math id="id1.1.1.m1.1" class="ltx_Math" alttext="\$31.00" display="inline"><semantics id="id1.1.1.m1.1a"><mrow id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml"><mo rspace="0.167em" id="id1.1.1.m1.1.1.1" xref="id1.1.1.m1.1.1.1.cmml">$</mo><mn id="id1.1.1.m1.1.1.2" xref="id1.1.1.m1.1.1.2.cmml">31.00</mn></mrow><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><apply id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.1.m1.1.1.1.cmml" xref="id1.1.1.m1.1.1.1">currency-dollar</csymbol><cn type="float" id="id1.1.1.m1.1.1.2.cmml" xref="id1.1.1.m1.1.1.2">31.00</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">\$31.00</annotation></semantics></math> ©2022 IEEE</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Autonomous vision-based spaceborne navigation is an enabling technology for future on-orbit servicing and space logistics missions. While computer vision in general has benefited from Machine Learning (ML), training and validating spaceborne ML models are extremely challenging due to the impracticality of acquiring a large-scale labeled dataset of images of the intended target in the space environment. Existing datasets, such as Spacecraft PosE Estimation Dataset (SPEED), have so far mostly relied on synthetic images for both training and validation, which are easy to mass-produce but fail to resemble the visual features and illumination variability inherent to the target spaceborne images. In order to bridge the gap between the current practices and the intended applications in future space missions, this paper introduces SPEED+: the next generation spacecraft pose estimation dataset with specific emphasis on domain gap. In addition to 60,000 synthetic images for training, SPEED+ includes 9,531 hardware-in-the-loop images of a spacecraft mockup model captured from the Testbed for Rendezvous and Optical Navigation (TRON) facility. TRON is a first-of-a-kind robotic testbed capable of capturing an arbitrary number of target images with accurate and maximally diverse pose labels and high-fidelity spaceborne illumination conditions. SPEED+ is used in the second international Satellite Pose Estimation Challenge co-hosted by SLAB and the Advanced Concepts Team of the European Space Agency to evaluate and compare the robustness of spaceborne ML models trained on synthetic images.</p>
</div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S1" title="In SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S2" title="In SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS1" title="In 2 Related Work ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Datasets for spacecraft pose estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS2" title="In 2 Related Work ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Testbeds for spaceborne optical navigation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS3" title="In 2 Related Work ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Datasets for domain gap</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S3" title="In SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>SPEED+ Overview</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS1" title="In 3 SPEED+ Overview ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>On Realism of TRON Simulation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S4" title="In SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Generating SPEED+ Images</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS1" title="In 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>TRON Facility Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS2" title="In 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Collecting SPEED+ HIL Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS3" title="In 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Post-Processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS4" title="In 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Manual Removal of Samples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS5" title="In 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Generating SPEED+ Synthetic images</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S5" title="In SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S5.SS1" title="In 5 Experiments ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Pose estimation CNNs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S5.SS2" title="In 5 Experiments ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Algorithms for domain gap</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S5.SS3" title="In 5 Experiments ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S5.SS4" title="In 5 Experiments ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S6" title="In SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S6.SS1" title="In 6 Discussions ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Dataset Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S6.SS2" title="In 6 Discussions ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Relation to Real Mission Constraints and Flight Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S6.SS3" title="In 6 Discussions ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Baseline Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S7" title="In SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S8" title="In SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Accessibility</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S8.SS1" title="In 8 Accessibility ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Kelvins Platform</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S8.SS2" title="In 8 Accessibility ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Baseline Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S9" title="In SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Camera Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S10" title="In SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Training Details</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S10.SS1" title="In 10 Training Details ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.1 </span>KRN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S10.SS2" title="In 10 Training Details ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.2 </span>SPN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S10.SS3" title="In 10 Training Details ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.3 </span>HigherHRNet</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S10.SS4" title="In 10 Training Details ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.4 </span>DANN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S10.SS5" title="In 10 Training Details ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.5 </span>Style Augmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S11" title="In SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span>Visualization</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S11.SS1" title="In 11 Visualization ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11.1 </span>Retained Artifacts in HIL Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S11.SS2" title="In 11 Visualization ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11.2 </span>Images of Rejected Samples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S11.SS3" title="In 11 Visualization ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11.3 </span>Example HIL Images with Model Projection</span></a></li>
</ol>
</li>
</ol></nav>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Autonomous navigation about a noncooperative Resident Space Object (RSO) is an enabling technology for future in-orbit servicing and space logistics missions, such as refueling space assets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and active debris removal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. As these missions involve interaction with a noncooperative target at close proximity, accurate determination and tracking of the target’s pose (i.e., position and orientation) with respect to the servicer spacecraft is vital for safe docking and capture. Moreover, driven by the limited on-board power and computational capabilities, a monocular camera is a favored choice of sensor as opposed to more complex systems such as stereovision, Range Detection and Ranging (RADAR) or Light Detection and Ranging (LIDAR). Recently, Machine Learning (ML) techniques based on Convolutional Neural Networks (CNN) have been explored in applications of vision-based spacecraft pose estimation and navigation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>; however, it is impractical to acquire large-scale imagery of the interested target spacecraft with accurate pose labels in space to train CNN models. This is the main reason that CNN models have not been deployed in space so far. In fact, spaceborne ML models up to date have been trained and, most importantly, validated virtually exclusively on synthetic imagery. While synthetic imagery is easy to mass-produce and annotate for training, it is prone to performance degradation when tested on the target spaeborne imagery as CNNs overfit to the features specific to the synthetic imagery. The phenomenon is known as <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">domain gap</em> in literature and is an active field of research in machine learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2110.03101/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="103" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example images from different domains of SPEED+.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While it is possible to train a more domain-invariant CNN model using methods such as domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and texture randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, its use in space missions requires a validation of performance in a highly representative space environment prior to mission deployment. Some authors resort to existing spaceborne images captured from previous missions to evaluate the robustness of CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. However, for a single target, these images lack diversity in terms of quantity, pose distribution and variability of environmental factors, which severely restrict the comprehensive evaluation of the model’s robustness for future missions or for future phases of the same mission. Moreover, these images mostly lack accurate pose labels, which has forced the previous evaluations to be strictly qualitative <span id="S1.p2.1.1" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> or based on hand-labeled pose annotations <span id="S1.p2.1.1.1" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></span></span>. Therefore, an alternative approach is to instead physically re-create and simulate the space environment using an on-ground Hardware-In-the-Loop (HIL) robotic testbed which is capable of capturing an arbitrary number of images of the target mockup model and annotating accurate pose labels for each image sample. Then, these HIL images can be used as a weaker surrogate of the target spaceborne images to evaluate on-ground the robustness of a CNN model on an image domain different from the synthetic training images.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To the best of the authors’ knowledge, the only publicly available benchmark dataset on spacecraft pose estimation with fully labeled images from multiple sources is the Spacecraft PosE Estimation Dataset (SPEED) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> which consists of 15,300 images of the Tango spacecraft from the PRISMA mission <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Specifically, the dataset comprises 15,000 synthetic images rendered with OpenGL-based Optical Stimulator (OS) camera emulator software <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> of the Stanford’s Space Rendezvous Laboratory (SLAB) multi-Satellite Software Simulator (<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="S^{3}" display="inline"><semantics id="S1.p3.1.m1.1a"><msup id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mi id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml">S</mi><mn id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1">superscript</csymbol><ci id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2">𝑆</ci><cn type="integer" id="S1.p3.1.m1.1.1.3.cmml" xref="S1.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">S^{3}</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and 300 simulated images captured from the robotic Testbed for Rendezvous and Optical Navigation (TRON) at SLAB. The dataset was made available as part of the Satellite Pose Estimation Competition (SPEC2019) co-hosted by SLAB and the Advanced Concepts Team (ACT) of the European Space Agency (ESA). Throughout the competition, the submitted entries were evaluated and ranked based on their performances on the synthetic test set images. While the models could be evaluated on the 300 simulated images as well, their restricted pose and illumination configurations fall short to perform a comprehensive analysis of the model’s robustness across domain gap.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In order to overcome the aforementioned challenges and facilitate the study of ML robustness for future space missions, this paper presents SPEED+, the next-generation spacecraft pose estimation dataset with specific emphasis on bridging the domain gap. As shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, SPEED+ comprises large-scale labeled synthetic imagery (<span id="S1.p4.1.1" class="ltx_text ltx_font_typewriter">synthetic</span>) for training and two unlabeled simulated HIL imageries (<span id="S1.p4.1.2" class="ltx_text ltx_font_typewriter">lightbox</span>, <span id="S1.p4.1.3" class="ltx_text ltx_font_typewriter">sunlamp</span>) with distinct visual features and characteristics for testing. More HIL examples are visualized in Figures <a href="#S11.F10" title="Figure 10 ‣ 11.3 Example HIL Images with Model Projection ‣ 11 Visualization ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, <a href="#S11.F11" title="Figure 11 ‣ 11.3 Example HIL Images with Model Projection ‣ 11 Visualization ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> of Appendix <a href="#S11.SS3" title="11.3 Example HIL Images with Model Projection ‣ 11 Visualization ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11.3</span></a> for reference. SPEED+ is made publicly available to the aerospace community and beyond as part of the second international Satellite Pose Estimation Competition (SPEC2021). Moreover, a few baseline performance studies are conducted using existing spacecraft pose estimation CNN and domain-bridging algorithms to characterize the domain gap and learnability of SPEED+.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In summary, the contributions of SPEED+ are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">SPEED+ is a first-of-a-kind dataset for vision-only spacecraft pose estimation and relative navigation with emphasis on domain gap between synthetic training images and Hardward-In-the-Loop (HIL) test images from a robotic simulation testbed.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The test images of SPEED+ extend the full orientation space and distance up to 10 m with realistic re-creation of Earth albedo and direct sunlight present in spaceborne imagery. SPEED+ provides unique and unprecedented quantity and quality of mockup spacecraft images, which allow comprehensive evaluation of robustness of spaceborne ML models under a wide range of high-fidelity simulation of environmental settings.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The SPEED+ dataset is used in the second international Satellite Pose Estimation Competition (SPEC2021) with emphasis on robustness of spaceborne ML models across domain gap. The baseline studies in this work exhibit and justify the challenges associated with bridging the performance gap between the synthetic training and HIL test images.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Datasets for spacecraft pose estimation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Apart from SPEED, a number of other datasets for spacecraft pose estimation and spaceborne computer vision problems have been made public as well. For example, URSO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> consists of synthetic and spaceborne images of Soyuz and Dragon spacecraft rendered on Unreal Engine 4, but the labels for spaceborne images are missing. Dung et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> introduced a dataset comprising about 3,000 synthetic and spaceborne images of random satellites with bounding box and segmentation labels. Other authors have also developed their own datasets, such as synthetic images of the Envisat spacecraft rendered with Blender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and Cygnus spacecraft rendered with Blender and its Cycles rendering engine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>SPEED/SPEED+ dataset compositions for different domains and splits.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">SPEED</td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">SPEED+</td>
</tr>
<tr id="S2.T1.1.2" class="ltx_tr">
<td id="S2.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_r">splits</td>
<td id="S2.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.2.2.1" class="ltx_text ltx_font_typewriter">synthetic</span></td>
<td id="S2.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.2.3.1" class="ltx_text ltx_font_typewriter">real</span></td>
<td id="S2.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.2.4.1" class="ltx_text ltx_font_typewriter">synthetic</span></td>
<td id="S2.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.2.5.1" class="ltx_text ltx_font_typewriter">lightbox</span></td>
<td id="S2.T1.1.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.2.6.1" class="ltx_text ltx_font_typewriter">sunlamp</span></td>
</tr>
<tr id="S2.T1.1.3" class="ltx_tr">
<td id="S2.T1.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Train</td>
<td id="S2.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">12000</td>
<td id="S2.T1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td id="S2.T1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">47966</td>
<td id="S2.T1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S2.T1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S2.T1.1.4" class="ltx_tr">
<td id="S2.T1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">Validation</td>
<td id="S2.T1.1.4.2" class="ltx_td ltx_align_center">-</td>
<td id="S2.T1.1.4.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S2.T1.1.4.4" class="ltx_td ltx_align_center">11994</td>
<td id="S2.T1.1.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S2.T1.1.4.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S2.T1.1.5" class="ltx_tr">
<td id="S2.T1.1.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Test</td>
<td id="S2.T1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb">2998</td>
<td id="S2.T1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">300</td>
<td id="S2.T1.1.5.4" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S2.T1.1.5.5" class="ltx_td ltx_align_center ltx_border_bb">6740</td>
<td id="S2.T1.1.5.6" class="ltx_td ltx_align_center ltx_border_bb">2791</td>
</tr>
</table>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Testbeds for spaceborne optical navigation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Testbeds to simulate vision-based closed-loop navigation and control algorithms in general employ air-bearing platforms on a flat epoxy or granite floor with thrusters and actuators, such as ASTROS at the Georgia Institue of Technology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, POSEIDYN at the Naval Postgraduate School <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and M-STAR at the California Institute of Technology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. While these testbeds excel at simulating the spacecraft maneuver commands, their capabilities for creating a large-scale annotated dataset have not been showcased. In an effort to do away with reliance on synthetic imagery, Pasqualetto Cassinis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> have recently used the GRASL facility at ESTEC to generate 100 simulated HIL images of the 1:25 mockup model of the Envisat satellite. However, as the target mockup is mounted onto a static tripod, the configurable pose distribution is severely restricted. To the best of the authors’ knowledge, there is no publicly available dataset that consists of more than just hundreds of HIL or spaceborne images of a single target with accurate pose labels. This is not sufficient for a comprehensive evaluation of the robustness of a spaceborne ML model.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Datasets for domain gap</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">A number of datasets with emphasis on domain gap are available for terrestrial computer vision applications, such as Office-31 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, Syn2Real <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and DomainNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for various tasks such as classification, object detection and semantic segmentation. Perhaps the application most similar to the spaceborne navigation is the semantic segmentation for autonomous driving, in which the large-scale synthetic images from gaming engines such as GTA V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> or SYNTHIA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> are used for training, whereas real street images available from KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> or Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> are used for testing. Evidently, none of these images contain unique, challenging visual features and characteristics encountered in spaceborne imagery.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>SPEED+ Overview</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">SPEED+ is the next-generation dataset for spacecraft pose estimation with specific emphasis on the model robustness across the domain gap. While the presented SPEED+ consists of images of the Tango spacecraft from the PRISMA mission <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, the spacecraft model can be seamlessly exchanged in the future, and the presented methodology of data collection is general. SPEED+ consists of three different domains of imageries from two distinct sources. The first source is the OpenGL-based Optical Stimulator (OS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> camera emulator software of the SLAB’s multi-Satellite Software Simulator (<math id="S3.p1.1.m1.1" class="ltx_Math" alttext="S^{3}" display="inline"><semantics id="S3.p1.1.m1.1a"><msup id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">S</mi><mn id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">𝑆</ci><cn type="integer" id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">S^{3}</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, which is used to create <span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter">synthetic</span> domain comprising 59,960 synthetic images. The labeled <span id="S3.p1.1.2" class="ltx_text ltx_font_typewriter">synthetic</span> domain is split into 80:20 train/validation sets and is intended to be the main source of training of a CNN model. The second source is the TRON facility at SLAB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, which is used to generate two simulated HIL domains with different sources of illumination: <span id="S3.p1.1.3" class="ltx_text ltx_font_typewriter">lightbox</span> and <span id="S3.p1.1.4" class="ltx_text ltx_font_typewriter">sunlamp</span>. Specifically, these two domains are constructed using the realistic illumination conditions using light boxes with diffuser plates for albedo simulation and a sun lamp to mimic direct high-intensity homogeneous light from the Sun. Compared to synthetic imagery, they capture corner cases, stray lights, shadowing, and visual effects in general which are not easy to obtain through computer graphics. The <span id="S3.p1.1.5" class="ltx_text ltx_font_typewriter">lightbox</span> and <span id="S3.p1.1.6" class="ltx_text ltx_font_typewriter">sunlamp</span> domains are unlabeled and thus intended mainly for testing, representing a typical scenario in developing a spaceborne ML models in which the labeled images from the target space domain are not available prior to deployment. Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Datasets for spacecraft pose estimation ‣ 2 Related Work ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the compositions and the splits for different datasets under the old SPEED and SPEED+. It shows that, compared to its previous version, SPEED+ offers much larger quantity of HIL images compared to about just 300 HIL (dubbed <em id="S3.p1.1.7" class="ltx_emph ltx_font_italic">real</em>) images of SPEED.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">SPEED+ is made publicly available under the second Satellite Pose Estimation Competition (SPEC2021) and open for the aerospace community and others to develop and compare the performance of the robust spacecraft pose estimation ML models using labeled synthetic and unlabeled HIL datasets. The detailed plan for hosting, maintaining and licensing the dataset and the competition is provided in Appendix <a href="#S8" title="8 Accessibility ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>On Realism of TRON Simulation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Much effort has been put into ensuring the realism of the HIL images. First, the mockup model is manufactured via a third-party vendor based on a CAD model. While some details are simplified and omitted, critical model features such as antennae, brackets and external detail features are emulated with tolerances of <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\pm 0.0625" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mo id="S3.SS1.p1.1.m1.1.1a" xref="S3.SS1.p1.1.m1.1.1.cmml">±</mo><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">0.0625</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">0.0625</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\pm 0.0625</annotation></semantics></math> in. The solar cells are printed on high performance automotive grade laminated vinyl, and the bus and main body of the model are covered in simulated flat black MLI thermal blankets.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.3" class="ltx_p">In order to reproduce realistic space environment, the albedo light boxes and the sun lamp are carefully designed and calibrated to accurately capture the illumination characteristics in space. Specifically, the light boxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> are designed and calibrated to provide a uniform maximum radiance of 14 W/m<sup id="S3.SS1.p2.3.1" class="ltx_sup"><span id="S3.SS1.p2.3.1.1" class="ltx_text ltx_font_italic">2</span></sup>sr, which corresponds to the mean radiance from the Earth for an albedo coefficient of 0.3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and a solar irradiance of 1366 W/m<sup id="S3.SS1.p2.3.2" class="ltx_sup"><span id="S3.SS1.p2.3.2.1" class="ltx_text ltx_font_italic">2</span></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. The sun lamp, on the other hand, consists of a metal halide arc lamp and a paraboloidal mirror, which together are designed to produce a collimated beam with 1.0 solar constant in space (nominal 1357 W/m<sup id="S3.SS1.p2.3.3" class="ltx_sup"><span id="S3.SS1.p2.3.3.1" class="ltx_text ltx_font_italic">2</span></sup>) and a spectral response close to 6000 K. The readers are referred to Beierle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for more details.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Generating SPEED+ Images</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section first describes the process of collecting and post-processing the HIL images of SPEED+<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Example footage of the data collection process is available at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://youtu.be/kMAvjDW5vX4</span></span></span></span>. Then, it ends with the brief description of creating the synthetic imagery of SPEED+.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2110.03101/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(a) The visualization of the TRON facility simulation room layout. Ten light boxes (L1 <math id="S4.F2.3.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.F2.3.m1.1b"><mo id="S4.F2.3.m1.1.1" xref="S4.F2.3.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.F2.3.m1.1c"><csymbol cd="latexml" id="S4.F2.3.m1.1.1.cmml" xref="S4.F2.3.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.3.m1.1d">\sim</annotation></semantics></math> L10) and three positions of the sun lamp (S1 <math id="S4.F2.4.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.F2.4.m2.1b"><mo id="S4.F2.4.m2.1.1" xref="S4.F2.4.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.F2.4.m2.1c"><csymbol cd="latexml" id="S4.F2.4.m2.1.1.cmml" xref="S4.F2.4.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.4.m2.1d">\sim</annotation></semantics></math> S3) are marked and noted. (b) The half-scale Tango spacecraft mockup model is illuminated by two light boxes (L1, L2). (c) The same model is illuminated by the sun lamp placed at S1.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Illumination configurations used in SPEED+ simulated imageries.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T2.1.1.1.1" class="ltx_text"></span> <span id="S4.T2.1.1.1.2" class="ltx_text">
<span id="S4.T2.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Illumination</span></span>
<span id="S4.T2.1.1.1.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Configuration</span></span>
</span></span><span id="S4.T2.1.1.1.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">1</td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">2</td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">3</td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">4</td>
<td id="S4.T2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">5</td>
<td id="S4.T2.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">6</td>
<td id="S4.T2.1.1.8" class="ltx_td ltx_align_center ltx_border_tt">7</td>
<td id="S4.T2.1.1.9" class="ltx_td ltx_align_center ltx_border_tt">8</td>
<td id="S4.T2.1.1.10" class="ltx_td ltx_align_center ltx_border_tt">9</td>
<td id="S4.T2.1.1.11" class="ltx_td ltx_align_center ltx_border_tt">10</td>
</tr>
<tr id="S4.T2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">
<span id="S4.T2.1.2.1.1" class="ltx_text"></span> <span id="S4.T2.1.2.1.2" class="ltx_text">
<span id="S4.T2.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.2.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Activated</span></span>
<span id="S4.T2.1.2.1.2.1.2" class="ltx_tr">
<span id="S4.T2.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Source IDs</span></span>
</span></span><span id="S4.T2.1.2.1.3" class="ltx_text"></span></td>
<td id="S4.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">L1</td>
<td id="S4.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">[L1, L2]</td>
<td id="S4.T2.1.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">L2</td>
<td id="S4.T2.1.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">L5</td>
<td id="S4.T2.1.2.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">[L5, L6]</td>
<td id="S4.T2.1.2.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">L6</td>
<td id="S4.T2.1.2.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">[L6, L7]</td>
<td id="S4.T2.1.2.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">S1</td>
<td id="S4.T2.1.2.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">S2</td>
<td id="S4.T2.1.2.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">S3</td>
</tr>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>TRON Facility Overview</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">The TRON facility at SLAB is an 8 <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><times id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\times</annotation></semantics></math> 3 <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mo id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><times id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\times</annotation></semantics></math> 3 (m) simulation room which consists of two 6 Degrees-Of-Freedom (DOF) KUKA robot arms. One robot arm is fixed to the ground and holds a lightweight, reduced-scale mockup model of the target object, whereas the other arm holds the camera and can move along the ceiling-mounted linear rail across the room. The movements of both objects are tracked using two independent measurement sources: 12 Vicon Vero cameras <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> around the facility which track the infrared (IR) markers attached to both objects, and the KUKA system’s internal telemetry of the end-effectors of both robots. The measurements from both sources are used to jointly calibrate the facility, which allows the user to recover the true pose label of the target with respect to the camera from available measurements of an arbitrary sample with millimeter position and sub-degree orientation accuracies at close range. The reader is referred to Park et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> for details on the facility and its calibration procedure.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.2" class="ltx_p">In order to emulate a high-fidelity spaceborne illumination setting, the facility contains 10 Earth albedo lightboxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and a metal halide arc lamp. The configuration of the lightboxes is visualized in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a) along with the IDs associated with each lightbox (i.e., L1 <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\sim</annotation></semantics></math> L10). Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a) also shows that the sun lamp is placed at three distinct locations at varying angles with respect to the ground-fixed robot arm (S1 <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mo id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><csymbol cd="latexml" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\sim</annotation></semantics></math> S3), with the lamp facing directly towards the spacecraft mockup model. Figures <a href="#S4.F2" title="Figure 2 ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b), (c) show both the lightboxes and the sun lamp in operation. The applicable components of the facility are painted black, and all deactivated sources of illumination are covered with light-absorbing black commando cloths to maximally suppress the reflection and ambient light.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>(<em id="S4.F3.16.1" class="ltx_emph ltx_font_italic">Left</em>) Position (<em id="S4.F3.17.2" class="ltx_emph ltx_font_italic">top</em>) and orientation (<em id="S4.F3.18.3" class="ltx_emph ltx_font_italic">bottom</em>) label distributions for <span id="S4.F3.19.4" class="ltx_text ltx_font_typewriter">lightbox</span>, and <span id="S4.F3.20.5" class="ltx_text ltx_font_typewriter">sunlamp</span> datasets. The position label is represented in the camera frame (<math id="S4.F3.5.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.F3.5.m1.1b"><mi id="S4.F3.5.m1.1.1" xref="S4.F3.5.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.F3.5.m1.1c"><ci id="S4.F3.5.m1.1.1.cmml" xref="S4.F3.5.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.5.m1.1d">C</annotation></semantics></math>) whose <math id="S4.F3.6.m2.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.F3.6.m2.1b"><mi id="S4.F3.6.m2.1.1" xref="S4.F3.6.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.F3.6.m2.1c"><ci id="S4.F3.6.m2.1.1.cmml" xref="S4.F3.6.m2.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.6.m2.1d">z</annotation></semantics></math>-axis is along the camera boresight, and the <math id="S4.F3.7.m3.1" class="ltx_Math" alttext="xy" display="inline"><semantics id="S4.F3.7.m3.1b"><mrow id="S4.F3.7.m3.1.1" xref="S4.F3.7.m3.1.1.cmml"><mi id="S4.F3.7.m3.1.1.2" xref="S4.F3.7.m3.1.1.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.F3.7.m3.1.1.1" xref="S4.F3.7.m3.1.1.1.cmml">​</mo><mi id="S4.F3.7.m3.1.1.3" xref="S4.F3.7.m3.1.1.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.7.m3.1c"><apply id="S4.F3.7.m3.1.1.cmml" xref="S4.F3.7.m3.1.1"><times id="S4.F3.7.m3.1.1.1.cmml" xref="S4.F3.7.m3.1.1.1"></times><ci id="S4.F3.7.m3.1.1.2.cmml" xref="S4.F3.7.m3.1.1.2">𝑥</ci><ci id="S4.F3.7.m3.1.1.3.cmml" xref="S4.F3.7.m3.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.7.m3.1d">xy</annotation></semantics></math>-axes form the image plane. The relative orientation distribution is paremetrized as ZYX Euler angles. (<em id="S4.F3.21.6" class="ltx_emph ltx_font_italic">Right</em>) The distribution of incident sunlamp direction as spherical coordinates in the target body reference frame (<math id="S4.F3.8.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.F3.8.m4.1b"><mi id="S4.F3.8.m4.1.1" xref="S4.F3.8.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.F3.8.m4.1c"><ci id="S4.F3.8.m4.1.1.cmml" xref="S4.F3.8.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.8.m4.1d">T</annotation></semantics></math>). Samples are color-coded for different illumination configurations for <span id="S4.F3.22.7" class="ltx_text ltx_font_typewriter">sunlamp</span> domain.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2110.03101/assets/figures/speed_real_distribution.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Camera poses for 305 real images of SPEED in the Tango model’s body frame (<math id="S4.F4.2.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.F4.2.m1.1b"><mi id="S4.F4.2.m1.1.1" xref="S4.F4.2.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.F4.2.m1.1c"><ci id="S4.F4.2.m1.1.1.cmml" xref="S4.F4.2.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.2.m1.1d">B</annotation></semantics></math>) from two views. The simplified wireframe model of the satellite is plotted in green; camera poses are plotted in red and black for test and training samples, respectively. Directly taken from Kisantal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2110.03101/assets/x4.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="160" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Raw (<em id="S4.F5.5.1" class="ltx_emph ltx_font_italic">top</em>) and processed (<em id="S4.F5.6.2" class="ltx_emph ltx_font_italic">bottom</em>) images from the <span id="S4.F5.7.3" class="ltx_text ltx_font_typewriter">lightbox</span> and <span id="S4.F5.8.4" class="ltx_text ltx_font_typewriter">sunlamp</span> domains.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2110.03101/assets/x5.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Pipeline for post-processing the <span id="S4.F6.2.1" class="ltx_text ltx_font_typewriter">sunlamp</span> images.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Collecting SPEED+ HIL Images</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The HIL images of SPEED+ involve a half-scale mockup model of the Tango spacecraft from the PRISMA mission <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The creation of the SPEED+ HIL imageries begins with generating a total of 10,000 pose labels, which are then translated into the commands of both robots. In theory, there is a countless number of pairs of robot commands that will achieve the same prescribed pose label. In order to simplify the process of commands generation and data collection, the target model is commanded at a constant location within the facility regardless of its orientation. Moreover, the camera and the target are commanded such that the camera’s boresight is always directed towards the target’s location along <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="-x" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mo id="S4.SS2.p1.1.m1.1.1a" xref="S4.SS2.p1.1.m1.1.1.cmml">−</mo><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><minus id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"></minus><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">-x</annotation></semantics></math>-axis as visualized in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a). The position labels are generated in compliance with these operational constraints, while ensuring (1) the camera’s pose is reconfigurable with the ceiling-mounted robot arm, and (2) the target’s center is visible within the image frame of the camera. On the other hand, the orientation labels are simply sampled from the uniform distribution in the <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">SO</em>(3) space using the subgroup algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. The mockup model of the Tango spacecraft is manufactured with two mounting holes at opposite sides, so that depending on the direction of the camera’s viewpoint with respect to the model, it can be mounted on either side to ensure the robot arm does not obstruct its view in any way.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.3" class="ltx_p">With all pose labels translated into the robot arm commands, they are divided into 10 partitions of random 1,000 images based on different illumination configurations shown in Table <a href="#S4.T2" title="Table 2 ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. It indicates that the configurations 1 <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mo id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\sim</annotation></semantics></math> 7 employ different sets of light boxes, whereas the configurations 8 <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mo id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\sim</annotation></semantics></math> 10 employ the sun lamp at different locations along the facility as marked in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a). Naturally, the 70% of the simulated images for configurations 1 <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mo id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><csymbol cd="latexml" id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">\sim</annotation></semantics></math> 7 are placed under <span id="S4.SS2.p2.3.1" class="ltx_text ltx_font_typewriter">lightbox</span> domain, and the remaining 30% under <span id="S4.SS2.p2.3.2" class="ltx_text ltx_font_typewriter">sunlamp</span> domain. During the data collection, the intensity of each activated lightboxes is randomly selected from a distribution that is representative of various orbit altitudes for each sample in <span id="S4.SS2.p2.3.3" class="ltx_text ltx_font_typewriter">lightbox</span> domain, whereas the sun lamp is left at constant intensity facing the same direction for <span id="S4.SS2.p2.3.4" class="ltx_text ltx_font_typewriter">sunlamp</span> domain.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1 TRON Facility Overview ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> depicts the pose distributions of the pose labels for the HIL images, which confirms that for both <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_typewriter">lightbox</span> and <span id="S4.SS2.p3.1.2" class="ltx_text ltx_font_typewriter">sunlamp</span> datasets, the position and orientation labels are well sampled from the uniform distributions of the 3D Cartesian and <em id="S4.SS2.p3.1.3" class="ltx_emph ltx_font_italic">SO</em>(3) spaces, respectively. This is a big improvement compared to the severely restricted pose distribution of SPEED real images as visualized in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1 TRON Facility Overview ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Moreover, Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1 TRON Facility Overview ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that for <span id="S4.SS2.p3.1.4" class="ltx_text ltx_font_typewriter">sunlamp</span> domain, the direction of incident light is well-distributed with respect to the mockup target, illuminating it from all possible angles.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Post-Processing</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">As shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.1 TRON Facility Overview ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the raw images include not only the target mockup model of interest but also other components of the facility, such as the robot arm, Vicon cameras, activated lightboxes, etc. While a robust ML model should be able to perform the task of spacecraft pose estimation regardless of the presence of these components, in the setting of the competition, they may pose risks of containing information that can be extracted and exploited to aid the given task. In order to eliminate such possibility altogether, these raw images are post-processed to mask out the unrelated items in the image while simultaneously striving to emulate as closely as possible the visual properties of the spaceborne imagery.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The post-processing procedure for <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">lightbox</span> images loosely follows that of the synthetic imagery of SPEED detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. First, the image backgrounds are simply masked out using the binary masks rendered based on the pose labels. Then, either random starfields rendered with the OS software or random patches of the Earth images captured from the Himawari-8 geostationary meteorological satellite <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> are substitued to the backgrounds, as shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.1 TRON Facility Overview ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Specifically, an Earth image with incident sunlight direction that approximately matches the active lightbox configuration is chosen for background substitution. Finally, Gaussian blurring and zero-mean Gaussian white noise are added to emulate the depth of field and additional thermal noise, respectively. Specifically, blurring helps mitigate the sharp boundaries between the satellite foreground and the starfield/Earth background.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.5" class="ltx_p">On the other hand, the <span id="S4.SS3.p3.5.3" class="ltx_text ltx_font_typewriter">sunlamp</span> raw images contain interesting properties unavailable in <span id="S4.SS3.p3.5.4" class="ltx_text ltx_font_typewriter">lightbox</span> raw images, such as patterned flare introduced by the sun lamp and intense surface glow due to high reflectivity and overexposure of the camera (see Figure <a href="#S4.F5" title="Figure 5 ‣ 4.1 TRON Facility Overview ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). In many cases, the glow is too strong such that it obfuscates the shape and boundaries of the target model, effectively acting as an occlusion noise in the image. Therefore, applying the same post-processing as to <span id="S4.SS3.p3.5.5" class="ltx_text ltx_font_typewriter">lightbox</span> images using binary masks will not only eliminate challenging visual effects created by the sun lamp, but it will also give away the exact silhouette of the target model that is otherwise unavailable in the raw images. To mitigate such issues, the post-processing of <span id="S4.SS3.p3.5.6" class="ltx_text ltx_font_typewriter">sunlamp</span> images instead blurs out all the unnecessary components in the image except the general area surrounding the model and the sun lamp, as shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.1 TRON Facility Overview ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The overall pipeline is shown in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.1 TRON Facility Overview ‣ 4 Generating SPEED+ Images ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, which involves the approximate mask for all the components that must be removed (<math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="m_{X}" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><msub id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">m</mi><mi id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">𝑚</ci><ci id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">m_{X}</annotation></semantics></math>) and the target model and its surrounding glow effects (<math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="m_{T}" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><msub id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mi id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml">m</mi><mi id="S4.SS3.p3.2.m2.1.1.3" xref="S4.SS3.p3.2.m2.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2">𝑚</ci><ci id="S4.SS3.p3.2.m2.1.1.3.cmml" xref="S4.SS3.p3.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">m_{T}</annotation></semantics></math>). Then, given the original image (<math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mi id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><ci id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">I</annotation></semantics></math>), the final result is equivalent to the output of the following MATLAB command, <span id="S4.SS3.p3.5.2" class="ltx_text ltx_font_typewriter">regionfill(<math id="S4.SS3.p3.4.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S4.SS3.p3.4.1.m1.1a"><mi id="S4.SS3.p3.4.1.m1.1.1" xref="S4.SS3.p3.4.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.4.1.m1.1b"><ci id="S4.SS3.p3.4.1.m1.1.1.cmml" xref="S4.SS3.p3.4.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.4.1.m1.1c">I</annotation></semantics></math>, <math id="S4.SS3.p3.5.2.m2.1" class="ltx_Math" alttext="m_{X}-m_{T}" display="inline"><semantics id="S4.SS3.p3.5.2.m2.1a"><mrow id="S4.SS3.p3.5.2.m2.1.1" xref="S4.SS3.p3.5.2.m2.1.1.cmml"><msub id="S4.SS3.p3.5.2.m2.1.1.2" xref="S4.SS3.p3.5.2.m2.1.1.2.cmml"><mi id="S4.SS3.p3.5.2.m2.1.1.2.2" xref="S4.SS3.p3.5.2.m2.1.1.2.2.cmml">m</mi><mi id="S4.SS3.p3.5.2.m2.1.1.2.3" xref="S4.SS3.p3.5.2.m2.1.1.2.3.cmml">X</mi></msub><mo id="S4.SS3.p3.5.2.m2.1.1.1" xref="S4.SS3.p3.5.2.m2.1.1.1.cmml">−</mo><msub id="S4.SS3.p3.5.2.m2.1.1.3" xref="S4.SS3.p3.5.2.m2.1.1.3.cmml"><mi id="S4.SS3.p3.5.2.m2.1.1.3.2" xref="S4.SS3.p3.5.2.m2.1.1.3.2.cmml">m</mi><mi id="S4.SS3.p3.5.2.m2.1.1.3.3" xref="S4.SS3.p3.5.2.m2.1.1.3.3.cmml">T</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.5.2.m2.1b"><apply id="S4.SS3.p3.5.2.m2.1.1.cmml" xref="S4.SS3.p3.5.2.m2.1.1"><minus id="S4.SS3.p3.5.2.m2.1.1.1.cmml" xref="S4.SS3.p3.5.2.m2.1.1.1"></minus><apply id="S4.SS3.p3.5.2.m2.1.1.2.cmml" xref="S4.SS3.p3.5.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p3.5.2.m2.1.1.2.1.cmml" xref="S4.SS3.p3.5.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS3.p3.5.2.m2.1.1.2.2.cmml" xref="S4.SS3.p3.5.2.m2.1.1.2.2">𝑚</ci><ci id="S4.SS3.p3.5.2.m2.1.1.2.3.cmml" xref="S4.SS3.p3.5.2.m2.1.1.2.3">𝑋</ci></apply><apply id="S4.SS3.p3.5.2.m2.1.1.3.cmml" xref="S4.SS3.p3.5.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p3.5.2.m2.1.1.3.1.cmml" xref="S4.SS3.p3.5.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS3.p3.5.2.m2.1.1.3.2.cmml" xref="S4.SS3.p3.5.2.m2.1.1.3.2">𝑚</ci><ci id="S4.SS3.p3.5.2.m2.1.1.3.3.cmml" xref="S4.SS3.p3.5.2.m2.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.5.2.m2.1c">m_{X}-m_{T}</annotation></semantics></math>)</span>, which effectively blurs out the foreground of the given mask.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">The final results of the HIL images inevitably include some artifacts that are unique to <span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_typewriter">lightbox</span> and <span id="S4.SS3.p4.1.2" class="ltx_text ltx_font_typewriter">sunlamp</span> images: the infrared markers and the mounting holes, which are not present in the <span id="S4.SS3.p4.1.3" class="ltx_text ltx_font_typewriter">synthetic</span> domain. As these components do not affect the pose labels and are indispensable for the data collection, they are retained in both HIL images. The other artifact unique to <span id="S4.SS3.p4.1.4" class="ltx_text ltx_font_typewriter">sunlamp</span> images is the shape of the surface glow, which is a byproduct of the post-processing step that aims to distinguish the glow effects from the target’s surface and the robot arm holding the target. Though somewhat unnatural, the irregular shape of the glow effect is retained as long as it does not add any information about the target’s shape. The examples of these artifacts are shown in Figure <a href="#S10.F8" title="Figure 8 ‣ 10.3 HigherHRNet [41] ‣ 10 Training Details ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> of Appendix <a href="#S11.SS1" title="11.1 Retained Artifacts in HIL Images ‣ 11 Visualization ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11.1</span></a>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Manual Removal of Samples</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">At the end of the post-processing, some samples are manually removed from both <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_typewriter">lightbox</span> and <span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_typewriter">sunlamp</span> images, leaving total 6,740 and 2,791 images for respective image domains. The majority of rejected <span id="S4.SS4.p1.1.3" class="ltx_text ltx_font_typewriter">lightbox</span> images are due to non-negligible misalignment between the spacecraft and its binary mask when Earth images are inserted as the background. While the pose labels estimated by the facility have millimeter-level and sub-degree-level position and orientation accuracies at close range, it is possible that there may be up to a centimeter-level position error beyond close range due to imperfect calibration of the facility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. For example, a position error induced by the misaligned camera boresight would scale linearly with the distance to the target, so 0.1<sup id="S4.SS4.p1.1.4" class="ltx_sup">∘</sup> error would cause 1.75 cm position error at 10 m distance. The misalignment of the binary mask then creates a gap between the spacecraft and the Earth background which could aid the pose estimation task. Therefore, any images with such misalignment observed at more than one edge of the spacecraft are manually discarded by the authors.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">For <span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_typewriter">sunlamp</span> images, the samples are rejected based on two criteria. One is the case in which the model’s surface reflection is so severe that a human cannot even approximate the pose of the target in an image. The second is when the post-processing step, in an attempt to retain the shape of the surface glow, fails to discriminate and mask the reflection caused by the robot arm holding the model. Some of the rejected samples are visualized in Figure <a href="#S10.F9" title="Figure 9 ‣ 10.3 HigherHRNet [41] ‣ 10 Training Details ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> of Appendix <a href="#S11.SS2" title="11.2 Images of Rejected Samples ‣ 11 Visualization ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11.2</span></a>.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Generating SPEED+ Synthetic images</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.3" class="ltx_p">Once the outliers are rejected from <span id="S4.SS5.p1.3.1" class="ltx_text ltx_font_typewriter">lightbox</span> and <span id="S4.SS5.p1.3.2" class="ltx_text ltx_font_typewriter">sunlamp</span> domains, the synthetic imagery is created based on the observed pose distribution of the complete HIL imageries. Specifically, the distribution of the position labels encompasses those of the HIL imageries with the separation along <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mi id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><ci id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">z</annotation></semantics></math>-axis sampled from <math id="S4.SS5.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{U}" display="inline"><semantics id="S4.SS5.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml">𝒰</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><ci id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">𝒰</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">\mathcal{U}</annotation></semantics></math><math id="S4.SS5.p1.3.m3.2" class="ltx_Math" alttext="(2.25,10.0)" display="inline"><semantics id="S4.SS5.p1.3.m3.2a"><mrow id="S4.SS5.p1.3.m3.2.3.2" xref="S4.SS5.p1.3.m3.2.3.1.cmml"><mo stretchy="false" id="S4.SS5.p1.3.m3.2.3.2.1" xref="S4.SS5.p1.3.m3.2.3.1.cmml">(</mo><mn id="S4.SS5.p1.3.m3.1.1" xref="S4.SS5.p1.3.m3.1.1.cmml">2.25</mn><mo id="S4.SS5.p1.3.m3.2.3.2.2" xref="S4.SS5.p1.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS5.p1.3.m3.2.2" xref="S4.SS5.p1.3.m3.2.2.cmml">10.0</mn><mo stretchy="false" id="S4.SS5.p1.3.m3.2.3.2.3" xref="S4.SS5.p1.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.3.m3.2b"><interval closure="open" id="S4.SS5.p1.3.m3.2.3.1.cmml" xref="S4.SS5.p1.3.m3.2.3.2"><cn type="float" id="S4.SS5.p1.3.m3.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1">2.25</cn><cn type="float" id="S4.SS5.p1.3.m3.2.2.cmml" xref="S4.SS5.p1.3.m3.2.2">10.0</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.3.m3.2c">(2.25,10.0)</annotation></semantics></math> [m]. The images are rendered using the camera’s intrinsic parameters estimated from the calibration of TRON, which are specified in Appendix <a href="#S9" title="9 Camera Model ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. Then, similar to the SPEED synthetic dataset, random Earth backgrounds are inserted to half of the entire <span id="S4.SS5.p1.3.3" class="ltx_text ltx_font_typewriter">synthetic</span> images. Finally, Gaussian blur and noise are added, similar to <span id="S4.SS5.p1.3.4" class="ltx_text ltx_font_typewriter">lightbox</span> images.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section provides a few baseline performance studies, starting with the <span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">synthetic</span>-only experiments (i.e., trained only on <span id="S5.p1.1.2" class="ltx_text ltx_font_typewriter">synthetic</span> training set) to provide a lower-bound performance on both HIL domains. A number of CNNs with different pose estimation architectures are tested on HIL images to demonstrate the domain gap between the training and test images. Then, two different domain adaptation and randomization algorithms are applied to one of the CNNs as a benchmark. The oracle performances are also approximated by in-domain training (i.e., train and test on the same HIL domain), where the performance is approximated via 5-fold cross validation, i.e., averaged from testing on one of the 5 equal-sized randomly partitioned sets after being trained on the rest. Thus, the oracle represents the upper-bound performance achievable by the given CNN model on the HIL domains. The purpose of these studies is to characterize the domain gap between the synthetic and HIL images and to show that the HIL images and labels from the robotic testbed do retain learnable features. Finally, the same model and the domain randomization algorithm are evaluated on the <span id="S5.p1.1.3" class="ltx_text ltx_font_typewriter">prisma25</span> dataset, which consists of 25 flight images of the same Tango spacecraft from the PRISMA mission <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Despite the limited availability of <span id="S5.p1.1.4" class="ltx_text ltx_font_typewriter">prisma25</span>, the comparative study between SPEED+ HIL and flight images gives insight to the applicability of HIL images as a surrogate for testing.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Pose estimation CNNs</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.4" class="ltx_p">First, the baseline performance study uses three different pose estimation CNNs: the Keypoint Regression Network (KRN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and Spacecraft Pose Network (SPN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, which are the two baseline models for SPEED, and HigherHRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, the state-of-the art model for the bottom-up human joint detection task. These networks employ different strategies for spacecraft pose estimation. For example, SPN simultaneously predicts <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">N</annotation></semantics></math> closest discrete attitude classes and the relative weights associated with each class, which are then averaged to compute the final attitude. Then, based on the target’s model, predicted attitude and bounding box, SPN solves for the relative position by minimizing the residuals between the bounding box and the extremal points of the projected target model using a Gauss-Newton algorithm. On the other hand, KRN directly regress the locations of 11 pre-designated keypoints of the Tango spacecraft’s corners and antennae. Then, given the known 2D-3D keypoint correspondences, a Perspective-<math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mi id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">n</annotation></semantics></math>-Point (P<math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><mi id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><ci id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">n</annotation></semantics></math>P) problem is solved to output the complete 6D pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. Finally, HigherHRNet is trained to detect the same 11 keypoints as in KRN, but it returns heatmaps associated with each keypoints. Then, P<math id="S5.SS1.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS1.p1.4.m4.1a"><mi id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><ci id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">n</annotation></semantics></math>P is solved for complete 6D pose.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Note that both KRN and SPN depend on a separate object detection network to detect and crop the bounding box around the spacecraft prior to running the pose estimation networks. In this study, the first-stage object detection network is skipped to simplify the study, especially since the problem of detecting a single known object is a much easier problem than the 6D pose estimation. Instead, the ground-truth bounding boxes computed from the pose labels are used to crop the images around the target during pre-processing.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S5.T3.21.1" class="ltx_text ltx_font_typewriter">synthetic</span>-only performances of the baseline models, tested on <span id="S5.T3.22.2" class="ltx_text ltx_font_typewriter">synthetic</span> validation, <span id="S5.T3.23.3" class="ltx_text ltx_font_typewriter">lightbox</span> and <span id="S5.T3.24.4" class="ltx_text ltx_font_typewriter">sunlamp</span> images.</figcaption>
<table id="S5.T3.16" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T3.16.17" class="ltx_tr">
<td id="S5.T3.16.17.1" class="ltx_td ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S5.T3.16.17.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="3"><span id="S5.T3.16.17.2.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">synthetic</span></td>
<td id="S5.T3.16.17.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="3"><span id="S5.T3.16.17.3.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">lightbox</span></td>
<td id="S5.T3.16.17.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="3"><span id="S5.T3.16.17.4.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">sunlamp</span></td>
</tr>
<tr id="S5.T3.12.12" class="ltx_tr">
<td id="S5.T3.12.12.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.12.12.13.1" class="ltx_text" style="font-size:70%;">Model</span></td>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="E_{\textrm{T}}" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><msub id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml"><mi mathsize="70%" id="S5.T3.1.1.1.m1.1.1.2" xref="S5.T3.1.1.1.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T3.1.1.1.m1.1.1.3" xref="S5.T3.1.1.1.m1.1.1.3a.cmml">T</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T3.1.1.1.m1.1.1.2.cmml" xref="S5.T3.1.1.1.m1.1.1.2">𝐸</ci><ci id="S5.T3.1.1.1.m1.1.1.3a.cmml" xref="S5.T3.1.1.1.m1.1.1.3"><mtext mathsize="49%" id="S5.T3.1.1.1.m1.1.1.3.cmml" xref="S5.T3.1.1.1.m1.1.1.3">T</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">E_{\textrm{T}}</annotation></semantics></math><span id="S5.T3.1.1.1.1" class="ltx_text" style="font-size:70%;"> [m]</span>
</td>
<td id="S5.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T3.2.2.2.m1.1" class="ltx_Math" alttext="E_{\textrm{R}}" display="inline"><semantics id="S5.T3.2.2.2.m1.1a"><msub id="S5.T3.2.2.2.m1.1.1" xref="S5.T3.2.2.2.m1.1.1.cmml"><mi mathsize="70%" id="S5.T3.2.2.2.m1.1.1.2" xref="S5.T3.2.2.2.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T3.2.2.2.m1.1.1.3" xref="S5.T3.2.2.2.m1.1.1.3a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><apply id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.2.2.2.m1.1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1">subscript</csymbol><ci id="S5.T3.2.2.2.m1.1.1.2.cmml" xref="S5.T3.2.2.2.m1.1.1.2">𝐸</ci><ci id="S5.T3.2.2.2.m1.1.1.3a.cmml" xref="S5.T3.2.2.2.m1.1.1.3"><mtext mathsize="49%" id="S5.T3.2.2.2.m1.1.1.3.cmml" xref="S5.T3.2.2.2.m1.1.1.3">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">E_{\textrm{R}}</annotation></semantics></math><span id="S5.T3.3.3.3.1" class="ltx_text" style="font-size:70%;"> [</span><sup id="S5.T3.3.3.3.2" class="ltx_sup"><span id="S5.T3.3.3.3.2.1" class="ltx_text" style="font-size:70%;">∘</span></sup><span id="S5.T3.3.3.3.3" class="ltx_text" style="font-size:70%;">]</span>
</td>
<td id="S5.T3.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T3.4.4.4.m1.1" class="ltx_Math" alttext="E_{\textrm{pose}}" display="inline"><semantics id="S5.T3.4.4.4.m1.1a"><msub id="S5.T3.4.4.4.m1.1.1" xref="S5.T3.4.4.4.m1.1.1.cmml"><mi mathsize="70%" id="S5.T3.4.4.4.m1.1.1.2" xref="S5.T3.4.4.4.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T3.4.4.4.m1.1.1.3" xref="S5.T3.4.4.4.m1.1.1.3a.cmml">pose</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.m1.1b"><apply id="S5.T3.4.4.4.m1.1.1.cmml" xref="S5.T3.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.4.4.4.m1.1.1.1.cmml" xref="S5.T3.4.4.4.m1.1.1">subscript</csymbol><ci id="S5.T3.4.4.4.m1.1.1.2.cmml" xref="S5.T3.4.4.4.m1.1.1.2">𝐸</ci><ci id="S5.T3.4.4.4.m1.1.1.3a.cmml" xref="S5.T3.4.4.4.m1.1.1.3"><mtext mathsize="49%" id="S5.T3.4.4.4.m1.1.1.3.cmml" xref="S5.T3.4.4.4.m1.1.1.3">pose</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.m1.1c">E_{\textrm{pose}}</annotation></semantics></math><span id="S5.T3.4.4.4.1" class="ltx_text" style="font-size:70%;"> [-]</span>
</td>
<td id="S5.T3.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T3.5.5.5.m1.1" class="ltx_Math" alttext="E_{\textrm{T}}" display="inline"><semantics id="S5.T3.5.5.5.m1.1a"><msub id="S5.T3.5.5.5.m1.1.1" xref="S5.T3.5.5.5.m1.1.1.cmml"><mi mathsize="70%" id="S5.T3.5.5.5.m1.1.1.2" xref="S5.T3.5.5.5.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T3.5.5.5.m1.1.1.3" xref="S5.T3.5.5.5.m1.1.1.3a.cmml">T</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.m1.1b"><apply id="S5.T3.5.5.5.m1.1.1.cmml" xref="S5.T3.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.5.5.5.m1.1.1.1.cmml" xref="S5.T3.5.5.5.m1.1.1">subscript</csymbol><ci id="S5.T3.5.5.5.m1.1.1.2.cmml" xref="S5.T3.5.5.5.m1.1.1.2">𝐸</ci><ci id="S5.T3.5.5.5.m1.1.1.3a.cmml" xref="S5.T3.5.5.5.m1.1.1.3"><mtext mathsize="49%" id="S5.T3.5.5.5.m1.1.1.3.cmml" xref="S5.T3.5.5.5.m1.1.1.3">T</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.m1.1c">E_{\textrm{T}}</annotation></semantics></math><span id="S5.T3.5.5.5.1" class="ltx_text" style="font-size:70%;"> [m]</span>
</td>
<td id="S5.T3.7.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T3.6.6.6.m1.1" class="ltx_Math" alttext="E_{\textrm{R}}" display="inline"><semantics id="S5.T3.6.6.6.m1.1a"><msub id="S5.T3.6.6.6.m1.1.1" xref="S5.T3.6.6.6.m1.1.1.cmml"><mi mathsize="70%" id="S5.T3.6.6.6.m1.1.1.2" xref="S5.T3.6.6.6.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T3.6.6.6.m1.1.1.3" xref="S5.T3.6.6.6.m1.1.1.3a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.m1.1b"><apply id="S5.T3.6.6.6.m1.1.1.cmml" xref="S5.T3.6.6.6.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.6.6.6.m1.1.1.1.cmml" xref="S5.T3.6.6.6.m1.1.1">subscript</csymbol><ci id="S5.T3.6.6.6.m1.1.1.2.cmml" xref="S5.T3.6.6.6.m1.1.1.2">𝐸</ci><ci id="S5.T3.6.6.6.m1.1.1.3a.cmml" xref="S5.T3.6.6.6.m1.1.1.3"><mtext mathsize="49%" id="S5.T3.6.6.6.m1.1.1.3.cmml" xref="S5.T3.6.6.6.m1.1.1.3">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.m1.1c">E_{\textrm{R}}</annotation></semantics></math><span id="S5.T3.7.7.7.1" class="ltx_text" style="font-size:70%;"> [</span><sup id="S5.T3.7.7.7.2" class="ltx_sup"><span id="S5.T3.7.7.7.2.1" class="ltx_text" style="font-size:70%;">∘</span></sup><span id="S5.T3.7.7.7.3" class="ltx_text" style="font-size:70%;">]</span>
</td>
<td id="S5.T3.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T3.8.8.8.m1.1" class="ltx_Math" alttext="E_{\textrm{pose}}^{*}" display="inline"><semantics id="S5.T3.8.8.8.m1.1a"><msubsup id="S5.T3.8.8.8.m1.1.1" xref="S5.T3.8.8.8.m1.1.1.cmml"><mi mathsize="70%" id="S5.T3.8.8.8.m1.1.1.2.2" xref="S5.T3.8.8.8.m1.1.1.2.2.cmml">E</mi><mtext mathsize="70%" id="S5.T3.8.8.8.m1.1.1.2.3" xref="S5.T3.8.8.8.m1.1.1.2.3a.cmml">pose</mtext><mo mathsize="70%" id="S5.T3.8.8.8.m1.1.1.3" xref="S5.T3.8.8.8.m1.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.8.m1.1b"><apply id="S5.T3.8.8.8.m1.1.1.cmml" xref="S5.T3.8.8.8.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.8.8.8.m1.1.1.1.cmml" xref="S5.T3.8.8.8.m1.1.1">superscript</csymbol><apply id="S5.T3.8.8.8.m1.1.1.2.cmml" xref="S5.T3.8.8.8.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.8.8.8.m1.1.1.2.1.cmml" xref="S5.T3.8.8.8.m1.1.1">subscript</csymbol><ci id="S5.T3.8.8.8.m1.1.1.2.2.cmml" xref="S5.T3.8.8.8.m1.1.1.2.2">𝐸</ci><ci id="S5.T3.8.8.8.m1.1.1.2.3a.cmml" xref="S5.T3.8.8.8.m1.1.1.2.3"><mtext mathsize="49%" id="S5.T3.8.8.8.m1.1.1.2.3.cmml" xref="S5.T3.8.8.8.m1.1.1.2.3">pose</mtext></ci></apply><times id="S5.T3.8.8.8.m1.1.1.3.cmml" xref="S5.T3.8.8.8.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.8.m1.1c">E_{\textrm{pose}}^{*}</annotation></semantics></math><span id="S5.T3.8.8.8.1" class="ltx_text" style="font-size:70%;"> [-]</span>
</td>
<td id="S5.T3.9.9.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T3.9.9.9.m1.1" class="ltx_Math" alttext="E_{\textrm{T}}" display="inline"><semantics id="S5.T3.9.9.9.m1.1a"><msub id="S5.T3.9.9.9.m1.1.1" xref="S5.T3.9.9.9.m1.1.1.cmml"><mi mathsize="70%" id="S5.T3.9.9.9.m1.1.1.2" xref="S5.T3.9.9.9.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T3.9.9.9.m1.1.1.3" xref="S5.T3.9.9.9.m1.1.1.3a.cmml">T</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.9.9.9.m1.1b"><apply id="S5.T3.9.9.9.m1.1.1.cmml" xref="S5.T3.9.9.9.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.9.9.9.m1.1.1.1.cmml" xref="S5.T3.9.9.9.m1.1.1">subscript</csymbol><ci id="S5.T3.9.9.9.m1.1.1.2.cmml" xref="S5.T3.9.9.9.m1.1.1.2">𝐸</ci><ci id="S5.T3.9.9.9.m1.1.1.3a.cmml" xref="S5.T3.9.9.9.m1.1.1.3"><mtext mathsize="49%" id="S5.T3.9.9.9.m1.1.1.3.cmml" xref="S5.T3.9.9.9.m1.1.1.3">T</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.9.9.9.m1.1c">E_{\textrm{T}}</annotation></semantics></math><span id="S5.T3.9.9.9.1" class="ltx_text" style="font-size:70%;"> [m]</span>
</td>
<td id="S5.T3.11.11.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T3.10.10.10.m1.1" class="ltx_Math" alttext="E_{\textrm{R}}" display="inline"><semantics id="S5.T3.10.10.10.m1.1a"><msub id="S5.T3.10.10.10.m1.1.1" xref="S5.T3.10.10.10.m1.1.1.cmml"><mi mathsize="70%" id="S5.T3.10.10.10.m1.1.1.2" xref="S5.T3.10.10.10.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T3.10.10.10.m1.1.1.3" xref="S5.T3.10.10.10.m1.1.1.3a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.10.10.10.m1.1b"><apply id="S5.T3.10.10.10.m1.1.1.cmml" xref="S5.T3.10.10.10.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.10.10.10.m1.1.1.1.cmml" xref="S5.T3.10.10.10.m1.1.1">subscript</csymbol><ci id="S5.T3.10.10.10.m1.1.1.2.cmml" xref="S5.T3.10.10.10.m1.1.1.2">𝐸</ci><ci id="S5.T3.10.10.10.m1.1.1.3a.cmml" xref="S5.T3.10.10.10.m1.1.1.3"><mtext mathsize="49%" id="S5.T3.10.10.10.m1.1.1.3.cmml" xref="S5.T3.10.10.10.m1.1.1.3">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.10.10.m1.1c">E_{\textrm{R}}</annotation></semantics></math><span id="S5.T3.11.11.11.1" class="ltx_text" style="font-size:70%;"> [</span><sup id="S5.T3.11.11.11.2" class="ltx_sup"><span id="S5.T3.11.11.11.2.1" class="ltx_text" style="font-size:70%;">∘</span></sup><span id="S5.T3.11.11.11.3" class="ltx_text" style="font-size:70%;">]</span>
</td>
<td id="S5.T3.12.12.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T3.12.12.12.m1.1" class="ltx_Math" alttext="E_{\textrm{pose}}^{*}" display="inline"><semantics id="S5.T3.12.12.12.m1.1a"><msubsup id="S5.T3.12.12.12.m1.1.1" xref="S5.T3.12.12.12.m1.1.1.cmml"><mi mathsize="70%" id="S5.T3.12.12.12.m1.1.1.2.2" xref="S5.T3.12.12.12.m1.1.1.2.2.cmml">E</mi><mtext mathsize="70%" id="S5.T3.12.12.12.m1.1.1.2.3" xref="S5.T3.12.12.12.m1.1.1.2.3a.cmml">pose</mtext><mo mathsize="70%" id="S5.T3.12.12.12.m1.1.1.3" xref="S5.T3.12.12.12.m1.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S5.T3.12.12.12.m1.1b"><apply id="S5.T3.12.12.12.m1.1.1.cmml" xref="S5.T3.12.12.12.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.12.12.12.m1.1.1.1.cmml" xref="S5.T3.12.12.12.m1.1.1">superscript</csymbol><apply id="S5.T3.12.12.12.m1.1.1.2.cmml" xref="S5.T3.12.12.12.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.12.12.12.m1.1.1.2.1.cmml" xref="S5.T3.12.12.12.m1.1.1">subscript</csymbol><ci id="S5.T3.12.12.12.m1.1.1.2.2.cmml" xref="S5.T3.12.12.12.m1.1.1.2.2">𝐸</ci><ci id="S5.T3.12.12.12.m1.1.1.2.3a.cmml" xref="S5.T3.12.12.12.m1.1.1.2.3"><mtext mathsize="49%" id="S5.T3.12.12.12.m1.1.1.2.3.cmml" xref="S5.T3.12.12.12.m1.1.1.2.3">pose</mtext></ci></apply><times id="S5.T3.12.12.12.m1.1.1.3.cmml" xref="S5.T3.12.12.12.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.12.12.12.m1.1c">E_{\textrm{pose}}^{*}</annotation></semantics></math><span id="S5.T3.12.12.12.1" class="ltx_text" style="font-size:70%;"> [-]</span>
</td>
</tr>
<tr id="S5.T3.13.13" class="ltx_tr">
<td id="S5.T3.13.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T3.13.13.1.1" class="ltx_text" style="font-size:70%;">SPN</span><sup id="S5.T3.13.13.1.2" class="ltx_sup"><span id="S5.T3.13.13.1.2.1" class="ltx_text" style="font-size:70%;">†</span></sup><span id="S5.T3.13.13.1.3" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.13.13.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S5.T3.13.13.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S5.T3.13.13.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.13.13.2.1" class="ltx_text" style="font-size:70%;">0.16</span></td>
<td id="S5.T3.13.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.13.13.3.1" class="ltx_text" style="font-size:70%;">7.77</span></td>
<td id="S5.T3.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.13.13.4.1" class="ltx_text" style="font-size:70%;">0.16</span></td>
<td id="S5.T3.13.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.13.13.5.1" class="ltx_text" style="font-size:70%;">0.45</span></td>
<td id="S5.T3.13.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.13.13.6.1" class="ltx_text" style="font-size:70%;">65.12</span></td>
<td id="S5.T3.13.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.13.13.7.1" class="ltx_text" style="font-size:70%;">1.21</span></td>
<td id="S5.T3.13.13.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.13.13.8.1" class="ltx_text" style="font-size:70%;">0.65</span></td>
<td id="S5.T3.13.13.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.13.13.9.1" class="ltx_text" style="font-size:70%;">92.95</span></td>
<td id="S5.T3.13.13.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.13.13.10.1" class="ltx_text" style="font-size:70%;">1.73</span></td>
</tr>
<tr id="S5.T3.14.14" class="ltx_tr">
<td id="S5.T3.14.14.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T3.14.14.1.1" class="ltx_text" style="font-size:70%;">KRN</span><sup id="S5.T3.14.14.1.2" class="ltx_sup"><span id="S5.T3.14.14.1.2.1" class="ltx_text" style="font-size:70%;">†</span></sup><span id="S5.T3.14.14.1.3" class="ltx_text" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.14.14.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S5.T3.14.14.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S5.T3.14.14.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.14.14.2.1" class="ltx_text" style="font-size:70%;">0.14</span></td>
<td id="S5.T3.14.14.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.14.14.3.1" class="ltx_text" style="font-size:70%;">3.69</span></td>
<td id="S5.T3.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.14.14.4.1" class="ltx_text" style="font-size:70%;">0.09</span></td>
<td id="S5.T3.14.14.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.14.14.5.1" class="ltx_text" style="font-size:70%;">2.25</span></td>
<td id="S5.T3.14.14.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.14.14.6.1" class="ltx_text" style="font-size:70%;">44.53</span></td>
<td id="S5.T3.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.14.14.7.1" class="ltx_text" style="font-size:70%;">1.12</span></td>
<td id="S5.T3.14.14.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.14.14.8.1" class="ltx_text" style="font-size:70%;">14.64</span></td>
<td id="S5.T3.14.14.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.14.14.9.1" class="ltx_text" style="font-size:70%;">80.95</span></td>
<td id="S5.T3.14.14.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.14.14.10.1" class="ltx_text" style="font-size:70%;">3.73</span></td>
</tr>
<tr id="S5.T3.15.15" class="ltx_tr">
<td id="S5.T3.15.15.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T3.15.15.1.2" class="ltx_text"></span><span id="S5.T3.15.15.1.3" class="ltx_text" style="font-size:70%;"> </span><span id="S5.T3.15.15.1.1" class="ltx_text" style="font-size:70%;">
<span id="S5.T3.15.15.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.15.15.1.1.1.2" class="ltx_tr">
<span id="S5.T3.15.15.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">HigherHRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite></span></span>
<span id="S5.T3.15.15.1.1.1.1" class="ltx_tr">
<span id="S5.T3.15.15.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">+ EP<math id="S5.T3.15.15.1.1.1.1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.T3.15.15.1.1.1.1.1.m1.1a"><mi id="S5.T3.15.15.1.1.1.1.1.m1.1.1" xref="S5.T3.15.15.1.1.1.1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.T3.15.15.1.1.1.1.1.m1.1b"><ci id="S5.T3.15.15.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.15.15.1.1.1.1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.15.15.1.1.1.1.1.m1.1c">n</annotation></semantics></math>P <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite></span></span>
</span></span><span id="S5.T3.15.15.1.4" class="ltx_text"></span><span id="S5.T3.15.15.1.5" class="ltx_text" style="font-size:70%;"></span>
</td>
<td id="S5.T3.15.15.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.15.15.2.1" class="ltx_text" style="font-size:70%;">0.05</span></td>
<td id="S5.T3.15.15.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.15.15.3.1" class="ltx_text" style="font-size:70%;">1.51</span></td>
<td id="S5.T3.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.15.15.4.1" class="ltx_text" style="font-size:70%;">0.04</span></td>
<td id="S5.T3.15.15.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.15.15.5.1" class="ltx_text" style="font-size:70%;">0.97</span></td>
<td id="S5.T3.15.15.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.15.15.6.1" class="ltx_text" style="font-size:70%;">34.71</span></td>
<td id="S5.T3.15.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.15.15.7.1" class="ltx_text" style="font-size:70%;">0.77</span></td>
<td id="S5.T3.15.15.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.15.15.8.1" class="ltx_text" style="font-size:70%;">0.85</span></td>
<td id="S5.T3.15.15.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.15.15.9.1" class="ltx_text" style="font-size:70%;">47.75</span></td>
<td id="S5.T3.15.15.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T3.15.15.10.1" class="ltx_text" style="font-size:70%;">0.98</span></td>
</tr>
<tr id="S5.T3.16.16" class="ltx_tr">
<td id="S5.T3.16.16.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="10">
<sup id="S5.T3.16.16.1.1" class="ltx_sup"><span id="S5.T3.16.16.1.1.1" class="ltx_text" style="font-size:80%;">†</span></sup><span id="S5.T3.16.16.1.2" class="ltx_text" style="font-size:80%;"> Assuming perfect bounding boxes for cropping during pre-processing.</span>
</td>
</tr>
</table>
</figure>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Style augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and DANN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> based on KRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> tested on <span id="S5.T4.22.1" class="ltx_text ltx_font_typewriter">lightbox</span>, <span id="S5.T4.23.2" class="ltx_text ltx_font_typewriter">sunlamp</span> domains of SPEED+ and <span id="S5.T4.24.3" class="ltx_text ltx_font_typewriter">prisma25</span> datasets. The oracle performances are averaged from 5-fold cross validation on the respective HIL domains.</figcaption>
<table id="S5.T4.18" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T4.18.19" class="ltx_tr">
<td id="S5.T4.18.19.1" class="ltx_td ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S5.T4.18.19.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="3"><span id="S5.T4.18.19.2.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">lightbox</span></td>
<td id="S5.T4.18.19.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="3"><span id="S5.T4.18.19.3.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">sunlamp</span></td>
<td id="S5.T4.18.19.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="3"><span id="S5.T4.18.19.4.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">prisma25</span></td>
</tr>
<tr id="S5.T4.12.12" class="ltx_tr">
<td id="S5.T4.12.12.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.12.12.13.1" class="ltx_text" style="font-size:70%;">Method</span></td>
<td id="S5.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T4.1.1.1.m1.1" class="ltx_Math" alttext="E_{\textrm{T}}" display="inline"><semantics id="S5.T4.1.1.1.m1.1a"><msub id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml"><mi mathsize="70%" id="S5.T4.1.1.1.m1.1.1.2" xref="S5.T4.1.1.1.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T4.1.1.1.m1.1.1.3" xref="S5.T4.1.1.1.m1.1.1.3a.cmml">T</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><apply id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.1.1.1.m1.1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T4.1.1.1.m1.1.1.2.cmml" xref="S5.T4.1.1.1.m1.1.1.2">𝐸</ci><ci id="S5.T4.1.1.1.m1.1.1.3a.cmml" xref="S5.T4.1.1.1.m1.1.1.3"><mtext mathsize="49%" id="S5.T4.1.1.1.m1.1.1.3.cmml" xref="S5.T4.1.1.1.m1.1.1.3">T</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">E_{\textrm{T}}</annotation></semantics></math><span id="S5.T4.1.1.1.1" class="ltx_text" style="font-size:70%;"> [m]</span>
</td>
<td id="S5.T4.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T4.2.2.2.m1.1" class="ltx_Math" alttext="E_{\textrm{R}}" display="inline"><semantics id="S5.T4.2.2.2.m1.1a"><msub id="S5.T4.2.2.2.m1.1.1" xref="S5.T4.2.2.2.m1.1.1.cmml"><mi mathsize="70%" id="S5.T4.2.2.2.m1.1.1.2" xref="S5.T4.2.2.2.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T4.2.2.2.m1.1.1.3" xref="S5.T4.2.2.2.m1.1.1.3a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.m1.1b"><apply id="S5.T4.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.2.2.2.m1.1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1">subscript</csymbol><ci id="S5.T4.2.2.2.m1.1.1.2.cmml" xref="S5.T4.2.2.2.m1.1.1.2">𝐸</ci><ci id="S5.T4.2.2.2.m1.1.1.3a.cmml" xref="S5.T4.2.2.2.m1.1.1.3"><mtext mathsize="49%" id="S5.T4.2.2.2.m1.1.1.3.cmml" xref="S5.T4.2.2.2.m1.1.1.3">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.m1.1c">E_{\textrm{R}}</annotation></semantics></math><span id="S5.T4.3.3.3.1" class="ltx_text" style="font-size:70%;"> [</span><sup id="S5.T4.3.3.3.2" class="ltx_sup"><span id="S5.T4.3.3.3.2.1" class="ltx_text" style="font-size:70%;">∘</span></sup><span id="S5.T4.3.3.3.3" class="ltx_text" style="font-size:70%;">]</span>
</td>
<td id="S5.T4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T4.4.4.4.m1.1" class="ltx_Math" alttext="E_{\textrm{pose}}^{*}" display="inline"><semantics id="S5.T4.4.4.4.m1.1a"><msubsup id="S5.T4.4.4.4.m1.1.1" xref="S5.T4.4.4.4.m1.1.1.cmml"><mi mathsize="70%" id="S5.T4.4.4.4.m1.1.1.2.2" xref="S5.T4.4.4.4.m1.1.1.2.2.cmml">E</mi><mtext mathsize="70%" id="S5.T4.4.4.4.m1.1.1.2.3" xref="S5.T4.4.4.4.m1.1.1.2.3a.cmml">pose</mtext><mo mathsize="70%" id="S5.T4.4.4.4.m1.1.1.3" xref="S5.T4.4.4.4.m1.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.m1.1b"><apply id="S5.T4.4.4.4.m1.1.1.cmml" xref="S5.T4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.4.4.4.m1.1.1.1.cmml" xref="S5.T4.4.4.4.m1.1.1">superscript</csymbol><apply id="S5.T4.4.4.4.m1.1.1.2.cmml" xref="S5.T4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.4.4.4.m1.1.1.2.1.cmml" xref="S5.T4.4.4.4.m1.1.1">subscript</csymbol><ci id="S5.T4.4.4.4.m1.1.1.2.2.cmml" xref="S5.T4.4.4.4.m1.1.1.2.2">𝐸</ci><ci id="S5.T4.4.4.4.m1.1.1.2.3a.cmml" xref="S5.T4.4.4.4.m1.1.1.2.3"><mtext mathsize="49%" id="S5.T4.4.4.4.m1.1.1.2.3.cmml" xref="S5.T4.4.4.4.m1.1.1.2.3">pose</mtext></ci></apply><times id="S5.T4.4.4.4.m1.1.1.3.cmml" xref="S5.T4.4.4.4.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.m1.1c">E_{\textrm{pose}}^{*}</annotation></semantics></math><span id="S5.T4.4.4.4.1" class="ltx_text" style="font-size:70%;"> [-]</span>
</td>
<td id="S5.T4.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T4.5.5.5.m1.1" class="ltx_Math" alttext="E_{\textrm{T}}" display="inline"><semantics id="S5.T4.5.5.5.m1.1a"><msub id="S5.T4.5.5.5.m1.1.1" xref="S5.T4.5.5.5.m1.1.1.cmml"><mi mathsize="70%" id="S5.T4.5.5.5.m1.1.1.2" xref="S5.T4.5.5.5.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T4.5.5.5.m1.1.1.3" xref="S5.T4.5.5.5.m1.1.1.3a.cmml">T</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.5.m1.1b"><apply id="S5.T4.5.5.5.m1.1.1.cmml" xref="S5.T4.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.5.5.5.m1.1.1.1.cmml" xref="S5.T4.5.5.5.m1.1.1">subscript</csymbol><ci id="S5.T4.5.5.5.m1.1.1.2.cmml" xref="S5.T4.5.5.5.m1.1.1.2">𝐸</ci><ci id="S5.T4.5.5.5.m1.1.1.3a.cmml" xref="S5.T4.5.5.5.m1.1.1.3"><mtext mathsize="49%" id="S5.T4.5.5.5.m1.1.1.3.cmml" xref="S5.T4.5.5.5.m1.1.1.3">T</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.5.m1.1c">E_{\textrm{T}}</annotation></semantics></math><span id="S5.T4.5.5.5.1" class="ltx_text" style="font-size:70%;"> [m]</span>
</td>
<td id="S5.T4.7.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T4.6.6.6.m1.1" class="ltx_Math" alttext="E_{\textrm{R}}" display="inline"><semantics id="S5.T4.6.6.6.m1.1a"><msub id="S5.T4.6.6.6.m1.1.1" xref="S5.T4.6.6.6.m1.1.1.cmml"><mi mathsize="70%" id="S5.T4.6.6.6.m1.1.1.2" xref="S5.T4.6.6.6.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T4.6.6.6.m1.1.1.3" xref="S5.T4.6.6.6.m1.1.1.3a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.6.m1.1b"><apply id="S5.T4.6.6.6.m1.1.1.cmml" xref="S5.T4.6.6.6.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.6.6.6.m1.1.1.1.cmml" xref="S5.T4.6.6.6.m1.1.1">subscript</csymbol><ci id="S5.T4.6.6.6.m1.1.1.2.cmml" xref="S5.T4.6.6.6.m1.1.1.2">𝐸</ci><ci id="S5.T4.6.6.6.m1.1.1.3a.cmml" xref="S5.T4.6.6.6.m1.1.1.3"><mtext mathsize="49%" id="S5.T4.6.6.6.m1.1.1.3.cmml" xref="S5.T4.6.6.6.m1.1.1.3">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.6.m1.1c">E_{\textrm{R}}</annotation></semantics></math><span id="S5.T4.7.7.7.1" class="ltx_text" style="font-size:70%;"> [</span><sup id="S5.T4.7.7.7.2" class="ltx_sup"><span id="S5.T4.7.7.7.2.1" class="ltx_text" style="font-size:70%;">∘</span></sup><span id="S5.T4.7.7.7.3" class="ltx_text" style="font-size:70%;">]</span>
</td>
<td id="S5.T4.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T4.8.8.8.m1.1" class="ltx_Math" alttext="E_{\textrm{pose}}^{*}" display="inline"><semantics id="S5.T4.8.8.8.m1.1a"><msubsup id="S5.T4.8.8.8.m1.1.1" xref="S5.T4.8.8.8.m1.1.1.cmml"><mi mathsize="70%" id="S5.T4.8.8.8.m1.1.1.2.2" xref="S5.T4.8.8.8.m1.1.1.2.2.cmml">E</mi><mtext mathsize="70%" id="S5.T4.8.8.8.m1.1.1.2.3" xref="S5.T4.8.8.8.m1.1.1.2.3a.cmml">pose</mtext><mo mathsize="70%" id="S5.T4.8.8.8.m1.1.1.3" xref="S5.T4.8.8.8.m1.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S5.T4.8.8.8.m1.1b"><apply id="S5.T4.8.8.8.m1.1.1.cmml" xref="S5.T4.8.8.8.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.8.8.8.m1.1.1.1.cmml" xref="S5.T4.8.8.8.m1.1.1">superscript</csymbol><apply id="S5.T4.8.8.8.m1.1.1.2.cmml" xref="S5.T4.8.8.8.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.8.8.8.m1.1.1.2.1.cmml" xref="S5.T4.8.8.8.m1.1.1">subscript</csymbol><ci id="S5.T4.8.8.8.m1.1.1.2.2.cmml" xref="S5.T4.8.8.8.m1.1.1.2.2">𝐸</ci><ci id="S5.T4.8.8.8.m1.1.1.2.3a.cmml" xref="S5.T4.8.8.8.m1.1.1.2.3"><mtext mathsize="49%" id="S5.T4.8.8.8.m1.1.1.2.3.cmml" xref="S5.T4.8.8.8.m1.1.1.2.3">pose</mtext></ci></apply><times id="S5.T4.8.8.8.m1.1.1.3.cmml" xref="S5.T4.8.8.8.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.8.8.m1.1c">E_{\textrm{pose}}^{*}</annotation></semantics></math><span id="S5.T4.8.8.8.1" class="ltx_text" style="font-size:70%;"> [-]</span>
</td>
<td id="S5.T4.9.9.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T4.9.9.9.m1.1" class="ltx_Math" alttext="E_{\textrm{T}}" display="inline"><semantics id="S5.T4.9.9.9.m1.1a"><msub id="S5.T4.9.9.9.m1.1.1" xref="S5.T4.9.9.9.m1.1.1.cmml"><mi mathsize="70%" id="S5.T4.9.9.9.m1.1.1.2" xref="S5.T4.9.9.9.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T4.9.9.9.m1.1.1.3" xref="S5.T4.9.9.9.m1.1.1.3a.cmml">T</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T4.9.9.9.m1.1b"><apply id="S5.T4.9.9.9.m1.1.1.cmml" xref="S5.T4.9.9.9.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.9.9.9.m1.1.1.1.cmml" xref="S5.T4.9.9.9.m1.1.1">subscript</csymbol><ci id="S5.T4.9.9.9.m1.1.1.2.cmml" xref="S5.T4.9.9.9.m1.1.1.2">𝐸</ci><ci id="S5.T4.9.9.9.m1.1.1.3a.cmml" xref="S5.T4.9.9.9.m1.1.1.3"><mtext mathsize="49%" id="S5.T4.9.9.9.m1.1.1.3.cmml" xref="S5.T4.9.9.9.m1.1.1.3">T</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.9.9.9.m1.1c">E_{\textrm{T}}</annotation></semantics></math><span id="S5.T4.9.9.9.1" class="ltx_text" style="font-size:70%;"> [m]</span>
</td>
<td id="S5.T4.11.11.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T4.10.10.10.m1.1" class="ltx_Math" alttext="E_{\textrm{R}}" display="inline"><semantics id="S5.T4.10.10.10.m1.1a"><msub id="S5.T4.10.10.10.m1.1.1" xref="S5.T4.10.10.10.m1.1.1.cmml"><mi mathsize="70%" id="S5.T4.10.10.10.m1.1.1.2" xref="S5.T4.10.10.10.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T4.10.10.10.m1.1.1.3" xref="S5.T4.10.10.10.m1.1.1.3a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T4.10.10.10.m1.1b"><apply id="S5.T4.10.10.10.m1.1.1.cmml" xref="S5.T4.10.10.10.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.10.10.10.m1.1.1.1.cmml" xref="S5.T4.10.10.10.m1.1.1">subscript</csymbol><ci id="S5.T4.10.10.10.m1.1.1.2.cmml" xref="S5.T4.10.10.10.m1.1.1.2">𝐸</ci><ci id="S5.T4.10.10.10.m1.1.1.3a.cmml" xref="S5.T4.10.10.10.m1.1.1.3"><mtext mathsize="49%" id="S5.T4.10.10.10.m1.1.1.3.cmml" xref="S5.T4.10.10.10.m1.1.1.3">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.10.10.10.m1.1c">E_{\textrm{R}}</annotation></semantics></math><span id="S5.T4.11.11.11.1" class="ltx_text" style="font-size:70%;"> [</span><sup id="S5.T4.11.11.11.2" class="ltx_sup"><span id="S5.T4.11.11.11.2.1" class="ltx_text" style="font-size:70%;">∘</span></sup><span id="S5.T4.11.11.11.3" class="ltx_text" style="font-size:70%;">]</span>
</td>
<td id="S5.T4.12.12.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<math id="S5.T4.12.12.12.m1.1" class="ltx_Math" alttext="E_{\textrm{pose}}" display="inline"><semantics id="S5.T4.12.12.12.m1.1a"><msub id="S5.T4.12.12.12.m1.1.1" xref="S5.T4.12.12.12.m1.1.1.cmml"><mi mathsize="70%" id="S5.T4.12.12.12.m1.1.1.2" xref="S5.T4.12.12.12.m1.1.1.2.cmml">E</mi><mtext mathsize="70%" id="S5.T4.12.12.12.m1.1.1.3" xref="S5.T4.12.12.12.m1.1.1.3a.cmml">pose</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T4.12.12.12.m1.1b"><apply id="S5.T4.12.12.12.m1.1.1.cmml" xref="S5.T4.12.12.12.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.12.12.12.m1.1.1.1.cmml" xref="S5.T4.12.12.12.m1.1.1">subscript</csymbol><ci id="S5.T4.12.12.12.m1.1.1.2.cmml" xref="S5.T4.12.12.12.m1.1.1.2">𝐸</ci><ci id="S5.T4.12.12.12.m1.1.1.3a.cmml" xref="S5.T4.12.12.12.m1.1.1.3"><mtext mathsize="49%" id="S5.T4.12.12.12.m1.1.1.3.cmml" xref="S5.T4.12.12.12.m1.1.1.3">pose</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.12.12.12.m1.1c">E_{\textrm{pose}}</annotation></semantics></math><span id="S5.T4.12.12.12.1" class="ltx_text" style="font-size:70%;"> [-]</span>
</td>
</tr>
<tr id="S5.T4.18.20" class="ltx_tr">
<td id="S5.T4.18.20.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T4.18.20.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">synthetic</span><span id="S5.T4.18.20.1.2" class="ltx_text" style="font-size:70%;"> only</span>
</td>
<td id="S5.T4.18.20.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.20.2.1" class="ltx_text" style="font-size:70%;">2.25</span></td>
<td id="S5.T4.18.20.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.20.3.1" class="ltx_text" style="font-size:70%;">44.53</span></td>
<td id="S5.T4.18.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.20.4.1" class="ltx_text" style="font-size:70%;">1.12</span></td>
<td id="S5.T4.18.20.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.20.5.1" class="ltx_text" style="font-size:70%;">14.64</span></td>
<td id="S5.T4.18.20.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.20.6.1" class="ltx_text" style="font-size:70%;">80.95</span></td>
<td id="S5.T4.18.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.20.7.1" class="ltx_text" style="font-size:70%;">3.73</span></td>
<td id="S5.T4.18.20.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.20.8.1" class="ltx_text" style="font-size:70%;">2.64</span></td>
<td id="S5.T4.18.20.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.20.9.1" class="ltx_text" style="font-size:70%;">86.04</span></td>
<td id="S5.T4.18.20.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.20.10.1" class="ltx_text" style="font-size:70%;">1.76</span></td>
</tr>
<tr id="S5.T4.18.21" class="ltx_tr">
<td id="S5.T4.18.21.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T4.18.21.1.1" class="ltx_text" style="font-size:70%;">Style Aug. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.18.21.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S5.T4.18.21.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S5.T4.18.21.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.21.2.1" class="ltx_text" style="font-size:70%;">1.06</span></td>
<td id="S5.T4.18.21.3" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.21.3.1" class="ltx_text" style="font-size:70%;">36.14</span></td>
<td id="S5.T4.18.21.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.21.4.1" class="ltx_text" style="font-size:70%;">0.81</span></td>
<td id="S5.T4.18.21.5" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.21.5.1" class="ltx_text" style="font-size:70%;">1.32</span></td>
<td id="S5.T4.18.21.6" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.21.6.1" class="ltx_text" style="font-size:70%;">62.85</span></td>
<td id="S5.T4.18.21.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.21.7.1" class="ltx_text" style="font-size:70%;">1.32</span></td>
<td id="S5.T4.18.21.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.21.8.1" class="ltx_text" style="font-size:70%;">4.06</span></td>
<td id="S5.T4.18.21.9" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.21.9.1" class="ltx_text" style="font-size:70%;">20.57</span></td>
<td id="S5.T4.18.21.10" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.21.10.1" class="ltx_text" style="font-size:70%;">0.71</span></td>
</tr>
<tr id="S5.T4.18.22" class="ltx_tr">
<td id="S5.T4.18.22.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T4.18.22.1.1" class="ltx_text" style="font-size:70%;">DANN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.18.22.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S5.T4.18.22.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S5.T4.18.22.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.22.2.1" class="ltx_text" style="font-size:70%;">0.95</span></td>
<td id="S5.T4.18.22.3" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.22.3.1" class="ltx_text" style="font-size:70%;">33.62</span></td>
<td id="S5.T4.18.22.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.22.4.1" class="ltx_text" style="font-size:70%;">0.74</span></td>
<td id="S5.T4.18.22.5" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.22.5.1" class="ltx_text" style="font-size:70%;">2.04</span></td>
<td id="S5.T4.18.22.6" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.22.6.1" class="ltx_text" style="font-size:70%;">65.37</span></td>
<td id="S5.T4.18.22.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.22.7.1" class="ltx_text" style="font-size:70%;">1.47</span></td>
<td id="S5.T4.18.22.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.22.8.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T4.18.22.9" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.22.9.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T4.18.22.10" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.22.10.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S5.T4.18.18" class="ltx_tr">
<td id="S5.T4.18.18.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.18.7.1" class="ltx_text" style="font-size:70%;">Oracle</span></td>
<td id="S5.T4.13.13.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T4.13.13.1.1" class="ltx_text" style="font-size:70%;">0.24 </span><math id="S5.T4.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.13.13.1.m1.1a"><mo mathsize="70%" id="S5.T4.13.13.1.m1.1.1" xref="S5.T4.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.13.13.1.m1.1b"><csymbol cd="latexml" id="S5.T4.13.13.1.m1.1.1.cmml" xref="S5.T4.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.13.13.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T4.13.13.1.2" class="ltx_text" style="font-size:70%;"> 0.04</span>
</td>
<td id="S5.T4.14.14.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T4.14.14.2.1" class="ltx_text" style="font-size:70%;">6.15 </span><math id="S5.T4.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.14.14.2.m1.1a"><mo mathsize="70%" id="S5.T4.14.14.2.m1.1.1" xref="S5.T4.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.14.14.2.m1.1b"><csymbol cd="latexml" id="S5.T4.14.14.2.m1.1.1.cmml" xref="S5.T4.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.14.14.2.m1.1c">\pm</annotation></semantics></math><span id="S5.T4.14.14.2.2" class="ltx_text" style="font-size:70%;"> 0.61</span>
</td>
<td id="S5.T4.15.15.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T4.15.15.3.1" class="ltx_text" style="font-size:70%;">0.15 </span><math id="S5.T4.15.15.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.15.15.3.m1.1a"><mo mathsize="70%" id="S5.T4.15.15.3.m1.1.1" xref="S5.T4.15.15.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.15.15.3.m1.1b"><csymbol cd="latexml" id="S5.T4.15.15.3.m1.1.1.cmml" xref="S5.T4.15.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.15.15.3.m1.1c">\pm</annotation></semantics></math><span id="S5.T4.15.15.3.2" class="ltx_text" style="font-size:70%;"> 0.01</span>
</td>
<td id="S5.T4.16.16.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T4.16.16.4.1" class="ltx_text" style="font-size:70%;">0.19 </span><math id="S5.T4.16.16.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.16.16.4.m1.1a"><mo mathsize="70%" id="S5.T4.16.16.4.m1.1.1" xref="S5.T4.16.16.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.16.16.4.m1.1b"><csymbol cd="latexml" id="S5.T4.16.16.4.m1.1.1.cmml" xref="S5.T4.16.16.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.16.16.4.m1.1c">\pm</annotation></semantics></math><span id="S5.T4.16.16.4.2" class="ltx_text" style="font-size:70%;"> 0.02</span>
</td>
<td id="S5.T4.17.17.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T4.17.17.5.1" class="ltx_text" style="font-size:70%;">5.33 </span><math id="S5.T4.17.17.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.17.17.5.m1.1a"><mo mathsize="70%" id="S5.T4.17.17.5.m1.1.1" xref="S5.T4.17.17.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.17.17.5.m1.1b"><csymbol cd="latexml" id="S5.T4.17.17.5.m1.1.1.cmml" xref="S5.T4.17.17.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.17.17.5.m1.1c">\pm</annotation></semantics></math><span id="S5.T4.17.17.5.2" class="ltx_text" style="font-size:70%;"> 0.36</span>
</td>
<td id="S5.T4.18.18.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T4.18.18.6.1" class="ltx_text" style="font-size:70%;">0.13 </span><math id="S5.T4.18.18.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.18.18.6.m1.1a"><mo mathsize="70%" id="S5.T4.18.18.6.m1.1.1" xref="S5.T4.18.18.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.18.18.6.m1.1b"><csymbol cd="latexml" id="S5.T4.18.18.6.m1.1.1.cmml" xref="S5.T4.18.18.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.18.18.6.m1.1c">\pm</annotation></semantics></math><span id="S5.T4.18.18.6.2" class="ltx_text" style="font-size:70%;"> 0.01</span>
</td>
<td id="S5.T4.18.18.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.18.8.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T4.18.18.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.18.9.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S5.T4.18.18.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T4.18.18.10.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
</table>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Algorithms for domain gap</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">This study adopts two algorithms that represent two distinct approaches to bridging the domain gap: unsupervised domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, which utilizes unlabeled images from the target domain to bridge the feature-level discrepancies, and domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, which instead randomizes various aspects of the input images such that the target image would be considered as another randomized version of the source image. The domain adaptation method used in this study is the Domain-Adversarial Neural Network (DANN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, which employs an adversarial training using a domain classifier to train a domain-invariant feature extractor. For domain randomization, the style augmentation method by Jackson et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> is used to randomize the spacecraft’s texture using the Style Transfer network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. The style transfer is performed online during the data augmentation phase for half of the training images. More details on the training are presented in Appendix <a href="#S10" title="10 Training Details ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Metrics</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.3" class="ltx_p">The pose accuracies are measured using the position error (<math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="E_{\textrm{t}}" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><msub id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mi id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">E</mi><mtext id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3a.cmml">t</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">𝐸</ci><ci id="S5.SS3.p1.1.m1.1.1.3a.cmml" xref="S5.SS3.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S5.SS3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3">t</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">E_{\textrm{t}}</annotation></semantics></math>), orientation error (<math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="E_{\textrm{q}}" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><msub id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mi id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">E</mi><mtext id="S5.SS3.p1.2.m2.1.1.3" xref="S5.SS3.p1.2.m2.1.1.3a.cmml">q</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2">𝐸</ci><ci id="S5.SS3.p1.2.m2.1.1.3a.cmml" xref="S5.SS3.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S5.SS3.p1.2.m2.1.1.3.cmml" xref="S5.SS3.p1.2.m2.1.1.3">q</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">E_{\textrm{q}}</annotation></semantics></math>), and pose error (<math id="S5.SS3.p1.3.m3.1" class="ltx_Math" alttext="E_{\textrm{pose}}" display="inline"><semantics id="S5.SS3.p1.3.m3.1a"><msub id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mi id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">E</mi><mtext id="S5.SS3.p1.3.m3.1.1.3" xref="S5.SS3.p1.3.m3.1.1.3a.cmml">pose</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p1.3.m3.1.1.2.cmml" xref="S5.SS3.p1.3.m3.1.1.2">𝐸</ci><ci id="S5.SS3.p1.3.m3.1.1.3a.cmml" xref="S5.SS3.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S5.SS3.p1.3.m3.1.1.3.cmml" xref="S5.SS3.p1.3.m3.1.1.3">pose</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">E_{\textrm{pose}}</annotation></semantics></math>) respectively defined as follows,</p>
<table id="S11.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S5.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S5.E1.m1.1" class="ltx_Math" alttext="\displaystyle E_{\textrm{t}}" display="inline"><semantics id="S5.E1.m1.1a"><msub id="S5.E1.m1.1.1" xref="S5.E1.m1.1.1.cmml"><mi id="S5.E1.m1.1.1.2" xref="S5.E1.m1.1.1.2.cmml">E</mi><mtext id="S5.E1.m1.1.1.3" xref="S5.E1.m1.1.1.3a.cmml">t</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.E1.m1.1b"><apply id="S5.E1.m1.1.1.cmml" xref="S5.E1.m1.1.1"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.cmml" xref="S5.E1.m1.1.1">subscript</csymbol><ci id="S5.E1.m1.1.1.2.cmml" xref="S5.E1.m1.1.1.2">𝐸</ci><ci id="S5.E1.m1.1.1.3a.cmml" xref="S5.E1.m1.1.1.3"><mtext mathsize="70%" id="S5.E1.m1.1.1.3.cmml" xref="S5.E1.m1.1.1.3">t</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m1.1c">\displaystyle E_{\textrm{t}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.E1.m2.1" class="ltx_Math" alttext="\displaystyle=\|\tilde{\bm{t}}-\bm{t}\|_{2}," display="inline"><semantics id="S5.E1.m2.1a"><mrow id="S5.E1.m2.1.1.1" xref="S5.E1.m2.1.1.1.1.cmml"><mrow id="S5.E1.m2.1.1.1.1" xref="S5.E1.m2.1.1.1.1.cmml"><mi id="S5.E1.m2.1.1.1.1.3" xref="S5.E1.m2.1.1.1.1.3.cmml"></mi><mo id="S5.E1.m2.1.1.1.1.2" xref="S5.E1.m2.1.1.1.1.2.cmml">=</mo><msub id="S5.E1.m2.1.1.1.1.1" xref="S5.E1.m2.1.1.1.1.1.cmml"><mrow id="S5.E1.m2.1.1.1.1.1.1.1" xref="S5.E1.m2.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.E1.m2.1.1.1.1.1.1.1.2" xref="S5.E1.m2.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S5.E1.m2.1.1.1.1.1.1.1.1" xref="S5.E1.m2.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S5.E1.m2.1.1.1.1.1.1.1.1.2" xref="S5.E1.m2.1.1.1.1.1.1.1.1.2.cmml"><mi id="S5.E1.m2.1.1.1.1.1.1.1.1.2.2" xref="S5.E1.m2.1.1.1.1.1.1.1.1.2.2.cmml">𝒕</mi><mo id="S5.E1.m2.1.1.1.1.1.1.1.1.2.1" xref="S5.E1.m2.1.1.1.1.1.1.1.1.2.1.cmml">~</mo></mover><mo id="S5.E1.m2.1.1.1.1.1.1.1.1.1" xref="S5.E1.m2.1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S5.E1.m2.1.1.1.1.1.1.1.1.3" xref="S5.E1.m2.1.1.1.1.1.1.1.1.3.cmml">𝒕</mi></mrow><mo stretchy="false" id="S5.E1.m2.1.1.1.1.1.1.1.3" xref="S5.E1.m2.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S5.E1.m2.1.1.1.1.1.3" xref="S5.E1.m2.1.1.1.1.1.3.cmml">2</mn></msub></mrow><mo id="S5.E1.m2.1.1.1.2" xref="S5.E1.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E1.m2.1b"><apply id="S5.E1.m2.1.1.1.1.cmml" xref="S5.E1.m2.1.1.1"><eq id="S5.E1.m2.1.1.1.1.2.cmml" xref="S5.E1.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S5.E1.m2.1.1.1.1.3.cmml" xref="S5.E1.m2.1.1.1.1.3">absent</csymbol><apply id="S5.E1.m2.1.1.1.1.1.cmml" xref="S5.E1.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E1.m2.1.1.1.1.1.2.cmml" xref="S5.E1.m2.1.1.1.1.1">subscript</csymbol><apply id="S5.E1.m2.1.1.1.1.1.1.2.cmml" xref="S5.E1.m2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.E1.m2.1.1.1.1.1.1.2.1.cmml" xref="S5.E1.m2.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S5.E1.m2.1.1.1.1.1.1.1.1.cmml" xref="S5.E1.m2.1.1.1.1.1.1.1.1"><minus id="S5.E1.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E1.m2.1.1.1.1.1.1.1.1.1"></minus><apply id="S5.E1.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E1.m2.1.1.1.1.1.1.1.1.2"><ci id="S5.E1.m2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E1.m2.1.1.1.1.1.1.1.1.2.1">~</ci><ci id="S5.E1.m2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E1.m2.1.1.1.1.1.1.1.1.2.2">𝒕</ci></apply><ci id="S5.E1.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E1.m2.1.1.1.1.1.1.1.1.3">𝒕</ci></apply></apply><cn type="integer" id="S5.E1.m2.1.1.1.1.1.3.cmml" xref="S5.E1.m2.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m2.1c">\displaystyle=\|\tilde{\bm{t}}-\bm{t}\|_{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S5.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S5.E2.m1.1" class="ltx_Math" alttext="\displaystyle E_{\textrm{q}}" display="inline"><semantics id="S5.E2.m1.1a"><msub id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml"><mi id="S5.E2.m1.1.1.2" xref="S5.E2.m1.1.1.2.cmml">E</mi><mtext id="S5.E2.m1.1.1.3" xref="S5.E2.m1.1.1.3a.cmml">q</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.E2.m1.1b"><apply id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.cmml" xref="S5.E2.m1.1.1">subscript</csymbol><ci id="S5.E2.m1.1.1.2.cmml" xref="S5.E2.m1.1.1.2">𝐸</ci><ci id="S5.E2.m1.1.1.3a.cmml" xref="S5.E2.m1.1.1.3"><mtext mathsize="70%" id="S5.E2.m1.1.1.3.cmml" xref="S5.E2.m1.1.1.3">q</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.1c">\displaystyle E_{\textrm{q}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.E2.m2.2" class="ltx_math_unparsed" alttext="\displaystyle=2\arccos|&lt;\tilde{\bm{q}},\bm{q}&gt;|," display="inline"><semantics id="S5.E2.m2.2a"><mrow id="S5.E2.m2.2b"><mo id="S5.E2.m2.2.3">=</mo><mn id="S5.E2.m2.2.4">2</mn><mi id="S5.E2.m2.2.5">arccos</mi><mo fence="false" stretchy="false" id="S5.E2.m2.2.6">|</mo><mo lspace="0.167em" id="S5.E2.m2.2.7">&lt;</mo><mover accent="true" id="S5.E2.m2.1.1"><mi id="S5.E2.m2.1.1.2">𝒒</mi><mo id="S5.E2.m2.1.1.1">~</mo></mover><mo id="S5.E2.m2.2.8">,</mo><mi id="S5.E2.m2.2.2">𝒒</mi><mo rspace="0em" id="S5.E2.m2.2.9">&gt;</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S5.E2.m2.2.10">|</mo><mo id="S5.E2.m2.2.11">,</mo></mrow><annotation encoding="application/x-tex" id="S5.E2.m2.2c">\displaystyle=2\arccos|&lt;\tilde{\bm{q}},\bm{q}&gt;|,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S5.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S5.E3.m1.1" class="ltx_Math" alttext="\displaystyle E_{\textrm{pose}}" display="inline"><semantics id="S5.E3.m1.1a"><msub id="S5.E3.m1.1.1" xref="S5.E3.m1.1.1.cmml"><mi id="S5.E3.m1.1.1.2" xref="S5.E3.m1.1.1.2.cmml">E</mi><mtext id="S5.E3.m1.1.1.3" xref="S5.E3.m1.1.1.3a.cmml">pose</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.E3.m1.1b"><apply id="S5.E3.m1.1.1.cmml" xref="S5.E3.m1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.cmml" xref="S5.E3.m1.1.1">subscript</csymbol><ci id="S5.E3.m1.1.1.2.cmml" xref="S5.E3.m1.1.1.2">𝐸</ci><ci id="S5.E3.m1.1.1.3a.cmml" xref="S5.E3.m1.1.1.3"><mtext mathsize="70%" id="S5.E3.m1.1.1.3.cmml" xref="S5.E3.m1.1.1.3">pose</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.1c">\displaystyle E_{\textrm{pose}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.E3.m2.2" class="ltx_Math" alttext="\displaystyle=E_{\textrm{q}}+E_{\textrm{t}}/\|\bm{t}\|," display="inline"><semantics id="S5.E3.m2.2a"><mrow id="S5.E3.m2.2.2.1" xref="S5.E3.m2.2.2.1.1.cmml"><mrow id="S5.E3.m2.2.2.1.1" xref="S5.E3.m2.2.2.1.1.cmml"><mi id="S5.E3.m2.2.2.1.1.2" xref="S5.E3.m2.2.2.1.1.2.cmml"></mi><mo id="S5.E3.m2.2.2.1.1.1" xref="S5.E3.m2.2.2.1.1.1.cmml">=</mo><mrow id="S5.E3.m2.2.2.1.1.3" xref="S5.E3.m2.2.2.1.1.3.cmml"><msub id="S5.E3.m2.2.2.1.1.3.2" xref="S5.E3.m2.2.2.1.1.3.2.cmml"><mi id="S5.E3.m2.2.2.1.1.3.2.2" xref="S5.E3.m2.2.2.1.1.3.2.2.cmml">E</mi><mtext id="S5.E3.m2.2.2.1.1.3.2.3" xref="S5.E3.m2.2.2.1.1.3.2.3a.cmml">q</mtext></msub><mo id="S5.E3.m2.2.2.1.1.3.1" xref="S5.E3.m2.2.2.1.1.3.1.cmml">+</mo><mrow id="S5.E3.m2.2.2.1.1.3.3" xref="S5.E3.m2.2.2.1.1.3.3.cmml"><msub id="S5.E3.m2.2.2.1.1.3.3.2" xref="S5.E3.m2.2.2.1.1.3.3.2.cmml"><mi id="S5.E3.m2.2.2.1.1.3.3.2.2" xref="S5.E3.m2.2.2.1.1.3.3.2.2.cmml">E</mi><mtext id="S5.E3.m2.2.2.1.1.3.3.2.3" xref="S5.E3.m2.2.2.1.1.3.3.2.3a.cmml">t</mtext></msub><mo id="S5.E3.m2.2.2.1.1.3.3.1" xref="S5.E3.m2.2.2.1.1.3.3.1.cmml">/</mo><mrow id="S5.E3.m2.2.2.1.1.3.3.3.2" xref="S5.E3.m2.2.2.1.1.3.3.3.1.cmml"><mo stretchy="false" id="S5.E3.m2.2.2.1.1.3.3.3.2.1" xref="S5.E3.m2.2.2.1.1.3.3.3.1.1.cmml">‖</mo><mi id="S5.E3.m2.1.1" xref="S5.E3.m2.1.1.cmml">𝒕</mi><mo stretchy="false" id="S5.E3.m2.2.2.1.1.3.3.3.2.2" xref="S5.E3.m2.2.2.1.1.3.3.3.1.1.cmml">‖</mo></mrow></mrow></mrow></mrow><mo id="S5.E3.m2.2.2.1.2" xref="S5.E3.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m2.2b"><apply id="S5.E3.m2.2.2.1.1.cmml" xref="S5.E3.m2.2.2.1"><eq id="S5.E3.m2.2.2.1.1.1.cmml" xref="S5.E3.m2.2.2.1.1.1"></eq><csymbol cd="latexml" id="S5.E3.m2.2.2.1.1.2.cmml" xref="S5.E3.m2.2.2.1.1.2">absent</csymbol><apply id="S5.E3.m2.2.2.1.1.3.cmml" xref="S5.E3.m2.2.2.1.1.3"><plus id="S5.E3.m2.2.2.1.1.3.1.cmml" xref="S5.E3.m2.2.2.1.1.3.1"></plus><apply id="S5.E3.m2.2.2.1.1.3.2.cmml" xref="S5.E3.m2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S5.E3.m2.2.2.1.1.3.2.1.cmml" xref="S5.E3.m2.2.2.1.1.3.2">subscript</csymbol><ci id="S5.E3.m2.2.2.1.1.3.2.2.cmml" xref="S5.E3.m2.2.2.1.1.3.2.2">𝐸</ci><ci id="S5.E3.m2.2.2.1.1.3.2.3a.cmml" xref="S5.E3.m2.2.2.1.1.3.2.3"><mtext mathsize="70%" id="S5.E3.m2.2.2.1.1.3.2.3.cmml" xref="S5.E3.m2.2.2.1.1.3.2.3">q</mtext></ci></apply><apply id="S5.E3.m2.2.2.1.1.3.3.cmml" xref="S5.E3.m2.2.2.1.1.3.3"><divide id="S5.E3.m2.2.2.1.1.3.3.1.cmml" xref="S5.E3.m2.2.2.1.1.3.3.1"></divide><apply id="S5.E3.m2.2.2.1.1.3.3.2.cmml" xref="S5.E3.m2.2.2.1.1.3.3.2"><csymbol cd="ambiguous" id="S5.E3.m2.2.2.1.1.3.3.2.1.cmml" xref="S5.E3.m2.2.2.1.1.3.3.2">subscript</csymbol><ci id="S5.E3.m2.2.2.1.1.3.3.2.2.cmml" xref="S5.E3.m2.2.2.1.1.3.3.2.2">𝐸</ci><ci id="S5.E3.m2.2.2.1.1.3.3.2.3a.cmml" xref="S5.E3.m2.2.2.1.1.3.3.2.3"><mtext mathsize="70%" id="S5.E3.m2.2.2.1.1.3.3.2.3.cmml" xref="S5.E3.m2.2.2.1.1.3.3.2.3">t</mtext></ci></apply><apply id="S5.E3.m2.2.2.1.1.3.3.3.1.cmml" xref="S5.E3.m2.2.2.1.1.3.3.3.2"><csymbol cd="latexml" id="S5.E3.m2.2.2.1.1.3.3.3.1.1.cmml" xref="S5.E3.m2.2.2.1.1.3.3.3.2.1">norm</csymbol><ci id="S5.E3.m2.1.1.cmml" xref="S5.E3.m2.1.1">𝒕</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m2.2c">\displaystyle=E_{\textrm{q}}+E_{\textrm{t}}/\|\bm{t}\|,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S5.SS3.p1.5" class="ltx_p">where <math id="S5.SS3.p1.4.m1.2" class="ltx_Math" alttext="(\tilde{\bm{t}},\bm{t})" display="inline"><semantics id="S5.SS3.p1.4.m1.2a"><mrow id="S5.SS3.p1.4.m1.2.3.2" xref="S5.SS3.p1.4.m1.2.3.1.cmml"><mo stretchy="false" id="S5.SS3.p1.4.m1.2.3.2.1" xref="S5.SS3.p1.4.m1.2.3.1.cmml">(</mo><mover accent="true" id="S5.SS3.p1.4.m1.1.1" xref="S5.SS3.p1.4.m1.1.1.cmml"><mi id="S5.SS3.p1.4.m1.1.1.2" xref="S5.SS3.p1.4.m1.1.1.2.cmml">𝒕</mi><mo id="S5.SS3.p1.4.m1.1.1.1" xref="S5.SS3.p1.4.m1.1.1.1.cmml">~</mo></mover><mo id="S5.SS3.p1.4.m1.2.3.2.2" xref="S5.SS3.p1.4.m1.2.3.1.cmml">,</mo><mi id="S5.SS3.p1.4.m1.2.2" xref="S5.SS3.p1.4.m1.2.2.cmml">𝒕</mi><mo stretchy="false" id="S5.SS3.p1.4.m1.2.3.2.3" xref="S5.SS3.p1.4.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.4.m1.2b"><interval closure="open" id="S5.SS3.p1.4.m1.2.3.1.cmml" xref="S5.SS3.p1.4.m1.2.3.2"><apply id="S5.SS3.p1.4.m1.1.1.cmml" xref="S5.SS3.p1.4.m1.1.1"><ci id="S5.SS3.p1.4.m1.1.1.1.cmml" xref="S5.SS3.p1.4.m1.1.1.1">~</ci><ci id="S5.SS3.p1.4.m1.1.1.2.cmml" xref="S5.SS3.p1.4.m1.1.1.2">𝒕</ci></apply><ci id="S5.SS3.p1.4.m1.2.2.cmml" xref="S5.SS3.p1.4.m1.2.2">𝒕</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.4.m1.2c">(\tilde{\bm{t}},\bm{t})</annotation></semantics></math> respectively denote the predicted and ground-truth relative position vector, <math id="S5.SS3.p1.5.m2.2" class="ltx_math_unparsed" alttext="&lt;\tilde{\bm{q}},\bm{q}&gt;" display="inline"><semantics id="S5.SS3.p1.5.m2.2a"><mrow id="S5.SS3.p1.5.m2.2b"><mo id="S5.SS3.p1.5.m2.2.3">&lt;</mo><mover accent="true" id="S5.SS3.p1.5.m2.1.1"><mi id="S5.SS3.p1.5.m2.1.1.2">𝒒</mi><mo id="S5.SS3.p1.5.m2.1.1.1">~</mo></mover><mo id="S5.SS3.p1.5.m2.2.4">,</mo><mi id="S5.SS3.p1.5.m2.2.2">𝒒</mi><mo id="S5.SS3.p1.5.m2.2.5">&gt;</mo></mrow><annotation encoding="application/x-tex" id="S5.SS3.p1.5.m2.2c">&lt;\tilde{\bm{q}},\bm{q}&gt;</annotation></semantics></math> denote the inner-product of the predicted and ground-truth quaternion vectors, and the pose error is the sum of the position error normalized by the magnitude of the ground-truth position vector and the orientation error in radians. The pose error, also known as the SPEED score, was the official competition metric used in SPEC2019, chosen as it has shown to properly balance the contribution of the position and orientation errors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.3" class="ltx_p">In SPEC2021, a modified pose error is used to account for the errors associated with the pose labels of the HIL images. Specifically, from the calibration results of the TRON facility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, the following best-performance thresholds are determined: <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="\theta_{q}" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><msub id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mi id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">θ</mi><mi id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">𝜃</ci><ci id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">\theta_{q}</annotation></semantics></math> = 0.169<sup id="S5.SS3.p2.3.1" class="ltx_sup">∘</sup> for orientation and <math id="S5.SS3.p2.3.m3.1" class="ltx_Math" alttext="\theta_{t}" display="inline"><semantics id="S5.SS3.p2.3.m3.1a"><msub id="S5.SS3.p2.3.m3.1.1" xref="S5.SS3.p2.3.m3.1.1.cmml"><mi id="S5.SS3.p2.3.m3.1.1.2" xref="S5.SS3.p2.3.m3.1.1.2.cmml">θ</mi><mi id="S5.SS3.p2.3.m3.1.1.3" xref="S5.SS3.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.3.m3.1b"><apply id="S5.SS3.p2.3.m3.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.3.m3.1.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p2.3.m3.1.1.2.cmml" xref="S5.SS3.p2.3.m3.1.1.2">𝜃</ci><ci id="S5.SS3.p2.3.m3.1.1.3.cmml" xref="S5.SS3.p2.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.3.m3.1c">\theta_{t}</annotation></semantics></math> = 2.173 mm/m for normalized translation. Then, if the predicted orientation and translation are both below respective thresholds, the pose error is considered perfect, i.e., for each sample,</p>
<table id="S5.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E4.m1.5" class="ltx_math_unparsed" alttext="E_{\textrm{pose}}^{*}=\begin{cases}0&amp;\textrm{if}~{}E_{q}&lt;\theta_{q}~{}${and}$~{}E_{t}/\||\bm{t}\|&lt;\theta_{t}\\
E_{\textrm{pose}}&amp;\textrm{otherwise}\end{cases}." display="block"><semantics id="S5.E4.m1.5a"><mrow id="S5.E4.m1.5.5.1"><mrow id="S5.E4.m1.5.5.1.1"><msubsup id="S5.E4.m1.5.5.1.1.2"><mi id="S5.E4.m1.5.5.1.1.2.2.2">E</mi><mtext id="S5.E4.m1.5.5.1.1.2.2.3">pose</mtext><mo id="S5.E4.m1.5.5.1.1.2.3">∗</mo></msubsup><mo id="S5.E4.m1.5.5.1.1.1">=</mo><mrow id="S5.E4.m1.4.4"><mo id="S5.E4.m1.4.4.5">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S5.E4.m1.4.4.4"><mtr id="S5.E4.m1.4.4.4a"><mtd class="ltx_align_left" columnalign="left" id="S5.E4.m1.4.4.4b"><mn id="S5.E4.m1.1.1.1.1.1.1">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S5.E4.m1.4.4.4c"><mrow id="S5.E4.m1.2.2.2.2.2.1"><mtext id="S5.E4.m1.2.2.2.2.2.1.2">if</mtext><msub id="S5.E4.m1.2.2.2.2.2.1.3"><mi id="S5.E4.m1.2.2.2.2.2.1.3.2">E</mi><mi id="S5.E4.m1.2.2.2.2.2.1.3.3">q</mi></msub><mo id="S5.E4.m1.2.2.2.2.2.1.4">&lt;</mo><msub id="S5.E4.m1.2.2.2.2.2.1.5"><mi id="S5.E4.m1.2.2.2.2.2.1.5.2">θ</mi><mi id="S5.E4.m1.2.2.2.2.2.1.5.3">q</mi></msub><mtext id="S5.E4.m1.2.2.2.2.2.1.6">and</mtext><msub id="S5.E4.m1.2.2.2.2.2.1.7"><mi id="S5.E4.m1.2.2.2.2.2.1.7.2">E</mi><mi id="S5.E4.m1.2.2.2.2.2.1.7.3">t</mi></msub><mo rspace="0em" id="S5.E4.m1.2.2.2.2.2.1.8">/</mo><mo lspace="0em" rspace="0.167em" id="S5.E4.m1.2.2.2.2.2.1.9">∥</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S5.E4.m1.2.2.2.2.2.1.10">|</mo><mi id="S5.E4.m1.2.2.2.2.2.1.1">𝒕</mi><mo lspace="0em" rspace="0.0835em" id="S5.E4.m1.2.2.2.2.2.1.11">∥</mo><mo lspace="0.0835em" id="S5.E4.m1.2.2.2.2.2.1.12">&lt;</mo><msub id="S5.E4.m1.2.2.2.2.2.1.13"><mi id="S5.E4.m1.2.2.2.2.2.1.13.2">θ</mi><mi id="S5.E4.m1.2.2.2.2.2.1.13.3">t</mi></msub></mrow></mtd></mtr><mtr id="S5.E4.m1.4.4.4d"><mtd class="ltx_align_left" columnalign="left" id="S5.E4.m1.4.4.4e"><msub id="S5.E4.m1.3.3.3.3.1.1"><mi id="S5.E4.m1.3.3.3.3.1.1.2">E</mi><mtext id="S5.E4.m1.3.3.3.3.1.1.3">pose</mtext></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="S5.E4.m1.4.4.4f"><mtext id="S5.E4.m1.4.4.4.4.2.1">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><mo lspace="0em" id="S5.E4.m1.5.5.1.2">.</mo></mrow><annotation encoding="application/x-tex" id="S5.E4.m1.5b">E_{\textrm{pose}}^{*}=\begin{cases}0&amp;\textrm{if}~{}E_{q}&lt;\theta_{q}~{}${and}$~{}E_{t}/\||\bm{t}\|&lt;\theta_{t}\\
E_{\textrm{pose}}&amp;\textrm{otherwise}\end{cases}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Results</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">The results of the baseline models are shown in Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Pose estimation CNNs ‣ 5 Experiments ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. First, it indicates significant domain gap between the synthetic and HIL images, especially for SPN and KRN, as evidenced by the performance drop during testing. The degraded performance on <span id="S5.SS4.p1.1.1" class="ltx_text ltx_font_typewriter">lightbox</span> images indicates that, despite milder illumination condition compared to <span id="S5.SS4.p1.1.2" class="ltx_text ltx_font_typewriter">sunlamp</span> images, the difference in spacecraft’s texture between computer graphics and a real, physical model is significant enough to prohibit successful pose prediction. As expected, the performance is much worse when tested on <span id="S5.SS4.p1.1.3" class="ltx_text ltx_font_typewriter">sunlamp</span> images, as they involve much more challenging illumination effects characterized by high contrast and camera overexposure. Interestingly, the performance drop is much smaller for HigherHRNet, which hints that the strategy of heatmap prediction is inherently more robust to domain gap than attitude classification and direct keypoints regression.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2110.03101/assets/x6.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="226" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Cumulative distributions of KRN’s SPEED score when tested on SPEED+ HIL and PRISMA flight images after 5 training sessions with different random seeds.</figcaption>
</figure>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 ‣ 5.1 Pose estimation CNNs ‣ 5 Experiments ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the performances of KRN after domain-bridging algorithms are applied. First, the oracle performance corroborates the overall accuracy of the pose labels of HIL images, and that they do retain the features learnable by the CNNs to perform the pose estimation task. This serves to justify the use of <span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_typewriter">lightbox</span> and <span id="S5.SS4.p2.1.2" class="ltx_text ltx_font_typewriter">sunlamp</span> images as the test images in SPEC2021. The results of domain adaptation using DANN and domain randomizing using style augmentation showcase the challenges associated with bridging the domain gap between synthetic and HIL images of SPEED+, and that more sophisticated domain-bridging algorithms and hyperparameter tuning will be required to further bridge the gap than a straightforward application of an existing approach.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Finally, Figure <a href="#S5.F7" title="Figure 7 ‣ 5.4 Results ‣ 5 Experiments ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> exhibits the cumulative distribution of KRN’s performance on HIL and <span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_typewriter">prisma25</span> test sets after 5 <span id="S5.SS4.p3.1.2" class="ltx_text ltx_font_typewriter">synthetic</span>-only training sessions with different random seeds. Its visualization confirms the challenges associated with <span id="S5.SS4.p3.1.3" class="ltx_text ltx_font_typewriter">sunlamp</span> images compared to those of <span id="S5.SS4.p3.1.4" class="ltx_text ltx_font_typewriter">lightbox</span> for vanilla models trained exclusively on <span id="S5.SS4.p3.1.5" class="ltx_text ltx_font_typewriter">synthetic</span> images. Moreover, despite the limited size of <span id="S5.SS4.p3.1.6" class="ltx_text ltx_font_typewriter">prisma25</span>, Figure <a href="#S5.F7" title="Figure 7 ‣ 5.4 Results ‣ 5 Experiments ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> also demonstrates that the distribution curves of SPEED scores on <span id="S5.SS4.p3.1.7" class="ltx_text ltx_font_typewriter">prisma25</span> more or less overlap with those of the SPEED+ HIL images. The coinciding performance trends between SPEED+ HIL and spaceborne images imply a similar level of domain gap against the <span id="S5.SS4.p3.1.8" class="ltx_text ltx_font_typewriter">synthetic</span> domain and the viability of the HIL images as surrogates of the actual flight images in the robustness analysis of CNNs.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This section describes a few limitations and planned future improvements regarding the current SPEED+ and the provided baseline performance studies.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Dataset Limitations</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">In its currently released form, SPEED+ consists of images of a single known target. However, this paper demonstrates that, given a mockup model of a target, the TRON facility can generate a large number of target HIL images and accurate pose labels with minimal human intervention. Therefore, future sequels of SPEED+ will include more targets to facilitate the study of spaceborne ML techniques beyond the application of pose estimation and navigation about a single known target. Moreover, the KUKA robot arms will undergo a pending update to enable more accurate pose labeling with less human intervention for sample rejection.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Relation to Real Mission Constraints and Flight Images</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In real missions, the servicer spacecraft typically does not have access to the target’s images until rendezvous. However, in SPEC2021, the unlabeled images from <span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">lightbox</span> and <span id="S6.SS2.p1.1.2" class="ltx_text ltx_font_typewriter">sunlamp</span> are available to the public. Such operational constraints are not placed in SPEC2021 to further engage the community’s participation: the participants are encouraged to both use and ignore the given unlabeled target images in training robust ML models.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Baseline Studies</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">To emphasize, the goal of the provided baseline performance studies is not to showcase the best CNN model architecture and training algorithm for the task at hand, but to characterize and justify the SPEED+ dataset and its suitability for SPEC2021 and robustness studies in spaceborne ML and vision applications. A more comprehensive analysis on the factors contributing to robust ML models will be conducted once the competition concludes.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This paper presents SPEED+, the next-generation dataset for spacecraft pose estimation and navigation. Compared to its predecessor, SPEED+ focuses on domain gap between training synthetic images and test images captured from a robotic testbed with high-fidelity spaceborne-like illumination conditions. SPEED+ serves to test and compare the robustness of different ML models as part of the international Satellite Pose Estimation Competition that begins in October 2021. This paper introduces and characterizes the SPEED+ dataset using existing pose estimation CNNs and well-established domain adaptation and randomization algorithms.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">The presented SPEED+ is an important step toward enabling autonomous proximity operations in space using machine learning techniques to support various future mission concepts, such as refueling defunct space assets and active debris removal. This will help ensure sustained access to near-Earth space, which is a finite resource that needs to be properly understood and managed for the future of mankind.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>The construction of the testbed was partly funded by the U.S. Air Force Office of Scientific Research (AFOSR) through the Defense University Research Instrumentation Program (DURIP) contract FA9550-18-1-0492, titled High-Fidelity Verification and Validation of Spaceborne Vision Based Navigation. We would like to thank OHB Sweden for the 3D model of the Tango spacecraft used to create the images used in this article and for the flight images in the <span id="S7.1.1" class="ltx_text ltx_font_typewriter">prisma25</span> dataset.
<span id="S7.2.2" class="ltx_ERROR undefined">\appendices</span> 
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Accessibility</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">SPEED+ is made available as part of the second Satellite Pose Estimation Competition (SPEC2021) organized by the Stanford’s Space Rendezvous Laboratory (SLAB) and the Advanced Concepts Team (ACT) of the European Space Agency (ESA). The competition begins on October 25th, 2021 and continues till the end of March 2022. This section briefly provides our plan with the hosting, maintenance, and licensing.</p>
</div>
<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Kelvins Platform</h3>

<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p">SPEC2021 is hosted on the Kelvins platform of ESA, which has hosted the first SPEC (SPEC2019) and many other space-related challenges. The platform includes a link to the dataset, the description of its structure, a leaderboard, and a page for public discussions about the dataset and the competition. More information can be found at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">kelvins.esa.int/pose-estimation-2021/</span>.</p>
</div>
<div id="S8.SS1.p2" class="ltx_para">
<p id="S8.SS1.p2.1" class="ltx_p">The SPEED+ dataset is publicly released through the Stanford Digital Repository with a unique DOI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, and the link to the dataset is available on Zenodo as well through the SPEC2021 platform. The dataset is released under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC BY-NC-SA 4.0) license.</p>
</div>
<figure id="S8.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Calibrated camera parameters</figcaption>
<table id="S8.T5.11" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S8.T5.11.12" class="ltx_tr">
<td id="S8.T5.11.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Parameter</td>
<td id="S8.T5.11.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Description</td>
<td id="S8.T5.11.12.3" class="ltx_td ltx_align_center ltx_border_tt">Value</td>
</tr>
<tr id="S8.T5.1.1" class="ltx_tr">
<td id="S8.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S8.T5.1.1.1.m1.1" class="ltx_Math" alttext="N_{u}" display="inline"><semantics id="S8.T5.1.1.1.m1.1a"><msub id="S8.T5.1.1.1.m1.1.1" xref="S8.T5.1.1.1.m1.1.1.cmml"><mi id="S8.T5.1.1.1.m1.1.1.2" xref="S8.T5.1.1.1.m1.1.1.2.cmml">N</mi><mi id="S8.T5.1.1.1.m1.1.1.3" xref="S8.T5.1.1.1.m1.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="S8.T5.1.1.1.m1.1b"><apply id="S8.T5.1.1.1.m1.1.1.cmml" xref="S8.T5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S8.T5.1.1.1.m1.1.1.1.cmml" xref="S8.T5.1.1.1.m1.1.1">subscript</csymbol><ci id="S8.T5.1.1.1.m1.1.1.2.cmml" xref="S8.T5.1.1.1.m1.1.1.2">𝑁</ci><ci id="S8.T5.1.1.1.m1.1.1.3.cmml" xref="S8.T5.1.1.1.m1.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.T5.1.1.1.m1.1c">N_{u}</annotation></semantics></math></td>
<td id="S8.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Number of horizontal pixels</td>
<td id="S8.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_t">1920</td>
</tr>
<tr id="S8.T5.2.2" class="ltx_tr">
<td id="S8.T5.2.2.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S8.T5.2.2.1.m1.1" class="ltx_Math" alttext="N_{v}" display="inline"><semantics id="S8.T5.2.2.1.m1.1a"><msub id="S8.T5.2.2.1.m1.1.1" xref="S8.T5.2.2.1.m1.1.1.cmml"><mi id="S8.T5.2.2.1.m1.1.1.2" xref="S8.T5.2.2.1.m1.1.1.2.cmml">N</mi><mi id="S8.T5.2.2.1.m1.1.1.3" xref="S8.T5.2.2.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S8.T5.2.2.1.m1.1b"><apply id="S8.T5.2.2.1.m1.1.1.cmml" xref="S8.T5.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S8.T5.2.2.1.m1.1.1.1.cmml" xref="S8.T5.2.2.1.m1.1.1">subscript</csymbol><ci id="S8.T5.2.2.1.m1.1.1.2.cmml" xref="S8.T5.2.2.1.m1.1.1.2">𝑁</ci><ci id="S8.T5.2.2.1.m1.1.1.3.cmml" xref="S8.T5.2.2.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.T5.2.2.1.m1.1c">N_{v}</annotation></semantics></math></td>
<td id="S8.T5.2.2.2" class="ltx_td ltx_align_center ltx_border_r">Number of vertical pixels</td>
<td id="S8.T5.2.2.3" class="ltx_td ltx_align_center">1200</td>
</tr>
<tr id="S8.T5.3.3" class="ltx_tr">
<td id="S8.T5.3.3.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S8.T5.3.3.1.m1.1" class="ltx_Math" alttext="f_{x}" display="inline"><semantics id="S8.T5.3.3.1.m1.1a"><msub id="S8.T5.3.3.1.m1.1.1" xref="S8.T5.3.3.1.m1.1.1.cmml"><mi id="S8.T5.3.3.1.m1.1.1.2" xref="S8.T5.3.3.1.m1.1.1.2.cmml">f</mi><mi id="S8.T5.3.3.1.m1.1.1.3" xref="S8.T5.3.3.1.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S8.T5.3.3.1.m1.1b"><apply id="S8.T5.3.3.1.m1.1.1.cmml" xref="S8.T5.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S8.T5.3.3.1.m1.1.1.1.cmml" xref="S8.T5.3.3.1.m1.1.1">subscript</csymbol><ci id="S8.T5.3.3.1.m1.1.1.2.cmml" xref="S8.T5.3.3.1.m1.1.1.2">𝑓</ci><ci id="S8.T5.3.3.1.m1.1.1.3.cmml" xref="S8.T5.3.3.1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.T5.3.3.1.m1.1c">f_{x}</annotation></semantics></math></td>
<td id="S8.T5.3.3.2" class="ltx_td ltx_align_center ltx_border_r">Horizontal focal length [m]</td>
<td id="S8.T5.3.3.3" class="ltx_td ltx_align_center">0.017513</td>
</tr>
<tr id="S8.T5.4.4" class="ltx_tr">
<td id="S8.T5.4.4.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S8.T5.4.4.1.m1.1" class="ltx_Math" alttext="f_{x}" display="inline"><semantics id="S8.T5.4.4.1.m1.1a"><msub id="S8.T5.4.4.1.m1.1.1" xref="S8.T5.4.4.1.m1.1.1.cmml"><mi id="S8.T5.4.4.1.m1.1.1.2" xref="S8.T5.4.4.1.m1.1.1.2.cmml">f</mi><mi id="S8.T5.4.4.1.m1.1.1.3" xref="S8.T5.4.4.1.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S8.T5.4.4.1.m1.1b"><apply id="S8.T5.4.4.1.m1.1.1.cmml" xref="S8.T5.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S8.T5.4.4.1.m1.1.1.1.cmml" xref="S8.T5.4.4.1.m1.1.1">subscript</csymbol><ci id="S8.T5.4.4.1.m1.1.1.2.cmml" xref="S8.T5.4.4.1.m1.1.1.2">𝑓</ci><ci id="S8.T5.4.4.1.m1.1.1.3.cmml" xref="S8.T5.4.4.1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.T5.4.4.1.m1.1c">f_{x}</annotation></semantics></math></td>
<td id="S8.T5.4.4.2" class="ltx_td ltx_align_center ltx_border_r">Vertical focal length [m]</td>
<td id="S8.T5.4.4.3" class="ltx_td ltx_align_center">0.017513</td>
</tr>
<tr id="S8.T5.6.6" class="ltx_tr">
<td id="S8.T5.5.5.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S8.T5.5.5.1.m1.1" class="ltx_Math" alttext="p_{x}" display="inline"><semantics id="S8.T5.5.5.1.m1.1a"><msub id="S8.T5.5.5.1.m1.1.1" xref="S8.T5.5.5.1.m1.1.1.cmml"><mi id="S8.T5.5.5.1.m1.1.1.2" xref="S8.T5.5.5.1.m1.1.1.2.cmml">p</mi><mi id="S8.T5.5.5.1.m1.1.1.3" xref="S8.T5.5.5.1.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S8.T5.5.5.1.m1.1b"><apply id="S8.T5.5.5.1.m1.1.1.cmml" xref="S8.T5.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S8.T5.5.5.1.m1.1.1.1.cmml" xref="S8.T5.5.5.1.m1.1.1">subscript</csymbol><ci id="S8.T5.5.5.1.m1.1.1.2.cmml" xref="S8.T5.5.5.1.m1.1.1.2">𝑝</ci><ci id="S8.T5.5.5.1.m1.1.1.3.cmml" xref="S8.T5.5.5.1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.T5.5.5.1.m1.1c">p_{x}</annotation></semantics></math></td>
<td id="S8.T5.6.6.2" class="ltx_td ltx_align_center ltx_border_r">Horizontal pixel length [<math id="S8.T5.6.6.2.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S8.T5.6.6.2.m1.1a"><mi id="S8.T5.6.6.2.m1.1.1" xref="S8.T5.6.6.2.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S8.T5.6.6.2.m1.1b"><ci id="S8.T5.6.6.2.m1.1.1.cmml" xref="S8.T5.6.6.2.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.T5.6.6.2.m1.1c">\mu</annotation></semantics></math>m]</td>
<td id="S8.T5.6.6.3" class="ltx_td ltx_align_center">5.86</td>
</tr>
<tr id="S8.T5.8.8" class="ltx_tr">
<td id="S8.T5.7.7.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S8.T5.7.7.1.m1.1" class="ltx_Math" alttext="p_{y}" display="inline"><semantics id="S8.T5.7.7.1.m1.1a"><msub id="S8.T5.7.7.1.m1.1.1" xref="S8.T5.7.7.1.m1.1.1.cmml"><mi id="S8.T5.7.7.1.m1.1.1.2" xref="S8.T5.7.7.1.m1.1.1.2.cmml">p</mi><mi id="S8.T5.7.7.1.m1.1.1.3" xref="S8.T5.7.7.1.m1.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S8.T5.7.7.1.m1.1b"><apply id="S8.T5.7.7.1.m1.1.1.cmml" xref="S8.T5.7.7.1.m1.1.1"><csymbol cd="ambiguous" id="S8.T5.7.7.1.m1.1.1.1.cmml" xref="S8.T5.7.7.1.m1.1.1">subscript</csymbol><ci id="S8.T5.7.7.1.m1.1.1.2.cmml" xref="S8.T5.7.7.1.m1.1.1.2">𝑝</ci><ci id="S8.T5.7.7.1.m1.1.1.3.cmml" xref="S8.T5.7.7.1.m1.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.T5.7.7.1.m1.1c">p_{y}</annotation></semantics></math></td>
<td id="S8.T5.8.8.2" class="ltx_td ltx_align_center ltx_border_r">Vertical pixel length [<math id="S8.T5.8.8.2.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S8.T5.8.8.2.m1.1a"><mi id="S8.T5.8.8.2.m1.1.1" xref="S8.T5.8.8.2.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S8.T5.8.8.2.m1.1b"><ci id="S8.T5.8.8.2.m1.1.1.cmml" xref="S8.T5.8.8.2.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.T5.8.8.2.m1.1c">\mu</annotation></semantics></math>m]</td>
<td id="S8.T5.8.8.3" class="ltx_td ltx_align_center">5.86</td>
</tr>
<tr id="S8.T5.9.9" class="ltx_tr">
<td id="S8.T5.9.9.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S8.T5.9.9.1.m1.3" class="ltx_Math" alttext="[r_{1},r_{2},r_{3}]" display="inline"><semantics id="S8.T5.9.9.1.m1.3a"><mrow id="S8.T5.9.9.1.m1.3.3.3" xref="S8.T5.9.9.1.m1.3.3.4.cmml"><mo stretchy="false" id="S8.T5.9.9.1.m1.3.3.3.4" xref="S8.T5.9.9.1.m1.3.3.4.cmml">[</mo><msub id="S8.T5.9.9.1.m1.1.1.1.1" xref="S8.T5.9.9.1.m1.1.1.1.1.cmml"><mi id="S8.T5.9.9.1.m1.1.1.1.1.2" xref="S8.T5.9.9.1.m1.1.1.1.1.2.cmml">r</mi><mn id="S8.T5.9.9.1.m1.1.1.1.1.3" xref="S8.T5.9.9.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S8.T5.9.9.1.m1.3.3.3.5" xref="S8.T5.9.9.1.m1.3.3.4.cmml">,</mo><msub id="S8.T5.9.9.1.m1.2.2.2.2" xref="S8.T5.9.9.1.m1.2.2.2.2.cmml"><mi id="S8.T5.9.9.1.m1.2.2.2.2.2" xref="S8.T5.9.9.1.m1.2.2.2.2.2.cmml">r</mi><mn id="S8.T5.9.9.1.m1.2.2.2.2.3" xref="S8.T5.9.9.1.m1.2.2.2.2.3.cmml">2</mn></msub><mo id="S8.T5.9.9.1.m1.3.3.3.6" xref="S8.T5.9.9.1.m1.3.3.4.cmml">,</mo><msub id="S8.T5.9.9.1.m1.3.3.3.3" xref="S8.T5.9.9.1.m1.3.3.3.3.cmml"><mi id="S8.T5.9.9.1.m1.3.3.3.3.2" xref="S8.T5.9.9.1.m1.3.3.3.3.2.cmml">r</mi><mn id="S8.T5.9.9.1.m1.3.3.3.3.3" xref="S8.T5.9.9.1.m1.3.3.3.3.3.cmml">3</mn></msub><mo stretchy="false" id="S8.T5.9.9.1.m1.3.3.3.7" xref="S8.T5.9.9.1.m1.3.3.4.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S8.T5.9.9.1.m1.3b"><list id="S8.T5.9.9.1.m1.3.3.4.cmml" xref="S8.T5.9.9.1.m1.3.3.3"><apply id="S8.T5.9.9.1.m1.1.1.1.1.cmml" xref="S8.T5.9.9.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S8.T5.9.9.1.m1.1.1.1.1.1.cmml" xref="S8.T5.9.9.1.m1.1.1.1.1">subscript</csymbol><ci id="S8.T5.9.9.1.m1.1.1.1.1.2.cmml" xref="S8.T5.9.9.1.m1.1.1.1.1.2">𝑟</ci><cn type="integer" id="S8.T5.9.9.1.m1.1.1.1.1.3.cmml" xref="S8.T5.9.9.1.m1.1.1.1.1.3">1</cn></apply><apply id="S8.T5.9.9.1.m1.2.2.2.2.cmml" xref="S8.T5.9.9.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S8.T5.9.9.1.m1.2.2.2.2.1.cmml" xref="S8.T5.9.9.1.m1.2.2.2.2">subscript</csymbol><ci id="S8.T5.9.9.1.m1.2.2.2.2.2.cmml" xref="S8.T5.9.9.1.m1.2.2.2.2.2">𝑟</ci><cn type="integer" id="S8.T5.9.9.1.m1.2.2.2.2.3.cmml" xref="S8.T5.9.9.1.m1.2.2.2.2.3">2</cn></apply><apply id="S8.T5.9.9.1.m1.3.3.3.3.cmml" xref="S8.T5.9.9.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S8.T5.9.9.1.m1.3.3.3.3.1.cmml" xref="S8.T5.9.9.1.m1.3.3.3.3">subscript</csymbol><ci id="S8.T5.9.9.1.m1.3.3.3.3.2.cmml" xref="S8.T5.9.9.1.m1.3.3.3.3.2">𝑟</ci><cn type="integer" id="S8.T5.9.9.1.m1.3.3.3.3.3.cmml" xref="S8.T5.9.9.1.m1.3.3.3.3.3">3</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S8.T5.9.9.1.m1.3c">[r_{1},r_{2},r_{3}]</annotation></semantics></math></td>
<td id="S8.T5.9.9.2" class="ltx_td ltx_align_center ltx_border_r">Radial distortion parameters</td>
<td id="S8.T5.9.9.3" class="ltx_td ltx_align_center">[-0.2238, 0.5141, -0.1312]</td>
</tr>
<tr id="S8.T5.11.11" class="ltx_tr">
<td id="S8.T5.10.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><math id="S8.T5.10.10.1.m1.2" class="ltx_Math" alttext="[t_{1},t_{2}]" display="inline"><semantics id="S8.T5.10.10.1.m1.2a"><mrow id="S8.T5.10.10.1.m1.2.2.2" xref="S8.T5.10.10.1.m1.2.2.3.cmml"><mo stretchy="false" id="S8.T5.10.10.1.m1.2.2.2.3" xref="S8.T5.10.10.1.m1.2.2.3.cmml">[</mo><msub id="S8.T5.10.10.1.m1.1.1.1.1" xref="S8.T5.10.10.1.m1.1.1.1.1.cmml"><mi id="S8.T5.10.10.1.m1.1.1.1.1.2" xref="S8.T5.10.10.1.m1.1.1.1.1.2.cmml">t</mi><mn id="S8.T5.10.10.1.m1.1.1.1.1.3" xref="S8.T5.10.10.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S8.T5.10.10.1.m1.2.2.2.4" xref="S8.T5.10.10.1.m1.2.2.3.cmml">,</mo><msub id="S8.T5.10.10.1.m1.2.2.2.2" xref="S8.T5.10.10.1.m1.2.2.2.2.cmml"><mi id="S8.T5.10.10.1.m1.2.2.2.2.2" xref="S8.T5.10.10.1.m1.2.2.2.2.2.cmml">t</mi><mn id="S8.T5.10.10.1.m1.2.2.2.2.3" xref="S8.T5.10.10.1.m1.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S8.T5.10.10.1.m1.2.2.2.5" xref="S8.T5.10.10.1.m1.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S8.T5.10.10.1.m1.2b"><interval closure="closed" id="S8.T5.10.10.1.m1.2.2.3.cmml" xref="S8.T5.10.10.1.m1.2.2.2"><apply id="S8.T5.10.10.1.m1.1.1.1.1.cmml" xref="S8.T5.10.10.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S8.T5.10.10.1.m1.1.1.1.1.1.cmml" xref="S8.T5.10.10.1.m1.1.1.1.1">subscript</csymbol><ci id="S8.T5.10.10.1.m1.1.1.1.1.2.cmml" xref="S8.T5.10.10.1.m1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S8.T5.10.10.1.m1.1.1.1.1.3.cmml" xref="S8.T5.10.10.1.m1.1.1.1.1.3">1</cn></apply><apply id="S8.T5.10.10.1.m1.2.2.2.2.cmml" xref="S8.T5.10.10.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S8.T5.10.10.1.m1.2.2.2.2.1.cmml" xref="S8.T5.10.10.1.m1.2.2.2.2">subscript</csymbol><ci id="S8.T5.10.10.1.m1.2.2.2.2.2.cmml" xref="S8.T5.10.10.1.m1.2.2.2.2.2">𝑡</ci><cn type="integer" id="S8.T5.10.10.1.m1.2.2.2.2.3.cmml" xref="S8.T5.10.10.1.m1.2.2.2.2.3">2</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S8.T5.10.10.1.m1.2c">[t_{1},t_{2}]</annotation></semantics></math></td>
<td id="S8.T5.11.11.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Tangential distortion parameters (<math id="S8.T5.11.11.2.m1.1" class="ltx_Math" alttext="\times 10^{-4}" display="inline"><semantics id="S8.T5.11.11.2.m1.1a"><mrow id="S8.T5.11.11.2.m1.1.1" xref="S8.T5.11.11.2.m1.1.1.cmml"><mi id="S8.T5.11.11.2.m1.1.1.2" xref="S8.T5.11.11.2.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S8.T5.11.11.2.m1.1.1.1" xref="S8.T5.11.11.2.m1.1.1.1.cmml">×</mo><msup id="S8.T5.11.11.2.m1.1.1.3" xref="S8.T5.11.11.2.m1.1.1.3.cmml"><mn id="S8.T5.11.11.2.m1.1.1.3.2" xref="S8.T5.11.11.2.m1.1.1.3.2.cmml">10</mn><mrow id="S8.T5.11.11.2.m1.1.1.3.3" xref="S8.T5.11.11.2.m1.1.1.3.3.cmml"><mo id="S8.T5.11.11.2.m1.1.1.3.3a" xref="S8.T5.11.11.2.m1.1.1.3.3.cmml">−</mo><mn id="S8.T5.11.11.2.m1.1.1.3.3.2" xref="S8.T5.11.11.2.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S8.T5.11.11.2.m1.1b"><apply id="S8.T5.11.11.2.m1.1.1.cmml" xref="S8.T5.11.11.2.m1.1.1"><times id="S8.T5.11.11.2.m1.1.1.1.cmml" xref="S8.T5.11.11.2.m1.1.1.1"></times><csymbol cd="latexml" id="S8.T5.11.11.2.m1.1.1.2.cmml" xref="S8.T5.11.11.2.m1.1.1.2">absent</csymbol><apply id="S8.T5.11.11.2.m1.1.1.3.cmml" xref="S8.T5.11.11.2.m1.1.1.3"><csymbol cd="ambiguous" id="S8.T5.11.11.2.m1.1.1.3.1.cmml" xref="S8.T5.11.11.2.m1.1.1.3">superscript</csymbol><cn type="integer" id="S8.T5.11.11.2.m1.1.1.3.2.cmml" xref="S8.T5.11.11.2.m1.1.1.3.2">10</cn><apply id="S8.T5.11.11.2.m1.1.1.3.3.cmml" xref="S8.T5.11.11.2.m1.1.1.3.3"><minus id="S8.T5.11.11.2.m1.1.1.3.3.1.cmml" xref="S8.T5.11.11.2.m1.1.1.3.3"></minus><cn type="integer" id="S8.T5.11.11.2.m1.1.1.3.3.2.cmml" xref="S8.T5.11.11.2.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.T5.11.11.2.m1.1c">\times 10^{-4}</annotation></semantics></math>)</td>
<td id="S8.T5.11.11.3" class="ltx_td ltx_align_center ltx_border_bb">[-6.650, -2.140]</td>
</tr>
</table>
</figure>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span>Baseline Models</h3>

<div id="S8.SS2.p1" class="ltx_para">
<p id="S8.SS2.p1.1" class="ltx_p">The SPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and KRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> baseline models, along with DANN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and style augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, are available in the following GitHub repository: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">github.com/tpark94/speedplusbaseline</span></p>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Camera Model</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">The SPEED+ HIL images are captured using the Point Grey Grasshopper 3 camera with a Xenoplan 1.4/17mm lens. The camera is calibrated prior to its use, and its calibrated parameters are shown in Table <a href="#S8.T5" title="Table 5 ‣ 8.1 Kelvins Platform ‣ 8 Accessibility ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S9.p2" class="ltx_para">
<p id="S9.p2.1" class="ltx_p">The radial and tangential distortion parameters follow the conventions used in OpenCV. The specified parameters are provided in the <span id="S9.p2.1.1" class="ltx_text ltx_font_typewriter">camera.json</span> file.</p>
</div>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Training Details</h2>

<div id="S10.p1" class="ltx_para">
<p id="S10.p1.1" class="ltx_p">This section describes the training details for the baseline studies. All methods are implemented with PyTorch v1.8.0 and trained on an NVIDIA GeForce RTX 2080 Ti 12GB GPU.</p>
</div>
<section id="S10.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.1 </span>KRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</h3>

<div id="S10.SS1.p1" class="ltx_para">
<p id="S10.SS1.p1.1" class="ltx_p">First, the training of KRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> largely follows the original work, except it instead uses the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> optimizer with <math id="S10.SS1.p1.1.m1.2" class="ltx_Math" alttext="\beta_{1}=0.9,\beta_{2}=0.999" display="inline"><semantics id="S10.SS1.p1.1.m1.2a"><mrow id="S10.SS1.p1.1.m1.2.2.2" xref="S10.SS1.p1.1.m1.2.2.3.cmml"><mrow id="S10.SS1.p1.1.m1.1.1.1.1" xref="S10.SS1.p1.1.m1.1.1.1.1.cmml"><msub id="S10.SS1.p1.1.m1.1.1.1.1.2" xref="S10.SS1.p1.1.m1.1.1.1.1.2.cmml"><mi id="S10.SS1.p1.1.m1.1.1.1.1.2.2" xref="S10.SS1.p1.1.m1.1.1.1.1.2.2.cmml">β</mi><mn id="S10.SS1.p1.1.m1.1.1.1.1.2.3" xref="S10.SS1.p1.1.m1.1.1.1.1.2.3.cmml">1</mn></msub><mo id="S10.SS1.p1.1.m1.1.1.1.1.1" xref="S10.SS1.p1.1.m1.1.1.1.1.1.cmml">=</mo><mn id="S10.SS1.p1.1.m1.1.1.1.1.3" xref="S10.SS1.p1.1.m1.1.1.1.1.3.cmml">0.9</mn></mrow><mo id="S10.SS1.p1.1.m1.2.2.2.3" xref="S10.SS1.p1.1.m1.2.2.3a.cmml">,</mo><mrow id="S10.SS1.p1.1.m1.2.2.2.2" xref="S10.SS1.p1.1.m1.2.2.2.2.cmml"><msub id="S10.SS1.p1.1.m1.2.2.2.2.2" xref="S10.SS1.p1.1.m1.2.2.2.2.2.cmml"><mi id="S10.SS1.p1.1.m1.2.2.2.2.2.2" xref="S10.SS1.p1.1.m1.2.2.2.2.2.2.cmml">β</mi><mn id="S10.SS1.p1.1.m1.2.2.2.2.2.3" xref="S10.SS1.p1.1.m1.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S10.SS1.p1.1.m1.2.2.2.2.1" xref="S10.SS1.p1.1.m1.2.2.2.2.1.cmml">=</mo><mn id="S10.SS1.p1.1.m1.2.2.2.2.3" xref="S10.SS1.p1.1.m1.2.2.2.2.3.cmml">0.999</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.p1.1.m1.2b"><apply id="S10.SS1.p1.1.m1.2.2.3.cmml" xref="S10.SS1.p1.1.m1.2.2.2"><csymbol cd="ambiguous" id="S10.SS1.p1.1.m1.2.2.3a.cmml" xref="S10.SS1.p1.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S10.SS1.p1.1.m1.1.1.1.1.cmml" xref="S10.SS1.p1.1.m1.1.1.1.1"><eq id="S10.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S10.SS1.p1.1.m1.1.1.1.1.1"></eq><apply id="S10.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S10.SS1.p1.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S10.SS1.p1.1.m1.1.1.1.1.2.1.cmml" xref="S10.SS1.p1.1.m1.1.1.1.1.2">subscript</csymbol><ci id="S10.SS1.p1.1.m1.1.1.1.1.2.2.cmml" xref="S10.SS1.p1.1.m1.1.1.1.1.2.2">𝛽</ci><cn type="integer" id="S10.SS1.p1.1.m1.1.1.1.1.2.3.cmml" xref="S10.SS1.p1.1.m1.1.1.1.1.2.3">1</cn></apply><cn type="float" id="S10.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S10.SS1.p1.1.m1.1.1.1.1.3">0.9</cn></apply><apply id="S10.SS1.p1.1.m1.2.2.2.2.cmml" xref="S10.SS1.p1.1.m1.2.2.2.2"><eq id="S10.SS1.p1.1.m1.2.2.2.2.1.cmml" xref="S10.SS1.p1.1.m1.2.2.2.2.1"></eq><apply id="S10.SS1.p1.1.m1.2.2.2.2.2.cmml" xref="S10.SS1.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S10.SS1.p1.1.m1.2.2.2.2.2.1.cmml" xref="S10.SS1.p1.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S10.SS1.p1.1.m1.2.2.2.2.2.2.cmml" xref="S10.SS1.p1.1.m1.2.2.2.2.2.2">𝛽</ci><cn type="integer" id="S10.SS1.p1.1.m1.2.2.2.2.2.3.cmml" xref="S10.SS1.p1.1.m1.2.2.2.2.2.3">2</cn></apply><cn type="float" id="S10.SS1.p1.1.m1.2.2.2.2.3.cmml" xref="S10.SS1.p1.1.m1.2.2.2.2.3">0.999</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.p1.1.m1.2c">\beta_{1}=0.9,\beta_{2}=0.999</annotation></semantics></math> for backpropagation. When training on <span id="S10.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">synthetic</span> training images, KRN is trained with batch size of 48 for 50 epochs and initial learning rate of 0.001 which decays by the factor of 0.95. When training on the HIL images, the number of training epochs and the decay factor are adjusted according to the number of the available training images.</p>
</div>
</section>
<section id="S10.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.2 </span>SPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</h3>

<div id="S10.SS2.p1" class="ltx_para">
<p id="S10.SS2.p1.2" class="ltx_p">The implementation of SPN follows the original tensorflow version of the author, which differs from the descriptions in Sharma &amp; D’Amico <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. In this work, the backbone is the first 5 layers of ImageNet-pretrained AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, which takes 227 <math id="S10.SS2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S10.SS2.p1.1.m1.1a"><mo id="S10.SS2.p1.1.m1.1.1" xref="S10.SS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.SS2.p1.1.m1.1b"><times id="S10.SS2.p1.1.m1.1.1.cmml" xref="S10.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.SS2.p1.1.m1.1c">\times</annotation></semantics></math> 227 images as inputs. The Branches 2 and 3 of SPN, along with the backbone, are trained together simultaneously. The labels are generated from 5,000 attitude classes uniformly sampled from the <math id="S10.SS2.p1.2.m2.1" class="ltx_Math" alttext="SO" display="inline"><semantics id="S10.SS2.p1.2.m2.1a"><mrow id="S10.SS2.p1.2.m2.1.1" xref="S10.SS2.p1.2.m2.1.1.cmml"><mi id="S10.SS2.p1.2.m2.1.1.2" xref="S10.SS2.p1.2.m2.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S10.SS2.p1.2.m2.1.1.1" xref="S10.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S10.SS2.p1.2.m2.1.1.3" xref="S10.SS2.p1.2.m2.1.1.3.cmml">O</mi></mrow><annotation-xml encoding="MathML-Content" id="S10.SS2.p1.2.m2.1b"><apply id="S10.SS2.p1.2.m2.1.1.cmml" xref="S10.SS2.p1.2.m2.1.1"><times id="S10.SS2.p1.2.m2.1.1.1.cmml" xref="S10.SS2.p1.2.m2.1.1.1"></times><ci id="S10.SS2.p1.2.m2.1.1.2.cmml" xref="S10.SS2.p1.2.m2.1.1.2">𝑆</ci><ci id="S10.SS2.p1.2.m2.1.1.3.cmml" xref="S10.SS2.p1.2.m2.1.1.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS2.p1.2.m2.1c">SO</annotation></semantics></math>(3) space, and at training and inference, 5 neighboring attitude classes are predicted.</p>
</div>
</section>
<section id="S10.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.3 </span>HigherHRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</h3>

<div id="S10.SS3.p1" class="ltx_para">
<p id="S10.SS3.p1.6" class="ltx_p">The network is directly taken from the official repository<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation</span></span></span> and simplified for the application of single-object pose estimation. The exact same data augmentation pipeline for KRN is used here. The input image is resized to 480 <math id="S10.SS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S10.SS3.p1.1.m1.1a"><mo id="S10.SS3.p1.1.m1.1.1" xref="S10.SS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.SS3.p1.1.m1.1b"><times id="S10.SS3.p1.1.m1.1.1.cmml" xref="S10.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.SS3.p1.1.m1.1c">\times</annotation></semantics></math> 320. The keypoint location is taken to be the pixel with the highest intensity in the predicted heatmaps. Then, EP<math id="S10.SS3.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S10.SS3.p1.2.m2.1a"><mi id="S10.SS3.p1.2.m2.1.1" xref="S10.SS3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S10.SS3.p1.2.m2.1b"><ci id="S10.SS3.p1.2.m2.1.1.cmml" xref="S10.SS3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S10.SS3.p1.2.m2.1c">n</annotation></semantics></math>P <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> is used to solve for the P<math id="S10.SS3.p1.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S10.SS3.p1.3.m3.1a"><mi id="S10.SS3.p1.3.m3.1.1" xref="S10.SS3.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S10.SS3.p1.3.m3.1b"><ci id="S10.SS3.p1.3.m3.1.1.cmml" xref="S10.SS3.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S10.SS3.p1.3.m3.1c">n</annotation></semantics></math>P problem. In case the HigherHRNet backbone fails to detect enough keypoints to run P<math id="S10.SS3.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S10.SS3.p1.4.m4.1a"><mi id="S10.SS3.p1.4.m4.1.1" xref="S10.SS3.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S10.SS3.p1.4.m4.1b"><ci id="S10.SS3.p1.4.m4.1.1.cmml" xref="S10.SS3.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S10.SS3.p1.4.m4.1c">n</annotation></semantics></math>P, a dummy solution of <math id="S10.SS3.p1.5.m5.3" class="ltx_Math" alttext="\bm{t}=[0,0,5]^{\top}" display="inline"><semantics id="S10.SS3.p1.5.m5.3a"><mrow id="S10.SS3.p1.5.m5.3.4" xref="S10.SS3.p1.5.m5.3.4.cmml"><mi id="S10.SS3.p1.5.m5.3.4.2" xref="S10.SS3.p1.5.m5.3.4.2.cmml">𝒕</mi><mo id="S10.SS3.p1.5.m5.3.4.1" xref="S10.SS3.p1.5.m5.3.4.1.cmml">=</mo><msup id="S10.SS3.p1.5.m5.3.4.3" xref="S10.SS3.p1.5.m5.3.4.3.cmml"><mrow id="S10.SS3.p1.5.m5.3.4.3.2.2" xref="S10.SS3.p1.5.m5.3.4.3.2.1.cmml"><mo stretchy="false" id="S10.SS3.p1.5.m5.3.4.3.2.2.1" xref="S10.SS3.p1.5.m5.3.4.3.2.1.cmml">[</mo><mn id="S10.SS3.p1.5.m5.1.1" xref="S10.SS3.p1.5.m5.1.1.cmml">0</mn><mo id="S10.SS3.p1.5.m5.3.4.3.2.2.2" xref="S10.SS3.p1.5.m5.3.4.3.2.1.cmml">,</mo><mn id="S10.SS3.p1.5.m5.2.2" xref="S10.SS3.p1.5.m5.2.2.cmml">0</mn><mo id="S10.SS3.p1.5.m5.3.4.3.2.2.3" xref="S10.SS3.p1.5.m5.3.4.3.2.1.cmml">,</mo><mn id="S10.SS3.p1.5.m5.3.3" xref="S10.SS3.p1.5.m5.3.3.cmml">5</mn><mo stretchy="false" id="S10.SS3.p1.5.m5.3.4.3.2.2.4" xref="S10.SS3.p1.5.m5.3.4.3.2.1.cmml">]</mo></mrow><mo id="S10.SS3.p1.5.m5.3.4.3.3" xref="S10.SS3.p1.5.m5.3.4.3.3.cmml">⊤</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S10.SS3.p1.5.m5.3b"><apply id="S10.SS3.p1.5.m5.3.4.cmml" xref="S10.SS3.p1.5.m5.3.4"><eq id="S10.SS3.p1.5.m5.3.4.1.cmml" xref="S10.SS3.p1.5.m5.3.4.1"></eq><ci id="S10.SS3.p1.5.m5.3.4.2.cmml" xref="S10.SS3.p1.5.m5.3.4.2">𝒕</ci><apply id="S10.SS3.p1.5.m5.3.4.3.cmml" xref="S10.SS3.p1.5.m5.3.4.3"><csymbol cd="ambiguous" id="S10.SS3.p1.5.m5.3.4.3.1.cmml" xref="S10.SS3.p1.5.m5.3.4.3">superscript</csymbol><list id="S10.SS3.p1.5.m5.3.4.3.2.1.cmml" xref="S10.SS3.p1.5.m5.3.4.3.2.2"><cn type="integer" id="S10.SS3.p1.5.m5.1.1.cmml" xref="S10.SS3.p1.5.m5.1.1">0</cn><cn type="integer" id="S10.SS3.p1.5.m5.2.2.cmml" xref="S10.SS3.p1.5.m5.2.2">0</cn><cn type="integer" id="S10.SS3.p1.5.m5.3.3.cmml" xref="S10.SS3.p1.5.m5.3.3">5</cn></list><csymbol cd="latexml" id="S10.SS3.p1.5.m5.3.4.3.3.cmml" xref="S10.SS3.p1.5.m5.3.4.3.3">top</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS3.p1.5.m5.3c">\bm{t}=[0,0,5]^{\top}</annotation></semantics></math> (m), <math id="S10.SS3.p1.6.m6.4" class="ltx_Math" alttext="\bm{q}=[1,0,0,0]^{\top}" display="inline"><semantics id="S10.SS3.p1.6.m6.4a"><mrow id="S10.SS3.p1.6.m6.4.5" xref="S10.SS3.p1.6.m6.4.5.cmml"><mi id="S10.SS3.p1.6.m6.4.5.2" xref="S10.SS3.p1.6.m6.4.5.2.cmml">𝒒</mi><mo id="S10.SS3.p1.6.m6.4.5.1" xref="S10.SS3.p1.6.m6.4.5.1.cmml">=</mo><msup id="S10.SS3.p1.6.m6.4.5.3" xref="S10.SS3.p1.6.m6.4.5.3.cmml"><mrow id="S10.SS3.p1.6.m6.4.5.3.2.2" xref="S10.SS3.p1.6.m6.4.5.3.2.1.cmml"><mo stretchy="false" id="S10.SS3.p1.6.m6.4.5.3.2.2.1" xref="S10.SS3.p1.6.m6.4.5.3.2.1.cmml">[</mo><mn id="S10.SS3.p1.6.m6.1.1" xref="S10.SS3.p1.6.m6.1.1.cmml">1</mn><mo id="S10.SS3.p1.6.m6.4.5.3.2.2.2" xref="S10.SS3.p1.6.m6.4.5.3.2.1.cmml">,</mo><mn id="S10.SS3.p1.6.m6.2.2" xref="S10.SS3.p1.6.m6.2.2.cmml">0</mn><mo id="S10.SS3.p1.6.m6.4.5.3.2.2.3" xref="S10.SS3.p1.6.m6.4.5.3.2.1.cmml">,</mo><mn id="S10.SS3.p1.6.m6.3.3" xref="S10.SS3.p1.6.m6.3.3.cmml">0</mn><mo id="S10.SS3.p1.6.m6.4.5.3.2.2.4" xref="S10.SS3.p1.6.m6.4.5.3.2.1.cmml">,</mo><mn id="S10.SS3.p1.6.m6.4.4" xref="S10.SS3.p1.6.m6.4.4.cmml">0</mn><mo stretchy="false" id="S10.SS3.p1.6.m6.4.5.3.2.2.5" xref="S10.SS3.p1.6.m6.4.5.3.2.1.cmml">]</mo></mrow><mo id="S10.SS3.p1.6.m6.4.5.3.3" xref="S10.SS3.p1.6.m6.4.5.3.3.cmml">⊤</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S10.SS3.p1.6.m6.4b"><apply id="S10.SS3.p1.6.m6.4.5.cmml" xref="S10.SS3.p1.6.m6.4.5"><eq id="S10.SS3.p1.6.m6.4.5.1.cmml" xref="S10.SS3.p1.6.m6.4.5.1"></eq><ci id="S10.SS3.p1.6.m6.4.5.2.cmml" xref="S10.SS3.p1.6.m6.4.5.2">𝒒</ci><apply id="S10.SS3.p1.6.m6.4.5.3.cmml" xref="S10.SS3.p1.6.m6.4.5.3"><csymbol cd="ambiguous" id="S10.SS3.p1.6.m6.4.5.3.1.cmml" xref="S10.SS3.p1.6.m6.4.5.3">superscript</csymbol><list id="S10.SS3.p1.6.m6.4.5.3.2.1.cmml" xref="S10.SS3.p1.6.m6.4.5.3.2.2"><cn type="integer" id="S10.SS3.p1.6.m6.1.1.cmml" xref="S10.SS3.p1.6.m6.1.1">1</cn><cn type="integer" id="S10.SS3.p1.6.m6.2.2.cmml" xref="S10.SS3.p1.6.m6.2.2">0</cn><cn type="integer" id="S10.SS3.p1.6.m6.3.3.cmml" xref="S10.SS3.p1.6.m6.3.3">0</cn><cn type="integer" id="S10.SS3.p1.6.m6.4.4.cmml" xref="S10.SS3.p1.6.m6.4.4">0</cn></list><csymbol cd="latexml" id="S10.SS3.p1.6.m6.4.5.3.3.cmml" xref="S10.SS3.p1.6.m6.4.5.3.3">top</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS3.p1.6.m6.4c">\bm{q}=[1,0,0,0]^{\top}</annotation></semantics></math> is returned. Such entries are not skipped to be in agreement with the submission rule of SPEC2021.</p>
</div>
<figure id="S10.F8" class="ltx_figure"><img src="/html/2110.03101/assets/x7.png" id="S10.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="207" height="264" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Visualization of the retained artifacts, such as the infrared markers (<em id="S10.F8.4.1" class="ltx_emph ltx_font_italic">top</em>, blue), mounting holes (<em id="S10.F8.5.2" class="ltx_emph ltx_font_italic">top</em>, red), and the unnatural shape of the surface glow (<em id="S10.F8.6.3" class="ltx_emph ltx_font_italic">bottom</em>).</figcaption>
</figure>
<figure id="S10.F9" class="ltx_figure"><img src="/html/2110.03101/assets/x8.png" id="S10.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="229" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Example samples rejected from <span id="S10.F9.4.1" class="ltx_text ltx_font_typewriter">sunlamp</span> domain due to extreme surface glow and camera overexposure (<em id="S10.F9.5.2" class="ltx_emph ltx_font_italic">left</em>) and failed post-processing (<em id="S10.F9.6.3" class="ltx_emph ltx_font_italic">right</em>).</figcaption>
</figure>
</section>
<section id="S10.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.4 </span>DANN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</h3>

<div id="S10.SS4.p1" class="ltx_para">
<p id="S10.SS4.p1.3" class="ltx_p">The domain classifier of DANN are attached to the MobileNetv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> feature extractor of KRN, which ends with a feature map of <math id="S10.SS4.p1.1.m1.1" class="ltx_Math" alttext="320\times 7\times 7" display="inline"><semantics id="S10.SS4.p1.1.m1.1a"><mrow id="S10.SS4.p1.1.m1.1.1" xref="S10.SS4.p1.1.m1.1.1.cmml"><mn id="S10.SS4.p1.1.m1.1.1.2" xref="S10.SS4.p1.1.m1.1.1.2.cmml">320</mn><mo lspace="0.222em" rspace="0.222em" id="S10.SS4.p1.1.m1.1.1.1" xref="S10.SS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S10.SS4.p1.1.m1.1.1.3" xref="S10.SS4.p1.1.m1.1.1.3.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S10.SS4.p1.1.m1.1.1.1a" xref="S10.SS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S10.SS4.p1.1.m1.1.1.4" xref="S10.SS4.p1.1.m1.1.1.4.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S10.SS4.p1.1.m1.1b"><apply id="S10.SS4.p1.1.m1.1.1.cmml" xref="S10.SS4.p1.1.m1.1.1"><times id="S10.SS4.p1.1.m1.1.1.1.cmml" xref="S10.SS4.p1.1.m1.1.1.1"></times><cn type="integer" id="S10.SS4.p1.1.m1.1.1.2.cmml" xref="S10.SS4.p1.1.m1.1.1.2">320</cn><cn type="integer" id="S10.SS4.p1.1.m1.1.1.3.cmml" xref="S10.SS4.p1.1.m1.1.1.3">7</cn><cn type="integer" id="S10.SS4.p1.1.m1.1.1.4.cmml" xref="S10.SS4.p1.1.m1.1.1.4">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS4.p1.1.m1.1c">320\times 7\times 7</annotation></semantics></math>. Here, after the gradient reversal layer, the domain classifier with two convolution layers is added. Specifically, the first layer is a 1 <math id="S10.SS4.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S10.SS4.p1.2.m2.1a"><mo id="S10.SS4.p1.2.m2.1.1" xref="S10.SS4.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.SS4.p1.2.m2.1b"><times id="S10.SS4.p1.2.m2.1.1.cmml" xref="S10.SS4.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.SS4.p1.2.m2.1c">\times</annotation></semantics></math> 1 convolution operation to 1280 channels followed by a ReLU activation and an average pooling layer. Then, the second layer uses another 1 <math id="S10.SS4.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S10.SS4.p1.3.m3.1a"><mo id="S10.SS4.p1.3.m3.1.1" xref="S10.SS4.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.SS4.p1.3.m3.1b"><times id="S10.SS4.p1.3.m3.1.1.cmml" xref="S10.SS4.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.SS4.p1.3.m3.1c">\times</annotation></semantics></math> 1 convolution to end with just 1 channel which is then classified to either source or target domain classes. The training of KRN with DANN follows the training routine of KRN above.</p>
</div>
</section>
<section id="S10.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.5 </span>Style augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</h3>

<div id="S10.SS5.p1" class="ltx_para">
<p id="S10.SS5.p1.2" class="ltx_p">The pre-trained style transfer network is directly taken from the official repository<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/philipjackson/style-augmentation</span></span></span>. Then, in order to facilitate online stylization at the data augmentation stage, the mean embedding of the SPEED+ <span id="S10.SS5.p1.2.1" class="ltx_text ltx_font_typewriter">synthetic</span> training set is interpolated with a random style embedding from <math id="S10.SS5.p1.1.m1.1" class="ltx_Math" alttext="{\mathbb{R}}^{100}" display="inline"><semantics id="S10.SS5.p1.1.m1.1a"><msup id="S10.SS5.p1.1.m1.1.1" xref="S10.SS5.p1.1.m1.1.1.cmml"><mi id="S10.SS5.p1.1.m1.1.1.2" xref="S10.SS5.p1.1.m1.1.1.2.cmml">ℝ</mi><mn id="S10.SS5.p1.1.m1.1.1.3" xref="S10.SS5.p1.1.m1.1.1.3.cmml">100</mn></msup><annotation-xml encoding="MathML-Content" id="S10.SS5.p1.1.m1.1b"><apply id="S10.SS5.p1.1.m1.1.1.cmml" xref="S10.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S10.SS5.p1.1.m1.1.1.1.cmml" xref="S10.SS5.p1.1.m1.1.1">superscript</csymbol><ci id="S10.SS5.p1.1.m1.1.1.2.cmml" xref="S10.SS5.p1.1.m1.1.1.2">ℝ</ci><cn type="integer" id="S10.SS5.p1.1.m1.1.1.3.cmml" xref="S10.SS5.p1.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS5.p1.1.m1.1c">{\mathbb{R}}^{100}</annotation></semantics></math> with <math id="S10.SS5.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S10.SS5.p1.2.m2.1a"><mi id="S10.SS5.p1.2.m2.1.1" xref="S10.SS5.p1.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S10.SS5.p1.2.m2.1b"><ci id="S10.SS5.p1.2.m2.1.1.cmml" xref="S10.SS5.p1.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S10.SS5.p1.2.m2.1c">\alpha</annotation></semantics></math> = 0.5. The training with style transfer network follows the training routine of KRN above.</p>
</div>
</section>
</section>
<section id="S11" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>Visualization</h2>

<section id="S11.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">11.1 </span>Retained Artifacts in HIL Images</h3>

<div id="S11.SS1.p1" class="ltx_para">
<p id="S11.SS1.p1.1" class="ltx_p">Figure <a href="#S10.F8" title="Figure 8 ‣ 10.3 HigherHRNet [41] ‣ 10 Training Details ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> visualizes the retained artifacts unique to HIL images.</p>
</div>
</section>
<section id="S11.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">11.2 </span>Images of Rejected Samples</h3>

<div id="S11.SS2.p1" class="ltx_para">
<p id="S11.SS2.p1.1" class="ltx_p">Figure <a href="#S10.F9" title="Figure 9 ‣ 10.3 HigherHRNet [41] ‣ 10 Training Details ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> visualizes the samples rejected from the <span id="S11.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">sunlamp</span> domain.</p>
</div>
</section>
<section id="S11.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">11.3 </span>Example HIL Images with Model Projection</h3>

<div id="S11.SS3.p1" class="ltx_para">
<p id="S11.SS3.p1.1" class="ltx_p">Figures <a href="#S11.F10" title="Figure 10 ‣ 11.3 Example HIL Images with Model Projection ‣ 11 Visualization ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> and <a href="#S11.F11" title="Figure 11 ‣ 11.3 Example HIL Images with Model Projection ‣ 11 Visualization ‣ SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> show example HIL images from both domains with the wireframe model of the Tango spacecraft projected based on associated labels and the provided camera properties. The alignment between the mockup and the wireframe models validates the accuracy of the HIL image labels estimated from the TRON facility.</p>
</div>
<figure id="S11.F10" class="ltx_figure"><img src="/html/2110.03101/assets/figures/lightbox.png" id="S11.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="300" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Example <span id="S11.F10.2.1" class="ltx_text ltx_font_typewriter">lightbox</span> images with the wireframe model of the Tango spacecraft projected based on associated labels and the provided camera properties.</figcaption>
</figure>
<figure id="S11.F11" class="ltx_figure"><img src="/html/2110.03101/assets/figures/sunlamp.png" id="S11.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="300" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Example <span id="S11.F11.2.1" class="ltx_text ltx_font_typewriter">sunlamp</span> images with the wireframe model of the Tango spacecraft projected based on associated labels and the provided camera properties.</figcaption>
</figure>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. B. Reed, R. C. Smith, B. J. Naasz, J. F. Pellegrino, and C. E. Bacon, “The
Restore-L servicing mission,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Aiaa Space 2016</em>, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. L. Forshaw, G. S. Aglietti, N. Navarathinam, H. Kadhem, T. Salmon,
A. Pisseloup, E. Joffre, T. Chabot, I. Retat, R. Axthelm, and et al.,
“RemoveDEBRIS: An in-orbit active debris removal demonstration mission,”
<em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Acta Astronautica</em>, vol. 127, p. 448–463, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Sharma and S. D’Amico, “Pose estimation for non-cooperative spacecraft
rendezvous using neural networks,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">2019 AAS/AIAA Space Flight
Mechanics Meeting, Ka’anapali, Maui, HI</em>, January 13-17 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T. H. Park, S. Sharma, and S. D’Amico, “Towards robust learning-based pose
estimation of noncooperative spacecraft,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2019 AAS/AIAA
Astrodynamics Specialist Conference, Portland, Maine</em>, August 11-15 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Kisantal, S. Sharma, T. H. Park, D. Izzo, M. Märtens, and S. D’Amico,
“Satellite pose estimation challenge: Dataset, competition design and
results,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Aerospace and Electronic Systems</em>, pp.
1–1, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
B. Chen, J. Cao, Á. P. Bustos, and T.-J. Chin, “Satellite pose estimation
with deep landmark regression and nonlinear pose refinement,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2019
IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</em>, pp.
2816–2824, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
L. P. Cassinis, R. Fonod, E. Gill, I. Ahrns, and J. G. Fernandez, “Cnn-based
pose estimation system for close-proximity operations around uncooperative
spacecraft,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">AIAA Scitech 2020 Forum</em>, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
P. F. Proenca and Y. Gao, “Deep learning for spacecraft pose estimation from
photorealistic rendering,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.04298</em>, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
L. Pasqualetto Cassinis, A. Menicucci, E. Gill, I. Ahrns, and J. Gil Fernandez,
“On-ground validation of a cnn-based monocular pose estimation system for
uncooperative spacecraft,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">8th European Conference on Space
Debris</em>, vol. 8, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K. Black, S. Shankar, D. Fonseka, J. Deutsch, A. Dhir, and M. Akella,
“Real-time, flight-ready, non-cooperative spacecraft pose estimation using
monocular imagery,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">31st AAS/AIAA Space Flight Mechanics Meeting</em>,
February 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira, “Analysis of
representations for domain adaptation,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, B. Schölkopf, J. Platt, and T. Hoffman,
Eds., vol. 19.   MIT Press, 2007.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and K. Saenko, “VisDA:
The visual domain adaptation challenge,” 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel,
“Domain randomization for transferring deep neural networks from simulation
to the real world,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2017 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS)</em>, 2017, pp. 23–30.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Zakharov, W. Kehl, and S. Ilic, “Deceptionnet: Network-driven domain
randomization,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF International Conference on Computer
Vision (ICCV)</em>.   Los Alamitos, CA, USA:
IEEE Computer Society, nov 2019, pp. 532–541. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00062</span>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and
W. Brendel, “Imagenet-trained CNNs are biased towards texture; increasing
shape bias improves accuracy and robustness.” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International
Conference on Learning Representations</em>, 2019. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=Bygh9j09KX</span>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Sharma, T. H. Park, and S. D’Amico, “Spacecraft pose estimation dataset
(speed),” Stanford Digital Repository. Available at:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.25740/dz692fn7184</span>, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. D’Amico, M. Benn, and J. L. Jørgensen, “Pose estimation of an
uncooperative spacecraft from actual space imagery,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">International
Journal of Space Science and Engineering</em>, vol. 2, no. 2, p. 171, 2014.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. D’Amico, P. Bodin, M. Delpech, and R. Noteborn, “PRISMA,” in
<em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Distributed Space Missions for Earth System Monitoring Space Technology
Library</em>, M. D’Errico, Ed., 2013, vol. 31, ch. 21, pp. 599–637.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Sharma, C. Beierle, and S. D’Amico, “Pose estimation for
non-cooperative spacecraft rendezvous using convolutional neural networks,”
in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Aerospace Conference</em>, March 2018, pp. 1–12.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C. Beierle and S. D’Amico, “Variable-magnification optical stimulator for
training and validation of spaceborne vision-based navigation,”
<em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Journal of Spacecraft and Rockets</em>, vol. 56, pp. 1–13, Feb 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
V. Giralo and S. D’Amico, “Development of the stanford gnss navigation testbed
for distributed space systems,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Institute of Navigation,
International Technical Meeting, Reston, Virginia</em>, January 29 - February 1
2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. A. Dung, B. Chen, and T.-J. Chin, “A spacecraft dataset for detection,
segmentation and parts recognition,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, June
2021, pp. 2012–2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
P. Tsiotras, “Astros: A 5dof experimental facility for research in space
proximity operations,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in the Astronautical Sciences</em>, vol.
151, pp. 717–730, 01 2014.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R. Zappulla, J. Virgili-Llop, C. Zagaris, H. Park, and M. Romano, “Dynamic
air-bearing hardware-in-the-loop testbed to experimentally evaluate
autonomous spacecraft proximity maneuvers,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Journal of Spacecraft and
Rockets</em>, vol. 54, no. 4, pp. 825–839, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. K. Nakka, R. C. Foust, E. S. Lupu, D. B. Elliott, I. S. Crowell, S.-J.
Chung, and F. Y. Hadaegh, “A six degree-of-freedom spacecraft dynamics
simulator for formation control research,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2018 AAS/AIAA
Astrodynamics Specialist Conference, Snowbird, UT</em>, August 19-23 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
K. Saenko, B. Kulis, M. Fritz, and T. Darrell, “Adapting visual category
models to new domains,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Computer Vision – ECCV 2010</em>,
K. Daniilidis, P. Maragos, and N. Paragios, Eds., 2010.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
X. Peng, B. Usman, K. Saito, N. Kaushik, J. Hoffman, and K. Saenko, “Syn2real:
A new benchmark for synthetic-to-real visual domain adaptation,”
<em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1806.09755, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang, “Moment matching
for multi-source domain adaptation,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
International Conference on Computer Vision</em>, 2019, pp. 1406–1415.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S. R. Richter, V. Vineet, S. Roth, and V. Koltun, “Playing for data: Ground
truth from computer games,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Computer Vision – ECCV 2016</em>,
B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds.   Cham: Springer International Publishing, 2016, pp. 102–118.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The synthia
dataset: A large collection of synthetic images for semantic segmentation of
urban scenes,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, June 2016.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? the
kitti vision benchmark suite,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, 2012.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic
urban scene understanding,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proc. of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, 2016.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
T. H. Park, J. Bosse, and S. D’Amico, “Robotic testbed for rendezvous and
optical navigation: Multi-source calibration and machine learning use
cases,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2021 AAS/AIAA Astrodynamics Specialist Conference, Big Sky,
Vitrual</em>, August 9-11 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
“Verification of light-box devices for earth albedo simulation,” Technical
Note, Stanford Space Rendezvous Lab (SLAB), January (2016).

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
D. Bhanderi and T. Bak, “Modeling earth albedo for satellites in earth
orbit,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of AIAA Conference on Guidance, Navigation and
Control</em>.   AIAA, 2005, null ;
Conference date: 15-08-2005 Through 18-08-2005.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
E. ASTM, “490, standard solar constant and zero air mass solar spectral
irradiance tables,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Space Simulation and Applications of Space
Technology</em>, 2000.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
“Vero: Compact super wide camera by vicon,”
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.vicon.com/hardware/cameras/vero/</span>, accessed April 29, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
K. Shoemake, “Iii.6 - uniform random rotations,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Graphics Gems III
(IBM Version)</em>, D. Kirk, Ed.   San
Francisco: Morgan Kaufmann, 1992, pp. 124 – 132.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
S. Sharma and S. D’Amico, “Neural network-based pose estimation for
noncooperative spacecraft rendezvous,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Aerospace
and Electronic Systems</em>, vol. 56, no. 6, pp. 4638–4658, 2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
K. Bessho, K. Date, M. Hayashi, A. Ikeda, T. Imai, H. Inoue, Y. Kumagai,
T. Miyakawa, H. Murata, T. Ohno, A. Okuyama, R. Oyama, Y. Sasaki, Y. Shimazu,
K. Shimoji, Y. Sumida, M. Suzuki, H. Taniguchi, H. Tsuchiyama, D. Uesawa,
H. Yokota, and R. Yoshida, “An Introduction to Himawari-8/9
— Japan’s New-Generation Geostationary Meteorological
Satellites,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Journal of the Meteorological Society of Japan. Ser.
II</em>, vol. 94, no. 2, pp. 151–183, 2016.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, and L. Zhang, “Higherhrnet:
Scale-aware representation learning for bottom-up human pose estimation,” in
<em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
V. Lepetit, F. Moreno-Noguer, and P. Fua, “EPnP: An accurate O(n) solution
to the PnP problem,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>,
vol. 81, no. 2, p. 155–166, 2008.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
P. T. Jackson, A. Atapour-Abarghouei, S. Bonner, T. P. Breckon, and B. Obara,
“Style augmentation: Data augmentation via style randomization,” in
<em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops</em>, 2019, pp. 83–92.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by
backpropagation,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd International Conference
on Machine Learning</em>, ser. Proceedings of Machine Learning Research, F. Bach
and D. Blei, Eds., vol. 37.   Lille,
France: PMLR, 07–09 Jul 2015, pp. 1180–1189.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
M. March, and V. Lempitsky, “Domain-adversarial training of neural
networks,” <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, vol. 17, no. 59, pp.
1–35, 2016. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://jmlr.org/papers/v17/15-239.html</span>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens, “Exploring the
structure of a real-time, arbitrary neural artistic stylization network,” in
<em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">British Machine Vision Conference</em>, 2017.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
T. H. Park, M. Märtens, G. Lecuyer, D. Izzo, and S. D’Amico, “Next
generation spacecraft pose estimation dataset (SPEED+),” Stanford Digital
Repository, 2021, available at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://purl.stanford.edu/wv398fc4383</span>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
I. Loshchilov and F. Hutter, “Fixing weight decay regularization in adam,”
<em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1711.05101, 2017. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/1711.05101</span>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with
deep convolutional neural networks,” in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.
Weinberger, Eds., vol. 25.   Curran
Associates, Inc., 2012.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. Chen,
“MobileNetv2: Inverted residuals and linear bottlenecks,” in <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">2018
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2018, pp.
4510–4520.

</span>
</li>
</ul>
</section>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\thebiography</span>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">{biographywithpic}</span>
<p id="p2.2" class="ltx_p">Tae Ha Parkbios/jeff.jpg
is a Ph.D. candidate in the Space Rendezvous Laboratory, Stanford University. He graduated from Harvey Mudd College with a Bachelor of Science degree (2017) in engineering. His research interest is in the development of machine learning techniques and GN&amp;C algorithms for spaceborne computer vision tasks, specifically on robust and accurate determination of the relative position and attitude of arbitrary resident space objects using monocular vision. Potential applications include space debris removal and refueling of defunct geostationary satellites with unprecedented autonomy and safety measures.</p>
</div>
<div id="p3" class="ltx_para">
<span id="p3.1" class="ltx_ERROR undefined">{biographywithpic}</span>
<p id="p3.2" class="ltx_p">Marcus Märtensbios/marcus.jpg
graduated from the University of Paderborn (Germany) with a Masters degree in computer science. He joined the European Space Agency as a Young Graduate Trainee in artificial intelligence where he worked on multi-objective optimization of spacecraft trajectories. He was part of the winning team of the 8th edition of the Global Trajectory Optimization Competition (GTOC) and received a HUMIES gold medal for developing algorithms achieving human competitive results in trajectory design. The Delft University of Technology awarded him a Ph.D. for his thesis on information propagation in complex networks. After his time at the network architectures and services group in Delft (Netherlands), Marcus rejoined the European Space Agency, where he works as a research follow in the Advanced Concepts Team. While his main focus is on applied artificial intelligence and evolutionary optimization, Marcus has worked together with experts from different fields and authored works related to neuroscience, cyber-security and gaming.</p>
</div>
<div id="p4" class="ltx_para">
<span id="p4.2" class="ltx_ERROR undefined">{biographywithpic}</span>
<p id="p4.1" class="ltx_p">Gurvan Lecuyerbios/gurvan.jpg
graduated from the National Institute of Applied Science of Rennes (France) with a Master’s degree in Computer Engineering. He then completed a Ph.D. in signal and image processing at the University of Rennes (France), working on an artificial intelligence algorithm to accelerate the annotation of surgical videos. After that, Gurvan Lecuyer joined the European Space Agency as a Research Fellow in the Advanced Concept Team and <math id="p4.1.m1.1" class="ltx_Math" alttext="\Phi" display="inline"><semantics id="p4.1.m1.1a"><mi mathvariant="normal" id="p4.1.m1.1.1" xref="p4.1.m1.1.1.cmml">Φ</mi><annotation-xml encoding="MathML-Content" id="p4.1.m1.1b"><ci id="p4.1.m1.1.1.cmml" xref="p4.1.m1.1.1">Φ</ci></annotation-xml><annotation encoding="application/x-tex" id="p4.1.m1.1c">\Phi</annotation></semantics></math>-lab to work on artificial intelligence, focusing on the application for Earth Observation.</p>
</div>
<div id="p5" class="ltx_para">
<span id="p5.1" class="ltx_ERROR undefined">{biographywithpic}</span>
<p id="p5.2" class="ltx_p">Dario Izzobios/dario.png
graduated as a Doctor of Aeronautical Engineering from the University Sapienza of Rome (Italy). He then took a second master in Satellite Platforms at the University of Cranfield in the United Kingdom and completed his Ph.D. in Mathematical Modelling at the University Sapienza of Rome where he lectured classical mechanics and space flight mechanics.</p>
</div>
<div id="p6" class="ltx_para">
<p id="p6.1" class="ltx_p">Dario Izzo later joined the European Space Agency and became the scientific coordinator of its Advanced Concepts Team. He devised and managed the Global Trajectory Optimization Competitions events, the ESA Summer of Code in Space and the Kelvins innovation and competition platform. He published more than 170 papers in international journals and conferences making key contributions to the understanding of flight mechanics and spacecraft control and pioneering techniques based on evolutionary and machine learning approaches.</p>
</div>
<div id="p7" class="ltx_para">
<p id="p7.1" class="ltx_p">Dario Izzo received the Humies Gold Medal and led the team winning the 8<math id="p7.1.m1.1" class="ltx_Math" alttext="{}^{\textrm{th}}" display="inline"><semantics id="p7.1.m1.1a"><msup id="p7.1.m1.1.1" xref="p7.1.m1.1.1.cmml"><mi id="p7.1.m1.1.1a" xref="p7.1.m1.1.1.cmml"></mi><mtext id="p7.1.m1.1.1.1" xref="p7.1.m1.1.1.1a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="p7.1.m1.1b"><apply id="p7.1.m1.1.1.cmml" xref="p7.1.m1.1.1"><ci id="p7.1.m1.1.1.1a.cmml" xref="p7.1.m1.1.1.1"><mtext mathsize="70%" id="p7.1.m1.1.1.1.cmml" xref="p7.1.m1.1.1.1">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p7.1.m1.1c">{}^{\textrm{th}}</annotation></semantics></math> edition of the Global Trajectory Optimization Competition.</p>
</div>
<div id="p8" class="ltx_para">
<span id="p8.1" class="ltx_ERROR undefined">{biographywithpic}</span>
<p id="p8.2" class="ltx_p">Simone D’Amicobios/simone.jpg
received the B.S. and M.S. degrees from Politecnico di Milano (2003) and the Ph.D. degree from Delft University of Technology (2010). From 2003 to 2014, he was research scientist and team leader at the German Aerospace Center (DLR). There, he gave key contributions to the design, development, and operations of spacecraft formation-flying and rendezvous missions such as GRACE (United States/Germany), TanDEM-X (Germany), PRISMA (Sweden/Germany/France), and PROBA-3 (ESA). Since 2014, he has been Assistant Professor of Aeronautics and Astronautics at Stanford University, Founding director of the Space Rendezvous Laboratory (SLAB), and Satellite Advisor of the Student Space Initiative (SSSI), Stanford’s largest undergraduate organization. He has over 150 scientific publications and 2500 google scholar’s citations, including conference proceedings, peer-reviewed journal articles, and book chapters. D’Amico’s research aims at enabling future miniature distributed space systems for unprecedented science and exploration. His efforts lie at the intersection of advanced astrodynamics, GN&amp;C, and space system engineering to meet the tight requirements posed by these novel space architectures. The most recent mission concepts developed by Dr. D’Amico are a miniaturized distributed occulter/telescope (mDOT) system for direct imaging of exozodiacal dust and exoplanets and the Autonomous Nanosatellite Swarming (ANS) mission for characterization of small celestial bodies. He is Chairman of the NASA’s Starshade Science and Technology Working Group (TSWG) and Fellow of the NAE’s US FOE Symposium. D’Amico’s research is supported by NASA, NSF, AFRL, AFOSR, KACST, and Industry. He is member of the advisory board of space startup companies and VC edge funds. He is member of the Space-Flight Mechanics Technical Committee of the AAS, Associate Fellow of AIAA, Associate Editor of the AIAA Journal of Guidance, Control, and Dynamics and the IEEE Transactions of Aerospace and Electronic Systems. Dr. D’Amico was recipient of the Leonardo 500 Award by the Leonardo Da Vinci Society and ISSNAF (2019), the Stanford’s Introductory Seminar Excellence Award (2019), the FAI/NAA‘s Group Diploma of Honor (2018), the Exemplary System Engineering Doctoral Dissertation Award by the International Honor Society for Systems Engineering OAA (2016), the DLR’s Sabbatical/Forschungssemester in honor of scientific achievements (2012), the DLR’s Wissenschaft Preis in honor of scientific achievements (2006), and the NASA’s Group Achievement Award for the Gravity Recovery and Climate Experiment, GRACE (2004).</p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2110.03100" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2110.03101" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2110.03101">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2110.03101" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2110.03102" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 02:53:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
